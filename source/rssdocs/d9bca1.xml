<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/B4v0rg7s7Pj9mDs6pVld</id>
            <title>智能体技术发展趋势：李鹏谈大模型智能体与开放领域融合</title>
            <link>https://www.infoq.cn/article/B4v0rg7s7Pj9mDs6pVld</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/B4v0rg7s7Pj9mDs6pVld</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 May 2024 16:07:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型智能体, 开放环境, 技术挑战, 智能体应用
<br>
<br>
总结: 大模型智能体作为前沿探索的焦点，正逐步塑造未来技术与社会交互的新形态。随着 AI 技术的飞速跃进，这些智能体被寄予厚望，期望能在复杂多变、充满不确定性的开放环境中自如运作，从个性化推荐到高级人机协作，其应用前景广阔无垠。然而，要实现这一愿景，必须跨越多重技术与理论障碍，包括但不限于如何使智能体在不断变化的环境中持续学习、有效适应各类环境后效性、在多元目标间实现精准平衡，以及如何主动感知并智能响应环境信息等。 </div>
                        <hr>
                    
                    <p>大模型智能体作为前沿探索的焦点，正逐步塑造未来技术与社会交互的新形态。随着 AI 技术的飞速跃进，这些智能体被寄予厚望，期望能在复杂多变、充满不确定性的开放环境中自如运作，从个性化推荐到高级人机协作，其应用前景广阔无垠。然而，要实现这一愿景，必须跨越多重技术与理论障碍，包括但不限于如何使智能体在不断变化的环境中持续学习、有效适应各类环境后效性、在多元目标间实现精准平衡，以及如何主动感知并智能响应环境信息等。</p><p></p><p>在这样的背景下，我们在 AICon 全球人工智能开发与应用大会，荣幸邀请到清华大学 智能产业研究院副教授李鹏为你分享《面向开放域的大模型智能体》，我们有幸采访了李鹏老师。在我们的独家访谈中，他指出当前大模型智能体在处理开放环境不确定性与动态变化时的核心挑战，包括但不限于推断时学习的作用机制、环境后效性的有效建模与利用、跨环境的泛化能力、多目标优化的复杂性，以及主动感知技术的迫切需求。</p><p></p><p>本文为采访实录，经编辑。</p><p></p><p>InfoQ：您如何看待当前大模型在处理不确定性和动态变化环境中的挑战？是否有特定的技术或方法可以提高其泛化能力和可解释性？</p><p></p><p>李鹏：面对当前 AI 发展的挑战，我总结了以下关键点，每个都深刻影响着大模型在开放环境下应用的效能与灵活性：</p><p></p><p>● 推断时学习（Inference-time Learning）的实现：传统模型训练依赖于大量静态数据集，但在动态、不确定的环境中，这种模式显得局限。我们需要模型具备“学后学习”能力，即在完成初始训练后，仍能高效学习新信息，同时避免“灾难性遗忘”。这要求平衡新知识的吸收与旧知识的保留，以及在有限、快速变化的数据中高效学习，这是对现有学习机制的一大挑战。</p><p></p><p>● 有效建模与利用环境后效性（Environmental Aftereffect）：智能体与环境的互动经常引起环境状态的持久变化，如在线购物平台根据用户行为调整推荐内容所展示的个性化效果。挑战在于，设计模型不仅需准确反映这种互动的后果（即后效性），还应能预见并利用这些变化以优化其后续行动。这意味着模型需集成复杂的因果推理和策略规划能力，以预测并积极引导环境变化，为达成目标服务。</p><p></p><p>● 跨环境泛化（Cross-environment Generalization）的实现：由于环境数量无限且存在后效性，无法为每个环境单独训练智能体。因此，可以在一些代表性环境中进行训练，以获取与环境无关或可泛化到广泛环境的基础能力或知识。这样，智能体在新环境中能够快速适应并执行任务，利用先前在其他环境中学到的知识和能力。这种跨环境泛化的方法有助于解决面对无限数量环境的挑战，提高智能体在不同环境中的适应性和表现。</p><p></p><p>● 多目标优化（Multi-objective Optimization），在复杂的开放领域应用场景中，智能体需同时追求多个可能相互矛盾的目标（如高效率、低成本、快速执行），这些目标间的权衡增加了决策的复杂度。传统上，多目标优化就是一个难题，而随着智能系统迈向更加开放和动态的环境，有效平衡及优化这些相冲突的目标变得尤为重要和紧迫。因此，开发能够自动调节和优先处理多目标间关系的算法，确保智能体在实际操作中既能达成高质量性能，又能考虑效率、经济性等其他关键指标，是当前研究与实践中的一个重大挑战。</p><p></p><p>● 主动感知（Active Perception），即智能体应具备根据当前任务需求和执行进度，自发地、有选择性地向环境索取信息的能力，而非依赖外部指令被动接收数据。这要求智能体不仅要能高效处理接收到的信息，还需智能地决定感知什么、何时感知以及如何感知，体现了与传统被动感知模型的根本区别。</p><p></p><p>至于说解决方案，当前大模型与智能体技术正处于快速发展阶段，全面应对上述挑战的成熟解决方案尚未完全形成。各个领域虽已见证了一系列积极探索，比如推断时学习算法的进步、基于环境交互的动态知识整合训练策略，以及我们团队正在进行的相关研究，但至今尚缺乏一套系统性、全方位解决这些复杂问题的技术框架。正是由于此现状，强调智能体与人类目标、环境需求、以及自身约束之间的统一对齐，即“智能体 - 人类 - 环境统一对齐原则”，显得尤为重要。</p><p></p><p>InfoQ：这个在大模型应用中，您认为哪些领域需要这种技术支持？</p><p></p><p>李鹏：几乎所有涉足开放域应用的场景都会不同程度地遭遇这些难题，尤其是那些强调个性化和具身化交互的领域。在这些情境下，无论是为了满足用户独特偏好的个性化需求，还是实现智能体在具体环境中的有效操作与适应，解决数据多样性、动态环境适应性、多目标优化、主动感知及少数样本学习等问题的重要性尤为凸显。因此，诸如个性化推荐、虚拟助理、沉浸式交互、自适应教育、智能健康监护、以及高级的人机协作系统等应用领域，对于支持开放域技术的需求尤为迫切。</p><p></p><p>InfoQ：您如何定义智能体、人类和环境的统一对齐？目前是否有切实的解决方案？</p><p></p><p>李鹏：我们的核心观点在于，智能体研究应当超越单纯追求下游任务成功率的局限，转向一个更为综合的视角，着重考虑智能体、人类用户与环境三者之间相互作用的需求协调。</p><p></p><p>这意味着，在设计和评估智能系统时，不仅要着眼于任务完成度，更要深入理解并满足人在交互中的便捷性期望、个性化偏好，以及适应环境变化的能力等。</p><p></p><p>以理想的购物助手为例，其价值不仅体现在完成购买操作，更在于能够通过简洁的指令理解复杂需求。用户期望无需详尽指定品牌、型号，智能助手便能基于用户历史偏好、上下文暗示，精准推荐所需商品，实现智能体与用户意图的无缝对接，这即是智能体需与人类意图对齐的体现。此外，该智能助手还需具备适应电商网站动态变化的能力，如商品更新、界面调整等，确保在不断演化的环境中依旧能有效执行任务，这即是智能体与环境规律对齐的体现。</p><p></p><p>进一步而言，用户不仅期望智能体能准确理解并迅速执行任务，如高效完成购物而不拖延，还期待整个过程的成本效益最大化。换句话说，用户不希望智能体的运行导致不必要的开销，或是因低效而增加等待时间。这就要求智能体的设计需兼顾效率与经济性，确保其自身运作的智能化，即在满足任务需求的同时，优化资源使用、降低成本，避免不必要的延迟或浪费。这就是智能体与自身限制对齐的体现。</p><p></p><p>至今为止，尚未有智能体能完全达到智能体、人类与环境三者间的理想对齐状态，这反映出该目标的实现颇具挑战且尚未成为广泛研究的重点。正因如此，我们认为当前提出这一议题极具价值和前瞻性。</p><p></p><p>InfoQ：如何通过智能体来指导代价敏感的特征获取过程？这种方法在哪些应用场景中表现最佳？</p><p></p><p>李鹏：在整个过程中，我们的目标并非单纯让智能体提取特征，而是探索其他途径以实现这一目标。尽管对智能体决策成本的研究已初见端倪并积累了一定成果，该领域仍处于发展阶段，存在广阔的探索空间。在即将呈现的演讲中，我们将详述一项创新方法，即当智能体的学习预算（budget）受限时，我们采用了一种融合预规划的手段来优化学习过程，并已观察到积极的效果。当然过往的研究中，学者们尝试利用大规模预训练模型设计奖励机制等策略，这些方法同样展现出了解决类似问题的巨大潜力，也是值得学习的。</p><p></p><p>InfoQ：您认为未来在代价敏感智能体方面，还有哪些待解决的关键技术问题？</p><p></p><p>李鹏：我认为，首要任务是建立健全针对代价敏感智能体及其评估框架的体系。当前的基准测试 (Benchmark) 大多侧重于任务完成度，却忽略了成本效益分析，这是一个亟待填补的空白。因此，开发一套全面考量智能体表现及成本的评估方法至关重要，它既要衡量成效，也要顾及成本开销，这两个核心指标本质上可能存在冲突，需要精心设计平衡，确保评估体系能精准识别并促进系统效能的提升，这是第一个值得深入探讨的议题。</p><p></p><p>其次，针对多目标优化策略的融入也是不可或缺的一环。在这样的复杂环境中，如何有效地整合多目标优化算法，以同时追求高效率与低成本，是实践中的又一挑战。</p><p></p><p>第三，更广泛地讲，我们应致力于研发更为先进的智能体学习机制。这一点虽然与多目标优化有所交集，但第二点更多的是关注代价函数，而这个学习机制将在更多层面发挥作用，如持续演进的探索机制等。</p><p></p><p>InfoQ：您如何看待将大模型与特定领域知识结合的趋势？这在提高智能体在特定任务上的表现方面有何优势？</p><p></p><p>李鹏：对于大模型与特定领域知识结合的趋势，我持肯定态度，我认为这是大模型迈向更广阔应用场景，尤其是在开放领域中不可或缺的发展路径。尽管现下关于最佳结合策略尚未形成统一意见，无论是通过微调、RAG 或是其他创新性推理学习机制，这一融合趋势本身已成为业界共识。</p><p></p><p>至于智能体技术在此背景下的角色，其作为连接知识获取与特定领域应用的抽象化手段，显得尤为重要。智能体不仅促进了领域知识的有效吸收，同时也得益于领域知识的加持，在特定任务中展现出更优的执行能力。这一互动过程类似于人类个体的专长发展：个人在特定领域的特长越显著，相关领域知识获取速度越快；反之，对该领域的深刻理解又反过来促进其专业能力的提升，形成了一个正向循环的增强过程。</p><p></p><p>InfoQ：在设计这类智能体时，如何平衡通用性和专业性，以适应不同的应用场景？</p><p></p><p>李鹏：我认为多智能体系统提供了一个天然的解决方案思路。单一智能体在同时追求高度专业化与广泛领域适应性上面临挑战，这要求它既要精通特定领域，又要保持足够的泛化能力，实为不易。而多智能体架构则巧妙绕过了这一难题，它允许系统中并存通用型智能体与领域专用智能体。通用智能体擅长处理高层次的策略规划、任务分解及综合归纳等全局性任务；与此同时，领域专用智能体则专注于特定领域的深度知识与高效执行。通过它们之间的协同作业，系统能够更灵活、高效地达成任务目标，从而实现两者平衡的优化。</p><p></p><p>InfoQ：大模型智能体在环境感知方面的最新进展是什么？这样的趋势下，我们应该如何提升智能体交互与理解能力？</p><p></p><p>李鹏：当前环境领域展现的最显著趋势是环境日益增长的复杂度与真实性，这一演变可从三个核心方面概述：</p><p></p><p>● 纯数字环境正经历显著变化，其特点在于设计者愈发重视环境的动态交互成本及其中任务的复杂层级，推动这些虚拟场景朝向更高程度的真实感发展。</p><p></p><p>● 数字模拟环境作为衔接虚拟与现实的桥梁，利用计算技术模仿物理环境，旨在解决实际物理世界的挑战。例如，近期备受瞩目的项目 Sora，其潜力在于可能充当物理世界的高效模拟器，凸显了该领域的前沿探索。</p><p></p><p>● 物理世界环境的融入加深，不仅体现在工业界和学术界的广泛关注上，还反映在致力于减少物理数据采集成本的努力中。引人注目的是，斯坦福大学等机构开发的创新硬件，如成本仅 400 美元的机械手示教设备，通过佩戴操作直接采集数据，极大促进了智能技术与物理环境融合的基础建设，加速了实体世界数据获取的效率与可行性。</p><p></p><p>为了提升智能体的交互理解能力，核心在于借鉴大型语言模型的成功要素：海量数据、更庞大模型及高效训练策略。针对开放领域的智能体，数据的丰富性尤为关键。不论是数字环境、模拟环境或实体环境，研究焦点集中于两方面：一是创造更多样化、复杂且逼真的场景，以模拟真实世界的广泛挑战；二是优化数据采集过程，降低成本，提高从环境中提取有效信息的效率。这一系列环境构建与优化的趋势，无疑将极大地推动智能体技术的进展。</p><p></p><p>InfoQ：您本次分享想要为听众带来什么分享？带来哪些收获？</p><p></p><p>李鹏：这次演讲，我核心将分享大模型的智能体和开放领域去结合的时候，将会面临的挑战以及解决挑战的典型的新思路，虽然可能我没有办法完全覆盖所有的思路，但是可以给大家一些启发。最后，我将简要总结大模型智能体与开放域结合方面未来的发展方向。</p><p></p><p>嘉宾介绍</p><p></p><p>李鹏：清华大学 智能产业研院（AIR） 副研究员 / 副教授，主要研究兴趣包括自然语言处理、预训练语言模型、跨模态信息处理、大模型智能体等，在人工智能重要国际会议与期刊发表论文 90 余篇，曾获 ACL 2023 杰出论文奖，曾在多个国际上深具影响力的榜单上超过 Google Research、OpenAI 等团队获得第一名，主持科技创新 2030 重大项目课题、国家自然科学基金面上等科技项目，曾任 NAACL、COLING、EACL、AACL 等会议领域主席或资深领域主席。研究成果在百度、腾讯微信等千万级日活产品中获得应用并取得显著成效，获得中国中文信息学会钱伟长中文信息处理科学技术奖一等奖。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/M1GL8X7JrHfwq2qew3DE</id>
            <title>在狂卷大模型的时代，这项生产要素影响着大模型的未来</title>
            <link>https://www.infoq.cn/article/M1GL8X7JrHfwq2qew3DE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/M1GL8X7JrHfwq2qew3DE</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 May 2024 15:36:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 生成式AI技术, 数据, 数据多样性, 数据质量和准确性
<br>
<br>
总结: 随着生成式AI技术的快速发展，数据已成为企业竞争力的核心要素。在这个时代，拥有全面、高效的数据基座是企业充分发挥数据潜力、加速生成式AI技术落地的关键。 </div>
                        <hr>
                    
                    <p>随着生成式AI技术的快速发展，数据已成为企业竞争力的核心要素。在这个时代，拥有全面、高效的数据基座是企业充分发挥数据潜力、加速生成式AI技术落地的关键。</p><p>&nbsp;</p><p>随着科技的飞速发展，人工智能已经进入了一个全新的时代——生成式人工智能时代。在这个时代，生成式人工智能模型的发展离不开一个关键因素，那就是数据。数据对于生成式人工智能模型的重要性不言而喻，它不仅是模型训练的基础，也是模型创新和应用的源泉。为什么对于大模型而言数据如此重要？</p><p>&nbsp;</p><p>数据是生成式人工智能模型训练的基础，这是业内普遍的共识。无论是传统的机器学习模型还是现代的深度学习模型，都需要大量的数据进行训练，以提高模型的准确性和泛化能力。生成式人工智能模型更是如此，它通过学习大量的高质量数据，从而实现对新生成数据的预测和生成。没有足够的数据支持，生成式人工智能模型很难达到理想的性能。</p><p>&nbsp;</p><p>此外，数据多样性对于生成式人工智能模型的发展至关重要。生成式人工智能模型需要处理各种类型的数据，包括文本、图像、音频等。这些数据不仅需要量大，而且需要具备多样性，以保证模型能够应对各种不同的场景和任务。数据多样性可以帮助模型学习到更多的特征和规律，从而提高模型的灵活性和适应性。</p><p>&nbsp;</p><p>同时，数据的质量和准确性也是生成式人工智能模型发展的关键。生成式人工智能模型需要处理大量的数据，如果数据存在错误、噪声或者不准确的情况，那么模型很可能会学习到错误的规律和特征，导致模型性能的下降。因此，保证数据的质量和准确性对于生成式人工智能模型的发展至关重要。</p><p>&nbsp;</p><p>数据的实时更新和迭代也同样不容忽视。随着社会的发展和技术的进步，新的数据不断涌现。生成式人工智能模型需要实时地获取和处理这些新数据，以不断提高模型的性能和适应性。数据的实时更新和迭代可以帮助模型紧跟时代的步伐，从而在各个领域发挥更大的作用。</p><p>&nbsp;</p><p>亚马逊云科技大中华区产品部总经理陈晓建表示：“在生成式AI时代，企业需要的是懂业务、懂用户的生成式AI应用，而打造这样的应用需要从数据做起。亚马逊云科技构建数据基座的三大核心能力涵盖从基础模型训练到生成式AI应用构建的重要场景，能够帮助企业轻松应对海量多模态数据，提升基础模型能力。”</p><p>&nbsp;</p><p>数据处理能力是生成式AI基础模型微调和预训练的关键。亚马逊云科技提供数据存储、清洗和治理服务，如Amazon S3、Amazon FSx for Lustre、Amazon EMR Serverless和Amazon Glue等，这些服务能够帮助企业高效地处理海量数据，提高模型训练质量。</p><p>&nbsp;</p><p>此外，数据与模型的快速结合也是企业数据基座的关键能力之一。亚马逊云科技将向量搜索的支持功能加入到主流的数据服务中，通过将数据和向量存储在一起，提升数据查询性能。这使得企业能够轻松利用RAG技术将专有数据提供给基础模型，从而释放更大价值。</p><p>&nbsp;</p><p>在处理生成式AI应用的新数据方面，亚马逊云科技提供了Amazon Memory DB等高效的数据处理服务，能够降低模型频繁调用成本并提升性能。此外，无服务器数据库服务和Amazon OpenSearch Serverless的引入，更是最大限度地减少了企业的运维负担和成本。</p><p>&nbsp;</p><p>根据IDC的数据显示，全球生成式AI市场规模预计将在2024年达到100亿美元，年复合增长率达到40%。这一数据充分展示了生成式AI技术的巨大潜力和市场需求。在这个时代，拥有全面、高效的数据基座和懂业务、懂用户的生成式AI应用将成为企业脱颖而出的关键。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/7i2VwDjI6MR4Bx8Y66D9</id>
            <title>裁员、人去楼空，这家估值80亿的AI编程工具独角兽不行了？</title>
            <link>https://www.infoq.cn/article/7i2VwDjI6MR4Bx8Y66D9</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/7i2VwDjI6MR4Bx8Y66D9</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 May 2024 09:46:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI编码工具, 裁员, Replit, 人工智能
<br>
<br>
总结: 近日，AI编码工具初创公司Replit宣布裁员30名员工，占员工总数近20%。公司首席执行官表示裁员是为了更好地满足不断变化的业务需求，同时强调公司的长期目标和使命。裁员消息引发了关于公司未来发展和人工智能在编码工具中的应用的讨论。 </div>
                        <hr>
                    
                    <p>近日，据外媒报道， AI编码工具初创公司Replit宣布将解雇 30 名员工，占其员工总数的近 20%。</p><p>&nbsp;</p><p>Replit 首席执行官 Amjad Masad 在发给员工的电子邮件中分享了这一消息，该电子邮件后来发布在 X（以前的 Twitter）上。 Masad 在邮件中承认，与 30 名“杰出同事”分道扬镳是一个艰难的决定，并强调此举对于 Replit 实现其长期目标和使命是必要的。</p><p>&nbsp;</p><p></p><h2>Replit宣布裁员30人，公司人去楼空？</h2><p></p><p>&nbsp;</p><p>Amjad Masad邮件全文翻译如下：</p><p></p><blockquote>在过去的一年里，我们一直在努力打造一个名为Replit的平台，该平台旨在使公司中的任何人都能成为程序员。虽然机会显而易见，但我逐渐意识到，为了服务企业，我们需要不同角色的组合。因此，我们决定与30%的杰出同事分道扬镳。很快您将收到一封电子邮件，告知我们是否不再需要您的职位。&nbsp;对于那些离开我们的人，我们不会轻率地看待这对您产生的影响。虽然我们知道您都很有才华，并期待您在新的角色中取得成功，为了让过渡更加容易，我们比正常情况更加慷慨：提供4个月的遣散费和6个月的医疗保险，免除1年的归属期限制，给您一年时间来决定是否行使您的股票期权，并允许您保留Replit提供的笔记本电脑和设备。我们的招聘团队将为需要的人提供求职支持，我们的投资者也准备将您介绍给其他公司。PeopleOps将提供更详细的福利信息。&nbsp;我知道鉴于我们强大的财务状况和未来的大量工作，这一消息可能会让您感到惊讶。通常在创业公司的这个阶段，公司会大力促进增长，尽可能多地招聘人员——而不是裁员。在Replit，我们总是与众不同，以较小的团队取得了巨大的成就。小团队可以更加专注，使我们能够更快地前进，同时给我们实现潜力的空间。&nbsp;现在是加快步伐的时候了，因为Replit的机会从未如此清晰。Replit和Al使几乎每一位知识工作者都有可能创造软件。企业需要Replit，超过一半的财富500强用户已经开始尝试使用我们。我们现在需要让构建出色的软件变得更加容易和快速。尽管技术趋势对我们有利，但成功并非必然。我们正在组建一个新的销售团队，并将致力于使我们对在公司工作的人有用，并帮助这些公司发现和支付我们提供的服务。&nbsp;我想向那些离开我们的人表达我深深的感谢。你们的奉献使我们走到今天，我们对此深感感激。你们是令人印象深刻的专业人士，我相信无论您走到哪里都会取得巨大的成功。我相信我们今天采取的行动将在长期内通过充分发挥Replit的潜力并使您的股权更加有价值来对您有利。&nbsp;对于留下来的人，前方的道路将充满挑战，但这也是我们生命中做最出色工作的机会。我想提前感谢你们的努力。今天晚些时候，我们将开会讨论这一变化。明天，我们将不再举行常规的“每周胜利”会议，我将很高兴接受问题并讨论我们计划使Replit变得更加伟大的计划。</blockquote><p></p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/3e/3e916c1855271af6ef2447d7661a5544.png" /></p><p></p><p>&nbsp;</p><p>Replit 是一个广泛使用的编码和托管项目平台，尽管目前公司有着强劲的财务业绩，但还是走到了裁员这一步。据悉，此次裁员的目的是重新调整员工队伍，以更好地满足其不断变化的业务需求。</p><p>&nbsp;</p><p>根据Pitchbook 的数据，Replit 已筹集了超过 2 亿美元的资金，其中包括2023 年 4 月的1 亿美元融资，在宣布裁员时，其员工约有 170 名。</p><p>&nbsp;</p><p>Masad 的电子邮件没有具体说明哪些部门受到影响。但邮件强调并保证所有受影响的员工将获得四个月的工资赔偿、六个月的医疗保健和求职支持。此外，被解雇的员工可以保留他们的公司笔记本电脑。Masad 重申，Replit 的使命是让编程更容易获得，将人工智能融入其平台的各个方面。</p><p>&nbsp;</p><p>该初创公司以其基于浏览器的集成开发环境而闻名，它为开发人员通常使用的传统桌面应用程序提供了替代方案，在全球拥有超过 1000 万用户。</p><p>&nbsp;</p><p>拥有基于浏览器的 IDE的优点是能使开发人员更轻松地启动和运行，因为在启动平台之前几乎不需要进行任何设置，这与基于桌面的 IDE 不同，后者可能需要几个小时才能完成配置。借助 Replit，开发人员只需导航到正确的 URL 即可立即开始编码。</p><p>&nbsp;</p><p>最近，Replit 大力推动将生成式人工智能功能集成到其编码工具中。就在上个月，它推出了一款名为 Replit Teams 的产品，该产品类似于GitHub 上流行的 Copilot 工具，提供了一个 AI Agent，可以与开发人员实时合作，提出修复编码错误的建议或提高他们编写代码效率的建议。</p><p>&nbsp;</p><p>目前尚不清楚 Replit 是否打算用AI取代其员工，但裁员仍然提醒人们这种创新的人力成本。</p><p>&nbsp;</p><p>Masad 在给员工的一封电子邮件中表示，公司打算将人工智能融入其编码平台的各个方面。他表示，Replit 的业务不是销售人工智能，而是“销售一个梦想，即让你梦想的软件更容易获得，让编程更容易获得”。</p><p>&nbsp;</p><p>值得注意的是，今天，X上已经有用户晒出了Replit在旧金山的办公室，目前已经人去楼空，办公室呈对外出租的状态。而根据公开资料，Replit的总部就位于旧金山。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/10/1098d2142ddee0a5bf148318c55ceef4.png" /></p><p></p><p>&nbsp;</p><p></p><h2>开发者工具生意不好做</h2><p></p><p>&nbsp;</p><p>多年来，Replit 一直专注于开发者工具的研发。截止到去年，Replit 上的开发人员达到 2000 万。他们已经创建了超过 2.4 亿个 Repl，从多人游戏到具有云中实时协作功能和先进的 AI 工具的生产软件。开发人员也在 Replit 上建立自己的业务，从独立黑客到Fig、AmpleMarket和BerriAI等 YC 支持的初创公司，再到Deel等独角兽。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/78/783efe069a79bf7a29100e35e2e45343.png" /></p><p></p><p>2015 年至 2023 年 Replit 开发者增长图表</p><p>&nbsp;</p><p>上个月，Replit 还刚刚发布了其新产品 Replit Teams，这款工具将允许开发人员实时协作开发软件项目，同时 AI Agent会自动修复编码错误。</p><p>&nbsp;</p><p>明明是一片欣欣向荣的景象，但 Replit 却在此时裁员并出租办公室，可见开发者工具这门生意也不好做。</p><p>&nbsp;</p><p>开发者工具在软件开发和编程领域的重要性不容忽视。它们为开发者提供了一系列强大的功能，包括但不限于代码编辑、编译、调试、测试、版本控制、部署等，极大地提高了开发效率，减少了错误和漏洞，缩短了开发周期，降低了开发成本。同时，开发者工具还能提升代码的可读性、可维护性和可扩展性，为软件的高质量发展提供了坚实保障。</p><p>&nbsp;</p><p>需求是有的，但想把好的工具产品卖出去，没有想象中容易。</p><p>&nbsp;</p><p>事实上，销售开发者工具的真正难题是，普通开发者没有权利去买这些工具！Stealth公司安全工程师Daniel Feldman在X上发帖讽刺了这种现象。</p><p>&nbsp;</p><p></p><blockquote>销售人员需要花费1000美元？在公司看来可能没什么大不了的。财务需要花费10万美元？可能也没什么大不了的。但是，如果工程师想买一本50美元的书？他们甚至要找副总裁去审批！</blockquote><p></p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ff799c94854f7a9f4298da54d76e43d8.png" /></p><p></p><p>&nbsp;</p><p>Feldman提出的问题得到了很多人的共鸣，一位Twitter网友说，他的团队曾经提出想购买一个9美元每月的工具，该工具能大大提升生产力，但他们的请求被残酷的拒绝了。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/17/179c1da61fffef6441c054b24767222e.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>在Hacker News上，这也是一个非常热门的话题，大家都太有相同的感受了。一位ID为jimnotgym的用户也对Daniel的观点发表了看法：</p><p>&nbsp;</p><p></p><blockquote>情况基本就是这样。我认为很多公司财务和销售人员没有这种权利来购买这类工具。这就是为什么你会看到大多数公司都在使用免费软件和工具。要去说服领导购买这些工具能够节省时间、能够让项目更加透明、能够更加清晰地分类等等。这可能就是Excel如此普遍的原因。不得不提，Excel算是你公司IT部门能为你提供的最强大的工具了。如果你从事销售或财务工作，可尝试让公司IT部门给你的机器上安装WSL，这可能就是网络应用如此流行和受欢迎的原因。</blockquote><p></p><p>&nbsp;</p><p>此外，另一位ID为al_borland的Hacker News用户称，到底是公司IT部门太愚蠢了无法帮他们解决问题，还是各个部门协作之间出了问题？IT部门的员工有自己的工作和可交付成果，所以他们没有一个人在为销售团队构建工具。“我不指望公司IT部门为销售团队开发东西，就像我不指望销售团队能帮助IT部门向管理层们索要开发所需的工具一样。”</p><p>&nbsp;</p><p>有观点认为，IT 部门被视为成本中心，因此管理层会尽可能地削减他们的预算。当 IT 部门连负担一个好的备份系统都勉为其难时，他们就更没有能力去研究如何支持能帮助你的工具了。</p><p>&nbsp;</p><p>另一位网友也表示赞同，他说他以前在一家工程公司的销售导向型子公司工作过，他们在IT预算上卡得很严，“早期时候，我可以带人去吃 100 美元的商务午餐，这没问题，但是我却买不了 30 美元的电脑配件卡。实际上，无论是母公司还是子公司，我什么事情都做不了，因为都需要填写表格来申请资金。所以许多成功的开发人员工具公司通过向老板营销来挣钱，而不是向开发人员营销。这实际上也适用于许多领域。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/aa/aa044b317529355369152f4d565c0ffc.jpeg" /></p><p></p><p>&nbsp;</p><p>另有网友夸张的说，“将开发者工具卖给开发人员就像卖玩具给孩子一样——你应该瞄准父母 (或者公司里支付费用的高管)。应该向 Slack、Postman 等公司学习，让它们成为开发人员/用户的默认工具，让他们满意，然后他们会去找父母/审批人购买。”</p><p>&nbsp;</p><p>另有网友反驳说，“如果比作卖玩具，实际这还不太一样。就像你到处都能看到儿童节目《汪汪队立大功》的周边商品一样：他们制作了一个让孩子们上瘾的动画节目，剩下的事情就交给孩子们了。但这在开发人员身上行不通，因为不像孩子的父母，企业里拥有支出权的人通常不在乎，在他们眼里开发人员只是一个成本中心，是图表上的一个数字。”</p><p>&nbsp;</p><p>实际上，开发人员开发了很多有价值的软件，但涉及到自己的工作时，却成了 “鞋匠的孩子没鞋穿”。只是如果有好用的开发者工具，生产力的提升将是巨大的。针对开发者工具行业来说，我们呼吁企业管理者能够更大方地给开发人员一个预算，让他们做出一些决定。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.hrkatha.com/hiring-firing/replit-announces-layoffs-amid-push-for-ai-integration/">https://www.hrkatha.com/hiring-firing/replit-announces-layoffs-amid-push-for-ai-integration/</a>"</p><p><a href="https://news.ycombinator.com/item?id=40029283">https://news.ycombinator.com/item?id=40029283</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ZR9b6a3OGg5SZgjYl9jW</id>
            <title>TiDB 如何利用 Copilot 优化数据库操作，提升用户体验与内部效率？</title>
            <link>https://www.infoq.cn/article/ZR9b6a3OGg5SZgjYl9jW</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ZR9b6a3OGg5SZgjYl9jW</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 May 2024 09:36:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据量增长, 实时数据处理, 数据安全与隐私保护, 多样化数据类型
<br>
<br>
总结: 数据库系统在面对数据量增长、实时数据处理、数据安全与隐私保护以及多样化数据类型等挑战时，需要应对这些问题并提供智能化解决方案，其中LLM技术在数据库领域的应用成为热点。LLM技术可以优化数据库系统性能和可靠性，提供智能化解决方案，从用户感知到内部操作优化都有显著贡献。 </div>
                        <hr>
                    
                    <p>在应对不断增长的数据量、复杂的业务逻辑和对更高性能与可靠性的追求中，数据库系统面临着重重挑战。其中，有效处理大规模数据并保障数据的安全性与隐私性是当前需要解决的问题。随着人工智能技术的不断演进，LLM 的应用成为了数据库领域的热点。LLM 技术不仅能够优化数据库系统的性能和可靠性，还能为数据库查询、流程优化等提供更智能化的解决方案。</p><p></p><p>在 AICon 全球人工智能开发与应用大会上，我们有幸邀请到 PingCAP AI Lab 负责人李粒为我们分享他的见解。他分享了 LLM 技术在数据库领域的应用前景与解决方案。会前，InfoQ 有幸采访了李粒，以下为采访对话～</p><p></p><p></p><h4>数据库领域挑战与应用案例</h4><p></p><p></p><h5>InfoQ：数据库领域当前最紧迫的挑战是什么？您认为这些挑战如何影响数据库系统的性能和可靠性？</h5><p></p><p></p><p>李粒： 当前数据库领域面临的最紧迫挑战之一是 ++ 如何处理和分析日益增长的数据量，同时保持高效的性能和可靠性 ++。这个挑战主要体现在以下几个方面：</p><p></p><p>首先，数据规模的持续增长 是一个显著挑战。随着物联网、社交媒体和企业应用等领域的快速发展，数据量呈指数级增长。这不仅要求数据库能够有效地存储和管理海量数据，还需要优化存储结构、索引机制和查询处理，以维持高效的性能。</p><p></p><p>其次，实时数据处理的需求日益增加。现代业务场景，如实时分析和在线事务处理，要求数据库系统能够在处理大量数据的同时，保证极低的延迟。这对数据库的设计和优化提出了更高的要求。</p><p></p><p>第三，数据安全与隐私保护 也是一个重大挑战。随着数据泄露事件的频发，如何通过加密、访问控制等措施保护数据安全，防止未授权访问或泄露，成为了数据库系统设计的一个重要方面。</p><p></p><p>此外，我们还面临着处理多样化数据类型和复杂数据关系的挑战。现代数据库不仅要处理结构化数据，还要能够有效管理半结构化和非结构化数据。同时，数据之间的关系也变得更加复杂，这对数据库的模型和查询语言提出了新的要求。</p><p></p><p>最后，高可用性和灾难恢复能力也是企业越来越关注的问题。任何数据丢失或服务中断都可能导致重大的业务损失，因此，确保数据库的高可用性和快速恢复能力是至关重要的。</p><p></p><p>这些挑战直接影响到数据库系统的性能和可靠性。例如，如果处理大规模数据时缺乏有效的索引和查询优化技术，将导致查询速度缓慢，严重影响用户体验。同样，如果安全措施不到位，数据可能面临泄露或损坏的风险，进而影响系统的整体可靠性。</p><p></p><p></p><h5>InfoQ：LLM 技术在数据库领域的应用案例有哪些？您可以分享一些具体的实例，以及这些案例是如何利用 LLM 技术解决现有数据库系统的挑战的？</h5><p></p><p></p><p>李粒：LLM 技术在数据库领域的应用非常广泛，从提高用户体验到内部操作优化，都有显著的贡献。</p><p></p><p>首先，在用户感知方面，LLM 技术可以极大地简化用户与数据库的交互。例如，基于文档的 ChatBot，如 TiDB Bot，可以在 Slack 或 Cloud 平台上支持用户的使用提问。这种 ChatBot 能够理解用户的查询意图，并提供关于数据库配置、日志管理、慢查询优化等方面的建议。这不仅提高了用户的操作便利性，还有助于用户更有效地管理数据库。</p><p></p><p>此外，LLM 技术还能够帮助用户直接通过自然语言生成 SQL 查询（NL2SQL）。这意味着即使用户不熟悉 SQL 语法，也能通过描述他们的查询需求来获取数据。更进一步，我们可以将这种技术扩展到从原始数据到商业洞察的转换（NL2Insight），这不仅仅是生成 SQL，而是提供更深层次的数据分析和业务洞察。</p><p></p><p>在诊断和故障恢复方面，LLM 技术也显示出巨大的潜力。通过集成到基于 ChatBot 的系统中，LLM 可以利用日志、慢查询、性能指标等信息，提供更深入的领域判断和业务问题分析。这有助于减少平均故障修复时间（MTTR），使得即使非专业的用户也能快速诊断并解决问题。</p><p></p><p>在用户不直接感知的内部使用方面，LLM 技术同样发挥着重要作用。例如，在自动化测试中，LLM 可以用来生成数据库系统的测试用例，提高测试的覆盖率和效率。在代码审查中，LLM 可以帮助分析代码质量和风格一致性，提高开发效率。此外，LLM 还可以自动化生成性能分析报告、故障报告等，帮助技术团队快速获取关键信息，并管理企业内部的知识库，提高信息共享和检索效率。</p><p></p><p></p><h5>InfoQ：您提到的 Flow 和 Agent 应用分别是什么？能否详细解释这些技术方向？</h5><p></p><p></p><p>李粒： 在 LLM 应用中，我们可以区分三个技术层次：Wrapper, Flow, 和 Agent。每个层次都代表了与 LLM 交互的不同复杂度和应用场景。</p><p></p><p>LLM Wrapper:</p><p></p><p>这是最基础的应用层次，涉及到与 LLM 的单次交互。在这个层次中，用户的请求直接被发送到模型，模型则返回一个响应。这种方式的能力上限直接受限于模型本身的推理能力。它适合于业务初期，当企业在寻找产品与市场契合度（PMF）时，可以快速开发和迭代。</p><p></p><p>Flow（DAG）:</p><p></p><p>在 Flow 层次上，业务逻辑通过有向无环图（DAG）构建，实现与 LLM 的多次交互。每次交互都专注于解决一个特定问题，例如意图判断、内容改写、提供回答或批评等。这种方法有效克服了单次交互的局限性，支持构建更复杂的应用。适用于那些对如何利用 LLM 解决业务问题有清晰理解的场景，需要处理更复杂逻辑和提高准确度时采用。</p><p></p><p>Agent（Loop）:</p><p></p><p>Agent 层次基于 Loop+Feedback 构建。在这里，LLM 能够根据人类输入自主决定和执行所需步骤，完成后自我评估是否存在异常，并据此进行调整。通过这种方式，LLM 能够显著提高结果的准确性，并解决更复杂的问题。构建 Agent 的逻辑与传统应用截然不同，其核心思想类似于构建一个团队或公司，每个 Agent 都是具有一定能力的工作力量。通过大量 Agent 的相互补充，最终共同做出相对合理的决策。</p><p></p><p>这些技术方向没有绝对的好坏，关键在于选择最适合当前业务需求的技术层次。随着业务的发展和需求的变化，可能需要从一个层次迁移到另一个层次，以适应更复杂的场景和提高系统的整体性能。</p><p></p><p>可以从这样的一张表格中，清晰的进一步认识不同层级应用的区别。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/bc/bcb80dacba40898c0b49893170fd6aa6.png" /></p><p></p><p></p><h4>数据库的 Copilot 技术实践</h4><p></p><p></p><p></p><h5>InfoQ：在使用 LLM 解决实际问题时，您认为典型的步骤是什么？在这些步骤中，您所遇到的主要挑战和困难是什么？您是如何应对这些挑战的？</h5><p></p><p></p><p>李粒： 使用 LLM 解决实际问题通常涉及几个关键步骤，每个步骤都有其独特的挑战。</p><p></p><p>典型的业务实现步骤包括：</p><p></p><p>业务洞察和需求判断：这是项目启动的第一步，需要深入理解业务需求和痛点。实验和可行性分析：在这一步，我们会进行初步的实验，测试 LLM 的适用性和效果。应用类型迭代：根据场景的复杂度，我们可能会从 Wrapper 开始，逐步迭代到 Flow 和 Agent。反馈设计和收集：设计有效的反馈机制，以收集用户反馈，这对模型的优化至关重要。持续优化设计和实施：根据收集到的反馈不断优化模型和业务流程。</p><p></p><p>在这些步骤中，我们遇到的主要挑战包括：</p><p></p><p>业务理解：深入理解业务需求常常需要与业务方紧密合作，这可能涉及到亲自参与业务流程。模型能力：模型的理论能力与实际应用效果之间可能存在差距。工具的成熟度：目前市场上的工具可能还不够成熟，我们有时需要进行开源贡献或自主研发。LLM 的稳定性：包括回答的稳定性和流程的稳定性，这需要通过精心设计的 Prompt 和流程控制机制来解决。LLM 的回答格式和质量：如何确保 LLM 提供的回答既符合业务需求又具有高质量，这需要通过持续的反馈和优化来实现。</p><p></p><p>应对策略：</p><p></p><p>深入合作：与业务方进行深入合作，确保充分理解业务需求。持续实验：通过持续的实验和可行性分析，不断调整和优化 LLM 的应用。反馈机制：设计有效的反馈机制，如 ChatBot 的点赞和相似性评价，SQL 的正确性评价等，以收集用户反馈并优化模型。增强工具和流程：开发或改进工具，优化业务流程，提高 LLM 的应用效果和稳定性。</p><p></p><p></p><h5>InfoQ：什么是 Copilot，和其他 AI 应用方法有什么区别？</h5><p></p><p></p><p>李粒：Copilot 是 AI 应用中的一种非常具体的交互方式，它在灵活性和易用性之间寻求平衡，旨在减轻用户的认知负担，同时提供有效的支持。</p><p></p><p>Copilot 的核心特性和应用：</p><p></p><p>Copilot 可以被视为用户的“数字助手”或“教练”，它嵌入到用户的工作流程中，提供主动的建议和支持。这种方法的关键在于它能够捕获大量的上下文信息，从而使 AI 能够提供更加精准和有用的建议。例如，GitHub Copilot 在编程环境中提供代码建议，而 Database Copilot 则可能在数据库管理或查询优化中提供帮助。</p><p></p><p>与其他 AI 应用方法相比，Copilot 的主要区别在于它的集成程度和交互方式。例如，与传统的聊天机器人相比，Copilot 更加深入地融入用户的具体任务和工作流程中，而不仅仅是提供一般性的对话支持。</p><p></p><p>与其他 AI 应用方法的比较：</p><p></p><p>一般聊天（Chat）：这种方法提供最高的自由度，用户可以自由地以自然语言与系统交互。然而，它可能在易用性方面不如其他方法，尤其是在需要具体指导或操作的任务中。如 ChatGPT。专业聊天（Specialized Chat）：这种方法通过限制讨论的主题范围来提供更专业的支持。它比一般聊天更具指导性，但牺牲了一定的灵活性。如 TiDB Bot 只讨论 TiDB 的问题。AI 启用的特性（AI-enabled Features）：这种方法提供最高的易用性，通常通过直接的按钮或切换来启用特定的 AI 功能。它的结果更可预测，但灵活性最低。如 Notion AI 的续写、翻译功能。</p><p></p><p></p><h5>InfoQ：数据库的 Copilot 是什么？</h5><p></p><p></p><p>李粒： 数据库 Copilot 是一种 AI 应用，它嵌入到用户的工作流程中，以提供实时的指导和建议，从而提高用户的工作效率和决策质量。这种应用模式在很多方面类似于飞行中的副驾驶，它不仅提供辅助，还能确保操作的正确性和安全性。</p><p></p><p>数据库 Copilot 的核心特性和工作方式：</p><p></p><p>集成与工作流程：数据库 Copilot 深入集成到用户的数据库操作和查询流程中。它通过分析用户的输入和行为，以及数据库的响应和状态，实时提供反馈和建议。主动建议：与传统的工具不同，数据库 Copilot 采用主动出击的方式，根据当前的上下文和历史操作模式，推荐最佳的操作步骤或查询优化建议。上下文感知：它能够理解和分析用户在数据库中的操作上下文，这使得它能够提供更加精准和有用的建议。学习与适应：数据库 Copilot 通过持续学习用户的操作习惯和偏好，不断优化其建议算法，以提供更加个性化的支持。</p><p></p><p>应用场景：</p><p></p><p>查询优化：对于复杂的 SQL 查询，Copilot 可以提供性能优化建议，帮助用户改写查询以提高执行效率。错误诊断：在用户遇到查询错误或性能瓶颈时，Copilot 可以提供诊断信息和修复建议。学习辅助：对于不熟悉数据库操作的用户，Copilot 可以作为一个实时的学习工具，提供操作指导和最佳实践。</p><p></p><p>挑战与对策：</p><p></p><p>用户信任：建立用户对 Copilot 建议的信任是一个挑战。为此，我们确保所有建议都基于最佳实践和精确的数据分析，同时提供足够的解释和文档支持。平衡自动化与控制：过度的自动化可能导致用户感觉失去控制。我们通过提供可调节的自动化级别和详细的用户控制选项来解决这一问题。持续学习：为了保持 Copilot 的效果，我们持续收集用户反馈和操作数据，用于训练和优化模型。</p><p></p><p></p><h5>InfoQ：在处理复杂业务逻辑和规则时，如何保证生成的 SQL 语句的业务逻辑正确性？您是如何验证 Copilot 生成的 SQL 语句是否符合业务需求的？</h5><p></p><p></p><p>李粒： 确保生成的 SQL 语句符合业务逻辑的正确性是一个多步骤的过程，涉及到从数据架构的增强到持续的优化和反馈收集。我可以分几个部分来详细说明这个过程。</p><p></p><p>Schema 增强：</p><p></p><p>在导入数据时，我们会对数据库的 schema 进行详细描述，包括列描述、表描述、表关系、数据库描述以及主实体。这有助于模型更好地理解数据结构和业务上下文。</p><p></p><p>语料库建设：</p><p></p><p>我们会在导入数据时同时引入与业务相关的语料库，这包括 schema 信息、领域知识和具体的 SQL 案例。这些语料库帮助模型学习特定业务领域的语言和逻辑。</p><p></p><p>Prompt（自问自答）：</p><p></p><p>我们使用自问自答的方式来优化查询生成过程。这包括任务重写、实体提取、子问题生成及其解答，以及查询合并。这一步骤是确保生成的 SQL 语句逻辑正确性的关键。</p><p></p><p>自我修正（Self-Fix）：</p><p></p><p>我们对生成的 SQL 进行优化，确保其可执行性。如果一个查询执行报错，系统会将错误信息反馈给 LLM，然后尝试生成新的查询，直到得到一个可以正确执行的结果。</p><p></p><p>持续优化：</p><p></p><p>一个持续学习和适应的 Agent，专门负责优化 SQL 查询的生成和执行。</p><p></p><p>Agent 的工作机制包括以下几个方面：</p><p></p><p>自动化监控和反馈循环：Agent 持续监控数据库操作的效果，包括查询的执行时间、资源消耗等关键性能指标。同时，它也收集用户对查询结果的反馈，如点赞、点踩、修改等。基于这些数据，Agent 可以自动识别哪些查询需要优化，哪些已经达到了较好的性能。动态学习和调整：Agent 使用机器学习算法来分析收集到的数据，从中学习如何改进 SQL 语句的结构和逻辑。这包括选择更有效的索引、调整查询的结构、优化连接和过滤条件等。它还能根据数据库的实时状态动态调整查询策略，以适应数据量的变化、数据库负载的波动等外部条件。生成和测试新的查询方案：在识别出需要优化的查询后，Agent 会自动生成一系列改进的查询方案。这些方案会在一个安全的测试环境中执行，以评估它们的性能和准确性。通过比较不同方案的执行结果，Agent 可以选择最优的查询方案，并将其推荐给用户或自动应用到生产环境中。持续迭代和优化：这个过程是持续进行的。Agent 会不断迭代和优化其学习模型和查询生成算法，以适应新的业务需求和技术变化。它还会定期清理和更新其语料库，去除过时或低效的数据，确保学习资源的质量和相关性。</p><p></p><p></p><h4>AI Agents 的高效运行和数据安全</h4><p></p><p></p><p></p><h5>InfoQ：LLM Agents 的落地涉及到服务开销和实时性，您是如何在平衡这两方面的情况下确保系统的高效运行和响应速度的？</h5><p></p><p></p><p>李粒： 这确实是在部署 LLM Agents 时面临的一个重要挑战。由于 LLM Agents 通常需要与 LLM 进行多次交互，这不仅增加了运行成本，还可能影响响应速度。这里的根本原因是 Agents 多次与 LLM 交互（常见有 20 次 -30 次）+ LLM 本身的运行速度 + LLM 成本很贵。</p><p></p><p>我们采取了几种策略来平衡这两方面的需求，以确保系统的高效运行。</p><p></p><p>优化交互次数和处理速度：</p><p></p><p>我们对不同类型的任务采用不同级别的 LLM 应用。例如，对于需要快速响应的任务，我们可能会使用 Wrapper 或 Flow 模式，这些模式的交互次数较少（通常是 1-5 次），可以在较短的时间内完成。对于可以容忍较长处理时间的任务，如后台分析、故障诊断等，我们会使用 Agents 模式。虽然这种模式需要更多的交互次数（20-30 次），但它可以处理更复杂的逻辑和循环，提供更深入的分析。</p><p></p><p>提升 LLM 的运行效率：</p><p></p><p>我们会根据需要调整服务的硬件配置，比如增加更多的 GPU 资源，以缩短处理时间。</p><p></p><p>成本控制和业务适配：</p><p></p><p>我们密切监控服务的成本和性能，确保在不牺牲用户体验的前提下，尽可能地降低成本。根据不同业务的特点和需求，选择最合适的 LLM 应用模式。例如，在一些非实时的应用场景中，可以接受较长的响应时间，这时可以使用更复杂的 Agents 模式来提高分析的准确性和深度。</p><p></p><p>混合使用不同的模式：</p><p></p><p>在某些情况下，我们会在 Flow 中嵌入 Agents 作为一个工具，用于解决特定的、易出错的问题。这种混合模式可以在保证效率的同时，解决复杂的问题。</p><p></p><p></p><h5>InfoQ：当 LLM Agents 需要获取相关业务或客户的数据以提高服务能力时，如何保护这些数据的安全和隐私？您采取了哪些措施来确保数据的保密性和完整性？</h5><p></p><p></p><p>李粒： 保护客户数据的安全和隐私是我们的首要任务。我们采取了多层次的措施来确保数据的保密性和完整性，这些措施涵盖了系统级和模型级的安全策略。</p><p></p><p>系统级措施：</p><p></p><p>数据访问控制：我们使用 TiDB 和 TiDB Cloud 来管理数据，这些系统具备强大的数据访问控制功能。除非得到用户的显式授权，否则无法访问业务数据。这确保了数据访问的合法性和安全性。数据脱敏：在处理业务数据时，我们会对数据进行脱敏处理。这意味着在数据被 LLM Agents 使用之前，所有敏感信息都会被去除或替换，以确保即使数据被泄露，也无法被恶意利用。</p><p></p><p>模型级措施：</p><p></p><p>内部模型与第三方模型的安全策略：对于内部模型，我们可以严格控制数据处理和存储的环境。对于涉及第三方模型的交互，我们采取以下措施：数据最小化和脱敏：我们尽可能减少处理的数据量，并对所有敏感数据进行脱敏处理。这包括使用数据掩码或伪匿化技术，确保敏感信息不被暴露。字段替换：在某些情况下，我们会将敏感字段替换为随机字符（如 abcd），并提供这些字符的解释，这样即使数据被泄露，也无法直接关联到具体的业务信息。</p><p></p><p>持续的安全审计和更新：</p><p></p><p>安全审计：我们定期进行安全审计，以检查和评估现有的数据保护措施的有效性。技术更新：随着安全技术的发展，我们持续更新我们的数据保护技术和策略，以对抗新的安全威胁和挑战。</p><p></p><p></p><h5>InfoQ：您认为未来数据库 Copilot 可能的发展方向是什么？</h5><p></p><p></p><p>李粒： 未来数据库 Copilot 的发展方向可能会集中在以下几个关键领域，以进一步提升其智能化水平和用户体验，同时解决现有的挑战：</p><p></p><p>更深层次的自然语言处理能力</p><p></p><p>随着自然语言处理技术的进步，未来的数据库 Copilot 将能更准确地理解复杂的自然语言查询和指令，甚至能处理含有多重意图和复杂关系的查询。这将使非技术用户能够更直观、更自然地与数据库交互。</p><p></p><p>增强的上下文理解和持续对话能力</p><p></p><p>未来的数据库 Copilot 可能会具备更强的上下文保持能力，能够在一系列交互中理解和引用之前的对话内容。这将使得进行复杂的数据分析和操作变得更加连贯和用户友好。</p><p></p><p>自动化数据分析和见解生成</p><p></p><p>数据库 Copilot 将进一步发展其能力，不仅能执行查询，还能自动分析数据，提供业务见解和建议。例如，它可以自动识别数据趋势、异常和潜在的优化点，并向用户提出建议。</p><p></p><p>更强的个性化和适应性</p><p></p><p>通过机器学习和用户行为分析，数据库 Copilot 将能够适应特定用户的查询习惯和偏好，提供更个性化的服务。例如，根据用户的角色和过去的查询历史，自动调整查询结果的展示方式和详细程度。</p><p></p><p>更广泛的集成和兼容性</p><p></p><p>未来的数据库 Copilot 将支持更多类型的数据库和数据存储解决方案，包括 NoSQL 数据库、云存储和实时数据流平台。同时，它也将更容易集成到各种业务应用和数据分析工具中。</p><p></p><p>增强的安全性和隐私保护</p><p></p><p>随着数据安全和隐私保护的重要性日益增加，未来的数据库 Copilot 将采用更先进的安全技术，如同态加密、访问控制和隐私保护算法，确保用户数据的安全和合规性。</p><p></p><p>自动化数据库管理和优化</p><p></p><p>数据库 Copilot 将能够自动执行更多的数据库管理任务，如性能监控、故障诊断、自动调优和备份管理。这将大大减轻数据库管理员的负担，提高数据库的运行效率和可靠性。</p><p></p><p>嘉宾介绍：</p><p></p><p>李粒 PingCAP AI Lab 负责人，研究领域涵盖推荐系统和强化学习。曾参与开发基于强化学习的围棋算法，击败时任围棋世界冠军朴廷桓。在 PingCAP，负责构建 Auto-Diagnosis 系统，推动自动驾驶数据库云的发展，持续关注 AI 领域的应用创新，推动其落地和融入生产，致力于推动企业 AI 应用的变革。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/cBzB0Eq0cqXS6h4SiZWp</id>
            <title>如何落地AI编程和可观测智能化？怎么从 0 到 1 训练大模型？阿里多位专家出席 ArchSummit 现身说法</title>
            <link>https://www.infoq.cn/article/cBzB0Eq0cqXS6h4SiZWp</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/cBzB0Eq0cqXS6h4SiZWp</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 May 2024 08:16:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 麦肯锡, AI编程助手, AIOps, SMS网关
<br>
<br>
总结: 麦肯锡的研究显示，AI编程助手可以显著提高代码文档的维护效率和新代码生成效率。AIOps是解决IT系统复杂性和监控挑战的有力工具，而构建高弹性的SMS网关是应对流量波动的关键。在ArchSummit峰会上，专家们将分享关于AI编程、AIOps、SMS网关等领域的最新技术和实践经验。 </div>
                        <hr>
                    
                    <p>麦肯锡的一项研究结果表明，在生成式 AI 的辅助下，可维护性代码文档可以在一半的时间内完成，新代码生成效率提升近一倍，而代码重构类任务的完成时间也节省近 1/3。</p><p></p><p>可见，随着 AI 大模型相关技术的快速发展， AI 编程助手的引入的确为软件开发带来了质的飞跃。那么，具体突破体现在哪些方面？基于大模型的 AI 编程工具存在哪些设计要点、难点和改进思路？从开发者自身的生命力出发，又如何用 AI 激活开发效率，提升生产力？</p><p></p><p>在 6 月 14 日 -6 月 15 日于深圳举办的 ArchSummit 全球架构师峰会上，我们邀请到了阿里巴巴研究员、阿里云云原生应用平台负责人丁宇（叔同），在 Keynote 主题演讲中分享<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5718"> AI 编程带来的革命性巨变</a>"，他将围绕以上话题展开深入介绍，并且从全球视角分享开发者对于 AI 编程的需求差异、AI 编程工具能力，以及 AI 编程领域未来的发展趋势。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2a/2a58a7c4317b485b858dd248471d04e6.webp" /></p><p></p><p>生成式 AI 的引入，在带来效率提升的同时，也使得 IT 系统复杂性日益增加，运维和监控领域面临的挑战随之增长。但“魔法”总能打败“魔法”，技术的问题同样可以用技术解决。AIOps 已成为解决以上挑战的有力工具，能够帮助自动化和优化监控流程，并显著提高效率。</p><p></p><p>在 ArchSummit 深圳，阿里云高级算法工程师陈昆仪博士将带来<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5813">《阿里云可观测智能化探索与实践》</a>"的议题分享，详细探讨智能算法在系统异常检测和故障根因定位方面的应用，如推荐告警阈值、时序预测算法、定位异常服务以及分析错误或缓慢的调用链。此外，还将介绍如何利用 LLM 将自然语言自动转换为 Prometheus 查询语言（PromQL）的技术，简化查询构建的过程。</p><p></p><p><img src="https://static001.geekbang.org/infoq/10/1084942512f4a342cc562a46a77d062d.webp" /></p><p></p><p>基础技术架构层面，突发的流量峰值和网络稳定性，是另一大挑战。尤其在大型活动、紧急通知或者促销营销期间，SMS 网关会遭遇剧烈的流量波动。为了保障服务的稳定性和响应速度，构建一个具备高弹性、高性能的 SMS 网关成为应对未来不断变化业务需求的当务之急。</p><p></p><p>对此，<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5819">阿里云云通信架构师张松然</a>"将在 ArchSummit 深圳分享，如何通过最新的技术和架构策略，打造一个能够自适应流量变化、保障消息准确无误送达的高弹性、高性能 SMS 网关。详细阐述使用云原生技术、微服务架构以及自动化扩缩容策略来构建 SMS 网关的实战过程，展示在实际突发流量情况下，如何通过优化资源管理和调度策略，有效提高系统的吞吐量和可用性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f2/f25324f4b00e4b0bebfc3f87744aab3b.webp" /></p><p></p><p>当然，所有生产力的提高，除了新工具的加持之外，还需要与之相匹配的技能提升和企业赋能，包括开发者、架构师等在内的技术从业者，都面临着全新挑战。如何适应时代、顺势而为，并且利用大模型将软件系统带入下一个发展阶段？</p><p></p><p>阿里云 CIO 产品线首席架构师黄永法将在 ArchSummit 深圳分享<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5850">《架构师在 AI 浪潮下的 4 个关键可迁移能力及提升技巧》</a>"：首先，讨论在 AI 时代架构师面临的挑战；其次，对架构师细分赛道及能力模型进行分析，得出架构师最需要关注的 4 个可迁移能力；最后，还将基于个人的实战经验，给出可操作的练习方法和技巧，希望能够帮助大家提升自我的竞争力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/87/875d23b6e7094fc5166269838518b9dc.webp" /></p><p></p><p>除此之外，为了让大家更进一步了解大语言模型的从零到一的训练过程，以及大语言模型的训练原理和最佳实践，阿里云资深技术专家李永还将带来<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5884">《超大规模 LLM 模型在 PAI 平台的最佳实践》</a>"的议题分享，主要介绍超大规模 LLM 模型从数据链路到训练优化的整个链路的最佳实践，包括数据清洗、调度、通信优化、训练、稳定性、RLHF 等方面的内容。</p><p></p><p><img src="https://static001.geekbang.org/infoq/04/04143c093c26cd8fb5deeda9b6e240aa.webp" /></p><p></p><p>除了阿里的众多优秀讲师之外，我们也邀请了（以下排名不分先后）腾讯、百度、网易、字节跳动 / 火山引擎等互联网技术大厂， vivo、知乎、高德地图、Uber 、蚂蚁集团、eBay、货拉拉、快手、哔哩哔哩、携程等头部互联网企业，以及 CNCF、Thoughtworks、顺丰集团、美的集团、鸿海科技集团（富士康母公司）、宁德核电、广发证券、微众银行、众安银行、天弘基金等众多机构和企业的专家共同探讨生成式 AI 技术对于企业未来架构的影响。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ab/ab93ea8c01a10a6f82e68abb2f2220a1.webp" /></p><p></p><p>目前，ArchSummit 深圳大会议程已经上线，并将持续更新，感兴趣的同学请点击链接锁定大会官网查看更多详情：<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">https://archsummit.infoq.cn/2024/shenzhen/schedule</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/YTiaW03CML94ZBlG7wVj</id>
            <title>李开复：不参与“价格战”、模型盲测国内第一欢迎PK</title>
            <link>https://www.infoq.cn/article/YTiaW03CML94ZBlG7wVj</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/YTiaW03CML94ZBlG7wVj</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 May 2024 06:17:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 模型表现, LMSYS, 大模型企业, 测评结果
<br>
<br>
总结: 李开复在分享会上表示，零一万物的模型表现超过其他模型，欢迎友商来LMSYS打擂台。最新排名显示，Yi-Large在总榜排名世界第七，中国大模型中第一，与GPT4o并列第一。LMSYS的盲测竞技场成为大模型金标准，结果可信度高。Yi-Large在编程能力、长提问、艰难提示词等方面表现出色。零一万物将继续坚持Scaling Law，致力做到中国最好的模型。 </div>
                        <hr>
                    
                    <p>“我们的模型表现超过了其他模型，欢迎不认同的友商来LMSYS打擂台，证明我是错的。但在那发生之前，我们会继续说我们是最好的模型。”李开复在5月21日的分享会上说道。</p><p>&nbsp;</p><p>李开复的底气来自Yi-Large一直以来不错的测评表现。而最近的5月20日，在 LMSYS 盲测竞技场最新排名中，零一万物的最新千亿参数模型 Yi-Large 总榜排名世界第七，中国大模型中第一，已经超过Llama-3-70B、Claude 3 Sonnet，中文榜更是与GPT4o 并列第一。</p><p>&nbsp;</p><p>零一万物也因此成为总榜上唯一一个自家模型进入排名前十的中国大模型企业。在总榜上，GPT 系列占了前十位的四个名额。以机构排序，零一万物 01.AI 仅次于 OpenAI、Google、Anthropic，正式进入国际顶级大模型企业阵营。</p><p></p><h2>榜单表现</h2><p></p><p>&nbsp;</p><p>让零一万物振奋的原因是LMSYS是大模型金标准，都是第三方匿名，而且每个模型都有数万用户评估，结果可信度非常高。OpenAI的Sam Altman和 Google CTO Jeff Dean都在最近的模型发布中引用了该测试结果。</p><p>&nbsp;</p><p>为了提高 Chatbot Arena 查询的整体质量，LMSYS实施了重复数据删除机制，并出具了去除冗余查询后的榜单。这个新机制旨在消除过度冗余的用户提示，如过度重复的“你好”。这类冗余提示可能会影响排行榜的准确性。LMSYS公开表示，去除冗余查询后的榜单将在后续成为默认榜单。</p><p>&nbsp;</p><p>在去除冗余查询后的总榜中， Yi-Large的Elo得分更进一步，与Claude 3 Opus、GPT-4-0125-preview并列第四。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ce/cef834e16496d270dd7f8f56d22800b4.png" /></p><p>&nbsp;</p><p>&nbsp;</p><p>国内大模型厂商中，智谱GLM4、阿里Qwen Max、Qwen 1.5、零一万物Yi-Large、Yi-34B-chat 此次都有参与盲测。在总榜之外，LMSYS 的语言类别上新增了英语、中文、法文三种语言评测，开始注重全球大模型的多样性。Yi-Large的中文语言分榜上拔得头筹，与 OpenAI GPT-4o 并列第一。</p><p></p><p><img src="https://static001.geekbang.org/infoq/20/20636ff6cd80ae579bdbc48fd51397bc.png" /></p><p>&nbsp;</p><p>在分类排行榜中，编程能力、长提问及最新推出的 “艰难提示词” 的三个评测是LMSYS所给出的针对性榜单，以专业性与高难度著称，可称作大模型“最烧脑”的公开盲测。</p><p>&nbsp;</p><p>在编程能力（Coding）排行榜上，Yi-Large 的Elo分数超过Anthropic 当家旗舰模型 Claude 3 Opus，仅低于GPT-4o，与GPT-4-Turbo、GPT-4并列第二。长提问（Longer Query）榜单上，Yi-Large 同样位列全球第二，与GPT-4-Turbo、GPT-4、Claude 3 Opus并列。</p><p>&nbsp;</p><p>艰难提示词（Hard Prompts）则是LMSYS为了响应社区要求，新增的排行榜类别。这一类别包含来自 Arena 的用户提交的提示，这些提示则经过专门设计，更加复杂、要求更高且更加严格。LMSYS认为，这类提示能够测试最新语言模型面临挑战性任务时的性能。在这一榜单上，Yi-Large 处理艰难提示的能力也得到印证，与GPT-4-Turbo、GPT-4、Claude 3 Opus并列第二。</p><p></p><p><img src="https://static001.geekbang.org/infoq/82/822430c3982eb0394c0c5e8df0ab6789.jpeg" /></p><p></p><p>测评结果：Coding、Longer Query、Hard Prompts</p><p>&nbsp;</p><p>在此之前，各种静态榜单几乎成为厂商必争的地方。在零一万物模型训练负责人黄文灏看来，所谓打榜主要是厂商要把模型某些单一能力做提升，但比较的时候大家可能并不了解，会带来一些bias。LMSYS提供了一种更接近于用户真实场景的评测方式，所以可以作为一个更好的衡量标准。</p><p></p><h2>李开复：不会对标“价格战”</h2><p></p><p>&nbsp;</p><p>用好的模型，贵不贵？当前，Yi-Large API的定价是0.02元/千tokens，大概是GPT-4&nbsp;Turbo成本和定价的三分之一。</p><p>&nbsp;</p><p>成本问题其实是零一万物一直以来就在关注的。“在大模型时代，模型训练和推理成本构成了每一个创业公司必须要面临的增长陷阱。”李开复曾说道。</p><p>&nbsp;</p><p>“我们关注到最近降价的现象，我认为我们的定价还是非常合理的，而且我们也在花很大精力希望它能再降下来。”李开复表示，整个行业每年降低10倍推理成本是可以期待的，而且也必然发生的，以这个角度看，现在的降价对整个行业来说就是一个好消息。</p><p>&nbsp;</p><p>但对于大模型公司，李开复认为，国内常看到ofo式的疯狂降价、双输的打法，大模型公司不会这么不理智，因为技术还是最重要的，如果技术不行，纯粹靠贴钱、赔钱做生意是行不通的。</p><p>&nbsp;</p><p>李开复以万知为例介绍到，零一万物内部也纠结过用 Yi-Medium，中尺寸模型有成本优势，但是大尺寸模型更有泛化和推理能力优势。考虑到万知用户也包括海外用户，还是需要最强的推理能力，因此团队最终选择了千亿参数的 Yi-Large。</p><p>&nbsp;</p><p>“虽然这并没有达到TC-PMF、还不能赚钱，但是技术的需求是不可妥协。推出之后，模型和Infra团队就一起快速把钱降下来。”李开复说道。</p><p>&nbsp;</p><p>对于当前的大模型价格战，李开复明确表示不会对标这样的（市场）定价。“如果中国市场就是这么卷，大家宁可赔光、通输也不让你赢，那我们就走外国市场。”</p><p></p><h2>“最小到最大的模型，做到中国最好”</h2><p></p><p>&nbsp;</p><p>在做大模型方面，零一万物将继续坚持 Scaling Law。从最小的6B到34B，到现在的千亿模型，还有训练中的万亿 MoE，零一万物技术团队明显看到模型性能随着参数量的增大，智能水平也在显著上升，Scaling Law给AGI指明了一个方向。</p><p>&nbsp;</p><p>以大模型为代表的就是大规模机器学习，需要过大量的算力做大量的实验来得到结论，同时需要算法和Infra做联合优化。</p><p>&nbsp;</p><p>在Scale up过程中，最能够高效使用算力的通用结构一般会获得较大成功。在模型结构上加了各种各样的prior（先验知识）、去调优可以获得更好效果，但这些prior也是约束条件，对模型效果产生影响。零一万物发现，最简单的模型就是最高效的，重要的是怎么去用好计算能力，而给定算力条件下的智能水平，最重要的是数据的质量和使用数据的效率、计算效率。</p><p>&nbsp;</p><p>黄文灏表示，零一万物需要算法、Infra和工程三位一体的人才，但这样的人在国内并不是很多。大模型研发中，人才的作用被放大，比如算法团队不需要特别多的人，一般是10～20人，但是他们后面是几万张卡，这些人的能力就被几万张卡放大了很多。</p><p>&nbsp;</p><p>目前，零一万物的系列大模型参数刚迈入千亿行列，但已经可以与GPT-4、Gemini 1.5 Pro等万亿级别的超大参数规模模型扳手腕。</p><p>&nbsp;</p><p>在Chatbot Arena测评的44款模型中，GPT-4o在最新的Elo评分中以1287分高居榜首，GPT-4-Turbo、Gemini 1 5 Pro、Claude 3 0pus、Yi-Large等模型则以1240左右的评分位居第二梯队；其后的Bard (Gemini Pro)、Llama-3-70b-Instruct、Claude 3 sonnet的成绩则断崖式下滑至1200分左右。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/1f/1f6adfd9007e055d7e0726910216d1a9.png" /></p><p></p><p>&nbsp;“我们的计划是从最小到最大的模型都能够做到中国最好。”李开复表示。一方面，根据 scaling law，越大尺寸的模型约有可能达到AGI；另一方面，小一些的模型也有各种应用机会。因此，零一万物的打法是“一个都不放过”，并且在每一个潜在尺寸上做到性能最高、推理成本最低。</p><p>&nbsp;</p><p>不过另一个现实是，零一万物GPU存量只有Google、Microsoft的5%，但李开复认为这并不代表企业就没有机会。</p><p>&nbsp;</p><p>“能用同样一张卡挤出更多的价值，这是今天我们能够达到这些成果的重要原因之一。”李开复说道。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/8jTyOe3qL4URASiQsjbz</id>
            <title>AI时代下的金融科技展望：发展中的问题只能用发展来解决</title>
            <link>https://www.infoq.cn/article/8jTyOe3qL4URASiQsjbz</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/8jTyOe3qL4URASiQsjbz</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 May 2024 04:06:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能技术, 金融科技领域, AI应用, 银行业
<br>
<br>
总结: 人工智能技术的快速发展推动着金融科技领域的革新，AI在银行业带来新的增长点，但也面临挑战和疑虑。银行业需要结合统计学方法和推理方法，利用生成式AI实现更精细化的客户定位、风险评估和合规性管理，以创造更大的商业价值。银行业在应用AI技术时需要谨慎，确保服务和产品的准确性和合规性。 </div>
                        <hr>
                    
                    <p>人工智能技术的快速发展正不断地推动着金融科技领域的革新。这一变革不仅仅关乎技术的演进，更在于它如何成为推动全球经济增长的新动力。在日前举办的“人工智能 X 金融科技创新大会”上，汇丰科技财富管理与个人银行的全球首席架构师夏勇博士就这一趋势<a href="https://www.infoq.cn/video/3qt0o3y04xKgJuoKk4mT?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">分享了他的见解</a>"。他认为，AI 的确会给金融行业带来新的增长点，但同时 AI 在银行业的应用仍存在不少疑虑和挑战，无论如何，发展中产生的问题只能用发展来解决。</p><p></p><p>以下为夏勇博士的演讲实录，内容经 InfoQ 进行不改变原意的编辑。</p><p></p><h2>AI 驱动下，金融行业迎来新的增长点</h2><p></p><p></p><p>AI 热潮爆发时，作为银行从业者，我们首先关注的就是 AI 可能带来哪些经济增长。在过去几年里我们能够看到，如传统的高级分析、传统机器学习和深度学习等数据分析方法等已经为社会带来了巨大影响。</p><p></p><p>根据麦肯锡的调查，这些技术已为全球经济带来 11 万亿到 17 万亿美元的增长，而生成式 AI 可能还会额外带来 2.6 万亿至 4.4 万亿美元的增长，这相当于一个英国的 GDP，再具体一点也就是中国 GDP 的五分之一。若生成式 AI 的影响与传统技术的影响相叠加，最终会带来 17.1 万亿到 25.6 万亿的增长，接近美国一年的 GDP。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e8/e82054ba0d0dce54e037f5da6ab24061.png" /></p><p></p><p>其次，我们还关心为业务部门开发的这些产品 IT 系统的建设。目前，我负责汇丰财富管理和零售银行的全球架构工作，这方面业务在全球的历史遗留应用大约有几千个，其中一些核心系统的建设可追溯至 60 至 70 年前，甚至更早，其中有一些程序还是用汇编语言编写的。这种语言在现在的大学课堂上都已不常见，因此其维护和更新是一项巨大的工程。然而，从另一方面来说，这些遗留系统中也沉淀着丰富的银行业务知识和多年的行业经验。</p><p></p><p>我们的任务就是将这些传统代码中蕴含的知识提取出来，并以此来理解金融业的历史发展。基于这些历史数据和知识，我们可以探索银行业务的未来机遇，寻求为客户提供更优质服务的可行方式。这对我们而言，特别是对我们银行内部做创新工作的同事们来说，是一个值得关注的议题。</p><p></p><p>同时，这些语言和大模型有所不同，它们其实是一些私域数据。如何将大模型的通用知识与我们银行的这些私域知识结合起来，以推动金融领域未来发展？这是一个我们觉得很有意思、但也非常必要的工作。</p><p>此外，随着人工智能技术的发展，我们现在能够提供比传统人工客服更高效的智能客服，这无疑将成为我们未来工作的一个重要方向。在市场营销、销售等其他领域，我们也将继续探索和应用新技术，以提升整体的业务效能。</p><p></p><p>作为一个受知识和技术驱动的行业，银行业的营销、客户运营等业务都已从先前存在的 AI 应用中获得了显著的好处。而生成式 AI 应用则可以提供额外的好处，特别是因为文本模态在诸如法规和编程语言等领域非常普遍，并且银行业面向客户，拥有许多 B2C 和小型企业客户。行业特点使银行业很适合集成生成式人工智能应用：</p><p>持续的数字化努力以及传统的 IT 系统。银行已经在技术方面投资了数十年，积累了大量的技术债务，以及一个孤立和复杂的 IT 架构。面向大客户的工作力量。银行业依赖于大量的服务代表，如呼叫中心代理商和财富管理金融顾问。严格的监管环境。作为一个受到严格监管的行业，银行业有大量的风险、合规和法律需求。知识工作者领域。生成式人工智能的影响可能贯穿整个组织，帮助所有员工编写电子邮件、创建业务演示文稿和其他任务。</p><p></p><p>下图展示了我们能预见到的 AI 在银行业的潜在应用领域与职能，包括投资银行、资产管理、对公业务、财富管理、零售银行等等。</p><p><img src="https://static001.geekbang.org/infoq/d6/d64614fcca8f974d837333da370b5ec5.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/e7/e75fae71af91c386e130107261f03a69.png" /></p><p></p><p>虽然这些应用的前景看起来很光明，但我不得不提的是，金融业、银行业都是重监管的部门，因为它们涉及到公众的薪资和养老金等重要资产。银行从业者肩负着重大的社会责任，这就要求我们在使用新技术时必须要秉持十分谨慎的态度。无论是对国家而言，还是对银行业而言、对银行业从业者而言，我们自身的信用都是最重要的。</p><p></p><p>AI 大语言模型的应用其实有很多技术路线，但本质上我们从数学角度来看，它其实只有两种：统计学方法和推理方法，也就是数字驱动和知识驱动。在我们的实际应用中，一定是将这两种方法相结合使用的。</p><p></p><p>我们今天能够看到，基于统计学方法的大型语言模型在金融业，尤其是银行业中的应用引发了一系列问题。如同刚刚提到的，银行承载着客户的工资、养老保险和退休基金等重要资产，这些业务对精确性的要求极高，我们必须确保所提供的服务和产品具备极高的准确性。因此，在开发和应用 AI 技术时，除了统计学模型和大型语言模型所提供的知识以外，我们还必须结合基于推理方法的知识驱动。</p><p></p><p>银行业在利用生成式 AI 的路上既面临机遇又要应对挑战：</p><p>借助生成式 AI，银行可以实现更精细化的客户定位、更准确的风险评估和更高效的合规性管理，从而创造更大的商业价值。银行业可以通过使用生成式 AI 技术，提高数据的利用效率和质量，从而提高风险管理和合规性管理的效率。生成式 AI 可以为银行提供更准确的预测和决策支持，从而帮助银行提高效率和降低成本。银行需要积极采取措施，以确保生成式 AI 的合规性和透明度，同时注意潜在的风险和挑战。银行需要积极推动生成式 AI 的应用，以实现数字化转型和业务创新，从而保持竞争优势。</p><p></p><h2>汇丰银行 AI 应用案例</h2><p></p><p></p><p>在近两年的“百模大战”中，我比较关注大模型在银行业，特别是 IT 方面的使用。从技术背景上来谈，大模型主要在以下几个方面发力：</p><p>海量数据训练：学习词汇、语法、句法结构等语言要素，具备生成流畅、连贯文本的能力；多领域知识：能够处理来自不同领域的问题；丰富的语言表示：将输入转化为高维向量表示，捕捉了语义、句法、语境等多个层面的信息；上下文理解：理解上下文并生成连贯的回复，能够更好地回应复杂的问题；可定制性 / 可持续学习：垂直领域数据微调。</p><p></p><p>但包括汇丰在内，目前全球范围内银行业直接用于 client facing 的大语言模型项目并没有真正落地。这主要受到了两个因素的影响：一是整个银行业的谨慎态度，二是监管因素。</p><p></p><p>在 client facing 落地之前，我们先开始了 staff facing（面向员工）的大模型研发应用。同时由于 IT 部门员工最熟悉大语言模型，故这一技术最先对他们开放。</p><p></p><p>虽然我们在文本方面看到了大语言模型的一些令人惊喜的能力，但我们今天的大语言模型仍然需要与人协作，基本上还是一个“Copilot”，而非“Autopilot”。因此面向员工的大模型应用的起始点就是，在整个软件开发的生命周期内，利用大语言模型帮助提高银行业务。</p><p></p><p>银行积累了大量的技术债务，并拥有分散且复杂的 IT 架构。我们目前在用 AI 辅助经济 API 软件的开发，让 AI 帮助事故处理等一系列 IT Operation 方面的工作。我们始终将大型语言模型视为与开发者协同工作的辅助工具，而非完全替代人类工作的自动化工具。</p><p></p><p>如果一家创业公司的新系统上线后因使用人数超出预期而崩溃了，舆论会持宽容的态度，反而认为这是新系统太受欢迎、是一件好事；而如果银行系统崩溃，则一定会遭受全球媒体的负面报道。这种监督压力使我们在用 GAI 时不会直接在面向客户的领域上线新项目，但 GAI 可以用于提高系统的鲁棒性，或在需求变更时提高帮助。</p><p></p><p>当系统运行出现事故， GAI 还可以自动推测软件 bug，我们也形成了一套 AI 驱动的事故管理系统。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c8/c88da6ca9212e0c723d041160c382050.png" /></p><p></p><p>此外，在银行业务方面，随着 GAI 技术的发展，我们的工作必须“道高一尺魔高一丈”。在风险合规方面，曾经的人脸识别、活体识别技术几乎已被 GAI 打败，这些技术必须跟随 GAI 一同进步，就比如现在的人脸识别环节都要求用户摇摇头、眨眨眼。</p><p></p><p>在下图呈现的更多银行业 AI 应用场景中，知识产权管理是一个值得关注的议题。当前，多模态模型的发展引发了关于模型训练中使用语料的知识产权问题的讨论。这些训练材料是否受知识产权保护？我们能否使用这些材料？根据现有的知识产权法律体系，大型语言模型在知识产权方面的合法性存在疑问，这方面的法律需要与时俱进。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ff42b4651d5bd12b41a996ac515c1bd0.png" /></p><p></p><h2>未来银行 AI 应用走向何方</h2><p></p><p></p><p>尽管最新技术如使用 AI Agent 已取得显著进展，但这些技术的实际效果与面向消费者的（to C）应用相比仍有差距，特别是在处理私域数据方面。对于小规模训练样本的数据，AI 技术所能发挥的作用相对有限。此外，AI 技术的应用主要集中在银行后台 staff facing 的员工使用领域。这是 AI 技术需要进步的方面。</p><p></p><p><img src="https://static001.geekbang.org/infoq/aa/aa16f44ae377f0bfe1338c8de9525392.png" /></p><p></p><p>刚刚提到的这些应用场景中，我们并没有探索全部场景并取得较好效果，下图标红的应用是我们尝试后觉得比较有信心、能够很快有所提高的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b7/b7a9faed83d1388ce5b99f2e1bcadd9f.png" /></p><p></p><p>在谈到 AI 特别是 GAI 时，有两种态度：第一，一种没必要的悲观情绪；第二，过于乐观的清晰。对于这件事，我认为既不需要觉得 AI 可以完全替代程序员了，也不用走另一个极端、期待 AI 带来非常大的提升。我们通过自己的实验发现，AI 能够提高 20% 到 30% 的效率。大家可能觉得这个数字很小，但比如我负责的架构部门有几千个程序、在全球有大规模的 IT 人员，如果都能提高 20%-30% 的效率，就将积累出很可观的效果。</p><p></p><p>最后我想谈一谈银行业 AI 应用面临的挑战。作为银行业从业者，我们一直在谈风险，银行业务的本质其实就是 Risk Management（风险管理），关乎到我们怎么看 credit（信用）、怎么看 liquid（流动性）风险的问题。我们在实践中发现，发展中的问题只能用发展来解决，回避肯定不是好方法。</p><p></p><p>今天我们在私域数据、强监管领域里看见了各种各样的风险挑战，但 AI 带来的技术进步同样不容忽视。在当前情况下，我们只能用发展的、更先进的技术来解决发展中产生的疑惑，包括歧视、数据安全、诈骗、风险监管等等方面。</p><p></p><p>那么随着 AI 技术不断发展，我们所有人都会被 AI 取代吗？传统岗位会不会全部不复存在？我认为“工作”本来就不一定是人必不可少的一个部分，如果 AI 技术真的发展到能够完全取代人的工作的地步，也未尝不是一件好事，我们可以做更多工作以外的事。</p><p></p><p>我相信无论是歧视、数据安全、诈骗、风险监管这些具体的问题，还是我们自身生存、工作这些更广泛的问题，都会在有了新技术后找到其他意义。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/1f87OE5DUEa7Dkm5r9vb</id>
            <title>AI大模型如何在各行业跑通业务闭环？</title>
            <link>https://www.infoq.cn/article/1f87OE5DUEa7Dkm5r9vb</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/1f87OE5DUEa7Dkm5r9vb</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 May 2024 02:16:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 主持, 嘉宾, 大模型, AI应用
<br>
<br>
总结: 本文讨论了大模型在AI应用中的重要性和挑战，以及在不同行业中的应用现状和探索。通过对话展示了在智能风控、金融、物流和供应链等领域中大模型技术的应用和发展。 </div>
                        <hr>
                    
                    <p>主持&nbsp;|&nbsp;高磊</p><p>嘉宾&nbsp;|&nbsp;平野，贾志鹏</p><p>&nbsp;</p><p>诸多新技术范式的出现正在重塑AI大模型应用的落地路径，大模型在推动企业向全面数智化转型的同时，也在对以往的AI应用开发与运维流程产生深远影响。当大模型爆红之初惊喜又兴奋的心情平复下来时，AI大模型落地行业场景时的诸多挑战逐一浮出水面。</p><p>&nbsp;</p><p>各行各业如何面对大模型应用探索中的新挑战？对于金融行业等数据密集、对产出结果精确度有很高要求的产业，或是要求严谨专业的物流与供应链领域而言，大模型应用如何平稳走进业务场景？下一步，AI大模型该如何结合行业特点、满足行业要求，向行业垂直领域大模型发展？&nbsp;</p><p>&nbsp;</p><p>在日前的《超级连麦.&nbsp;数智大脑》x<a href="https://archsummit.infoq.cn/2024/shenzhen/">&nbsp;ArchSummit&nbsp;</a>"直播节目上，顺丰科技运筹优化算法总工程师高磊、天弘基金人工智能部负责人平野、Fabarta高级技术专家贾志鹏就这些问题展开了深度探讨。</p><p>&nbsp;</p><p>以下内容根据对话整理，篇幅有删减，<a href="https://www.infoq.cn/video/mmXGof9LkcI06AJVC6XD">点击链接</a>"可观看直播回放：<a href="https://www.infoq.cn/video/mmXGof9LkcI06AJVC6XD">https://www.infoq.cn/video/mmXGof9LkcI06AJVC6XD</a>"</p><p></p><p>ArchSummit 深圳大会议程已经上线，感兴趣的同学请锁定大会官网：<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">https://archsummit.infoq.cn/2024/shenzhen/schedule</a>"。</p><p></p><h3>大模型应用现状简介</h3><p></p><p></p><h5>高磊：平野老师曾经在支付宝第五代智能风控引擎Alpharisk的开发中发挥了重要作用，能否分享一些AI和大模型在智能风控领域的技术创新和实践经验？目前，天弘基金在大模型领域又有哪些探索？</h5><p></p><p>&nbsp;</p><p>平野：在支付宝支付风控部门任职期间，我参与了风控引擎架构的转型建设。最初，风控引擎的设计主要依赖于策略，当时大约70%以上的风险防控措施都是基于策略的。制定策略时先由专业分析师根据具体场景构建特征，再通过这些特征组合成策略进行风险管理。</p><p>&nbsp;</p><p>随着时间的推移，支付宝开始探索以AI模型替代人工的策略，在2017年至2019年这一早期阶段，尽管全球成功的AI风控案例并不多，我们仍坚持在AI风控领域不断探索和创新。风控引擎架构从传统的策略开始，逐渐过渡到一些机器学习模型，最终逐步引入了更高级的技术，如图神经网络和深度神经网络。我们还设计了高性能的分布式支付决策链路，显著提升了风控效率。</p><p>&nbsp;</p><p>在AI应用方面，我们进行了多项独特创新，包括基于MOE的多任务学习框架，并针对具体场景进行了创新。此外，我们还开发了图算法和可信网络等技术，这些技术在支付宝的风险防控中发挥了重要作用。</p><p>&nbsp;</p><p>Alpharisk作为支付宝风控引擎的核心组件，经过数次迭代，已发展至第五代，其智能化水平显著提升。从风险感知、决策到结果演化，这一系列决策过程现在主要都由模型自动完成。</p><p></p><h5>高磊：阿里巴巴开发的大模型通义千问非常强大，支付宝在这方面是否有过合作？</h5><p></p><p>&nbsp;</p><p>平野：在天弘基金大模型项目进行的过程中，我们曾经考虑过与第三方厂商合作，或者直接调用一些行业领先的大模型API来实现我们的目标。然而，我们很快意识到，金融业务场景对数据的准确性和专业性有着极高的要求。这一行业特点使我们不得不面临两点问题：</p><p>&nbsp;&nbsp;首先，现有通用大模型，包括去年早期发布的ChatGPT&nbsp;3.5，它们在数据实质内容方面的完善程度并没有达到我们预期；&nbsp;&nbsp;其次，为使大模型在特定场景下的功能更智能化，就要求它能深入理解我们的业务场景，并专门学习不同的业务目标。但目前的通用大模型只能提供泛化的回答，并不能提供精准和实质性的内容。</p><p>基于这些考虑，我们决定主要通过自主研发来完成大模型项目，以更好地满足我们的业务需求。</p><p></p><h5>高磊：贾老师在Fabarta担任高级技术专家，曾在IBM、阿里云、HSBC等公司从事金融、制造和汽车等行业的业务解决方案咨询与实施工作，能不能从您的角度介绍一下，大模型技术在这些不同领域的应用现状，主要能解决哪些业务问题？不同行业之间存在哪些独特挑战？</h5><p></p><p>&nbsp;</p><p>贾志鹏：在公司成立之后，我们为多家企业提供了解决方案服务。这些服务包括针对不同行业设计定制化的解决方案，如金融、制造和医疗等行业。在这一过程中，我们注意到不同企业对大型AI模型（大模型）抱有多种期待。根据我们的经验，可以将这些期待大致分为以下三类。</p><p>现有系统优化：一些企业在大模型出现之前已经在运营或建设自己的系统，如文档管理、知识库和知识图谱等。这些企业希望通过大模型技术来丰富和优化现有系统，无论是优化数据接入还是提升最终产出。生成式能力应用：大模型具有强大的生成式能力，企业希望探索这些能力在特定场景下的应用。例如，在前期构建知识库后，企业可能想要实现企业内部文档的问答系统，挖掘现有文档价值、解析问题并提供答案。此外，企业还希望通过大模型自然语言表达来实现自动发掘企业内数据，如报表和数据，这在BI（商业智能）场景中尤为重要。智能决策探索：近期业界在探索如何通过大模型实现智能决策，这涉及到使用AI&nbsp;Agent技术将不同的业务场景与大模型、甚至企业原有的小模型结合起来，共同解决特定问题，以实现智能决策。</p><p>&nbsp;</p><p>高磊：我在顺丰集团负责智慧供应链的建设工作。在公司内部，我们正在探索大模型技术在物流和供应链领域的应用。我们正在构建基于开源模型的“丰语”大模型，该模型专注于物流供应链领域的专业知识。依托此模型，我们尝试了多种应用，主要分为三大领域：</p><p>售前售后服务：在售前，我们尝试为快递员配备智能助手，帮助他们即时回答客户的收寄物品问题，而不必依赖个人经验或咨询同事；在售后，我们将大模型技术应用于智能客服系统，自动生成对话摘要，减轻客服人员的工作量，并利用大模型进行服务质检和客户声音洞察，帮助响应客户共性问题并提高服务规范性。运营知识管理：我们尝试使用大模型技术自动从官方网站抽取国际通关的标准信息，并将其结构化，以便管理和应用。此外，我们还利用大模型优化快递下单时托寄物分类和描述的流程，以应对物品种类繁多带来的挑战。BI和智慧办公：在BI方面，我们为业务提供分析助手，帮助客户洞察和分析供应链表现、有无异常和根本原因，并提出优化建议；在智慧办公场景中，我们正在构建企业内部知识中台，来回答人力资源、财务、IT等方面的问题，并在内部即时通讯系统中集成大模型，自动生成会议纪要和聊天摘要等。</p><p></p><h3>大模型进入各行各业核心业务，难在何处</h3><p></p><p></p><h5>高磊：金融行业对前沿技术的接纳度是比较高的，不仅场景丰富、数据丰富，并且技术基础也较为完善，技术投入能力较好。根据两位的观察，目前大模型在金融行业的落地进展如何？能够满足最开始业界的预期吗？其中的阻碍和挑战主要来自哪些层面？</h5><p></p><p>&nbsp;</p><p>贾志鹏：目前在金融行业中，将图智能技术与大模型结合的应用非常广泛。金融业务的复杂性导致交付过程中会涉及多个方面，包括风险控制、合规性、营销以及企业知识库和运维场景。尽管这些场景可以被统称为金融行业，但在不同场景下的具体需求有所不同。</p><p>&nbsp;</p><p>对于风险控制，支付宝的风控手段如Alpharisk，已经非常成熟。我们目前的重点是利用新技术，如图智能技术等，结合大模型，以更深层次、更多样化的手段来进一步完善风控体系。例如，在风控领域，除了传统的风险阻断措施外，还需要向客户提供合理的解释，这就需要模型的输出结果具有权威性，并且能够被把握和解释。</p><p>&nbsp;</p><p>在合规性方面，随着金融监管的加强，金融机构需要遵守国家规定（外规）和内部规定（内规）。这要求我们将这些规定融合，并从中提取关键知识点。同时，还需处理包括司法判决、行政处罚在内的文档，并关联提取这些数据。这涉及到整合包括文本数据、企业内部交易数据和客户数据等多元异构数据的问题，是当前金融行业面临的重大挑战。</p><p></p><p>平野：金融行业在大模型技术的发展和应用上，经历了几个明显的阶段。</p><p>初期兴奋阶段：随着ChatGPT的诞生，特别是在去年早期，&nbsp;ChatGPT&nbsp;3.5在短时间内迅速吸引了2亿用户，这让人们产生了AI能够解决一切问题的错觉。许多公司开始尝试将大型基础模型应用于金融行业的各种场景，如客服、合规风控、审核和用户体验等。质疑阶段：在大模型技术落地应用的过程中，人们开始意识到并不是所有问题都能通过一个模型解决，特别是金融行业对合规性的要求非常高。大模型有时会生成不准确或误导性的信息，这对于用户和投资者而言是不可接受的。这导致了对大模型实际效用的质疑。理智阶段：如今，金融行业对大模型的应用更为理智和审慎。技术发展开始向真正可落地的技术转向，比如基于大模型衍生出的新型架构，如RAG（Retrieval-Augmented&nbsp;Generation）架构和AI&nbsp;Agent架构。这些架构除了能够增加模型回答的实时性，还让大模型更接近人类的思考方式，能够将复杂问题分解为多个子任务，并利用大模型的泛化能力进行准确的意图理解和回答。</p><p>&nbsp;</p><p>除了传统的应用场景，金融行业还在探索如何将大模型应用于投资决策辅助等新领域。例如，Bloomberg（彭博）等公司发布的金融大模型，以及FinChat、FinGPT等，都在尝试利用大模型技术进行投资研究。天弘基金也在尝试将大模型应用于投资研究方向。</p><p></p><h5>高磊：大模型在金融领域除了理解摘要方面的工作外，还有没有一些创新的想法？</h5><p></p><p>&nbsp;</p><p>平野：在金融领域，大模型的应用经常被优先考虑用于客户服务和智能问答，这与ChatGPT以对话框形式出现时的自然联想相吻合。</p><p>&nbsp;</p><p>在天弘基金，我们探索了一条不同的路径，特别是在投资研究方面。我们开发了一款名为“弘小助”的内部产品，专注于投研领域。弘小助的主要特色在于采用了RAG架构，进行实体化的检索。</p><p>&nbsp;</p><p>在行业研究方面，我们进行了一项特别创新的工作，即将研究员的思维模式融入大模型中。通过COT（Chain&nbsp;of&nbsp;Thought）这种思维模式，我们结合了研究员的思维方式和大模型的能力，创新了一项名为“COM”的技术。这个过程将“thought”转化为我们的“mind”，使我们能够将意图拆解为多个子意图，并进行更深层次的分析。例如，如果我们要了解今天光伏行业的投资机会，通用大模型可能会提供一个一般性的回答，这可能不是研究人员真正想要的深层信息。通过将研究思维整合进大模型，并进行创新性的工作，我们可以在一定程度上为研究人员提供辅助工作，帮助他们更深入地理解市场和投资机会。</p><p></p><h5>高磊：请问平野老师，大模型来做投研，个人投资者有没有机会使用？或者是否能对其有所帮助？应该如何使用？</h5><p></p><p>&nbsp;</p><p>平野：投资通常被视为一门艺术，而不仅仅是技术。尽管如此，随着人工智能的快速发展，越来越多原本需要人工完成的思考和分析工作，现在可以由机器来辅助完成。</p><p>&nbsp;</p><p>从技术角度来看，大模型在投资中主要发挥辅助作用。它不会直接告诉投资者应该买入或卖出什么，因为这样的建议可能缺乏可信度。大模型的真正价值在于提供深入的行业分析、资金流向、市场行情、政策变化等信息，帮助投资者更好地理解投资环境。如果投资者对某个特定的投资标的或行业感兴趣，大模型可以提供该行业的现状、资金流向、市场行情等详细信息。对于基金产品，如ETF，大模型可以帮助分析近期的市场异动，包括政策、资金和基本面的变化。此外，大模型还可以分析宏观层面的因素，如美联储的加息决策对投资的潜在影响。通过整合金融领域的数据库，大模型能够提供准确的数据支持，帮助投资者形成自己的投资逻辑链。例如，在光伏行业，大模型可以展示产业链上中下游企业的财务和经营状况，使投资者能够判断企业的优劣和发展前景。</p><p>&nbsp;</p><p>尽管大模型可以提供数据、舆情、投资链路和财务分析等综合信息，并生成总结性摘要，但最终投资决策仍然需要人的参与。投资者应根据大模型提供的信息，结合自己的判断，做出正确概率较大的投资选择。</p><p></p><h5>高磊：平野老师将在ArchSummit深圳大会上分享《AI&nbsp;Agent：超越文本，走向自主决策与交互》话题演讲。那在天弘基金的业务实践中，AI&nbsp;Agent&nbsp;落地的场景包括哪些？在这个过程中面临的主要挑战是什么？您是如何解决这些挑战的？</h5><p></p><p>&nbsp;</p><p>平野：与其他行业相比，金融行业对结果的可靠性和专业性要求极高。当前大模型存在的幻觉问题和知识过时问题，可以通过AI&nbsp;Agent技术有效规避。AI&nbsp;Agent能够为大模型提供私有且可靠的知识注入，同时获取市场实时信息，如指数地图和每日行情等。在各个场景中，大模型被视为一个能够调动各种工具（tools）的智能体，更贴近人类的思考和行为模式，同时比人类更聪明高效。我们调研了多个金融行业业务场景中AI&nbsp;Agent技术的应用效果，包括投研投顾、风控营销、客服合规等。特别是在金融知识问答、研报知识解读、文章精读等场景中，AI&nbsp;Agent技术显示出较大的潜力。此外在销售过程中，AI&nbsp;Agent可以快速生成营销物料以抓住时机，对提高效率有显著帮助。</p><p>&nbsp;</p><p>AI&nbsp;Agent落地过程中面临的挑战：</p><p>低容错率：由于金融行业对时效性和准确率的高要求，模型推理过程必须非常清晰，以确保结果的有效性和可靠性。合规监管要求：金融行业合规监管要求较高，导致许多公司更倾向于本地化部署模型，这增加了自研成本。推理分析的高要求：金融行业对推理分析的要求超过其他行业，因为金融问题往往缺少标准答案，需要用模型进行事实性的推理，这要求技术向特定架构倾斜，并进行强化学习。多模态理解：金融行业涉及多模态信息理解，如财务报表、图表、视频等，有效关联和分析这些信息很困难。</p><p></p><h5>高磊：对于AI&nbsp;Agent技术，您提到了感知和决策的能力是至关重要的，您团队是如何利用深度学习、自然语言处理等技术来提升AI&nbsp;Agent的感知和决策能力的？</h5><p></p><p>&nbsp;</p><p>平野：在AI&nbsp;Agent架构中，感知能力和决策制定构成了其核心。特别是在金融投资领域，市场信息的准确感知对于形成有效决策至关重要。投资经理在做出决策前，必须收集和消化包括研报、国际与国内市场状况、宏观经济、行业动态、公司情况及监管变化在内的大量信息。传统AI模型在处理此类复杂信息存在局限，而大模型的应用为解决这一问题提供了可能：它能够处理信息差异，帮助拆解和解决投资决策中的问题。</p><p>&nbsp;</p><p>我们内部开发的Copilot工具，是一个基于自然语言处理技术的交互式数据查询工具，可以提升市场感知能力。该工具分为三个关键部分：首先，它需要理解并计算海量金融指标；其次，确认所采取的步骤是否正确；最后，根据不同用户角色的需求定制知识库，减少模型需要处理的指标范围。此外，通过RAG技术，模型不仅能召回所需的指标字段，还能根据用户的历史搜索习惯进行智能联想，从而提高查询的准确性。</p><p>&nbsp;</p><p>在决策层面，将用户的自然语言描述转换为可执行的查询，需要模型对金融领域有深刻的理解。这包括基于基座模型进行预训练，输入大量金融数据和特定的金融指标，如收益率、Alpha、Beta等，再训练模型理解这些指标及其应用场景。此外，通过指令对齐和利用相似查询案例的预训练，可以提升模型识别用户查询意图的精准度。在执行层面，大模型根据对场景的理解调用不同的指令，以适应不同场景的需求，从而提供更加精准的结果。</p><p></p><h5>高磊：在多Agent系统的实际应用中，专家们是否有过将其落地到具体场景的经验？当涉及到跨企业间的AI应用交互时，有哪些重要的注意事项需要考虑？具体来说，应该留意哪些关键的事项以确保AI应用交互的有效性和安全性？</h5><p></p><p>&nbsp;</p><p>平野：在天弘基金，我们已经将多Agent技术应用到实际场景中，尤其是在我们提到的“弘小助”产品中。这个产品需要处理来自不同方面的问题，比如某个ETF在特定行业的表现、当天股市的大盘情况、金融宏观层面的总体状况，甚至是更具体的投资模型问题，如Fama-French三因子模型的含义和应用。</p><p>&nbsp;</p><p>这些问题不是单一场景下的通用问题，而是需要多角度分析的复杂问题。我们采用了多Agent技术，将其拆解为三个维度来处理。</p><p>发散性思维：针对抽象的金融问题，如光伏行业的投资时机，研究员需要从多个角度思考和收集信息。我们利用大模型的能力，结合研究员标注的专业图谱进行训练，模拟研究员的思维模式，进行多角度分析。顺序性思维：对于需要逻辑性和时间顺序的任务，如写作，我们使用Agent技术规划整个写作蓝图，然后通过大模型生成基于召回信息的有序内容，确保逻辑性和连贯性。抽象思维：针对具体且复杂的金融综合性问题，如在特定宏观经济条件下的政策影响，我们利用大模型对知识结构的压缩和对指令的全面理解，将复杂问题拆解为多个子问题再分别处理，最终整合出全面的回答。&nbsp;</p><p>&nbsp;</p><p>多Agent技术的应用，不仅提升了我们处理问题的效率，并且通过模拟研究员的思维模式引导了整个分析过程，使我们能够更好地理解和回答复杂的金融问题。通过这种方式，我们能够将大模型的生成能力和传统模型的计算能力结合起来，解决供应链中的一些核心问题，如路径规划和销量预测，这些都是序列生成任务。尽管目前还存在一些挑战，但我们相信，随着技术的发展，大模型技术将在供应链决策中发挥越来越重要的作用。</p><p>&nbsp;</p><p>贾志鹏：我们公司在构建企业知识中台。作为一家研发公司，我们在内部经常会遇到需要确定谁对某个技术领域更熟悉的问题。例如，当我们想要了解谁对引擎的存储层最了解时，需要从多个维度进行评估。首先，我们会查看代码提交记录，通过Git平台的API接入来分析谁在相关领域的代码提交量最多。其次，我们会分析设计文档、分享材料和会议记录等文档资料，以确定谁在这些文档中的记录和贡献最多。此外，我们还会利用内部结构化数据来辅助评估。通过代码提交分析、文档解析和内部数据查询这三个渠道，我们能综合召回结果，并形成一个全面的回答。在回答的过程中，我们不仅提供答案，还强调可追溯性，即清晰展示得出这个答案的过程和依据。例如，如果我们得出结论认为某位同事对引擎最熟悉，我们会展示支持这一结论的证据，包括代码提交量、文档贡献以及组织架构中的相关信息等。这一过程确保了我们的回答不仅准确，而且具有很高的透明度和可信度。</p><p></p><h5>高磊：志鹏老师的公司是做图智能技术的，在金融领域，您认为图智能技术的应用潜力体现在哪些方面？具体落地到企业的业务中，面临的最大挑战又是什么？您的团队是如何解决这些挑战的？</h5><p></p><p>&nbsp;</p><p>贾志鹏：我们公司一直在积极推动图智能技术的实际应用落地。在讨论图技术时，人们通常会想到知识图谱或图神经网络。知识图谱是我们的一个应用领域，它之所以受到重视，是因为它包含了大量经过专家验证和精心运维的确定性知识。尽管知识图谱也有一定的运维成本，但在达到一定规模后，其价值是显而易见的。</p><p>&nbsp;</p><p>但如何构建一个完整和完善的知识图谱是一大挑战，它需要整合来自不同来源的数据，包括结构化数据库、文档提取以及专家经验等。大模型的出现为解决知识图谱构建中的一些基础问题提供了新的可能性，例如利用大模型辅助数据标注、实体提取和实体关系连接等。</p><p>&nbsp;</p><p>除了知识图谱，图技术在其他应用场景中也展现出其潜力，如金融风控和合规领域。这些领域并不需要构建一个完整的知识图谱，而是需要实时生成交易图或关联图谱，以便业务人员能够通过可视化手段探索和分析数据模式。在贷款审查或交易链路探查过程中，图技术可以帮助揭示循环担保、家族式交易或集团内部隐藏关系等问题。这些模式一旦被业务专家通过图技术探索出来，就可以通过技术手段复用，并推广到更全面的数据样本中去发现类似问题。这引出了一个新的挑战：如何节省业务人员在发现问题后编写报告的时间。在反洗钱领域，银行或金融机构需要定期处理大量的排查名单，如何快速应用已发现的模式并生成总结报告是一个亟待解决的问题。</p><p>&nbsp;</p><p>大模型结合图模式可以作为解决这一挑战的工具，帮助生成和加速报告生成过程。在营销领域，尽管我们不是指大规模的互联网或电商营销，银行内部的金融产品、信用卡产品推广，以及银行APP流量和商户推荐等，都可以通过构建图谱并发现关联关系来进行更有效的营销推广。在这个过程中，图神经网络可以发挥作用，虽然它需要大量数据和算力来实现最佳效果。</p><p>&nbsp;</p><p>在图智能技术应用过程中，需要综合考虑不同场景下的因素，这些因素将决定最终的架构决策。数据如何进入图谱是一个关键问题，特别是对于中小企业如何应用大模型能力这一问题，我们非常感兴趣，并一直在实践中帮助客户构建这样的能力。</p><p>&nbsp;</p><p>此外在数据准备阶段，企业需要处理多源数据，包括结构化和非结构化数据，并解决数据的关联和组织问题。我们目前正在进行的“元数据知识化”项目，旨在通过大模型辅助生成标注，并通过图的方式组织数据，从而为后续的数据分析和问答提供清晰的数据关系和可追溯性。这是我们在图智能技术应用中的一个重点投入方向。</p><p></p><h5>高磊：不同行业图谱的构建和生成有哪些共性？</h5><p></p><p>&nbsp;</p><p>贾志鹏：根据个人经验，在金融、制造、医疗这三个方向上，图谱的构建和应用具有一定的共性。首先，知识图谱因其确定性知识的特性，与特定领域紧密相关。在提取知识构建图谱的过程中，我们不依赖通用的方法，而是采用技术框架来组织数据，关键是如何快速有效地将数据整合入图，形成有用的图谱。</p><p>&nbsp;</p><p>目前，我们在数据整合方面积累了几个方向的经验。我们将数据分为结构化、半结构化，并采取自动和半自动的方式入图。半自动入图涉及使用大模型技术预先处理数据，随后采取人工审核和反馈循环进行修正。自动入图则依赖信息化过程中产生的结构化数据，如交易数据或供应链中的供应商信息，这些可以直接映射入图。</p><p>&nbsp;</p><p>除了技术手段，图谱构建还需要理论支持和方法论指导。在实施过程中，我们强调业务目标的中心地位，避免一开始就构建一个过于庞大的图谱。以风控为例，我们会筛选与风控业务相关的信息设计图谱的schema，并裁剪现有数据以形成有效的图谱。如果基础图谱设计得既完善又合理，并且业务方能够清晰理解图谱所表达的内容，那么在使用过程中就能够带来显著的效果。</p><p></p><h5>高磊：志鹏老师，在ArchSummit深圳的演讲中，会提到大模型与图智能技术结合的三种方式，可以先大致给大家介绍一下吗？您认为其中在金融领域的应用前景最广阔的是哪一种方式？为什么？</h5><p></p><p>&nbsp;</p><p>贾志鹏：这一课题的准备工作始于4月份，当时我们提出了三种结合方式，前两种较为常见：</p><p>&nbsp;</p><p>第一种是通过大模型辅助图谱构建，利用大模型从非结构化数据中提取知识，并将其整合到图谱中，随后通过人工审核完成图谱的构建。第二种是大模型增强的图谱使用，例如在风控领域，通过图模式发现数据后，利用大模型生成用户报告。第三种是图增强概念。近一两周，我们注意到微软等公司也发表了相关研究文章，提出了图增强GraphRAG等概念。我们认为，知识图谱的优势在于承载确定性知识，若在大模型问答过程中能够结合图谱进行知识召回，那么所召回的知识将更具权威性和业务价值。通过这种方式，可以提高回答的准确性。</p><p>&nbsp;</p><p>我们认为这三种方式相辅相成，不仅在图谱构建阶段利用大模型，在图谱使用过程中也通过大模型增强其效能，而在大模型召回结果时，图谱起到了关键作用。我们预见，大模型与图智能技术结合后的应用前景广阔，尤其是在决策智能方面。结合图谱的确定性知识与大模型的生成式知识，可以对企业现有的多模态数据进行加工沉淀，形成具有生成能力和可解释性的决策支持。以金融大模型投资为例，个人投资者可以在不同阶段获得理解和辅助，最终做出决策。</p><p>&nbsp;</p><p>我们希望通过图的方式将决策所需的知识串联起来，构建一个确定性的图谱，为用户提供确定性的答案。同时，通过关联方式呈现召回逻辑和回答逻辑，让用户清晰了解决策背后的逻辑，这一逻辑是结合了图谱确定性先验知识的。</p><p>&nbsp;</p><p>高磊：事实上，大模型在金融、风控、营销等多个领域，智能制造背景下的大模型和AI技术展现出巨大的应用潜力。例如，在药物研发领域，DeepMind公司近期在《自然》杂志上发表的研究成果AlphaFold3，展示了AI准确预测蛋白质空间结构及其与化学物质相互作用的能力，这对药物设计具有重大意义，显著降低了实验和筛选候选药物的成本。</p><p>&nbsp;</p><p>众多国内外创业公司正在开发AI辅助的工业设计和服装设计软件或系统。谷歌也曾发表使用强化学习辅助芯片设计的研究。这些新技术，包括大模型和AI技术，在智能制造的设计和研发阶段提供显著帮助。在生产阶段，AI技术，包括大模型，在生产计划制定、排程排产、物流优化、工艺参数智能调优、AR/VR技术应用以及计算机视觉辅助质检等方面，都有很大的应用前景和空间。</p><p>&nbsp;</p><p>在即将到来ArchSummit深圳大会上，AI助力工业和制造智能化专题论坛将邀请来自美的、心智优化、生活科技、红海科技、腾讯云等企业的专家分享AI在工业制造场景下的最新应用案例和最佳实践。论坛将探讨如何有效利用AI技术推动制造业智能化转型，以及如何克服AI应用过程中的问题和挑战。</p><p>&nbsp;</p><p>在物流和供应链领域，AI模型应用面临独特挑战。尽管大模型具备强大的通用能力，如海量知识、规划推理能力，甚至编写代码能力，但它们也有局限性，例如易产生幻觉、不擅长精确数值计算，以及对深入业务理解的不足。这些问题同样困扰着大模型在供应链领域的应用。目前，我们在供应链领域的大模型应用探索主要集中在售前、售后和智能办公等周边领域。</p><p>&nbsp;</p><p>供应链核心领域的智能优化和决策是非常专业和严谨的，任何不准确的信息或计算错误都可能对业务造成重大影响。智能决策领域的问题，如网络规划和路径优化，通常是NP-Hard问题，需要大量计算，且输入输出是非文字和非图片的结构化数据。因此，大模型在供应链智能决策领域目前难以发挥作用，也不是其擅长的领域。目前可以通过Agent技术结合传统模型，将传统模型和技术封装为工具，由大模型负责人机交互界面和高层控制规划，从而在传统技术和大模型技术之间架起桥梁。</p><p>&nbsp;</p><p>此外，供应链领域的许多决策任务本质上是序列生成任务，如旅行商问题可以抽象为序列生成问题。因此，可以利用大模型底层技术，如Transformer架构，来解决这些问题。尽管目前还存在学术研究中的问题简化、距离计算的欧式空间问题、实际问题规模的扩大等挑战，但随着研究的深入，预期未来大模型技术能够在供应链核心决策问题上实现突破，并在行业中得到普及和应用。顺丰也在持续关注这一领域，并尝试解决这些问题，期望未来能分享相关进展。</p><p></p><h5>高磊：中小规模的公司想做顺丰现在实现了的一些场景，大概要花费多少投入？需要组建什么规模的团队？</h5><p></p><p>&nbsp;</p><p>高磊：在讨论大模型应用的投资问题时，需要考虑几个关键因素，包括赛道、行业特性、专业数据的需求以及算力和数据的投入。</p><p>行业特性与场景需求：首先，投资的规模和方向取决于所处的行业和具体应用场景。如果行业有大量专业知识和特定数据需求，而这些是公开模型无法处理的，那么可能需要更多的投资来进行模型的微调（fine-tuning）、使用自有数据进行训练。算力与数据投资：在金融等数据密集型行业，算力和数据通常是投资的重头。如果需要自行微调模型，这将涉及到基建方面的较大投资。人才与研发投入：如果不需要对模型进行微调，或者不需要通过复杂的技术嵌入行业知识，研发人员的投入可能并不会特别大。在这种情况下，少数算法工程师可能就足以推动项目前进。定制化需求与投资规模：如果行业缺乏公开的大模型，或现有技术无法满足业务需求，需要自行收集数据并进行模型微调，那么所需投入将会显著增加。</p><p></p><h3>趋势与展望：各行业大模型应用将走向何方</h3><p></p><p></p><h5>高磊：在基金行业，如果上述提到的来自技术、业务、组织、人才等方面的挑战可以解决，平野老师，您认为AI和大模型技术还有哪些新的可能性和潜力？</h5><p></p><p>&nbsp;</p><p>平野：在未来，大模型在解决业务问题上将更倾向于深入各个垂直领域，针对特定问题提供解决方案。随着技术的不断进步，大模型的应用将从单一的文本处理方式扩展到更多模态的解决方式，比如结合视觉、语音等，这将推动更多复杂场景和问题的解决。</p><p>&nbsp;</p><p>将来在金融行业，大模型能够极大地提高研究效率。例如，以往一个研究员可能一天只能阅读十篇左右的研究报告，而未来大模型可以快速提炼关键信息，使研究员在短时间内阅读上百篇研报，并通过辅助工具进行深入分析和个性化阅读，这与整个行业发展趋势是相契合的。</p><p>&nbsp;</p><p>随着经济的崛起，金融行业的体量将不断扩大。以基金经理为例，当管理的资产规模从一亿增长到十亿、百亿甚至千亿时，需要关注的公司数量也会成倍增加。在这种情况下，大模型能够帮助基金经理更高效地处理信息，从而在决策过程中做出更精准的选择。大模型不仅在提高工作效率方面发挥作用，还能提升决策的专业程度，为使用者提供准确和有效的信息。在金融行业，大模型的应用将主要集中在提高研究效率和决策精准度上。</p><p>&nbsp;</p><p>展望其他行业，我认为大模型将逐渐承担起重复性和基础性的推理工作，而人类工作者将更多地从事架构层面和决策层面的最终决策。这样的分工将使整体工作流程更加高效，同时也能够达到比以往更令人满意的结果。</p><p></p><h5>高磊：志鹏老师，您对AI和大模型技术在金融领域的发展前景有何展望？未来，您认为AI技术将如何提升金融行业的智能化水平和服务能力？</h5><p></p><p>&nbsp;</p><p>贾志鹏：我非常认同平野老师提到的两个关键词：多模态和决策智能。我们公司在内部对未来大模型应用落地的展望也集中在这两个方向。多模态是指结合多种信息形式，如文本、图像、声音等，而决策智能则涉及到利用AI进行复杂决策的过程。</p><p>&nbsp;</p><p>平野老师也提到了通过多个Agent联合来实现决策智能。随着大模型和多模态智能在企业中的广泛应用，我们预见这将加速企业内部数据布局的整理工作。金融领域的数据极为复杂，不仅包括各种指标平台和不同的指标口径，还涉及银行、经纪人持有的数据以及行业和国家标准数据。要有效地支持大模型并整理多模态数据以实现准确决策，需要确保数据的准确性和就绪性。这意味着企业可能需要整合其所有数据，探索不同数据间的关联关系，并进一步挖掘和整合数据。包括元数据在内的不同数据类型需要被整合和关联起来，以便它们能够与大模型有效地结合，再通过多个Agent的学习和协作，承担完整的决策职能。大模型的应用可能会催生或加速企业数据整合的过程，推动企业更高效地利用其数据资源，以实现更精准的决策。</p><p>&nbsp;</p><p>高磊：除了金融领域，我还想提一下AI大模型在智能供应链体系建设方面的应用前景。AI大模型与智慧供应链的融合，是一个充满潜力和激动人心的领域，有许多令人期待的应用场景和可能性。</p><p>&nbsp;</p><p>第一，AI辅助的产品设计和研发，如AlphaFold3在医药研发领域的应用，以及AI在芯片设计和工业设计中的辅助作用，预示着这类技术将越来越普及。</p><p>&nbsp;</p><p>第二，随着AI技术的发展，例如特斯拉最近发布的擎天柱机器人二代，以及李飞飞教授新成立的聚焦空间智能的公司，集成智能的机器人将能够完成更多复杂任务。在生产制造和供应链领域，当这些具备大模型加持的具身智能机器人的能力达到一定水平，许多危险和重复性劳动完全可以由它们来承担。</p><p>&nbsp;</p><p>第三，Agent技术的应用将越来越广泛，未来可能逐渐替代一些B端软件。在公司中，无论是一线工作人员还是管理人员，都可能配备多个Agent助理，通过日常交流完成大量工作。</p><p>&nbsp;</p><p>第四，目前大模型在供应链智能决策核心的应用还处于外围阶段。但我相信，未来大模型将直接作用于智能决策的核心领域，将会有更多专业模型涌现出来。这些模型不再是通用的行业大模型，而是具备特定行业专精知识的模型，如专注于路径规划或销量预测的专业大模型。它们的应用形式可能不再局限于文本或图片等信息载体，而是直接输出对应的决策信息以支持业务需求。</p><p>&nbsp;</p><p>这些是我对AI大模型在未来，特别是在供应链和制造领域应用场景的一些展望和设想。随着技术的不断进步，我们有理由期待这些设想将逐步变为现实，为行业带来更多创新和变革。</p><p></p><h5>高磊：大模型可以替代金融分析师生成以行研报告吗？或者竞品技术分析有哪些方面的配置要求？</h5><p></p><p>&nbsp;</p><p>贾志鹏：我认为这个问题可能与平野老师刚才介绍的平台报告相关。基于我的观察和经验，技术并不能完全替代人类，它只能无限逼近人类的能力和决策水平。在某些方面，特别是在我们讨论的传导行业或ToB领域中，我们不能单一地从技术角度来衡量问题。实际上，管理因素在这些行业的落地应用中占据了重要地位。</p><p>&nbsp;</p><p>平野：分析师可以用大模型辅助撰写行业报告。我们认为大模型并不是要替代分析师，而是与之相辅相成，帮助他们更高效地完成任务。过去，编写一篇行业研究报告可能需要数小时、数周、甚至一个月的调研时间。如今在大模型的帮助下，这一过程的用时会大大缩短，甚至在一天之内就能完成。我们已经在这方面进行了实践，并开发了相关的落地产品。</p><p>&nbsp;</p><p>至于观众提到的竞品技术分析，我认为这实际上是一种情报分析。金融领域的信息通常并不完全透明，不同机构对信息的处理方式各异；同时金融业是一个信息量爆炸的行业，对信息的实时性和准确性要求很高。在大模型的支持下，分析师可以在短时间内迅速发现市场热点，这些热点很可能也是其他公司关注的焦点。利用BI工具的数据处理能力，结合大模型的推理生成能力，可以部分替代金融分析师的工作，甚至一个经过训练的大模型可以处理多个金融分析师汇总的信息。</p><p>&nbsp;</p><p>如果要考虑配置要求，我认为可能需要在市面上已有的模型基础上进行个性化训练，这可能涉及一定的硬件资源投入，如训练用的计算卡，同时也需要相应的人才来进行预训练和模型调优等工作。</p><p></p><h4>嘉宾介绍</h4><p></p><p>高磊，顺丰科技运筹优化算法总工程师，拥有10年+机器学习与运筹优化算法经验，研究方向为运筹优化、强化学习等。2016年加入顺丰，现任顺丰科技运筹优化算法总工程师，曾主导顺丰集团内部多个数智化项目的研发与落地工作，涉及领域包括业务量预测、陆运干支线规划与调度、航空规划与调度、运力规划、场站选址、物资调拨等。</p><p>目前主要负责集团智慧供应链体系建设相关工作。期间带领团队获得十余项发明专利，中物联物流技术创新奖、CCF&nbsp;BDCI一等奖、最具商业价值奖，运筹帷幄年度行业实践奖与学术应用奖等荣誉。</p><p>&nbsp;</p><p>平野，天弘基金人工智能部负责人，毕业于英国曼彻斯特大学计算机系。天弘基金算法团队负责人，负责营销、风控和投资智能化业务领域。天弘金融大模型负责人。曾就职于百度、蚂蚁金服等，是支付宝第五代智能风控引擎Alpharisk主要开发者之一，该项目获2019年浙江省科技进步一等奖。百度昊天镜业务风控模型团队总负责人。</p><p>&nbsp;</p><p>贾志鹏，Fabarta&nbsp;高级技术专家，曾先后就职于&nbsp;IBM、阿里云、HSBC，专注于金融、制造和汽车等行业的业务解决方案咨询与实施工作。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/w81bV9tKJqCXL7o5GfYe</id>
            <title>微软一夜革新生产力：Copilot 势不可挡，小模型持续炸街，奥特曼放话要把大模型价格打下来</title>
            <link>https://www.infoq.cn/article/w81bV9tKJqCXL7o5GfYe</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/w81bV9tKJqCXL7o5GfYe</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 May 2024 02:02:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 微软 Build 开发者大会, Phi-3小模型家族, Copilot, 人工智能时代
<br>
<br>
总结: 北京时间 5 月 22 日凌晨，微软在 Build 开发者大会上发布了一系列新品，包括 Phi-3小模型家族和 Copilot，讲述了人工智能时代将如何释放新机遇，改变开发人员的工作方式。Copilot被定位为生成式AI，微软希望通过这些新产品实现计算机理解人类、帮助人们进行推理和规划的梦想。Phi-3小模型家族的发布将为开发人员提供更多强大的工具和模型。 </div>
                        <hr>
                    
                    <p>北京时间 5 月 22 日凌晨，外界期待已久的微软 Build 开发者大会在美国西雅图开幕。微软 CEO 萨蒂亚·纳德拉 (Satya Nadella) 登台官宣一系列新品发布，并讲述了人工智能时代将如何释放新机遇、改变开发人员的工作方式并提高各行业的业务生产力。</p><p>&nbsp;</p><p>今天的微软 Build大会上，微软发布了一系列 AI 全家桶以及众多开发者工具，其中较为亮眼的要属Phi-3小模型家族中的一系列产品。</p><p>&nbsp;</p><p>纳德拉称已经在Huggingface上发布了Phi-3-medium，Phi-3-small，以及Phi-3-vision系列模型。其中Phi-3-medium-128k-instruct成为目前消费级硬件上可用的最好的模型。</p><p>&nbsp;</p><p>据介绍，Phi-3-vision仅4.2B参数。Phi-3-vision是其 Phi-3 系列人工智能小语言模型 (SLM) 的最新成员，目前正在预览中。 Phi-3 型号专为个人设备量身定制，将强大功能与成本效益融为一体。 Phi-3-Vision 拥有 42 亿个参数，能够处理各种视觉推理任务，包括图表、图形和表格的分析。它使用户能够输入图像和文本，生成基于文本的响应。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/8d/8d39cdd846aed84832f3a4ba068a5a76.png" /></p><p></p><p>此外，之前发布的 Phi-3-small 和 Phi-3-medium 现已在 Microsoft Azure 上推出，为开发人员提供需要强大推理、有限计算和延迟限制场景的生成式 AI 应用程序模型。此外，之前提供的 Phi-3-mini和 Phi-3-medium 现在也可以通过 Azure AI 的模型即服务产品提供，使用户能够快速轻松地开始使用。</p><p>&nbsp;</p><p>Phi-3 小 128k 开源地址：<a href="https://huggingface.co/microsoft/Phi-3-small-128k-instruct">https://huggingface.co/microsoft/Phi-3-small-128k-instruct</a>"</p><p>Phi-3-medium 128k 开源地址：<a href="https://huggingface.co/microsoft/Phi-3-medium-128k-instruct">https://huggingface.co/microsoft/Phi-3-medium-128k-instruct</a>"</p><p>Phi-3-small 8k 开源地址：<a href="https://huggingface.co/microsoft/Phi-3-small-8k-instruct">https://huggingface.co/microsoft/Phi-3-small-8k-instruct</a>"</p><p>Phi-3-medium 4k 开源地址：<a href="https://huggingface.co/microsoft/Phi-3-medium-4k-instruct">https://huggingface.co/microsoft/Phi-3-medium-4k-instruct</a>"</p><p>&nbsp;</p><p>此系列的所有模型的License均为MIT，相对Llama-3的License更友好。Phi-3-medium参数量为14B，包含4K和128K两个版本。用了512块H100，在4.8T块上训练了42天。</p><p>&nbsp;</p><p>值得注意的是，该系列小模型在代码和数学能力方面有巨大的提升，整体性能与Mixtral 8x22B, Llama 3 70-instruct接近，超过Command R+ 104B和GPT 3.5。</p><p></p><h2>Copilot进一步融入全家桶</h2><p></p><p>&nbsp;</p><p>Satya Nadella 在 Build 2024 大会开幕式上讨论了新的 AI 时代。他表示，微软几十年来一直有两个梦想：</p><p>&nbsp;</p><p>1）计算机可以理解我们而不是我们必须理解计算机吗？</p><p>&nbsp;</p><p>2）在这个信息不断增加的世界里，计算机能否帮助我们根据所有这些信息进行推理、规划和更有效地采取行动？</p><p>&nbsp;</p><p>纳德拉将这波人工智能浪潮定位为微软梦想的答案。</p><p>&nbsp;</p><p>微软使用 Copilot 一词来指代生成式 AI，在过去九个月的时间里，微软产品组合中已经有大约 135 个 Copilot&nbsp;surfaces，他们对人工智能的热情都表现在Copilot上了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b0/b07242744dcfefd6b39b8e6d3b18dbaf.jpeg" /></p><p></p><p></p><h2>Copilot 之于 AI 就像 Win32 之于图形用户界面</h2><p></p><p>&nbsp;</p><p>“就像 Win32 之于图形用户界面一样，我们相信 Windows Copilot 运行时将用于 AI — 它从我们的 Windows Copilot 库开始，这是这些即用型本地 API 的集合，可帮助您将所有我们昨天分享了人工智能功能。” Satya Nadella 将此比作 Win32 等重大时刻，让开发人员能够更轻松地将 AI 构建到他们的 Windows 应用程序中。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/96/9674d49a4ba652024f288c7810ffafa0.png" /></p><p></p><p>GitHub Copilot 是 Microsoft 生成式 AI 的首批重要产品之一，目前拥有 180 万付费用户。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/56/562388b3a2cc7ce1f3bf7dd8be57aa68.png" /></p><p></p><p>GitHub 推出首套由微软和第三方合作伙伴开发的GitHub Copilot 扩展，现已开启受邀预览。新增功能允许开发者、企业通过其偏好的服务（如 Azure、Docker、Sentry 等）直接在 GitHub Copilot Chat 中定制其GitHub Copilot 智能体验。</p><p>&nbsp;</p><p>作为微软推出的扩展功能之一，GitHub Copilot for Azure展示了如何利用自然语言和更广泛的功能来提高开发速度。通过 Copilot Chat 使用该扩展，开发者可以探索和管理 Azure 资源，同时排除故障、查找相关日志和代码。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a3/a314f92cf8c123ee6c833bfbe556ddd1.jpeg" /></p><p></p><p>Microsoft 宣布推出 Team Copilot，这是 Microsoft 365 中 Copilot 功能的重大演变。</p><p>&nbsp;</p><p>从个人 AI 助理转变为不可或缺的团队助理，Team Copilot 将可以跨各种协作平台（例如 Teams、Loop 和 Planner）进行访问。这一增强功能使 Team Copilot 能够扮演多种角色：会议主持人，管理议程、跟踪会议时间和记录会议内容；聊天中的协作者识别关键信息、跟踪任务和解决问题；项目经理确保项目效率并促进团队贡献。</p><p>&nbsp;</p><p>这些初始功能将于今年晚些时候提供预览，标志着新的创新阶段的开始，Team Copilot 越来越多地代表个人和团队行事。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5b/5bbb09319e8b3f7bdb4b54a4eea83b39.png" /></p><p></p><p>Microsoft Copilot Studio 正在推出高级agent功能，使开发人员能够创建能够自主对数据和事件做出反应的Copilot，并针对不同的任务和角色进行定制。这些增强的功能使Copilot能够利用记忆和背景知识，对行动和输入做出合理的决策，根据用户反馈进行调整，并在面临不熟悉的挑战时寻求帮助，从而自主监督复杂的、持续的业务运营。</p><p>&nbsp;</p><p>这项创新意味着用户现在可以在各个领域部署 Copilot ：从管理 IT 设备采购流程到充当销售和服务领域的客服。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/b0/b07242744dcfefd6b39b8e6d3b18dbaf.jpeg" /></p><p></p><p>纳德拉还宣布将通过Windows DirectML提供对PyTorch、WebNN框架的原生支持，这将为开发人员提供更多可用的工具。处理器的 NPU 应该可以帮助这些工具比以往更快地执行。</p><p></p><p><img src="https://static001.geekbang.org/infoq/46/46fb3a00919a827dc05b6aa13aee5851.jpeg" /></p><p></p><p>随后，纳德拉谈到了人工智能基础设施。</p><p>&nbsp;</p><p>他称，Azure在60多个地区可用。该公司已将其扩展到比以往更多的地区，并承诺提供可持续的云服务。有些内容对于普通人来说可能有点难以理解。但简而言之，如果开发人员能够创建让生活更轻松的应用程序，那么我们所有人都将获益。</p><p></p><h2>奥特曼压轴登场，要让大模型更便宜</h2><p></p><p>&nbsp;</p><p>在微软官宣了一系列更新后，OpenAI CEO 奥特曼也来到了活动现场以示支持。他鼓励开发者和初创公司利用当前的人工智能热潮，认为这是自移动设备繁荣甚至互联网出现以来最令人兴奋的时刻。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/7a/7a26f67a584eb203e147df71abe38bec.webp" /></p><p></p><p>在模型方面，奥特曼透露，未来，GPT-4o 将变得更快，但更便宜。</p><p>&nbsp;</p><p>他还预告下一个大模型即将问世，而且微软为这项工作建造了一台更大的超级计算机（如同虎鲸一样规模的超算）。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/73/73f023d92d5479c147b7642acb972791.webp" /></p><p></p><p>奥特曼暗示，新的模态和整体智能将是 OpenAI 下一个模型的关键。</p><p>&nbsp;</p><p>“最重要的一点，听起来也是我能说的最无聊的一点...... 模型会变得越来越智能，总的来说是全面智能。”</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ZYFrPLd3YtzayUJdKPe6</id>
            <title>在夏日开启一场技术狂欢，这份攻略等你查收！| Q推荐</title>
            <link>https://www.infoq.cn/article/ZYFrPLd3YtzayUJdKPe6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ZYFrPLd3YtzayUJdKPe6</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 May 2024 01:20:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 亚马逊云科技, 开发者展区, 生成式 AI, 技术盛宴
<br>
<br>
总结: 2024 亚马逊云科技中国峰会在上海举办，聚焦生成式 AI，展示技术洞见和数字化创新，开发者可参与各种活动，如主线任务、限时挑战等，体验最前沿技术产品，获得官方认证。 </div>
                        <hr>
                    
                    <p>5 月 29 日，2024 亚马逊云科技中国峰会将在上海·世博中心正式开幕，属于开发者们的技术盛宴再次来袭！这一次的峰会聚焦生成式 AI，论坛将覆盖 200+ 热点话题，汇集各行各业技术洞见；峰会展区也设置了 30+ 互动展区、30+ 明星展项、近 400 个展位，以及 200+ 数字化应用场景展示，开发者不仅可以和亚马逊云科技共探技术趋势，还能在展区沉浸式体验前沿科技，感受各行各业数字化创新进程。</p><p></p><p>今年开发者展区全面升级。在以「生成式 AI 时代重塑云上构建体验」为主题的“开发者大讲堂”现场，开发者不仅可以收听技术专家、行业先锋的深度分享，还能与亚马逊云科技 Heros 展开深度交流；峰会史上最大 hands on 区“挑战俱乐部”正式开赛，动手体验最前沿技术产品的同时，开发者还将参与进“鱿鱼游戏”中，凭借实力积分打榜，争夺“Top Fighter”称号与大奖；除此之外，还有全球技术社区组成的“开发者市集”、培训认证、开发者小讲堂、两大联赛...... 等一系列创新区域，等待着开发者前往解锁体验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/97/970acdd2da50dcfcbdf3a2f923ad3644.webp" /></p><p></p><p>那么，如何才能“速通”整个开发者展区，收获超爽游玩体验呢？这一次，InfoQ 化身“路透社”，提前对开发者展区进行了深入了解，并制定了游玩攻略。今天，InfoQ 将这份开发者展区游玩指南双手奉上，建议各位开发者赶快收藏！</p><p></p><h2>开发者展区游玩指南</h2><p></p><p></p><p>如何高效打卡所有内容？</p><p></p><p>有什么不容错过的活动？</p><p></p><p>如何在两天参会过程中有所收获？</p><p></p><p>这一次，开发者展区设置了丰富的玩法活动，既有「主线任务」，还有「限时挑战」，InfoQ 还为大家设计了「初次试炼」「进阶之路」「炉火纯青」三种体验路线，开发者不仅可以做任务赢好礼，还能通过展区项目提升技术水平，获得官方认证。</p><p></p><p>展区为所有开发者设计了一个主线任务和两大限时活动，凭借现场发放的探索地图，每个人都能通过打卡体验收获奖励，Play and Earn!</p><p></p><p>4 大模块，12 大区域，等你来探索</p><p></p><p><img src="https://static001.geekbang.org/infoq/c4/c41b4a70d9087ee6db1a0ef04a2c0541.webp" /></p><p></p><p></p><h3>主线任务</h3><p></p><p></p><h4>&nbsp;神秘扭蛋机，打卡赢好礼！</h4><p></p><p></p><p>在开发者会客厅的中央，将会出现一个神秘扭蛋机！每个扭蛋里都有趣味奖品，想要获取扭蛋机会，就需要参与进展区体验项目中。在开发者展区的每个一个体验区域，都设置了相对应的专属徽章，获取四枚不同徽章，即可换取一次扭蛋机会。</p><p></p><p>想要获取徽章，就要相应完成专属任务：</p><p></p><p><img src="https://static001.geekbang.org/infoq/0d/0dffeca1817c5f56535d7187948ed8ec.webp" /></p><p></p><p>针对完成全部徽章收集的“大佬”，开发者展区也设置了神秘奖品！</p><p></p><p>* 需要注意的是，每枚徽章限领一枚，在领取后工作人员会在探索地图的对应区域进行打孔，所以一定要保存好自己的徽章哦～</p><p></p><p></p><h3>限时挑战</h3><p></p><p></p><p>除了集徽章、赢扭蛋的主线任务，开发者展区还针对开发者大讲堂和挑战俱乐部两大体验区设置了限时活动。作为整个开发者展区的重头戏，开发者大讲堂和挑战俱乐部的内容项目含金量十足。</p><p></p><p></p><h4>&nbsp;GenAI 图书漂流全新来袭！</h4><p></p><p></p><p>最新关于生成式 AI 入门与实战的书籍，译者将会亲临现场！针对这场读书分享活动，开发者展区设计了“送你一本书”图书漂流活动，在会场的每一个角落，都可能藏有图书兑换券！找到它，你就可以在开发者市集根据“异步社区”展台指引领取书籍！</p><p></p><p><img src="https://static001.geekbang.org/infoq/6e/6e456a286d42fa285c1977fedfdfe041.webp" /></p><p></p><p>与此同时，展区还特意安排了送书福利官，他们可能是大会的讲师、现场的嘉宾、展位的工作人员...... 与他们深度交流，你不仅能收获丰富的技术见解，还有机会获得图书兑换券，让智慧的火花在交流中迸发！</p><p></p><p>* 请注意，图书券限量发放，请不要放过每一个角落，并积极与峰会现场的伙伴展开交流。</p><p></p><p></p><h4>&nbsp;胜者为王！挑战俱乐部实力打榜</h4><p></p><p></p><p>来峰会史上最大的 Hands on 区域——挑战俱乐部，开启一场关于速度与激情的挑战！两天中午，挑战俱乐部将准时开赛。在十场定制实验中，你可以感受 GenAI 时代全新构建体验，上手亚马逊云科技的最新技术产品，自动驾驶竞速调优、智能 Agent、GenAI 快速生成应用...... 你想了解的，这里都有！</p><p></p><p>挑战俱乐部还设置了独特的积分排行榜，只要在单场实验中进入前十名，即可收获奖品与积分，而参与多场实验并累积最高分的开发者，将成为这场开发者“鱿鱼游戏”的胜者，收获“Top Fighter”称号及神秘大奖！</p><p>亚马逊云科技还携手 Peet's Coffee 皮爷咖啡，将 Serverlesspresso 带到了挑战俱乐部 Hands-on Zone！</p><p></p><p>开发者可以在现场直接通过代码拖拽的方式，直观感受serverless 架构带来的开发效率，体验“地球最IN的咖啡点单”。在峰会现场挑战俱乐部完成挑战的开发者，将直接获得一张免费咖啡券，享受⼀杯由 serverless 系统下单的专属咖啡。</p><p></p><p>要注意，咖啡数量有限，先到先得！</p><p></p><p></p><h3>为开发者量身定制！三大路线等你来选！</h3><p></p><p></p><p>为了便于开发者交流和学习，我们在这里也为大家整理了不同的推荐打卡路线：</p><p></p><p>【初次试炼】</p><p>适用于对亚马逊云科技感兴趣，但是了解较少的开发者。通过这条路线，你可以在技术分享中快速了解亚马逊云科技的技术产品，在基础实验中小试牛刀！</p><p></p><p>&nbsp;【进阶之路】</p><p>适用于对亚马逊云科技有一定了解、且使用过部分产品的开发者，以及培训与认证初级玩家。走出“新手村”！这条路线会带你挑战难度更高的实验项目，在体验中成长，在实践中进阶！</p><p></p><p>【炉火纯青】</p><p>对亚马逊云科技非常了解、使用过大部分产品的开发者，以及培训与认证资深玩家。你就是我们最需要的“高端玩家”！现场“拷问”亚马逊云科技 Heros、挑战最高难度实验，与亚马逊云科技共探未来！</p><p></p><p>在峰会现场，每位开发者都可以领取到详细的路线内容，相信通过这些路线快速了解亚马逊云科技的技术产品，每个人都能在分享交流与动手体验中收获成长。</p><p></p><p><img src="https://static001.geekbang.org/infoq/21/215184b20d9af504f3a444fc0a224f26.gif" /></p><p></p><h2>时间有限，赶快报名吧！</h2><p></p><p></p><p>以上就是《开发者展区游玩指南》的全部内容，在了解了开发者展区的玩法后，是不是更想去到现场“大展身手”了？这场属于开发者的狂欢即将开始，5 月 29-30 日，上海·世博中心，感受生成式 AI 时代的全新构建体验，请扫描下方二维码，即刻报名！</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d148347a613bb7c5aa037a95214e4ea5.webp" /></p><p></p><p>InfoQ 相信，无论你是技术新手还是资深开发者，都能在亚马逊云科技中国峰会找到属于自己的乐趣，收获成长与感动。现在出发！让我们在亚马逊云科技中国峰会相遇，共同开启一段难忘的科技之旅！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/GdVpT8LwFv9dooKjvzma</id>
            <title>13分钟颠覆传统电脑！微软Copilot+ PC 抢装GPT-4o、叫板苹果，网友不买账：用大炮打蚊子</title>
            <link>https://www.infoq.cn/article/GdVpT8LwFv9dooKjvzma</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/GdVpT8LwFv9dooKjvzma</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 May 2024 10:47:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: PC, 人工智能, Copilot+, Windows
<br>
<br>
总结: 微软推出了专为AI设计的新型Windows PC，称为Copilot+ PC，拥有强大的AI算力和全天电池续航时间，能够实现快速访问文件、实时生成图像、实时翻译字幕等功能。用户可以通过Recall功能快速定位文件和网页，进行近乎实时的图像编辑，以及实时字幕转换。Copilot+ PC的推出意味着PC将加速人工智能创新，为用户提供更丰富的AI体验。 </div>
                        <hr>
                    
                    <p>整理&nbsp;|&nbsp;华卫</p><p></p><p>“我们正处于一个转折点，PC将加速人工智能创新。只有当云和设备协同工作时，才能实现最丰富的&nbsp;AI&nbsp;体验。”现在，微软似乎正迫切希望将生成式&nbsp;AI&nbsp;带到&nbsp;Windows以及&nbsp;PC&nbsp;运行端的最前沿。</p><p></p><p>5&nbsp;月&nbsp;20&nbsp;日，&nbsp;微软宣布推出专为&nbsp;AI&nbsp;设计的新型&nbsp;Windows&nbsp;PC，称为&nbsp;Copilot+&nbsp;PC。全程介绍共13分钟，根据微软的说法，Copilot+&nbsp;PC&nbsp;拥有强大的新芯片，硬件&nbsp;AI&nbsp;算力达40+&nbsp;TOPS、具有全天电池续航时间并能“访问最先进的&nbsp;AI&nbsp;模型”，能够完成任何其他&nbsp;PC&nbsp;无法完成的事情，如快速访问任何文件及网页、实时生成和优化图像、实时翻译字幕等一系列生成式&nbsp;AI&nbsp;驱动的功能。</p><p></p><p>“Copilot+&nbsp;PC&nbsp;是有史以来最快、最智能的&nbsp;Windows&nbsp;PC。”微软声称，搭载&nbsp;Copilot+&nbsp;PC&nbsp;的Surface&nbsp;Pro，在持续的多线程性能方面比苹果M3的&nbsp;15&nbsp;英寸&nbsp;MacBook&nbsp;Air&nbsp;高出&nbsp;58%，同时还提供更长的全天电池续航时间，本地视频播放的电池电量比MacBook&nbsp;Air&nbsp;高出20%，一次充电可提供长达&nbsp;22&nbsp;小时的本地视频播放或&nbsp;15&nbsp;小时的网页浏览。</p><p></p><p>现在微软已对外开启&nbsp;Copilot+&nbsp;PC&nbsp;的预订，起价为&nbsp;999&nbsp;美元（折合人民币约为7229元），6&nbsp;月&nbsp;18&nbsp;日开始供货。据悉，首批&nbsp;Copilot+&nbsp;PC&nbsp;将配备高通的&nbsp;Snapdragon&nbsp;X&nbsp;Elite&nbsp;和&nbsp;Plus&nbsp;芯片，并采用定制的高通&nbsp;Oryon™&nbsp;CPU，英特尔和AMD的版本会随后跟上。</p><p></p><h2>提供大量快速、实时的&nbsp;AI&nbsp;功能</h2><p></p><p></p><h2>网友：大象枪打蚊子？</h2><p></p><p>Copilot+&nbsp;PC&nbsp;利用强大的处理器和多个最先进的&nbsp;AI&nbsp;模型（包括微软的几个世界级&nbsp;SLM）来解锁新的功能，引入了可以直接在设备上本地运行的体验，以消除以前对延迟、成本甚至隐私等方面的限制，帮助提高沟通、工作效率和创造力。&nbsp;</p><p></p><p>快速定位到任何文件及网页</p><p></p><p>首先是为&nbsp;PC&nbsp;用户提供“拥有照相记忆”式体验的&nbsp;Recall，该功能可以“记住”用户几周甚至几个月前在电脑上访问或操作过的所有内容和应用程序，将允许&nbsp;Windows&nbsp;11&nbsp;用户使用记住的任何提示，都可快速直观地检索到要查找的内容。微软表示，Recall可以在颜色、图像等之间建立关联，让用户在PC上以自然语言搜索几乎任何内容。</p><p></p><p>为了发挥作用，Recall&nbsp;会记录用户在&nbsp;PC&nbsp;上执行的所有操作，包括应用程序中的活动、实时会议中的通信以及访问用于研究的网站。据介绍，Recall&nbsp;使用&nbsp;Copilot+&nbsp;PC&nbsp;高级处理功能，每隔几秒钟拍摄一次活动屏幕的图像，这些快照会被加密并保存在&nbsp;PC&nbsp;的硬盘上。用户可以使用&nbsp;Recall&nbsp;访问特定时间段的快照，为他们正在搜索的事件或时刻提供上下文。</p><p></p><p>尽管进行了加密和本地存储，但新功能依然涉及到某些&nbsp;Windows&nbsp;用户的隐私问题。对此，微软声称，与Recall相关的所有用户数据都是保密的，并且都在设备上，不会用于训练AI模型。“用户可以删除单个快照、调整和删除时间范围，或者直接从“系统托盘”暂停该任务，还可以过滤应用程序和网站，使其免于保存。”</p><p></p><p>而对于该功能，有网友评价道，&nbsp;“这似乎就像用大象枪射击蚊子，微软是否根本无法弄清楚如何进行良好的&nbsp;Windows&nbsp;搜索，所以找&nbsp;AI&nbsp;来为他们做这件事。”</p><p></p><p>近乎实时的图像编辑</p><p></p><p>现在&nbsp;Windows&nbsp;中的&nbsp;AI&nbsp;比以往任何时候都多，其中一些仅在新的&nbsp;Copilot+&nbsp;PC&nbsp;上。Copilot&nbsp;现在可以分析图像，为用户提供创意构图的想法。在&nbsp;Copilot+&nbsp;PC&nbsp;上，用户可以免费、快速地生成无穷无尽的图像，并能够根据自己的喜好微调图像。</p><p></p><p>一项名为&nbsp;Cocreator&nbsp;的功能，能够将墨迹笔触与文本提示相结合，以近乎实时的方式生成新图像，用户还可以要求&nbsp;AI&nbsp;模型按照他们正在绘制的内容来更改或重新设计图像。</p><p></p><p>Cocreator&nbsp;还将照片编辑和图像创建提升到一个新的水平。借助“重新设置图像样式”功能，用户可以结合图像生成和照片编辑进行重新构想，预设样式可用于更改背景、前景或完整图像，以创建全新的图像；还可以快速启动下一个创意项目，并使用照片中的图像创建器获取视觉灵感。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/53/534c951e0a40e8084c660336a049312b.png" /></p><p></p><p></p><p>40&nbsp;种语言的实时字幕转换</p><p></p><p>由&nbsp;NPU&nbsp;提供支持的&nbsp;Live&nbsp;Captions&nbsp;可在所有&nbsp;Copilot+&nbsp;PC&nbsp;上使用，支持约40种语言的实时字幕翻译，可以将电脑中任何应用程序或视频平台中的任何实时或预先录制的音频，转换为用户选择的语言字幕体验。并且，它还允许用户使用转录和翻译语音的人工智能功能搜索他们参加的电话会议和观看的视频。</p><p></p><p>此外，Copilot+&nbsp;PC&nbsp;还和主流应用程序合作，利用&nbsp;NPU&nbsp;的强大功能来提供新的创新&nbsp;AI&nbsp;体验。&nbsp;在&nbsp;Windows&nbsp;Studio&nbsp;Effects&nbsp;中，“快速设置”选项可通过自动调整视频通话的图像来清理视频，以缓解光线不足或添加滤镜。Adobe的旗舰应用程序也即将登陆Copilot+&nbsp;PC，包括Photoshop、Lightroom和Express。使用完全通过&nbsp;NPU&nbsp;在设备上运行的&nbsp;AI&nbsp;功能，可以对文档进行更快、更智能的注释。DaVinci&nbsp;Resolve&nbsp;Studio中使用NPU加速的Magic&nbsp;Mask，轻松将视觉效果应用于物体和人物。</p><p></p><h2>“Copilot&nbsp;+&nbsp;PC”时代将到来？</h2><p></p><p>“第一波&nbsp;Copilot+&nbsp;PC&nbsp;只是一个开始。在过去一年里，我们看到了云上人工智能的惊人创新速度，Copilot能够做我们做梦都想不到的事情。现在，我们开始了设备上&nbsp;AI&nbsp;创新的新篇章，以&nbsp;AI&nbsp;为中心，完全重新构想了整个&nbsp;PC——从芯片到操作系统、应用层到云，这标志着&nbsp;Window&nbsp;平台几十年来最重大的变化。&nbsp;”</p><p></p><p>据介绍，&nbsp;Copilot+&nbsp;PC&nbsp;连接到&nbsp;Azure&nbsp;云中运行的大型语言模型&nbsp;（LLM）&nbsp;与本地的小型语言模型&nbsp;（SLM）&nbsp;，可以实现“前所未有的性能水平”，其在运行&nbsp;AI&nbsp;工作负载时的功能增强了&nbsp;20&nbsp;倍，效率提高了&nbsp;100&nbsp;倍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c8/c8521756ebf83e671c9fc9bd9e563e1a.png" /></p><p></p><p></p><p>而且，每台&nbsp;Copilot+&nbsp;PC&nbsp;都配备了强大的&nbsp;AI&nbsp;代理，只需使用&nbsp;Copilot&nbsp;键即可访问。在接下来的几周内，微软将从合作伙伴&nbsp;OpenAI&nbsp;那里获得包括&nbsp;GPT-4o&nbsp;在内的最新模型，来支持自然语音交互。</p><p></p><p>目前，为Recall和Super&nbsp;Resolution等功能提供动力的是Windows&nbsp;Copilot&nbsp;Runtime，它由~40个生成式AI模型组成，微软将其描述为Windows的“新应用层”。Windows&nbsp;Copilot&nbsp;Runtime&nbsp;是一个基于向量的系统，可与语义索引（单个&nbsp;Copilot+&nbsp;PC&nbsp;的本地系统）配合使用，使生成式&nbsp;AI&nbsp;驱动的应用程序（包括第三方应用程序）无需互联网连接即可运行。</p><p></p><p>微软表示，TikTok&nbsp;所有者字节跳动推出的流行视频编辑器&nbsp;CapCut&nbsp;也将使用&nbsp;Windows&nbsp;Copilot&nbsp;Runtime来加速其&nbsp;AI&nbsp;功能，如结合&nbsp;Copilot+&nbsp;PC&nbsp;提供的可快速去除任何视频片段背景的自动剪切功能体验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/dc/dc05371f721745b402f0a7a635491ff9.png" /></p><p></p><p></p><p>除微软的&nbsp;Surface&nbsp;Pro和Surface&nbsp;Laptop&nbsp;外，其&nbsp;OEM&nbsp;合作伙伴宏碁、华硕、戴尔、惠普、联想和三星也将推出Copilot&nbsp;+&nbsp;PC体验，首批型号都已公布。Surface&nbsp;Pro&nbsp;的起价为&nbsp;1000&nbsp;美元，配备&nbsp;OLED&nbsp;显示屏和&nbsp;Snapdragon&nbsp;X&nbsp;Elite&nbsp;芯片、16GB&nbsp;RAM&nbsp;和&nbsp;512GB&nbsp;SSD&nbsp;的版本售&nbsp;1500&nbsp;美元，而同样搭载&nbsp;X&nbsp;Elite&nbsp;芯片的&nbsp;Surface&nbsp;Laptop起价为&nbsp;1299&nbsp;美元。</p><p></p><p>在微软对新款Surface设备的演示和基准测试中，Surface&nbsp;Pro与&nbsp;MacBook&nbsp;Air&nbsp;进行了多项比较，都名列前茅。近年来，Windows&nbsp;PC&nbsp;一直难以跟上苹果芯片的步伐，但在得到生成式&nbsp;AI&nbsp;技术以及高通的芯片支持后，现在似乎正迎头赶上来。</p><p></p><p>参考链接：</p><p><a href="https://www.macrumors.com/2024/05/20/microsoft-ai-windows-pcs/">https://www.macrumors.com/2024/05/20/microsoft-ai-windows-pcs/</a>"</p><p><a href="https://techcrunch.com/2024/05/20/microsoft-build-2024-windows-ai-operating-system-copilot-plus-pcs/">https://techcrunch.com/2024/05/20/microsoft-build-2024-windows-ai-operating-system-copilot-plus-pcs/</a>"</p><p><a href="https://blogs.microsoft.com/blog/2024/05/20/introducing-copilot-pcs/">https://blogs.microsoft.com/blog/2024/05/20/introducing-copilot-pcs/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/CmqtsoF4PPsNVChj0IlC</id>
            <title>面壁智能发布最强端侧多模态模型：超越Gemini Pro 、GPT-4V，图像编码快150倍！</title>
            <link>https://www.infoq.cn/article/CmqtsoF4PPsNVChj0IlC</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/CmqtsoF4PPsNVChj0IlC</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 May 2024 06:21:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 端侧多模态模型, OCR 能力, 多语言支持, 系统级多模态加速
<br>
<br>
总结: 5月20日，面壁智能小钢炮 MiniCPM 系列推出端侧多模态模型MiniCPM-Llama3-V 2.5并开源，具有超越其他模型的OCR能力和支持30+多种语言的特性。此外，该模型还实现了首次端侧系统级多模态加速。 </div>
                        <hr>
                    
                    <p>5月20日，面壁智能小钢炮 MiniCPM 系列推出端侧多模态模型MiniCPM-Llama3-V 2.5并开源。据悉，该模型且支持 30+ 多种语言，并且具有以下特性：</p><p>&nbsp;</p><p>最强端侧多模态综合性能：超越多模态巨无霸 Gemini Pro 、GPT-4V；OCR 能力 SOTA！9 倍像素更清晰，难图长图长文本精准识别；图像编码快 150 倍！首次端侧系统级多模态加速。</p><p>&nbsp;</p><p>MiniCPM-Llama3-V 2.5 开源地址：</p><p><a href="https://github.com/OpenBMB/MiniCPM-V">https://github.com/OpenBMB/MiniCPM-V</a>"</p><p>&nbsp;</p><p>&nbsp;MiniCPM 系列开源地址：</p><p>&nbsp;<a href="https://github.com/OpenBMB/MiniCPM">https://github.com/OpenBMB/MiniCPM</a>"</p><p>&nbsp;</p><p>Hugging Face 下载地址：</p><p><a href="https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5">https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5</a>"</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>8 B 端侧模型，超越 GPT-4V、Gemini Pro</h2><p></p><p>&nbsp;</p><p>MiniCPM-Llama3-V 2.5 以 8B 端侧模型参数量级，贡献了惊艳的 &nbsp;OCR（光学字符识别）SOTA 成绩，以及端侧模型中的最佳多模态综合成绩与幻觉能力水平。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6c/6cefddef794310fd72a4fbb9cb821728.png" /></p><p></p><p>模型雷达图</p><p>&nbsp;</p><p>在综合评测权威平台 OpenCompass 上，MiniCPM-Llama3-V 2.5 以小博大，综合性能超越多模态“巨无霸” GPT-4V 和 Gemini Pro。</p><p>&nbsp;</p><p>OCR（光学字符识别）是多模态大模型最重要的能力之一，也是考察多模态识别与推理能力的硬核指标。新一代 MiniCPM-Llama3-V 2.5 &nbsp;在 OCR 综合能⼒权威榜单 OCRBench 上，越级超越了 GPT-4o、GPT-4V、Claude 3V Opus、Gemini Pro 等标杆模型，实现了性能 SOTA。</p><p>&nbsp;</p><p>在评估多模态大模型性能可靠性的重要指标——幻觉能力上，MiniCPM-Llama3-V 2.5 在 Object HalBench 榜单上超越了 GPT-4V 等众多模型（注：目标幻觉率应为 0）。</p><p>&nbsp;</p><p>在旨在评估多模态模型的基本现实世界空间理解能力的 RealWorldQA 榜单上，MiniCPM-Llama3-V 2.5 再次超越 GPT-4V 和 Gemini Pro，这对 8B 模型而言难能可贵。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/6b/6b111c5b4b8d7c09a28bc0464f53a9ef.jpeg" /></p><p></p><p>榜单成绩：OpenCompass | OCRBench | Object HalBench | RealWorldQA</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>快 150 倍！首次端侧系统级多模态加速</h2><p></p><p>&nbsp;</p><p>面壁智能首次进行端侧系统加速，MiniCPM-Llama3-V 2.5 目前已可以高效部署在手机端。</p><p>&nbsp;</p><p>在图像编码方面，面壁首次整合 NPU 和 CPU 加速框架，并结合显存管理、编译优化技术，在 MiniCPM-Llama3-V 2.5 图像编码方面实现了 150 倍加速提升。</p><p>&nbsp;</p><p>在语言模型推理方面，目前开源社区的报告结果中，Llama 3 语言模型在手机端侧的解码速度在 0.5 token/s 上下，相比之下，多模态大模型的端侧运行面临着更大的效率挑战，经过 CPU、编译优化、显存管理等优化方式，将 MiniCPM-Llama3-V 2.5 在手机端的语言解码速度提升到 3-4 token/s。</p><p>&nbsp;</p><p>有别于常见的中英双语模型，MiniCPM-Llama3-V2.5 可支持 30+ 多种语言，包括德语、法语、西班牙语、意大利语、俄语等主流语言，基本覆盖一带一路国家。</p><p>&nbsp;</p><p>基于自研的跨语言泛化技术，仅通过少量翻译的多模态数据的指令微调，就可对多语言多模态对话性能高效泛化。</p><p><img src="https://static001.geekbang.org/infoq/ef/ef7d51f983e0e6c6777cb6e95f765443.png" /></p><p>多语言版本 LLaVABench 评测结果</p><p></p><h2>9 倍像素更清晰，难图长图长文本精准识别</h2><p></p><p>&nbsp;</p><p>OCR 技术进一步打磨，复杂推理与多模态识别能力再进化，MiniCPM-Llama3-V 2.5 对于难图、长图、长文本的精准识别，再度带来出众表现。</p><p>&nbsp;</p><p>面壁自研了高清图像高效编码技术，可以高效编码及无损识别 180 万高清像素图片，并且支持任意长宽比，包括 1:9 极限比例，突破了传统技术仅能识别 20 万像素小图的瓶颈。</p><p>&nbsp;</p><p>另外，MiniCPM-Llama3-V 2.5 在复杂推理能力上进一步突破：可更好地深入洞察图像，在更复杂、更接近人类的水平上进行思考和解决问题。该模型不仅能理解单一文本或图像等模态信息，还能跨越不同模态间的综合信息，做出更准确和深入的分析。</p><p>&nbsp;</p><p>比如，给定一张充满繁密字迹的建筑风景图，人眼难以辨别，但 MiniCPM-Llama3-V 2.5 能够一眼看懂其中的《三体》主题，还能正确推理出这些建筑是为了纪念《三体》及其对中国科幻文学的贡献而设计：</p><p></p><p><img src="https://static001.geekbang.org/infoq/da/da02c74f80c2103e8c3cea19203903d6.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/8b/8b7730f320fa4310a89ea96be15892fb.png" /></p><p></p><p>&nbsp;</p><p>把同样的问题抛给GPT-4 V ，结果并不理想：</p><p></p><p><img src="https://static001.geekbang.org/infoq/84/84b52f54f4adb65f79995e6eb95858d5.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>另外，识别包含复杂逻辑的流程图是多模态模型推理能力的直观体现，MiniCPM-Llama3-V 2.5 不仅能够看懂流程图中不同模块的文字、箭头之间的空间位置和复杂逻辑关系，还能给出清晰易懂的解释说明：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/64/6479ab28998fae045f3b18437e463dfe.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/e9/e98652c6def8a66d38b3370559c87a89.png" /></p><p></p><p>&nbsp;</p><p>全文OCR能力方面，输入一张手机拍摄的火车票，MiniCPM-Llama3-V 2.5 也能准确提取信息，给出无误的"json"格式输出：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/81/817cfe9439bae162fb57590646701056.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/GF4Jqtkgho4EhcsoYFLF</id>
            <title>阿里通义千问GPT-4级主力模型降价97%，1 块钱200万 tokens</title>
            <link>https://www.infoq.cn/article/GF4Jqtkgho4EhcsoYFLF</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/GF4Jqtkgho4EhcsoYFLF</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 May 2024 06:18:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 阿里云, 通义千问, GPT-4, Qwen-Long
<br>
<br>
总结: 阿里云推出通义千问GPT-4级主力模型Qwen-Long，降价幅度高达97%，使得大模型应用进入密集探索期，推动AI应用爆发。通过公共云+API方式，企业可以以更低的成本和更高的性能使用大模型，实现多模型调用和数据安全保障。 </div>
                        <hr>
                    
                    <p>5月21日，阿里云抛出重磅炸弹：通义千问GPT-4级主力模型Qwen-Long，API输入价格从0.02元/千tokens降至0.0005元/千tokens，直降97%。这意味着，1块钱可以买200万tokens，相当于5本《新华字典》的文字量。这款模型最高支持1千万tokens长文本输入，降价后约为GPT-4价格的1/400，击穿全球底价。</p><p></p><p><img src="https://static001.geekbang.org/infoq/db/db5368ebb70e7b548c0326d5516f6bf9.png" /></p><p>&nbsp;</p><p>Qwen-Long是通义千问的长文本增强版模型，性能对标GPT-4，上下文长度最高达1千万。除了输入价格降至0.0005元/千tokens，Qwen-Long输出价格也直降90%至0.002元/千tokens。</p><p>&nbsp;</p><p>相比之下，国内外厂商GPT-4、Gemini1.5 Pro、Claude 3 Sonnet及Ernie-4.0每千tokens输入价格分别为0.22元、0.025元、0.022元及0.12元，均远高于Qwen-long。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/a0/a045c7a220a0cb5a9daed8f8dffaa5af.png" /></p><p>&nbsp;</p><p>通义千问本次降价共覆盖9款商业化及开源系列模型：</p><p>&nbsp;</p><p>通义千问商业化模型：Qwen-Turbo、Owen-Plus、Qwen-Long、Qwen-Max；通义千问开源模型：Qwen1.5-7B、Qwen1.5-14B、Qwen1.5-32B、Qwen1.5-72B、Qwen1.5-110B。</p><p></p><p>其中，不久前发布的通义千问旗舰款大模型Qwen-Max，API输入价格降至0.04元/千tokens，降幅达67％。Qwen-Max 在权威基准OpenCompass上性能追平GPT-4-Turbo，并在大模型竞技场Chatbot Arena中跻身全球前15。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/43/43373d17d98dab573eed782cd21c8b9c.png" /></p><p>&nbsp;</p><p>&nbsp;</p><p>业界普遍认为，随着大模型性能逐渐提升，AI应用创新正进入密集探索期，但推理成本过高依然是制约大模型规模化应用的关键因素。</p><p>&nbsp;</p><p></p><h2>“公共云+API”企业大模型主流应用方式</h2><p></p><p>&nbsp;</p><p>在武汉AI智领者峰会现场，阿里云智能集团资深副总裁、公共云事业部总裁刘伟光表示：“作为中国第一大云计算公司，阿里云这次大幅降低大模型推理价格，就是希望加速AI应用的爆发。我们预计未来大模型API的调用量会有成千上万倍的增长。”</p><p>&nbsp;</p><p>刘伟光认为，不管是开源模型还是商业化模型，公共云+API将成为企业使用大模型的主流方式。</p><p>&nbsp;</p><p>首先，公共云的技术红利和规模效应，带来巨大的成本和性能优势。</p><p>&nbsp;</p><p>刘伟光介绍，阿里云可以从模型自身和AI基础设施两个层面不断优化，追求极致的推理成本和性能。阿里云基于自研的异构芯片互联、高性能网络HPN7.0、高性能存储CPFS、人工智能平台PAI等核心技术和产品，构建了极致弹性的AI算力调度系统，结合百炼分布式推理加速引擎，大幅压缩了模型推理成本，并加快模型推理速度。</p><p>&nbsp;</p><p>即便是同样的开源模型，在公共云上的调用价格也远远低于私有化部署。以使用Qwen-72B开源模型、每月1亿tokens用量为例，在阿里云百炼上直接调用API每月仅需600元，私有化部署的成本平均每月超1万元。</p><p>&nbsp;</p><p>其次，云上更方便进行多模型调用，并提供企业级的数据安全保障。</p><p>&nbsp;</p><p>刘伟光表示，阿里云可以为每个企业提供专属VPC环境，做到计算隔离、存储隔离、网络隔离、数据加密，充分保障数据安全。目前，阿里云已主导或深度参与10多项大模型安全相关国际国内技术标准的制定。</p><p>&nbsp;</p><p>最后，云厂商天然的开放性，能为开发者提供最丰富的模型和工具链。</p><p>&nbsp;</p><p>刘伟光表示，阿里云百炼平台上汇聚通义、百川、ChatGLM、Llama系列等上百款国内外优质模型，内置大模型定制与应用开发工具链，开发者可以便捷地测试比较不同模型，开发专属大模型，并轻松搭建RAG等应用。从选模型、调模型、搭应用到对外服务，一站式搞定。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/b1be8e2769248aa344e6d8070</id>
            <title>KubeAI大模型推理加速实践｜得物技术</title>
            <link>https://www.infoq.cn/article/b1be8e2769248aa344e6d8070</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/b1be8e2769248aa344e6d8070</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 May 2024 02:13:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 推理速度, 加速优化, 注意力计算
<br>
<br>
总结: 本文分享了在部署大模型推理集群时的经验，探讨了大模型推理速度提升的方法，介绍了一些业界内的大模型加速技术，以及大模型发展面临的挑战和优化方向。其中，注意力计算是推理过程中最耗时的部分，针对其进行速度优化可以显著提高整体推理性能。 </div>
                        <hr>
                    
                    <p>一、背景</p><p>最近我们在生产环境批量部署了大模型专用推理集群，并成功让包括70B在内的大模型推理速度提升50%，大幅缩减部署成本，稳定应用于生产环境。本文基于我们在部署大模型推理集群时的一些经验，分享一些有效提升大模型的推理速度方法。最后，我们在结尾处推荐了几个经过我们评测且表现优异的大模型推理框架。希望这些建议能帮助读者在项目中选择适合自己的推理框架。</p><p>OpenAI的科学家Hyung Won Chung在2023年的公开演讲《Large Language Models》[8]中指出，大模型的某些能力仅在达到特定规模时才能显现，可见未来大模型的参数量肯定会越来越大，这也是大模型的发展趋势。随着参数量的增加，对大模型的推理速度要求越来越高，有哪些方法可以提高大模型的推理速度或吞吐量？</p><p>首先我们将探讨大模型的加速优化方向，随后文章将依据时间线，介绍一些业界内较为经典的实用大模型加速技术，包括但不限于“FlashAttention[1]”和“PageAttention[3]”等技术。</p><p>以下为按时间顺序业界的一些经典大模型推理加速技术，本文试图为读者提供一个按时间发展顺序的大模型加速方法综述。</p><p><img src="https://static001.geekbang.org/infoq/54/548ea315bfa45754e44fbba404aeeb7d.png" /></p><p></p><p>除了上面提到的技术外，提高大模型推理速度的还有大模型的量化技术等，这里先不探讨，后面有机会，我们会单独发文章来介绍。</p><p>二、大模型发展面临的挑战</p><p>未来大模型的参数量肯定会越来越大，这也是大模型的发展趋势，对推理加速的要求会越来越高。</p><p>OpenAI在其论文《Scaling Laws for Neural Language Models》[7]中介绍了大模型的扩展规则，这些规则阐释了模型能力与其规模之间的关系。具体来说，模型的能力强烈依赖于其规模，包括模型参数的数量，数据集的大小，以及训练过程中所需的计算量。此外，OpenAI的科学家Hyung Won Chung在2023年的公开演讲《Large Language Models》[8]中指出，大模型的某些能力仅在达到特定规模时才能显现。</p><p><img src="https://static001.geekbang.org/infoq/a3/a3bd942a70296078aca91f594a349071.png" /></p><p></p><p>上图摘自Hyung Won Chung演讲中的ppt[8]。图中主要表达一个观点，随着模型规模的增大，比如GPT3到GPT4，模型的能力变的越来越强，甚至会出现新的能力。</p><p>但是随着模型的规模增大，大模型的推理速度将会逐渐降低，这是因为更多的参数量需要更多的GPU计算。推理速度的下降进一步带来更差的用户体验，因此如何对大模型推理加速变得越来越重要了。</p><p><img src="https://static001.geekbang.org/infoq/7a/7a9fe75dd98901632579dd43ac1f09d5.png" /></p><p></p><p>三、大模型推理加速的优化方向</p><p>Llama2的模型结构</p><p>我们先简单了解一下Llama 2模型系列的结构，参考自Llama 2的论文[9]。目前，像Llama系列这样的大多数生成式语言模型，主要采用了Transformer架构中的Decoder模块。在Huggingface平台上，这类模型结构通常被称作CausalLM，即因果语言模型。</p><p><img src="https://static001.geekbang.org/infoq/8a/8a3b6aff983a45c63eee6b7e0b391361.png" /></p><p></p><p>上图为Llama2大模型的结构，其中最核心的是注意力计算(Llama Attention)。这也是整个推理过程中最耗费时间的模块，后面的优化大部分都是基于Attention去实施的。为了更好的理解Llama 2大模型的结构，我们先简单对Llama2模型的整个推理过程进行拆解，不感兴趣同学可以直接跳过。</p><p>用户向模型提交Prompt后，模型首先进行的操作是预测下一个字符(Token)，并将预测出的字符添加到输入中继续进行预测。这个过程会一直持续，直到模型输出一个停止符号(STOP token)，此时预测停止，模型输出最终结果。在生成下一个字符(Token)的过程中，模型需要执行N次的Llama解码器层(Llama Decoder Layer)计算。具体来说，Llama-2-7B模型执行32次计算，而Llama-2-13B模型执行40次。Llama解码器层(Llama Decoder Layer)中最关键的计算环节是注意力(Llama Attention)的计算。大部分推理时间都消耗在Attention的计算上，因此多种优化技巧都旨在提高Attention计算的效率。</p><p>大模型推理的加速方向有哪些</p><p>从Llama 2模型的结构分析中，我们可以总结出大模型在推理计算过程中表现出以下特点：</p><p>在整个推理过程中，最耗时的部分为注意力(Attention)计算。针对Attention的计算进行速度优化，可以显著提高整体推理性能。注意力(Attention)计算过程中，键值对缓存(KV Cache)占用了大量显存资源。以13B模型为例，处理一个Prompt序列大约需要3GB额外显存，并且这部分显存会被频繁地分配和释放，产生大量碎片，如果能减少显存碎片，也能提升大模型的吞吐。推理过程GPU需要处理和计算大量的参数。7B模型拥有70亿参数，而13B模型则包含130亿参数，最新全球最强大模型DBRX更是高达1300亿参数，这需要高效地处理这些参数。这里也可以有优化空间。</p><p>针对上述三个特性，目前业界提出了多种有效的优化方法，典型如下：</p><p><img src="https://static001.geekbang.org/infoq/eb/eb4f7ca7ad5e3797a2515d083e409230.png" /></p><p></p><p>1. FlashAttention-Attention计算速度优化</p><p>FlashAttention[1]在不改变Attention算子的计算结果的前提下，提升Attention算子的计算速度。FlashAttention在各种模型和任务上展示了显著的性能提升。例如，在BERT-large、GPT-2等模型上，相比于基线实现，FlashAttention能够实现15%到3倍的端到端加速。</p><p>2. PageAttention-KV Cache显存管理优化</p><p>PageAttention[3]的目标是减少显存碎片，基于PageAttention的VLLM系统能够将流行的大型语言模型（LLM）的吞吐量提高到10倍以上，同时保持耗时分布平稳。</p><p>3. MOE-缩减推理时模型参数</p><p>MOE(Mixture of Experts)[4]目标是减少模型推理时参与计算的参数量。</p><p>实验效果：Mixtral模型在多数基准测试中表现优于Llama 2 70B模型，并且其推理速度比后者快了6倍。该模型支持多种语言，具有强大的代码生成能力，并可以细化配置以遵循具体指令，从而在MT-Bench基准测试中取得了高分。</p><p>后面我们将针对上面的每个方向详细介绍。</p><p>四、FlashAttention-Attention算子计算优化</p><p>FlashAttention先后发表了两篇论文阐述对Attention算子的优化，包括FlashAttention-1[1]与FlashAttention-2[2]，我们以FlashAttention-1[1]为例了解下他的优化原理。</p><p>我们先了解下GPU的内存分层结构，参考下图，图片来自论文FlashAttention-1[1]。</p><p><img src="https://static001.geekbang.org/infoq/e3/e35f218aafbbd8d169ec2d9605d3532e.jpeg" /></p><p></p><p>GPU的内存层次结构由三个主要部分组成：SRAM、HBM和DRAM，下面为A100GPU的参考配置。</p><p>SRAM（静态随机访问存储器）具有最快的访问速度（19TB/s），但其容量相对较小（仅20MB）。</p><p>HBM（高带宽存储器）提供较大的存储空间（40GB）和高速的数据访问（1.5TB/s）。</p><p>DRAM（动态随机访问存储器），在这里特指GPU外部的主存，容量最大（超过1TB），但访问速度最慢（12.8GB/s）。</p><p>从上述配置中可以看出，内存容量越小，处理速度就越快。</p><p><img src="https://static001.geekbang.org/infoq/1b/1b9f6204e93f189a1583b1b964ce0546.png" /></p><p></p><p>在传统的Attention计算过程中，大量的输入/输出操作都是通过访问HBM来完成的。FlashAttention算法通过优化Attention计算流程，减少了对HBM的访问次数，以提高计算效率，所以它是一种IO感知的优化算法。</p><p>下图为FlashAttention的加速方法，来自论文FlashAttention-1[1]</p><p><img src="https://static001.geekbang.org/infoq/4e/4e0389e8e85ff04408383d40ecf2bb10.jpeg" /></p><p></p><p>FlashAttention利用了一个聪明的技巧来快速且内存高效地计算注意力机制，即它通过将输入数据分块（tiling）来避免一次性处理整个巨大的注意力矩阵，这通常需要大量的内存和计算资源。想象一下，我们有一个巨大的图书馆(矩阵)，而FlashAttention的方法就像是把图书馆里的书分成几个小堆，然后每次只处理一堆书。这样，我们就不需要一次性把所有书都拿出来放在桌子上（这需要很大的桌子和很多时间）。</p><p>具体来说，FlashAttention在做矩阵计算的时候，通过将数据分块并利用GPU上的快速但容量较小的存储（SRAM）去计算，有效减少了对慢速但容量大的存储（HBM）的访问。这样不仅加快了计算速度，而且大幅减少了显存的需求。</p><p>通过减少对慢速存储的依赖，FlashAttention能够显著提高模型训练的速度，同时保持或甚至提高模型的性能。例如，让BERT-large的训练比MLPerf 1.1的记录快15%，GPT-2训练速度是HuggingFace和Megatron-LM基线的三倍，长序列领域训练速度提升至2.4倍。</p><p>下图来自huggingface 对flash attention介绍的blog[14]，可以更好的理解Flash Attention对矩阵拆分的方式。</p><p><img src="https://static001.geekbang.org/infoq/b0/b0de9c9031ff76416e0ebe42e0c4f863.png" /></p><p></p><p>既然Flash Attention可以加速计算，那么支持Flash Attention计算的框架包括都有哪些，文章后半部我们会推荐一些比较优秀的推理框架。</p><p>五、PageAttention-显存管理优化</p><p>PageAttention[3]的概念最初由VLLM的作者Woosuk Kwon提出，它也是VLLM推理框架的最主要的优化策略。Woosuk Kwon在其论文中介绍了如何通过PageAttention来解决大型语言模型（LLM）服务中的一个关键问题——在不增加延迟的情况下有效管理内存以提升吞吐量。</p><p>我们先了解下大模型在推理的情况下的内存结构分布，下图来自论文[3]。</p><p><img src="https://static001.geekbang.org/infoq/bb/bbdede18f2c3f4b05325ec44e650adb9.jpeg" /></p><p></p><p>这是一个在NVIDIA A100上服务一个拥有13B参数的大型语言模型的内存布局，13B LLM 推理显存占用分部，13B LLM的参数占用26G显存，每个请求，KV Cache会占用12G显存，随着QPS的增加，KVCache会快速上升，并且会被频繁的分配与释放，系统会产生大量的显存碎片，如果不加处理，系统就会慢慢崩掉。</p><p><img src="https://static001.geekbang.org/infoq/c2/c24d910f5dcc02c410af0aa0b38c976b.png" /></p><p></p><p>那么VLLM是如何通过PageAttention解决显存碎片的问题的呢？下图来自文章[14]，为VLLM的显存管理技术。</p><p><img src="https://static001.geekbang.org/infoq/f6/f6de234255145832df46f30827cf0ed0.png" /></p><p></p><p>PageAttention的工作原理是通过将键值缓存（KV缓存）分割成固定大小的块（或“页面”），并允许这些块在内存中非连续地存储。这种方法灵感来源于操作系统的虚拟内存和分页技术，目的是为了更灵活和高效地管理内存资源。</p><p>在传统的注意力机制中，一个请求的KV缓存需要在内存中连续存储，这会导致两个主要问题：内存碎片化和无法高效共享内存。内存碎片化限制了批处理的大小，而无法共享内存则导致重复数据，浪费宝贵的内存资源。</p><p>PageAttention通过以下步骤工作来解决这些问题：</p><p>分割KV缓存：将每个请求的KV缓存划分为多个较小的块，这些块的大小是固定的，可以根据模型和硬件的具体需求进行调整。非连续存储：与传统KV缓存块在内存中连续存储不同，PageAttention允许这些块在物理内存中非连续地分布。这样，就可以根据实际需要动态地分配和回收内存块，减少内存浪费。动态管理：通过类似于操作系统中虚拟内存管理的方式，PageAttention动态地管理这些内存块。系统可以根据当前的内存使用情况，按需分配或释放KV缓存块，从而优化内存使用。内存共享：PageAttention还支持在不同请求之间或同一个请求中的不同序列之间共享KV缓存块。这种共享是灵活的，可以基于块级别进行，进一步减少内存使用和提高效率。</p><p>通过这种方式，PageAttention允许LLM服务系统在保持相同延迟的情况下，通过减少内存浪费和提高内存共享，显著提高处理请求的吞吐量。</p><p>通过PageAttention的优化，VLLM对LLaMA 7B与13B的吞吐量提升了10倍以上，下图来自文章[11]。</p><p><img src="https://static001.geekbang.org/infoq/7f/7f87ec9b5fa68b86bd74281f58645a6b.png" /></p><p></p><p>六、MOE-缩减推理时模型参数</p><p>最近发布的全球最强开源大模型1300亿参数的DBRX，以及Mistral的8x7B开源大模型都是基于MOE架构的。为什么参数量越大的模型越要使用MOE架构呢？我们以Mistral的8x7B开源大模型为例，介绍下MOE架构在性能方面的优势。</p><p><img src="https://static001.geekbang.org/infoq/b0/b08c6ad09f22721ccc236b0ce553916f.jpeg" /></p><p></p><p>说到MOE大模型，我们先对比下普通大模型与MOE大模型在结构上的区别，参考上图。在MOE大模型中，把大模型的参数分成了8个小组外加一个路由器，每个小组我们称作专家组。当请求过来的时候，MOE大模型则先有路由器从8个专家组中选择两个，只有这两个专家组参与了计算。而对比普通大模型，则需要所有参数都参加GPU计算。</p><p>所以MOE大模型要比同等级的普通大模型推理速度快四倍左右。</p><p>我们来看下Mistral MOE的实现，Mistral MOE是由mistral.ai发布的8*7B大模型[12]，下图来自论文[12]，是其8*7B大模型的专家层的结构。</p><p><img src="https://static001.geekbang.org/infoq/09/099d8d251965d1cfcb4a39c8b79c4e9f.jpeg" /></p><p></p><p>Mixtral 8x7B是一个稀疏混合专家（Sparse Mixture of Experts, SMoE）语言模型，它基于Mistral 7B的架构，但每一层都由8个前馈块（即专家）组成。在处理每个令牌时，每层的一个路由网络会选择两个专家来处理当前状态并结合它们的输出。虽然每个令牌只与两个专家交互，但在每个时间步骤中选取的专家可以不同，因此每个令牌可以接触到47B的参数，但在推理过程中只使用13B的活跃参数。</p><p>Mixtral在多项基准测试上展示了其卓越的性能，尤其是在数学、代码生成和多语言理解方面。相比于Llama 2 70B和GPT-3.5，Mixtral在大多数评估指标上表现出类似或更优的性能。具体来说，Mixtral使用的活跃参数（13B）比Llama 2 70B（70B）少5倍，但在几乎所有类别中的表现都更好或相当。</p><p>MOE大模型实现了增加参数量的同时，推理速度并不降低，是未来大模型的发展趋势。</p><p><img src="https://static001.geekbang.org/infoq/47/47f989f8b43a06ac47ac1da7e76ef7a6.png" /></p><p></p><p>七、Tensor parallelize-张量并行</p><p>如果你有多卡GPU，可以采用张量并行进一步加速大模型的推理速度。</p><p>想象一下，你有一本非常厚的书，你想一次性复印整本书，但是你的复印机一次只能复印几页。这时，你可以把这本书分成几个部分，每个部分分别复印，最后再把所有复印好的部分按顺序拼接起来，这样就完成了整本书的复印。</p><p>在张量并行中，我们要处理的大模型就像是那本厚书，而GPU则像是复印机。因为单个GPU无法一次处理整个大模型，我们就需要把模型（在这个例子中是权重张量）分成几个部分，让不同的GPU分别处理（相当于复印书的不同部分）。在处理输入数据时，就像是把书的每一页分别复印，然后再把复印好的各个部分拼接起来，形成完整的输出结果。</p><p>这样，通过分担工作，多个GPU协同完成了一个本来单个GPU无法完成的大任务。这就是张量并行的工作方式，它让我们能够处理那些非常大的模型。</p><p><img src="https://static001.geekbang.org/infoq/23/23ed8ca8a9e06e2a403c429a81f45789.png" /></p><p></p><p>图片来自文章[13]</p><p>张量并行技术用于将大模型分布式地部署在多个GPU上。以矩阵乘法来打个比方，当输入张量与第一个权重张量进行矩阵相乘时，这个操作可以视作先将权重张量按列进行分割，接着将分割后的每列分别与输入张量相乘，然后把这些乘积结果合并。这些合并后的输出会被从GPU中导出，并聚合以形成最终的输出结果，过程上图，参考文章[13]。</p><p>八、推理框架推荐</p><p>在前文中，我们探讨了几种加速和优化技术，诸如Flash Attention、Page Attention、MOE以及张量并行技术。接下来，基于我们自身的实际操作和评估，我们将向您推荐一些当前表现较为出色的推理框架。</p><p><img src="https://static001.geekbang.org/infoq/83/83ae59e30f1000c0fe189cd1fc71bd7b.png" /></p><p></p><p>九、总结与展望</p><p>在本文中，我们深入探讨了一系列旨在提升大模型推理速度的技术和方法，包括但不限于Flash Attention、Page Attention、MOE以及张量并行技术。通过在生产环境中批量部署专用大模型推理集群，我们成功地将包括70B规模模型在内的推理速度降低了50%，稳定地应用这些技术于生产环境，从而证明了这些优化方法的有效性和实用性。</p><p>随着大型模型在各个领域的应用越来越广泛，如何有效地提升推理速度、降低推理成本成为了一项挑战。我们的实践不仅展示了目前可用的一些加速技术，还基于我们的经验，推荐了几款经过评测表现优秀的大模型推理框架。这些建议旨在帮助读者在面对众多选择时，能够挑选出最适合自己需求的推理框架。</p><p>展望未来，随着技术的不断进步和新算法的不断涌现，我们相信还会有更多的加速优化技术被开发出来，进一步推动大模型推理效率的提升。最后，我们也期待未来有机会深入探讨和介绍更多提升大模型推理速度的新技术和方法。</p><p></p><p>参考资料</p><p>[1] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness(https://arxiv.org/abs/2205.14135)</p><p>[2] FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning(https://arxiv.org/abs/2307.08691)</p><p>[3] Efficient Memory Management for Large Language Model Serving with PagedAttention(https://arxiv.org/abs/2309.06180)</p><p>[4] mixtral-of-experts(https://mistral.ai/news/mixtral-of-experts/)</p><p>[5] Mixtral of Experts(https://arxiv.org/abs/2401.04088)</p><p>[6] MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads(https://arxiv.org/pdf/2401.10774.pdf)</p><p>[7] Scaling Laws for Neural Language Models(https://arxiv.org/pdf/2001.08361.pdf)</p><p>[8] Hyung Won Chung(OpenAI), Large Language Models (in 2023) , talked at Seoul National University</p><p>[9] Llama 2: Open Foundation and Fine-Tuned Chat Models(https://arxiv.org/abs/2307.09288)</p><p>[10] Attention Is All You Need(https://arxiv.org/pdf/1706.03762.pdf)</p><p>[11] https://blog.vllm.ai/2023/06/20/vllm.html</p><p>[12] https://arxiv.org/pdf/2401.04088.pdf</p><p>[13] https://huggingface.co/docs/text-generation-inference/en/conceptual/tensor_parallelism</p><p>[14] https://huggingface.co/docs/text-generation-inference/en/conceptual/flash_attention</p><p>[15] https://blog.vllm.ai/2023/06/20/vllm.html</p><p>*文/&nbsp;linggong</p><p>本文属得物技术原创，更多精彩文章请看：<a href="https://tech.dewu.com/">得物技术</a>"</p><p>未经得物技术许可严禁转载，否则依法追究法律责任！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/uolKho9bFBB7eBzqdKO6</id>
            <title>将大模型疯狂用到军事上，这家企业创始人“疯了”？</title>
            <link>https://www.infoq.cn/article/uolKho9bFBB7eBzqdKO6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/uolKho9bFBB7eBzqdKO6</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 May 2024 01:25:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI应用于战争, Palantir, GPT-4, 人工智能平台
<br>
<br>
总结: 一家软件公司将AI产品供应给以色列国防军，与微软等企业合作在政府云中部署大型语言模型，展示了AI在军事领域的潜力。在会议上，Palantir CEO和其他高官讨论未来战争，展示了对暴力和战争的态度和看法。同时，会议也反映了人们对技术在战争中的重要性和影响的不同看法。 </div>
                        <hr>
                    
                    <p>&nbsp;</p><p></p><blockquote>讨论将AI应用于战争，这家企业“疯了”！</blockquote><p></p><p>&nbsp;</p><p>5月7号到8号，华盛顿特区举办了一场特别的大会。此次会议的主赞助商则是由美国企业家与风险资本家、政治活动家、PayPal联合创始人Peter Thiel参与创立的软件厂商Palantir，这家公司因在2019年疫情封闭期间联手移民和海关执法局（Ice）组织抗议活动而闻名。目前，Palantir正在向以色列国防军供应部分AI产品。</p><p>&nbsp;</p><p>此外，微软、谷歌、亚马逊、Groq等20多家企业也是这场大会的赞助商。</p><p>&nbsp;</p><p>当时，微软宣布已在隔离的Azure 政府绝密云中部署了 GPT-4 大型语言模型，供国防部使用。一旦该工具获得认可，五角大楼官员将能够在安全的环境中使用该技术。这个消息在当时被广泛关注，但更多主题可能被大家忽略。</p><p>&nbsp;</p><p>亲身参加了这次会议的外媒记者 Caroline Haskins 近日发文详细记录了自己的所见所闻。他说到，会议厅里满是代表美国军方及其数十家承包商的展位，其中包括博思艾伦公司，还有某家可称为“飞机软件界的Uber”的企业。</p><p>&nbsp;</p><p>“在这样的行业内部会议上，权贵们往往更加直言不讳、疏于掩饰——他们自认为身处安全空间，毕竟身边全都是朋友和同事。”Haskins 表示，“这也让我加倍好奇，他们对于加沙地带的AI驱动暴力行动，乃至于战争的未来形态究竟有何看法？”</p><p>&nbsp;</p><p>与会者们被告知，会议最大的亮点就是主厅后方房间里的一连串展板。事实上，那个房间专门展示一个主题，其余部分则由 Schmidt 本人和Palantir公司CEO Alex Karp主持的fire-breathing定基调。</p><p>&nbsp;</p><p>值得注意的是，2023年9月，《时代》周刊发布了首届全球百大AI人物，Karp 被评为全球AI领袖。在此前联名呼吁“暂停人工智能（AI）研究”的活动中，Karp反驳了公开反驳，并回怼那是因为他们自己没有AI产品。</p><p>&nbsp;</p><p>今年4月底，Palantir 推出了自己最新的生成式人工智能平台，该平台理论上可以为军事指挥官提供军事决策、命令下达、作战监控，实现优化决策流程、缩短决策时间、获得最优作战方案、保障作战质量的目标。Palantir 与美国政府的合作一直都很紧密。</p><p></p><p><img src="https://static001.geekbang.org/infoq/69/69b0109c6a73ae7620105dd76556d606.png" /></p><p></p><p>Palantir联合创始人兼CEO Alex Karp与海军上将Tony Radakin</p><p>&nbsp;</p><p>这场大会把参与者们划分成了两类：一类将战争视为经济与战略问题，另一类将战争理解为死亡问题。Haskins表示，与会的大多数人都属于前一类。</p><p>&nbsp;</p><p>“我之前就一直在关注科技企业与军方机构之间的关系，所以本来不该对在这次会议上看到或听到的任何情况感到意外。但在会议结束、离开华盛顿返回家中时，我感到自己的生命力似乎已经被这场狂暴的活动抽干了。”Haskins写道。</p><p>&nbsp;</p><p>Haskins究竟看到了什么？下面我们跟随Haskins的视角，近距离看看会议上的权贵们都说了些什么。</p><p>&nbsp;</p><p></p><h2>Palantir CEO的“高谈阔论”</h2><p></p><p>&nbsp;</p><p>跟着大群观众，我们穿过主厅去观看核心议题的小组讨论。Karp 和 Schmidt连同CIA中情局副局长Divad Cohen，以及负责战争事务的高级官员Mark Milley在这里发表了讨论。</p><p>&nbsp;</p><p>Schmidt自我介绍时麦克风意外故障，于是Cohen把自己的麦克递了过去。Schmidt开玩笑说，“中情局总能及时伸出援手。”在接下来的90分钟里，讨论就在这样的轻松氛围下进行。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a5/a5df066127e1c0978d7635fcc715e0a7.png" /></p><p></p><p>Eric Schmidt、David Cohen、Mark Milley将军、Palantir公司CEO Alex Karp以及Andrew Ross Sorkin齐聚小组讨论现场</p><p>&nbsp;</p><p>当主持人询问小组成员对于未来战争的看法时，Schmidt和Cohen都回答得相当谨慎。</p><p>&nbsp;</p><p>而向来以态度激进闻名的Karp则极度纵容暴力，而且不吝以饥渴的眼神凝视观众，希望从我们身上得到或赞许、或震惊的明确反馈。</p><p>&nbsp;</p><p>这位AI领袖首先表示，美国必须在战中“吓死我们的对手。”在谈到哈马斯去年10月7日对以色列的袭击时，他指出“如果我们美国遭遇到类似的突袭，那地球表面就会多出一个大洞。”Karp坚称，“和平活动家就是战争活动家。所以我们明显属于和平活动家。”</p><p>&nbsp;</p><p>随着Karp的加入，Milley的言辞也愈发夸张。在讨论结束时，他甚至直接将反对加沙战争的美国人称为“恐怖组织的支持者。”</p><p>&nbsp;</p><p>Schmidt同时提到了无人机和自动化技术在战争中的重要性。他本人也在低调创办自己的军用无人机公司。</p><p>&nbsp;</p><p>Cohen则敦促大家将10月7日的袭击视为对军事环境中技术升级的“重大警告”。Cohen指出，尽管以色列在防御的监控技术方面“投入了大量资金”，但仍然未能阻止垄断。“我们确实要保持一点谦逊之心。”</p><p>&nbsp;</p><p>但从多数发言者的观点来看，人们普遍比Cohen更加乐观，只是觉得系统故障属于技术问题，用更多、更新的技术就可以解决。</p><p>&nbsp;</p><p>我带着麻木的脑袋和僵硬的身体离开了小组讨论现场。</p><p>&nbsp;</p><p>我无意间听到了身边其他与会者们那冷漠的话语。他们有为这么恐怖的发言而震撼吗？完全没有，他们在讨论午餐、周末去哪玩，还有下一场小组讨论的内容。我们似乎完全生活在不同的世界里。</p><p>&nbsp;</p><p></p><h2>“我就是新时代的奥本海默”</h2><p></p><p>&nbsp;</p><p>在到处转悠了大概10分钟之后，我给手机充上电，然后向身边的另一位与会者主动打了招呼。这是位50多岁的男性，我问他对这场小组讨论怎么看，他温和地微笑回应，说Milley的二战观点很“有趣”。</p><p>&nbsp;</p><p>他问道，“你看过‘奥本海默’吗？”</p><p>&nbsp;</p><p>我说，“没有，但我读过《原子弹的诞生（The Making of the Atomic Bomb）》。”</p><p>&nbsp;</p><p>我以为他想聊聊那帮制造战争武器的家伙有多傲慢。但相反，他说他在洛斯阿拉莫斯实验室从事核武器研究。他把手伸进背包，给我拿了几根带着阿拉莫斯logo的钢笔的贴纸。</p><p>&nbsp;</p><p>他没有透过太多关于工作的细节，但给我看了几眼他花钱租来的豪车。聊了几分钟后，他开始收拾东西，并突然笑着说，“我刚刚想到一件事，我就是新时代的奥本海默！”</p><p>&nbsp;</p><p>看着他返回阿拉莫斯实验室的展位，我勉强挤出礼貌的微笑。</p><p>&nbsp;</p><p>在整场会议期间，我游走于不同的展位，并最终遇到了两个校友。在国家安全局的展位上，一名年轻女士告诉我，安全局岗位有着良好的“工作生活平衡”。我还参观了Palantir的招聘展位，那里的员工Elizabeth Watts告诉我，愿意在Palantir工作的人首先要能承受Karp的言论。“想要捍卫西方民主国家的人、对国家安全感兴趣的人都知道，这个世界并不是非黑即白。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/b8/b81a612ae31ec6032bcead3b85d7de7c.png" /></p><p></p><p>Palantir公司为士兵们提供一款新型增强现实工具</p><p>&nbsp;</p><p>在Palantir巨大的主展位中，我试着戴上VR头显来测试Palantir为士兵们提供的新型增强现实工具。有人告诉我，我可以在正常观察身边环境的同时指挥卡车或者无人机。但在戴上头显后，我发现自己的视野很不稳定而且会失焦。</p><p>&nbsp;</p><p>一位Palantir员工向我解释说，现场有很多人都在试戴这款头显。要想确保视野清晰，头显必须完美贴合使用者的头部和眼睛。但他没有主动帮助我调整头显，所以我的高科技作战视野仍然一片模糊。</p><p>&nbsp;</p><p>活动第一天晚间，Palantir举办了一场提供免费酒水的社交活动。现场只提供两款IPA精酿啤酒，我选择的叫“the Corruption”，可以说是这辈子我喝过的最难喝的酒。之后我跟一位名叫Sata的加拿大男子攀谈，他看上去也就20多岁。他说自己是Palantir的投资者，所以我当然好奇他这么年轻怎么会有闲钱投资。</p><p>&nbsp;</p><p>他说“我出了车祸”。在正常治疗之外，他把剩下的钱拿去投资。总之他的投资比较成功，但这趟会务出行却是亏的。</p><p>&nbsp;</p><p></p><h2>Palantir 地图工具：帮助选择轰炸目标</h2><p></p><p>&nbsp;</p><p>我还参加了Palantir展位上名叫“缓解平民伤害”的小组讨论。这场讨论由两名“隐私与公民自由工程师”主持，这是一对言语无味的年轻男女。他们用各种委婉的说法来形容轰炸和死亡。其中的女士称，Palantir的Gaia地图工具能帮助用户“处理目标筛选流程”以挑选“感兴趣的目标”，也就是帮助士兵选择具体要轰炸哪些地点。</p><p>&nbsp;</p><p>在交互式地图上点击了几个选项之后，目标区域就亮起了耀眼的蓝色斑点。她解释称，这些亮点代表的是医院和学校等民用区域。以往民用地点大多采用文字描述，但阅读起来费时费力。因此，Gaia利用大语言模型（例如ChatGPT）来筛选信息并加以简化。从本质上讲，选择轰炸目标的士兵们可以借此快速了解哪里聚集着大量儿童或者病人。</p><p>&nbsp;</p><p>我后来问这位工程师，“假设你在一个存在大量平区域的地方行动，比如加沙地带，Palantir 是否会阻止你在平民地点‘指定目标’？”</p><p>&nbsp;</p><p>这位女士直接回答不会，“一切由最终用户决定。”</p><p>&nbsp;</p><p>接下来我看到了一个小小的沉浸式展位，在灰色墙体上张贴着关注普通民众受到战争影响的海报。这里就是国际红十字会的地盘。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/2d/2d56653b067866ca220de612c785873e.png" /></p><p></p><p>国际红十字会的展位</p><p>&nbsp;</p><p>穿过一道看似平常的出口，我又迈进一处“紧急避难所”。这是一处为冲突地区的年轻民众提供庇护的样板房间，里面有一张小沙发，上面摆着一只摊开的睡袋，角落里还放着儿童玩具。黄色字样警告称居民应“留在指定的安全区域”。厨房桌子上的收音机似乎正在播放新闻，但信号不太稳定。</p><p>&nbsp;</p><p>这处避难所虽然规模不大，但在这场堪称军工复合体狂欢的会议上却是如此引人瞩目。终于有一个地方在呼吁人们关注战争的受害者，至少应该注意到他们。</p><p>&nbsp;</p><p>走出避难所，我与国际红十字会员工Thomas Glass聊了一会。他很专注、很投入，但看起来也很疲惫。他说他刚刚花了几周时间，在加沙南部建立了一处野战医院并搭设起了公共厨房。</p><p>&nbsp;</p><p>我问与会者们对红十字的展位有何反应。Glass表示，他遇到的大多数人都持开放态度，但也有些人问红十字跟到军工展会来干什么。也许更可悲的是，这么问的人并不是在刻意挑衅，他们是真的无法理解。</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>目前，五角大楼官员已经接受了在整个部门内使用生成式AI工具。五角大楼官员将生成式AI视为一种可以在整个部门内使用的工具，具体包括增强后台功能乃至辅助一线作战人员等。</p><p>&nbsp;</p><p>然而，仍有一些安全问题需要解决。他们即将发布一项新指令，以指导该部门运用生成式AI、特别是大语言模型方面。“大语言模型很强，堪称生产力的巨大驱动引擎，让我们能够完成更多工作。但作为一项新技术……在我看来，目前仍处于AI泡沫阶段。放眼整个行业，每个人都在积极竞争，希望尽快打造出最出色的大语言模型。在这方面，我们仍存在一些差距。所以最重要的就是不能直接借用外部大语言模型、指望登录上去并输入我们的数据就能获得符合预期的AI回复。”陆军数据、工程与软件副助理秘书Jennifer Swanson表示，</p><p>&nbsp;</p><p>她指出，这样做可能会导致陆军敏感数据通过互联网及对手也能访问到的训练模型而泄露至公共领域。“这将非常危险。因此，我们正在研究内部应对之策，包括在影响等级IL5和IL6范围之内，可以采取哪些行动。”</p><p>&nbsp;</p><p>一位美军方人士表示，“我们的总体目标就是让大模型以数据功能的形式存在，借此为军方的供应商提供辅助。我们会先以特定用例试水，比如为美国政府构建一套模型的话应该是什么样子、要为军方总部构建的模型又应该是什么样子，以及这些模型之间是什么关系等等。这一切将成为推动后续合作及发展的起点。”</p><p>&nbsp;</p><p>Garciga透露，他们正在对指南备忘录中的内容做初步整理，目前内容主要集中在数据保护方面，包括设想当中的护栏应当具备哪些功能、政府及产业之间又将以何种方式围绕生成式AI开展交互等。</p><p>&nbsp;</p><p>显然，即将出台的新政策将高度关注安全问题。但这种安全只是系统的安全，更广泛的人类范畴内，可能并不在讨论范围里。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.theguardian.com/technology/article/2024/may/17/ai-weapons-palantir-war-technology">https://www.theguardian.com/technology/article/2024/may/17/ai-weapons-palantir-war-technology</a>"</p><p><a href="https://defensescoop.com/2024/05/09/army-policy-guidance-use-large-language-models-llm/">https://defensescoop.com/2024/05/09/army-policy-guidance-use-large-language-models-llm/</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/GdkidIFChUxVslICMamx</id>
            <title>奥特曼被吓坏了：两篇小作文接连否认“封口”离职条款，但没人相信他了</title>
            <link>https://www.infoq.cn/article/GdkidIFChUxVslICMamx</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/GdkidIFChUxVslICMamx</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 May 2024 10:39:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, 离职员工, AI安全, ChatGPT
<br>
<br>
总结: OpenAI公司因为离职员工的问题和对AI安全的关注而备受争议，ChatGPT产品推出后，员工被要求保持沉默，引发了公众对公司管理的质疑。 </div>
                        <hr>
                    
                    <p></p><blockquote>“我不相信他”：负责捍卫人类利益的OpenAI团队为何分崩离析。</blockquote><p></p><p>&nbsp;</p><p></p><h2>ChatGPT 可以说话，但 OpenAI 员工不能</h2><p></p><p>&nbsp;</p><p>Ilya Sutskever与Jan Leike上周二宣布将离开OpenAI。这两位是公司superalignment“超级对齐”团队的负责人，主要工作就是确保AI技术与开发者的目标保持一致，避免对人类造成不可预测的损害。</p><p>&nbsp;</p><p>选择离开的不只有他们。自去年11月以来，也就是OpenAI董事会试图以“宫斗”方式解雇Sam Altman之时，公司里至少5名关注AI安全的员工已经或主动、或被动地离开OpenAI。</p><p>&nbsp;</p><p>这到底在闹哪样？如果大家一直在社交媒体上关注此事，可能会以为是OpenAI悄然取得了巨大技术突破。表情包“Ilya看见了什么？”认为这位前首席科学家之所以仓惶离场，是因为他看到了令人恐惧的东西——比如可能毁灭人类的AI系统。</p><p>&nbsp;</p><p>但真正的答案可能跟对技术的恐惧无关，而仍然出在人的身上——也就是OpenAI掌门Altman。熟悉该公司内情的消息人士称，关注AI安全的稳健派已经彻底对Altman失去了信任。</p><p>&nbsp;</p><p>一位不愿透露姓名的内部知情人士表示，“信任崩溃是个逐渐的过程，就如同多米诺骨牌的倒落一样。”</p><p>&nbsp;</p><p>没有多少员工愿意公开讨论这个问题。这一方面是因为OpenAI向来会与离职员工签订相当严苛的离职协议，而拒绝签署则意味着放弃补偿权益，有可能损失数百万美元。</p><p>&nbsp;</p><p>OpenAI有着极其严格的离职协议，其中包含前 OpenAI 员工必须遵守的保密和保密条款。它禁止他们在余生中批评他们的前雇主。即使承认 NDA 的存在也被视为违反了协议。</p><p>&nbsp;</p><p>离职员工如果拒绝签署该协议，或泄露协议内容，他们将有可能失去在职期间所获得的所有已归属期权。对于像 OpenAI 这样的初创公司员工来说，期权收益是一项重要甚至是高于其薪资的补偿形式。因此，用这份收益作为威胁，是让离职员工保持沉默的一种非常有效的方式。</p><p>&nbsp;</p><p>有点讽刺的是，OpenAI刚宣布了令人兴奋的新产品 ChatGPT 4o，让ChatGPT 可以像人类一样说话。是的，ChatGPT可以说话，但OpenAI 员工必须保持沉默。“封口令”的存在，让OpenAI获得了几乎一边倒的批评意见。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9b565b2788472834a065ddcd8e8ffc59.jpeg" /></p><p></p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ed/ede8bb53946c0e681171752bbb2c92ba.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><blockquote>风向变了。OpenAI 正处于全民唾弃的边缘。</blockquote><p></p><p>&nbsp;</p><p></p><h2>OpenAI 否认三连，但没人相信他们了</h2><p></p><p>&nbsp;</p><p>鉴于当前舆论哗然的形势，OpenAI发出了一份声明，强调「我们从未剥夺任何现任或前雇员的应得利益，也不会因对方拒绝签署离职或禁止负面评论的协议而剥夺其利益。」而在询问这是否反映出政策内容有所变更时，OpenAI的回应是「声明反映了事实」。</p><p>&nbsp;</p><p>昨天下午，Altman本人在一条推文中承认，OpenAI公司的离职协议中确有一条关于离职员工「潜在股权撤销」的规定，但表示该公司已经在调整具体内容。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/a9/a9f80bf39daf65a23255fd8a0cc03aa2.jpeg" /></p><p></p><p></p><blockquote>关于近期出现的有关OpenAI如何处理权益一事的讨论：我们从未因对方拒绝签署离职协议（或禁止负面评论协议）而剥夺任何人的应得权益，未来也不会这样做。应得权益就是应得的，无需讨论。关于我们原有文件中提出的潜在股权撤销规定，尽管我们从未实际实施，但也承认这条内容本就不该存在。这是我的问题，也是我在执掌OpenAI以来最尴尬的情况；我确实不知道有这么一条，抱歉。过去这一个月来，相关团队已经在调整标准离职条款。如果有任何前员工担心旧协议引发问题，都可以与我联系并共同解决这个问题。再次抱歉。</blockquote><p></p><p>&nbsp;</p><p>但还有一位前雇员，他拒绝签署离职协议，为的就是可以自由批评该公司。Daniel Kokotajlo于2022年加入OpenAI，加入治理团队后一直希望引导公司拥抱安全部署理念，但最终于上个月选择辞职。他说这也意味着他放弃了高达85%的家庭净资产。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/5c/5c5230e729c815b369119311cde9f7e6.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>Kokotajlo在上周接受采访时表示，“OpenAI正在训练越来越多的AI系统，目标就是最终超越人类智能。这可能是人类有史以来最激动人心的目标，但如果我们不能谨慎行动，那也可能陷入万劫不复的深渊。”</p><p>&nbsp;</p><p>OpenAI曾明确表示希望建立通用人工智能（AGI），这是一种理想系统，能够在诸多领域实现等同甚至超越人类的智能表现。</p><p>&nbsp;</p><p>Kokotajlo坦言，“我曾充满希望，认为OpenAI就是技术发展的灯塔，会以负责任的态度逐步迈向AGI。但现在很多人都意识到根本就不可能，我逐渐对OpenAI领导层及其处理AGI的负责态度失去信任，并最终选择辞职。”</p><p>&nbsp;</p><p>Leike上周五也在X上发帖解释了自己辞去超级对齐团队联合负责人职务的原因，情况与Kokotajlo基本相似。他写道，“我一直对公司的核心优先事项设置保留意见，而形势最终发展到了临界点。”</p><p>&nbsp;</p><p>现在，Altman社交媒体下面，都是大片质疑的声音：“为什么安全要退居次要位置？”“为什么禁止前员工批评OpenAI？”“理性的人不会信任你。”</p><p>&nbsp;</p><p>Greg Brockman也不得不出面发表了一条署名为“Sam and Greg”的长推文，表示他们没有放弃安全，并且在“努力减轻风险”。网友们照样不买账，认为这都是废话，一看就是Altman的手笔。</p><p></p><p><img src="https://static001.geekbang.org/infoq/97/97808a8789e9fda28b368e0a2445d540.jpeg" /></p><p></p><p>&nbsp;</p><p>外媒对此评论说，这是Altman已经被大家的反应“吓坏了”。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/4d/4dd4b8a0073007243edfd6a2318dbe4a.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>OpenAI安全团队为何不信任Sam Altman？</h2><p></p><p>在回答这个问题，我们需要将时间倒回去年11月。当时，身为OpenAI董事会成员的Ilya也参与过对Altman的“逼宫”。董事会指出，Altman“在沟通中未能一直保持坦诚。”换句话说：我们不相信他。</p><p>&nbsp;</p><p>但这场行动最终失败，Altman和他最忠诚的盟友、公司总裁Greg Brockman威胁要将OpenAI的顶尖人才一股脑带去微软。也就是说除非立刻让Altman官复原职，否则OpenAI就会当场爆炸。面对这种威胁，董事会只得屈服，劫后余生的Altman比以往任何时候都更加强大，并立即组织起更支持他、愿意让他放手做事的新董事会。</p><p>&nbsp;</p><p>入宫行刺失败，事情就绝对不可能善了。</p><p>&nbsp;</p><p>虽然Ilya和Altman曾多次对外大秀二人的深厚友情，但在上周Ilya还是宣布离职，并表示将投身于“对我个人而言意义重大的项目。”几分钟后，Altman也在X上发帖，称“这让我非常难过。Ilya是……我的亲密好友。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/98/9862a19aee388cf1645f117d9d8fd507.png" /></p><p></p><p>&nbsp;</p><p>Sam Altman和 Ilya Sutskever 于 2023 年 6 月 5 日在一所大学里共同发表演讲</p><p>&nbsp;</p><p>但实际情况是，自从政变失败以来，这半年间Ilya就没有出现在OpenAI的办公室。他一直以远程方式领导超级对齐团队，以确保未来的AGI能够与人类利益保持一致。虽然想法不错，但却与公司的日常运营彼此分离，OpenAI的主要精力完全放在Altman领导下的商业化产品开发方面。在Altman复职后不久，Ilya还曾发布并迅速删除过这样一条推文：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/44/4406ff2a816d0b848c358011fb4838ef.png" /></p><p></p><p></p><blockquote>这个月我学到了很多。与其费心运用各种手段，倒不如“棍棒底下出孝子”。</blockquote><p></p><p>&nbsp;</p><p>因此尽管对外总在刻意展示二人的亲密关系，但在经历了政变之后，Ilya和Altman还能不能算朋友真的要打个大大的问号。</p><p>&nbsp;</p><p>而Altman对“逼宫”一事的反应，也揭示出他性格中的某些侧面：除非董事会让他官复原职，否则他就拼个鱼死网破，而且坚持对董事会成员进行大洗牌以巩固自身地位，表现出牢牢把握权力的坚定决心，甚至直接消除了未来再次面临质疑的可能性。已经有多位前同事和雇员证明Altman就是个控制狂，当面一套、背后一套——比如他曾多次在人前强调安全，但实际工作时却根本不在乎。</p><p>&nbsp;</p><p>举例来说，Altman会从沙特阿拉伯筹款，希望借此建立新的AI芯片公司，为自己的AI前沿探索储备充足的算力资源。此事让关注安全的员工们感到震惊。如果Altman真的在以安全方式构建和部署AI，为什么要以近乎疯狂的方式攫取芯片，不惜一切代价加快技术开发？他又为什么要跟沙特合作，坐视对方利用AI增强数字监控或人权侵犯等潜在风险？</p><p>&nbsp;</p><p>内部知情人士指出，于员工们而言，所有这一切都导致了文章开头提到的“失去信任，因此哪怕OpenAI如何强调自己对某件事的重视，人们也都不再相信。”</p><p>&nbsp;</p><p>而这个渐进的过程，在上周开始全面爆发。</p><p>&nbsp;</p><p>超级对齐团队联合负责人Jan Leike就拿出了鲜明的态度。他在脱离OpenAI的几小时后就在X上发帖称“我辞职了。”没有温暖的告别，也没有对公司管理层的哪怕虚与委蛇的夸赞。</p><p>&nbsp;</p><p>其他重视安全的多位前雇员则给Leike的离职帖点赞，同时加上了爱心的表情符号。Leopold Aschenbrenner作为其中一位，就是上个月被OpenAI开除的超级对齐团队成员。媒体报道指出，他和同团队的另一位研究人员Pavel Izmailov因泄漏信息而被解雇。但OpenAI方面并未提供关于泄漏的证据。鉴于每位员工在加入OpenAI时都需要签署严格的保密协议，所以对于一位身经百战的硅谷老兵来说，Altman完全可以将最无害的信息分享也定义成“泄漏”，借此把Ilya一系的员工全都清理出OpenAI之外。</p><p>&nbsp;</p><p>就在Aschenbrenner和Izmailov被离职的同一个月，另一位安全研究员Cullen O’Keefe也离开了公司。</p><p>&nbsp;</p><p>两周之前，公司一位安全研究员William Saunders在EA论坛上发表一篇神秘的帖子。EA论坛是有效利他主义运动成员们的线上聚会场所，一直在积极参与AI安全事业。Saunders总结了自己作为超级对齐团队成员在OpenAI所做的工作，写道“我于2024年2月15日从OpenAI辞职。”一位评论者则提出了核心问题：Saunders为什么要专门发帖讨论这事？</p><p>&nbsp;</p><p>Saunders回应称，“无可奉告。”用户们由此猜测，他很可能是受到了禁止负面评论协议的约束。</p><p>&nbsp;</p><p>将上述消息跟公司内部人士的话语结合起来，我们至少发现有七位前雇员都曾努力在内部推动OpenAI的安全意识，但却最终对公司主导者彻底失去信心，并最终选择退出。</p><p>&nbsp;</p><p>知情人士指出，“我认为公司里很多认真关注安全和社会影响的同事，心里都抱有一个悬而未决的疑问：为OpenAI这样的公司工作，到底是不是对的？要想让员工们放心，OpenAI就必须对所做之事深思熟虑、同时承担起责任。”</p><p>&nbsp;</p><p></p><h2>随着安全团队的解散，OpenAI的工作安全该由谁保障？</h2><p></p><p>由于Leika不再负责超级对齐团队的管理，OpenAI任命公司联合创始人John Schulman取代了他的位置。</p><p>&nbsp;</p><p>但该团队已经被掏空，Schulamn也仍忙于处理他之前的主要工作，确保OpenAI现有产品的安全。在这样的背景下，谁能指望OpenAI会认真开展具有前瞻性的安全工作？</p><p>&nbsp;</p><p>恐怕没戏。</p><p>&nbsp;</p><p>知情人士解释称，“超级对齐团队当初建立的目的，就是如果公司成功打造出通用人工智能，那么必然会引发各种类型的安全问题。该团队实际是一笔面向未来的专项投入。”</p><p>&nbsp;</p><p>但即使该团队满负荷运转，这笔“专项投入”也只能调动OpenAI内的一小部分研究人员，且只承诺为其提供20%的算力资源。现如今，这批算力可能会被移交给其他OpenAI团队，也不清楚是否将继续探索如何避免未来可能出现的AI灾难性风险。</p><p>&nbsp;</p><p>需要明确一点，这绝不是说OpenAI当前发布的产品（例如能够与用户开展顺畅对话的最新大模型GPT-4o）就会毁灭人类。但随着这项技术的快速发展，未来如何谁也不敢保证。</p><p>&nbsp;</p><p>知情人士表示，“最重要的就是搞清楚他们目前是否正在构建和部署不安全的AI系统，以及能不能指望他们安全构建和部署AGI或者超级智能。前一条我不知道，但后面这点我认为是指望不上。”</p><p>&nbsp;</p><p>Leike在上周五的X帖子中也表达了同样的担忧。他指出，他的团队一直在努力争取足够的算力来完成工作，但总体上可谓是“逆水行舟”。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/f9/f98129b42c554c58c615495df81d1532.png" /></p><p></p><p></p><blockquote>之所以加入OpenAI，是因为我觉得这里是最适合开展这项研究的场所。然而很长一段时间以来，我跟OpenAI领导层在公司核心优先事项方面一直存在着分歧，这种分歧最终走到了临界点。我认为我们应该将更多的资源花在为下一代模型做好准备上，具体包括安全、监控、准备、安全对抗稳健性、超级对齐、保密性以及社会影响等相关主题。这些问题很难解决，我甚至担心OpenAI还没找到正确的路线。过去这几个月间，我的团队一直在逆水行舟。有时候我们会在算力方面遇到困难，也让这项重要的研究变得愈发举步维艰。</blockquote><p></p><p>&nbsp;</p><p>其中最重要的一条，就是Leike提到“我认为我们应该将更多的资源花在为下一代模型做好准备上，具体包括安全、监控、准备、安全对抗稳健性、超级对齐、保密性以及社会影响等相关主题。这些问题很难解决，我甚至担心OpenAI还没找到正确的路线。”</p><p>&nbsp;</p><p>当AI安全领域最举足轻重的从业者之一表示世界领先的AI厂商还没找到正确路线时，恐怕我们都有理由感到担忧。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.lesswrong.com/posts/kovCotfpTFWFXaxwi/simeon_c-s-shortform">https://www.lesswrong.com/posts/kovCotfpTFWFXaxwi/simeon_c-s-shortform</a>"</p><p><a href="https://www.vox.com/future-perfect/2024/5/17/24158403/openai-resignations-ai-safety-ilya-sutskever-jan-leike-artificial-intelligence">https://www.vox.com/future-perfect/2024/5/17/24158403/openai-resignations-ai-safety-ilya-sutskever-jan-leike-artificial-intelligence</a>"</p><p><a href="https://www.vox.com/future-perfect/2024/5/17/24158478/openai-departures-sam-altman-employees-chatgpt-release">https://www.vox.com/future-perfect/2024/5/17/24158478/openai-departures-sam-altman-employees-chatgpt-release</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/CyJK5Ki8uVC48mbAloWH</id>
            <title>发布屡次截胡？OpenAI与谷歌携新版大模型再度交锋 | 大模型一周大事</title>
            <link>https://www.infoq.cn/article/CyJK5Ki8uVC48mbAloWH</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/CyJK5Ki8uVC48mbAloWH</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 May 2024 09:14:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 技术动态, AI巨头, 新一代模型
<br>
<br>
总结: 大模型的快速发展使得了解最新技术动态和积极学习成为从业者的必修课。本周人工智能领域迎来了一波大模型发布的高潮，包括OpenAI、谷歌、百度和腾讯等公司推出的新一代模型。这些新模型在多模态理解、长文本理解和运行速度等方面有所突破，预示着AI技术在未来将扮演更加关键的角色。 </div>
                        <hr>
                    
                    <p>大模型的快节奏发展，让了解最新技术动态、积极主动学习成为每一位从业者的必修课。InfoQ研究中心期望通过每周更新大模型行业最新动态，为广大读者提供全面的行业回顾和要点分析。现在，让我们回顾过去一周的大模型重大事件吧。</p><p></p><h2>一、重点发现</h2><p></p><p>本周，人工智能领域迎来了一波大模型发布的高潮，行业玩家纷纷推出自家的创新成果，AI&nbsp;巨头间的角力再次升温。OpenAI、谷歌、百度和腾讯等公司相继亮相了各自的大模型。其中，OpenAI&nbsp;的新一代模型&nbsp;GPT-4o&nbsp;与谷歌的&nbsp;Gemini&nbsp;家族最为引人注目。新模型不仅在多模态理解能力、长文本理解、运行速度等性能上有所突破，更在应用场景和用户体验上带来了新的想象空间，预示着AI技术将在未来扮演更加关键的角色。</p><p></p><h2>二、具体内容</h2><p></p><p></p><h3>大模型持续更新</h3><p></p><p>5&nbsp;月&nbsp;12&nbsp;日，斯坦福大学的研究者开发了一个名为&nbsp;ThunderKittens&nbsp;的&nbsp;AI&nbsp;加速框架。该框架通过简化的&nbsp;CUDA&nbsp;DSL&nbsp;让开发者能够更容易地编写高效的&nbsp;GPU&nbsp;内核，显著提高了&nbsp;GPU&nbsp;利用率。&nbsp;ThunderKittens&nbsp;在&nbsp;RTX&nbsp;4090&nbsp;上实现了约&nbsp;122&nbsp;TFLOP&nbsp;的性能，且在&nbsp;H100&nbsp;上的性能比&nbsp;FlashAttention-2&nbsp;高出约&nbsp;30%。5&nbsp;月&nbsp;14&nbsp;日，OpenAI发布了新一代模型&nbsp;GPT-4o&nbsp;，这是一个全能模型。该模型集成了文本、语音、图像三种模态的理解力，能够实时生成文本、音频和图像的输出。GPT-4o&nbsp;在英语文本、代码、非英语文本、视觉和音频理解方面都有显著提升。5&nbsp;月&nbsp;15&nbsp;日，谷歌发布&nbsp;Gemini&nbsp;家族新成员&nbsp;Gemini&nbsp;1.5&nbsp;Flash&nbsp;，并宣布更新&nbsp;Gemini&nbsp;1.5&nbsp;Pro&nbsp;。Gemini&nbsp;1.5&nbsp;Flash&nbsp;是一款专为速度而优化的小型模型，旨在处理高频任务，提供快速响应。它能够分析和处理包括文本、图片和视频在内的多种信息类型，拥有高达100万个Token的处理能力。Gemini&nbsp;1.5&nbsp;Pro&nbsp;&nbsp;具备&nbsp;200&nbsp;万&nbsp;token&nbsp;的超长上下文窗口，能够处理大量信息，如&nbsp;2&nbsp;小时视频、&nbsp;22&nbsp;小时音频、超过&nbsp;6&nbsp;万行代码或&nbsp;140&nbsp;多万单词。5&nbsp;月&nbsp;15&nbsp;日，百度发布了全球首个&nbsp;L4&nbsp;级自动驾驶大模型&nbsp;Apollo&nbsp;ADFM&nbsp;，并宣称其安全性是普通人类驾驶员的10倍以上，能覆盖城市级全域复杂场景。5&nbsp;月&nbsp;16&nbsp;日，亚信科技认知增强平台&nbsp;TAC&nbsp;MaaS&nbsp;与渊思·编程大模型、渊思·自智网络大模型、渊思·智能运维大模型&nbsp;3&nbsp;个行业大模型。5&nbsp;月&nbsp;17&nbsp;日，腾讯云正式发布教育行业大模型。该模型基于自研混元大模型，融合了教材、习题、论文等资源，并通过腾讯云TI平台优化，特别在中文阅读理解、问答和教育相关任务上表现优异。5&nbsp;月&nbsp;17&nbsp;日，字节跳动发布了豆包大模型（原云雀大模型）&nbsp;AI&nbsp;产品家族。豆包大模型家族包括九款模型，满足不同场景需求，并且字节跳动还推出了&nbsp;AI&nbsp;应用产品“扣子”和豆包&nbsp;App&nbsp;。</p><p></p><h4>开源领域</h4><p></p><p>5&nbsp;月&nbsp;13&nbsp;日，零一万物发布了其&nbsp;Yi&nbsp;大模型家族的新成员&nbsp;Yi-1.5&nbsp;并正式开源。&nbsp;Yi-1.5&nbsp;包含&nbsp;6B、9B、34B&nbsp;三个版本的预训练和微调模型，采用&nbsp;Apache&nbsp;2.0&nbsp;许可证。作为&nbsp;Yi-1.0&nbsp;的持续预训练版本，&nbsp;Yi-1.5&nbsp;在&nbsp;500B 个&nbsp;token&nbsp;上进行了训练，以提升编码、推理和指令执行能力，并在&nbsp;300&nbsp;万个指令调优样本上进行了精细调整。&nbsp;5&nbsp;月&nbsp;14&nbsp;日，腾讯宣布其混元文生图大模型全面开源。该模型支持中英文双语输入及理解，拥有&nbsp;15&nbsp;亿参数量，并采用了与&nbsp;Sora&nbsp;一致的&nbsp;DiT（Diffusion&nbsp;With&nbsp;Transformer）&nbsp;架构，使其在文生图生成方面表现优异，效果超越开源的&nbsp;Stable&nbsp;Diffusion&nbsp;模型。</p><p></p><h4>多模态领域</h4><p></p><p>5&nbsp;月&nbsp;15&nbsp;日，谷歌发布了视频生成模型&nbsp;Veo&nbsp;，该模型能够根据文本提示生成超过&nbsp;60&nbsp;秒的高质量&nbsp;1080p&nbsp;视频，支持多种电影风格，并具备深层次的语言与视觉理解能力。Veo能够准确捕捉文本中的细微差别，并在视频场景中逼真呈现细节。</p><p></p><h4>科研领域</h4><p></p><p>5&nbsp;月&nbsp;13&nbsp;日，百度大数据实验室与上海交通大学团队合作开发了名为&nbsp;RNAErnie&nbsp;的基于Transformer的RNA语言模型。该模型通过基序感知预训练和类型引导的微调策略，在多个数据集和任务中表现出色，准确率和F1得分显著提高，证明了其在RNA序列分析方面的优越性和泛化潜力。5&nbsp;月&nbsp;16&nbsp;日，来自亚马逊与得克萨斯大学奥斯汀分校的研究团队发表论文《SynthesizRR:&nbsp;Generating&nbsp;Diverse&nbsp;Datasets&nbsp;with&nbsp;Retrieval&nbsp;Augmentation》。&nbsp;SynthesizRR&nbsp;是一种创新的数据集合成技术，通过结合检索和精细化（Refinement）方法，解决了传统大型语言模型在生成示例时出现的重复性、偏差和风格差异问题。该技术通过引入多样化的内容“种子”，显著提升了词汇和语义的多样性，并在多个复杂任务的数据集上，与人类文本的相似性以及学生模型的提炼性能方面取得了显著进步。</p><p></p><h3>应用探索</h3><p></p><p></p><h4>智能体</h4><p></p><p>5&nbsp;月&nbsp;13&nbsp;日，宇树科技推出了新款人形机器人&nbsp;Unitree&nbsp;G1&nbsp;，其起步价为&nbsp;9.9&nbsp;万元人民币，相比之前推出的&nbsp;Unitree&nbsp;H1&nbsp;价格大幅下降。Unitree&nbsp;G1身高&nbsp;1.27&nbsp;米，体重&nbsp;35&nbsp;公斤，具有多达&nbsp;43&nbsp;个关节电机（基础版为&nbsp;23&nbsp;个），能够模拟复杂动作并实现精细的运动控制。这款机器人可以折叠存放，运行速度可达&nbsp;2&nbsp;米/秒，并且配备了&nbsp;3D&nbsp;LiDAR&nbsp;传感器和深度摄像头，具备360度全景深度感知能力。5&nbsp;月&nbsp;15&nbsp;日，谷歌发布名为&nbsp;Project&nbsp;Astra&nbsp;的&nbsp;AI&nbsp;Agent&nbsp;。Project&nbsp;Astra&nbsp;能够接收信息、记忆所看到的内容、处理信息并理解上下文细节，以实现与周围世界的自然交互。它在声音和视觉处理方面表现出色，能够进行无延迟的实时语音交互，并快速响应用户的问题，通过连续编码视频帧和组合视频、语音信息来处理收到的内容。</p><p></p><h3>基础设施</h3><p></p><p>5&nbsp;月&nbsp;15&nbsp;日，谷歌发布第六代AI芯片&nbsp;Trillium&nbsp;。这款新型&nbsp;TPU&nbsp;在计算性能上实现了高达&nbsp;4.7&nbsp;倍的提升，同时内存带宽翻倍，能效比上一代产品提高了&nbsp;67%&nbsp;。Trillium&nbsp;芯片采用了谷歌自研的第三代&nbsp;SparseCore&nbsp;技术，有效加速了模型训练并降低了服务延迟。预计&nbsp;Trillium&nbsp;将在今年年底向云客户提供，进一步巩固其在云计算和AI领域的领导地位。</p><p></p><p>报告推荐</p><p>AGI&nbsp;概念引发热议。那么&nbsp;AGI&nbsp;究竟是什么？技术架构来看又包括哪些？AI&nbsp;Agent&nbsp;如何助力人工智能走向&nbsp;AGI&nbsp;时代？现阶段营销、金融、教育、零售、企服等行业场景下，AGI应用程度如何？有哪些典型应用案例了吗？以上问题的回答尽在《中国AGI市场发展研究报告&nbsp;2024》，欢迎大家扫码关注「AI前线」公众号，回复「AGI」领取。</p><p></p><p><img src="https://static001.geekbang.org/infoq/69/69f5f30dc6564327e46c59d969be2524.jpeg" /></p><p></p><p></p><p>报告预告</p><p>金融行业是否找到了大模型落地应用的最佳路径？取得了哪些具体应用成果?&nbsp;又存在哪些难以逾越的挑战与桎梏？金融机构一定要应用大模型吗？如何考量金融大模型应用效果？欢迎大家持续关注InfoQ研究中心即将发布的《大模型在金融领域的应用洞察》。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9a/9ab971ff9c3c1b68ee2abbf12e27f748.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/05iCMhVcvIq6QDAq5piF</id>
            <title>未来智能王松：聚焦会议垂直场景，打造最实用的AI会议助理</title>
            <link>https://www.infoq.cn/article/05iCMhVcvIq6QDAq5piF</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/05iCMhVcvIq6QDAq5piF</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 May 2024 03:51:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能, 会议耳机, 大模型, AI助理
<br>
<br>
总结: 未来智能在AI会议助理领域的全面规划和应用，通过大模型技术提升会议效率和安全性，为用户提供全方位的智能服务体验。 </div>
                        <hr>
                    
                    <p>5月17日，AICon全球人工智能开发与应用大会暨大模型应用生态展北京站召开。人工智能硬件公司未来智能CTO王松受邀参加了大会，并在大会上发表了以“探索大模型在会议领域中的应用”为主旨的演讲，分享了未来智能在办公会议垂直场景，打造 “减轻会议焦虑、提升会议效率”AIGC智能耳机的思考及规划。今年两会，国家提出了"人工智能+"蓝图，推动AI赋能各行各业，促进新质生产力发展。目前，中国已经进入到AI高速发展的阶段，越来越多的社会各界人士认为，2024年将是AI应用爆发的元年。</p><p>&nbsp;</p><p>5月15日，未来智能发布了包括讯飞会议耳机Pro 2、iFLYBUDS2两款新一代讯飞会议耳机，其中最重要的迭代，是升级了viaim AI会议助理，可一键生成「摘要总结」，或一键生成『代办事项』，大幅提升用户会后整理纪要的效率之外。此外，还新增了“有问必答”功能，让用户可以通过语音或文字，直接快速查询会议内容，进一步提高了办公效率，这也标志着讯飞会议耳机从“智能工具”成功进化为“智能助理”，也让新一代讯飞会议耳机成为了当下AIGC智能耳机硬件新标杆。</p><p></p><p><img src="https://static001.geekbang.org/infoq/75/759fe3290f1dff3cfc420687333b474c.png" /></p><p></p><p>下面是王松在大会上演讲内容的摘要，内容主要聚焦未来智能关于讯飞会议耳机AI能力整体布局思考，帮助读者进一步了解，在办公会议场景下，未来智能打造“最实用AI会议助理”的产品思维与逻辑。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1e/1ee53d5b9f4afa9d5933368ce674da91.png" /></p><p></p><h2>会前，用数据驱动会议规划</h2><p></p><p>&nbsp;</p><p>在演讲中，王松提到，讯飞会议耳机的长远目标，是能够成为用户的私人会议AI助理。在这样的愿景下，基于领先的语音识别及语义分析技术而来的全场景录音转写功能、多语种翻译功能、以及viaim AI带来的会后高效信息整理等功能，构筑了讯飞会议耳机的核心功能框架。</p><p>&nbsp;</p><p>真正的“助理”，需要能够基于用户的个人数据信息，能够帮助用户进行会议规划。王松提到的“会议规划”，是指耳机的AI能力，能够利用个人历史的会议数据，能够提前预测出某次会议的会议时长、参与人数、会议流程，进而优化会议流程，提高会议效率，减少不必要的时间成本。王松举了一个例子：假设你每周在固定时间都会开个周会，会上会讨论公司销售、生产、财务、市场、产品、技术等等的各个方面的进展和需要沟通的问题。这时，AI就能够通过历史的会议记录和内容预测出我周一早晨的这个会议时长、流程，动态的在会议中根据发言人的内容和时长来实时提醒会议节奏。这对会议主持人来讲，是非常实用的功能，甚至可以做到会议由AI 主持来自动提醒。</p><p>&nbsp;</p><p>然后，AI能根据上周甚至是数周之前的会议内容罗列出未解决的问题，然后在会议前通过 App 通知用户，用户可以提前填写进度，他不需要写所有的，只需要按照 AI 的提示或者问题来回答就可以，然后由AI 自动生成发言稿。这项功能特别适合有规律的、汇报型的会议。</p><p>&nbsp;</p><p>此外，还有一个有趣的功能：AI还能通过实时分析对会议发言人的语速、口气、情绪，以及对表述内容的分析，给出某个发言人在会议中的表现打分。在会议中，“沉默是金”的参会者，“废话文学”的参会者得到的分数都不会太高。他们的低效表现，也将会被AI识别。</p><p>&nbsp;</p><p>王松表示，未来智能目前已经做到了通过监控数据，大模型生成提醒内容，然后语音合成，最后通过会议系统发送等功能的设计，未来成熟后有望通过OTA升级在讯飞会议耳机上呈现。</p><p></p><h2>会中及会后，用大模型提升会议效率</h2><p></p><p>&nbsp;</p><p>讯飞会议耳机的核心基础功能实时录音转写。基于这项基础功能，再附加大模型能力，可以进一步提升用户在会议中的效率。</p><p>&nbsp;</p><p>王松介绍到，在会中，未来智能的大模型，可实现“实时总结”功能，用户可以向AI提问，例如问刚刚会议讲了什么，AI就能总结出刚才的会议内容；还能实现“生僻词解释”功能，在会中遇到听不懂的单词或者术语，就可以向AI提问，或者在记录中的高亮下划线词汇下点击寻找解释。在这里，AI扮演了很好的助手的角色。</p><p>&nbsp;</p><p>大模型还能帮助用户提高会议参与度。比如，上面提到的“参会者打分”，可以提醒用户改善会议中的表现。还可以通过“实时问答”的相关功能，对于会议中出现陌生问题，AI能够自动搜索并帮助写答案，提高用户的参会效率。</p><p>&nbsp;</p><p>在会后，除了目前讯飞会议耳机目前已经实现的会议记录整理和分发，viaim AI生成「待办事项」强化会后跟踪和落实行动计划等，未来还将新增会议“整体评估”等功能，不断强化讯飞会议耳机AI会议助理属性。</p><p></p><h2>用大模型，加强会议安全</h2><p></p><p>&nbsp;</p><p>在一些重要且涉密的会议中，安全是最大的课题。讯飞会议耳机同样也极为重视“安全”相关的功能开发，解决会议的后顾之忧。</p><p>&nbsp;</p><p>王松介绍到，在安全方面，未来智能通过大模型，目前主要从两个方面入手加强安全，分别是“异常行为检测”、“内容安全过滤”。异常行为方面，大模型通过分析参会者的行为模式（如发言模式等），检测出异常行为等。安全内容过滤，主要包括敏感信息检测：实时监控会议中的文本、语音和视频内容，识别和过滤敏感信息（如机密数据、敏感关键词等），防止信息泄露；不当内容监控：利用大模型检测和过滤会议中的不当内容，如不适当的语言、图像或行为，确保会议环境的专业性和安全性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a4/a47f330cd9689d5a1ddad2d3f55e160d.png" /></p><p></p><p>通过王松的演讲，可以看到未来智能对于会议耳机AI应用方面思考，已经非常全面且深入，洞察到了会议方方面面的各类需求，并有针对性的提出了解决方案。而在消费者体验方面，则聚焦在会前、会中、会后，打造全方面的AI会议助理服务体验。当下的讯飞会议耳机，已经能够有效解决用户痛点，非常使用，当未来智能全面实现了所有的规划，人们的工作生活，获奖因此变得更加美好。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/d9NQ0Ydi3Jqxu5PmAktO</id>
            <title>演示文生图时出现sleep代码，华为回应造假嫌疑；微软将中国AI团队集体打包到美国；百度ECharts创始人“下海”养鱼｜Q资讯</title>
            <link>https://www.infoq.cn/article/d9NQ0Ydi3Jqxu5PmAktO</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/d9NQ0Ydi3Jqxu5PmAktO</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 May 2024 03:11:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 华为, 大模型演示, 字节跳动, 价格比较
<br>
<br>
总结: 华为回应大模型演示造假事件，称并非调取预置图片；字节跳动发布豆包大模型，价格比行业便宜99%；传微软将中国AI团队集体打包去美国，官方回应。 </div>
                        <hr>
                    
                    <p>整理&nbsp;|&nbsp;Tina、梓毓</p><p></p><p></p><blockquote>华为昇腾回应“大模型演示造假”：并非调取预置图片；字节跳动发布豆包大模型，价格比行业便宜&nbsp;99%！传微软将中国&nbsp;AI&nbsp;团队集体打包去美国，官方回应；字节跳动或放弃出售沐瞳科技；大厂新发岗位薪资排名揭晓；OpenAI&nbsp;首席科学家官宣离职；OpenAI&nbsp;官宣旗舰模型&nbsp;GPT-4o；网传阿里巴巴员工贪腐案，一年受贿&nbsp;9200&nbsp;多万！谷歌云一键“删库”；微软因&nbsp;Cortana&nbsp;专利侵权被判赔偿&nbsp;2.42&nbsp;亿美元；Android&nbsp;15&nbsp;引入私人空间；苹果新一代&nbsp;iPad&nbsp;Pro&nbsp;被发现存在渲染失常问题……</blockquote><p></p><p></p><h2>科技公司</h2><p></p><p></p><h4>华为昇腾回应“大模型演示造假”：并非调取预置图片</h4><p></p><p>5&nbsp;月&nbsp;16&nbsp;日，网传在一场华为的发布会上，其展示大模型“文生图”能力时疑似造假。事件起源于&nbsp;5&nbsp;月&nbsp;10&nbsp;日的鲲鹏昇腾开发者大会，当时在一场面向开发者的技术讨论会上，华为演示了&nbsp;mxRAG&nbsp;SDK&nbsp;功能，展示如何通过十几行代码即可完成&nbsp;RAG&nbsp;应用开发。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4b/4b2ef668cbcbbbb05c954029fd8a3f8f.webp" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/36/3646e0d5d5fe416f74a20e366e9714fc.webp" /></p><p></p><p>网传视频及聊天截图显示，华为在演示文生图功能时，按下&nbsp;Crtl-C&nbsp;中断，显示对应代码为&nbsp;time.sleep(6)。作为开发者，很显然大家对这段代码非常感兴趣。国内外网友们就&nbsp;time.sleep(6)&nbsp;的作用进行了讨论，有人认为这是暂停&nbsp;6&nbsp;秒，再调取预置图片，怀疑其图文结果并非大模型生成，而是人为操控。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6e/6e4a62039d6d80d3a967e2bfc9d4575a.webp" /></p><p></p><p>也有人认为截图并不能说明华为是造假&nbsp;sleep&nbsp;6&nbsp;秒后创建（write）了图片然后在&nbsp;vscode&nbsp;里打开。但有人反驳说，视频显示&nbsp;time.sleep(6)&nbsp;是关键路径，也就是说执行&nbsp;main&nbsp;一定会执行&nbsp;time.sleep(6)，并且在视频里第二次执行的时候恰好执行了&nbsp;6&nbsp;秒就返回了。如果整个程序总共花费了&nbsp;6&nbsp;秒，那其中真正生成图片的逻辑花费了多少秒？另外，还有评论说，“sleep（6）是写在库里的，更离谱的是写在库包的&nbsp;_init_.py&nbsp;文件里，我作为超过&nbsp;5&nbsp;年的&nbsp;Python&nbsp;开发，从没见过在这个文件里放&nbsp;sleep&nbsp;语句的。”（亲爱的读者，你们如何解读这段&nbsp;Python&nbsp;代码？欢迎留言中评论！）</p><p></p><p></p><p></p><p>针对网络上的质疑，昇腾社区回应称，现场图片为实时生成，调用的是开源大模型。代码中有&nbsp;time.sleep(6)&nbsp;等表述，是命令等待读取外部开源大模型实时生成的图片，并非调取预置图片。本次展示的均为真实代码，也将在昇腾社区上开放，欢迎开发者使用并提出宝贵建议。</p><p></p><h4>字节跳动发布豆包大模型，价格比行业便宜&nbsp;99%！</h4><p></p><p>5&nbsp;月&nbsp;15&nbsp;日，字节跳动正式对外发布豆包大模型。火山引擎是字节跳动旗下云服务平台，豆包大模型原名“云雀”，是国内首批通过算法备案的大模型之一。目前豆包大模型日均处理&nbsp;1200&nbsp;亿&nbsp;Tokens&nbsp;文本，生成&nbsp;3000&nbsp;万张图片。</p><p></p><p>火山引擎总裁谭待重点披露了豆包大模型的商业化价格——豆包主力模型在企业市场的定价为&nbsp;0.0008&nbsp;元&nbsp;/&nbsp;千&nbsp;Tokens，即&nbsp;0.8&nbsp;厘的价格可处理&nbsp;1500&nbsp;多个汉字，较行业平均价格便宜&nbsp;99.3%。市面上同规格模型的定价一般为&nbsp;0.12&nbsp;元&nbsp;/&nbsp;千&nbsp;Tokens，是豆包模型价格的&nbsp;150&nbsp;倍。</p><p></p><p>但火山方面并未披露豆包大模型的具体参数规模。有专业人士认为，不谈参数量谈价格不能说明到底有多便宜。火山方面人士对记者表示，目前参数规模已经不是衡量大模型能力的唯一指标。采访中谭待表示，“今年行业不再比拼参数规模了，因为大家都‘悟’了。”不同尺寸的模型具备不同性能，价格自然不同，但豆包是以最终能力最强的主力模型来定价，同时与行业价格进行对比。另外也有专业人士认为，如果该模型能够达到&nbsp;llama&nbsp;70b&nbsp;或者&nbsp;mixtral&nbsp;8×7b&nbsp;的效果，那性价比就非常好了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8c/8ccbf019dc0ef271091053125e081d62.webp" /></p><p></p><h4>传微软将中国&nbsp;AI&nbsp;团队集体打包去美国，官方回应</h4><p></p><p>5&nbsp;月&nbsp;15&nbsp;日，据多家媒体报道，微软中国部分员工收到公司邮件，询问是否愿意迁移至其他地区工作，选择包括美国、澳大利亚、爱尔兰等国家在内。涉及的员工包括&nbsp;AI&nbsp;platform&nbsp;的&nbsp;Azure&nbsp;ML&nbsp;团队等有上百名员工。</p><p></p><p>报道中还提到，收到邮件的员工需要在&nbsp;6&nbsp;月&nbsp;7&nbsp;日前做决定，要么去美国，要么选择拿补偿离职。据称，微软美国还可帮助解决家属签证。</p><p></p><p>微软相关人士对媒体回应称，本次是给部分员工一个可选的内部调动机会，微软并没有说明多少员工得到了这一机会。据了解，微软中国&nbsp;C+AI&nbsp;的&nbsp;ML&nbsp;团队可以转到美国西雅图，Azure&nbsp;团队转到澳洲，DevDiv（开发平台事业部）则维持现状。</p><p></p><p><img src="https://static001.geekbang.org/infoq/00/00dfd4df0a1a753b437f81007941919a.webp" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/16/16499882e05ce08930d29dc7b9ba7e4e.webp" /></p><p></p><p>微软称，在运营管理全球业务的过程中，一直有向员工提供内部轮岗机会的机制，并表示公司将继续致力于中国市场，同时在中国和其他市场开展业务。</p><p></p><h4>字节跳动或放弃出售沐瞳科技，将任命新&nbsp;CEO</h4><p></p><p>5&nbsp;月&nbsp;14&nbsp;日消息，&nbsp;据外媒报道，字节跳动放弃了出售沐瞳科技公司的计划，并计划为该游戏工作室任命一位新&nbsp;CEO。</p><p></p><p>报道称，接手沐瞳科技的新任&nbsp;CEO&nbsp;是完美世界前高管张云帆，他将取代沐瞳科技联合创始人兼&nbsp;CEO&nbsp;袁菁。公开报道显示，张云帆曾在完美世界公司担任各个职务，包括首席运营官。目前还不清楚袁菁是否会留在沐瞳科技。</p><p></p><p>对于上述消息，字节跳动方面暂未予以回应。</p><p></p><p>2021&nbsp;年&nbsp;3&nbsp;月，瞳科技&nbsp;CEO&nbsp;袁菁发布内部信，宣布沐瞳科技与字节跳动旗下游戏业务品牌朝夕光年达成战略收购协议。收购完成后，沐瞳科技将保持独立运营，并加强在游戏、电竞等领域与字节跳动的深度融合，共同开拓全球游戏市场。同时，袁菁将继续作为&nbsp;CEO&nbsp;留任，沐瞳科技的各条汇报线保持不变。对于此次收购的具体金额，沐瞳科技和字节跳动方面均未做回应。有外媒报道称，此次收购，字节跳动付出了&nbsp;100&nbsp;亿人民币的现金和价值&nbsp;150&nbsp;亿的股权，支付对价约合&nbsp;40&nbsp;亿美金。</p><p></p><p>然而，仅仅两年后，市场传出字节跳动正在寻求以&nbsp;50&nbsp;亿美元出售沐瞳科技的消息，原因与后者的业绩表现不理想有关。收购两年，字节与沐瞳科技的业务协同未能如期实现。</p><p></p><h4>大厂新发岗位薪资排名揭晓：抖音登顶，华为缺席前十</h4><p></p><p>近日，脉脉高聘人才智库发布的《2024&nbsp;春招高薪职业和人才洞察》报告显示，抖音、亚⻢逊、大疆霸榜高薪公司，岗位平均薪资超&nbsp;5&nbsp;万元（月薪）。</p><p></p><p>数据显示，2024&nbsp;年&nbsp;Q1，新发岗位平均薪资最高的&nbsp;20&nbsp;个公司中，抖音以&nbsp;55363&nbsp;元位居高薪榜第⼀，其次是亚马逊&nbsp;55295&nbsp;元，大疆&nbsp;53485&nbsp;元位列第三。</p><p></p><p><img src="https://static001.geekbang.org/infoq/64/641877dcc46f2eee158915d8f9bafe13.webp" /></p><p></p><p>2024&nbsp;年&nbsp;Q1，在新发岗位平均薪资超过&nbsp;4&nbsp;万元的企业中，AI&nbsp;四小龙之⼀的商汤科技新发岗位平均薪资涨幅&nbsp;12799&nbsp;元，位列涨幅榜第⼀，其次是&nbsp;AIGC&nbsp;领上市企业万兴科技，涨幅&nbsp;11826&nbsp;元。同样薪资增长超过⼀万元的还有自动驾驶公司元戎启行。这些企业都紧密围绕&nbsp;AI&nbsp;业务发展。</p><p></p><h4>OpenAI&nbsp;首席科学家官宣离职，GPT-4&nbsp;负责人接任</h4><p></p><p>当地时间&nbsp;5&nbsp;月&nbsp;14&nbsp;日，OpenAI&nbsp;联合创始人、首席科学家伊尔亚·苏茨克维（Ilya&nbsp;Sutskever）宣布决定离开&nbsp;OpenAI。几个月前，围绕着&nbsp;OpenAI&nbsp;联合创始人兼首席执行官山姆·奥特曼（Sam&nbsp;Altman）的罢免事件中，这位科学家曾被视为关键人物，而随着&nbsp;Sam&nbsp;Altman&nbsp;的回归和董事会改组，OpenAI&nbsp;的权力斗争落幕，也使得&nbsp;Sutskever&nbsp;如今的出走显得没有那么“意料之外”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3e/3e079ba279d221dc206344045f2bab33.webp" /></p><p></p><p>OpenAI&nbsp;CEO&nbsp;奥特曼在推特上发文表示，Ilya&nbsp;与&nbsp;OpenAI&nbsp;的分道扬镳令人非常难过。即将成为下一任&nbsp;OpenAI&nbsp;首席科学家的&nbsp;Jakub&nbsp;Pachocki&nbsp;也对自己的前任&nbsp;Ilya&nbsp;表达了感谢。</p><p></p><p></p><blockquote>Ilya&nbsp;把我带入了深度学习研究的世界，多年来一直是我的导师和伟大的合作者。他对深度学习的令人难以置信的愿景成为&nbsp;OpenAI&nbsp;和&nbsp;AI&nbsp;领域如今的基础。我非常感谢他与我们进行了无数次对话，从有关&nbsp;AI&nbsp;未来进步的高层讨论，到深入的技术白板会议。Ilya，我会想念与你一起工作的日子。</blockquote><p></p><p></p><p>另外，该公司现已聘请了金融软件公司&nbsp;Intuit&nbsp;的前&nbsp;Kubernetes&nbsp;平台工程主管&nbsp;Delyan&nbsp;Raychev，他已经在&nbsp;OpenAI&nbsp;进行招聘，&nbsp;LinkedIn&nbsp;上将其描述为“立即专注于构建和扩展我们的&nbsp;Kubernetes&nbsp;平台”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4b/4b978f58bd6c0799bd894352f2da9780.webp" /></p><p></p><p>更多阅读：《<a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651205864&amp;idx=1&amp;sn=52069452bab630e2fd7a00e67d674a73&amp;chksm=bdbbc8bb8acc41adb1bc07648f595d47fa43e6e98559390a23deaddda1948a5e05beafcba63e&amp;scene=21#wechat_redirect">OpenAI&nbsp;的元老科学家们都跑光了！一个时代结束了？</a>"》</p><p></p><h4>OpenAI&nbsp;官宣旗舰模型&nbsp;GPT-4o，完全免费、无障碍与人交谈！</h4><p></p><p>5&nbsp;月&nbsp;14&nbsp;日凌晨，OpenAI&nbsp;在首次「春季新品发布会」上搬出了新一代旗舰生成模型&nbsp;GPT-4o、桌面&nbsp;App，并展示了一系列新能力。这一次，技术颠覆了产品形态，OpenAI&nbsp;用行动给全世界的科技公司上了一课。并冲上国内微博热搜。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d6/d6c4510e0758652ba2ca8f85224f56be.webp" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/4b/4b8cac6bcd63a93d5b26fc9153e7a226.webp" /></p><p></p><p>在此次&nbsp;OpenAI&nbsp;仅有&nbsp;26&nbsp;分钟的春季发布会中，OpenAI&nbsp;首席技术官穆里·穆拉提（Muri&nbsp;Murati）宣布推出名为&nbsp;GPT-4o&nbsp;的新旗舰生成式&nbsp;AI&nbsp;模型，其集文本音频视觉于一身，能力全新升级。</p><p></p><p>此前不少爆料提到，OpenAI&nbsp;将推出&nbsp;AI&nbsp;搜索，与谷歌搜索竞争，从而增强&nbsp;ChatGPT&nbsp;的功能并开拓新市场，并称这款产品将在谷歌本周的开发者大会前推出。</p><p></p><p>不过，OpenAI&nbsp;CEO&nbsp;山姆·奥特曼对此否认，其表示，“不是&nbsp;GPT-5，也不是搜索引擎，但我们一直在努力开发一些我们认为人们会喜欢的新东西！对我来说就像魔法一样。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/5a/5ab1b458281db51bf7647a7be35b63d4.webp" /></p><p></p><p>然而就在本周，OpenAI&nbsp;官宣了&nbsp;Altman&nbsp;口中的“就像魔法一样”的东西。另外，在&nbsp;API&nbsp;中，GPT-4o&nbsp;的价格是&nbsp;GPT-4-turbo&nbsp;的一半，速度是&nbsp;GPT-4-turbo&nbsp;的两倍、5&nbsp;倍速率限制。</p><p></p><p>更多阅读：《<a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651205710&amp;idx=2&amp;sn=e9f5a3ad81cbfcdfa910ac97f0837f17&amp;chksm=bdbbc81d8acc410b1d5202d39a2c6c0f978066983311b77622495e97c07347353a6577426080&amp;scene=21#wechat_redirect">OpenAI&nbsp;官宣旗舰模型&nbsp;GPT-4o，完全免费、无障碍与人交谈！奥特曼：这是我们最好的模型</a>"》</p><p></p><h4>网传阿里巴巴员工贪腐案：一年受贿&nbsp;9200&nbsp;多万！</h4><p></p><p>5&nbsp;月&nbsp;14&nbsp;日早间，据央视新闻报道，浙江杭州警方侦破了一起民营企业内部腐败案件。王某是电商平台基础岗位的一名运营人员，他在短短一年的时间，收受商家贿赂高达&nbsp;9200&nbsp;多万元，受贿情节触目惊心。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0a/0ae7f4f0350231be15f3a6f2386192d2.webp" /></p><p></p><p>据警方调查，王某虽职位不高，却手握店铺入驻审批的关键权力。短短一年，王某伙同多人共同收受贿赂高达&nbsp;1.3&nbsp;亿余元，其中王某受贿金额为&nbsp;9200&nbsp;多万元，&nbsp;逐渐形成了一条倒卖家具类官方旗舰店指标的灰黑产业链。就在这时，王某所在电商平台收到的一封匿名举报信，揭开了这个黑灰产业链的冰山一角。</p><p></p><p><img src="https://static001.geekbang.org/infoq/22/2266cd14b1ab369533b8ad738bafec1a.webp" /></p><p></p><p>警方通过深入调查很快查清了王某等人的犯罪事实，但在对王某实施抓捕搜查时，警方并未在其家中发现这些巨额赃款。通过调查钱的去向，警方发现，为了逃避打击，王某在实施作案前就“做足了准备”。</p><p></p><p>报道还提到，随着案件的侦破，这家电商平台也被深深触动，他们没有想到一个基层的运营人员，竟然能够受贿如此之多。在警方的建议帮助下，该电商平台重新设计了审批流程，运用大数据技术进行分析判断，以减少人为因素的干扰。</p><p></p><p>对于该报道所提及的电商企业，有记者获悉为阿里巴巴，就此向阿里巴巴方面求证，至今阿里方面暂未公开回应。</p><p></p><h4>李开复：中国需要自己的&nbsp;ChatGPT，当下国内&nbsp;AI&nbsp;工具“都还不够好”</h4><p></p><p>5&nbsp;月&nbsp;13&nbsp;日消息，彭博社刊登了对李开复的专访，李开复认为中国需要自己的&nbsp;ChatGPT，以加快人们对人工智能的兴趣、采用和投资。</p><p></p><p>李开复谈到了“ChatGPT&nbsp;时刻”——&nbsp;对于美国人来说，“ChatGPT&nbsp;时刻”发生在&nbsp;17&nbsp;个月之前_（2022&nbsp;年&nbsp;12&nbsp;月，聊天机器人&nbsp;ChatGPT&nbsp;开始大火）_。但他认为，中国用户还没有迎来“ChatGPT&nbsp;时刻”，直到现在，国内的聊天机器人或工具“都还不够好”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b4/b4e8bc602951132ca0e0b64992db4246.webp" /></p><p></p><p>李开复披露了自家&nbsp;AI&nbsp;公司零一万物的近况：已经接近盈利。在对国内外的数据集进行模型训练之后，李开复正在将自家的模型和应用推向全球，并与国内外客户签约。“今年将是中国生成式&nbsp;AI&nbsp;的应用的爆发期。”但李开复也表示，“当&nbsp;GPT-5&nbsp;问世之后，我们将会落后一步。”</p><p></p><p>李开复称，自家的模型是在合法进入国内的&nbsp;H100s&nbsp;处理器上训练出来的。“需要是发明之母，我们从现有的计算能力中榨取一切可以榨取的东西。”</p><p></p><h4>谷歌云一键“删库”：波及&nbsp;50&nbsp;多万用户、崩溃一周</h4><p></p><p>近日，谷歌云全球首席执行官&nbsp;Thomas&nbsp;Kurian&nbsp;与澳大利亚非盈利性养老基金&nbsp;UniSuper&nbsp;的负责人联合发表声明，就&nbsp;UniSuper&nbsp;私有云账户因谷歌云服务的“错误配置”而被意外删除的事件，向&nbsp;UniSuper&nbsp;的&nbsp;62&nbsp;万名会员表达了“极其令人沮丧、极其令人失望”的歉意。</p><p></p><p><img src="https://static001.geekbang.org/infoq/df/df587506f78535ad758332f0e13d5ec3.webp" /></p><p></p><p>此次故障导致&nbsp;UniSuper&nbsp;基金的&nbsp;50&nbsp;多万会员自&nbsp;5&nbsp;月&nbsp;2&nbsp;日起，在整整一周内无法访问自己的退休金账户。尽管服务已于周四开始陆续恢复，但投资账户的余额数据仍需更新，以反映上周的金额。</p><p></p><p>事件发生后，UniSuper&nbsp;基金负责人及&nbsp;Google&nbsp;Cloud&nbsp;全球&nbsp;CEO&nbsp;发表了联合声明，声明中提到，此次中断源自配置错误所引发的&nbsp;UniSuper&nbsp;云账户意外删除，而这种情况在&nbsp;Google&nbsp;Cloud&nbsp;上从未发生过。UniSuper&nbsp;管理着约&nbsp;1250&nbsp;亿美元的资金，此次服务中断引起了业界的广泛关注和担忧，同时也对全球云服务的安全性和稳定性提出了质疑。谷歌云作为全球领先的云服务提供商，此次失误对其声誉造成了重大影响。</p><p></p><p>此次事件也提醒了全球云服务用户，注意数据安全和业务连续性计划的重要性。随着云服务的普及，如何确保服务的稳定性和安全性，已成为所有云服务提供商和用户必须共同面对的挑战。</p><p></p><p>更多阅读：《<a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651205504&amp;idx=1&amp;sn=4746861e0a6286762ccce9474da5ffdd&amp;chksm=bdbbd7d38acc5ec5902e1084327da1e425c218dadf43ebf2d704885439adb6f870a73706d444&amp;scene=21#wechat_redirect">谷歌云删库宕机一周：千亿基金数据和备份被删光，技术负责人当场被裁，谷歌最后只说一句&nbsp;Sorry？</a>"》</p><p></p><h4>微软因&nbsp;Cortana&nbsp;专利侵权被判赔偿&nbsp;2.42&nbsp;亿美元</h4><p></p><p>5&nbsp;月&nbsp;12&nbsp;日消息，据路透社报道，5&nbsp;月&nbsp;10&nbsp;日，美国特拉华州联邦陪审团裁定微软公司侵犯了&nbsp;IPA&nbsp;Technologies&nbsp;的一项专利，并勒令微软向其支付&nbsp;2.42&nbsp;亿美元_（当前约&nbsp;17.5&nbsp;亿元人民币）_的赔偿金。陪审团认为，微软的&nbsp;Cortana&nbsp;语音助手软件侵犯了&nbsp;IPA&nbsp;在计算机通信软件方面的专利权。</p><p></p><p>此次裁决源于&nbsp;IPA&nbsp;Technologies&nbsp;2018&nbsp;年提起的一场诉讼。IPA&nbsp;当时指控微软的语音识别技术侵犯了与其个人数字助理和语音数据导航相关的多项专利。经过审理，案件焦点最终集中于&nbsp;IPA&nbsp;的一项专利。微软方面辩称其并未侵权，该专利本身也无效。</p><p></p><p>IPA&nbsp;Technologies&nbsp;是专利许可公司&nbsp;Wi-LAN&nbsp;的子公司，Wi-LAN&nbsp;由加拿大科技公司&nbsp;Quarterhill&nbsp;和两家投资公司共同持有。据悉，该涉案专利由&nbsp;IPA&nbsp;从&nbsp;SRI&nbsp;国际的&nbsp;Siri&nbsp;公司收购，而苹果公司在&nbsp;2010&nbsp;年收购了&nbsp;Siri&nbsp;公司及其技术，并将其应用于自家的&nbsp;Siri&nbsp;语音助手。</p><p></p><p>对于这一裁决，微软发言人表示：「我们仍然坚信微软并未侵犯&nbsp;IPA&nbsp;的专利，并将对该判决提出上诉。」IPA&nbsp;和&nbsp;Wi-LAN&nbsp;的代表暂未对此作出回应。</p><p></p><h2>IT&nbsp;业界</h2><p></p><p></p><h4>Android&nbsp;15&nbsp;引入私人空间，盗窃检测和&nbsp;AV1&nbsp;支持</h4><p></p><p>近日，Google&nbsp;I/O&nbsp;开发者大会上谷歌预告了&nbsp;Android&nbsp;15&nbsp;的新功能，其中之一是私人空间（Private&nbsp;Space），应用抽屉将引入一个新的默认隐藏的部分，用于容纳敏感应用，访问该部分需要第二轮的锁屏身份验证，该验证可以与主屏幕的锁屏身份验证不同。类似工作类应用，隐藏类的应用在独立的配置文件上运行。</p><p></p><p>对系统而言，这些应用由不同的“用户”使用不同的数据运行，非私人的应用无法访问这些数据。当用户锁定私人空间时，配置文件将暂停，应用将停止活动，不会显示通知。</p><p></p><p>另一项功能是盗窃检测锁，使用加速计和&nbsp;Google&nbsp;AI&nbsp;感知是否有人从使用者手中抢走手机并试图带着它逃跑、骑车或开车逃离。任何类似盗窃的震动都会使手机自动锁定。Android&nbsp;15&nbsp;还加入了&nbsp;AV1&nbsp;编码器的软件解码。</p><p></p><h4>苹果新一代&nbsp;iPad&nbsp;Pro&nbsp;被发现存在渲染失常问题</h4><p></p><p>5&nbsp;月&nbsp;14&nbsp;日消息，据外媒报道，按计划，苹果公司在&nbsp;5&nbsp;月&nbsp;7&nbsp;日晚间的新品发布会上推出的首代&nbsp;OLED&nbsp;屏&nbsp;iPad&nbsp;Pro，将在本周三上市。</p><p></p><p>但同苹果公司此前推出的新品一样，即将上市的&nbsp;iPad&nbsp;Pro，也存在一些问题，有外媒在评测中就已发现了渲染失常问题。从报道来看，有一家专注于苹果产品的外媒在初期的评测中发现，部分特定的&nbsp;HDR&nbsp;内容在&nbsp;iPad&nbsp;Pro&nbsp;上播放时，显示为斑点或白色条文，出现在特定的蓝色色调中，例如海军蓝或靛蓝。</p><p></p><p>此外，发现新一代&nbsp;iPad&nbsp;Pro&nbsp;渲染失常的这家外媒也表示，这一问题只有在特定的环境中才能被发现，在&nbsp;iPhone&nbsp;15&nbsp;Pro&nbsp;等其他的苹果&nbsp;OLED&nbsp;屏产品上并未出现，目前也还不清楚为何会在&nbsp;iPad&nbsp;Pro&nbsp;上出现。</p><p></p><p>同此前在硬件产品中发现的问题一样，新一代&nbsp;iPad&nbsp;Pro&nbsp;渲染失常的这一问题，也将通过后续的软件升级修复。苹果已经告知外媒注意到了他们所发现的问题，正在通过软件升级修复。</p><p></p><h4>Reddit&nbsp;与&nbsp;OpenAI&nbsp;达成内容授权协议</h4><p></p><p>在&nbsp;Google&nbsp;之后，社媒平台&nbsp;Reddit&nbsp;与&nbsp;OpenAI&nbsp;达成了内容协议，这一消息推动其股价上涨逾十分之一。根据该协议，OpenAI&nbsp;将获得&nbsp;Reddit&nbsp;内容的访问权限，同时它将为&nbsp;Reddit&nbsp;提供&nbsp;AI&nbsp;驱动功能。和&nbsp;Stack&nbsp;Overflow&nbsp;类似，Reddit&nbsp;的内容都是用户创造和管理的，它的高质量内容应该早就被&nbsp;OpenAI&nbsp;抓取并被用于训练大模型。OpenAI&nbsp;等&nbsp;AI&nbsp;公司正面临来自众多版权所有者的诉讼，通过与&nbsp;Reddit&nbsp;等公司达成协议，AI&nbsp;公司正试图合法化其训练数据。</p><p></p><p>另外，截至目前，Reddit&nbsp;已经累计签署了价值&nbsp;2.03&nbsp;亿美元的授权协议，包括年初和&nbsp;google&nbsp;的&nbsp;6000&nbsp;万合同，这些合同协议期限从&nbsp;2&nbsp;年到&nbsp;3&nbsp;年不等，并且正在谈判达成更多的授权协议。</p><p></p><h4>百度知名开源项目&nbsp;ECharts&nbsp;创始人“下海”&nbsp;养鱼</h4><p></p><p>媒体近日报道称，ECharts&nbsp;创始人林峰已投身农业&nbsp;——“下海”&nbsp;养鱼并养出了顶流。</p><p></p><p>Apache&nbsp;ECharts&nbsp;是一款基于&nbsp;JavaScript&nbsp;的开源可视化图表库，最初由百度团队开源，并于&nbsp;2018&nbsp;年初捐赠给&nbsp;Apache&nbsp;基金会，成为&nbsp;ASF&nbsp;孵化级项目。</p><p></p><p>据都市快报消息，林峰就职的一米八海洋科技，核心创始团队大多来自蚂蚁和阿里，主打贻贝和大黄鱼两样海产品。1&nbsp;号员工为前阿里员工胡晓明（花名：孙权），林峰是&nbsp;3&nbsp;号员工，也是创始合伙人。在正式加入一米八海洋科技之前，林峰做过百度工程师，也自己创过业，2016&nbsp;年加入蚂蚁集团后，成为蚂蚁集团中台产品体验技术和数据可视化方向负责人，带着几百人团队。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2f/2f8499a4c9dac8df6c6e549a3ef11319.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ZFjmsScSL6syvApMOZKw</id>
            <title>大佬都在讨论AGI，行业应用究竟如何？一篇报告带你拆解五大行业 50+ 场景应用现状</title>
            <link>https://www.infoq.cn/article/ZFjmsScSL6syvApMOZKw</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ZFjmsScSL6syvApMOZKw</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 May 2024 02:13:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工通用智能, AGI, 应用场景, 中国行业
<br>
<br>
总结: 随着人工智能技术的不断进步，人工通用智能（AGI）在中国行业中的应用场景成为热议焦点。通过分析现有的应用案例，揭示AGI技术在实际业务场景中的具体应用程度和潜在价值。InfoQ研究中心通过报告详细审视AGI在营销、金融、教育、零售以及企业服务等关键行业领域的应用情况，展示中国AGI的发展现状和企业布局。 </div>
                        <hr>
                    
                    <p>随着人工智能技术的不断进步，人工通用智能（AGI）这一概念已经成为科技界和产业界热议的焦点。InfoQ研究中心，作为一直关注AI、大模型及其商业应用的研究机构，本次通过报告的形式，分析研究中国AGI的技术架构，详细审视AGI在当前市场中的应用情况，特别是在营销、金融、教育、零售以及企业服务等关键行业领域。通过分析现有的应用案例，揭示AGI技术在实际业务场景中的具体应用程度和潜在价值。</p><p>本篇文章将立足InfoQ研究中心刚发布的<a href="https://www.infoq.cn/minibook/6WyXxdu179Di1O75JPUM">《中国AGI市场发展研究报告&nbsp;2024》</a>"，说明一些现象，也提出一些问题。关于问题的解答，欢迎大家点击<a href="https://www.infoq.cn/minibook/6WyXxdu179Di1O75JPUM">「链接」</a>"，下载完整报告阅读。</p><p></p><h4>营销、零售、金融、教育、企业服务五大行业&nbsp;AGI&nbsp;先行</h4><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/b0/b02c4f1acbb8a4fe2579c9496929cc74.png" /></p><p></p><p>InfoQ研究中心在研究过程中发觉，目前各行业数字基础不同，应用场景需求紧迫性不同，因此AGI在各行业应用程度也不同。因此根据探索时间、应用成果、用户反馈等内容，并结合专家访谈，InfoQ研究中心将AGI在各行业的应用生命周期划分为四个阶段：应用探索期、产品测试期、市场投放期和应用成熟期。</p><p>整体来看，营销、零售、金融、教育、企业服务场景探索早、成果多，但现阶段尚未形成完全成熟的应用。五大行业究竟应用场景如何？有哪些应用成果积累？又有哪些厂商已经躬身入局了？欢迎大家点击「阅读原文」，下载完整报告阅读。</p><p>此外，InfoQ研究中心也根据五大行业&nbsp;50+&nbsp;应用场景拆解了具体的企业图谱，以更好地展现现在中国AGI的探索现状和企业布局。</p><p></p><h5>中国行业AGI应用全景图</h5><p></p><p><img src="https://static001.geekbang.org/infoq/b0/b05254d210092b021be798bc80d83ed9.png" /></p><p></p><h4>以教育场景为例，AGI&nbsp;三大有效能力渗透学生、教师和学校三方教育场景</h4><p></p><p>教育领域中，AGI在学校及教师侧的应用都还在非常早期，这主要受到智慧校园/教师的整体解决方案的成熟度，以及大模型应用面临技术集成、数据管理以及内容生成质量和匹配性的难题。</p><p>学生侧目前的应用大多都还在单点场景进行探索，例如作文辅导、英语口语等，这些场景的需求较为明确，且与大模型在语言生成的能力提升适配度较高，因此这些场景应用程度相对较高。但像个性化学习这类全流程型的应用，仍处于非常早期。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5b/5b84f4cb92a78f45551ba5e941ce0282.png" /></p><p></p><p></p><h4>中国&nbsp;AGI&nbsp;十大潜力发展场景研判</h4><p></p><p>InfoQ研究中心根据过往研究成果和积累，对于现有的应用场景进行了分析判断。选择出了企业内外部应用场景中的十大潜力发展场景。企业内部应用中，内容生成类包含营销中的物料生成、企业服务中的协同办公和辅助编程、游戏场景中的智能游戏NPC；专家类包含企业服务中的数据分析和知识查询，以及金融场景中的知识库。对完场景中包含零售场景中的数字人导购/直播、教育中的智慧硬件，以及营销和零售场景中的智能投放。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d18affcdf7e5c68ee0d1ca7ec1bf3db0.png" /></p><p></p><p>各行各业都在被AGI改造，InfoQ研究中心也期待同大家一起，共同探索和解密中国AGI的发展。</p><p>更多关于中国&nbsp;AGI&nbsp;发展历程、市场规模、技术架构等内容，欢迎大家点击<a href="https://www.infoq.cn/minibook/6WyXxdu179Di1O75JPUM">「链接」</a>"，下载完整报告阅读。同时，您也可以点击<a href="https://www.infoq.cn/theme/191">「专题链接」</a>"直达50+ InfoQ研究中心过往研究成果~</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/XNDSks1uepQbyGJStGF9</id>
            <title>豆包大模型家族发布、火山方舟升级，火山引擎如何打造全栈AI技术服务？</title>
            <link>https://www.infoq.cn/article/XNDSks1uepQbyGJStGF9</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/XNDSks1uepQbyGJStGF9</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 May 2024 10:33:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数字化浪潮, 大模型, AI模型应用, 火山引擎
<br>
<br>
总结: 在当前数字化浪潮中，大模型作为推动业务创新的引擎，企业对高效、经济的AI模型应用需求迫切。然而，大模型应用面临着效果不佳、高成本和难以落地等挑战。火山引擎在大会上发布了豆包大模型家族和火山方舟2.0等创新产品，以超低价定价为企业市场提供模型服务，致力于打造全栈AI技术服务生态。豆包大模型在内部业务和企业端场景中持续进化，为企业智能化升级提供支持。 </div>
                        <hr>
                    
                    <p>在当今的数字化浪潮中，大模型以其卓越的语言理解和生成能力，正成为推动业务创新的重要引擎。随着“模型即服务”在云服务领域的崛起，企业对于高效、经济的 AI 模型应用的需求日益迫切。然而，现实中大模型的应用并非一帆风顺，企业在尝试将其融入业务流程时，往往面临着效果不尽人意、成本高昂以及落地难度大等诸多挑战。</p><p></p><p>在这样的背景下，大模型的实践应用不应仅仅是市场的“噱头”，企业客户迫切需要真正“好用、能用、有用”的产品和服务。更重要的是“性价比”，产品能力固然重要，但在市场竞争如此激烈的今天，价格已经成为了企业客户决策的第一要素。</p><p></p><p>5 月 15 日，2024 火山引擎 FORCE 原动力大会上，火山引擎重磅发布了豆包大模型家族和火山方舟 2.0 等一系列创新产品，并且宣布豆包主力模型在企业市场的定价为 0.0008 元 / 千 tokens，0.8 厘就能处理 1,500 多个汉字，比行业便宜 99.3%，是当之无愧的“超低价”。</p><p></p><p>“大模型的超低定价，来源于我们在技术上有信心优化成本。” &nbsp;火山引擎总裁谭待在大会上表示，技术上的优势，为火山引擎提供了定价的底气。火山引擎不仅想为企业提供模型服务，更致力于打造一个完备而有效的全栈 AI 技术服务生态，为企业提供模型服务的全链路解决方案。</p><p></p><p>那么，相对于市面上的其他产品，字节跳动的模型产品和火山引擎的模型服务到底有何优势？火山引擎又将如何打造全栈模型服务能力，为企业的智能化升级提供支持？</p><p></p><p></p><h2>模型应用进化，实践是最好的磨刀石</h2><p></p><p></p><p>相比市面上大多数 AI 产品的“大张旗鼓”，字节跳动的 AI 产品一直保持着低调作风，但不知不觉中，以豆包 App、扣子为代表的字节系 AI 应用已然成为了用户的热门选择。</p><p></p><p>以豆包 App 为例，自 2023 年 8 月上线以来，就攻陷了各大应用市场的下载榜单，据悉，截止目前，豆包 App 下载量已经超过 1 亿，桌面端 +App 的月活用户数量已经达到了 2600 万，有超过 800 万个智能体被创建，是当之无愧的明星 AI 产品；扣子也早已在海内外打出名气，凭借超强的扩展性和性价比攻城略地。能在火热的 AI 应用市场中脱颖而出，并在大众对于 AI 认知趋于理性后，仍旧保持着高访问量、调用量和活跃度，豆包 App 和扣子的“实力”有目共睹。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ee/eee466aa32d8a9a0b8334a8719ce8216.webp" /></p><p></p><p>这直接戳中了当下 AI 应用普遍面临着的痛点：不怕不好用，只怕没人用。基于大模型产品的技术特性，只有最大的使用量，才能打磨出最好的模型，只有最多的实践数据，才能催动“智能涌现”的发生，而在庞大使用量和实际场景的锻炼下，应用才能更好地满足用户的使用需求，实现持续迭代。</p><p></p><p>遗憾的是，对于很多产品来说，没有 GPT 那样的顶流地位，想实现“应用进化”非常艰难。</p><p></p><p>而作为行业中的“标杆案例”，豆包大模型就是在“千锤百炼”下长成的。现如今，豆包大模型平均每天处理 1200 亿 tokens（约 1800 亿汉字），生成 3000 万张图片，对于火山引擎和豆包大模型来说，模型服务早已不是“纸上谈兵”，其正在用户、企业的实际应用中不断迭代与进化。</p><p></p><p>首先是字节跳动内部 50+ 业务的持续打磨。基于字节跳动庞大的业务生态，豆包大模型光是在内部就能接触到足够丰富的业务场景。据悉，豆包大模型不仅参与到办公智能助手、数据智能分析、编程助手等企业内部的办公开发场景，还覆盖了电商导购、售后客服、营销创作等前端对客场景，在字节跳动 50 余个实际业务的打磨之下，豆包大模型被应用在一线使用场景中，在字节系产品的庞大用户量、数据量的催化下，豆包大模型得以快速迭代。</p><p></p><p>除了内部打磨、C 端实践，企业端场景的应用至关重要，这决定着模型是否能真正成为“生产力工具”。现如今，豆包大模型已经被广泛应用于火山引擎的企业服务中，覆盖了智能终端、汽车、金融、消费等多个重要行业，更多触达了 OPPO、vivo、小米、荣耀、三星、华硕、招行、捷途、吉利、北汽、智己、广汽、东风本田、海底捞、飞鹤...... 等知名企业。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9c/9c4f3d25570c62e9ba2d07c8f8db6730.webp" /></p><p></p><p>大会上，火山引擎与企业客户宣布共同成立两大模型联盟：智能终端大模型联盟与汽车大模型生态联盟，在智能终端 AI、汽车全场景 AI 等领域进一步展开探索，这也让豆包大模型的未来发展充满了更多可能性。</p><p></p><p></p><h2>豆包大模型家族发布：多元场景、安全可控</h2><p></p><p></p><p>实践应用是模型能力进化的关键环节，但如何为模型服务开拓更多用户、寻找更多落地机会，则需要更深入的思考。不得不承认，当前企业客户对于模型服务存在一种“为了用而用”的误区，这往往导致模型服务与实际需求之间出现偏差。当需求得不到满足时，对前沿技术的过度依赖也可能变成一种资源浪费，这不利于整个行业的可持续发展。</p><p></p><p>对于火山引擎等行业内的领军企业来说，谁能真正洞察企业客户的深层需求，谁就能占据有利地位。从当前云计算与智能化融合的趋势来看，AI 及大模型若想助力企业业务创新升级，主要需要实现三大目标：</p><p></p><p>利用 AI 打造差异化优势，提升业务场景的创新能力，助力用户体验升级。降低成本、提高效率，通过智能化手段提升业务效率，加快决策和工作流程。在满足企业多样化需求的同时，确保模型服务的安全性和稳定性。</p><p></p><p>能满足上述三大目标的模型服务产品，不仅要有模型本身的卓越性能，还需要满足可用性、易用性、成本可控、安全合规等需求。这就要求模型服务产品本身不能有明显的缺陷。</p><p></p><p>或许正是基于这样的洞察，字节跳动选择推出豆包大模型家族，加持火山引擎的模型服务能力。</p><p></p><p>首先，模型的性能效果仍然是核心。正如上文所述，豆包大模型在模型效果上实现了显著提升。以字节跳动自研的 LLM 模型专业版“豆包通用模型 pro”为例，其最大窗口尺寸可达 128K，且全系列可精调，具备强大的理解、生成、逻辑和记忆能力，适用于问答、摘要、创作、文本分类、角色扮演等通用场景，功能全面。</p><p></p><p>针对不同的业务场景和多模态需求，豆包大模型也实现了进一步的进化。除了通用模型，还包括 5 秒即可实现声音 1:1 克隆的声音复刻模型、具有超自然语音合成能力的语音合成模型、准确率极高的语音识别模型、扣子背后的主力模型 Function Call，以及角色扮演模型、文生图模型、向量化模型等。它们共同构成了豆包大模型家族，旨在满足各行业、多元场景的服务需求。</p><p></p><p>针对企业的个性化需求，豆包的主力模型提供了通用全面的 pro 版本和低延迟、高性价比的 lite 版本，企业可以根据自身需求灵活选择。同时，全系列语言模型均支持继续预训练或 SFT 精调，使企业能够基于自身业务场景，自主开发更适配的 AI 应用。豆包将模型精调和预训练的能力赋予客户，使企业能够实现“一个模型，多元应用”。</p><p></p><p>更重要的是，这一次豆包大模型真正做到了“人人用得起”，豆包主力模型在企业市场的定价只有 0.0008 元 / 千 tokens，0.8 厘就能处理 1,500 多个汉字，比行业便宜 99.3%。在这样的“卷”的价格之下，企业可以真正做到降本增效，用低成本创造新价值。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3b/3b3e44f5a0dd0054d779fcb81f23f1f8.webp" /></p><p></p><p>大会上，火山引擎总裁谭待表示，有信心通过技术优化降低成本。例如通过对模型结构的优化调整、在工程上从以前的单机推理演进到现在的分布式推理、把不同负载的推理混合调度，这些技术能够把各种各样底层算力用得更好，同时实现大幅的降低成本，让每一家企业都能用得起大模型。</p><p></p><p>在金融、科技等行业客户极为关注的安全问题上，豆包大模型作为首批通过大模型服务安全备案的产品，满足了合规性需求。在火山方舟平台全周期安全可控方案的支持下，豆包大模型在数据加密传输、信息内容安全、防止恶意攻击和数据泄露等方面提供了有力保障，让企业能够放心使用。</p><p></p><p></p><h2>从应用到平台，火山引擎全栈模型服务是如何炼成的？</h2><p></p><p></p><p>豆包大模型家族已经为火山引擎的企业客户提供了强大的模型服务解决方案，但这只是一个开始。火山引擎的终极目标是通过其大模型云计算能力，全面赋能企业，助力其在 AI 时代实现数字化与智能化的升级。这一愿景在火山方舟 2.0 的推出中得到了充分体现。</p><p></p><p>作为一站式大模型服务平台，火山方舟 2.0 在性能和系统承载力方面实现了显著提升。平台拥有海量资源，能够通过资源潮汐调度保障流量高峰时业务的稳定性。同时，其瞬时可用的特性，使得创建模型接入点后 5 秒即可使用，大大提升了业务的响应速度。极致弹性的扩缩容能力，为企业有效支撑突发流量和业务高峰提供了保障，同时降低了成本。</p><p></p><p>在企业 AI 应用的稳定性和成本控制方面，火山方舟 2.0 为企业级 AI 应用的落地提供了坚实的基础。而三大核心插件则进一步加速了企业 AI 应用的产出与创新。</p><p></p><p>联网插件提供了与头条、抖音相同的搜索能力，结合多模态交互方式和领先的意图识别技术，大幅提升了模型的信息获取能力。内容插件则依托字节跳动体系的海量内容资源，通过基于意图的垂直内容信息检索，提供了内容时效性更强的解决方案。RAG 知识库插件以其毫秒级的高性能检索和流式知识库索引更新，降低了企业在使用 AI 模型时的“幻觉”，提升了应用的实用性。</p><p></p><p>扣子专业版的推出，使得企业或创业者可以接入更多高级特性，享有企业级的各项能力。在扣子原有功能基础之上，扣子专业版提供企业级性能的智能体运行时。保障各项服务 SLA，包括但不限于并发量、响应时长等。并开放 SSO、组织权限管理等企业特性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/56/56f69dc96c862b05ef2eaecbf640331f.webp" /></p><p></p><p>可以预见，未来将有越来越多的企业利用扣子专业版进行多场景的开发，非编程人员也可以更好地使用上 AI，为工作全流程进行提效。除此之外，火山引擎还推出了智能创作云 2.0，发布了智能数据洞察 AI 助手 ChatBI、销售 AI 助手 Sales Copilot 等 AI 应用，帮助企业快速实现 AI 升级。</p><p></p><p>全栈 AI 技术服务生态的构建不仅仅在于模型服务本身和 AI 应用开发，基础设施的匹配升级同样重要。在这次升级中，火山引擎还对旗下云底座进行了全面升级。会上，火山引擎全新发布了混合云 veStack 智算版，其拥有着万卡集群组网，3.2T 高性能无损网络的超大规模优势、可以实现 97.78% 训练加速比和分钟级故障发现和自愈的极致性能，还能够适配十余种 GPU 和主流国产化 GPU，应对本地部署需求。</p><p></p><p>通过这些升级，火山引擎展现了其全栈 AI 技术服务生态的构建能力。从基础设施到模型即服务（MaaS）、模型应用，火山引擎的技术服务生态为企业提供了全方位的支持，帮助企业在 AI 时代实现数字化与智能化的升级，推动业务的持续增长和创新。</p><p></p><p>火山引擎的这一系列动作，无疑是对 AI 技术应用的一次深刻洞察和前瞻性布局。火山方舟 2.0 的推出，不仅为企业提供了更加强大和灵活的 AI 应用开发平台，更是在推动整个行业向更高效、更智能的方向发展。随着火山引擎全栈 AI 技术服务生态的不断完善，我们有理由相信，它将为企业带来更多的可能性，开启 AI 技术应用的新篇章。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/6i9qbIGk02IdUiAfNSqi</id>
            <title>AICon 2024 重磅开幕！60 余位大咖干货集结：20 年来云首次革命性变化、大模型才刚刚开始……</title>
            <link>https://www.infoq.cn/article/6i9qbIGk02IdUiAfNSqi</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/6i9qbIGk02IdUiAfNSqi</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 May 2024 09:16:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AICon, 大会议题板块, 嘉宾阵容, 大模型应用生态展
<br>
<br>
总结: 5 月 17 日，由极客邦旗下 InfoQ 中国倾力打造的 AICon 全球软件开发大会暨智能软件开发生态展在北京正式开幕，会场内人头攒动，盛况空前！演讲嘉宾阵容强大，既有行业领军人物分享战略远见，也有技术大咖深入剖析最新成果，到场的每一位观众都受益匪浅。本次大会设置了丰富的 14 大议题板块，涵盖 AI Agent、RAG 检索生成技术、企业级生成式 AI 助手 Amazon Q、Copilot 辅助程序开发、大规模模型的训练与推理优化策略等内容，同时还特别策划了首届大模型应用生态展，让现场参会者进深入了解并沉浸式体验生成式 AI 在未来的无数可能。 </div>
                        <hr>
                    
                    <p>5 月 17 日，由极客邦旗下 InfoQ 中国倾力打造的AICon 全球人工智能开发与应用大会暨大模型应用生态展在北京正式开幕，会场内人头攒动，盛况空前！演讲嘉宾阵容强大，既有行业领军人物分享战略远见，也有技术大咖深入剖析最新成果，到场的每一位观众都受益匪浅。</p><p></p><p>本次大会设置了丰富的 14 大议题板块，涵盖 AI Agent、RAG 检索生成技术、企业级生成式 AI 助手 Amazon Q、Copilot 辅助程序开发、大规模模型的训练与推理优化策略、基础设施搭建、LLMOps 实践、多模态大模型研究、大模型与行业创新应用融合、AI 最前沿的探索领域，以及针对大模型在全球范围内的机遇与 AI Agent、RAG 检索与生成、Copilot 应用构建、大模型训练以及推理优化、基础设施构建、LLMOps、多模态大模型、大模型 + 行业创新应用、AI 前沿探索以及大模型全球化机会和挑战等。</p><p></p><p>超过 60 位来自 Google、微软、字节、阿里、科大讯飞、智谱、亚马逊云科技、月之暗面、MiniMax、无问芯穹、Lepton AI、数势科技、北京智源人工智能研究院、腾讯等行业头部企业的嘉宾将齐聚一堂，在现场带来精彩纷呈的见解与分享。</p><p></p><p>除此之外，大会还特别策划了首届大模型应用生态展，邀请众多致力于 AI 和大模型行业落地应用探索，有实践、有创新、有成果的企业，将应用案例和创新产品搬到 AICon 现场，让现场参会者进深入了解并沉浸式体验生成式 AI 在未来的无数可能。</p><p></p><p></p><h2>开幕精华：洞悉行业变迁</h2><p></p><p></p><p>本次大会于今日上午 9 点正式开幕，极客邦科技 / 事业合伙人、InfoQ 极客传媒 &amp; 极客时间企业版总经理汪丹（Yolanda）为大会致开幕辞。她首先阐述了这一年 InfoQ 围绕生成式 AI 和大模型技术发展所展开的内容工作和现有成果，接着介绍了今年 AICon 大会的所有看点，包括精彩议题和现场丰富的体验及试驾活动。现在大语言模型对不少业务来说已足够智能，而生成式 AI 的落地关键在于数据战略、大模型选择和实现方式。经过一年多的发展，中国生成式 AI 领域涌现出了不少优秀的企业和案例。</p><p></p><p><img src="https://static001.geekbang.org/infoq/19/19ec38f54c3855e0e77b8b9f65ef5be2.jpeg" /></p><p></p><p>继 2020 年正式推出中国技术力量年度榜单品牌之后，今年 InfoQ 再次面向 AIGC 赛道推出【中国技术力量 2024 之 AIGC 先锋榜】。现场，汪丹正式揭晓了榜单结果。经过对来自互联网、金融、通信、制造、教育等众多领域的多轮优秀案例评选，30 家杰出企业脱颖而出。</p><p></p><p>其中，凭借各自的优秀创新实践案例上榜【AIGC 最佳实践案例 TOP20】的企业，包括快手、作业帮、网易数帆、阿里云函数计算团队、蚂蚁科技集团股份有限公司、顺丰科技、李白人工智能实验室、360 集团、上海笑聘网络科技有限公司、北京文因互联科技有限公司、中国人民人寿保险股份有限公司、北京衡石科技有限公司、吉利汽车集团、深圳前海百递网络有限公司、德邦证券股份有限公司、杭州卓印智能科技有限公司、杭州座头鲸科技有限公司、上海蜜度科技股份有限公司、中国联合网络通信有限公司上海市分公司、深智透医疗科技发展（上海）有限责任公司。</p><p></p><p>而在进行技术攻坚性、方案成熟度、标杆客户案例、客户服务能力等多维度的评分后，数势科技、网易 CodeWave、北京白海科技有限公司、北京潞晨科技有限公司、硅基流动、容联云、优刻得科技股份有限公司、智子引擎、南京柯基数据科技有限公司、江苏汇智智能数字科技有限公司上榜【AIGC 最佳技术服务商 TOP10】。</p><p></p><p>随后，InfoQ 研究总监兼首席分析师姜昕蔚正式发布《中国 AGI 市场发展研究报告 2024》，并对报告进行了详细解读。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ec07966d0455da22e10ccd4f37094c8e.jpeg" /></p><p></p><p>报告指出，目前各行业数字基础不同，应用场景需求急迫性不同，因此 AGI 在各行业应用程度也不同。InfoQ 研究中心根据探索时间、应用成果、用户反馈等内容，并结合专家访谈，将 AGI 在各行业的应用生命周期划分为四个阶段：应用探索期、产品测试期、市场投放期和应用成熟期。</p><p></p><p>整体来看，营销、零售、金融、教育、办公场景探索早、成果多，但现阶段尚未形成完全成熟的应用。营销行业 AGI 将在四个方面引领变革，包括革新内容的创造过程和效率、改变流量的分配和获取方式、提升服务体验、降低商业洞察门槛并颠覆市场研究模式。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0d/0daa3ecedaaeb22d7af469bdcb7d14d4.png" /></p><p></p><p>零售场景中，围绕效率提升和体验优化，AGI 本轮生成能力的升级促进了 AI 商拍、营销物料生成等全新场景的诞生和发展。同时，围绕 Agent 驱动的商家助手和智能投放，各家电商平台也正在频繁发布更新。</p><p></p><p>金融行业整体处于应用探索期，正逐步向产品测试期迈进。绝大部分中小型金融机构尚未找到大模型与业务的融合点，对大模型应用处于观望阶段或仅将大模型产品应用于通用业务场景中。部分头部金融机构积极创新，不仅能通过大模型产品解决通用业务问题，还应用于解决非决策类业务问题。个别大型新兴金融科技公司已推出 AI Agent 产品或相关框架，即将迈进市场投放期。</p><p></p><p>企业服务场景中，文本总结、知识查询等协同办公相关的应用，由于需求明确、同 AGI 现阶段的能力适配性高，发展较为迅速。企业资源管理、供应链管理等涉及多个模块，技术更为复杂且安全性与可靠性要求较高，所以应用程度相对较低。</p><p></p><p>教育领域中，受智慧校园 / 教师的整体解决方案的成熟度、大模型应用面临技术集成、数据管理以及内容生成质量和匹配性影响，AGI 在学校及教师侧的应用都还处于非常早期的阶段。目前，学生侧的应用大多都还在单点场景进行探索，如作文辅导、英语口语等场景的需求较为明确，且与大模型在语言生成的能力提升适配度较高，因此应用程度相对较高；但像个性化学习这类全流程型的应用，同样仍处于早期阶段。</p><p></p><p></p><h2>主题演讲：把握技术创新潮流</h2><p></p><p></p><h4>汪玉教授：《可持续的智能：大模型高能效系统前瞻》</h4><p></p><p></p><p>在首场主题演讲中，清华大学电子工程系教授、系主任兼无问芯穹发起人汪玉探讨了大模型高能效系统的未来。他表示 AI 算法算力需求激增，硬件系统的能耗开销可能导致算力供不应求与能源使用的不可持续。在 AI 2.0 时代，生成式任务的智能算法模型规模扩大，对算力及能量的需求急剧增加。如何使用软硬件协同优化加速大模型计算、降低推理成本，成为大模型设计范式的研究重点。汪玉介绍，利用算法数据特征，面向算法模型、数据结构、数据表示、计算图进行算法电路协同设计的方法，可在保证准确率的前提下优化速度与能效，并展示了其团队如何实现全球首个单块 FPGA 上的 7B 大语言模型高效推理。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d3/d30a4d890c84d34e15cbe060757ed34f.jpeg" /></p><p></p><p>汪玉还介绍，他发起的 AI Infra 公司无问芯穹正在产业中实践相关能效方案，并针对部分芯片产能不够的问题，开发了多种不同芯片混合训练的框架。目前已经支持六种不同芯片两两组合间的百卡级别异构混合训练，接下来将支持千卡混训。由于多元芯片性能差别较大，如何在不同芯片之间进行训练负载分配，成为了混训中的一个重要课题。无问芯穹基于自研的一种预测误差小于 3% 的训练性能预测工具，可以实现对不同芯片、不同模型结构的训练性能精准预测，从而实现在多元芯片之间的负载自动切分，提升训练效率。当前，无问芯穹的 Infini-AI MaaS 平台已支持了 30 多个主流开源模型、1 个智谱闭源模型和 8 个芯片品牌，并有望于年底实现从多模型到多硬件的自动路由。结合其底层软硬件协同设计与多元芯片兼容能力，可持续加速大模型计算、降低推理成本。</p><p></p><p>此外，清华电子系孵化的公司清鹏智能也正在以自研的能源大模型为核心就能源与算力融合发展整体解决方案做相关研究。“算力本身的耗能属性需要能源的保障，同时算力的发展能够反哺能源产业进行数字化升级，在一定程度上决定了智能的可持续发展潜力。”汪玉表示，从面向智能的软硬件协同设计出发，构建 AI 2.0 时代的算力生态，促进算电双力深度融合，可为大模型的可持续发展筑稳根基。</p><p></p><p></p><h4>贾扬清：《从互联网到 AI：云产业的重构和演进》</h4><p></p><p></p><p>紧接着， Lepton AI 联合创始人兼 CEO 贾扬清讲述了 AI 领域最近一年的趋势和自身的感悟，不仅对互联网到人工智能的转型过程进行回顾，还展望了云产业与 AI 融合的新时代。他表示，AI 已经成为 IT 策略的第三个核心支柱，正极大地促进芯片和云领域的创新。在如今 AI 和大语言模型流行的情况下，很多原本需要整个工程师团队完成的功能任务可以在非常短的时间被创造完成。</p><p></p><p><img src="https://static001.geekbang.org/infoq/24/24d4a65e6c686a538f2a39a944608047.jpeg" /></p><p></p><p>同时，贾扬清指出，大模型实际应用中，在公域和私域面对的设计场景是不一样的，对企业来说重要的是找到可以把业务需求和 AI 能力结合起来的方法论。而小模型在企业应用中有很大潜力，不仅便宜、可定制化，而且微调模型在垂直领域能够达到比通用大模型更好的效果。</p><p></p><p>“云价值主张开始有巨变，只有高性能计算硬件和云原生软件相结合，才能保证 AI 性能。”在贾扬清看来，一个优秀的云化台相较开源 LLM 推理性能提升 3-5 倍，比公共云 GPU 产品更具成本效益，且使开发人员的效率更高。“这是 20 年来，第一次云的基础架构产生革命性的变化。”贾扬清说道。</p><p></p><p></p><h4>黎科峰博士：《大模型时代，基于 AI Agent 的数据分析与决策新趋势》</h4><p></p><p></p><p>大模型和 AI Agent 是否会颠覆 To B 软件？现场，数势科技创始人兼 CEO 黎科峰博士分享了大模型时代下基于 AI Agent 的数据分析新趋势，以及大模型技术在企业数字化转型中的关键作用。首先，他谈到了大模型和 AI Agent 在企业中的落地场景，包括业务分析、内容生成、企业知识库和风控等专业领域。接着，黎博士指出，作为企业经营的“眼”和“脑”，企业数据分析与决策要经历过往、当前、未来三个阶段：数据从结构化数据到加上部分非结构化数据，再发展到结合行业知识 / 数据；使用人群从数据工程师到业务决策者，最后发展到业务全员。“</p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/ca8c14d4208e68fe1f785a1b6c033143.jpeg" /></p><p></p><p>大模型和 Agent 的出现，推动了企业数据分析与决策的范式变革。”黎博士表示。同时，企业也要认识到，智能分析 AI Agent 还存在几个要解决的关键问题，包括数据准确性、数据源的全面性、人际沟通的准确性和体验感、产品的智能性、以及数据计算查询效率及性能问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c4/c462b893aaff7cd35a199560498e947a.png" /></p><p></p><p>此次大会现场，数势科技正式发布智能分析助手 SwiftAgent 2.0，全面解决上述问题，帮助企业实现数据现状 - 数据资产 - 洞察和归因 - 智能决策的完整闭环，有效释放数据价值。</p><p></p><p></p><h4>林咏华：《大模型背后的荆棘之路》</h4><p></p><p></p><p>北京智源人工智能研究院副院长兼总工程师林咏华带来了以“大模型背后的荆棘之路”为主题的演讲。她表示，大模型一年，AI 开源社区受到前所未有的关注和使用。首要问题是，选择哪个基座模型？当前评测技术的发展跟不上大模型的发展速度，且用于比较大模型性能的各种榜单容易激发各种争议，主要存在的评测问题有三项：第一，评测集被“过拟合”，难以区分真正的模型性能；第二，评测方法陈旧，不能反映大模型新的使用场景；第三，新的大模型能力不断出现。</p><p></p><p><img src="https://static001.geekbang.org/infoq/00/00d8036efb4eb82e5cf20f278146f8b1.jpeg" /></p><p></p><p>在训练过程中，基础模型也会出现数据问题，针对行业领域进行持续训练学习是其中的一方面，如数据的来源、已训练数据的遗忘现象如何降低、构造持续训练的数据集、领域数据和通用预训练数据的配比、多种领域数据的训练顺序。林咏华指出，基座模型的变化会影响行业模型性能和行业应用，其性能决定了下游行业模型及行业应用的性能；所依赖的基座模型发生变化后，需重新训练行业模型、重新测试下游模型性能，应用集成后的各种出错处理也要重新打磨。</p><p></p><p>为此，智源研究院牵头共建了北京人工智能数据平台和高质量训练数据集。推动三大数据使用模式，并研制大模型评测体系及开放评测平台 FlagEval，还开源了面向大模型的 Triton 算子库。“当我们拿到一个大模型（开源 / 闭源）后，一切才刚刚开始。需要各种数据、评测、算力的相关技术攻关才能让模型实现产业的落地。” 林咏华表示。</p><p></p><p></p><h4>曹志斌博士：《 The Next Wave：Explore the Strategy on Generative AI》</h4><p></p><p></p><p>接下来，亚马逊云科技的全球生成式 AI 产品营销总监曹志斌博士发表了题为《The Next Wave：Explore the Strategy on Generative AI》的深度演讲，分享了全球不同客户的生成式 AI 应用场景，剖析了下一波生成式 AI 技术浪潮中，应采纳的前沿策略与核心应对机制。他表示，生成式 AI 正在创造巨大的商业价值。关于生成式 AI 应用策略，曹志斌博士提出了三点战略建议：明确业务场景适应性；设定全面数据战略；重视实现的方法和工具。</p><p></p><p><img src="https://static001.geekbang.org/infoq/aa/aa999c46e017408cd1530d90192f1b5f.jpeg" /></p><p></p><p>“不会有一个生成式 AI 基础模型能适用所有业务场景。”曹志斌博士表示，评估生成式 AI 用例的适用性，要看团队、可行性、时间表、预算、投资回报率、数据、风险；而选择大模型需考虑到六个方面，包括模型的大小和能力、预训练数据的知识截止时间、推理性能和延迟表现、是否支持灵活微调、可访问性和总体成本开支、模型相关的道德和责任问题。此外，他提到，正确的工具能够简化基础模型的调用和管理，加速构建生成式 AI 应用。</p><p>&nbsp;</p><p></p><h4>刘威：《腾讯混元大模型技术和应用实践》</h4><p></p><p></p><p>随后，腾讯杰出科学家、腾讯混元大模型技术负责人之一刘威分享了腾讯混元大模型技术与应用实践方面的最新进展。他介绍道，腾讯混元当前已升级为万亿级大模型，在这个过程积累大量自研技术，其中包括创新的专家路由 Routing 算法、独创的 MoE Scaling Law 机制以及合成数据技术，实现模型总体性能相比上一代 Dense 模型提升 50%，对比开源 MoE 模型，在代码、数学和多学科能力领先较多。</p><p></p><p><img src="https://static001.geekbang.org/infoq/65/65a3650f7041936a4631dfaa64d528c7.jpeg" /></p><p></p><p>在文生图方面，腾讯混元实现了基于 LLM + DiT 的生成能力；视频生成上，腾讯混元拥有文生视频、图生视频、图文生视频、视频生视频等多种能力，支持 1k~4k 的分辨率。据悉，目前腾讯混元大模型已接入 600+ 司内业务应用，包括微信读书、腾讯文档 AI 智能助手、腾讯广告妙思文生图平台等。</p><p></p><p></p><h2>现场回顾：技术洪流中的灵感碰撞</h2><p></p><p></p><p>大会现场人头攒动，座无虚席，气氛热闹非凡。与会者们反映，这次大会分享的内容不仅干货满满，且技术观点足够前沿和创新，让其受益匪浅、意犹未尽，更激发了对 AI 未来发展的无限想象和创新灵感。我们倍感欣慰与鼓舞，对每一位参与者给予的支持与认可致以最诚挚的谢意。未来，我们将继续前行，持续提供优质的技术内容和交流平台，致力于推动技术界的发展与创新，力求一路做技术传播领域的佼佼者。</p><p><img src="https://static001.geekbang.org/infoq/3b/3ba8532ad509c706a12ceb27fbd72d5c.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/0f/0fcf479ba5d842dd93129b65003ead2a.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/6c/6c4ae2ee54258e5de22f3c4798274339.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/84/84e97cf9d996a54679a2042ef5f48b7f.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/35/356004cd230204d2c22cff529b99f0b2.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/bc/bcf38a888e64206cf9c7c388e851364b.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/71/711b95477b9e369a5cf174201567230c.jpeg" /></p><p></p><p></p><h2>精彩瞬间：活动亮点集锦</h2><p></p><p></p><h4>大模型应用生态展</h4><p></p><p></p><p>除延续高浓度的技术内容外，本次 AICon 还特别设置了大模型应用生态展，带到场者一起猎奇 AI 智域，探索生成式 AI 的未来可能。其中，讯飞带来可以上手体验的星火大模型 SparkDesk 问答机器人，Rokid 设置了有保卫农场、完美弧线、飞镖大赛等空间计算游戏的灵境虚拟展，商汤将主打“自动生成代码”的代码小浣熊和“聊着天就把数据分析做了”的办公小浣熊产品带给参会者，还有“造车新势力×智驾领航者”蔚来汽车的展车和亚马逊云科技满载生成式 AI 黑科技的大巴车开到现场。</p><p></p><p>在现场的【OpenTalk】交流区，多位专家大咖与到场的 AICon 开发者们面对面讨论了最新的技术趋势和技术应用经验，议题包括进击的开源大模型、基于混合检索赋能 RAG 和 Agent 应用、商汤大模型在应用场景的落地实践、数据开源如何赋能全球 AI 开源开放生态以及讯飞星火大模型应用生态创新实践等。</p><p></p><p>此次，展区现场还策划了【Workshop】区域——智能编码工具体验区，无论是资深软件工程师还是代码新手，都有机会在这里亲手试用提升编码技巧的灵感和工具，体验如何通过自然语言处理技术自动生成代码，以及利用 AI 进行代码审查和优化。让我们一起回顾这些精彩瞬间吧！</p><p><img src="https://static001.geekbang.org/infoq/6c/6c171ca91ed443be7af9c45fb00abf26.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/25/25dd44eda7d8d063289a7cfbcd5e7fb3.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/32/32f3bd0e5ec782721534adbc82a71cac.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/8c/8c875e25c2f642a0a446cc4956c545f5.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/4b/4b0d3453d59cf4e6bad75da59d6de416.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/ce/ce08e702ab4cf28efdfa119a65c44c20.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/39/397bb97233a114bc5cb8bcde535f2698.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/e4/e4babc926952d671cdf02ada1ea8c052.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/26/26a4e478a0d452be3eea9598071c9fad.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/61/613695e3d6030099a3b59879850e5e21.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/6f/6f94bc8e5e9955e259b319bfa18d0b53.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/fe/feb168f225ef47112dc7fdcc81ed06bf.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/9c/9c5b67460a5073870fbfff7fa496195a.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/0d/0d399b713ffc8ae99b0b75963189e70e.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/d6/d6433852ce87d1c99105d2d2a6635e03.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/0a/0a94722a0cb0db6d1a841747fe5036cb.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/ef/ef3438287fac1d35ab1432d2afcab7bb.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/1b/1b2f0070dbcc4ed68c3915200225874c.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/27/2786c09e0f83ab1cb30f64a59fcba624.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/55/55a22f4638c1921f682a910c7a7a8cc6.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/35/354c9880a76bc0384e2f07287cd63a95.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/47/47693ed2db4150ed69652ffe1f0bcbbf.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/af/af440ef73f10ee7bfeb49a1492196af9.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/5c/5c9c4a07ba5c9e39b5b9b6e8d8112f84.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/5c/5ce88a1a41a9f76dd5b0cc5443d4cf1b.jpeg" /></p><p></p><p></p><h4>赞助商展示区</h4><p></p><p></p><p>AICon 的圆满举行，离不开赞助商们贡献的力量。在他们的慷慨助力下，我们得以持续推动技术的传播与发展，为行业创新注入不竭源泉。本次 AICon 大会得到了众多赞助商的大力支持，包括数势科技、亚马逊云科技、Google Cloud、支付宝小程序云、UCloud优刻得、七牛云、百道数据、未来智能、PPIO派欧云、intel 等。他们的参与不仅为大会增色不少，也为技术共享和行业发展提供了坚实基础。</p><p></p><p><img src="https://static001.geekbang.org/infoq/75/753cc990af5c2491b3ae175ce24cdf5c.jpeg" /></p><p></p><p></p><h2>AICon 晚场活动推荐</h2><p></p><p></p><p></p><h4>与三位业内大咖共议：AI 智能体落地的挑战与应对策略</h4><p></p><p>时间：5 月 17 日 18:30-20:00</p><p>这次交流对所有 InfoQ 粉丝免费开放！有线上和线下两种参与形式，扫描下方二维码，即可线上参加。</p><p></p><p><img src="https://static001.geekbang.org/infoq/36/3664702a4f38a0e61a3e2dfc238e2f14.png" /></p><p></p><p>AICon 特别策划了一场关于 AI 智能体落地的晚场圆桌讨论，邀请的三位业内专家将与大家分享他们的经验和见解，并与听众互动探讨——</p><p>蓝莺 IM CEO 梁宇鹏</p><p>机器姬 CTO 刘智勇</p><p>天弘基金 AI 负责人 平野</p><p>期待与你一同深入探讨 AI 智能体落地的挑战与应对策略。</p><p></p><h4>极客邦活动推荐</h4><p></p><p></p><p>今年， 极客邦科技旗下 InfoQ 中国已圆满启动两场技术盛会，之后还将于 8 月 18 -19 日举办上海站的 AICon 大会。如您感兴趣，可点击「阅读原文」查看更多详情。结合生成式 AI 领域的一系列最新动态，AICon 上海站将增加围绕多模态实时交互、长文本背后技术能力、AI 智能体相关的应用案例实践等更多话题内容 。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7b/7b0f14953c348896a9aabdd313b5ac53.png" /></p><p></p><p>购票或咨询其他问题请联系票务同学：13269078023，或扫描上方二维码添加大会福利官，可领取福利资料包。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/hEiM1DUSJUJ898Lh2tPd</id>
            <title>开发者不可错过！与 AI 技术有关的一切都在 Microsoft AI Day</title>
            <link>https://www.infoq.cn/article/hEiM1DUSJUJ898Lh2tPd</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/hEiM1DUSJUJ898Lh2tPd</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 May 2024 09:13:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: GPT-4o, AI 技术, 微软 AI 技术峰会, 生成式 AI 技术
<br>
<br>
总结: 5 月 14 日凌晨，OpenAI 发布了 GPT-4o，提供了“GPT-4 级别”的智能，改进了 GPT-4 在文本、视觉和音频方面的能力。微软将举办 AI 技术峰会，探讨 AI 技术的前沿发展和应用场景，以及如何利用 AI 技术实现智能化转型。会议将涵盖生成式 AI 技术、大模型时代、企业数据与生成式 AI 技术、AI 技术在生产中的应用等内容。参与者将有机会与专家面对面交流，探索最新的 AI 技术解决方案。同时，线上直播也将提供精彩内容。 </div>
                        <hr>
                    
                    <p>5 月 14 日凌晨，OpenAI 又发布了一款名为 GPT-4o 的新旗舰生成式人工智能模型，它提供了“GPT-4 级别”的智能，改进了 GPT-4 在文本、视觉以及音频方面的能力。毋庸置疑的是，在当今这个以数据驱动的时代，AI 技术的革新正以惊人的速度重塑着各行各业的面貌。</p><p></p><p>然而，对于众多开发者和企业而言，如何紧跟 AI 技术的前沿发展、如何将这些技术有效应用于解决实际问题、如何在激烈的市场竞争中保持领先，仍是他们面临的重大挑战。为了帮助开发者和企业了解 AI 技术的前瞻见解和行业应用场景，微软将于 2024 年 6 月 14 日在北京国际饭店会议中心举办微软 AI 技术峰会（Microsoft AI Day in Beijing），主题演讲与部分精彩课程将于官方平台同步直播（文末扫码报名或预约直播～）</p><p></p><h2>洞悉 AI 技术趋势，加速企业智能化转型</h2><p></p><p></p><p>为了让大家了解微软在 AI 领域的最新进展和创新实践，微软全球资深副总裁、微软亚太研发集团主席王永东、微软中国区总裁原欣、微软亚洲区 Microsoft Azure 策略运营总经理康容、微软大中华区首席运营官陶然等微软高层将在主题分享分析 AI 技术如何影响未来的商业格局，探讨企业如何利用 AI 技术实现智能化转型。</p><p></p><p>除了前沿技术趋势，技术专题将聚焦四大技术主题，为开发者及企业带来可供参考的实践经验。</p><p></p><p>生成式 AI 技术的最新进展及创新潜力：生成式 AI 技术是当前 AI 领域的热点之一。微软将分享其在全球业务下在生成式 AI 领域的最新研究成果，探讨如何利用这一技术推动企业创新。大模型时代构建企业竞争力：随着 AI 模型规模的不断扩大，大模型已成为提升企业竞争力的重要工具。微软将分享其在全球业务下的大模型领域的实践经验，帮助国际企业构建基于大模型的核心竞争力。企业数据与生成式 AI 技术的新纪元：数据是 AI 技术的基础。微软将探讨如何合理的利用生成式 AI 技术挖掘企业数据的潜在价值，开启企业数据利用的新纪元。AI 技术提升生产的实践：AI 技术在生产领域的应用越来越广泛。微软将分享其在 AI 智能技术提升生产效率方面的实践经验，为企业提供实用的技术指导。</p><p></p><h2>实操体验 &amp; 专家面对面，深入 AI 技术实践</h2><p></p><p></p><p>参与此次 Microsoft AI Day，你将不仅仅是一个旁观者，更是一个实践者和探索者。来自微软、NVIDIA 的专家们将通过现场演示，带你一起探索如何利用最新的 AI 技术解决现实世界中的复杂问题。</p><p></p><p>此外，Microsoft AI Day 的线下展区是另一个不容错过的亮点。这里汇聚了微软及其合作伙伴的最新解决方案和应用展示。你可以带着自己的疑问和好奇，与现场的专家进行一对一的交流，获取针对性的指导和建议。无论你关心的是 AI 技术的最新动态，还是如何在特定场景下应用 AI 技术，这里都有答案。也欢迎你来打卡 GitHub、Microsoft Learn 等展位活动，带走大会专属纪念品，留下 Microsoft AI Day 的专属记忆！（搜索“微软市场活动”公众号报名，一起线下打卡~）</p><p></p><h2>线上同步转播，精彩不间断</h2><p></p><p></p><p>如果你对 AI 技术的发展趋势与落地实践感兴趣，但无法亲临现场，也可选择观看线上直播。报名通道现已开启，<a href="https://open.weixin.qq.com/connect/oauth2/authorize?appid=wx23e7efe66bb8d9eb&amp;redirect_uri=https%3a%2f%2fchinaevent.microsoft.com%2fwcp%2fwechat%2fAuthCallback%3fwechatId%3d49666cda-230c-40d3-a87c-432b19ae135e&amp;response_type=code&amp;scope=snsapi_userinfo&amp;state=STATE#wechat_redirect">欢迎扫描下方二维码提前报名</a>"，不要错过精彩内容！</p><p></p><p><img src="https://static001.geekbang.org/infoq/18/18831e2dd1f06a5a0685609f88040e91.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/NQROQ7BzTNAT8igtIaSE</id>
            <title>InfoQ 中国技术力量之【AIGC 先锋榜单】结果正式公布！</title>
            <link>https://www.infoq.cn/article/NQROQ7BzTNAT8igtIaSE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NQROQ7BzTNAT8igtIaSE</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 May 2024 00:30:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div>         关键词: AIGC, 技术服务商, 实践案例, 评选
        <br>
        <br>
        总结: InfoQ在今年4月份启动了"AIGC先锋榜"案例征集活动，吸引了来自不同领域的数百个优秀案例。经过评审团评分，最终评选出了AIGC最佳实践案例TOP20和AIGC最佳技术服务商TOP10。评选过程中考量了技术攻坚性、方案成熟度、场景创新性等多个维度。活动展示了生成式AI在各行业的实践探索，未来还将举办年终榜单评选活动。 </div>
                        <hr>
                    
                    <p>在今年4月份，InfoQ面向AIGC领域正式启动<a href="https://www.infoq.cn/form/?id=2098">【中国技术力量&nbsp;2024&nbsp;之AIGC先锋榜】</a>"案例征集，以期深入技术变革，洞见&nbsp;AIGC&nbsp;的产业未来。本次案例征集共分为两个维度，分别是【AIGC&nbsp;最佳实践案例】和&nbsp;【AIGC&nbsp;最佳技术服务商】。</p><p></p><p>在不到一个月的周期内，InfoQ征集到了来自互联网、金融、通信、制造、教育等众多领域的优秀案例达数百个，经过专家评审团的评分，我们的最终结果终于出来啦。根据提报材料的整体数量和质量，最终我们评选出了【AIGC最佳实践案例&nbsp;TOP20】和【AIGC最佳技术服务商TOP10】（以下排名均无先后，按照提报时间顺序展示）。</p><p><img src="https://static001.infoq.cn/resource/image/48/c1/481b712596c6798089fcf8a64b93fec1.jpg" /></p><p>其中，【AIGC最佳技术服务商】榜单，专家评委根据企业提报的信息从技术攻坚性、方案成熟度、标杆客户案例、客户服务能力等多个维度进行了评分，最终根据平均分取排名靠前的10家企业上榜。</p><p><img src="https://static001.infoq.cn/resource/image/0e/51/0e372c572b3195ff8696815fa9e59351.jpg" /></p><p></p><p><img src="https://static001.infoq.cn/resource/image/3e/03/3e3d0c815792eafa67da936b046bc103.jpg" /></p><p></p><p>【AIGC最佳实践案例】榜单，专家评委则根据企业提报的信息从场景创新性、实践成果、行业价值等多个维度进行评分，最终根据平均分确认出上榜的20家企业。</p><p></p><p>最后，再次感谢所有企业的参与，我们从中看到了生成式AI在千行百业的初步实践探索，比如智能营销、智能写作、自动驾驶、医学影像增强、智能库存分析、寄快递等众多场景。遗憾错过本次榜单的企业也欢迎积极关注InfoQ中国技术力量的年终榜单预告，我们预计将于10月份左右发起年终榜单评选，届时将通过InfoQ网站、微信公众号等渠道对外官宣。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/a0XsHUI5y7sVUzlqCXC7</id>
            <title>OpenAI的元老科学家们都跑光了！一个时代结束了？</title>
            <link>https://www.infoq.cn/article/a0XsHUI5y7sVUzlqCXC7</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/a0XsHUI5y7sVUzlqCXC7</guid>
            <pubDate></pubDate>
            <updated>Thu, 16 May 2024 07:08:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, 离职, 安全意识, 人工智能
<br>
<br>
总结: 一些关键人物离开了OpenAI，其中包括拥有安全意识的人员，他们担心人工智能可能带来危险。这些离职引发了人们对OpenAI未来方向和安全性的担忧。 </div>
                        <hr>
                    
                    <p>5 月 15 日，OpenAI 联合创始人 Ilya Sutskever在社交平台上发文表示，决定离开 OpneAI。几个小时后，OpenAI 超级对齐团队的负责人Jan Leike 也宣布离职，离职宣言没有像 Ilya 那样写小作文，他就写了一句话“我辞职了（I resigned）”。</p><p>&nbsp;</p><p>值得注意的是，拥有OpenAI 20% 计算资源的超级对齐团队（Superalignment&nbsp;Team）是由上面两个人领导的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1e/1eb5dbde273b3cdea6bff50d4e34f6b8.jpeg" /></p><p></p><p>根据统计，自OpenAI 董事会事件和 Altman 复职以来，离开 OpenAI 的具有安全意识的人名单包括：Ilya Sutskever、Jan Leike、Leopold Aschenbrenner、Pavel Izmailov、William Saunders、Daniel Kokotajlo 和 Cullen O'Keefe。</p><p>&nbsp;</p><p>此外，近期离职的人还包括非营利组织和战略计划主管Chris Clark和社会影响主管Sherry Lachman。</p><p>&nbsp;</p><p>每个OpenAI离职员工宣布离职后，几乎都可以看到有人问：What did you see ? 当然这个问题并没有人回答。</p><p>&nbsp;</p><p>“OpenAI 似乎确实没有多少使命了——他们的CEO散发着二手车推销员的气息，他最近提到考虑允许他们的人工智能生成色情内容，现在又发布了一个调情的AI女友作为他给人类的礼物。”有网友评价道。</p><p>&nbsp;</p><p></p><h2>“元老科学家”所剩无几</h2><p></p><p>&nbsp;</p><p></p><blockquote>“Karpathy 和 Ilya 现在都已从 OpenAI 消失了。看起来，现在是Sam Altman 和 Greg Brockman的表演舞台了。不得不承认，在这四个人中，Karpathy 和 Ilya 是给我印象最深刻的两个。”</blockquote><p></p><p>&nbsp;</p><p>马斯克也曾这样称赞 Ilya：Ilya Sutskever 是 OpenAI 成功的关键。 Altman 也在宣布离职的帖子里说到，“没有他，OpenAI就不会存在。”</p><p>&nbsp;</p><p>去年11月，Ilya 与另外三名董事会成员一道，迫使该公司高调的首席执行官Sam Altman辞职，但后来他表示后悔。据报道，双方争论的焦点是对 OpenAI 方向的分歧：Ilya 对 Altman 以牺牲安全工作为代价而急于推出人工智能产品感到沮丧。</p><p>&nbsp;</p><p>Altman 在被赶下台的五天后就回到了 OpenAI，重申了自己的控制权，并继续推动越来越强大的技术，这让他的一些批评者感到担忧。Ilya 仍然是OpenAI的员工，但他再也没有回去工作。</p><p>&nbsp;</p><p>围绕Ilya 工作的模糊性引发了一个迷因：Ilya 在哪里？他看到了什么？ OpenAI 联合创始人马斯克经常在他拥有的平台 X（以前的 Twitter）上<a href="https://twitter.com/elonmusk/status/1768706295291314586">亲自提出这个问题</a>"。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8e/8e5697a848732e05b4dc2ed9be0b18b7.png" /></p><p></p><p>能看到的动态是，Ilya 在去年帮助 OpenAI 创建了超级对齐团队，任务是建立防护措施，以防止人工通用智能（AGI）失控。和其他人一样，他越来越担心人工智能可能变得危险，甚至可能毁灭人类。</p><p>&nbsp;</p><p>但是，Ilya 和 Leike 领导的这个超级对齐团队人员非常不稳定。</p><p>&nbsp;</p><p>今年2月，William Saunders 离开了 OpenAI。自2021年以来，Saunders一直在安全团队工作，该团队后来成为超级对齐团队。Saunders 还是可解释性团队的经理，该团队研究如何使AGI安全，并检查模型如何以及为什么会这样表现。他与人合作撰写了几篇关于人工智能模型的论文。</p><p>&nbsp;</p><p>也是在这个月，备受尊敬的研究科学家Andrej Karpathy也宣布离开 OpenAI。他表示，自己的离开并不是因为任何事件、问题或戏剧性事件，而是他要去追求自己的项目。</p><p>&nbsp;</p><p>Karpathy 是 OpenAI 的创始成员，最初于 2017 年离开公司加入特斯拉。2022 年，他离开特斯拉，并在大约一年前重新加入 OpenAI。Karpathy 在社交媒体和 YouTube 上拥有大量粉丝，发布了有关新兴领域发人深省的文章以及解释人工智能内部运作原理的视频。</p><p>&nbsp;</p><p>3月，对齐研究员 Ryan Lowe 离开，参与过GPT-4对抗性测试的 Daniel Kokotajlo 也离开了OpenAI。Kokotajlo在他的网上论坛LessWrong个人主页上写道，他退出是因为“对AGI时代的行为失去信心”。</p><p>&nbsp;</p><p>他还曾参与关于暂停AGI开发的讨论。Kokotajlo 写道：“大多数要求暂停的人都是在试图反对‘选择性暂停’，以及要求对处于进步前沿的大型实验室的实际暂停。”</p><p>&nbsp;</p><p>他认为，目前的奥弗顿之窗（overton window ）似乎围绕评估风险和采取缓解措施的组合，这具有很高的监管俘获风险（即导致选择性暂停，而这并不适用于最需要暂停的大公司！)“我的幻灭感是我离开OpenAI的原因之一。”</p><p>&nbsp;</p><p>4月，据知情人士透露，OpenAI 解雇了两名涉嫌泄露信息的研究人员，其中包括超级对齐团队的 Leopold Aschenbrenner，Aschenbrenner 是 llya 的盟友。另一位从事推理研究的研究员 Pavel Izmailov 也曾在安全团队工作过。目前，Pavel Izmailov已经跳槽到马斯克旗下的xAI，明年也将成为纽约大学助理教授。</p><p>&nbsp;</p><p>最近，多名涉嫌透露消息给外界的“内鬼”也被OpenAI开除。</p><p>&nbsp;</p><p>“OpenAI 正在失去最优秀、最注重安全的人才。”这是大家对此的评价。鉴于最近从OpenAI离职的人数之多，不少网友都开始调侃：“我从OpenAI离职了”。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/5b/5ba9f2df6d98eb6108870f7df0295af1.jpeg" /></p><p></p><p></p><h2>OpenAI 被营销支配？</h2><p></p><p>&nbsp;</p><p></p><blockquote>“六位顶尖科学家早已离去。OpenAI 如今由营销、业务、软件和产品化人员运营。”</blockquote><p></p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/50/500beb93e2e06e2b7a0a5a92dbef6892.png" /></p><p></p><p>从左到右：Jakub Pachocki、Greg Brockman、Ilya Sutskeve、Sam Altman、Muri Murati</p><p>&nbsp;</p><p>上图中，除去 Ilya，几乎就是当前OpenAI的重要管理层了。</p><p>&nbsp;</p><p>OpenAI 的关键研究员 Jakub Pachocki 将接替 Ilya 担任该公司的首席科学家。在 Altman 被罢黜前几周，曾帮助监督 GPT-4 创建的 Pachocki 被提升到公司研究总监的位置，一度被提升到与Ilya 并肩的职位。</p><p>&nbsp;</p><p>Pachocki 于 2017 年加入 OpenAI Dota 团队，担任研究主管，该团队构建了一个能够在 Valve 的 Dota 2 策略游戏中击败人类玩家的人工智能系统。随后，Pachocki 成为 OpenAI 深度学习组织推理和科学的研究负责人，然后晋升为研究总监。目前尚不清楚 Pachocki 是否也会接任 OpenAI Superalignment 团队的负责人。</p><p>&nbsp;</p><p>而Jan Leike 离职后，他的职位将由该公司另一位联合创始人 John Schulman 担任。Schulman 在去年失败的董事会政变中站在了 Altman 一边。另外，Schulman 在Superalignment 团队还担任了监督者角色。</p><p>&nbsp;</p><p>当然，OpenAI也在不断引进新的人才，年轻力量正在支撑OpenAI。比如GPT-4o的多模态负责人Prafulla Dhariwal，实际只有本科学历；Sora的论文作者中有一位研究员今年刚满21岁，仅有高中毕业证。</p><p>&nbsp;</p><p>但众所周知，OpenAI更多使用的是谷歌提出的技术路线，其核心研发实力不如他们的工程能力。元老科学家们的出走还是让大家对OpenAI 的未来产生了担忧：OpenAI 还能实现 AGI 吗？</p><p>&nbsp;</p><p>AI 行业人才短缺是不争的事实，AI相关的部门很难找到合适的员工。AI 人才争夺战已经开始，甚至有企业都给出了100万美元年薪。</p><p>&nbsp;</p><p>薪酬数据和职业平台 Levels.fyi 联合创始人 Zuhayeer Musa在采访中表示，OpenAI 提供的中位工资（包括奖金和股权）为 925,000 美元。Meta 的 344 名机器学习和人工智能工程师，包括奖金和股权在内的薪酬中位数约为 40 万美元。</p><p>&nbsp;</p><p>除了巨额薪酬之外，从小型初创公司到 OpenAI、Meta 等，都在提供加速的股票兑现计划，甚至试图挖走整个团队。</p><p>&nbsp;</p><p>“他们没有护城河。那些从事科学研究的人现在正在为其他公司做研究，并且会让 OpenAI 感到震惊。”有网友对OpenAI的人才出走评价道。</p><p>&nbsp;</p><p>“OpenAI 对 Microsoft 的需要几乎就像 Microsoft 对 OpenAI 的需要一样”。有网友认为，“当下一波新的深度学习创新浪潮席卷全球时，微软会吃掉OpenAI 剩下的东西。他们赚了很多钱，但除非他们弥补失去的东西，否则就没有未来。”</p><p>&nbsp;</p><p>OpenAI与微软的紧密联系让一些人希望，至少出于对品牌保护，微软能够在安全研究上做一定的投入。但具有讽刺意味的是，微软在发布“人工智能”产品之前不进行安全检查方面是已经出名了的。</p><p>&nbsp;</p><p>还有很多人认为，OpenAI 全力以赴地让大模型这只“金鹅”产生更多收益、专注于如何通过嵌入广告实现货币化，并通过主题限制继续提供“安全”等，而不是进一步沿着 AGI 路线前进。</p><p>&nbsp;</p><p>“LLM 通往 AGI 或超级智能的机会为零。因此，如果这就是 OpenAI 在未来 5 年里要关注的全部内容，那么与Superalignment 相关的小组就没有必要了。”有网友评价道。</p><p>&nbsp;</p><p>有人推测，要么离 AGI 太远，以至于无论“对齐”意味着什么都是不必要的，要么就是奥特曼等人已确定这是商业成功的障碍。</p><p>&nbsp;</p><p>“事实证明，我们已经结盟了，这就是所谓的资本主义。”也有人说道，“资本主义本身就是一种不结盟的人工智能，从这个角度来理解就可以澄清很多事情。”</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>“近十年后，我决定离开 OpenAI。这家公司的发展轨迹堪称奇迹，我相信OpenAI将打造出既安全又有益的人工智能。”38岁的llya 补充说，他正在启动一个新项目，但没有详细说明。</p><p>&nbsp;</p><p>Karpathy 和 Ilya 都有了自己的项目，显然，人们希望那些伟大的人工智能科学家还能在一起做一些有意义的事情。不过，我们应该很快能看到他们多年从事AI 研发的总结成果。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://twitter.com/ilyasut/status/1790517455628198322">https://twitter.com/ilyasut/status/1790517455628198322</a>"</p><p><a href="https://www.businessinsider.com/openai-safety-researchers-quit-superalignment-sam-altman-chatgpt-2024-5">https://www.businessinsider.com/openai-safety-researchers-quit-superalignment-sam-altman-chatgpt-2024-5</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/k3QwZc0Ty20kuXygmTmH</id>
            <title>百度文心智能体平台举办开发者沙龙，打造国内领先的智能体生态</title>
            <link>https://www.infoq.cn/article/k3QwZc0Ty20kuXygmTmH</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/k3QwZc0Ty20kuXygmTmH</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 May 2024 14:47:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div>         关键词: 人工智能技术, 智能体, 百度文心智能体平台, 智能体生态
        <br>
        <br>
        总结: 随着人工智能技术的发展，智能体作为大模型应用的新趋势，正在改变生活和工作方式。百度文心智能体平台通过全新升级，致力于打造国内领先的智能体生态，吸引了大量技术开发者和人工智能爱好者参与。平台提供多样化的智能体，覆盖广泛应用场景，呼吁更多行业伙伴和开发者加入。通过提供详尽的智能体开发指南，平台帮助开发者快速创建和优化智能体。活动中展示了智能体在实际应用中的进阶技巧，激发开发者创意潜能，并邀请他们参加智能体大赛。2024百度移动生态万象大会将推出更多智能体相关服务和能力，致力于让智能体人人可用。 </div>
                        <hr>
                    
                    <p>随着人工智能技术的飞速发展，智能体作为大模型应用的新趋势，正逐步改变我们的生活和工作方式。</p><p>&nbsp;</p><p>百度创始人、董事长兼首席执行官李彦宏曾表示，智能体是未来离每个人最近、最主流的大模型使用方式。在这一背景下，百度文心智能体平台（AgentBuilder）经过全新升级，致力于打造国内领先的智能体生态。</p><p>&nbsp;</p><p>5月15日，百度文心智能体平台联合InfoQ，举办了一场主题为「拥抱智能体，人人都能成为超级个体」的沙龙活动，吸引了大量技术开发者以及对人工智能充满热情的参与者。</p><p>&nbsp;</p><p><img src="https://static001.infoq.cn/resource/image/32/66/32c2d9e2c1f88a1e92fe3f8e9fae1f66.jpeg" /></p><p></p><p>据介绍，文心智能体平台除了开发门槛低之外，还有智能调优、广泛分发、直通商业化等特点。百度搜索也会在接下来的时间里，重点布局智能体生态，用搜索生态天然带有的「亿级用户+超级流量+精准算法」，打通「开发+分发+商业化」全链条，让智能体释放出更大潜力。</p><p>&nbsp;</p><p>&nbsp;百度文心智能体生态负责人马宝云分享了文心智能体平台的核心优势。&nbsp;</p><p>&nbsp;</p><p>百度是业内最早布局智能体的大厂之一，2023年9月，百度发布「灵境矩阵」文心一言插件生态平台，同年12月升级为「灵境矩阵智能体平台」，在今年4月举办的Create 2024百度AI开发者大会上则升级更名为「文心智能体平台」。全新升级后的文心智能体平台，有5个「超能力」：技术底子厚、开发成本低、快速可成长、分发渠道广、商业可闭环。</p><p>&nbsp;</p><p>据悉，文心智能体平台发布至今已有9个月，最近又经历了全新升级，仅仅是5月，智能体的数量就已经环比增长167%。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/fa/69/faf7d08b0450428a45df844f8600a569.jpeg" /></p><p></p><p>&nbsp;她展示了平台如何通过提供创作助手、专家顾问、AI分身、学习工具、生活帮手、互动游戏和设计助手等多样化的智能体，来满足不同用户的需求。这些智能体不仅覆盖了广泛的应用场景，也体现了平台对各行业伙伴的开放性和包容性。她呼吁更多的行业伙伴和开发者加入文心智能体。</p><p>&nbsp;</p><p>文心智能体平台高级产品经理梁伟以文心智能体平台为例，给广大开发者提供了一份详尽的「从0到1智能体开发指南」。</p><p>&nbsp;</p><p><img src="https://static001.infoq.cn/resource/image/36/fb/367ddyy504cdba45433315fc53720bfb.jpeg" /></p><p>&nbsp;</p><p>他展示了如何通过简单的一句话描述来快速创建智能体，通过层次分明的表单配置来完善智能体的高级设置。他还分享了如何通过智能体生成和优化指令，如何通过知识库和工具来增强智能体的功能，还介绍了数字人配置的选项，包括形象设定和语音风格，以及如何通过实时预览调优来测试智能体的效果。</p><p>&nbsp;</p><p>文心智能体平台运营经理李实则分享了智能体在实际应用中的进阶技巧。</p><p>&nbsp;</p><p>李实表示，向AI大模型提供具体指令（prompt）会直接影响智能体的效果。他建议指令应包含角色和目标、指导与限制、澄清和个性化四个部分，以确保智能体能够精确模拟特定角色的思维方式，提供符合实际情景的回答。在现场，李实展示了怎样用搜索增强和文心一格生图等工具来提升智能体的交互体验。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/2f/bf/2f8c41baa14576f2c89d77bca5d625bf.jpeg" /></p><p>&nbsp;</p><p>活动特别安排了自由问答和现场互动体验环节，为参与者提供了交流和探讨智能体技术及应用的机会。参与者有机会亲身体验智能体的强大功能，感受人工智能带来的便捷和智能。</p><p>&nbsp;</p><p>据介绍，为激发开发者的创意潜能，文心智能体平台发起「文心智能体大赛」，为开发者提供百万奖金池、百亿流量包、与技术大咖深度交流、免费AI课程等支持，诚邀广大开发者积极参与，共同探索无限可能。感兴趣的开发者现在就可以报名参加。</p><p>&nbsp;</p><p>据悉，2024百度移动生态万象大会将于5月30日在苏州举办，本次大会的主题是「让智能体人人可用」，百度搜索、百度APP、百度文库、文心一言APP、百度电商等百度移动生态业务都将推出更多智能体相关的服务和能力。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/VKUTp0UkRPvGWHU5dT1S</id>
            <title>AIGC智能耳机硬件新标杆，未来智能发布新一代讯飞会议耳机</title>
            <link>https://www.infoq.cn/article/VKUTp0UkRPvGWHU5dT1S</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/VKUTp0UkRPvGWHU5dT1S</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 May 2024 10:27:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 未来智能, 讯飞会议耳机Pro 2, viaim AI, AIGC智能耳机
<br>
<br>
总结: 2024年5月15日，人工智能硬件公司未来智能发布了讯飞会议耳机Pro 2、iFLYBUDS 2以及Kit 2三款旗舰新品，为用户带来全新升级的viaim AI，也为AIGC智能耳机树立了新标杆。在发布会上，未来智能CEO马啸表示，讯飞会议耳机Pro 2是未来智能最新集大成之作，依托领先AI技术，成功进化至“智能助理”，引领了AIGC场景应用趋势。新一代产品采用全新工艺设计，全面升级音质、降噪、操控等方面，实现了多语种录音转译等功能基础上的闪录、语种扩充、viaim AI三大进化，大幅提升办公效率，成为AIGC时代的办公会议生产力标配。viaim AI会议助理智能分析记录内容，提取重点并生成摘要总结和待办事项，新增智能询问功能，全面解放用户双手，提升办公效率。多语种录音转写及翻译功能支持32种语言，让耳机化身全场景AI翻译官。硬件体验实现了进一步突破，音质全面升级，降噪深度可达48dB，续航时间长达36小时，外观设计高端质感，语音控制更便捷。讯飞会议耳机Pro 2定位于商务旗舰，iFLYBUDS 2定位于职场Buff，Kit 2是讯飞会议耳机的天生搭档，助力用户提高工作效率。 </div>
                        <hr>
                    
                    <p>2024年5月15日，人工智能硬件公司未来智能发布了讯飞会议耳机Pro&nbsp;2、iFLYBUDS 2以及Kit 2三款旗舰新品，为用户带来全新升级的viaim&nbsp;AI，也为AIGC智能耳机树立了新标杆。</p><p></p><p>在发布会上，未来智能CEO马啸表示：在AIGC领域，垂直场景的服务性工具比泛智能工具实用性更强，未来智能在垂直的办公会议领域，已经形成了数据的马太效应，打造出了非常实用的AI会议助理。以讯飞会议耳机Pro&nbsp;2为代表的未来智能新一代产品，是未来智能最新集大成之作，标志着未来智能团队多年来在办公会议垂直场景中的产品解决方案深挖，以及持续的技术积累，迎来了“质变”时刻：依托领先AI技术，讯飞会议耳机从“智能工具”成功进化至“智能助理”，成为当下最实用的AIGC智能耳机之一，引领了AIGC场景应用趋势。</p><p></p><p>新一代未来智能新品矩阵中，最具代表性的商务旗舰产品讯飞会议耳机Pro&nbsp;2采用了全新的工艺设计，带来更高级的质感体验，音质、降噪、操控等方面全面升级，更在全场景录音转文字、多语种录音转译等功能基础上，实现了闪录、语种扩充、viaim AI三大进化，进一步拓展了讯飞会议耳机的应用场景，全面释放AI生产力，大幅提升办公效率，成为AIGC时代的办公会议生产力标配。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c3/c397a4bda62148dd386d0cb993e40ea5.png" /></p><p></p><p>viaim AI再进化，讯飞会议耳机更“聪明”了</p><p></p><p>新一代讯飞会议耳机Pro&nbsp;2搭载了全新升级的viaim AI会议助理，AI性能大幅提升，让耳机变得更“聪明”了。面对冗长繁琐的会议内容，viaim AI能够智能分析记录内容，自动提取纪录中的重点，2小时会议1分钟即可一键生成「摘要总结」，大幅简化会后总结难度，还能提取纪录中的关键任务生成「待办事项」，让待办事项一目了然。</p><p></p><p>而让用户更加惊喜的升级，则是viaim AI新增了「智能询问」功能，用户只需语音/文字输入问题，viaim AI就能回答用户关于当前记录内提到的问题和扩展问题，让用户快速获取记录内容中需要的信息。新的AI功能做到了真正全面解放用户双手，再一次提升办公效率。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fc/fcffc6a9684055ae2cfd7b51aa198e42.png" /></p><p></p><p>语种大幅扩充，讯飞会议耳机化身全场景AI翻译官</p><p></p><p>商务精英，经常会在不同外语环境中与不同的人打交道，一部掌握多种语言的小巧耳机，其实是最优雅的突破语言障碍的工作神器。跟随讯飞会议耳机Pro&nbsp;2等新品的发布，未来智能大幅扩充了多语种录音转写及翻译功能所支持的语言，从原来的支持11种语言扩充到支持32种语言、还在支持12种方言基础上，新增了2种民族语言，还拥有同传听译、面对面翻译两种模式，让耳机化身全场景AI翻译官，无论多复杂的语言环境也能帮助用户轻松应对。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2a/2afc04013cee5a0dac2013c95c3eaa50.png" /></p><p></p><p>硬件全能进化，讯飞会议耳机Pro&nbsp;2旗舰品质再突破</p><p></p><p>新一代讯飞会议耳机的硬件体验也实现了进一步突破。讯飞会议耳机Pro&nbsp;2带来了全新升级的闪录功能「红点录」。在会议现场，打开充电盒盖，一键按下充电盒内红色按键，即可进入现场录音模式。无需打开APP，也无需连接手机，录音存储在耳机中，现场拾音辐射距离高达7m，左右耳机合计可存储4小时录音，轻松应对各种会议场景，确保不错过任何重要内容。「红点录」进一步拓展了讯飞会议耳机独家闪录功能应用场景，标志着讯飞会议耳机在全能全场景进化的道路上再一次实现了突破。</p><p></p><p><img src="https://static001.geekbang.org/infoq/53/5318c8fe3de0d1005544113c5617b563.png" /></p><p></p><p>生产力升级之外，讯飞会议耳机Pro&nbsp;2没有忘记耳机体验的进化。其采用11mm镀钛原生刚性振膜单元以及极具高弹性和刚性的TPU镀钛材质，配合讯飞AI音频实验室专业调音，实现了音质全面升级，带来旗舰级悦耳音质体验。同时，支持LHDCTM高清音频解码，至高可达1000Kbps，音质表现达到行业第一阵营，并荣获了Hi-Res金标音质认证。</p><p></p><p>降噪方面，讯飞会议耳机Pro&nbsp;2集成自适应ANC主动降噪，智能捕捉环境噪音，并根据噪音强度自动切换降噪等级，降噪深度可达48dB，即使在喧闹的场合里也能享受会议室般安静，降噪品质得到中国电子音响协会降噪等级认证：A级。此外，讯飞会议耳机Pro&nbsp;2在三麦克风通话降噪算法上新增了骨声纹拾音麦克风，利用头骨震动的方式精准采集用户声音，大幅提升通话质量，即便身处嘈杂环境也能清晰如同面对面交流。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d6/d6dc1a5cd67b649d7f528ad570b88aa7.png" /></p><p></p><p>讯飞会议耳机Pro 2的续航表现同样值得称赞，单次使用长达9小时，搭配充电盒可延长至36小时。而且还具备快速充电功能，充电5分钟可以提供长达1小时的续航时间，更支持无线充电，彻底告别续航焦虑。</p><p>外观上，讯飞会议耳机Pro 2延续了经典的滑盖设计，整体采用PPG大师漆，正面悬浮镂空全透效果logo以及充电仓真空电镀装饰，搭配夜影黑、幻影银、午夜蓝(限量版)未来科幻感配色，轻巧便携更沉稳大气，尽显高端质感。全新升级的语音控制，不仅「说话」就能操控耳机，更能一键触控录音、无感配对，带来更便捷操控及连接体验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6f/6f5b4c7cc860a3739cf3d15149bf3348.png" /></p><p></p><p>新一代未来智能产品矩阵中，讯飞会议耳机Pro 2定位于“商务旗舰”，为商务精英人群量身打造。同期发布的iFLYBUDS&nbsp;2，则定位于“职场Buff”，其软件体验与讯飞会议耳机Pro&nbsp;2相近，硬件形态上则采用了半入耳式设计，更适合耳道敏感人群，即使长时间佩戴也毫无压力，可以帮助更广泛职场人持续提高工作效率。Kit 2是讯飞会议耳机的天生搭档，让耳机端的实时录音转写文字等功能在电脑上实现，带来更高效的桌面办公体验，专为深度会议用户量身定制。</p><p></p><p>目前三款新品已经在京东/天猫商城上线销售，用户可结合自身需要酌情选购。</p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/B14OwDrE1ZZ3VMl1goHm</id>
            <title>打磨三年、支持万亿 MoE，腾讯混元模型团队的真实推理实力到底如何？</title>
            <link>https://www.infoq.cn/article/B14OwDrE1ZZ3VMl1goHm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/B14OwDrE1ZZ3VMl1goHm</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 May 2024 08:26:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 腾讯混元大模型, 刘凯, 推理能力, 技术实力
<br>
<br>
总结: 2023年9月，腾讯推出自研的混元大模型，支持50多个业务产品，推理性能优异。刘凯介绍了腾讯在大模型技术探索和优势方面，以及混元大模型的推理能力和压缩方法。模型规模庞大，采用不同推理和压缩方法，以及如何在保持性能效果的前提下将大模型做“小”的技术思路。 </div>
                        <hr>
                    
                    <p>采访嘉宾｜刘凯，腾讯混元大模型推理方向负责人</p><p>作者&nbsp;|&nbsp;华卫</p><p></p><p>2023&nbsp;年&nbsp;9&nbsp;月，腾讯终于在一片翘首以盼中推出自研的混元大模型。对于入局早晚的问题，腾讯董事会主席兼首席执行官马化腾曾这样说道，“我们在埋头研发，但并不急于早早做完，把半成品拿出来展示。”</p><p></p><p>据悉，混元大模型未来能支持&nbsp;50&nbsp;多个腾讯业务产品，而幻觉比主流开源大模型降低&nbsp;30%&nbsp;至&nbsp;50%、文生图推理耗时缩短至&nbsp;3-4&nbsp;秒，是混元大模型目前已达到的推理性能。那么，其背后的核心团队究竟做了哪些努力？技术实力到底如何？</p><p></p><p>就此，InfoQ&nbsp;对腾讯混元大模型推理方向负责人刘凯进行了专访，听他详细讲述了腾讯混元大模型在推理和压缩方面的技术能力与团队实践。在即将召开的<a href="https://sourl.co/faYrKr">AICon全球人工智能开发与应用大会&nbsp;暨&nbsp;大模型应用生态展</a>"上，InfoQ&nbsp;也邀请到刘凯老师来做演讲分享，他将进一步透露大模型推理加速与压缩的技术方法以及腾讯混元大模型的落地进展。</p><p>&nbsp;</p><p>以下为访谈实录，经编辑。</p><p></p><p></p><h2>如何在推理赛道扳回“一局”？</h2><p></p><p>InfoQ：作为较晚入场大模型的国内互联网大厂，腾讯团队有什么优势？</p><p>刘凯：对于晚入场这个说法，并不准确。早在2020年，腾讯出于自身业务需要已经展开预训练大模型的技术探索和积累，并率先在内部业务譬如广告上进行应用投产。腾讯对于处理前沿技术探索和输出的关系，一贯以来是比较一致的，对于正在探索的技术路线，往往会用自身业务作为试验田对方案进行反复验证和完善，之后才会对外发布和输出。</p><p>说到优势，我觉得在大模型技术的前沿探索中，腾讯在以下方面具备相当的积累和竞争力：1、在数据、算法、工程等方向，我们有一批经验丰富的专家；2、我们有一个强大的机器学习平台Angel(曾获&nbsp;2023年中国电子学会科学技术进步一等奖)；3、腾讯内部有大量适合大模型落地的业务应用场景，能在和业务的合作中助力腾讯混元团队能力的快速成长。</p><p></p><p>InfoQ：推理能力对大模型而言十分关键，腾讯混元大模型做到了什么水平？目前是否有量化的能力指标？</p><p>刘凯：目前腾讯混元大模型的吞吐能力达到开源框架的2倍以上，文生图&amp;文生视频推理耗时下降65%。规模上，模型支持万亿MoE、上下文长度保持256K以上，同时支持多种压缩方法，包括量化、蒸馏、裁剪、稀疏、并行解码、步数蒸馏等，能在保证效果无损的基础上，将吞吐提升2~8倍。</p><p></p><p>InfoQ：不同模态的内容生成框架下，混元大模型采用的推理和压缩方法有差异吗？</p><p>刘凯：会存在一定的差异。比如文生文&amp;图生文的场景，由于模型较大一般需要采用分布式推理；而文生图&amp;文生视频的扩散模型，在大部分场景下使用单卡推理即可，不过随着模型的逐步增大，我们也在支持分布式推理。</p><p>压缩方法上也存在一定的差异，文生图&amp;文生视频扩散模型使用步数蒸馏收益更大，所以蒸馏的优先级会高于其他方法；而在生文场景，量化由于简单高效，优先级最高、之后逐步是蒸馏、投机采样、裁剪稀疏等方法。</p><p></p><p>InfoQ：目前有哪些可以有效提高模型推理速度和准确度的技术？主要优化思路是什么？</p><p>刘凯：并行解码等相关技术都值得一试，其主要思路是通过使用更小的模型或者一次更多的生成token数来加快速度，同时使用base模型进行结果校验来保证生成的效果。</p><p></p><p>InfoQ：对腾讯混元大模型来说，端侧推理是一个降低推理成本的好方式吗？是否有可能实现？</p><p>刘凯：是的，端侧推理是腾讯混元大模型逐步推进的一个方向。腾讯内部有很多业务适合端侧推理，比如会议、文档、输入法等。</p><p></p><p></p><h2>将模型从大化“小”的心得</h2><p></p><p>InfoQ：模型的规模参数大到一定程度后，会产生哪些负面效应？</p><p>刘凯：模型参数的持续上升，会带来成本的上升和耗时的增加，同时也给推理优化带来了很大的挑战。首先我们知道大模型推理的瓶颈主要集中在显存和带宽上，为了放下更大的模型，我们需要进行单机多卡、多机多卡的部署。</p><p>当使用多机多卡时，带宽就涉及到显存带宽、卡间带宽、网络带宽等三个方面，其速度依次递减，耗时会逐步上升，而部署卡数的上升必然会带来卡成本及配套设备成本的上升。此外，框架3D并行能力并非无限制无损扩展，如果超大模型设计的不合理，会使得优化难度成倍上升。</p><p>InfoQ：如何在保持性能效果的前提下将大模型做“小”？腾讯有什么好的技术思路分享？</p><p>刘凯：模型压缩方法主要包括蒸馏、裁剪、稀疏、量化等。在上述方法中，量化容易实现，是最稳定的，也是各大公司广泛使用的方法。以腾讯混元大模型为例，我们在Dense以及MoE模型都大规模使用了量化模型，从精度上覆盖了INT8、FP8、INT4，并在逐步尝试2bit、1bit的压缩，目前在范围上已经支持了权重、激活、KV-Cache的量化。</p><p>由于腾讯内部应用场景很多，对模型规模有多样的需求，我们也开发了裁剪+蒸馏的方式来快速扩展模型矩阵，保证各个业务可以使用适合自己的大模型。稀疏这块，其实服务器侧的使用会比较少，但腾讯在这块有持续打磨。除了上述通用方法之外，针对大模型也有一些新的压缩方法，比如文生文当中的GQA/MQA，并行解码，Cache方案等；文生图、文生视频的步数蒸馏等。</p><p>InfoQ：现实应用中，当落地场景的训练数据未知或不可获得时，如何合理进行模型压缩？</p><p>刘凯：针对这个问题我想稍微扩展一下，首先我们知道模型压缩一般分为Training-Base和Training-Free两种方法，但大模型压缩时我们一般还是建议走Training-Free过程，因为大模型的训练过程长、成本高、调参复杂，一般情况不建议去触碰。并且，随着模型规模的增大，无损压缩的难度是减小的，所以使用简单便捷的Training-Free的方法比较好。</p><p>使用Training-Free也需要一些数据进行校准，如果获得不到训练的数据时，我们的建议是通过两种方法解决：1、选取通用数据集的数据进行校准；2、使用大模型生成一定的数据来进行校准。</p><p>InfoQ：在即将到来的AICon上，您准备向听众分享哪些方面的内容？</p><p>刘凯：在即将到来的AICon上，我会给大家分享腾讯混元大模型推理框架Angel-HCF、压缩工具SNIP的技术进展以及腾讯混元大模型的落地情况，并针对GPU底层优化、服务化能力、压缩算法的优缺点进行剖析，让大家能快速了解大模型推理相关技术。</p><p></p><p></p><h4>嘉宾介绍：</h4><p></p><p>刘凯，腾讯高级工程师，腾讯混元大模型推理方向负责人，负责文生文、文生图等大模型压缩优化及推理加速。10&nbsp;年以上&nbsp;GPU&nbsp;高性能优化经验，丰富的深度学习推理框架优化经验。带领团队完成大模型压缩&nbsp;&amp;&nbsp;推理框架从&nbsp;0&nbsp;到&nbsp;1&nbsp;的构建。</p><p>&nbsp;&nbsp;&nbsp;</p><p>活动推荐：</p><p><a href="https://sourl.co/faYrKr">AICon全球人工智能开发与应用大会&nbsp;暨&nbsp;大模型应用生态展</a>"将于5月17日正式开幕，本次大会主题为「智能未来，探索AI无限可能」。如您感兴趣，可点击「阅读原文」查看更多详情。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f3/f325163430e0188b28bcaaf57a37a8ff.png" /></p><p>&nbsp;</p><p>会议即将开幕，扫码可预约主题演讲直播，购票或咨询其他问题请联系票务同学：13269078023，或扫描上方二维码添加大会福利官，可领取福利资料包。</p><p>追踪链接：<a href="https://sourl.co/faYrKr">https://sourl.co/faYrKr</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/GKuBaJYaVxmQtxAJI8XB</id>
            <title>巨头们涌入的医疗大模型，何时迎来最好的商业时代？</title>
            <link>https://www.infoq.cn/article/GKuBaJYaVxmQtxAJI8XB</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/GKuBaJYaVxmQtxAJI8XB</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 May 2024 08:21:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 医疗大模型, 商业化, 数据质量, 社会接受度
<br>
<br>
总结: 当下医疗大模型在商业化领域备受关注，但仍需克服数据质量、成本、幻觉等挑战，同时提高社会接受度。 </div>
                        <hr>
                    
                    <p>采访嘉宾｜刘升平，云知声AI&nbsp;Labs&nbsp;研发副总裁</p><p>作者&nbsp;|&nbsp;华卫</p><p>&nbsp;</p><p>当下极为火爆的大模型，在医疗赛道同样炙手可热。谷歌刚刚发布了准确率达&nbsp;91.1%、性能远超&nbsp;GPT-4&nbsp;系列的多模态医学大模型Med-Gemini，国内市场亦很热闹。自2023年以来，百度、腾讯、京东等诸多大厂都相继加码医疗大模型领域，与医疗相关的大模型产品和应用如雨后春笋般正不断涌现出来，其中更不乏&nbsp;AI&nbsp;和医疗企业的手笔。</p><p>&nbsp;</p><p>目前，已有部分医疗大模型产品投入到导诊、预问诊等医院场景中。然而，医疗大模型虽有一定潜力，但现阶段仍有不少要跨越的落地门槛。</p><p>&nbsp;</p><p>为此，InfoQ&nbsp;对云知声AI&nbsp;Labs&nbsp;研发副总裁刘升平进行了专访，听他聊一聊现阶段医疗大模型的商业化能力，以及面向这类应用场景的行业大模型该如何定制优化。在即将召开的<a href="https://sourl.co/faYrKr">AICon&nbsp;全球人工智能开发与应用大会&nbsp;暨&nbsp;大模型应用生态展</a>"上，InfoQ&nbsp;也邀请到了刘升平老师来做演讲分享，他将进一步分享医疗大模型的构建方法和应用落地经验。</p><p>&nbsp;</p><p>以下为访谈实录，经编辑。</p><p></p><h2>医疗大模型距离商业化有多远？</h2><p></p><p>InfoQ：现阶段，医疗大模型要规模化落地还面临哪些现实问题？</p><p>刘升平：主要的问题还是有不少，首先是医生和患者的接受度，特别是有些场景要改变医生的使用习惯。还有一个问题是大模型的部署成本，如果在院里大规模并发使用医疗大模型，硬件成本会比较高。</p><p>&nbsp;</p><p>InfoQ：“幻觉”的偶发出现是大模型目前公认的一个问题，医疗场景对准确度要求会更高，山海在这方面是怎么做的？</p><p>刘升平：“幻觉”的确是核心要解决的问题，我们采用多种手段从多方面降低幻觉，包括保证医疗预训练语料和微调数据的质量和多样性、采用能降低知识幻觉的解码策略、融合医疗知识图谱的知识增强大模型技术、医疗知识检索增强、大模型结果后校验、大模型输出置信度评估等。</p><p>&nbsp;</p><p>InfoQ：您认为哪一个评价标准最能代表医疗大模型的水平？</p><p>刘升平：临床有效性是最能代表医疗大模型水平的关键评价标准，包括模型在实际临床环境中的诊断准确性、治疗建议的合理性以及与专业医生的决策一致性。此外，模型的鲁棒性、泛化能力、可解释性、用户友好性、数据隐私保护以及合规性也是重要的评价维度。然而，临床有效性直接关系到患者的安全和健康，因此如果把医疗大模型应用与临床实践中，它可能是最重要的评价标准。</p><p>&nbsp;</p><p>InfoQ：现在行业内有您认为还不错的其他医疗大模型产品吗？国内外均可。</p><p>刘升平：除了云知声的山海大模型医疗版，最近看到的是谷歌的多模态医疗大模型Med-Gemini，在多项临床任务评测中都表现很好，但还没有在医院得到广泛使用。</p><p>&nbsp;</p><p>InfoQ：在医疗大模型的技术实现、应用效果以及成本部署上，国内与国外有区别吗？</p><p>刘升平：没有显著区别。</p><p>&nbsp;</p><p>InfoQ：您认为医疗大模型真正迎来商业化时代还需要多久？</p><p>刘升平：预计2-5年吧。今年是医疗大模型的应用元年，有部分医院开始尝试一些医疗大模型的应用，随着这些医院推广与积累医疗大模型应用经验，预计医疗大模型会在2-5年内进入更广泛的商业化阶段。</p><p>&nbsp;</p><p>InfoQ：社会接受度上，如何让大众认可大模型的诊断或治疗方案？</p><p>刘升平：要让大众接受并信任大模型的诊断或治疗方案，是一个长期的过程，要考虑很多方面。第一，要提高模型的决策过程透明度，提供可解释的输出，让用户理解模型是如何得出结论的。这有助于建立用户信任，尤其是对于医疗决策这样敏感的问题。第二，要有严格的临床试验，证明模型的诊断或治疗方案与专业医生的判断相当或更优，且这些结果应由独立的第三方机构审核并公开。第三，要让医生参与到模型的开发和应用中，他们可以提供专业知识，确保模型的输出符合医学实践，并在实际应用中监督和调整。第四，要开展公众教育活动，解释人工智能在医疗领域的潜力和限制，消除误解，提高公众的理解和接受度。&nbsp;通过这些措施，应该可以逐步提高社会对大模型在医疗领域应用的接受度和信任度。</p><p></p><h1>山海大模型的实践经验</h1><p></p><p>InfoQ：医疗相比其他场景更复杂且严谨，难度自然也不小，驱动云知声选择在这一领域开发大模型的最重要因素是什么？</p><p>刘升平：云知声选择在医疗领域开发大模型，主要有两个关键因素。一是应用潜力，而医疗领域是一个富文本、富知识的行业，并且医疗大模型在处理医疗病历文书、辅助诊断、药物研发等方面展现出巨大潜力，因为医疗领域是一个很适合大语言模型技术的应用领域。此外，医疗AI市场具有巨大的商业价值，随着技术的成熟和接受度的提高，未来有望形成规模化的商业模式。二是专业积累，云知声深耕医疗领域多年，对医疗业务场景有深入的理解，在医疗数据和医疗AI技术有深厚的积累，也积累了数百家的医疗客户，这有助于医疗大模型的研发和商业化推广应用。</p><p>&nbsp;</p><p>InfoQ：大模型训练过程本身就对数据质量有较高要求，医疗领域的数据则更为特殊，还具有隐私保护、专业知识复杂、经验化知识难以结构化等难题，山海是如何克服的？</p><p>刘升平：山海医疗大模型在训练过程中面临数据质量、隐私保护和专业知识复杂性等挑战，我们采取了两种策略来克服这些问题。一是数据清洗与预处理，对收集到的医疗数据进行严格的清洗，去除噪声和不一致的信息，确保数据的准确性和一致性；同时使用专业的医疗知识进行预处理，如标准化术语等。二是匿名化与脱敏，在遵守相关法规的前提下，对个人健康信息进行匿名化和脱敏处理，以保护患者隐私。</p><p>&nbsp;</p><p>InfoQ：使用开源数据集可能出现产品同质化现象，山海在数据资源方面是如何使用的？</p><p>刘升平：云知声在开发山海医疗大模型时，采取了多种策略来避免产品同质化，确保模型的竞争力。第一，我们使用了不少专有数据集，即云知声多年的医疗业务积累的大量内部医疗数据。这些专有数据可以提高大模型在特定场景的应用效果。第二，&nbsp;我们采用了一些数据增强技术来自动生成训练数据，例如，通过数据合成、噪声注入、标签变换等技术，增加数据的多样性和复杂性，使模型在不同条件下表现更为全面和鲁棒。第三，我们还与医疗专家合作来确保医疗数据的准确性和专业性，同时利用专家的知识来指导数据的预处理和标注。通过这些策略，云知声的山海医疗大模型能够与只使用开源数据集训练的大模型有显著区别，并且在面向具体的医疗场景应用时有更好的效果。</p><p>&nbsp;</p><p>InfoQ：云知声的山海医疗大模型主要做了哪些场景？目前哪个场景的应用率最高？哪个场景能算作山海的“杀手锏”？</p><p>刘升平：对于云知声的山海医疗大模型，主要做了以下场景：</p><p>病历生成：包括基于医患对话的门诊病历和出院小结、手术记录生成等住院病历的生成，以及放射科报告生成等医技科报告。病历质控：对住院病历（包括病案首页）做过程和终末质控，支持1000+形式和内涵质控点，大幅提高病历的质量。单病种上报：对国家卫健委要求的57个病种做自动数据汇集及上报。医保控费：按照医保局的规范，监管医院的临床诊疗行为和收费合理性，确保医疗费用的合规。保险理赔的医疗审核：审核在保险理赔中涉及到的医疗费用，剔除不合理费用。专病库平台：将病历等临床数据自动抽取和导入到专病库。智能问诊：作为AI医生，与患者进行对话，收集症状，并提供初步的健康咨询和建议。</p><p>目前，山海应用率最高的场景是病历生成、病历质控和保险理赔的医疗审核。结合云知声在语音技术上强项开发出的门诊病历生成系统，结合云知声在医疗知识图谱的积累开发的病历质控系统和保险理赔医疗审核系统均可以视为“杀手锏”场景。</p><p>&nbsp;</p><p>InfoQ：针对于山海医疗大模型，您更推荐医疗机构采用哪种部署方式落地？具体是如何考虑的？</p><p>刘升平：云知声的山海医疗大模型在医疗机构的部署通常有以下两种方式：云端部署和私有化部署。至于选择哪种部署方式，主要考虑几个因素吧。一是如果医疗机构对数据安全有极高要求，那就倾向于私有化部署。二是考虑成本与资源，云端部署通常成本较低；私有化部署初期投入大，但长期运营成本可能更低。</p><p>&nbsp;</p><p>InfoQ：现在市面上的医疗大模型不少，国内有许多大厂也在做，山海的独特之处是什么？</p><p>刘升平：这和云知声做医疗大模型的动机是一样的，山海医疗大模型的独特之处主要有两点。&nbsp;一是在专业领域深度方面，云知声专注于医疗领域，有深厚的数据、知识、场景和客户积累，这使得山海医疗大模型在效果上业内领先，目前在医疗大模型综合评测PromptCBLUE和MedBench上都是排名第一。二是在技术融合方面，结合云知声在语音识别和医疗知识图谱技术的专长，山海医疗大模型在语音交互式医疗应用上具有优势，且在临床应用上的医疗知识幻觉也大为减少。</p><p>&nbsp;</p><p>InfoQ：在即将到来的AI&nbsp;con上，您准备向听众分享哪些方面的内容？</p><p>刘升平：主要是分享医疗大模型是怎么用的，是如何做的。我还会以医疗领域为案例，介绍面向应用场景的通用大模型定制优化方法论，相信这对于大模型的行业应用开发有一定的借鉴意义。</p><p>&nbsp;</p><p>嘉宾介绍：</p><p>刘升平，云知声AI&nbsp;Labs&nbsp;研发副总裁，北京大学数学学院博士毕业，是前&nbsp;IBM&nbsp;中国研究院资深研究员，中文信息学会语言与知识计算专委会委员。曾在语义网，机器学习、信息检索，医学信息学，自然语言处理等领域发表过数十篇学术论文和国际国内发明专利。在&nbsp;IBM&nbsp;中国研究院信息与知识组工作期间，刘博士主要负责语义技术及其应用的研发，曾多次获得过&nbsp;IBM&nbsp;研究成就奖。&nbsp;2012&nbsp;年底，刘博士加入云知声&nbsp;AI&nbsp;Labs，领导认知智能团队，负责大语言模型、知识图谱和智慧医疗等方面的研发及管理工作。在云知声期间，主持研发了山海大模型，获得国内外&nbsp;AI&nbsp;评测冠亚军&nbsp;13&nbsp;个，获得北京市科技进步奖一等奖一项。</p><p>&nbsp;</p><p>活动推荐：</p><p><a href="https://sourl.co/faYrKr">AICon全球人工智能开发与应用大会&nbsp;暨&nbsp;大模型应用生态展</a>"将于5月17日正式开幕，本次大会主题为「智能未来，探索AI无限可能」。如您感兴趣，可点击「阅读原文」查看更多详情。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f3/f325163430e0188b28bcaaf57a37a8ff.png" /></p><p>&nbsp;</p><p>会议即将开幕，扫码可预约主题演讲直播，购票或咨询其他问题请联系票务同学：13269078023，或扫描上方二维码添加大会福利官，可领取福利资料包。</p><p>追踪链接：<a href="https://sourl.co/ih3ffe">https://sourl.co/ih3ffe</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/fkmGs83XTyKMBJPs8aFE</id>
            <title>老便宜了！字节跳动豆包大模型开始营业，一元钱能买125万Tokens，月活用户量达2600万</title>
            <link>https://www.infoq.cn/article/fkmGs83XTyKMBJPs8aFE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/fkmGs83XTyKMBJPs8aFE</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 May 2024 08:15:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 火山引擎, 豆包大模型, 模型推理成本, 大模型服务平台
<br>
<br>
总结: 火山引擎发布了由字节跳动研发的豆包大模型家族，以厘计价定价，降低模型推理成本，推出一站式大模型服务平台火山方舟，提供多模态内容家族和个性化定制智能体。 </div>
                        <hr>
                    
                    <p>作者 | 华卫</p><p></p><p>5 月 15 日，火山引擎发布了字节跳动研发的豆包大模型家族，今天起正式开启对外服务。而豆包的定价，让大模型从以分计价进入到了以厘计价的时代。</p><p></p><p>“不仅效果好，人人用得起的才是好模型。”火山引擎总裁谭待表示，大的使用量，才能打磨出好模型，也能大幅降低模型推理的单位成本。</p><p></p><p>据披露，豆包主力模型 pro-32k 版的模型推理输入价格仅为 0.0008 元 / 千 Tokens，相当于一元钱就能买到 125 万 Tokens，比行业价格低 99.3%；在处理 128K 长文本时，豆包通用模型 pro 的推理输出价格为 0.005元/ 千 Tokens。</p><p></p><p>谭待认为，大模型要做好有三个关键挑战：模型效果、推理成本、落地难度，用的人越多，调用量越大，才能让模型越来越好。在 2024 火山引擎春季 Force 原动力大会上，火山引擎推出的一站式大模型服务平台火山方舟、扣子应用也带来了最新的技术升级动态升级。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ff087a1f4f1450373a549c7ad9741cf8.jpeg" /></p><p></p><p>豆包模型官网：https://www.volcengine.com/product/doubao</p><p></p><p></p><h1>豆包模型家族亮相</h1><p></p><p></p><h1>日均处理 1200 亿 Tokens</h1><p></p><p></p><p>豆包系列模型由字节跳动研发，包括从语义、声音到图像的多模态内容家族，还可以创建个性化定制的智能体，能够通过便捷的自然语言或语音交互，高效完成互动对话、信息获取、协助创作等任务。</p><p>其中，豆包通用模型 pro 是字节跳动自研 LLM 模型专业版，具有理解、生成、逻辑和记忆等综合能力，窗口尺寸最大支持 128K 长文本，并可精调，适配场景更加通用。豆包通用模型 lite 是性价比更高的轻量版，对比 pro 版本千 Tokens 成本下降 84%、延迟降低 50%，为企业提供灵活经济的模型选择。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d19ea74921772b27e2cbe9f3fe04b0f4.jpeg" /></p><p></p><p>在声音方面，豆包有具备语音合成、声音复刻和语音识别方面的三个模型，不仅善于表达多种情绪，而且 5 秒即可实现声音一比一克隆，对音色相似度和声音自然度进行高度还原，还支持复刻声音的跨语种迁移。语音识别效果尤其在科技，教育，医疗等垂直领域表现突出，并善于处理口音、噪音等复杂场景的语音识别。</p><p>而豆包·文生图模型擅长对中国特色文化的理解和输出，豆包·Function Call 模型是当前支持扣子的主力模型，可根据不同的输入指令和情景，选择不同的函数和算法来执行相关任务。</p><p>豆包·角色扮演模型则可以根据人物设定进行演绎，具备个性化的角色创作能力、上下文感知能力强和剧情推动能力，可以满足用户更加个性化的角色扮演需求。据字节跳动产品和战略副总裁朱骏透露，豆包上已有超过 800 万个智能体被创建。</p><p></p><p>此外，朱骏还谈到很多豆包在产品设计上的思考。“用户的核心需求没有变化，包括高效获取信息、工作提效、自我表达、社交娱乐等，在快速演化的是技术。对于大模型的应用，其定义了三个设计原则：拟人化、离用户近、个性化。</p><p></p><p>豆包名字的由来正是，希望产品的名字和大模型一样是拟人化的，像身边亲密的朋友或家人在日常生活当中愿意用的昵称一样，能够成为用户随身携带的“语音百事通”、桌面端文案创作小助手、嵌入到用户现有使用环境的代码生成和注释助手。</p><p></p><p>“经过一年时间的迭代和市场验证，豆包大模型正成为国内使用量最大、应用场景最丰富的大模型之一，目前日均处理 1200 亿 Tokens 文本，生成 3000 万张图片。”谭待表示。</p><p></p><p>现场，谭待还首次披露了豆包大模型的月度活跃用户情况，双端月活用户量达到 2600 万。目前，豆包模型已用于豆包 App、扣子、河马爱学、飞书智能伙伴、抖音电商、剪映、番茄小说等字节跳动旗下产品及业务，并通过火山方舟向智能终端、汽车、金融、消费等行业的众多客户提供服务。</p><p></p><p></p><h1>火山方舟升级 2.0 版来了</h1><p></p><p></p><p>此次火山方舟平台进行了全新的升级，推出方舟 2.0 平台，新平台发布了三个重要的大模型插件。火山方舟是火山引擎发布的大模型服务平台，提供模型训练、推理、评测、精调等全方位功能与服务，并重点支撑大模型生态。</p><p></p><p>火山方舟 2.0 升级的主要亮点如下：</p><p>联网插件：提供抖音头条同款搜索能力，能够实时连接海量优质互联网数据和抖音的独有数据，并且可以通过业内领先的意图识别能力，提供给用户更准确和更全面的回答。内容插件：独家上架了抖音内容插件，可以独家的提供抖音丰富的视频和图文内容，并且作为相关重要信息去丰富大模型和用户的交互过程。RAG 知识库插件：内置了字节跳动多年实践沉淀的大规模高性能向量检索能力，百亿级别数据可以实现毫秒级检索，支持秒级索引流式更新，可以实现新增数据能够实时被检索到，知识库插件也内置了豆包向量化模型，中文场景效果领先， 可以给用户提供更好的搜索相关性。同时，文档解析环节集成了飞书优秀的文档解析能力，支持 pdf、doc、ppt、excel、txt、markdown 等多种复杂类型文档解析能力。</p><p></p><p>除了核心插件外，方舟 2.0 也对系统的承载能力、安全保护能力和算法服务能力进行全面提升。首先是系统承载能力，火山方舟提供了超过万卡公有云 GPU 资源池来支持大模型的推理服务，并能够提供 5 秒接入新建精调模型的弹性调度，仅需 3 分钟就能完成千卡扩容，来支撑企业在应用大模型过程中可能出现的突发流量和业务高峰。</p><p></p><p>在安全可信上，方舟 2.0 通过传输加密、数据加密和独有的大模型安全沙箱功能，能够在模型精调、部署和应用的过程中实现安全增强，不仅可以防止恶意攻击模型的污染，而且可以有效保护企业内部数据不会发生泄露。</p><p></p><p>算法服务方面，火山方舟平台配备了专属的大模型的算法团队。</p><p></p><p></p><h1>“人人都是 AI 应用开发者”</h1><p></p><p></p><h1>扣子专业版发布</h1><p></p><p></p><p>“AI 在通常的理解中是一个难且贵的概念，难在于大模型本身的技术复杂性，而贵在于它的训练和推理成本。目前其主要的时间场景仍局限在搜索引擎和修图工具，但大语言模型真正的潜力远不止于此。”扣子产品经理潘宇扬表示，扣子产品能够连接大模型和用户场景。</p><p></p><p>据介绍，作为新一代 AI 应用开发平台，无论是否有编程基础，都可以在扣子上快速搭建基于大模型的各类 bot，并将其发布到各种社交平台、通讯软件或部署到网站等其他渠道。</p><p></p><p>目前，扣子专业版已集成在火山引擎的大模型服务平台“火山方舟”上，提供企业级 SLA 和高级特性。招商银行、海底捞火锅、超级猩猩、猎聘等企业，已在扣子上搭建了智能体。复旦大学、浙江大学等名校也为课程和实验搭建 AI“助教”。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>