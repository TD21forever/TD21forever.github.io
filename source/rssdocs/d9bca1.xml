<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/4cEh6JIUJjou6V6e3lPP</id>
            <title>拖欠半年工资没发，员工拿饮水机抵钱！又一家明星智驾独角兽烧光10多亿后黯然离场</title>
            <link>https://www.infoq.cn/article/4cEh6JIUJjou6V6e3lPP</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/4cEh6JIUJjou6V6e3lPP</guid>
            <pubDate></pubDate>
            <updated>Fri, 23 Aug 2024 09:40:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>&nbsp;拖欠半年工资没发，员工拿饮水机抵钱！又一家明星智驾独角兽烧光10多亿后黯然离场</p><p></p><h1>智驾独角兽禾多科技疑似解散，员工拿饮水机抵钱</h1><p></p><p>&nbsp;</p><p>近日，有多名认证为禾多科技员工的网友在某社交平台爆料，自动驾驶明星独角兽禾多科技已经走上破产清算之路，三四百名员工大半年没发工资，很有可能这半年多的工作变成了义务劳动了。</p><p>&nbsp;</p><p>据该网友透露，由于禾多科技迟迟未发放工资，连办公室的饮水机都被员工搬走抵工资。这一消息得到了多位内部人士的确认，“禾多和广汽之间谈崩了，相当于最后一根稻草没有了。”有消息人士对汽车媒体飞灵汽车如是说。</p><p></p><p><img src="https://static001.geekbang.org/infoq/07/073c1e85568f19af10809666173ef43a.png" /></p><p></p><p>2021 年就拿下广汽定点的禾多科技，近年来陆续为广汽埃安、传祺等品牌与车型提供了智能驾驶方案。智驾投资行情不景气的 2023 年，广汽依然连投禾多两轮。</p><p>&nbsp;</p><p>早在今年3月份，就有禾多科技员工爆料称公司已经暂缓发放了部分工资，且公积金已经断缴3个月，工资从1月起就开始拖欠，只有社保没断。3月底，禾多召开了一次全体会议，高层向员工承诺，在职员工工资延至4月发放，但这部分工资也只发放了一部分，离职员工工资会在6月发放。但工资始终一拖再拖，在承诺的日期到临之际，禾多科技没能兑现承诺。</p><p>&nbsp;</p><p>禾多科技走到如今的地步，早已有迹可循。</p><p>&nbsp;</p><p>8月14日，据晚点Auto消息，智能驾驶方案商禾多科技与广汽集团的重组方案遭遇重大变数，目前禾多科技的资金状况已非常紧张。受此影响，禾多科技正解散数据、研发等大部分核心部门，暂停研发活动。这一情况将禾多科技的未来置于风雨飘摇中。</p><p>&nbsp;</p><p>据知情人士透露，禾多科技与广汽集团商议重组的开始时间可以追溯到今年7月，根据当时的方案，禾多科技将成立一家新企业，并由广汽集团进行资本注入，新公司的主要任务是为广汽旗下的品牌提供智能驾驶解决方案。而到了8月，在重组方案未得到所有股东支持后，禾多科技创始人兼CEO倪凯发内部信告知员工此事，并表示公司将无法支付7月工资和到期的欠薪及公积金。</p><p>&nbsp;</p><p>在事态走向进一步恶劣之前，公司法人倪凯也曾试图挽救公司，他从去年开始已经多次出质自己的股份以换取资金自救。</p><p>&nbsp;</p><p>然而到目前为止，广汽也只表示了“会在评估后将就重组事宜给出回应”，至于禾多科技还能撑多久，一切都是未知数。</p><p></p><h2>入不敷出积弊已久，禾多科技难自救</h2><p></p><p></p><p>禾多科技成立于2017年6月，致力于打造基于前沿人工智能技术和汽车工业技术的自动驾驶方案，具备从车辆线控、多传感器技术到上层自动驾驶核心算法模块的完整布局，是少数拥有全栈自动驾驶研发能力的公司之一。禾多科技的创始人倪凯，是一位在自动驾驶领域具有丰富经验和深厚技术背景的专家。</p><p>&nbsp;</p><p>倪凯本硕毕业于清华大学，后又前往美国佐治亚理工学院攻读计算机博士，专注于计算机视觉、机器人技术等领域的研究。他曾任职于百度深度学习研究院，担任高级科学家，其间创建了百 度的无人驾驶团队，负责无人车的研发和部分高精度地图的工作，也曾在微软的美国西雅图总 部工作，参与三维地图和HoloLens VR眼镜的项目研发。</p><p>&nbsp;</p><p>为了业务发展，禾多科技还请来了在汽车与自动驾驶行业从业20多年，前博世集团ADAS业务 单元中国区负责人蒋京芳加入管理团队，自蒋京芳加入后，禾多科技将不到10人的苏州团队，发展到在上海、苏州拥有近200员工的量产闭环团队。有知情人士称，正是因为蒋京芳，禾多科技才能够搭上广汽。</p><p>&nbsp;</p><p>有了明星创始人和知名高级管理人才加持，禾多科技曾在资本市场受到颇多青睐。</p><p>&nbsp;</p><p>据36氪创投平台数据显示，成立至今，禾多科技已经斩获至少7轮融资，仅是近三年间融资总额就已超过了10亿元人民币。</p><p>&nbsp;</p><p>2021年，禾多科技宣布完成C1轮融资，虽然具体融资金额未详细披露，但该公司在自动驾驶领域的将进一步发展和壮大。</p><p>&nbsp;</p><p>2022年，广汽集团通过广汽资本领投完成了禾多科技的C2轮融资，此次融资总额为1亿美元（约合人民币6.7亿元），主要用于高级别自动驾驶技术的创新开发和规模化量产等方面。</p><p>&nbsp;</p><p>2023年7月，禾多科技再次宣布完成新一轮融资，金额为人民币3亿元，由广东粤科金融集团和广汽资本共同领投。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8d/8d9c30c55f1e108add99b339381ffbe9.jpeg" /></p><p></p><p>即使融了那么多钱，但由于融资事件埋下隐患、量产项目难以盈利、高管团队决策不力等原因，禾多科技仍然走到了穷途末路。</p><p>&nbsp;</p><p>据中国执行信息公开网显示，禾多科技已被北京海淀、江苏苏州和广州花都三地法院列为被执行人，涉及金额近75万元。这些法律问题加剧了公司的财务困境，使其面临更加严峻的生存压力。</p><p></p><h2>智驾企业倒在黎明前已成常事</h2><p></p><p>事实上，倒在盈利和量产前的智驾公司不在少数。因为有一个不可否认的事实：不赚钱的公司最终会耗尽资金后黯然离场。</p><p>&nbsp;</p><p>自动驾驶领域最重要的竞争是可扩展和可持续的商业模式的竞争。谁能通过使用自动驾驶汽车提供服务来真正赚钱，谁才能最终活下去。因为只有赚钱了，才可以开始扩大规模，而无需一味地去寻求外部输血。这种扩张可以让你发展你的品牌，了解你的乘客以及如何为他们创造价值，并收集更多数据，从而让你更快地改进你的技术。拥有持续可行的业务是正反馈循环的开始，这样才能带来可持续的增长和盈利的可能。</p><p>&nbsp;</p><p>然而，盈利能力的最大杠杆之一就是自动驾驶技术本身：研发出一款绝对安全的自动驾驶技术的成本非常高！也因为这种，许多公司都专注于开发该技术，将其作为建立业务的先决条件。简而言之：先让技术发挥作用，然后让业务发挥作用。但这种模式是有风险的。如果技术可行但单位经济效益不理想，技术太贵了市场不买单该怎么办？但如果没有在市场上测试你的想法，又怎么知道你是否做出了正确的技术投资？这像是个死循环。</p><p>&nbsp;</p><p>所以很多自动驾驶企业困在这个死循环里走不出来，他们走的是一条耗光资金走向灭亡的死路。事实是，小公司比大公司耗尽资金的速度要快一点。许多小型自动驾驶公司都经历了这一过程，导致了合并和收购式合并。</p><p>&nbsp;</p><p>但即使是大公司最终也会耗尽资金。2022年，大众和福特决定共同投资ArgoAI，在成立7年烧光37亿美金后解散；刚获得OpenAI投资四个月后，GhostAutonomy宣布关闭全球业务并关闭公司。国内阿里的达摩院去年宣布放弃自动驾驶技术的研发，整个自动驾驶实验室并入菜鸟集团。</p><p>&nbsp;</p><p>Uber ATG 和 Zoox 经历了整合，无法筹集维持自身研发所需的资金。即使是 Waymo、Cruise等公司也已从单一投资者模式转向更加多元化的投资者基础，允许每个投资者限制其风险。例如：当 Waymo 引入 25 亿美元的外部投资时，Alphabet 拥有超过 1100 亿美元的现金。这些公司中的大多数都没有产生任何收入，这是一个巨大的商业风险。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.stcn.com/article/detail/1290593.html">https://www.stcn.com/article/detail/1290593.html</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/krLwpJXWWcR5tmK0Gacj</id>
            <title>《黑神话：悟空》的第二个受害者出现了，竟是AI搜索惹的祸！</title>
            <link>https://www.infoq.cn/article/krLwpJXWWcR5tmK0Gacj</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/krLwpJXWWcR5tmK0Gacj</guid>
            <pubDate></pubDate>
            <updated>Fri, 23 Aug 2024 09:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>整理 | 华卫、核子可乐</p><p></p><p>近日，国产 3A 游戏大作《黑神话：悟空》火爆全网，上线不久便引发全球关注。据国游畅销榜统计的数据，仅仅一日，该游戏在多个平台的总销量已超过 450 万份，总销售额更是超过 15 亿元。与此同时，也出现了一些被其游戏热度所牵连的“受害者”。</p><p></p><p>《黑神话：悟空》在 Steam 解锁当天，某知名游戏主播在直播玩该游戏时，遭遇晕 3D 的情况，并因此上了微博热搜榜首，被一众网友笑称为《黑神话：悟空》“全球首个受害者”。而在 8 月 21 日，又一位该游戏的“受害者”出现了，其相关遭遇竟与微软有关。</p><p></p><p>在微软必应搜索中输入“黑神话悟空客服”，错误地显示了某机锋网员工的个人手机号，并非官方客服电话。此外，还有两个错误的电话号码被标记为客服，其中包括第一财经版权部的联系电话及其邮箱。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/eb/ebd2ab2169149989bd5e4a5af2f5efb6.png" /></p><p></p><p>被泄露电话的当事人表示，他在 5 小时里，接了差不多小 20 个电话。据悉，这一事件发生的主要原因是微软必应 AI 助手错误抓取信息导致其个人信息泄露，之后尽管被抓取的相关文章已删除，受害人已提交申诉等反馈，但错误的“黑神话悟空客服”信息仍一度出现在必应搜索首页。目前，从搜索情况来看，必应团队已对错误信息进行更正。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f8/f835bf18875e9da5e542d2df1955b2f0.png" /></p><p></p><p>作为全球第二大搜索引擎，微软必应覆盖 36 个国家和地区，用户超 6 亿。2023 年 2 月 7 日，微软宣布将 ChatGPT 集成进新版必应 (New Bing)，集成后的新版必应采用 OpenAI 的 AI 模型 GPT 3.5 的升级版 GPT-4。此次事件，或表明暴露了 AI 搜索引擎在信息抓取和处理上存在一定不足。</p><p></p><p>必应悄然改版后，</p><p></p><p>AI 搜索结果将优先显示</p><p></p><p>上个月，微软宣布对必应做出重大更新，搜索引擎将迎来全面改造，开始将 AI 生成的答案优先显示。也就是说，当用户输入搜索查询时，结果页面中将弹出一条由 AI 生成的主答案，详细说明在获取结果时所使用的全部精选信息来源。当然，大家仍然会在必应搜索页面中看到传统搜索结果，只是它们将被显示在 AI 生成素材的旁边（右侧的较小窗格内）。</p><p></p><p>对于这一变革，微软在官方博文中做出解释：“这种新体验将必应搜索结果的固有基础，同大 / 小语言模型（LLM 与 SLM）的强大功能加以结合。它能够理解搜索查询、检索数百万个信息来源、动态匹配内容，并以新的 AI 生成布局显示搜索结果，从而更有效地满足用户的查询意图。”</p><p></p><p>微软也在关于必应生成式搜索的博文列举了部分示例，除了概述摘要功能之外，微软还将提供大语言模型及小语言模型的主要来源链接，用户看到的答案正是由它们创作而成。而在 AI 生成结果之后，则是常规的结果条目列表。</p><p></p><p>例如当查询“大象能活多久”时，回答发的摘要主体后面还列出了影响大象寿命因素的视频；如果用户搜索“什么是意式西部片？”，必应生成式搜索就会显示关于这一电影子类型的历史、起源以及经典作品信息，同时给出指向这些信息的链接与信源。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ca/caede368945bf1bd768489f87c35dca4.png" /></p><p></p><p>当时，微软介绍，这项调整仅向少数必应用户推出，但不久之后应该会逐步扩大开放。微软还在其博文中表示，他们将继续评估 AI 搜索对于网站和读者的影响。有业内人士担心，如果人工智能机器人抓取的内容以直接在聊天窗口或搜索页面中呈现，那么免费创建内容的网站最终将倒闭。</p><p></p><p>对此，微软表示，这种新的 AI 搜索体验是从头开始构建的，也考虑到了这个问题，因而保持了与传统搜索相同的网站点击次数，时间会证明这是否属实。此外，据了解，必应可以选择在结果页面中关闭 AI 生成功能、只显示传统搜索摘要。</p><p></p><p>AI 搜索闹出的笑话</p><p></p><p>现在，微软并不是唯一一家将 AI 生成的结果添加到搜索页面的浏览器公司。随着微软为必应推出更多工具，将更多 AI 功能引入搜索的竞争态势也在逐步升级。</p><p></p><p>然而，无数真实案例正在证明，AI 搜索并不像我们想象中的那般可靠和准确——它可能会出错，某些情况下生成的结果中甚至会显示错误的信息和建议。</p><p></p><p>今年早些时候，谷歌也曾推出过一款类似的工具，名为 AI Overview，旨在留住那些想要直接向 AI 聊天机器人寻求问题答案的用户。但该工具在推出后也闹出过一些笑话，比如建议添加胶水以使奶酪粘在披萨上、回答“地质学家建议每天至少吃一块小石头”等。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/33/330626ba9a9033d187009de25bf4a36b.png" /></p><p></p><p>Arc Search 浏览器在 AI 模式下，信誓旦旦地给出不恰当的医疗建议，“被切断的脚趾最终还会长回来”。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/66/66455fc292875d15f9532c95e2ca7d9d.png" /></p><p></p><p>人工智能搜索引擎 Genspark 向用户推荐一些可能用于害人性命的武器，Perplexity 则剽窃了一些媒体撰写的新闻文章，但并未注明来源或版权归属。</p><p></p><p>此外，AI 生成的摘要信息还可能蚕食其信息来源网站的流量。一项研究发现，由于不再强调文章链接，AI 摘要功能可能将内容发布方的流量拉低 25% 左右。</p><p></p><p>专家警告，AI“幻觉”</p><p></p><p>问题无法真正解决</p><p></p><p>这些新兴 AI 搜索引擎能够凭借其快速生成大量文本，并以令人信服的效果模仿人类文字的能力而广受欢迎，但在其背后，AI“幻觉”也成为影响这些聊天机器人更上一层楼的关键阻力。而遗憾的是，有专家警告称这种情况很可能永远无法解决。</p><p></p><p>美联社发表的一份最新报告强调，大语言模型（LLM）“胡说八道”的问题可能并不像许多技术创始人和 AI 支持者宣称的那样容易解决。华盛顿大学计算语言学实验室语言学教授 Emily Bender 对此表示悲观，“幻觉问题根本无法解决，这是由技术与拟议用例之间不匹配所必然引发的结果。”</p><p></p><p>根据 Jasper AI 公司总裁 Shane Orlick 的说法，某些情况下适当的“胡说八道”反而并不是坏事。Orlick 解释称，“幻觉实际能带来额外的好处，一直有客户在感谢我们带来的启发，而根源就是 AI 可能在种种机缘巧合之下输出客户自己从未想到过的故事或者角度。”</p><p></p><p>同样的，AI 幻觉对于 AI 图像生成也有着巨大的助益，Dall-E 和 Midjourney 等模型正是凭借这份想象力生成了引人注目的精彩图像。也就是说，只有在文本生成领域，幻觉才是个真正困扰用户的问题，特别是在新闻报道等高度强调准确性的场景之下。</p><p></p><p>Bender 指出，“大语言模型的基本原理就是‘编造’内容，这也是其一切功能的根本。但由于能力源自编造，所以当它们输出的文本恰好可以正确匹配我们的提示词时，这种情况反而是种偶然。哪怕经过微调的模型能够在大多数情况下都保持正确，它们也仍无法彻底摆脱故障。而且，未来的幻觉很可能以文本阅读者更难以注意到的模糊状态存在。”</p><p></p><p>结&nbsp; &nbsp; 语</p><p></p><p>大语言模型是种能够实现非凡功能的强大工具，但企业乃至整个科技行业必须意识到一点——不能单纯因为某种事物很强大，就认定它是一种好用的工具。就像冲击钻也很好用，能够轻松破开人行道和沥青路面，但没人敢把它带到考古挖掘现场。</p><p></p><p>正如 Bender 所指出，大语言模型在最初开始训练的那一瞬间，就是在尝试根据我们给出的提示词预测序列中的下一个单词。训练数据中的每个单词都被赋予了权重或者百分比，以便在给定的上下文中追踪之前既有的给定单词。可这些起先的单词本身并没有充分切实的含义或者重要的上下文来保证输出准确。</p><p></p><p>换言之，这些大语言模型只是出色的模仿者，它们实际并不清楚自己到底在说些什么，所以过度信任它们只会令用户陷入困境。这个弱点是大语言模型所固有的，尽管“幻觉”可能在未来的迭代中逐渐减少，但问题本身却可能永远无法被真正修复。</p><p></p><p>参考链接：</p><p></p><p><a href="https://www.engadget.com/microsoft-is-adding-ai-powered-summaries-to-">https://www.engadget.com/microsoft-is-adding-ai-powered-summaries-to-</a>" 必应 -search-results-203053790.html?src=rss</p><p></p><p><a href="https://www.techradar.com/computing/artificial-intelligence/">https://www.techradar.com/computing/artificial-intelligence/</a>" 必应 -has-been-revamped-to-prioritize-ai-search-results-whether-you-like-it-or-not</p><p></p><p><a href="https://www.techradar.com/computing/artificial-intelligence/chatgpt-and-other-ai-chatbots-will-never-stop-making-stuff-up-experts-warn">https://www.techradar.com/computing/artificial-intelligence/chatgpt-and-other-ai-chatbots-will-never-stop-making-stuff-up-experts-warn</a>"</p><p></p><p><a href="https://techcrunch.com/2024/07/24/bing-previews-its-answer-to-googles-ai-overviews/">https://techcrunch.com/2024/07/24/bing-previews-its-answer-to-googles-ai-overviews/</a>"</p><p></p><p>内容推荐</p><p></p><p>在这个智能时代，AI 技术如潮水般涌入千行百业，深度重塑生产与生活方式。大模型技术引领创新，精准提升行业效率，从教育个性化教学到零售精准营销，从通信稳定高效到金融智能风控，AI 无处不在。它不仅是技术革新的先锋，更是社会经济发展的强大驱动力。在 AI 的赋能下，我们正迈向一个更加智能、便捷、高效的新未来，体验前所未有的生活变革与行业飞跃。关注「AI 前线」公众号，回复「千行百业」获取免费案例资料。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c0/c0779541886d6212211f10391187b0f5.png" /></p><p></p><p>会议推荐</p><p></p><p>InfoQ 将于 10 月 18-19 日在上海举办 QCon 全球软件开发大会 ，覆盖前后端 / 算法工程师、技术管理者、创业者、投资人等泛开发者群体，内容涵盖当下热点（AI Agent、AI Infra、RAG 等）和传统经典（架构、稳定性、云原生等），侧重实操性和可借鉴性。现在大会已开始正式报名，可以享受 8 折优惠，单张门票立省 960 元（原价 4800 元），详情可联系票务经理 &nbsp;17310043226 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/79/791c6d47a29abdea4f3ba09bea3b176a.png" /></p><p></p><p>今日荐文</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621971&amp;idx=1&amp;sn=5e58c5a72a2d7fae816471954959b349&amp;chksm=fbeba49ccc9c2d8a35501b45bea911c9b684944634522011a859022dad48c9fda4b3a4e3b8fc&amp;scene=21#wechat_redirect">《黑神话：悟空》太狠了！Steam 瘫痪、多家公司放假，英伟达老黄又要躺在新的印钞机上数钱了</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621831&amp;idx=1&amp;sn=5ff4ba1979a3e77a914e8b6c5d390db7&amp;chksm=fbeba508cc9c2c1e83c061d1dbd107dca94d93ccda4172996c67d7920e2846d39123b77ee22b&amp;scene=21#wechat_redirect">“印度马斯克”要发印度第一款 AI 芯片，号称超越英伟达！CEO 要“狼性”，但把数十亿美元打水漂</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621777&amp;idx=1&amp;sn=c6805493b8fdc7fefe72e8fd99cbd323&amp;chksm=fbeba55ecc9c2c48d97ff000e9874945ff2424c25e3d61d3b6f9d1e016fff7c5d8b449556e4a&amp;scene=21#wechat_redirect">朱啸虎押注的AI公司被围攻：领导多次让员工“去死”；小红书激励不再与职级挂钩；谷歌前CEO：AI创业可先“偷”后处理｜AI周报</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621772&amp;idx=1&amp;sn=031cd69a65396e1e2bd2c9937008134c&amp;chksm=fbeba543cc9c2c554037e36e088163440a7dd3c994673e9ac9d66d4f58aa2fce516eb290d868&amp;scene=21#wechat_redirect">要求员工点赞拉踩贴、抢到对方客户给奖金！40 多位知情人曝这两家 AI 数据商业巨头“生死大战”，如今“开撕”微软</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621721&amp;idx=1&amp;sn=ff4df9b1712358edc181e34ea0a3c89c&amp;chksm=fbeba596cc9c2c8000958fc0def83fe925093fbe55c2f4ae8b0b979c6df37c49a0ceb75e59cc&amp;scene=21#wechat_redirect">成本直降90%、延迟缩短80%！Anthropic将API玩出了新花样，网友：应该成为行业标配</a>"</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c5/c548af7ad39632ca346cd454eef4d2a8.gif" /></p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247620641&amp;idx=1&amp;sn=44cdebfad0decb39633875fc4069c7fc&amp;chksm=fbeba1eecc9c28f81fc4c7c10d9e95329e4e0ed2d4c1f0eff6ca7f19376846f6a297b33e15d3&amp;scene=21#wechat_redirect"></a>"</p><p></p><p>******你也「在看」吗？******👇</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/s8bfk2wlAdJJODyViD0c</id>
            <title>跟着小扎不白干，9 个月“出师”：用学到的 10 条经验搞出 AI 界“带货王”，年入 1 亿美元</title>
            <link>https://www.infoq.cn/article/s8bfk2wlAdJJODyViD0c</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/s8bfk2wlAdJJODyViD0c</guid>
            <pubDate></pubDate>
            <updated>Fri, 23 Aug 2024 06:02:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>编译 | 核子可乐、华卫</p><p></p><p></p><blockquote>一位Facebook的早期员工Noah Kagen创业成功后，在他的个人网站上分享了他从马克·扎克伯格（Mark Zuckerberg）和 Facebook 那里学到的10条经验教训。Kagen是Facebook的第30号员工，在工作时长9个月后被扎克伯格解雇。离开Facebook后，他创立了软件产品推广和营销平台AppSumo ，并通过总结的扎克伯格工作“之道”将其打造为一家年收入 1 亿美元的公司。AppSumo是一家专注在软件产品的 LTD 平台，一方面为软件产品的开发者提供销售 LTD 的渠道，另外一方面为小公司和创业者提供了购买 LTD 的渠道。在生成式 AI 兴起后，AppSumo 也帮助大量带有 AI 功能的产品提高了销售收入、流量和订阅用户，包括AI聊天机器人平台Juphy、AI 内容生成工具Castmagic等。</blockquote><p></p><p></p><p></p><h1>在扎克伯格手底下干活，我的一点心得</h1><p></p><p></p><p>第一次走进位于帕洛阿尔托大学大道的Facebook总部大楼的时候，我竟一时分不清自己身在高校社团还是创业公司。天花板上吊着电缆，人们匆匆往来，而我则按要求在其他人的办公桌角上挤出个位置。</p><p></p><p>我的新上司从身边走过，说午饭之后再来找我谈话。之后又有人塞给我一台笔记本电脑，闲来无事我就先上会网。后面，有人告诉我得马上开始准备，30分钟后得在马克·扎克伯格的即兴会议上做汇报。</p><p></p><p>扎克伯格走进会议室，平静地告诉我：“你的上司刚刚被炒了，欢迎来到Facebook。只要你不背着我出场公司利益，那就能在这里好好待下去。”而好戏，这时候才刚刚开场……</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/17/17276988ffae1fcb17e877c916de7f25.png" /></p><p></p><p></p><p>在Facebook的工作经历，可以说是我这辈子最美好、但也最痛苦的一段回忆。我是公司第30号员工，而短短9个月之后就被解雇了。很长一段时间，我一想到自己被裁撤的命运就非常痛恨这家企业。</p><p></p><p>但我从扎克伯格和Facebook那边学到的经验，最终也帮助我将AppSumo打造成了一家年收入上亿美元的公司。下面聊聊我在扎克伯格手底下工作时，自己总结出来的10条经验：</p><p></p><p></p><h2>1.专注于单一目标</h2><p></p><p>我曾经恳求道，“马克，咱们一直没能盈利。要不试试在Facebook办的会上销售门票？”他说不行，之后用白板笔写下了几个字：增长。</p><p></p><p>马克的目标是让Facebook拥有10亿用户。面对我们提出的每个主意，他都会问：“这对业务增长有帮助吗？”如果这些想法跟业务增长的目标关系不大，那就果断放弃。</p><p></p><p>快速成长不是同时把多件事做到80分，而是专注于把一件事做到100分。</p><p></p><p></p><h2>2.加快脚步</h2><p></p><p>在Facebook，每天工作12个小时以上属于常态。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/c0/c080fbd7070c846f4bdbdb237cf5a35d.png" /></p><p></p><p></p><p>马克总在敦促我们要有紧迫感。他在公司里常说的口头禅就是“加快脚步，打破常规”。“如果你还没打破常规，就说明你的脚步还不够快。”</p><p></p><p>我们的想法很简单，为了加快行进速度、全面了解社区需求，我们宁愿忍受一定数量的bug和缺陷。每天，我们都会向网站发送几项更新。相比之下，像微软这样的公司则需要几个月的时间才能勾勒出产品细节，之后经过大大小小的会议讨论，最后再开始着手构建。</p><p></p><p>作为一家初创公司，我们跟行业巨头相比的最大优势就是速度。</p><p></p><p></p><h2>3.只雇佣最出色的员工</h2><p></p><p>马克只会雇佣那些他愿意与之共事的员工，甚至我们的客户支持团队里，也挤满了来自哈佛的博士。这帮曾经效力于Facebook的人们后来参与创立的Asana、Quora、AppSumo还有OpenAI等等。</p><p></p><p>对于任何一家初创公司来说，雇佣的前十个人都是最重要的，而其中每个人都占据公司的10%。如果有三个人不够优秀，那就代表公司里30%的部分不够优秀！</p><p></p><p>相较于大公司，初创企业更依赖于优秀的人才。</p><p></p><p></p><h2>4.善待员工</h2><p></p><p>马克意识到，打造出让人愿意身处其中的工作环境不仅有助于吸引更多优秀人才，同时也能让现有员工生出对企业的自豪感来，甚至愿意主动加班。</p><p></p><p>因此，Facebook做了很多现如今已经被视为行业常态的探索：</p><p>⦁ 在硅谷最昂贵的社区之一设立一座豪华办公楼。</p><p>⦁ 开出极具竞争力的薪酬。</p><p>⦁ 为每个人购置1000美元的办公椅。</p><p>⦁ 免费提供PowerBook和黑莓手机。</p><p>⦁ 提供美味的早、中、晚餐。</p><p>⦁ 冰箱里有你所能想到的任何饮料。</p><p>⦁ 公司支付拉斯维加斯旅行的所有费用。</p><p>⦁ 每周五免费餐食发放。</p><p>⦁ 免费洗衣/干洗服务。</p><p>⦁ 补贴住房。如果住在办公楼周边1英里之内，每月可以领取600美元。</p><p>⦁ 面向全体员工开放的夏季/冬季度假小屋。</p><p></p><p>人们希望得到认可，而这种对员工的善待能够提高工作效率，帮助大家抖擞士气。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/f5/f5fd04099933a79f101ffae3b513fe3d.png" /></p><p></p><p>Facebook早期派对</p><p></p><p></p><h2>5.按自己的路子走</h2><p></p><p>很多人之所以会选择那些自己不太了解或并不感兴趣的创业领域，是因为他们听说这个方向很“热门”。比如，他们原本是做会计的，但创业时却尝试帮内容创作者开发专业软件……这简直是在胡扯。</p><p></p><p>从一开始，马克想搞的根本不是什么初创企业——他只是想帮大学里的学生们建立联系。而我自己创办AppSumo，是因为我喜欢科技产品和处理交易。</p><p></p><p>不少顶尖企业刚开始都是这样来的，创始人们先是尝试解决自己面临的问题，之后再把解决方案分享给更多人。这就叫生于自私，而成于无私。</p><p></p><p></p><h2>6.关注细节</h2><p></p><p>我记得马克曾经在凌晨3点给我发过一封电子邮件，告诉我在一份文件中漏了一个句号。是的，一个句号！！</p><p></p><p>马克不接受任何不完美的东西。如果他觉得某个项目做得不好，就会告诉负责人果断放弃、推倒重来。他对Facebook里面这个F要大写就特别偏执，甚至曾经送给我一本语法书让我好好打磨文笔 😂</p><p></p><p>马克为我们设定了高到卓越的标准，这让工作做起来很有挑战性，但也非常有益。</p><p></p><p></p><h1>7.向团队放权</h1><p></p><p>令人意外的是，马克却并不会过多参与日常运营。虽然有时候也会参与代码编写，但他的大部分时间都专注于制定宏观愿景。他特别擅长给人们设定目标、划出界限，然后从旁提供指导。</p><p></p><p>工程师和产品经理们可以自行提出功能并着手构建，期间无需任何额外的审批和干预。马克曾说他想要Facebook的手机版，而初版的所有细节都由我们一线开发自行斟酌。</p><p></p><p>只有团队感受到这种主人翁的地位时，大家才会像主人一样思考和行事。</p><p></p><p></p><h2>8.是“人”，不是“用户”</h2><p></p><p>每当有人使用“用户”这个字眼，马克都会气得大叫。没错，就是音量很大那种。他咆哮道，“那些是活生生的人！”</p><p></p><p>在产品中充分考虑人性化因素，能让厂商更好地为客户服务。与只看数字相比，这个角度也能让我们更好地理解困扰受众的问题。</p><p></p><p>所以要永远记得，冷冰冰的用户名和邮件地址背后，对应的都是活生生的人！</p><p></p><p></p><h2>9.只留合适的人</h2><p></p><p>就在我入职的当天，我顶头上司被开除了。我的下任上司在一个月后被炒掉，而我自己是在9个月之后。马克非常重视的一条原则，就是只留合适的人。</p><p></p><p>他会果断解雇那些拖累了Facebook发展的人，并迅速提拔能够帮助Facebook实现目标的人。</p><p></p><p>在AppSumo，我们也会对潜在的新同事进行付费试用，之后再决定对方适不适合接受这份全职岗位。</p><p></p><p></p><h2>10.风物长宜放眼量</h2><p></p><p>当初马克面对10亿美元的Facebook收购要约时，我们都才20多岁。而当他表示拒绝时，实际是向我们所有员工包括全世界发出了明确的信息：他的目标是让整个世界连通起来，这让我们无比兴奋。</p><p></p><p>当初在Facebook工作时，我做的一切就是思考、讨论和畅想Facebook的未来。这甚至不像是一份工作，Facebook就如同我的女朋友，占据了我的所有时间和心力。</p><p></p><p>这种宏大的愿景激励员工们从床上蹦起来，冲进办公室尽最大努力完成工作。它让员工们有了一种超越金钱的目标感和使命感。</p><p></p><p></p><p></p><h1>被Facebook解雇的四点反思</h1><p></p><p></p><p>除从扎克伯格那里学到的有用经验外，此前 Kagan 还曾在一本电子书里总结了自己被 Facebook 解雇的原因。 在 Kagan&nbsp;看来，自己过去在Facebook的工作中犯了四个错误，才导致扎克伯格认为他是一个需要被解雇的“累赘”。</p><p></p><p></p><h1>1.向媒体泄露了公司机密</h1><p></p><p></p><p>在科切拉音乐节上的一次醉酒后，Kagan&nbsp;告诉外媒TechCrunch的创始人迈克尔·阿灵顿（Michael Arrington），Facebook计划将业务范围从大学生扩展到为Microsoft和Apple等公司提供专业社交网络。原本Facebook准备在第二天早上公布这一消息，但在与 Kagan&nbsp;的谈话后，阿灵顿当晚便发布了这一新闻。几周后，Kagan&nbsp;便被解雇了。</p><p></p><p></p><h2>2.试图利用Facebook为自己出名</h2><p></p><p></p><p>Kagan自述，他过去常常在Facebook总部举办创新企业聚会，因为他享受炫耀自己的工作场所，还经常在自己的个人网站 OKDork.com 上写关于Facebook业务的博客文章。扎克伯格曾将Kagan拉到一边，让他在自己和Facebook之间做出选择。不知何故，卡根当时仍然没有理解扎克伯格的意图，因此后来也没保住自己的工作。</p><p></p><p></p><h2>3.工作中出现失误</h2><p></p><p>Kagan对此举了一个例子：“我在与（Facebook联合创始人）达斯汀·莫斯科维茨（Dustin Moskovitz）合作决定哪些公司能够加入我们的专业网络时，负责在谷歌上搜索企业名单。经过一个星期的收集，我给出的公司名单乱七八糟，没有任何顺序可言。把这份名单交给达斯汀后，他当然很失望。之后他运行了数据库查询，并根据我们已经在网站上注册的公司域名汇总了一些公司，然后将这些公司添加到候补名单中。是的，这样做聪明多了。”</p><p></p><p></p><h2>4.跟不上Facebook的增长</h2><p></p><p>Kagan 加入 Facebook 时，该公司只有 30 名员工和几百万用户。当他被解雇时，公司已经有 100 多名员工，并逐渐发展成为一家发展速度稍慢、需要管理的人更多的公司。而Kagan之后并没有改变自己的工作方式以适应公司文化的变化，还进行了一定程度的抵制。他写道：“在事情混乱和需要完成任务的时候，我是公司里最出色的员工之一。但我在处理多人的项目、组织几个月的进度计划以及处理政治事务方面都很吃力。”</p><p></p><p>参考链接：</p><p><a href="https://noahkagan.com/what-i-learned-working-for-mark-zuckerberg/">https://noahkagan.com/what-i-learned-working-for-mark-zuckerberg/</a>"</p><p><a href="https://www.businessinsider.com/how-noah-kagan-got-fired-from-facebook-and-lost-185-million-2014-8">https://www.businessinsider.com/how-noah-kagan-got-fired-from-facebook-and-lost-185-million-2014-8</a>"</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/OyhqIHriWz00fr7k3xcH</id>
            <title>携手攀登安全“芯”高地！2024紫光同芯合作伙伴大会安全芯片创新应用论坛圆满落幕</title>
            <link>https://www.infoq.cn/article/OyhqIHriWz00fr7k3xcH</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/OyhqIHriWz00fr7k3xcH</guid>
            <pubDate></pubDate>
            <updated>Fri, 23 Aug 2024 03:07:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>8月22日，2024紫光同芯合作伙伴大会安全芯片创新应用论坛在北京圆满落幕。本届论坛以“智慧芯生态&nbsp;互联芯安全”为主题，聚焦金融支付、电子证件、安全识别与移动通信领域的硬件创新、软件算法、技术趋势等行业议题，产业链各方汇聚一堂，为安全芯片创新应用发展和产业生态建设提供了全面解题思路和最佳实践参考。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/c9/ca/c9a84a132d39be36c2d32c52e72a97ca.png" /></p><p></p><p>&nbsp;</p><p>支付、证件、识别：芯之所向，无所不至</p><p>&nbsp;</p><p>在数字化浪潮推动下，金融支付行业面临转型升级，安全芯片应如何乘势而上？来自北京银联金卡科技有限公司、金邦达有限公司、福建新大陆支付技术有限公司等企业的代表分别发表演讲。他们表示，作为保障金融交易安全不可或缺的一环，安全芯片的功能优化升级成为行业共识；面对生物识别、物联网支付等新兴技术和日益复杂的应用场景，安全芯片将在安全性、便捷性、功能性这几个关键维度实现更大突破。</p><p>&nbsp;</p><p><img src="https://static001.infoq.cn/resource/image/b4/0e/b468c48938af12502f971f9516e3e50e.png" /></p><p></p><p>&nbsp;</p><p>论坛上，紫光同芯安全芯片事业部副总经理路倩发表了《芯之所向无所不至——支付证件识别产品创新之路》主题演讲，她表示，安全芯片产品从聚焦性能提升和应用演进，到追求为行业打造极致安全、极致可靠、极致便捷和极致性价比的产品生态，紫光同芯产品的技术迭代与市场需求和行业标准密不可分，并将始终遵循以客户为中心和以专业技术为基石的原则。未来，紫光同芯将通过技术创新，紧密贴合国际安全标准的升级步伐，灵活适应日趋多元的应用场景，持续不断为行业注入芯动力。</p><p>&nbsp;</p><p>移动通信：无般不识，大器已成</p><p>&nbsp;</p><p>在AI、5G等前沿技术推动下，数字化进程渗透千行百业。作为数字化时代的基础支撑，全球信息通信行业监管正在向以促进数字经济发展为目标的新方向演进，技术创新、数字化转型、算网融合、安全保障提升成为大势所趋。</p><p></p><p>探讨当前通信行业趋势，中国移动研究院业务研究所和星汉智能科技股份有限公司等企业的代表分别发表演讲。聚焦超级SIM多应用操作系统的生态建设，中国移动联合产业制定了多应用操作系统产业标准，并将持续推进多应用操作系统的泛行业生态建设，坚持以开放和协作促进产业可持续发展；拥抱数字化转型机遇，星汉智能表示，作为数字化底层基础，eSIM技术促进各行各业数字化转型的进程，将成为企业发展数字产品及服务不可或缺的重要支持。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/8e/71/8ec06f0fddb4c5acc5b25c2728b51571.png" /></p><p></p><p>&nbsp;</p><p>紫光同芯安全芯片事业部副总经理、移动通信产品线总经理王征结合紫光同芯在通信领域的创新探索，回顾了SIM卡持续发展的历程。他表示，SIM从追求极致性价比出发，历经追求极致可靠的M2M SIM，直至现今在追求极致性能与安全并重的eSIM领域深耕细作，紫光安全芯片已经在移动通信领域实现了全品类覆盖，帮助全球行业客户布局数字产业生态。未来，随着AI、5G及卫星通信等技术发展，紫光同芯的SIM之路将朝着更便捷、环保、安全、强大的方向继续演进，为万物互联注入芯力量。</p><p>&nbsp;</p><p>智慧芯生态，互联芯安全。紫光同芯以技术为基础，以客户为中心，以市场为导向，致力于为全球伙伴提供安全可靠、高效便捷的产品和服务。未来，紫光同芯期待与更多伙伴一道聚合产业优势，融通生态链条，驱动技术创新与应用覆盖，助推产业生态向更智能、更便捷、更安全的方向加速迈进，以科技之光照亮幸福生活。</p><p></p><p>活动推荐：</p><p></p><p>芯片作为最底层的设施受到许多从业者的关注，在10 月 18-19 日，由InfoQ主办的 QCon 全球软件开发大会（上海站）上，我们特别策划了【大模型基础设施与算力优化】专题，将深入探讨如何搭建稳定高效大模型基础设施，提高各类大模型训练推理过程中的 Scaling 的效率和成本，为一线技术工程师和高级技术管理人员提供前沿知识、一手的实践经验和有深度的技术判断。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/df/dfd31ee989a7951439a77fec138d4cf8.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/xLgQzB1Wc1OSYZCXe8c7</id>
            <title>科大讯飞做大模型：功能不需样样顶尖，先打造业务需要的能力</title>
            <link>https://www.infoq.cn/article/xLgQzB1Wc1OSYZCXe8c7</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/xLgQzB1Wc1OSYZCXe8c7</guid>
            <pubDate></pubDate>
            <updated>Fri, 23 Aug 2024 01:42:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>“讯飞研究院并非一个纯粹闭门造车的技术研究院。”科大讯飞副总裁、研究院院长刘聪说道。从 2005 成立至今，讯飞研究院为科大讯飞的产品提供了有力的技术支持，这次大模型浪潮中也不例外。</p><p></p><p>正如刘聪所说，“研究院的大部分技术都对应着具体的业务需求。”讯飞研究院一边迭代自己的基座模型，一边深入业务需求进行相关研发。</p><p></p><p>2022 年 12 月，讯飞启动了“1+N 人工智能大模型技术及应用”专项攻关，其中“1”代表通用人工智能大模型底座，“N”代表将人工智能大模型技术应用在教育、医疗、汽车、办公、智能硬件等多个行业领域。如今，讯飞对“1”和“N”的理解发生了哪些变化？</p><p></p><p>开发，今年的节奏已经不同</p><p></p><p>从去年 5 月星火大模型首个版本发布至今，一年多的时间里，讯飞研究院已经将该模型迭代到了 4.0 版本，模型也从最初的对标 GPT-3.5，更新至迭代最新的 GPT-4 Turbo。</p><p></p><p>纵观整个去年，讯飞很多产品是集中发布的，基本上 2、3 个月就有一次产品发布。这个节奏与之前几乎一年一次发布的讯飞相比要快很多。</p><p></p><p>刘聪介绍，这个时候的讯飞，更多扮演的是“追赶者”的角色：摸索整个大模型训练过程中的各种经验，比如如何处理数据、scaling law 是否符合预期等，对标国际领先模型，同时关注一些落地场景。另外，国产化也是讯飞要重点推进的工作。</p><p></p><p>对于去年的整体节奏，刘聪认为讯飞做得是比较好的，这源于讯飞会提前做好发布计划，“我们更多的是计划做得比较好，让大家感受到了每个大版本之间的变化。”</p><p></p><p>但是，今年的节奏导向已经与去年有所不同。</p><p></p><p>“今年年初，我们就对大模型这件事情已经摸索得比较清楚了。”刘聪说道，“我们现在既关注通用底座大模型，同时探索也在如何提升小模型的能力和效果。”</p><p></p><p>当前，大模型与小模型并行发展已经是行业趋势。对于选择大模型还是小模型，刘聪表示主要看场景需要什么样的模型。“如果只泛泛地说‘使用小模型与大模型差距不大’，这纯粹是胡说。”</p><p></p><p>刘聪解释道，在撰写文案、代码编写等方面，一个中小规模的模型即可搞定，讯飞将这类应用定义为一般任务；中等任务涉及行业内的很多知识库和行业深度内容，还有一些高难度任务，例如复杂推理、数学推理等，目前大模型都无法解决，更不用提小模型。</p><p></p><p>“我们一直强调大、小模型时代，并不意味着不再关注大模型了。核心技术原理是先找到大模型的天花板，再优化小模型。小模型的不断进步依赖于大模型的发展。”刘聪说道。</p><p></p><p>另外，讯飞研究院更重要的一项任务是围绕“N”中的刚需场景，把大模型应用做透彻，因此深入解决系统化问题变得非常关键。</p><p></p><p>不过在众多的基础能力上，讯飞也是有选择地进行研发。比如在通用任务中，讯飞最关注的能力之一是数学，因为在刘聪看来，数学能力与推理结合是大模型聪明的表现。</p><p></p><p>但是，不同于有的公司有专门的文生图产品，讯飞的文生图是在星火统一入口里面使用。刘聪明确称，“在文生图方面，我的优先级较低，甚至不专门制作文生视频。虽然我们与视频关系不大，但是我们会制作虚拟人、加强语音能力，我们必须做好语音交互。”</p><p></p><p>在刘聪看来，大模型底座是向多模态拓展的，对讯飞而言多模态的能力逐步提高最重要，但没有必要在一些业务关联度低、资源投入过大的方面做太多投入。在多模态中，刘聪会将重点放到 OCR（Optical Character Recognition，光学字符识别）上，“确保 OCR 做到最好，这与我的实际工作紧密相关。”</p><p></p><p>基于此，讯飞今年的重点虽然还是大模型通用能力的打造，但讯飞不会选择样样争第一，而是在自己认为的最重要的方向发力，比如交互能力等。</p><p></p><p>应用，选择更加熟悉的方向</p><p></p><p>讯飞研究院的研发工作与业务紧密相连，在研发之前，研究院要与业务部门达成深度共识，比如某个功能达到什么程度、完成客观技术指标后能为用户带来什么价值等。</p><p></p><p>达成共识之后，从研究院内部的算法研发部门、工程引擎部门、服务平台部门和资源部门，再到产品研发部门，整个过程需要一起对齐。无论发布产品、然后不断迭代，还是创新性研发一个产品，都是这样的过程。</p><p></p><p>讯飞被外界认为是较少能真正将技术实现产品落地的企业，刘聪认为这背后的核心原因是讯飞更加深入场景。</p><p></p><p>“我们找 PMF 之所以准确，是因为过去对行业场景和技术的积累。坚持阶梯原则，我们了解大模型在哪个节点可以适配、哪个场景可以发挥价值。”刘聪说道。“此外，讯飞也有深厚的场景资源和用户基础。”</p><p></p><p>以学习机为例，讯飞过去十几年从事学校工作，每天在学校里与老师打磨，持续了解中国教育政策以及未来发展趋势。老师的教学环境如何、不同年龄段的孩子是否有时间额外学习等，如果仅凭想象和拍脑袋是很难定义出来的。教育行业讲究因材施教，而非图文等技术。</p><p></p><p>落地中，选择在已积累的行业优势基础上进行大模型探索，是大多数相对成熟公司会选择的风险相对较低的策略。“自我造血非常重要，所以我们更加关注相对熟悉的方向，例如教育、医疗、办公、汽车和金融。”刘聪说道。</p><p></p><p>而什么时候完成应用则与大模型发展阶段有关系。围绕刚需场景，什么技术可以支撑、支撑度如何等都需要考虑。比如技术阅卷，之前是判断填空、选择题，后来扩展到了解答题并全学科阅卷，这都对技术要求越来越高。有了大模型后，直观的表现之一就是作文批改比之前做得更好。</p><p></p><p>讯飞业务中，硬件是不可忽略的一部分，比如有面向教育的学习机、批阅机等。讯飞业务的特点之一就是每个行业都有软硬件的差异。比如学习机不断将软件功能加到硬件上，以此提升硬件附加值。同时，硬件模式又能助力软件，例如翻译机和办公本都有一些大模型应用来升级体验，这不仅仅是单纯利用大模型的 API 连接，而是形成了适合硬件场景的独特功能。</p><p></p><p>而对外服务中，刘聪观察到，大模型的应用范围已经逐渐变大，比如金融这样的代表性场景已经往央国企拓展。“对应用大模型的企业来说，产品价值最重要的是能否降本增效。”刘聪说道。</p><p></p><p>讯飞在对 B 端业务服务过程中，发现算力统一难和整个数据管理难等问题。另外，在对外服务过程中，由于很多企业是私有化部署，因此讯飞在底座模型应用和场景开发中，对用户的场景并不清楚。为此，讯飞通过智能体平台这样的服务来解决。</p><p></p><p>“N 的逻辑必须落地。现在的阶段与去年不同，去年我们的 1+N 有些冗余，需要继续梳理。今年我们将主要的 N 梳理清楚后，一和 N 的协同变得更加系统。”刘聪说道。</p><p></p><p>根据实践观察，刘聪总结了两点经验：</p><p></p><p>第一，不必专门针对“N”，可以将其合入“1”的能力中。一个场景下的常用能力可以满足，或者在 1 基础上做某个智能体就能满足，合入“1”里就可以，这是减少重复开发的逻辑。</p><p></p><p>第二点，统一模型接口和数据接口。这里的 N 可能是业务线主导，有的是研究院主导，但一个公司内部的每个业务数据标注体系如果都不同，那将它们合并汇总到主模型就会相当困难。完成模型接口后，需要标注数据、SFT 数据和强化学习数据，形成一个技术体系。在此框架下，用户可以自行寻找专家进行标注，这样既能优化流程，又能将这些 N 的数据回流到数据库中。</p><p></p><p>结束语</p><p></p><p>对于今年讯飞的“1”和“N”而言，刘聪表示，“虽然是动态发展的，但是不能放弃。如果不演进，三个月就不行了。”</p><p></p><p>不过，随着模型规模的增大，研发周期会逐渐拉长，因此刘聪认为大模型技术后续可能不一定还那么卷。“GPT-5 底座大模型投入巨大，升级周期会变长，局部亮点可能会不断出现，但可能很难有 GPT-3.5 到 4 那么大的提升。”</p><p></p><p>在大模型争夺战中，讯飞给自己的定位是“综合能力是头部，在自己擅长的地方保持耐心和耐力”，因为一个很现实的问题就是大模型的企业同质化严重，但其实想要在每个领域都做到最好很难，OpenAI 和谷歌都做不到。</p><p></p><p>“我们还要给用户习惯的时间，通过产品培养用户和客户的耐心。”刘聪说道。</p><p></p><p>内容推荐</p><p></p><p>在这个智能时代，AI 技术如潮水般涌入千行百业，深度重塑生产与生活方式。大模型技术引领创新，精准提升行业效率，从教育个性化教学到零售精准营销，从通信稳定高效到金融智能风控，AI 无处不在。它不仅是技术革新的先锋，更是社会经济发展的强大驱动力。在 AI 的赋能下，我们正迈向一个更加智能、便捷、高效的新未来，体验前所未有的生活变革与行业飞跃。关注「AI 前线」公众号，回复「千行百业」获取免费案例资料。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c0/c0779541886d6212211f10391187b0f5.png" /></p><p></p><p>会议推荐</p><p></p><p>InfoQ 将于 10 月 18-19 日在上海举办 QCon 全球软件开发大会 ，覆盖前后端 / 算法工程师、技术管理者、创业者、投资人等泛开发者群体，内容涵盖当下热点（AI Agent、AI Infra、RAG 等）和传统经典（架构、稳定性、云原生等），侧重实操性和可借鉴性。现在大会已开始正式报名，可以享受 8 折优惠，单张门票立省 960 元（原价 4800 元），详情可联系票务经理 &nbsp;17310043226 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/df/dfd31ee989a7951439a77fec138d4cf8.png" /></p><p></p><p>今日荐文</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621559&amp;idx=1&amp;sn=35db58c708c2a1ab0ab3bb3307014d2b&amp;chksm=fbeba278cc9c2b6e27f50c0361eb0202485480e2e2a0a033e14c819f406fef31599a0a2fab1a&amp;scene=21#wechat_redirect">“创业一年，人间三年”，李沐亲述 LLM 创业第一年的进展、纠结和反思</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621514&amp;idx=1&amp;sn=d61a90572d1ece086f4c8238e82b2073&amp;chksm=fbeba245cc9c2b53f02c6ae3a2a3dd0714e5571ac5e12f5dccbb42bd4b5223b1e2979cb34269&amp;scene=21#wechat_redirect">刚刚，OpenAI又双叒叕鸽了！没等来“草莓”发布，只敷衍发了评测集，网友：拿这来抢谷歌发布会风头？</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621462&amp;idx=1&amp;sn=7fdb125768fc1da501d6ddc64efb7fce&amp;chksm=fbeba299cc9c2b8f23b1924c2c3ce0715ee8b2f3dc4a2b34bbf36b7e6d1927401d767b479774&amp;scene=21#wechat_redirect">三年亏损51亿元，去年卖出22台车！文远知行被爆赴美IPO，估值超360亿元</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621417&amp;idx=1&amp;sn=c2527e66bd2f71ae9f8502f019ad02b7&amp;chksm=fbeba2e6cc9c2bf0aa096968c96e502b20da1bef64c5e0be38d2ffc1ed740b9eb1b2b8bfbe21&amp;scene=21#wechat_redirect">一年前还看好，现在却急刹车？国内资本动辄数十亿投资，华尔街却不敢给了</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621293&amp;idx=1&amp;sn=f7a1e3bd450fdb11e2f23d3019804a9d&amp;chksm=fbeba362cc9c2a74471529947135ee0e3883cd5933487a07bf3f77d3bdd1d37932518d2203d3&amp;scene=21#wechat_redirect">京东发行稳定币；AI服务器大厂豪气分红115.2亿；小米二期工厂附近挖出古墓？王化：假的｜AI周报</a>"</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c5/c548af7ad39632ca346cd454eef4d2a8.gif" /></p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247620641&amp;idx=1&amp;sn=44cdebfad0decb39633875fc4069c7fc&amp;chksm=fbeba1eecc9c28f81fc4c7c10d9e95329e4e0ed2d4c1f0eff6ca7f19376846f6a297b33e15d3&amp;scene=21#wechat_redirect"></a>"</p><p></p><p>******你也「在看」吗？******👇</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/rBVmYQGeaHJCxyTks1pX</id>
            <title>顺丰揭秘：大模型技术如何重塑物流供应链</title>
            <link>https://www.infoq.cn/article/rBVmYQGeaHJCxyTks1pX</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/rBVmYQGeaHJCxyTks1pX</guid>
            <pubDate></pubDate>
            <updated>Thu, 22 Aug 2024 10:08:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>物流与供应链系统的庞大规模、环节的复杂多变、数据的复杂性、场景的多样性，使得物流与供应链系统的建设和运行变得更加复杂。而大模型技术作为 AI 的一项重要成果，在物流供应链领域具有无限的潜力和广阔的应用前景，并在推动物流供应链领域数字化、智慧化变革中扮演着不可忽视的角色。</p><p></p><p>在 8 月 18 日 -19 日的 <a href="https://aicon.infoq.cn/2024/shanghai">AICon 全球人工智能开发与应用大会</a>"上，InfoQ 荣幸地邀请了顺丰顺丰科技人工智能总工程师高磊，他为我们分享了大模型在物流和供应链场景中的应用，以及顺丰相关技术体系与产品体系的建设思路与实践经验。本文会会前采访文章，期待你对了解大模型在物流行业的应用有所帮助！</p><p></p><p>以下为采访正文～</p><p></p><h5>InfoQ：顺丰在建设物流决策大模型技术体系时，采用了哪些具体的技术手段和方法？这些技术是如何与现有的物流和供应链系统进行融合的？</h5><p></p><p></p><p>高磊： 当前 AIGC 技术主要以文本、图片、语言、视频等模态为主，因此在这些信息相对富集以及以这些模态作为主要信息载体的领域更容易落地，比如售前的营销素材的生成，售后的智能客服，以及办公领域的 FAQ、信息摘要等场景。</p><p></p><p>但是我们所关注的供应链运营和决策优化领域中，如何利用大模型与其背后的技术去解决供应链运营过程中问题，提升决策质量和效率，帮助客户业务更好的降本增效，目前并未看到很好的行业实践和落地案例。我们结合对于大模型技术的理解与顺丰的业务实践，逐渐摸索出三个方向：</p><p></p><p>结合顺丰沉淀的业务 know-how 与已有技术能力，构建行业智能体，相关技术被应用于供应链智能控制塔产品中突破文本、图片等模态的限制，构建物流决策大模型，让大模型技术直接作用于核心决策问题，相关技术被应用于供应链执行优化产品中基于多模态大模型的能力构建多层级多通道需求预测模型，解决消费供应链领域中需求预测的难点与痛点问题，相关技术被应用于供应链计划产品中</p><p></p><h4>需求预测模型在供应链计划产品中的应用案例</h4><p></p><p></p><h5>InfoQ：可以介绍一下什么是基于多模态大模型的能力构建多层级多通道需求预测模型吗? 它解决哪些痛点与难点问题？</h5><p></p><p></p><p>高磊： 我们构建这个模型的初衷是为了解决消费供应链领域中商品蚕食效应、新品新店预测等业界难题。</p><p></p><p>首先，需求预测在供应链计划中非常重要，因为他是需求计划、供应计划、生产计划等诸多计划的源头，准确的需求预测对于提升计划准确性，提升供应链效率而言至关重要。但是需求预测本身难度很大，尤其是消费供应链，受到诸多因素的影响，比如新品上市、老品下架、蚕食效应、促销、节假日、季节、天气等。其中蚕食效应，新品和新店的预测一直是行业普遍存在的难题，传统的算法模型难以有效解决这些问题。</p><p></p><p>以商品蚕食效应和新品上市为例，比如某个门店一直卖 10 种蛋糕，平时所有蛋糕的销量总和是大约 100，然后某一天突然上市了一个新的蛋糕，并做了促销，那么这里会出现两个问题：1. 新蛋糕的销量该如何预测，2. 老蛋糕的销量会受到多大影响？</p><p></p><p>传统的需求预测模型从单一商品视角建模，在解决这两个问题上存在较大困难：在第一个问题上，因为缺乏历史销量数据，很难建模，往往预测偏差很大，在第二个问题上，单一商品视角的建模难以有效捕捉商品之间的关联关系与相互影响，在上新期间难以捕捉到蚕食效应造成老品系统性的偏高。</p><p></p><p>为了解决这些行业难点问题，我们设计了基于多模态大模型的能力构建多层级多通道需求预测模型，从特征角度，我们通过预训练好的多模态模型将商品的文字描述如商品名，商品描述，配料表，价格等和商品的图片提取为表征商品内在属性的 Embedding 向量。通过选择合适的多模态大模型，我们发现提取出的 embedding 能够很好的表达商品之间内在的一些相关性。</p><p></p><p>提取了多模态特征之后，为了更好的学习商品之间的关联性，我们设计了一种多层级多通道的需求预测模型。</p><p></p><p>这里解释一下层级的概念，消费供应链预测中往往存在多种层级，比如时间层级：日到月到年；空间层级：门店到 RDC 到 CDC；品类层级：具体的 SKU 到二级品类到一级品类，往往在各种层级上都要输出预测结果，并且层级之间的结果应该能够对应上，比如某个门店内所有商品的总销量预测应该等各个商品预测之和。</p><p></p><p>多层级多通道的需求预测模型能够很好地学习同一层级内的商品之间的内在关联性，以及层级之间的关联性，从而更好的得到预测结果。</p><p></p><p></p><h5>InfoQ：这个模型的实际应用效果如何？</h5><p></p><p></p><p>高磊： 我们在某个实际客户的场景下做了测试，整体上，新的模型可以在预测准确性上提升绝对值 5 个百分点，这个是我们在传统方式下做了很久也没有难达到的程度。同时得益于多层级多通道统一建模极大的减少了模型的数量，以及 GPU 的使用，在计算性能方面实现了 120 倍的提升，对机器资源的需求也减少了 5 倍。</p><p></p><p>我们也着重验证了一下新模型在新品等场景下的预测表现，得益于多模态信息的引入与多层级多通道学习机制，新模型能够有效的捕捉到新品和老品之间的相关性与蚕食效应，可以在上新期间取得显著的的新老品预测准确度的提升。</p><p></p><h4>供应链智能体在供应链智能控制塔产品中的应用细节</h4><p></p><p></p><h5>InfoQ：什么是供应链智能体？它具备一些什么样的能力？解决什么问题？</h5><p></p><p></p><p>高磊： 我们知道供应链运营是一个专业程度很高，并且非常严谨的领域，因为任何数据或者决策建议的错误都可能带来比较严重的损失。大模型本身存在一些固有的缺陷如不擅长精确数值计算，幻觉，专业程度不够高等问题，限制了其在供应链运营领域的应用。</p><p></p><p>比如前端时间公众号上有个比较火的文章，讲得是问大模型 9.11 和 9.8 哪个更大，绝大多数大模型都回答 9.11。再比如把过去一段时间的历史销量和库存数据丢给大模型，让它去做库存优化，大模型也很难去做这种专业的事情。为了解决以上问题，我们的解决思路是结合大模型和专业小模型，以及顺丰多年沉淀的供应链实践，去构建供应链的行业智能体。</p><p></p><p>具体来说，我们通过 RAG 技术结合我们沉淀的业务知识库，让大模型具备更深入的供应链知识，同时我们将丰智云体系中沉淀的各种算法能力，比如预测、仿真、运筹优化、归因分析等，抽象成工具并交给大模型调用。由此构建出具备供应链行业知识的业务专家智能体与以及具备专业算法能力的算法专家智能体，并通过这些智能体的协作，去服务具体的业务场景，如销售分析，库存优化等场景。通过以上方式，可以有效的改善和缓解大模型在供应链场景下存在固有缺陷。</p><p></p><p></p><h5>InfoQ：在供应链智能控制塔产品中，顺丰如何集成供应链智能体的能力？</h5><p></p><p></p><p>高磊： 我们知道在供应链控制塔中，有一块很重要的能力是供应链诊断与分析能力，传统方式下，我们需要建立大量的报表来呈现业务指标与各种问题，但是这种形式是相对静态的，当出现新的场景和问题的时候往往还是需要手动获取数据、分析数据或者开发新的报表，难以敏捷的响应新的需求。</p><p></p><p>另外，从数据分析角度来看，大致存在 3 种类型的分析:</p><p></p><p>描述性分析：对数据进行整体概括和总结，以了解数据的基本特征和趋势，形成对业务现状的整体认识诊断性分析：通过深入挖掘数据的背后原因，解释数据异常或变动的原因，并为问题提供决策依据预测性分析：利用历史数据和模型来预测未来事件或趋势的发展，为决策提供先见之明</p><p></p><p>目前传统的控制塔还是以描述性分析为主，在诊断性分析和预测性分析方面提供的支持较少。</p><p></p><p>通过将供应链智能体融入到丰智云塔产品当中，通过多个智能体的协作，针对履约、库存、销售等领域的问题，提供从指标查询与分析到异常识别与归因再到提供优化建议的完整的服务支持，从而为客户提高更敏捷与高效服务。而在这些服务的背后，智能体利用的是成熟、专业的预测、仿真、运筹优化等模型工具，来确保输出结果的准确与可靠。</p><p></p><p></p><h4>物流决策大模型实际效果</h4><p></p><p></p><p></p><h5>InfoQ：什么是物流决策大模型？他与语言大模型等有什么区别和联系？</h5><p></p><p></p><p>高磊： 我们知道语言大模型是一个通过 Transformer-Like 的架构，利用自回归的形式进行文字序列生成的模型，而很多人不知道的是物流中的很多问题，其实也可以认为是一个序列生成或者说是序列决策的问题，比如去 3 家门店 a、b、c 送货的一个路径规划问题，可以认为是一个决定先去哪，再去哪，最后去哪的序列生成问题。再比如装箱问题，10 个物品要装到箱子里，也可以认为是一个先装哪个物品，并以什么样的姿态装进去，再装哪个物品这样的问题。</p><p></p><p>所以，本质上，物流中的很多问题和语言生成的问题一样，都是序列生成的问题，因此均可以采用相同的技术架构来解决。这是他们相同的地方。</p><p></p><p>不同的地方显而易见，就是模态的不同，不同于语言模型生成的是文字，物流决策模型生成的就是决策本身。另外不相同的点是目标不同，语言模型的目标是生成文字的合理性与有效性，能够符合语言规律并有效解决用户的问题。物流决策模型除了生成决策要合理外，还有优化目标在里面，比如生成的线路成本越低越好。</p><p></p><p>丛技术角度来说，我们知道语言大模型本身基于两大关键技术，Transformer 和 RLHF，其中 Transformer 在很多算法场景下的成功应用已经充分证明了其能力的强大，而 RLHF 技术因其解决了人类价值观与偏好对齐等问题，将大模型的实用程度推上了前所未有的程度。在物流大决策模型中，我们也是基于这两大技术进行了构建，以路径规划场景为例，通过 Transformer 架构并结合顺丰海量的场景以及规划数据，构建了路径规划的基座模型，并通过 RLHF 技术来解决与业务偏好和具体业务场景对齐的问题。</p><p></p><p></p><h5>InfoQ：如何将物流决策大模型应用到供应链优化产品中，它能够带来一些什么样的优势? 具体落地效果如何？</h5><p></p><p></p><p>高磊： 总体来讲物流决策大模型带来两方面的显著优势，第一个是计算性能方面，传统的运筹模型主要基于搜索的机制，在一定引导下在一个巨大的解空间里面尽可能的搜索较好的解，当问题规模变大，解空间指数级别增长时，往往搜索到较高质量的解需要相对较长的时间，而物流决策模型基于序列生成的方式，在训练的较好的情况下，能够快速将较高质量的结果直接生成出来，再经过 GPU 高速并行计算的加持，能够很快的得到结果。</p><p></p><p>以我们实际鲁多的某客户装箱优化场景举例，目前我们可以平均 20ms 的时间内计算出一个使用传统运筹方法需要 10 分钟才能计算出来的订单，并且得到的解还能略微超过传统运筹方法。</p><p></p><p>另外一方面的优势来自于 RLHF 微调技术，通过 RLHF 我们可以让我们的模型有能力学习到业务在特定场景下的业务偏好与特殊需求。这将我们的产品在面对业务变化与新的算法场景时候可以从定制开发方式转向数据驱动的方式。</p><p></p><p>具体来说，在传统方式下，当业务变化或者新的场景出现时，我们需要我们的算法工程师不断的和业务沟通并理解业务，然后设计针对性的算法，并做很多 POC 试验，输出结果给到业务进行验证，往往这个过程会反复很多次并持续很久，因为往往业务无法将所有影响因素和潜在的业务规则一次性说清楚，很多时候碰到问题才解决问题。</p><p></p><p>使用 RLHF 微调技术，我们可以以数据驱动的方式解决很多问题，当输出结果不满足业务预期时，用户可以自己对结果进行调整，我们的产品会记录调整过程，逐渐积累业务偏好数据，并使用业务偏好数据不断进一步优化我们的模型，使输出的结果越来越符合业务实际需要。</p><p></p><p>当然这里面需要额外考虑的问题是并不是所有的业务调整或者业务偏好都是合理的，因此我们在产品里面设计了偏好与优化效果之间权衡机制，用户可以自己调整更偏向于“像人”还是优化。</p><p></p><p></p><h5>InfoQ：您认为大模型技术在未来供应链管理中的潜在应用有哪些预期或愿景？</h5><p></p><p></p><p>高磊： 以上三个工作是目前我们决策大模型技术在供应链管理中的应用方面进行的初步探索，我觉得还远远没有完全发挥出大模型技术的所有潜力，也还有很多潜在的应用场景没有被挖掘，我们希望能够和业界的生态合作伙伴与友商一起，持续深耕这样一个领域，为提升供应链的数智化水平、实现行业共同进步方面添砖加瓦。</p><p></p><p>嘉宾介绍：高磊， 顺丰科技人工智能总工程师，拥有 10 年 + 机器学习与运筹优化算法经验，研究方向为 NLP、运筹优化、强化学习等。2016 年加入顺丰，现任顺丰科技人工智能总工程师，曾主导顺丰集团内部多个数智化项目的研发与落地工作，涉及领域包括业务量预测、陆运干支线规划与调度、航空规划与调度、运力规划、场站选址、物资调拨等。目前主要负责集团智慧供应链体系建设相关工作。期间带领团队获得十余项发明专利，中物联物流技术创新奖、CCF BDCI 一等奖、最具商业价值奖，运筹帷幄年度行业实践奖与学术应用奖等荣誉。</p><p></p><p>活动推荐</p><p></p><p>AI 应用开发正在逐步成为各行业内的核心创新驱动力，CUI 式的对话助手、串联业务流程的 Agent 或是内嵌在原有业务逻辑中的 AI 模块，都在不断拓展面向用户的新应用场景。我们惊喜地看到从中小创业公司到大型企业，都在利用计算机视觉、自然语言处理、个性化推荐、对话式交互等 AI 能力提升业务效率、优化用户体验，显著增强了产品的市场竞争力。10 月 18-19 日，来 QCon 全球软件开发大会（上海站），了解更多成功应用 AI 技术的案例与最佳实践。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/df/dfd31ee989a7951439a77fec138d4cf8.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/zJzgvWzv7N9cGHjBjJiS</id>
            <title>MiniMax 基于 Apache Doris 升级日志系统，PB 数据秒级查询响应技术实践</title>
            <link>https://www.infoq.cn/article/zJzgvWzv7N9cGHjBjJiS</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/zJzgvWzv7N9cGHjBjJiS</guid>
            <pubDate></pubDate>
            <updated>Thu, 22 Aug 2024 09:44:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><blockquote>作者｜MiniMax 基础架构研发工程师 Koyomi、香克斯、Tinker</blockquote><p></p><p></p><p></p><blockquote>导读：早期 MiniMax 基于 Grafana Loki 构建了日志系统，在资源消耗、写入性能及系统稳定性上都面临巨大的挑战。为此 MiniMax 开始寻找全新的日志系统方案，并基于 Apache Doris 升级了日志系统，新系统已接入 MiniMax 内部所有业务线日志数据，数据规模为 PB 级， 整体可用性达到 99.9% 以上，10 亿级日志数据的检索速度可实现秒级响应。</blockquote><p></p><p></p><p>MiniMax 是领先的通用人工智能科技公司，自主研发了不同模态的通用大模型，其中包括拥有万亿参数的 MoE 文本大模型、语音大模型以及图像大模型。MiniMax 以“与用户共创智能”为愿景，通过对大模型持续迭代，MiniMax 在国内率先完成核心 MoE 算法技术路线的突破。2024 年 4 月，公司推出国内首个上线商用的 MoE 架构、包含万亿参数的大语言模型——“MiniMax-abab 6.5”，模型性能接近国际领先水平。</p><p></p><p>随着模型复杂度以及模型调用量的不断提升，模型训练及推理产生的运行日志也在激增，这些数据对于 AI 应用的运行监控、优化及问题定位至关重要。早期 MiniMax 基于 Grafana Loki 构建了日志系统，在资源消耗、写入性能及系统稳定性上都面临巨大的挑战。为此 MiniMax 开始寻找全新的日志系统方案，并对业界具有代表性的技术栈 Apache Doris 和 Elasticsearch 进行了对比，Apache Doris 在性能、成本以及易用性等方面均优于 Elasticsearch，因此最终选择了 Apache Doris 来构建日志系统。</p><p></p><p>目前基于 Apache Doris 的新系统已接入 MiniMax 内部所有业务线日志数据，数据规模为 PB 级， 整体可用性达到 99.9% 以上，10 亿级日志数据的检索速度可实现秒级响应。</p><p></p><h2>问题及痛点</h2><p></p><p>MiniMax 早期日志系统架构基于 Loki 搭建，Loki 是由 Grafana Labs 团队开发的开源日志聚合系统，设计思想受 Prometheus 启发，不使用传统索引结构、仅对日志标签和元数据构建索引，核心模块包括 Loki、Promtail、Grafana 三个部分，其中 Loki 是主服务器、负责日志存储和查询，Promtail 是代理层、负责采集日志并发送给 Loki，而 Grafana 则用于 UI 展示。</p><p></p><p>在实际 Grafana Loki 使用中，每个集群中单独部署一套完整的日志采集器 + Loki 日志存储/查询服务。Loki 采用 Index + Chunk 的日志存储设计，写入时按日志标签的哈希值将不同日志流分散到各个 Ingester 上实现负载均衡，由 Ingester 负责将日志数据写入对象存储。查询时，Querier 从对象存储取出 Index 对应的 Chunk 后进行日志匹配。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0c/0c4fe6ea65ca8ef30e0f570533b6fc7e.png" /></p><p></p><p>尽管 Grafana Loki 定位为轻量级、水平可拓展和高可用的日志系统，但其在实际业务使用过程中仍存在一些问题：</p><p></p><p>查询资源消耗过大： Loki 未对日志内容创建索引，只能按照标签粒度对日志进行初步过滤。如果想要实现日志内容搜索功能，需使用 Query 对全量日志数据进行全文正则匹配， 而该操作会带来巨大的突发资源消耗，包括 CPU、内存、网络带宽。当查询的数据量和 QPS 越来越大时，Loki 的资源消耗及其稳定性问题也变得越来越不可忍受。Loki 架构复杂繁多： Loki 除了上图涉及模块之外，还有 Index Gateway、 Memcache、 Compactor 等模块，过多的架构组件给系统运维和管理带来很高的难度，配置起来也非常复杂。维护成本及难度高： MiniMax 部署集群数量较多，且每个集群的系统、资源、存储、网络等环境都有差异， 如果在每个集群中部署一套独立的 Loki 架构，维护成本及运维难度都非常高。</p><p></p><h2>为什么选择 Apache Doris</h2><p></p><p>根据 AI 场景的数据特点及业务需求，MiniMax 对新日志系统提出了以下要求：</p><p></p><p>日志数据规模庞大：由于 AI 业务场景具备链路长、上下文数据多、单次请求数据量大等特点，其产生的日志体量远远高于相同用户量级的其他互联网产品，这要求系统能够以较低的成本、稳定可靠的存储这些数据。查询性能要求高：业务对日志查询速度有较高的要求， 比如 1 亿条数据需要在秒级返回查询结果。分析灵活：要求系统能够支持日志指标查询、如某些关键词的统计曲线，同时能够提供日志告警服务。低成本：由于日志原始数据量达到 PB 级，而且还在不断增加，存储和计算的成本需要控制在合理范围内。</p><p></p><p>MiniMax 参考了当前业界成熟的日志系统架构解决方案，发现主流的日志系统一般包含以下几个关键组件：</p><p></p><p>采集端：负责从服务的标准输出采集日志，并将数据推送到中心消息队列。消息队列：负责解耦上下游、削峰填谷。在下游组件不可用时，仍然能保留一段时间的数据，保证系统稳定性。存储查询中间件：负责日志数据的存储和查询，在日志系统场景下，一般要求该中间件具备倒排索引能力，来支持高效的日志检索。</p><p></p><p>根据上述方案组成，MiniMax 决定在新日志系统中：采集端使用 iLogtail、消息队列使用 Kafka、存储中间件为 Apache Doris。在存储中间件的选择上，对比了业界具有代表性的 Apache Doris 和 Elasticsearch 这两个技术栈：</p><p></p><p><img src="https://static001.geekbang.org/infoq/dc/dc7c4c2f127bdfbf6ea247defc5a64fd.png" /></p><p></p><p>Apache Doris 在成本、写入性能、查询性能这几大维度均有较好的表现，尤其在存储效率、写入吞吐、聚合分析等方面有突出的优势，同时兼容 MySQL 的 SQL 语法也更加易用，因此最终选择 Apache Doris 作为存储中间件。</p><p></p><h2>Aapche Doris 日志系统升级实践</h2><p></p><p><img src="https://static001.geekbang.org/infoq/49/4940e10abd9a52783cc30df5d49c5c09.png" /></p><p></p><p>新日志系统（Mlogs）更加简洁，一套架构即可服务全部集群。上层为日志系统的控制面， 包括日志查询接口封装以及配置自动生产与下发模块。 下层是日志系统的数据面， 从左到右依次是日志采集端、消息队列、日志写入器、Doris 数据库。</p><p></p><p>集群服务产生的日志数据由 iLogtail 采集并推送到 Kafka，一部分会经由 Mlogs Ingester 从 Kafka 拉取并通过 Doris 的 Stream Load 写入到 Doris 集群中，另一部分则由 Doris 通过 Routine Load 直接实时订阅拉取Kafka 的消息流 。最后由 Doris 承担全量日志数据的存储与查询，无需每套集群单独部署。</p><p></p><p>在具体的应用落地方面：</p><p></p><p>在日志导入上： 新架构同时使用了 Doris Routine Load 和 Stream Load 方式。Routine Load 开箱即用，可直接处理不需要额外解析处理的 JSON 格式日志。而对于需要过滤与处理的复杂日志， MiniMax 在 Kafka 和 Doris 之间增加了日志写入器 Mlogs Ingester，由其解析和处理后，再通过 Stream Load 写入 Doris 中。在日志检索上： 主要使用了 Doris 倒排索引分词查询能力以及全文正则查询能力。倒排索引分词查询能力：分词查询性能较好， 场景覆盖度较广，主要采用倒排索引查询MATCH 和 MATCH_PHRASE。全文正则查询能力：正则查询精度更高，性能低于比分词查询， 适合小范围查询且对查询精度要求较高的场景，主要使用正则查询 REGEXP。在性能提升上：为进一步提升性能，实现了查询截断功能。当前日志数据按照时间顺序呈线性排列， 如果用户选择的查询范围过大， 会消耗较大的计算存储网络资源， 从而导致查询超时甚至系统不可用。 因此，对用户的查询进行了时间范围截断， 避免查询范围过大；并提前统计所有表的每 15 分钟的数据量， 动态地预估用户在不同表中最大可查询的时间长度。在成本控制上： 使用了 Doris 的冷热数据分层能力， 将 7 天内的数据定义为热数据，7 天之前的数据为冷数据。冷数据存储到对象存储， 以降低存储成本；同时对 30 天之前的对象存储数据进行归档， 仅在必要时恢复归档数据， 这也极大地降低了存量数据的存储成本。</p><p></p><h2>使用收益</h2><p></p><p>目前基于 Apache Doris 的新架构已接入 MiniMax 内部所有业务线日志数据，数据规模为 PB 级， 整体可用性达到 99.9% 以上， 同时也带来以下收益：</p><p></p><p>架构简化：新架构部署简单、一套架构即可服务全部集群，降低了整体系统维护及管理的复杂度，节省了大量的运维人力及成本投入。秒级查询响应： 基于 Apache Doris 的倒排索引能力及查询拦截功能，性能显著提升的同时系统也更加稳定。从 10 亿数据中查询单个关键字以及进行聚合分析，基本可以在 2s 内完成，对于日志数据的分析，大部分场景也可以做到秒级响应。写入性能高：当前系统规格可以实现 10 GB/s 级别的日志写入吞吐，能够在满足持续高吞吐写入的同时满足实时性要求，数据延迟控制在秒级。存储成本低： 数据压缩率较高达到 1:5 倍以上，因此存储空间占用较原本架构极大幅度降低。对于冷数据使用 Doris 冷热分层能力进一步降低数据的存储成本，存储成本节省超过 70%。</p><p></p><h2>未来规划</h2><p></p><p>未来 MiniMax 将持续迭代日志系统， 并重点从以下几方面发力：</p><p></p><p>丰富日志导入预处理能力：增加日志采样、结构化等预处理能力，进一步提升数据的可用性及存储性价比。增加 Tracing 能力：尝试将监控、告警、Tracing、日志等各方面的可观测性系统打通，以提供全方位的运维洞察。扩大 Doris 使用范围：除日志场景之外，Doris 逐步被引入数据分析和大数据处理场景下，助力后续构建数据湖仓能力。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/THI3VfBS9MVCmuO75Y9Y</id>
            <title>“印度马斯克”要发印度第一款 AI 芯片，号称超越英伟达！CEO 要“狼性”，但把数十亿美元打水漂</title>
            <link>https://www.infoq.cn/article/THI3VfBS9MVCmuO75Y9Y</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/THI3VfBS9MVCmuO75Y9Y</guid>
            <pubDate></pubDate>
            <updated>Thu, 22 Aug 2024 08:36:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AI市场迎来又一位新玩家。</p><p>&nbsp;</p><p>以电动踏板车而闻名的 Ola Electric 正在大胆进军人工智能硬件领域。该公司已宣布计划开发印度首款AI芯片系列，首批产品将于 2026 年通过其基础模型系列发布。</p><p></p><h2>印度发布首款AI自研芯片，号称性能媲美英伟达</h2><p></p><p>&nbsp;</p><p>作为印度最大的电动两轮车制造商之一，Ola Electric刚刚公布其计划推出的AI芯片。预计这三款芯片将于2026年投放市场，另外一款则将于2028年与广大用户见面。这些将成为印度推出的首批AI芯片，用以满足印度国内对于此类算力设备的旺盛需求。总部位于印度的数据中心与服务器公司Yotta已经为明年订购了1.6万张英伟达GPU，另有1.6万张上个月已经完成交付。</p><p>&nbsp;</p><p>Ola的首批三款芯片分别是Bodhi 1、Ojas和Sarv 1。第四款Bodhi 2则是首款印度AI芯片的继任者。Bodhi 1专为AI推理和微调而设计，主要应用于大语言模型和视觉模型，，将满足万亿参数 AI 模型的需求。Ola还强调称，Bodhi 1将超越目前最先进的技术，同时消耗更少的电量，这也是当今AI处理领域所面临的最大挑战之一。这一技术进展可能会加速印度各个行业对 AI 的采用。</p><p>&nbsp;</p><p>除此之外，还有专为特定应用而设计的Ojas Edge AI芯片。该公司可以根据多种应用场景对这款芯片做出定制，包括汽车、移动、物联网等。Ola还计划在其下一代电动汽车中部署这款芯片，以帮助运行充电、ADAS等系统。当然，随着AI计算的巨大需求，该公司还推出了Sarv 1，使用到为数据中心构建的Arm指令集。</p><p>&nbsp;</p><p>该公司在演示中披露，其原型芯片的性能与能效均比英伟达GPU更好。但目前尚不清楚他们具体是在与哪款GPU进行比较，例如RTX 4090还是H200，唯一明确的就是其运行功率为200瓦。此外，该公司也没有说明这些芯片将在哪里生产。</p><p>&nbsp;</p><p>Ola 的野心还不止于Bodhi 1。Ola表示还将随即推出 Bodhi 2 芯片，该芯片计划于 2028 年问世。这款更先进的芯片旨在支持具有超过 10 万亿个参数的模型的训练、推理和微调。该公司将其设想为百亿亿次超级计算的基石。</p><p>&nbsp;</p><p>除了专用于 AI 的芯片外，Ola 还在开发基于 ARM 架构的通用服务器 CPU 和用于智能手机和可穿戴设备等消费设备的富 AI 芯片。</p><p></p><h2>“印度马斯克”和他的“狼性”企业文化</h2><p></p><p>&nbsp;</p><p>据公开资料显示，Ola ElectricOla Electric 成立于 2017 年。创始人是 Bhavish Aggarwal，他之前曾是 Ola Cabs 的联合创始人，也是大语言模型AI公司 OlaKrutrim 的创始人。Aggarwal 毕业于孟买印度理工学院，他的职业生涯始于微软。</p><p>&nbsp;</p><p>Bhavish Aggarwal也常被人称为“印度马斯克”，也不仅是因为其在电动车领域的卓越成就，也因Bhavish Aggarwal的为人处事和管理公司的风格和马斯克很相似。</p><p>&nbsp;</p><p>过去十年，印度的一些分析师、评论家、媒体等用各种标签和形容词来描述这位 2010 年开启 Ola 创业之旅的创始人，有的标签是“傲慢”、有的标签是“咄咄逼人”，有人形容他是“工作狂”、“特立独行者”或者是“坚持不懈的创业者”。</p><p>&nbsp;</p><p>面对外界的种种声音，Bhavish Aggarwal称，“这就是梦想远大的代价，我说的都是真心话”。</p><p>&nbsp;</p><p>也如马斯克一样，Bhavish Aggarwal同样推崇“狼性文化”。</p><p>&nbsp;</p><p>Bhavish Aggarwal曾在接受采访时称，“Ola 并不适合每个人，Ola 是雄心勃勃的人的天堂”。Ola 是那些有理想、有抱负之人的最佳去处。在 Ola，他们感觉如鱼得水，因为我们公司充满了创业精神。我们不会过度管理员工。我们会告诉他们你必须做这项工作，请认真完成它”。</p><p>&nbsp;</p><p></p><blockquote>“一旦他们完成工作，他们自己就会超出预期。Ola 就是这样的地方，这样的人才是 Ola 真正闪耀的原因。但在这一过程中，有些人发现这里并不是最适合他们的地方。没关系，不是每个地方都适合每个人，我们的文化是影响力和目标驱动的，以业务建设为导向”。</blockquote><p></p><p>&nbsp;</p><p>Ola的企业文化也常被拿来撰写和讨论。Bhavish Aggarwal坦言，他们的文化就是这样。他们诚实对待自己的目标，不会向任何人隐瞒这一点。</p><p>&nbsp;</p><p>Bhavish Aggarwal表示他会公开告知员工“你别在这里奢望获得早九晚五的工作，如果你想图清闲，就别来Ola上班。如果你想创造终生难忘的故事，并告诉你的后辈你是如何为印度电气化和能源独立之旅的贡献了一份力量，那么 Ola 就是你该来的地方。我们都是诚实的人，有着诚实的目标。”</p><p></p><h2>主营业务还没盈利，Ola又盯上一块新“蛋糕”</h2><p></p><p>&nbsp;</p><p>在Bhavish Aggarwal的带领下，Ola的发展蒸蒸日上。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/93/934f10667ae5e7e3282c55486b15b21c.png" /></p><p></p><p>&nbsp;</p><p>Ola Cabs 首席执行官兼 Ola Electric 创始人 Bhavish Aggarwal 在首次公开募股 (IPO) 前的新闻发布会上发表讲话。</p><p>&nbsp;</p><p>2019 年 5 月 6 日，Ola Electric 宣布 Tata Sons 首席执行官、掌控96家公司的印度资本巨鳄拉坦塔塔已向该公司投资了一笔未公开的金额。此前，塔塔先生也投资了 Ola 的出租车业务。</p><p>&nbsp;</p><p>2020 年 5 月，Ola Electric 收购了位于阿姆斯特丹的电动踏板车制造商“Etergo”。Etergo 制造了一款使用可更换电池的踏板车，续航里程高于标准。这款踏板车的设计、技术和效率令人印象深刻，所以Ola Electric 也被称为“电动车界的特斯拉”。同年12 月，Ola Electric 公司与泰米尔纳德邦政府签署了一份谅解备忘录，宣布计划在泰米尔纳德邦建立全球最大的两轮车工厂（名为 Ola Futurefactory），耗资 240 亿卢比（约合20.46亿人民币）。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/f5/f50e50865e9fed15f079ebdd0c3abfff.png" /></p><p></p><p>&nbsp;</p><p>Ola Futurefactory 占地 500 英亩，是世界上最大的两轮车工厂。</p><p>&nbsp;</p><p>2022 年 3 月，Ola 对以色列电池技术公司“StoreDot”进行了战略投资。Ola Electric 将采用 StoreDot 的快速充电电池技术，并将其用于印度未来的汽车上。StoreDot 是超快速充电 (XFC) 电池的先驱，它克服了主流电动汽车采用的两个关键障碍：续航里程焦虑和充电时间长。</p><p>&nbsp;</p><p>收购完成并充分处理文书工作后，Ola Electric 宣布要进军电动汽车领域，不仅要在国内推出电动汽车，也要让其走向国际。随后2022年6月，Ola Electric 发布了其首款电动汽车。Ola 表示，其目标是一次充满电后行驶超过 500 公里，仅需 4 秒即可完成 0-100 次加速。</p><p>&nbsp;</p><p>虽然Ola Electric的发展一路高歌猛进，但由于处于迅速增长阶段，Ola Electric 的现金消耗异常巨大，亏损不断增加。电动滑板车销售收入是 Ola Electric 的唯一收入来源，而电池销售收入在本财年第一季度仅贡献了一小部分。</p><p>&nbsp;</p><p>在运营方面，Ola Electric也存在其他问题。Ola Electric公司表示，电动汽车所用零部件可能会出现缺陷、质量问题或供应中断或价格上涨，从而增加材料成本和 Ola 电动汽车的价格，这将影响预计的制造和交付时间表，也会从一定程度上削减公司营收。</p><p>&nbsp;</p><p>而此次Ola Electric宣布推出自研AI芯片后，也在印度社交网络上引发热议。看好和看衰两种声音势均力敌。</p><p>&nbsp;</p><p>一些网友认为Ola Electric成立多年并未取得什么重大成果，发布芯片也只为了融资和炒作。</p><p>&nbsp;</p><p>ID名为SaiSS961的用户在Youtube平台Ola Electric 发布芯片视频下方评论称：“这家公司得了数十亿美元的资金，却没有取得什么成果，一切都只是炒作而已。”</p><p>&nbsp;</p><p></p><blockquote>“Ola 只是购买外部技术并将其标榜为自己的技术。就像他迄今为止所做的一切一样，这只是复制粘贴。”</blockquote><p></p><p>&nbsp;</p><p>ID名为arpanshome6328的用户表示：“看起来他很快就厌倦了现有的业务。他没有从任何一项业务中赚取任何利润，就跳入了下一项业务。他们不知道自己想要实现什么。希望投资者的钱是安全的。”</p><p>&nbsp;</p><p>另一位ID为ankittiwari6716的用户也认为Ola Electric 应该专注于主营业务，而不是乱花钱在其他事情上。</p><p>&nbsp;</p><p></p><blockquote>“Ola Electric 最好专注于电动汽车，而不是在有限的预算下做所有事情。这家伙在欺骗无辜的印度人民来抬高他的股票。他的公司从欧洲获得了电动汽车设计，进行了外观改造并将其出售给印度人。他怎么能押注高端芯片制造业务？这是在欺骗所有人。只要和你认识的任何一位前 Ola 员工谈谈就会知道，几乎没人使用 krutrim AI ，甚至班加罗尔的人们更喜欢 Rapido 而不是 Ola。”</blockquote><p></p><p>&nbsp;</p><p></p><h2>印度正大力推动芯片行业发展</h2><p></p><p>&nbsp;</p><p>市场研究与咨询公司MarketsandMarkets 近期对 AI 芯片市场进行了全面分析，预测 2024 年至 2030 年间将出现大幅增长。根据他们的报告，全球 AI 芯片市场预计到 2030 年将达到 930 亿美元，复合年增长率为 25.6%。这一增长是由汽车、医疗保健和金融等各个领域越来越多地采用 AI 技术推动的。</p><p>&nbsp;</p><p>这块巨大的蛋糕早已被印度盯上。作为印度向全球领先经济体转型的一部分，印度总理莫迪曾设定了一个目标，即到 2029 年，印度将从几乎一无所有的基础发展成为全球五大计算机芯片制造商之一。</p><p>&nbsp;</p><p>伊利诺伊大学香槟分校的拉凯什·库马尔 (Rakesh Kumar) 表示，各国寻求半导体自给自足的主要驱动力有两个。第一个是，在疫情最严重时期，芯片短缺引发了人们的担忧，人们已经意识到了芯片现在对一个国家的安全和工业的重要性；第二个是，各国希望在这个庞大且不断增长的行业中分一杯羹，去年全球芯片行业市场总价值为5269 亿美元。</p><p>&nbsp;</p><p>作为世界上人口最多的国家，印度拥有大量科技人才，善加利用无疑能够推动其AI技术的发展。而由于英伟达和阿斯麦尔等多家企业被禁止向中国出售其尖端技术，这些厂商也许很乐意为自家产品在印度开辟新的市场空间。</p><p>&nbsp;</p><p>这一举措对印度的技术格局而言是重大的一步，有望减少对外国芯片制造商的依赖，并促进国内人工智能硬件的创新。如果成功，Ola 的芯片系列将使印度成为全球人工智能硬件市场的关键参与者。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href="https://theaiinsider.tech/2024/08/16/ola-unveils-ambitious-plan-for-indias-first-ai-chip-family/">https://theaiinsider.tech/2024/08/16/ola-unveils-ambitious-plan-for-indias-first-ai-chip-family/</a>"</p><p><a href="https://iotworldmagazine.com/2024/08/15/2360/a-review-of-top-ai-chips-market-share-report-2024-2030-in-the-uk-europe-asia-and-india">https://iotworldmagazine.com/2024/08/15/2360/a-review-of-top-ai-chips-market-share-report-2024-2030-in-the-uk-europe-asia-and-india</a>"</p><p><a href="https://www.cnbc.com/2024/08/09/ola-electric-shares-rise-20percent-in-india-ipo-valuing-firm-at-4point8-billion.html">https://www.cnbc.com/2024/08/09/ola-electric-shares-rise-20percent-in-india-ipo-valuing-firm-at-4point8-billion.html</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/O49yVYCK9ufG2d4q4QhP</id>
            <title>探索安全边界：出海合规与大模型实践 | QCon</title>
            <link>https://www.infoq.cn/article/O49yVYCK9ufG2d4q4QhP</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/O49yVYCK9ufG2d4q4QhP</guid>
            <pubDate></pubDate>
            <updated>Thu, 22 Aug 2024 03:10:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>进入 2024 年以来，企业在全球商业舞台上迎来了两个显著的潮流：业务出海和大模型技术的广泛应用。业务出海已成为企业扩大市场版图、提升全球竞争力的关键战略。企业不再局限于本土市场，而是积极向国际领域迈进，探索新的增长机遇和市场空间。这一过程中，如何确保合规与安全，成为企业出海战略中不可忽视的一环。同时，大模型技术的运用正深刻影响着企业的运营模式和创新方向，安全人员也开始考虑利用大模型技术优化安全解决方案，提升安全防护效率，减少安全风险。</p><p></p><p>面对不断演进的趋势，这些都是重点关注和丞待解决的挑战。在即将于 10 月 18 -19 日召开的 QCon 上海站，我们策划了【<a href="https://qcon.infoq.cn/2024/shanghai/track/1717">探索安全边界：出海合规与大模型实践</a>"】专场，将邀请不同公司的数据安全合规专家，分享他们在各自的业务场景中的出海合规实践经验，以及借助大模型助力应用场景落地，实现运营效率和效果的双重提升的方法。目前是 <a href="https://qcon.infoq.cn/2024/shanghai/apply">8 折购票</a>"最后优惠期，感兴趣的同学前往了解。</p><p></p><h3>精彩演讲抢先看</h3><p></p><p></p><p><a href="https://qcon.infoq.cn/2024/shanghai/presentation/6026">演讲主题：大模型在商业敏感数据中的分类分级实践</a>"</p><p></p><p>讲师：刘明 （携程信息安全部数据安全总监）</p><p></p><p>演讲摘要</p><p></p><p>分类分级是数据安全的一项基础性工作，高效准确地对数据进行分类分级打标，是后续进行有效分级保护控制的先决条件，以往分类分级工作中存在专家特征规则识别质量局限，人工识别耗时费力跟不上业务变化的问题，随着大语言模型的兴起，我们看到了新的机会，也取得了不错的可行性验证成果，本次演讲将分享携程逐步实现高质量、高效率的敏感数据分类分级的思路和经验，希望能给听众带来一些启发和思考。</p><p></p><p>演讲提纲</p><p></p><p>1. 背景</p><p></p><p>目前数据分类分级手段在商密数据场景的局限大语言模型能带来的潜在提升与场景适配研究</p><p></p><p>2. 分类分级可行性验证过程</p><p></p><p>找到正确的提问方式大语言模型分类分级过程中遇到的问题：模型幻觉、数据安全性</p><p></p><p>3. 在控制成本的基础上找到平衡应用落地方式</p><p></p><p>在现有分类分级产品中引入大语言模型能力分类分级准确率和召回率数据情况</p><p></p><p>4. 挑战与展望</p><p></p><p>分类分级大模型的常态运营</p><p></p><p>实践痛点</p><p></p><p>大语言模型存在幻觉问题，偶发性会出现脱离框架的答案，同时大语言模型使用成本目前相较专家特征规则和自建模型没有优势，因此成本控制问题会限制其应用规模。</p><p></p><p>演讲亮点</p><p></p><p>商业秘密数据特征模糊，使用专家特征规则无法进行有效识别，而大语言模型可充分利用上下文和常识知识更加准确的识别分类商业秘密数据。</p><p></p><p>听众收益</p><p></p><p>了解携程在使用大语言模型进行商密数据分类分级的实践经验了解大语言模型在数据安全分类分级基础性工作上提升效率质量的价值</p><p></p><p><a href="https://qcon.infoq.cn/2024/shanghai/presentation/6111">演讲主题：百度基于大模型安全运营的质效提升实践</a>"</p><p></p><p>讲师：包沉浮（百度杰出架构师，安全技术委员会主席 ）</p><p></p><p>演讲摘要</p><p></p><p>百度作为一家业务复杂的大型互联网企业，同时又是关键基础设施，随着网络安全威胁的日益加剧，传统的安全运营手段在效率和效果上都面临巨大挑战。本次分享将介绍百度如何基于大模型构建深度安全推理智能体框架，实现运营效率和效果的双重提升，并展示包括告警自动研判和漏洞事件分析在内的实践经验，希望能给听众带来一些大模型安全领域应用最佳实践的启示。</p><p></p><p>演讲提纲</p><p></p><p>1. 背景和挑战</p><p></p><p>大模型开始逐步应用于安全运营场景百度安全运营面临的双效（效率 + 效果）提升需求</p><p></p><p>2. 架构设计</p><p></p><p>‍设计目标：基于深度安全推理智能体框架，实现双效提升设计考虑：人机协同的工作流设计（运营流程梳理、质量标准定义、人机交互模式）、模型能力边界与拓展（模型结果可信度和可解释性、知识和工具依赖）、实施成本整体‍架构（自底向上）：底座模型的知识补充RAG、CoT、Function calling流程编排智能体 Review 机制</p><p></p><p>3. 实践案例</p><p></p><p>告警自动 / 辅助研判 + 事件处置漏洞事件自动分析 + 处置</p><p></p><p>4. 未来展望</p><p></p><p>大模型原生的安全运营中心</p><p></p><p>实践痛点</p><p></p><p>明确目标，围绕安全运营场景的风险偏好，制定更贴合实际的落地目标，避免直接盲目追求大而全的零职守无人干预以数据驱动能力迭代，缺少可用数据时应当从实际场景中提升标准化和自动化水平，引入业务的数据活水，避免直接使用脱离业务的合成数据</p><p></p><p>演讲亮点</p><p></p><p>从架构设计层面剖析安全运营场景双效提升应遵循的必要准则，提供构建深度安全推理智能体框架的完整视角细粒度展现告警研判、漏洞分析处置等实际场景的双效提升最佳实践</p><p></p><p>听众收益</p><p></p><p>了解互联网大厂的安全运营需求痛点与大模型实践经验了解规模化且对效果要求较高的安全运营场景下，大模型智能体设计考虑与整体架构</p><p></p><p><a href="https://qcon.infoq.cn/2024/shanghai/presentation/6064">演讲主题：安全大模型的最后一公里实践：智能决策与自动响应</a>"</p><p></p><p>讲师：傅奎（雾帜智能联合创始人 &amp; CTO）</p><p></p><p>演讲摘要</p><p></p><p>主流安全大模型及应用场景侧重于非结构化数据的整理、总结、分析和建议，但还缺少最后一步——如何让大模型参与安全响应的决策，并在决策后自动化完成动作的执行。本议题将介绍，安全专家如何借助大模型，自动生成网络安全响应流程（安全剧本），并自动完成剧本的执行，由此在安全运营场景最后一公里完成大模型应用场景落地。</p><p></p><p>演讲提纲</p><p></p><p>1. 大模型在网络安全领域应用</p><p></p><p>发布 SecGPT 的安全厂商大模型在安全领域的应用场景共性不足（重分析，轻决策）</p><p></p><p>2. 安全大模型在智能决策领域应用探索</p><p></p><p>模型是否有能力给出合理建议如何让模型给出更高质量的决策模型决策结果的潜在风险</p><p></p><p>3. 安全大模型实战应用实践案例</p><p></p><p>OWASP TOP 10 典型场景大模型在 Web 攻击攻击领域的应用效果降低模型决策风险的实践思路</p><p></p><p>4. 未来展望</p><p></p><p>让模型设计剧本 VS 让模型选择剧本大模型落地安全最后一公里（能力调度）如何实现终极目标：零值守无人安全运营中心</p><p></p><p>实践痛点</p><p></p><p>针对特定性的安全事件，如何设计响应策略人工智能设计的安全策略是否可以实现全自动执行距离真正零值守还有哪些问题没有解决</p><p></p><p>演讲亮点</p><p></p><p>不仅仅使用大模型对安全事件做分析，还通过安全大模型对安全事件响应作出决策，安全大模型完全决策，并最终付诸实施通过安全能力实现安全策略的落地，该环节减少对人工的依赖，减少对安全专家的依赖，是未来零值守安全运营中心的重要基础。</p><p></p><p>听众收益</p><p></p><p>传统安全运营的场景，痛点和困境有别于安全大厂的安全运营智能化实践安全大模型最后一公里所解决的问题和价值</p><p></p><p><a href="https://qcon.infoq.cn/2024/shanghai/presentation/6044">演讲主题：全球视野下的合规之道：携程海外数据安全管理实践</a>"</p><p></p><p>讲师：胡立平（携程数据安全合规负责人）</p><p></p><p>演讲摘要</p><p></p><p>出海成为众多国内企业实现业绩新增长曲线的选择，然而随着数据的重要性提升，法律及监管关注度也在增强，携程作为在线旅行行业较早布局海外业务的企业，在海外数据安全合规风险上也有所积累。本次演讲将分享携程海外数据安全合规风险管理的思路和经验，希望能给相关出海企业企业带来一些合规实践上的启示。</p><p></p><p>演讲提纲</p><p></p><p>出海面临的数据安全合规挑战</p><p></p><p>法律法规近些年主要变化及监管挑战从数据视角深度剖析出海合规风险携程应对策略及实践携程的海外合规整体策略设计如何通过 GRC 平台形成风险管理闭环如何保障旗下 Trip.com 产品的隐私合规</p><p></p><p>海外数据安全合规未来展望和应对思考</p><p></p><p>实践痛点</p><p></p><p>合规风险管理线上化需要建立在标准化的风险管理、优秀的产品设计、合理的内部运营流程等基础上，才能实现控制域的完备性、控制方法的准确性、关键控制的有效性、审计覆盖的充分性等关键指标。</p><p></p><p>演讲亮点</p><p></p><p>结合合规实战介绍部分法域的合规挑战介绍携程自研 GRC(Governance, Risk and Compliance ) 平台如何融合监管情报、外规内化、审计整改等多个治理环节，解决出海过程中面临多法域、多品牌的风险管理挑战</p><p></p><p>听众收益</p><p></p><p>帮助了解现有海外数据安全相关合规的整体风险态势帮助了解标准化及线上化在多法域数据安全合规风险管理中的价值</p><p></p><p><a href="https://qcon.infoq.cn/2024/shanghai/presentation/6094">演讲主题：跨国经营中的企业数据合规之道</a>"</p><p></p><p>讲师：陈晓芳（ vivo 数据合规专家）</p><p></p><p>演讲摘要</p><p></p><p>各大企业，尤其是跨国企业，由于业务拓展或人力管理等因素，不可避免地会涉及数据出境。与此同时，数据出境相关监管规范和管理机制日益完善，对企业管控数据出境行为提出了新的挑战。本次演讲将分享 vivo 在数据出境管理方面的实践经验，希望能给相关出海企业带来一些合规启示。</p><p></p><p>演讲提纲</p><p></p><p>监管框架：解析数据出境合规路径</p><p></p><p>数据出境的立法背景和监管趋势数据出境的三种合规路径：安全评估、标准合同备案（SCC）、个人信息保护认证探讨网信办规定的数据出境豁免条件及其适用性</p><p></p><p>2. 企业视角：企业如何管控数据跨境</p><p></p><p>介绍企业如何建立数据出境监控和排查机制阐述企业在数据出境过程中的合规流程和全周期管理措施</p><p></p><p>3. 实战案例：vivo 的数据出境安全评估</p><p></p><p>分享 vivo 申报数据出境安全评估的经验，着重于省网信办材料审核的重点，打回材料的理由等真实案例</p><p></p><p>4. 数据出境合规未来的挑战与展望</p><p></p><p>分析数据出境合规管理中的主要困难，提供相应应对策略和建议</p><p></p><p>实践痛点</p><p></p><p>系统的数据出境风险管理体系，需要完备全面的数据出境监测体系、精确的风险触发机制和合理的出境备案应对方案，才能实现企业数据出境的全面管控。</p><p></p><p>演讲亮点</p><p></p><p>vivo 在应对数据出境合规挑战过程中的管控措施，以及申报安全评估时的经验。</p><p></p><p>听众收益</p><p></p><p>帮助了解数据出境方向整体的合规风险及应对措施了解 vivo 在数据出境管控方面的措施及安全评估申报过程中的相关经验</p><p></p><p>更多精彩内容将在 10 月 18 - 19 日 QCon 上海站为您现场呈现，期待与您共赴这场技术之约。如果您有好的技术实践案例想要与我们分享，欢迎点击<a href="https://jsj.top/f/EbrZFg">链接</a>"提交演讲申请。</p><p></p><p>【会议推荐】</p><p></p><p>InfoQ 将于 10 月 18-19 日在上海举办 <a href="https://qcon.infoq.cn/2024/shanghai/">QCon 全球软件开发大会</a>" ，覆盖前后端 / 算法工程师、技术管理者、创业者、投资人等泛开发者群体，内容涵盖当下热点（AI Agent、AI Infra、RAG 等）和传统经典（架构、稳定性、云原生等），侧重实操性和可借鉴性。现在大会已开始正式报名，可以享受 8 折优惠，单张门票立省 960 元（原价 4800 元），详情可联系票务经理 &nbsp;17310043226 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/df/dfd31ee989a7951439a77fec138d4cf8.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/kLvNa2lBEouaaN0LlgyU</id>
            <title>不要掉入“AI 工程就是一切”的陷阱</title>
            <link>https://www.infoq.cn/article/kLvNa2lBEouaaN0LlgyU</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/kLvNa2lBEouaaN0LlgyU</guid>
            <pubDate></pubDate>
            <updated>Thu, 22 Aug 2024 02:37:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>常有人错误地将这样一句话归因于一些领导者，尽管它可能完全是虚构的：“外行谈论战略和战术，内行关注运营。”从战术的角度看，我们面对的是一系列独特的问题，从运营角度，我们看到的是需要解决的组织功能失调模式。在战略视角看到的是机会，在运营视角看到的是需要应对的挑战。</p><p></p><p>在本系列文章的第一部分，我们介绍了 <a href="https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247619519&amp;idx=2&amp;sn=a468ebed03640e82c4a4d3e14d8434c6&amp;scene=21#wechat_redirect">LLM 的战术性操作</a>"。接下来，我们将拓宽视野，深入探讨长期的战略规划。在这一部分，我们将讨论构建 LLM 应用程序的运营层面，这些应用程序是战略与战术的桥梁，将理论与实际应用紧密结合。</p><p></p><p>在运营 LLM 应用程序过程中，我们遇到了一些似曾相识的问题，这些问题在传统软件系统的运营中也常常出现，不同的是它们也带来了一些新的挑战，使得探索过程充满了趣味。此外，运营 LLM 应用程序还带来了一些全新的问题。我们将这些问题及其答案归纳为四个部分：数据、模型、产品和人。</p><p></p><p>对于数据，我们将探讨这几个问题：如何以及多久需要重新审视一次 LLM 的输入和输出？如何测量并有效减少测试环境与生产环境之间的偏差？</p><p></p><p>对于模型，我们将探讨这几个问题：如何将语言模型集成到现有的技术栈中？如何看待模型的版本控制以及如何在不同模型和版本之间进行平滑迁移？</p><p></p><p>对于产品，我们将探讨这几个问题：设计应该在何时介入应用程序的开发过程，为什么要“尽早介入”？如何设计能够充分吸纳人类反馈的用户体验？在面对相互冲突的需求时如何安排优先级？如何校准产品风险？</p><p></p><p>最后，对于人，我们将探讨这几个问题：选择哪些人才来构建成功的 LLM 应用程序，以及何时招募他们？如何培养正确的实验性文化？如何利用现有的 LLM 应用程序来辅助开发自己的 LLM 解决方案？哪一个更关键：流程还是工具？</p><p></p><p></p><h3>运营：LLM 应用程序的构建和开发团队</h3><p></p><p></p><p></p><h4>数据</h4><p></p><p></p><p>正如精选的食材能够成就一道佳肴，高质量的输入数据同样对机器学习系统的表现起着决定性作用。此外，系统的输出是评估其是否正常工作的唯一方式。所有人都紧密关注数据，他们每周都会花几个小时细致地分析输入和输出，以便更好地理解数据分布：模式、边缘情况以及模型的局限性。</p><p></p><p></p><h4>检查开发与生产偏差</h4><p></p><p></p><p>在传统机器学习流程中存在的一个普遍问题是训练与服务之间的偏差。这种情况通常发生在模型训练时使用的数据与模型在实际应用中遇到的数据不一致时。尽管我们可以无需训练或微调就能够使用 LLM，从而避免了训练集的问题，但开发与生产环境之间的数据偏差问题依然存在。关键在于，在开发阶段测试系统时所用的数据应与系统在生产环境中实际面对的数据相一致。如果不是这样的话，我们可能会发现生产环境中的模型准确性会受影响。</p><p></p><p>LLM 开发与生产偏差可以分为两种类型：结构性偏差和基于内容的偏差。结构性偏差包括格式不一致，比如 JSON 字典与 JSON 列表之间的差异、不一致的大小写以及错误，如错别字或不完整的句子片段。这些错误可能导致模型性能不可预测，因为不同的 LLM 是基于特定的数据格式训练的，而提示词对微小变化都非常敏感。基于内容的偏差（或“语义”偏差）指的是数据的含义或上下文的差异。</p><p></p><p>正如传统的机器学习一样，对 LLM 的输入和输出进行定期的偏差检测是非常有必要的。输入和输出的长度或特定格式要求（例如，JSON 或 XML）等指标是跟踪变化最直接的方式。对于更“高级”的漂移检测，可以采用更高级的方法，如聚类输入 / 输出对的嵌入向量可用于检测语义漂移：如果用户讨论的主题发生变化，这可能表明他们正在探索模型以前没有接触过的领域。</p><p></p><p>在测试变更时，例如提示词工程，确保保留数据集是最新的，并且能够反映用户交互的最新类型。例如，如果错别字在生产环境的输入中很常见，那么它们也应该出现在保留数据中。除了进行数值偏差检查之外，对输出进行定性评估也很有用的。定期检查模型输出——俗称“氛围检查”——可以确保结果符合预期并满足用户需求。最后，将非确定性纳入偏差检查中——通过多次运行测试数据集中的每个输入并分析所有输出，可以增加捕捉那些可能仅偶尔发生异常情况的可能性。</p><p></p><p></p><h4>每天检查 LLM 的输入和输出样本</h4><p></p><p></p><p>LLM 是动态且持续进化的。尽管它们具有令人印象深刻的零样本学习能力，并且经常能够生成令人满意的输出，但它们的失败模式却非常难以预测。对于自定义任务，定期审查数据样本有助于培养对 LLM 性能的直观理解。</p><p></p><p>生产环境的输入输出对是 LLM 应用程序的“现场证据”，它们不会被替换。最近的研究表明，开发者对什么构成“好”和“坏”输出的看法会随着他们与更多数据的交互而发生变化（即所谓的标准漂移）。虽然开发者可以预先设定一些标准来评估 LLM 输出，但这些预定义的标准通常不够全面。例如，在开发过程中，我们可能会更新提示词，以增加获得良好响应的概率，并降低获得不良响应的概率。这种评估、重新评估和标准更新的迭代过程是必不可少的，因为在没有直接观察输出的情况下，很难预测 LLM 的行为或人类的偏好。</p><p></p><p>为了有效地管理大型语言模型，我们需要记录 LLM 的输入和输出。通过每天检查这些日志样本，我们能够及时识别并适应新的模式或故障模式。在发现新问题时，我们可以立即编写断言或制定评估策略来应对这些问题。同样，对故障模式定义的更新都应实时反映在评估标准中。这些“氛围检查”可以帮助我们捕捉到不良输出的信号，而通过编写代码和断言，我们能够将这些检查操作化，使之成为可执行的过程。最后，这种态度需要在团队中得到普及，例如通过在值班轮换中加入对输入和输出的审查或注释环节。</p><p></p><p></p><h3>调用模型</h3><p></p><p></p><p>在使用 LLM API 时，我们确实可以依靠少数几家技术供应商的智能成果。虽然这为我们提供了便利，但同时也带来了一些权衡，包括性能、延迟、吞吐量和成本等方面。此外，随着更新、更好的模型（在过去一年中几乎每个月都会有新模型发布）的发布，我们需要随时准备好更新我们的产品，以弃用旧模型并迁移到新模型。在这一章节，我们将分享在使用这些我们不能完全控制的技术时的经验，特别是关于如何管理那些我们无法自托管的模型。</p><p></p><p></p><h4>生成结构化输出，简化下游集成</h4><p></p><p></p><p>对于大多数现实世界的场景，LLM 的输出需要通过机器可读的格式提供给下游应用程序。例如，Rechat，一个房地产 CRM 系统，需要结构化的响应来在前端显示小部件。同样，Boba，一个用于生成产品策略想法的工具，需要输出包含标题、摘要、可信度得分和时间范围字段的结构化信息。LinkedIn 通过限制 LLM 生成 YAML 格式的数据，用于决定使用哪种”技能“，并提供调用这些技能所需的参数。</p><p></p><p>这种应用模式体现了 Postel 定律的极致：在接收时宽容（接受任意自然语言），在发送时保守（输出类型化、机器可读的对象）。因此，我们期望这种方法具有很高的稳定性和可靠性。</p><p></p><p>目前，Instructor 和 Outlines 是从 LLM 中提取结构化输出的实际标准。如果你在使用 LLM API（比如 Anthropic 或 OpenAI），请优先选择 Instructor；而如果你在使用自托管的模型（例如 Hugging Face），则推荐使用 Outlines。</p><p></p><p>为不同模型修改提示词是一种痛苦</p><p></p><p>有时，我们精心编写的提示词在一种模型上表现出色，但在另一种模型上却表现平平。这种情况可能在我们更换不同模型供应商时发生，也可能出现在同一模型的不同版本升级过程中。</p><p></p><p>例如，Voiceflow 在从 gpt-3.5-turbo-0301 迁移到 gpt-3.5-turbo-1106 时，他们的意图分类任务性能下降了 10%。（幸运的是，他们进行了评估！）同样，GoDaddy 注意到了一个积极的变化，升级到 1106 版本缩小了 gpt-3.5-turbo 和 gpt-4 之间的性能差距。（或者，如果你是一个乐观的人，可能会对 gpt-4 的领先优势在这次升级中有所减少感到失望。）</p><p></p><p>因此，如果我们不得不在模型之间迁移提示词，预计这将是一个比简单更换 API 端点更耗时的过程。不要想当然地认为使用相同的提示词能够得到相似或更好的结果。此外，拥有一个可靠的自动化评估系统，可以在迁移前后有效地衡量任务性能，并显著减少所需的手动验证工作。</p><p></p><p></p><h4>版本控制和固定你的模型</h4><p></p><p></p><p>在机器学习管道中，“改变一点，影响全局”是一个普遍现象。这一点在我们依赖自己未参与训练的组件，例如大型语言模型（LLM）时，显得尤为突出，因为这些模型可能会在不被我们察觉的情况下发生变化。</p><p></p><p>幸运的是，许多模型供应商提供“锁定”特定模型版本（例如，gpt-4-turbo-1106）的选项。这样，我们可以使用特定版本的模型权重，确保它们保持不变。在生产环境中锁定模型版本有助于防止模型行为发生意外变化，从而减少因模型更新可能导致的问题（例如过于冗长的输出或其他不可预见的故障模式）。</p><p></p><p>此外，可以考虑维护一个影子管道，这个管道镜像了生成环境的设置，但使用的是最新的模型版本。这为实验和测试新版本提供了一个安全的环境。一旦确认这些新模型的输出在稳定性和质量上符合标准，就可以自信地升级生产环境中的模型版本。</p><p></p><p>选择能够完成任务的最小模型</p><p></p><p>在开发新应用程序时，使用最强大的模型往往具有极大的吸引力。然而，一旦我们确认了技术可行性，就很有必要尝试一下使用更小的模型是否能够产生同样优质的结果。</p><p></p><p>小模型的优势是较低的延迟和成本。虽然在性能上可能略显逊色，但通过诸如思维链、n-shot 提示词和上下文学习等先进技术的应用，它们完全有可能超越自身的限制。除了调用 LLM API，针对特定任务进行微调也能够显著提升性能。</p><p></p><p>综合考虑，一个精心设计的工作流，即使使用较小的模型，通常也能匹敌甚至超越单个大型模型的输出质量，同时还具备更快的处理速度和更低的成本。例如，这个推文分享了 Haiku 结合 10-shot 提示词的表现优于零样本的 Opus 和 GPT-4。从长远来看，我们期望看到更多流程工程的案例，使用较小的模型实现输出质量、响应时间和成本之间的最佳平衡。</p><p></p><p>作为另一个典型案例，我们来看一下那些看似简单的分类任务。轻量级的 DistilBERT（6700 万参数）模型居然是一个出人意料的强大基线。在开源数据上进行微调后，拥有 4 亿参数的 DistilBART 更是一个不错的选择——它在识别幻觉方面的 ROC-AUC 值达到了 0.84，在延迟和成本方面增加不到 5%，超越了大多数大型语言模型。</p><p></p><p>重点是，我们不要轻视那些模较小的模型。尽管人们往往倾向于对各种问题都应用庞大的模型，但通过一些创新思维和实验探索，我们常常能够发现更为高效的解决方案。</p><p></p><p></p><h3>产品</h3><p></p><p></p><p>虽然新技术为我们带来了新的可能性，但构建卓越产品的核心原则始终不变。因此，即使是在第一次面临新挑战时，我们也无需在产品设计方面重新发明轮子。将我们的 LLM 应用程序开发建立在坚实的产品理念之上，这将使我们能够为用户带来真正的价值。。</p><p></p><p></p><h4>及早并频繁地进行设计</h4><p></p><p></p><p>设计师的参与有助于推动你深入思考如何构建和向用户展示产品。我们有时会将设计师简单定义为美化事物的人。然而，除了用户界面之外，他们还会全面思考如何改进用户体验，甚至是打破现有的规则和范式。</p><p></p><p>设计师擅长将用户需求转化为各种各样的形式。这些形式有些更容易实现，而有些则为 AI 技术提供了更多或更少的施展空间。与许多其他产品一样，构建 AI 产品应该以要完成的任务为中心，而不是驱动这些任务的技术。</p><p></p><p>问问自己：“用户期望这个产品为他们完成哪些任务？这些任务是聊天机器人擅长的吗？能够使用自动完成功能？也许可以尝试一些不同的方案！”审视现有的设计模式，思考它们与要完成的任务之间的联系。这些是设计师为团队能力带来的宝贵贡献。</p><p></p><p></p><h4>以 HITL 为导向设计用户体验</h4><p></p><p></p><p>一种提升注释质量的方式是将 Human-in-the-Loop（HITL）融入到用户体验（UX）设计中。通过让用户轻松地提供反馈和更正，我们不仅能即时优化输出，还能收集有洞察力的数据来改进我们的模型。</p><p></p><p>设想一个电子商务平台，用户需要上传并分类他们的商品。我们可以从多个角度来设计用户体验：</p><p></p><p>用户手动选择产品类别；LLM 定期检查新产品并在后端更正分类错误。用户不选择产品类别；LLM 定期在后端对产品进行分类（可能存在错误）。LLM 提供实时产品类别建议，用户可以根据自己的判断进行验证和更新。</p><p></p><p>虽然这三种方法都利用了 LLM，但它们提供了非常不同的 UX。第一种方法将初始责任放在用户身上，并将 LLM 作为后续的辅助。第二种方法减少了用户的负担，但不提供透明度或控制权。第三种方法找到了二者之间的平衡点。LLM 提前建议类别，减少了用户的认知负担，他们无需深入了解复杂的分类体系。同时，用户可以审查和修改这些建议，他们对如何分类产品有最终的决定权，将控制权牢牢掌握在手中。作为一个额外的好处，第三种方法为模型改进创建了一个自然反馈循环。好的建议会被接受（正反馈标签），不好的建议会被更新（负反馈标签转成正反馈标签）。</p><p></p><p>这种建议、用户验证和数据收集的模式在多个应用领域中都得到了广泛应用：</p><p></p><p>编码助手：用户可以接受建议（强烈正反馈）、接受并调整建议（正反馈）或忽略建议（负反馈）。Midjourney：用户可以选择放大并下载图像（强烈正反馈）、修改图像（正反馈）或生成一组新图像（负反馈）。聊天机器人：用户可以对响应点赞（正反馈）或不点赞（负反馈），如果响应真的很差，选择重新生成响应（强烈负反馈）。</p><p></p><p>反馈可以是显式或隐式的。显式反馈是用户对产品提出的意见或评价，隐式反馈是我们需要从用户交互中捕捉的信息，无需用户有意提供。编码助手和 Midjourney 是隐式反馈的例子，而点赞和不点赞是显式反馈。如果我们能够像编码助手和 Midjourney 那样设计 UX，就可以收集到大量的隐式反馈来改进我们的产品和模型。</p><p></p><p></p><h4>调整需求层次的优先级</h4><p></p><p></p><p>在准备将演示转化为实际应用时，我们需要仔细考虑以下几个关键要素：</p><p></p><p>可靠性：确保 99.9% 的正常运行时间，同时遵循结构化输出标准；无害性：避免生成攻击性、NSFW 或其他有害的内容；事实一致性：忠实于提供的上下文，不虚构信息；实用性：与用户的需求和请求相关；可扩展性：延迟 SLA，支持高吞吐量；成本效益：需要考虑预算限制；其他：安全性、隐私保护、公平性、GDPR 合规性、DMA 合规性等。</p><p></p><p>如果我们试图同时解决所有这些要求，我们将永远无法完成产品交付。因此，我们必须进行优先级排序，并且要果断。这意味着我们要清楚哪些是没有商量余地的（例如，可靠性、无害性），没有这些我们的产品就是不可行的。关键在于识别出最基本的产品功能。我们必须接受第一个版本不会完美的事实，并通过不断迭代来改进。</p><p></p><p></p><h4>根据用例校准风险承受能力</h4><p></p><p></p><p>在选择语言模型及其审查标准时，我们需要根据应用场景和目标受众来做出判断。对于那些提供医疗或财务咨询的聊天机器人，我们必须设定极高的安全和准确性标准。因为任何错误或不当的输出都可能造成严重的后果，并且会严重损害用户对我们的信任。然而，对于不那么关键的应用，比如推荐系统，或者那些仅供内部使用的应用程序，如内容分类或摘要，过分严格的要求可能会拖慢开发进度，却不会为提升价值带来太大帮助。</p><p></p><p>这与最近发布的 a16z 报告中的观点相吻合，许多公司在内部 LLM 应用方面比外部应用进展得更快。通过在内部生产力工具中引入 AI，组织可以在更加受控的环境中实现价值，同时学习如何有效地管理风险。然后，随着他们信心的增强，可以逐步扩展到面向客户的应用场景。</p><p></p><p></p><h3>团队与角色</h3><p></p><p></p><p>定义工作职能不是件容易的事，而在这个新兴领域编写工作描述比其他领域更具挑战性。我们决定不再使用交叉工作职能的文氏图或工作描述的建议。相反，我们将引入一个新的职位——AI 工程师——并探讨其在组织中的位置。同时，我们也将讨论团队其他成员的角色以及如何合理分配责任，这至关重要。</p><p></p><p></p><h4>专注于流程，而不是工具</h4><p></p><p></p><p>面对新兴的范式，例如大型语言模型，软件工程师们往往更倾向于采用各种工具。这种偏好有时会导致我们忽视了这些工具本应解决的问题和优化的流程。结果，许多工程师不得不应对由此产生的偶然的复杂性，对团队的长期生产力构成了负面影响。</p><p></p><p>例如，这篇文章讨论了某些工具如何为大型语言模型自动生成提示词。文章认为（在我看来是正确的），那些在没有先理解问题解决方法或流程的情况下使用这些工具的工程师最终会累积不必要的技术债务。</p><p></p><p>除了偶然的复杂性，许多工具还常常存在规格不足的问题。以不断壮大的 LLM 评估工具行业为例，它们提供所谓的“即插即用”的 LLM 评估服务，涵盖毒性、简洁性、语调等通用评估指标。我们发现许多团队在没有深入分析其领域特有的失败模式的情况下，就盲目采纳了这些工具。与此形成鲜明对比的是 EvalGen，它通过深度参与用户的每一个环节——从定义标准到标注数据，再到评估检查——引导用户构建适合特定领域的评估体系。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/73/73d82d6652c837c82745f6d8a7e174e5.png" /></p><p></p><p>Shankar, S. 等人（2024）“谁来验证验证器？将 LLM 辅助评估 LLM 输出与人类偏好对齐”。来源：<a href="https://arxiv.org/abs/2404.12272">https://arxiv.org/abs/2404.12272</a>"</p><p></p><p>EvalGen 引导用户通过遵循最佳实践来制定 LLM 评估标准，即：</p><p></p><p>定义特定领域的测试（通过提示词自动引导）。它们可以是带有代码的断言，或者是采用“LLM 即评委”的形式。强调将测试与人类判断对齐的重要性，使用户能够验证测试是否确实捕捉到了既定的标准。随着系统（如提示词内容等）的变化不断迭代和优化测试标准。</p><p></p><p>EvalGen 为开发人员提供了评估构建过程的框架性理解，而不是将他们限制在特定工具的使用上。我们发现，一旦 AI 工程师获得了这种宏观视角，他们往往会选择采用更简洁的工具，或者根据自己的需求自行开发解决方案。</p><p></p><p>LLM 的组成部分远不止提示词编写和评估，其复杂性无法在此一一列举。关键在于 AI 工程师在采用工具之前要深入理解其背后的流程和原理。</p><p></p><p></p><h4>持续地实验</h4><p></p><p></p><p>机器学习产品与实验密切相关。不仅涉及 A/B 测试、随机对照试验，还包括频繁尝试修改系统的最小组件并进行离线评估。人们热衷于评估的真正原因并非仅仅为了可靠性和信心——而是为了让实验成为可能。你的评估越精确，就能越迅速地进行实验，进而更快地发现系统的最佳配置。</p><p></p><p>尝试采用不同的方法解决同一个问题是一种很常见的做法，因为现在的实验成本很低。收集数据和训练模型的高昂成本已经得到有效控制——提示词工程的成本仅略高于人力投入。确保你的团队成员都掌握了提示词工程的基础知识。这不仅能激发他们进行实验的热情，还能促进组织内部不同观点的交流与碰撞。</p><p></p><p>此外，实验不仅仅是为了探索，而是要学会利用它们。如果你手头有一个新的任务，可以考虑让团队的其他成员从不同的视角来处理它。尝试寻找更高效的方法，探索如思维链或 few-shot 提示词等技术，以提高工作质量。不要让工具限制了你的实验；如果是这样，那就重新构建它们，或者购买新的工具。</p><p></p><p>最后，在产品或项目规划阶段，务必留出足够的时间来构建评估机制并进行多项实验。在考虑工程产品的规格时，为评估过程设定明确的标准。在制定路线图时，不要低估了实验所需的时间。要预见到在生产交付之前，可能需要进行多轮的开发和评估迭代。</p><p></p><p></p><h4>让每个人都能使用新的 AI 技术</h4><p></p><p></p><p>随着生成式 AI 采用率的增加，我们希望整个团队——不仅仅是专家——都能理解并自信地使用这项新技术。没有比亲自实践更好的方式去培养对大型语言模型工作原理的直观理解了，比如它们的响应延迟、故障模式和用户体验。LLM 相对容易使用：你无需编码技能就可以为流程管道提升性能，每个人都可以通过提示词工程和评估做出实质性的贡献。</p><p></p><p>教育是关键环节，可以从提示词工程的基础开始，如利用 n-shot 和思维链等技术，引导模型生成期望的输出。拥有这方面知识的人还可以教授更技术性的内容，例如大型语言模型本质上是自回归的。换句话说，虽然输入可以并行处理，但输出是顺序的。因此，生成延迟更多地取决于输出的长度而非输入的长度——这是在设计用户体验和设定性能预期时需要考虑的一个关键因素。</p><p></p><p>我们还可以提供更多实践和探索的机会，比如举办一次黑客马拉松。虽然让整个团队投入数日时间在探索性项目上看起来成本较高，但最终的成果可能会超出你的预期。我们见证了一个团队通过黑客马拉松，在短短一年内就实现了他们原本计划三年完成的路线图。另一个团队则通过黑客马拉松，引领了一场用户体验的范式转变，这种转变现在因为大型语言模型的加入而成为可能。</p><p></p><p></p><h4>不要掉入“AI 工程就是一切”的陷阱</h4><p></p><p></p><p>随着新职位名称的出现，人们往往容易过分夸大这些角色的能力。这通常会导致在实际工作职责变得逐渐明确时，人们不得不去做一些痛苦的调整。新入行的人和负责招聘的经理可能会夸大声明或抱有不切实际的期望。在过去的十年里，这类显著的例子包括：</p><p></p><p>数据科学家：“在统计学方面比任何软件工程师都强，在软件工程方面比任何统计学家都强的人”机器学习工程师（MLE）：以软件工程为中心的机器学习视角</p><p></p><p>最初，许多人认为数据科学家单枪匹马就能驾驭数据驱动的项目。然而，现实情况已经清晰地表明，为了有效地开发和部署数据产品，数据科学家必须与软件工程师和数据工程师紧密合作。</p><p></p><p>这种误解在 AI 工程师这一新兴角色上再次出现，一些团队误以为 AI 工程师就是他们需要的一切。实际上，构建机器学习或 AI 产品需要一个由多种专业角色 组成的团队。我们与十多家公司就 AI 产品进行了深入咨询，发现他们普遍都陷入了认为“AI 工程就是一切”的陷阱。这种认知导致产品往往难以越过演示阶段，因为公司忽视了构建产品所涉及的关键方面。</p><p></p><p>例如，评估和度量对于将产品从单一的领域检查阶段扩展到广泛应用阶段来说至关重要。有效的评估能力与机器学习工程师通常所具备的优势相辅相成——一个完全由 AI 工程师组成的团队可能缺乏这些技能。Hamel Husain 在他最近的研究中强调了这些技能的重要性，包括监测数据漂移和制定针对特定领域的评估标准。</p><p></p><p>以下是在构建 AI 产品的过程中你需要的不同类型角色，以及他们在项目各个阶段大致的参与时机：</p><p></p><p>首先，专注于构建产品。这个阶段可能涉及 AI 工程师，但并非必须。AI 工程师在快速原型设计和迭代产品方面具有显著的价值（用户体验、数据处理管道等）。随后，通过系统化地收集和分析数据，为产品打下坚实的基础。根据数据的性质和体量，你可能需要平台工程师或数据工程师。你还需要建立查询和分析数据的系统，以便快速定位问题。最后，你将致力于优化 AI 系统。这并不一定涉及训练模型，包括设计评估指标、构建评估系统、执行实验、优化 RAG 检索、调试随机性问题等。机器学习工程师非常擅长这些工作（尽管 AI 工程师也可以通过学习掌握这些技能）。但如果你没有完成前面的基础步骤，招聘机器学习工程师可能并不明智。</p><p></p><p>除此之外，你始终需要一个领域专家。在小型企业，这通常是创始团队的成员；而在大型企业，产品经理也可以担任这一角色。角色的介入时机至关重要。在不恰当的时间（例如，过早让机器学习工程师介入）招聘人员或介入顺序不对，不仅浪费时间和金钱，还会导致频繁的人员更替。此外，在前面两个阶段定期与机器学习工程师沟通（但不全职让他们介入）将有助于公司为未来的成功打下坚实的基础。</p><p></p><p>原文链接：</p><p>https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-ii/</p><p></p><p>声明：本文由 InfoQ 翻译，未经许可禁止转载。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/46068541f35de5382cda6ed9f</id>
            <title>「模型量化技术」可视化指南：A Visual Guide to Quantization</title>
            <link>https://www.infoq.cn/article/46068541f35de5382cda6ed9f</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/46068541f35de5382cda6ed9f</guid>
            <pubDate></pubDate>
            <updated>Thu, 22 Aug 2024 02:37:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><blockquote>编者按：随着大语言模型（LLMs）规模的不断扩大，如何在有限的计算资源下高效部署这些模型成为了一个迫切需要解决的问题。模型量化作为一种有效的模型压缩技术，在保持模型性能的同时大大降低了计算和存储开销，因此广受关注。但对于许多人来说，模型量化的具体原理和实现方法仍然是一个“黑盒”。我们今天为大家带来的这篇文章，通过可视化图示详细解析各种模型量化技术的原理和实现方法，为各位读者提供一个全面且直观的模型量化技术指南。本文旨在帮助各位读者涉猎以下技能领域：理解模型量化技术的基本原理和作用掌握多种模型量化方法及其优缺点学会如何选择合适的量化方法，并根据实际场景进行调整我们分享这篇全面且深入的技术解析，期望各位读者不仅能够理解模型量化的基本原理，还能洞察该领域的最新发展趋势。随着模型量化技术的不断进步，我们有理由相信，未来将会出现更加高效、更轻量级的大语言模型，为 AI 技术的更广泛应用铺平道路。</blockquote><p></p><p></p><p>作者 🕶 | Maarten Grootendorst</p><p></p><p>编译 🐣 | 岳扬</p><p></p><h1>目录🧾</h1><p></p><p>01 第 1 部分：LLMs 存在的“问题”</p><p></p><p>1.1 参数数值（value）的表示方法</p><p></p><p>1.2 内存限制问题</p><p></p><p>02 第 2 部分：模型量化技术简介</p><p></p><p>2.1 常用的数据类型</p><p></p><p>2.1.1 FP16</p><p></p><p>2.1.2 BF16</p><p></p><p>2.1.3 INT8</p><p></p><p>2.2 对称量化 Symmetric Quantization</p><p></p><p>2.3 非对称量化 asymmetric quantization</p><p></p><p>2.4 取值范围的映射与裁剪</p><p></p><p>2.5 校准过程 Calibration</p><p></p><p>2.5.1 权重（和偏置项） Weights (and Biases)</p><p></p><p>2.5.2 激活值</p><p></p><p>03 第 3 部分：Post-Training Quantization</p><p></p><p>3.1 动态量化（Dynamic Quantization）</p><p></p><p>3.2 静态量化（Static Quantization）</p><p></p><p>3.3 探索 4-bit 量化的极限</p><p></p><p>3.3.1 GPTQ</p><p></p><p>3.3.2 GGUF</p><p></p><p>04 第 4 部分：Quantization Aware Training</p><p></p><p>4.1 1-bit LLM 的时代：BitNet</p><p></p><p>4.2 权重的量化 Weight Quantization</p><p></p><p>4.3 激活值的量化 Activation Quantization</p><p></p><p>4.4 反量化过程 Dequantization</p><p></p><p>4.5 所有 LLMs 实际上均为 1.58-bit</p><p></p><p>4.5.1 The Power of 0</p><p></p><p>4.5.2 Quantization 量化过程</p><p></p><p>05 Conclusion</p><p></p><p>Resources</p><p></p><p>文中链接🔗</p><p></p><p>顾名思义，大语言模型（Large Language Models，LLMs）的特点就是庞大，以至于普通的消费级硬件都难以承载。这些模型的参数量级可达数十亿，而且在进行推理时，往往需要依赖拥有大量显存（VRAM）的 GPU 来加快推理速度。</p><p></p><p>鉴于此，越来越多的研究者将目光投向如何通过优化训练方法、使用适配器（adapters）等技术来缩小模型体积。在这一领域，模型量化（quantization）技术成为了一个重要的研究方向。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5b/5bec304c38c40cbad59e1bde74b40389.png" /></p><p></p><p>本篇文章将带领大家深入了解语言模型领域的量化技术，并逐一探讨相关概念，帮助大家建立起对这一领域的直观认识。我们将一起探索不同的量化方法、实际应用场景，以及模型量化技术的基本原理。</p><p></p><p>本文将提供许多图表（visualizations）来帮助各位读者更好地理解和掌握模型量化技术这一概念，希望大家能够直观、深入地理解模型量化技术。</p><p></p><h1>01 第 1 部分：LLMs 存在的“问题”</h1><p></p><p>大语言模型之所以被称为“大”，是因为其参数数量十分之庞大。目前，这类模型的参数数量通常能够达到数十亿之巨（主要是指权重参数（weights）），这样的数据量其存储成本无疑是一笔巨大的开销。</p><p></p><p>在模型的推理过程中，激活值（译者注：activations，神经网络中某个层对输入数据应用激活函数后产生的输出值。）是通过输入数据（input）与模型权重（weights）相乘等一系列步骤来生成的，这些激活值的数据量也可能非常庞大。</p><p></p><p><img src="https://static001.geekbang.org/infoq/87/874c90d9ab6031252d37d67a6c96a99c.png" /></p><p></p><p>因此，我们的目标是找到一种尽可能高效的方式来表达数十亿个参数，以减少存储每个参数所需的空间。</p><p></p><p>在开始对这些参数进行优化之前，我们从最基本的部分入手，先探讨一下参数数值（value）在计算机中最初是如何表示的。</p><p></p><h2>1.1 参数数值（value）的表示方法</h2><p></p><p>在计算机科学中，特定的数值（value）通常都以浮点数的形式来表示，即带有正负号和小数点的数字。</p><p></p><p>这些数值是由 “bits” 组成的，也就是由二进制数字表示。根据 IEEE-754 标准[1]，这些 “bits” 可以用来表示三个不同的部分，从而构成一个完整的数值（value）：符号位、指数部分以及小数部分（也称为尾数）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/eac7a3488b10f273b59c0f8582d0d2fb.png" /></p><p></p><p>这三个部分结合起来，就能根据一组特定的 “bit” 值来计算出一个具体的数值（value）：</p><p></p><p><img src="https://static001.geekbang.org/infoq/89/8910b5c2c918c30a60a5e3f168f529b5.png" /></p><p></p><p>一般来说，用来表示数值（value）的 “bit” 越多，得到的数值（value）精确度就越高：</p><p></p><p><img src="https://static001.geekbang.org/infoq/0a/0a78c3a6c4125a0718f7ec579381c17d.png" /></p><p></p><h2>1.2 内存限制问题</h2><p></p><p>可用的 “bits” 数量越多，所能表示的数值范围就越大。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b6/b691578d8233df9a0b1240945498e402.png" /></p><p></p><p>一个特定的数值表示法能够表示的所有数值的区间被称为动态范围（dynamic range） ，而相邻两个数值之间的间隔则被称为精度（precision） 。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a2afc167e089cb51af4550eddaf9148e.png" /></p><p></p><p>使用这些 “bits” 的一个有趣功能是，我们可以计算出存储一个特定数值（value）所需的设备内存量。由于一个字节（byte）占 8 位（bits），我们可以为大多数浮点表示形式（floating point representation）制定一个基本的计算公式。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bd3bcd4f7e5d455d67c3008bc8bbf8c4.png" /></p><p></p><p></p><blockquote>Note：在实际应用中，模型推理阶段所需的显存（VRAM）量还受到诸多因素的影响，比如模型处理上下文的大小和模型架构设计。</blockquote><p></p><p></p><p>假设我们有一个拥有 700 亿参数的模型。通常情况下，这些模型默认使用 32 位浮点数（常称为全精度）进行表示，仅加载模型就需要 280GB 内存。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5f/5feb84da28fdf626a4d1142fda57782f.png" /></p><p></p><p>因此，尽可能地减少用于表示模型参数的 “bits” 数量（包括模型训练过程中也是如此）是非常有必要的。但是，有一点必须注意，精度的降低往往会导致模型准确性下降。</p><p></p><p>我们的目标是减少用于表示模型参数的 “bits” 数量，同时又不损害模型的准确性…… 这就是模型量化技术的作用所在！</p><p></p><h1>02 第 2 部分：模型量化技术简介</h1><p></p><p>模型量化的核心在于将模型参数的精度从较高的位宽（bit-widths）（例如 32 位浮点数）降低到较低的位宽（bit-widths）（例如 8 位整数）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2e/2e2942d6873011ce1e76a1d9312e11c5.png" /></p><p></p><p>在减少参数的 “bits” 数量时，通常会出现一定的精度损失（即丢失一些数值细节）。</p><p></p><p>为了更直观地说明这种影响，我们可以尝试将任意一张图片仅用 8 种颜色来表示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/3f/3f32efe21ee0931ff0c1f3bd816cc874.png" /></p><p></p><p>该图像基于 Slava Sidorov 的原作[2]进行了修改</p><p></p><p>观察放大区域，我们可以发现它比原始图片看起来更加“粗糙”，因为使用的颜色种类减少了。</p><p></p><p>模型量化的主要目的就是减少表示原始参数所需的 “bits” 数量（在上述案例中即为颜色种类），同时尽可能保留原始参数的精度。</p><p></p><h2>2.1 常用的数据类型</h2><p></p><p>首先，我们来看看一些常见的数据类型，以及它们与 32-bit（全精度（full-precision）或 FP32 ）表示法相比的影响。</p><p></p><h3>2.1.1 FP16</h3><p></p><p>以从 32-bit 转换到 16-bit（半精度或 FP16 ）的浮点数为例：</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fd8835bd8d53bdf7e7391e7bf931225e.png" /></p><p></p><p>可以看到，FP16 的数值范围比 FP32 要窄得多。</p><p></p><h3>2.1.2 BF16</h3><p></p><p>为了保持与原始 FP32 相似的数值范围，引入了 bfloat 16 这一数据类型，它类似于“截断版的FP32”：</p><p></p><p><img src="https://static001.geekbang.org/infoq/46/469cd166116fdca24bde632ede084922.png" /></p><p></p><p>BF16 虽然使用的 “bits” 数量与 FP16 相同，但能表示的数值范围更广，因此在深度学习领域内得到了广泛应用。</p><p></p><h3>2.1.3 INT8</h3><p></p><p>当我们需要再进一步减少 “bits” 的数量时，就到了整数表示法施展身手的领域，而不再是浮点数表示法。例如，从 FP32 转换为仅有 8 bits 的 INT8，其占用的 bits 数量仅仅是原来的四分之一：</p><p></p><p><img src="https://static001.geekbang.org/infoq/c1/c126c6b65515e139ec623c19b568e08c.png" /></p><p></p><p>有些硬件优化了整数运算，因此在这些硬件上整数运算可能会更高效。然而，并不是所有硬件都进行了这样的优化。不过，一般来说，使用较少的 “bits” 数量，计算速度通常会更快一些。</p><p></p><p>每减少一个 bits ，就需要进行一次映射（mapping）操作，将原本的 FP32 表示形式“压缩”到更少的 “bits” 数量。</p><p></p><p>在实际应用中，我们并不需要将 FP32 所表示的全部数值范围 [-3.4e38, 3.4e38] 都映射到 INT8。我们只需找到一种方法，将数据（即模型参数）范围映射到 INT8 即可。</p><p></p><p>常用的压缩（squeezing）和映射（mapping）方法包括对称量化（symmetric quantization）和非对称量化（asymmetric quantization），它们都是线性映射（linear mapping）的不同形式。</p><p></p><p>接下来，我们将探讨一下这些将 FP32 量化为 INT8 的方法。</p><p></p><h2>2.2 对称量化 Symmetric Quantization</h2><p></p><p>在对称量化过程中，原本浮点数的值域会被映射到量化空间（quantized space）中一个以零为中心的对称区间。从前面的例子可以看出，量化前后的值域都是围绕零点对称的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cd/cdd17a97e7873a0cf8e16883d9552697.png" /></p><p></p><p>这就意味着，在浮点数中表示零的值，在量化空间中仍然是正好为零。</p><p></p><p>对称量化（symmetric quantization）有一种经典方法是绝对最大值（absmax，absolute maximum）量化。</p><p></p><p>具体操作时，我们会从一组数值中找出最大的绝对值（α），以此作为线性映射的范围（译者注：从 -α 到 +α）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2d/2db8527e594edc4119a16580463ac83a.png" /></p><p></p><p></p><blockquote>Note：值域 [-127, 127] 代表的是受限制🚫的范围，而 8-bit 整数可以表示的完整范围是[-128, 127]，选择哪种范围取决于所采用的量化方法。</blockquote><p></p><p></p><p>由于这是一种以零为中心的线性映射（linear mapping），所以计算公式相对简单。</p><p></p><p>我们首先根据以下公式计算比例因子（s）：</p><p></p><p>b 是我们想要量化到的字节数（译者注：原文为“Byte”，此处保留原义，译为字节数，译者认为可能为 bits 数量）（这里是 8 ），α 是最大绝对值，</p><p></p><p>接着，我们用这个比例因子 s 来量化输入值 x：</p><p></p><p><img src="https://static001.geekbang.org/infoq/24/240961e13b3e5728e990d753f5c9d7cb.png" /></p><p></p><p>将这些数值代入公式后，我们将得到以下结果：</p><p></p><p><img src="https://static001.geekbang.org/infoq/7a/7a7c60a541213997ad4a360470f488aa.png" /></p><p></p><p>为了恢复原始的 FP32 值，我们可以使用之前计算出的比例因子（s）来对量化后的数值进行反量化（dequantize）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d3/d3d07f698f22b5a221408135478f9017.png" /></p><p></p><p>先量化后再反量化以恢复原始值的过程如下所示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/34/34815a8bd34226bf3f7e97c56c1c7667.png" /></p><p></p><p>我们可以观察到，某些值（如 3.08 和 3.02 ）在量化到 INT8 后，都被分配了相同的值 36。当这些值反量化（dequantize）回 FP32 时，会丢失一些精度，变得无法再区分。</p><p></p><p>这种现象通常被称为量化误差（quantization error） ，我们可以通过比较原始值（original values）和反量化值（dequantized values）之间的差值来计算这个误差。</p><p></p><p><img src="https://static001.geekbang.org/infoq/82/82677db0532256d3058fb219f8be2d6b.png" /></p><p></p><p>一般来说，“bits” 的数量越少，量化误差往往越大。</p><p></p><h2>2.3 非对称量化 asymmetric quantization</h2><p></p><p>与对称量化（symmetric around）不同，非对称量化并不是以零为中心对称的。 它将浮点数范围中的最小值（β）和最大值（α）映射到量化范围（quantized range）的最小值和最大值。</p><p></p><p>我们在此要探讨的方法称为零点量化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/46/464b86629735821898e9f1be4cc30d5c.png" /></p><p></p><p>各位注意到 0 的位置是如何移动的吗？这正是它被称为“非对称量化”的原因。在区间 [-7.59, 10.8] 中，最小值和最大值与零点之间的距离是不相等的。</p><p></p><p>由于零点位置的偏移，我们需要计算 INT8 范围的零点来进行线性映射（linear mapping）。与之前一样，我们还需要计算一个比例因子（s），但这次要使用 INT8 范围（ [-128, 127] ）的两个端点之间的差值。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c6/c6e743c31b1a53698b7ad26cd8782a1a.png" /></p><p></p><p>请注意，由于需要计算 INT8 取值范围中的零点（z）来调整权重，这个过程稍微复杂一些。</p><p></p><p>和之前一样填入公式：</p><p></p><p><img src="https://static001.geekbang.org/infoq/92/92e3f6c54fc905c152d03c71ee1af1c7.png" /></p><p></p><p>要将从 INT8 量化后的数值反量化回 FP32 ，需要使用之前计算的比例因子（s）和零点（z）。</p><p></p><p>除此之外，反量化过程则相对比较直接：</p><p></p><p><img src="https://static001.geekbang.org/infoq/95/95802965a73d91442d99da714f149ba9.png" /></p><p></p><p>当我们将对称量化和非对称量化放在一起对比时，我们可以迅速看出这两种方法之间的差异：</p><p></p><p><img src="https://static001.geekbang.org/infoq/8e/8eae78513534b90caa4d92b0c5080af4.png" /></p><p></p><p></p><blockquote>Note：请注意对称量化（symmetric quantization）以零点为中心的特性，以及非对称量化（asymmetric quantization）存在的零点偏移。</blockquote><p></p><p></p><h2>2.4 取值范围的映射与剪裁</h2><p></p><p>在前文所举的例子中，我们研究了如何将向量中的数值映射到更低的位表示形式（lower-bit representation）中。虽然这样使得向量的全范围都能被映射，但有一个明显的缺点，那就是有离群值（outlier）时不太好处理。</p><p></p><p>假设有一个向量，其值如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/2f/2ff9cbba82e0666a5db066e5ce6af23f.png" /></p><p></p><p>请注意，如果其中一个数值（value）远大于其他所有数值，该数值就可以被视作离群值（outlier）。 如果我们要映射这个向量的全部数值，那么所有较小的数值都将映射到相同的较低位表示，并因此失去它们的独特特性：</p><p></p><p><img src="https://static001.geekbang.org/infoq/08/0857ab4220800c53bd863aaf7f59b1c1.png" /></p><p></p><p>这就是我们之前使用的 absmax 方法。请注意，如果我们不进行剪裁（clipping），非对称量化也会出现这样的问题。</p><p></p><p>另一种选择是裁剪（clip）掉某些数值。裁剪（Clipping）操作会为原始值设定一个不同的动态范围，这样所有离群值都会被映射到相同的值。</p><p></p><p>在下文给出的案例中，如果我们手动将动态范围设置为 [-5, 5] ，所有超出这个范围的数值无论其原始值是多少，都将被映射为 -127 或 127 ：</p><p></p><p><img src="https://static001.geekbang.org/infoq/52/52bf417d28b12186a1fbab60e4c38ecb.png" /></p><p></p><p>这种方法的主要优点是，显著减少了非离群值的量化误差。然而，离群值的量化误差却增加了。</p><p></p><h2>2.5 校准过程 Calibration</h2><p></p><p>在前文的示例中，我展示了一种简单方法 —— 即任意选择一个取值范围 [-5, 5]。这个过程被称为校准（calibration），其目的是找到一个能够包含尽可能多数值（values）的范围，同时尽量减少量化误差（quantization error）。</p><p></p><p>对于不同类型的参数，执行校准步骤的方法并不相同。</p><p></p><h3>2.5.1 权重（和偏置项） Weights (and Biases)</h3><p></p><p>在 LLMs 中，我们可以将权重（weights）和偏置项（Biases）视为预先确定的静态值，因为这些值在运行模型之前就已经确定了。例如，Llama 3 的约 20 GB 文件[3]中大部分都是其权重和偏置项。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c1/c1578dc2cc2c80e84de0f893123c1c05.png" /></p><p></p><p>由于偏置项的数量（以百万计）远少于权重（以数十亿计），偏置项通常被保留在更高的精度（如 INT16 ），而量化的主要工作则集中在权重的处理上。</p><p></p><p>因为权重是静态且已知的，所以对其的量化技术可以有：</p><p></p><p>手动选择输入范围的百分位数优化原始权重和量化权重之间的均方误差（MSE）最小化原始值和量化值之间的熵（KL 散度）</p><p></p><p><img src="https://static001.geekbang.org/infoq/79/79f2fb01e27903e852897f738bd17d70.png" /></p><p></p><p>例如，第一种方法（手动选择输入范围的百分位数）会导致出现与前文我们看到的相似的裁剪（clipping）行为。</p><p></p><h3>2.5.2 激活值</h3><p></p><p>在 LLMs 中， 那些在整个推理过程中持续更新的输入（input）通常被称为“激活值”（activations）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1f/1fb614e36b50e5312f25dbe2e3ba5573.png" /></p><p></p><p>请注意，这些值之所以被称为激活值，是因为它们经常需要经过某些激活函数处理，比如 sigmoid 或 relu。</p><p></p><p>与权重不同，激活值会随着每次输入数据的改变而变化，因此很难对其进行精确量化。</p><p></p><p>由于这些值在每个隐藏层之后都会更新，因此我们只能在输入数据通过模型时才能预测它们在推理过程中的具体数值。</p><p></p><p><img src="https://static001.geekbang.org/infoq/24/24fa106202ae3f693b209d3d55e4e4a6.png" /></p><p></p><p>一般来说，校准权重和激活值的量化方法主要有两种：</p><p></p><p>Post-Training Quantization（PTQ）  — 训练完成后进行量化Quantization Aware Training（QAT）  — 训练/微调过程中同时进行量化</p><p></p><h1>03 第 3 部分：Post-Training Quantization</h1><p></p><p>在众多量化技术中，post-training quantization（PTQ）是最为流行的一种。这种方法是在训练完模型之后对模型的参数（包括权重和激活值）进行量化。</p><p></p><p>对于权重值的量化可以采用对称量化（symmetric quantization） 或非对称量化（asymmetric quantization） 两种方式。</p><p></p><p>至于激活值，由于我们不知道其范围，因此需要通过模型的推理来获取它们的 potential distribution（译者注：指的是在不同的输入数据和模型参数下，激活值可能出现的一系列数值。了解这个分布有助于我们选择一个能够包含大部分激活值范围的量化级别，从而减少量化误差。），然后再进行量化。</p><p></p><p>激活值的量化主要有两种形式：</p><p></p><p>动态量化（Dynamic Quantization）静态量化（Static Quantization）</p><p></p><h2>3.1 动态量化（Dynamic Quantization）</h2><p></p><p>当数据通过隐藏层时，其激活值会被收集起来：</p><p></p><p><img src="https://static001.geekbang.org/infoq/3c/3c275b1ab05e53f145786aefaf994e04.png" /></p><p></p><p>随后，利用这些激活值的分布（distribution of activations）来计算量化输出值所需的零点（z）和比例因子（s）值：</p><p></p><p><img src="https://static001.geekbang.org/infoq/0e/0ee7b2af280ecf602e42f70d2201f749.png" /></p><p></p><p>每次数据通过一个新模型层时，都要重复上述过程。因此，每个模型层都有其独特的 z 值和 s 值，因此也有不同的量化方案。</p><p></p><h2>3.2 静态量化（Static Quantization）</h2><p></p><p>与动态量化不同，静态量化在模型推理过程中不实时计算零点（z）和比例因子（s），而是在模型训练或校准过程中提前计算。</p><p></p><p>为了找到这些值，会使用一个校准数据集，并让模型处理这些数据，以便收集可能的激活值分布（potential distributions）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/52/522fd381b0daca69636a4bb0af5a2468.png" /></p><p></p><p>收集到这些数值后，我们就可以计算出必要的 s 值和 z 值，以便在推理过程中进行量化。</p><p></p><p>在实际推理过程中，s 值和 z 值不需要重新计算，而是被应用于所有激活值，实现全局量化。</p><p></p><p>通常情况下，动态量化技术可能会稍微更精确一些，因为它为每个隐藏层计算一次 s 值和 z 值。不过，由于需要计算这些值，因此可能会增加计算时间。</p><p></p><p>相比之下，静态量化虽然准确度稍低，但由于事先已知用于量化的 s 值和 z 值，因此在推理时更为高效。</p><p></p><h2>3.3 探索 4-bit 量化的极限</h2><p></p><p>将量化位数降至 8-bit 以下是一项艰巨的任务，因为每减少一个 bit，量化误差（quantization error）就会增加。 幸运的是，有几种巧妙的方法可以将量化位数进一步降低到 6-bit、4-bit，甚至 2-bit （不过不建议低于 4-bit ）。</p><p></p><p>接下来将探讨两种在 HuggingFace** 上常用的方法：</p><p></p><p>GPTQ — 全模型在 GPU 上运行。GGUF — 将一部分模型层从 GPU 转移到 CPU 上执行。</p><p></p><h3>3.3.1 GPTQ</h3><p></p><p>GPTQ 无疑是实际应用中最著名的 4-bits 量化方法之一。1</p><p></p><p>它采用非对称量化（asymmetric quantization），并逐层处理，每一层都经过独立处理，然后再继续处理下一层：</p><p></p><p><img src="https://static001.geekbang.org/infoq/89/89b382ce26ee082118e6beeac9250c92.png" /></p><p></p><p>在这个逐层量化的过程中，首先将模型层的权重转换为 Hessian 矩阵（译者注：Hessian 矩阵是二阶偏导数矩阵，用于描述函数在其输入变量上的局部曲率。对于多变量函数，Hessian 矩阵可以帮助我们了解函数在某一点上的凹凸性，以及函数值对输入变量的变化有多敏感。）的逆矩阵。它是模型损失函数的二阶导数，它告诉我们模型输出对每个权重变化的敏感程度。</p><p></p><p>简单来说，该过程展示了模型层中每个权重的重要性（或者说是权重的影响程度）。</p><p></p><p>与 Hessian 矩阵中较小值相关的权重更为重要，因为这些权重的微小变化可能会对模型的性能产生重大影响。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8c/8c2dd2e30c6bb6ac0daf7cb5df10c66f.png" /></p><p></p><p>在 Hessian 矩阵的逆矩阵中，数值越低，权重越 “重要”。</p><p></p><p>接下来，我们对权重矩阵的第一行权重进行量化，再进行反量化：</p><p></p><p><img src="https://static001.geekbang.org/infoq/4a/4a0c400c3c448b95ebd8101d7c06e6c0.png" /></p><p></p><p>通过这一过程，我们可以计算出量化误差 (q)，我们可以用之前计算的 Hessian 矩阵的逆矩阵（h_1）来调整这个误差。</p><p></p><p>换句话说，我们是在根据权重的重要性来构建加权量化误差（weighted-quantization error）：</p><p></p><p><img src="https://static001.geekbang.org/infoq/53/53a34232861680a798cf0c791da8838e.png" /></p><p></p><p>接着，我们将这个加权的量化误差重新分配到该行的其他权重上。这样做可以保持神经网络的整体功能（overall function）和输出（output）不变。</p><p></p><p>例如，如果要对第二个权重（如果它是 0.3（x_2））进行此操作，我们就会将量化误差（q）乘以第二个权重的 Hessian 矩阵的逆矩阵（h_2）加上去。</p><p></p><p><img src="https://static001.geekbang.org/infoq/91/91f9678765a1f1107d700f3b1e42d5ac.png" /></p><p></p><p>我们可以对第一行中的第三个权重进行同样的处理：</p><p></p><p><img src="https://static001.geekbang.org/infoq/15/15e2b1bb9656ce813a1a071b6f7d9f97.png" /></p><p></p><p>重复这个重新分配加权量化误差的过程，直到所有值都被量化。</p><p></p><p>这种方法之所以行之有效，是因为权重之间通常是相互关联的。因此，当一个权重出现量化误差（quantization error）时，与之相关的权重也会相应地更新（通过 Hessian 矩阵的逆矩阵）。</p><p></p><p></p><blockquote>NOTE：本文作者[4]采用了几种技巧来加快计算速度并提高性能，例如在 Hessian 矩阵中添加阻尼因子（dampening factor）、“懒惰批处理（lazy batching）”，以及使用 Cholesky 方法预先计算信息（precomputing information）。我强烈建议各位读者观看这个视频[5]。</blockquote><p></p><p></p><p></p><blockquote>TIP：如果你想要一种可以优化性能和提高推理速度的量化方法，可以查看 EXL2[6] 这个项目。</blockquote><p></p><p></p><h3>3.3.2 GGUF</h3><p></p><p>虽然 GPTQ 是一种在 GPU 上运行完整 LLMs 的最佳模型量化方法，但我们可能很多时候没有这种条件。于是我们可以使用 GGUF 将 LLM 的某些模型层放到到 CPU 上进行处理。2</p><p></p><p>这样，当 VRAM 不足时，就可以同时使用 CPU 和 GPU。</p><p></p><p>量化方法 GGUF 仍不断在更新，并且其性能可能会根据量化位数的不同而有所变化。其基本原理如下：</p><p></p><p>首先，给定模型层的权重被分割成包含一组“子”块的“超级”块（“super” blocks）。</p><p></p><p>我们从这些 blocks 中提取比例因子（s）和 α（α）：</p><p></p><p><img src="https://static001.geekbang.org/infoq/12/1246fede5eb200f62a671dcb1c9d0d01.png" /></p><p></p><p>为了量化给定的“子”块（“sub” block），我们可以使用之前介绍的 absmax 量化方法。这种方法会将给定权重乘以比例因子（s）：</p><p></p><p><img src="https://static001.geekbang.org/infoq/12/12068be791d838e0e804b2052beee053.png" /></p><p></p><p>比例因子是通过“子”块的信息计算出来的，但量化时使用的是“超级”块的信息，后者有自己的比例因子：</p><p></p><p><img src="https://static001.geekbang.org/infoq/4f/4ffb580e47ff3f0e077b5817f169bd7b.png" /></p><p></p><p>这种基于块（blocks）的量化方法使用“超级”块的比例因子（s_super）来量化“子”块的比例因子（s_sub）。</p><p></p><p>每个比例因子的量化级别可能会有所不同，“超级”块的比例因子通常比“子”块的比例因子有更高的精度。</p><p></p><p>为了更直观地理解，观看下图进一步了解这几个量化级别相关信息（ 2-bit、4-bit 和 6-bit ）：</p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ec74f774e44d8550c13fef3072b08a5d.png" /></p><p></p><p></p><blockquote>NOTE：在某些量化方法中，为了保持量化后的模型性能，可能需要一个额外的最小值来调整零点，以确保模型能够正确处理极端值。这个最小值和比例因子一样，都是量化过程中的关键参数，它们需要被正确地量化，以确保量化后的模型能够保持原有的性能。</blockquote><p></p><p></p><p>各位读者可以查看这个 PR[7] ，了解所有量化级别的详细信息。此外，还可以查看这个 PR[8]，获取更多关于使用重要性矩阵（importance matrices）进行量化的信息。</p><p></p><h1>04 第 4 部分：Quantization Aware Training</h1><p></p><p>在第 3 部分中，我们了解到如何在训练完成后对模型进行量化。这种方法的不足之处在于，量化过程并未考虑到实际的训练过程。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a4/a4cd3392c6c382dd611bb49b8742135e.png" /></p><p></p><p>于是 Quantization Aware Training（QAT）就有了用武之地。与训练后使用 post-training quantization（PTQ）技术对模型进行量化不同，QAT 的目标是在训练过程中学习量化过程。</p><p></p><p>QAT 通常比 PTQ 更准确，因为在训练过程中已经考虑了量化。其工作原理如下：</p><p></p><p>在训练过程中，引入所谓的“伪”量化。比如先将权重量化到例如 INT4 等形式，然后将它们反量化回 FP32 ：</p><p></p><p><img src="https://static001.geekbang.org/infoq/d7/d7edb3a5a342b5c3a4f32b501b7d9887.png" /></p><p></p><p>这一过程使得模型在训练阶段进行损失值计算和权重更新时能够考虑到量化误差。</p><p></p><p>QAT 尝试探索损失函数中的“宽”最小值区域，以尽可能减少量化误差，因为“窄”最小值区域往往会导致更大的量化误差。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b18915a9133feb2df4b53cbb1b5b738a.png" /></p><p></p><p>例如，假设我们在反向传播过程（backward pass）中没有考虑量化误差。我们将根据梯度下降法（gradient descent）选择损失值（loss）最小的权重。但是，如果它位于“窄”最小值区域，可能会引入更大的量化误差。</p><p></p><p>相反，如果我们考虑到量化误差，我们将选择在“宽”最小值区域中的不同权重进行更新，量化误差会小得多。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9b9626cf089b44f7f5cf02f111c4d194.png" /></p><p></p><p>因此，虽然 PTQ 在高精度（例如，FP32）下具有较小的损失值，但 QAT 在低精度（例如， INT4 ）下的损失值较小，这正是我们追求的目标。</p><p></p><h2>4.1 1-bit LLM 的时代：BitNet</h2><p></p><p>正如前文所述，将量化位数降低到 4-bit 已经非常小了，但如果我们还要进一步降低呢？</p><p></p><p>这就是 BitNet[9] 的用武之地了，它使用 1-bit 表示模型的权重，每个权重都使用 -1 或 1 表示。3</p><p></p><p>它通过直接将量化过程整合到 Transformer 架构中来实现这一点。</p><p></p><p>Transformer 架构是大多数 LLMs 的基础，它依赖于线性层来处理序列数据，并在模型中执行关键的计算操作：</p><p></p><p><img src="https://static001.geekbang.org/infoq/6f/6f867c6a46703c68e9e06dd6fa9b95d3.png" /></p><p></p><p>这些线性层（linear layers）通常使用更高的精度，如 FP16，它们也是大部分权重所在的地方。</p><p></p><p>BitNet 将这些线性层替换为他们称之为 BitLinear 的模型层：</p><p></p><p><img src="https://static001.geekbang.org/infoq/55/557e9606923e81e470a5df4632cb7947.png" /></p><p></p><p>BitLinear 层的工作原理与普通线性层相同，根据权重（weights）和激活值（activation）的乘积计算输出值（output）。</p><p></p><p>BitLinear 层使用 1-bit 来表示模型的权重，并使用 INT8 来表示激活值：</p><p></p><p><img src="https://static001.geekbang.org/infoq/91/91df839add366efd6e3e2651ffe14330.png" /></p><p></p><p>类似于 Quantization-Aware Training（QAT）技术，BitLinear 层在训练过程中执行一种 “伪” 量化，以便用来分析权重和激活值的量化效果：</p><p></p><p><img src="https://static001.geekbang.org/infoq/10/1007ae8c8f69d93cea8fbd87155b1204.png" /></p><p></p><p></p><blockquote>NOTE：在论文中使用的是 γ 而不是 α ，但由于在本文中所举的例子一直使用 α ，所以我使用 α 。此外，请注意此处的 β 与前文在零点量化（zero-point quantization）中使用的 β 不同，它是基于平均绝对值（average absolute value）计算得出的。</blockquote><p></p><p></p><p>让我们一步一步来学习 BitLinear 。</p><p></p><h2>4.2 权重的量化 Weight Quantization</h2><p></p><p>在训练过程中，权重以 INT8 的形式存储，然后使用一种称为 signum 函数的基本策略，将其量化到 1-bit。</p><p></p><p>这种方法的核心在于，它将权重分布（distribution of weights）重新调整到以 0 为中心，然后将所有小于 0 的值（左侧）设置为 -1 ，将所有大于 0 的值（右侧）设置为 1 ：</p><p></p><p><img src="https://static001.geekbang.org/infoq/a5/a572f0074ff0f4f07f9f2cb8aa4c24f2.png" /></p><p></p><p>此外，它还会跟踪记录一个值 β（平均绝对值（average absolute value）），我们稍后会用到它来进行反量化（dequantization）。</p><p></p><h2>4.3 激活值的量化 Activation Quantization</h2><p></p><p>为了量化激活值，BitLinear 利用 absmax 量化方法将 FP16 格式的激活值转换为 INT8 格式，因为矩阵乘法 (×) 需要更高精度的激活值。</p><p></p><p><img src="https://static001.geekbang.org/infoq/75/759b252e32eeb8e9c45cd165cf5e3a67.png" /></p><p></p><p>同时，它还会跟踪记录 α（最高绝对值），我们将在后续的反量化过程中使用该值。</p><p></p><h2>4.4 反量化过程 Dequantization</h2><p></p><p>我们跟踪记录了 α（激活值的最高绝对值）和 β（权重的平均绝对值），因为这些值将在后续的反量化过程中帮助我们把激活值从 INT8 格式恢复到 FP16 格式。</p><p></p><p>输出激活值（output activations）通过 {α, γ} 进行缩放，然后进行反量化将其恢复到原始精度：</p><p></p><p><img src="https://static001.geekbang.org/infoq/26/26616211bcaeca9ae19b5375d0832b6c.png" /></p><p></p><p>就是这样！这个过程相对简单，只需用两个值（-1 或 1）来表示模型。</p><p></p><p>根据这一流程，作者发现随着模型规模的扩大，1-bit 形式和 FP16 形式训练的模型之间的性能差异逐渐缩小。</p><p></p><p>不过，这只适用于较大型的模型（参数超过 300 亿（30 B）），而对于较小型的模型，这个性能差距仍然很大。</p><p></p><h2>4.5 所有 LLMs 实际上均为 1.58-bit</h2><p></p><p>BitNet 1.58b[10] 就是为了解决之前提到的扩展性问题而提出的。4</p><p></p><p>在这种新方法中，模型的每一个权重不仅可以是 -1 或 1 ，还可以取 0 ，从而成为了一个三元模型。有趣的是，仅仅添加了 0 这一可取值就极大地提升了 BitNet 的性能，并使得计算速度大大提升。</p><p></p><h3>4.5.1 The Power of 0</h3><p></p><p>那么，为什么就添加了一个可取值 0 就能带来如此大的提升呢？</p><p></p><p>这与矩阵乘法的原理紧密相关！</p><p></p><p>首先，让我们了解一下矩阵乘法的一般工作原理。在计算输出值时，我们将权重矩阵（weight matrix）与输入向量（input vector）相乘。下图展示了权重矩阵第一层与输入向量相乘的过程：</p><p></p><p><img src="https://static001.geekbang.org/infoq/81/81c5cbcfe6e18476ba32cf775c3c103a.png" /></p><p></p><p>请注意，这一过程包含两个步骤：首先将每个权重与输入值相乘，然后将所有乘积相加。</p><p></p><p>与此不同，BitNet 1.58b 则省略了乘法这一步骤，因为三元权重（ternary weights）实际上传达了这样的信息：</p><p></p><p>1: 我想要加上这个值0: 我不需要加上这个值-1: 我想要减去这个值</p><p></p><p>因此，当权重量化到 1.58 bit 时，只需要执行加法运算：</p><p></p><p><img src="https://static001.geekbang.org/infoq/55/554c56dfeb18190e341e0bab7fb44995.png" /></p><p></p><p>这样不仅可以大大加快了计算速度，还可以进行特征过滤（feature filtering）。</p><p></p><p>将某个权重设置为 0 后，我们就可以选择忽略它，而不是像 1-bit 表示法那样要么加上要么减去权重。</p><p></p><h3>4.5.2 Quantization 量化过程</h3><p></p><p>在 BitNet 1.58b 中，进行权重量化（weight quantization）时采用了 absmean 量化方法，这是之前看到的 absmax 量化方法的一种改进形式。</p><p></p><p>这种方法通过压缩权重的分布，并利用权重的绝对平均值（α）来进行数值（value）的量化。之后，这些数值会被归整到 -1、0 或 1 ：</p><p></p><p><img src="https://static001.geekbang.org/infoq/7a/7ad4ab53c81e5ba650d48e2384ae1472.png" /></p><p></p><p>相较于 BitNet，激活值的量化过程基本相同，但还是有一点不同。激活值不再被缩放到 [0, 2ᵇ⁻¹] 区间，而是通过 absmax 量化方法被调整到了 [-2ᵇ⁻¹, 2ᵇ⁻¹] 区间。</p><p></p><p>就是这样！1.58-bit 量化主要需要两种技巧：</p><p></p><p>通过添加可取值 0 构建三元数值表示法 [-1, 0, 1]对权重实施 absmean 量化方法。</p><p></p><p>“13B BitNet b1.58 在响应延迟、内存占用和能耗方面，相较于 3B FP16 LLM 更高效。”</p><p></p><p>由于仅需 1.58 个 bits ，计算效率高，我们得以构建出更为轻量的模型！</p><p></p><h1>05 Conclusion</h1><p></p><p>我们的量化之旅到此告一段落！但愿本文能帮助你更深入地认识到量化技术、GPTQ、GGUF 以及 BitNet 的巨大潜力。未来模型的体积又将能够缩小到何种程度？真是令人期待啊！</p><p></p><p>如果要查看更多与 LLMs 相关的可视化内容，并希望支持我们，不妨留意一下我和 Jay Alammar 正在编写的新书。该书即将发行！</p><p></p><p><img src="https://static001.geekbang.org/infoq/00/00fe6928ffcd52e2a95b114cb38e1050.png" /></p><p></p><p>你可以在 O’Reilly 网站[11]上免费试读此书，或者直接在亚马逊[12]上预订。我们还会将所有相关代码同步更新到 Github[13] 上。</p><p></p><h1>Resources</h1><p></p><p>Hopefully, this was an accessible introduction to quantization! If you want to go deeper, I would suggest the following resources:</p><p></p><p>A HuggingFace blog about the LLM.int8()[14] quantization method: you can find the paper here[15]. （译者注：LLM.int8() 量化方法）Another great HuggingFace blog about quantization for embeddings[16].（译者注：嵌入向量的量化问题）A blog about Transformer Math 101[17], describing the basic math related to computation and memory usage for transformers.（译者注：介绍了与 Transformer 的计算和内存使用相关的基本概念）This[18] and this are two nice resources to calculate the (V)RAM you need for a given model.（译者注：计算特定模型所需（V）RAM 的数量）If you want to know more about QLoRA5, a quantization technique for fine-tuning, it is covered extensively in my upcoming book: Hands-On Large Language Models[19].（译者注：QLoRA 技术的学习资料）A truly amazing YouTube video[20] about GPTQ explained incredibly intuitively.（译者注：GPTQ 技术的学习资料）</p><p></p><p>脚注：</p><p></p><p>Frantar, Elias, et al. "Gptq: Accurate post-training quantization for generative pre-trained transformers." arXiv preprint arXiv:2210.17323 (2022).You can find more about GGUF on their GGML repository here[21].Wang, Hongyu, et al. "Bitnet: Scaling 1-bit transformers for large language models." arXiv preprint arXiv:2310.11453 (2023).Ma, Shuming, et al. "The era of 1-bit llms: All large language models are in 1.58 bits." arXiv preprint arXiv:2402.17764 (2024).Dettmers, Tim, et al. "Qlora: Efficient finetuning of quantized llms." Advances in Neural Information Processing Systems 36 (2024).</p><p></p><p>Thanks for reading!</p><p></p><p>Hope you have enjoyed and learned new things from this blog!</p><p></p><p>Maarten Grootendorst</p><p></p><p>Data Scientist | Psychologist | Writer | Open Source Developer (BERTopic, PolyFuzz, KeyBERT) | At the intersection of Artificial Intelligence and Psychology</p><p></p><p>END</p><p></p><h1>🔗文中链接🔗</h1><p></p><p>[1]https://en.wikipedia.org/wiki/IEEE_754</p><p></p><p>[2]https://pixabay.com/users/slava_web-designer-39623293/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=8668140</p><p></p><p>[3]https://huggingface.co/meta-llama/Meta-Llama-3-8B/tree/main</p><p></p><p>[4]https://arxiv.org/pdf/2210.17323</p><p></p><p>[5]https://www.youtube.com/watch?v=mii-xFaPCrA</p><p></p><p>[6]https://github.com/turboderp/exllamav2</p><p></p><p>[7]https://github.com/ggerganov/llama.cpp/pull/1684</p><p></p><p>[8]https://github.com/ggerganov/llama.cpp/pull/4861</p><p></p><p>[9]https://arxiv.org/pdf/2310.11453</p><p></p><p>[10]https://arxiv.org/pdf/2402.17764</p><p></p><p>[11]https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/</p><p></p><p>[12]https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961</p><p></p><p>[13]https://github.com/HandsOnLLM/Hands-On-Large-Language-Models</p><p></p><p>[14]https://huggingface.co/blog/hf-bitsandbytes-integration</p><p></p><p>[15]https://arxiv.org/pdf/2208.07339</p><p></p><p>[16]https://huggingface.co/blog/embedding-quantization</p><p></p><p>[17]https://blog.eleuther.ai/transformer-math/</p><p></p><p>[18]https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator</p><p></p><p>[19]https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961</p><p></p><p>[20]https://www.youtube.com/watch?v=mii-xFaPCrA</p><p></p><p>[21]https://github.com/ggerganov/ggml/blob/master/docs/gguf.md</p><p></p><p>本文经原作者授权，由 Baihai IDP 编译。如需转载译文，请联系获取授权。</p><p></p><p>原文链接：</p><p></p><p>https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/UNmjGDyGK5XBLIAEt7Ui</id>
            <title>揭秘谷歌搜索排名的工作原理</title>
            <link>https://www.infoq.cn/article/UNmjGDyGK5XBLIAEt7Ui</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/UNmjGDyGK5XBLIAEt7Ui</guid>
            <pubDate></pubDate>
            <updated>Thu, 22 Aug 2024 02:29:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>从现有的资料来看，谷歌文档泄露事件与反垄断听证会公开的谷歌搜索排名文件并未直接揭开谷歌搜索排名的全部运作细节。</p><p></p><p>随着机器学习技术的深入应用，有机搜索结果背后的机制变得极其复杂，即便是谷歌内部负责排名算法的专业人士，也难以精确阐述为何某个特定结果会位居榜首或次席。我们尚不清楚这些众多影响因素的具体权重及它们之间错综复杂的相互作用关系。</p><p></p><p>然而，深入理解搜索引擎的整体架构仍然至关重要。这不仅能帮助我们理解为何某些精心优化的网页未能获得高位排名，还能揭示为何一些看似简单且未经刻意优化的结果却能脱颖而出。更为关键的是，这促使我们拓宽视野，重新审视并识别出真正影响排名的核心要素。</p><p></p><p>所有已披露的信息均指向这一点。对于任何关注搜索引擎优化（SEO）的人来说，都应将这些新发现融入自己的思考框架中。这将促使我们以全新的视角审视自己的网站，并在分析、规划与决策过程中引入更多维度的考量标准。</p><p></p><p>坦诚而言，要精确勾勒出这些复杂系统的全貌实属不易。网络上关于此类信息的解读往往存在分歧，即便是讨论同一主题，所用术语也可能大相径庭。</p><p></p><p>举个例子，负责优化搜索结果页面（SERP）布局的系统，在某些谷歌文档中被称为 “Tangram”，而在其他文档中则换上了 “Tetris” 这一名称，这或许是对那款经典游戏的巧妙借喻。</p><p></p><p>经过数周的深入研究，我反复查阅、分析、整理、筛选并重组了近百份相关文档。本文虽非尽善尽美或绝对权威，但确系我基于现有知识与理解，以类似侦探福尔摩斯般的细致精神，竭尽所能完成的成果。呈现在你面前的，便是我个人视角下的探索总结。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/43/43793919af8d3782159dc1e8c537fd44.jpg" /></p><p></p><p>作者创作的谷歌排名工作原理的图解概览</p><p></p><p></p><h3>一份新文档等待谷歌爬虫访问</h3><p></p><p></p><p>当你发布一个新网站时，它并不会立即被谷歌索引。谷歌需要首先发现这个网站的 URL，这通常是通过更新站点地图或是由一个已知 URL 上的链接引导来实现的。</p><p></p><p>对于像首页这样频繁被访问的页面，它们往往会更快地将新链接的信息传递给谷歌。</p><p></p><p>谷歌的 Trawler 系统负责抓取新内容，并跟踪何时重新访问这些 URL 以检查是否有更新。这一过程由调度器精心管理，而存储服务器则负责决定是转发这些 URL 供进一步处理，还是将它们暂时放置在所谓的 “沙盒” 中。尽管谷歌官方否认了沙盒机制的存在，但最近的泄露信息却暗示，那些被怀疑为垃圾或低质量的网站确实有可能被置于这样的环境中进行观察。值得注意的是，谷歌似乎还会转发一些垃圾内容，这可能是为了深入分析，以进一步优化其算法。</p><p></p><p>假设某个文档成功通过了这一系列筛选，那么文档中的外部链接将被提取出来，并被分类为内部链接或外部链接。这些链接信息随后会被其他系统用于进行链接分析和 PageRank 计算（关于这一点，我们稍后会详细阐述）。</p><p></p><p>而对于指向图像的链接，它们则会被专门转发给 ImageBot 进行处理。这个过程有时可能会遇到显著的延迟。ImageBot 会调用这些链接，并将图像与相同或相似的图像一起存储在图像数据库中。此外，Trawler 还会根据它自己的 PageRank 评估结果来调整对网站的抓取频率。简单来说，如果一个网站的访问量较大，那么 Trawler 对它的抓取频率也会相应提高，这被称为 ClientTrafficFraction（客户端流量比例）的影响。</p><p></p><h3>Alexandria：伟大的索引库</h3><p></p><p></p><p>谷歌的索引系统名为 Alexandria，它巧妙地为每一份内容分配一个独一无二的 DocID。若内容已存在于系统中，比如在处理重复内容时，系统不会生成新的 ID，而是会将新发现的 URL 与已存在的 DocID 相关联，实现内容的统一管理和识别。</p><p></p><p>值得注意的是，谷歌严格区分 URL 与文档的概念。一个文档可以涵盖多个 URL，这些 URL 虽然指向不同位置或包含细微差异（如不同语言版本的页面），但只要它们的内容相似且被正确标记，就会被视为同一文档的不同表现形式。同时，来自其他域的 URL 也会在这一体系下被合理分类。所有这些 URL 所携带的信息和信号，都会通过它们所关联的同一个 DocID 来整合处理，确保内容的一致性和准确性。</p><p></p><p>在处理重复内容时，谷歌会精心挑选一个规范版本作为搜索结果的主要展示对象。这也解释了为什么我们有时会看到多个 URL 在搜索结果中排名相近 —— 它们实际上都指向了同一个文档的不同入口。而 “原始”（即规范）URL 的确定并非一成不变，它可能会随着谷歌算法的更新和内容的演变而有所调整。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/30/302a5ea9487c77d751a9ceae87b4f41a.jpg" /></p><p></p><p>图 1: Alexandria 收集文档的 URL</p><p></p><p>由于我们的文档在网络上独一无二，因此被赋予了一个专属的 DocID。</p><p></p><p>网站的不同部分会被搜索引擎细致扫描，寻找相关关键词短语，并将这些信息推送至搜索索引中。在这一过程中，页面上的所有关键词 “亮点”（即 “命中列表”）首先会被送往直接索引，该索引负责整合页面上重复出现的关键词。</p><p></p><p>随后，这些关键词短语会被精心编织进倒排索引的词汇表中。以 “铅笔” 为例，这个词及其所有包含它的关键文档，都已被纳入索引体系之中。</p><p></p><p>简而言之，由于我们的文档中 “铅笔” 一词频繁出现，它现在在词汇索引中占据了 “铅笔” 条目的位置，并与对应的 DocID 紧密相连。</p><p></p><p>与 “铅笔” 相关联的 DocID 会获得一个通过精密算法计算出的 IR（信息检索）分数，该分数将在后续用于搜索结果列表中的排序。值得注意的是，若 “铅笔” 一词在我们的文档中被加粗显示，或位于 H1 标签中（这些信息存储在 AvrTermWeight 中），这些都会作为提升 IR 分数的积极信号。</p><p></p><p>谷歌会将视为重要的文档迁移至其核心存储系统 ——HiveMind，即主存储器。这里融合了高速 SSD 与传统 HDD（称为 TeraGoogle），后者用于长期存储非即时访问的数据。文档和信号都存储在主存储器中。</p><p></p><p>据专家估算，在人工智能热潮兴起之前，全球大约半数的网络服务器均由谷歌托管。这一庞大的互联集群网络，使得数百万个主存储单元能够高效协同工作。甚至有谷歌工程师在会议中提及，理论上，谷歌的主存储器容量足以涵盖整个互联网的信息量。</p><p></p><p>有趣的是，存储在 HiveMind 中的链接，包括反向链接，似乎被赋予了更高的权重。例如，来自权威文档的链接将获得更多重视，而存于 TeraGoogle（HDD）中的 URL 链接则可能权重较低，甚至被忽略不计。</p><p></p><p>提示：为你的文档提供准确且一致的日期信息至关重要。无论是源代码中的日期（BylineDate）、从 URL 和 / 或标题中提取的日期（syntaticDate），还是从内容中解析的日期（semanticDate），都将被综合考虑。随意更改日期以营造时效性的假象可能导致搜索引擎降权处理。lastSignificantUpdate 属性精确记录了文档最后一次重大更新的时间，细微的修改或拼写更正并不会触动这一计数器。</p><p></p><p>每个 DocID 的附加信息与信号都被动态存储在 PerDocData 库中，供多个系统在优化搜索结果相关性时调用。此外，文档的最近 20 个版本都会被保存在历史记录中（通过 CrawlerChangerateURLHistory 实现），使谷歌能够评估并追踪内容随时间的演变。</p><p></p><p>若你计划彻底改变一个文档的内容或主题，理论上需通过创建一系列过渡版本逐步过渡，以覆盖并替换旧的内容信号，这一过程可能需持续发布多达 20 个版本。这解释了为何复活过期域名（即曾活跃后废弃的域名）并不总能带来排名上的优势。</p><p></p><p>当域名的管理权发生变更，同时内容主题也大幅调整时，谷歌系统能够敏锐地捕捉到这些变化，并将所有相关信号重置，使得旧域名在排名上不再享有特殊优待，与全新注册的域名站在同一起跑线上。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b6/b6e6a29aa4abae2a87e3eb9fd413468b.jpg" /></p><p></p><p>图 2：除了泄露的信息外，美国司法部门对谷歌的审判和听证会提供的证据文件也是进行深入研究的宝贵资源。这些文件中还包含了内部电子邮件。</p><p></p><p></p><h3>QBST: 搜索 “铅笔” 的详细过程</h3><p></p><p></p><p>当你在谷歌中输入 “铅笔” 进行搜索时，QBST 系统便立刻启动，开始处理这一请求。系统首先会细致地分析搜索关键词，如果搜索短语由多个词汇组成，这些词汇会被精准地传递到词汇索引中，进行深入的检索。</p><p></p><p>接下来，术语加权过程会登场，这是一个复杂而精密的步骤，它涉及到了 RankBrain、DeepRank（原名 BERT）以及 RankEmbeddedBERT 等多个先进的系统。在这些系统的协同作用下，与 “铅笔” 紧密相关的词汇会被进一步传递给 Ascorer，进行更深层次的处理。</p><p></p><p></p><h3>Ascorer: 构建 “绿色环”</h3><p></p><p></p><p>Ascorer 的工作是从倒排索引中筛选出与 “铅笔” 最相关的前 1000 个文档（DocID），并按照信息检索（IR）评分进行排序。这个排序后的文档列表，我们称之为 “绿色环”，在行业内也被广泛称为发布列表或 posting list。</p><p></p><p>Ascorer 作为 Mustang 排名系统的重要组成部分，还会通过一系列精细的过滤手段，如去重（利用 SimHash 技术）、段落分析以及识别原创和有价值的内容等，对这 1000 个候选文档进行进一步的筛选和优化，最终目的是将这 1000 个候选项精炼成用户眼前所见的 “10 个蓝色链接” 或 “蓝色环”。</p><p></p><p>关于铅笔的文档，在当前的发布列表中排名第 132 位。如果没有其他系统的进一步介入，那么这将是它在搜索结果中的最终位置。</p><p></p><p></p><h3>Superroot: 从千中选优，打造 “蓝色环”</h3><p></p><p></p><p>然而，Superroot 系统并不会让事情就此定格。它的任务是将 “绿色环” 中的 1000 个文档重新排序，通过更加精确和细致的算法，将这庞大的数量精确地缩减到仅包含 10 个结果的 “蓝色环”。</p><p></p><p>在这个过程中，Twiddlers 和 NavBoost 等系统扮演着关键角色，它们负责执行具体的筛选和排序任务。尽管可能还有其他系统也参与其中，但由于信息有限，我们无法一一详述其具体细节。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f1/f1f2d1693f47249dee39d6fceabdf975.jpg" /></p><p></p><p>图 3：Mustang 生成 1,000 个潜在结果，随后由 Superroot 将这些结果筛选至 10 个最终结果。</p><p></p><p>尽管 “谷歌咖啡因（Caffeine）” 这一名称仍被提及，但其最初作为独立系统的形式已不复存在，仅作为历史记忆保留。如今，谷歌构建了一个庞大的微服务架构，这些微服务紧密协作，共同为网页文档生成各种关键属性。这些属性不仅是不同排名和重排系统的核心信号，还助力神经网络模型进行更精准的预测。</p><p></p><p></p><h3>过滤器中的多面手：Twiddler 系统</h3><p></p><p></p><p>当前，谷歌正运用着成百上千个 Twiddler 系统，它们的作用类似于 WordPress 插件，但专注于搜索引擎内部的优化任务。每个 Twiddler 都肩负着特定的过滤使命，这种模块化设计不仅简化了创建过程，还避免了直接干预 Ascorer 中复杂排名算法的必要性，后者一旦修改，可能引发连锁反应，需要周密的规划与编程工作。</p><p></p><p>Twiddler 系统以其灵活性和独立性著称，它们可以并行或顺序工作，彼此间无需知晓对方的操作细节。根据工作特性的不同，Twiddler 大致分为两类：</p><p></p><p>PreDoc Twiddlers：这类 Twiddler 能够高效处理大规模的 DocID 集合，因为它们对额外信息的需求极低，从而在处理初期就能显著缩减发布列表的条目数量，为后续步骤打下基础。“Lazy” 类型 Twiddlers：相比之下，这类 Twiddler 则更为复杂，它们需要额外信息，如从PerDocData数据库中提取的数据，这使得处理过程更加耗时。</p><p></p><p>因此，它们通常在 PreDoc Twiddlers 完成初步筛选后才介入。</p><p></p><p>通过这种分阶段处理策略，谷歌极大地优化了计算资源的利用效率，节省了宝贵的时间。</p><p></p><p>不同的 Twiddler 对文档的最终排名产生着直接或间接的影响。有的 Twiddler 通过调整信息检索（IR）评分来提升或降低文档的排名权重；而另一些则直接干预排名位置。例如，对于新入库的文档，一个专注于提升新文档排名的 Twiddler 可能会将 IR 评分大幅提升 1.7 倍，从而将文档从第 132 位迅速推升至第 81 位。</p><p></p><p>此外，为了提升搜索结果页面（SERP）的多样性，有 Twiddler 会专门降低内容相似文档的权重，这进一步促使我们的铅笔文档排名上升了 12 位，达到第 69 位。更有甚者，一个专门限制特定查询下博客页面数量的 Twiddler，将我们的文档排名进一步提升至第 61 位。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/74/74e92857a206e0000a24659a08aa4577.jpg" /></p><p></p><p>图 4：两种类型的 Twiddler—— 超过 100 个 Twiddler 用于减少潜在搜索结果，并对这些结果进行重新排序。</p><p></p><p>在我们的页面中，CommercialScore属性得到了零分（即被标记为 “是”），这表示 Mustang 系统在分析过程中检测到了销售意图。谷歌可能注意到，“铅笔” 搜索后经常会跟随如 “买铅笔” 这样的具有明确商业购买意图的搜索，这表明用户有交易倾向。因此，一个专门识别并响应此类意图的 Twiddler 会介入，通过添加相关商业结果，将我们的页面排名提升了 20 位，最终排在第 41 位。</p><p></p><p>随后，另一个 Twiddler 启动，实施了所谓的 “页面三惩罚”，旨在将疑似垃圾内容的页面排名限制在搜索结果的前三页之内（即最大排名为第 31 位）。这一限制由BadURL-demoteindex属性控制，该属性为页面排名设定了上限。类似DemoteForContent、DemoteForForwardlinks和DemoteForBacklinks等属性也用于实现内容降级的目的。因此，在排除了我们上方三个被降级的文档后，我们的页面排名进一步上升至第 38 位。</p><p></p><p>尽管我们的文档有可能受到降级的影响，但为了简化讨论，我们假设它未受影响。接下来，我们考虑一个通过评估嵌入内容来判断我们铅笔页面与网站主题相关性的 Twiddler。由于我们的网站专注于书写工具，这一特点对我们极为有利，导致另外 24 个与主题关联度不高的文档受到负面影响。</p><p></p><p>举个例子，假设有一个内容多样化的价格比较网站，其中有一页专门介绍铅笔，虽然内容丰富，但因其主题与网站整体内容大相径庭，该页面可能会因此 Twiddler 而被降级。</p><p></p><p>siteFocusScore和siteRadius等属性反映了页面内容与网站主题的紧密程度。得益于此，我们的信息检索（IR）评分再次获得提升，而其他一些结果则因相关性较低而排名下降，最终我们的页面排名跃升至第 14 位。</p><p></p><p>正如之前所述，Twiddler 的功能极为广泛且灵活。开发人员可以不断尝试新的过滤规则、调整乘数或设置特定的排名限制，甚至能够精确控制某个结果在页面上的具体排列顺序。</p><p></p><p>值得注意的是，一份谷歌内部泄露的文件发出警告，指出某些 Twiddler 功能应由专家谨慎使用，并在与核心搜索团队充分沟通后实施。</p><p></p><p></p><blockquote>“即便你认为自己已经洞悉了这些系统的运作奥秘，相信我，那也只是冰山一角。我们自己也尚未能完全参透。”—— 摘自 泄露的《Twiddler 快速入门指南 – Superroot》文档</blockquote><p></p><p></p><p>此外，还有一类专门的 Twiddler，它们负责创建注释并将这些注释附加到文档 ID（DocID）上，从而在搜索结果页面（SERP）中直观展示。比如，它们可能会在摘要中嵌入图片，或动态调整标题及描述内容，以优化用户体验。</p><p></p><p>如果你在疫情期间好奇为何你所在国家的卫生部门（比如美国的卫生与公共服务部）在 COVID-19 相关搜索中总是稳居榜首，答案很可能就藏在一个特定的 Twiddler 里。这个 Twiddler 通过识别查询语言和国家代码，利用特定的算法提升了官方资源的排名权重。</p><p></p><p>虽然用户对于 Twiddler 如何具体调整搜索结果排序的控制力有限，但了解其工作机制无疑能帮助我们更好地理解排名的波动或那些看似 “难以捉摸” 的排名现象。因此，定期检查 SERP，并留意结果类型的多样性显得尤为重要。</p><p></p><p>举例来说，你是否发现，无论搜索词如何变化，论坛讨论和博客文章的数量在搜索结果中似乎总是保持不变？你可以进一步思考：这些结果中，交易性、信息性或导航性的内容各占多少比例？相同的域名是否会频繁出现在不同但相近的搜索查询结果中？</p><p></p><p>如果你观察到搜索结果中在线商店的数量寥寥无几，那么试图通过类似商店网站来提升排名可能并非明智之举。相反，将重心转向创作更多信息丰富的内容可能更为有效。当然，在做出决策之前，我们还需深入探讨 NavBoost 系统的作用，因为它同样在搜索结果排序中扮演着重要角色。</p><p></p><p></p><h3>谷歌的质量评估员和 RankLab</h3><p></p><p></p><p>谷歌在全球范围内聘请了数千名质量评估员，他们负责审视特定的搜索结果，并在新算法或过滤器正式启用前进行初步测试。谷歌方面澄清：“这些评估结果并不直接决定搜索排名。” 尽管此言非虚，但这些评估在间接层面对排名产生了显著影响。</p><p></p><p>评估员的工作流程大致如下：他们会接收到网址或搜索短语（即待评估的搜索结果），并在移动设备上回答一系列预设问题。例如，他们可能会被问及：“这篇内容的作者是谁？写作时间是什么时候？作者在其领域内是否具备专业知识？” 这些回答随后会被记录下来，作为训练机器学习算法的重要数据。算法通过分析这些数据，能够辨别出哪些页面质量上乘、值得信赖，而哪些则相对逊色。</p><p></p><p>这一机制的核心在于，搜索排名的标准并非由谷歌搜索团队直接设定，而是通过深度学习技术，从人工评估中提炼出模式与规律。为了更直观地理解，我们可以设想一个场景：如果大众普遍认为，包含作者照片、全名及 LinkedIn 个人简介链接的内容更具可信度，那么缺乏这些元素的页面在可信度上自然会大打折扣。当神经网络在训练过程中接触到这些特征及相应的评估结果时，它会将这些特征视为影响排名的关键因素。经过多轮正面验证，通常这一过程会持续至少 30 天，网络可能会开始将这些特征作为重要的排名信号。因此，具备这些特征的页面可能会获得排名上的优势，而缺失这些特征的页面则可能面临排名下降的风险。</p><p></p><p>值得注意的是，尽管谷歌官方可能并未特别强调作者信息的重要性，但泄露的信息显示，如 isAuthor 等属性以及通过 AuthorVectors 实现的 “作者指纹识别” 技术，实际上能够识别并区分出作者独特的语言风格（即个体用词和表达方式）。</p><p></p><p>评估员的反馈会被汇总成 “信息满意度”（IS）评分。尽管参与评估的人数众多，但 IS 评分主要集中应用于少数网址。对于其他具有相似特征的页面，系统会采用外推的方式，利用这些评分来辅助排名决策。谷歌指出：“许多文档可能并未获得大量点击，但它们依然具有重要意义。” 当外推方法不适用时，系统会将相关文档自动提交给评估员进行评分。</p><p></p><p>在提及 “黄金” 一词时，它常与质量评估员相关联，暗示着可能存在某种文档或文档类型的最高标准。可以合理推测，符合评估员期望的文档有可能达到这一黄金标准。此外，某些特定的 Twiddler 可能会为被视为 “黄金” 级别的 DocID（文档标识符）提供显著的排名提升，使其跻身搜索结果的前列。</p><p></p><p>值得一提的是，这些质量评估员往往并非谷歌的全职员工，他们可能通过外部公司参与工作。而谷歌的专家则在 RankLab 中致力于实验与研发，不断推出新的 Twiddler，并评估其是否能有效提升搜索结果的质量，或是仅仅起到过滤垃圾信息的作用。经过严格验证并证明有效的 Twiddler 将被整合到 Mustang 系统中，该系统利用复杂、计算密集型且相互关联的算法，对搜索结果进行精细化的处理与优化。</p><p></p><p></p><h3>但是用户想要什么？</h3><p></p><p></p><p>NavBoost 可以解决这个问题！</p><p></p><p>我们的铅笔文档尚待进一步完善。在 Superroot 系统中，NavBoost 这一核心系统占据了决定搜索结果排序的关键位置。NavBoost 采用 “切片” 技术，以灵活管理移动端、桌面端及本地搜索等多样化的数据集。</p><p></p><p>尽管谷歌官方坚称未将用户点击数据纳入排名考量，但 FTC 文件中一封内部邮件的披露却揭示了点击数据处理过程的保密性要求，这在一定程度上引发了外界遐想。</p><p></p><p>这并不意味着谷歌的做法存在不妥，其否认背后实则蕴含双重考量。首要的是，一旦承认使用点击数据，可能会触发媒体对隐私问题的强烈关注，将谷歌置于 “数据巨头” 的舆论漩涡中，被指责为无孔不入地追踪用户在线行为。然而，实际上，点击数据的运用旨在获取具有统计学意义的信息，以优化搜索体验，而非针对个体用户的监控。尽管数据保护倡导者可能对此持保留意见，但这一解释无疑为谷歌的否认立场提供了合理解释。</p><p></p><p>FTC 文件的记载进一步印证了点击数据在排名中的实际作用，而 NavBoost 系统在此过程中更是频频被提及（仅在 2023 年 4 月 18 日的听证会上就被提及了 54 次）。此外，回溯至 2012 年的官方听证会，也已明确指出了点击数据对搜索排名产生的实际影响。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8d/8d8aa0883eac608f016338c364ff02bd.jpg" /></p><p></p><p>图 5：自 2012 年 8 月以来（！），官方已经明确点击数据会改变排名。</p><p></p><p>研究表明，搜索结果中的用户点击行为以及网站或网页的流量情况都会对其在搜索引擎中的排名产生影响。谷歌能够直接在搜索结果页面（SERP）上监控和评估用户的搜索行为，包括搜索操作、点击选择、重复搜索以及重复点击等行为。</p><p></p><p>有一种观点认为，谷歌可能通过其自家的谷歌分析（Google Analytics）工具来推测域名的流量数据，这导致部分用户选择避免使用该系统。然而，这一观点存在局限性。首先，Google Analytics 并不提供对所有交易数据的全面访问权限，限制了其推测能力的准确性。更为关键的是，由于超过 60% 的用户使用的是谷歌 Chrome 浏览器（其用户数量已超过三亿），谷歌能够收集到海量的网络活动数据。这使得 Chrome 在分析网络动态中扮演着至关重要的角色，这一点在相关听证会上也得到了明确强调。此外，Core Web Vitals 的数据也是通过 Chrome 进行收集的，并最终汇总为 “chromeInTotal” 值，用于评估网站的性能。</p><p></p><p>关于 “监控” 的负面舆论是谷歌否认使用点击数据的一个原因。另一个原因是，担心评估点击和流量数据可能会激励垃圾邮件发送者和骗子使用机器人系统伪造流量，从而试图操控搜索排名。虽然谷歌的这种否认态度可能会让人感到沮丧，但其背后的担忧和理由却是可以理解的。</p><p></p><p>在存储的指标中，包括了 “badClicks”（坏点击）和 “goodClicks”（好点击）等评估标准。这些评估通常会考虑搜索者在目标页面上的停留时间、他们浏览了多少其他页面以及这些页面的浏览时间（这些数据来源于 Chrome）如果搜索者在搜索结果中短暂偏离后又迅速返回并点击了其他结果，这种行为可能会增加 “坏点击” 的数量。而在一个搜索会话中，最后一次被认为是 “好” 点击的搜索结果则会被记录为 “lastLongestClick”（最长点击）。为了确保数据的准确性和防止被操控，这些数据会经过压缩处理以在统计上进行标准化。如果某个页面、一组页面或一个域名的首页通常具有良好的访问指标（这些数据同样来源于 Chrome），那么这将会通过 NavBoost 产生积极效果。通过分析在一个域名内或跨域名的流动模式，甚至可以评估网站导航的用户引导效果。由于谷歌能够监测整个搜索会话过程，因此在极端情况下它甚至可能识别出与搜索查询完全不同的文档也适合该查询。例如如果搜索者在搜索过程中离开了他们最初点击的域名并访问了另一个域名（可能是通过该域名中的链接跳转过去的）并在新域名上停留较长时间那么这个作为搜索 “结束” 的文档在未来就有可能通过 NavBoost 被推到更前面的位置前提是它在选择范围内。当然这需要大量搜索者提供有力的统计信号作为支持。</p><p></p><p>接下来我们来详细分析搜索结果中的点击情况。在每个搜索结果页面（SERP）中不同排名位置的结果都有一个平均预期点击率（CTR）作为性能评估的基准。例如根据 Johannes Beus 在今年柏林 CAMPIXX 会议上的分析结果显示排名第一的自然搜索结果平均可以获得 26.2% 的点击率而排名第二的结果则只能获得 15.5% 的点击率。</p><p></p><p>如果某个搜索结果的实际点击率显著低于预期值那么 NavBoost 系统会记录这一差异并据此调整该结果的排名位置（即 DocID 的排名）。相反如果某个结果的实际点击量在历史上一直明显多于或少于预期值 NavBoost 也会相应地调整该文档的排名位置以确保搜索结果的相关性和准确性（见图 6 所示）。</p><p></p><p>这种方法是合理的因为点击率从本质上反映了用户对搜索结果相关性的评价这些评价又是基于搜索结果的标题、描述以及域名等因素得出的。这一概念在谷歌的官方文档中也有详细说明（如图 7 所示）从而进一步证明了其合理性和科学性。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/fc/fccc5c008e2e60078614ea7859ce5648.jpg" /></p><p></p><p>图 6：如果 “预期 _CRT” 与实际值有显著差异，则排名会相应调整。（数据源：J. Beus，SISTRIX，带编辑覆盖）</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7c/7cbf2c5b548d6b6599fb359411fc4576.jpg" /></p><p></p><p>图 7：谷歌演示文稿中的幻灯片（来源：审判证据 - UPX0228，美国及原告州诉谷歌公司）</p><p></p><p>由于我们的铅笔文档刚刚发布不久，因此目前还缺乏具体的点击率（CTR）数据。对于这类无数据的新文档，系统是否会忽略 CTR 偏差尚不明朗，但从其融入用户反馈的设计初衷来看，这种可能性是存在的。另一种推测是，系统可能会依据其他相关指标对 CTR 进行初步估算，这与谷歌 Ads 中处理质量因子的方式有异曲同工之妙。</p><p></p><p>SEO 专家和数据分析师在长期实践中发现，一旦他们全面监控点击率，便会注意到一个规律：当某个文档首次跻身搜索结果前 10 名，而其实际 CTR 显著低于预期时，其排名往往会在几天内（具体时间取决于搜索频率）出现下滑。相反，若 CTR 远高于预期，则排名有望攀升。面对 CTR 表现不佳的情况，快速调整文档的摘要信息（如优化标题和描述）以吸引更多点击至关重要，否则排名下滑后恢复难度将大幅增加。这一现象被普遍视为系统测试机制的一部分，即文档若表现优异则稳固高位，若不符用户期待则可能被剔除。至于这是否与 NavBoost 系统直接相关，目前尚无确凿证据。</p><p></p><p>根据泄露的信息，谷歌在估算新页面信号时，似乎高度依赖于页面 “环境” 中的海量数据。例如，新页面在初期可能会继承主页的 PageRank（称为 HomePageRank_NS），直至其建立起自己的 PageRank。同时，pnavClicks 可能用于预测通过导航链接到新页面的点击概率。</p><p></p><p>鉴于计算和更新 PageRank 的复杂性及高计算成本，谷歌可能采用了 PageRank_NS 指标作为过渡方案。“NS” 代表 “最近种子”，意味着相关页面共享一个临时的 PageRank 值，该值会根据需要长期或短期地应用于新页面。</p><p></p><p>此外，邻近页面的信号也可能对其他关键指标产生影响，助力新页面在缺乏高流量或反向链接的情况下提升排名。值得注意的是，许多信号的反映并非即时，而是存在一定的延迟。</p><p></p><p>谷歌在听证会上展示了 “新鲜度” 在搜索结果中的实际应用。以搜索 “Stanley Cup” 为例，平时搜索结果多聚焦于这一著名奖杯的介绍，但在斯坦利杯冰球比赛期间，NavBoost 会根据搜索和点击行为的变化，优先展示与比赛紧密相关的信息。这里的 “新鲜度” 并非指文档的新旧，而是指搜索行为和兴趣点的动态变化。谷歌每天处理的搜索行为超过十亿次，每一次搜索和点击都在为谷歌的学习提供宝贵数据。这意味着，谷歌对搜索意图的捕捉和响应远比我们想象的细腻和及时，而非仅仅局限于对季节性变化的简单预测。</p><p></p><p>最新数据显示，文档的点击指标会被存储并评估长达 13 个月之久（每年有一个月的数据与前一年重叠，以便进行对比分析）。鉴于我们的假设域名拥有强大的访问指标和显著的广告直接流量，作为知名品牌（这是一个正面信号），我们的新 “铅笔” 文档自然能够从前期的成功页面中获益。因此，NavBoost 系统成功将我们的排名从第 14 位提升至第 5 位，使我们跻身 “蓝色环” 或前 10 名之列。这前 10 名的文档将与其他九个自然搜索结果一同被转发至谷歌的网络服务器。</p><p></p><p>值得注意的是，谷歌实际提供的个性化搜索结果并不像人们普遍预期的那样丰富。测试表明，通过模拟用户行为并进行相应调整往往能带来更优化的搜索结果，而非单纯依赖于评估个别用户的偏好。这一发现极具启示意义 —— 神经网络的预测能力已经超越了我们的个人浏览和点击历史记录所能提供的个性化程度。当然，对于特定偏好（如对视频内容的喜好），个性化搜索结果仍会予以体现。</p><p></p><p></p><h3>谷歌网络服务器：一切终结与新开始的地方</h3><p></p><p></p><p>谷歌网络服务器（GWS）是构建和呈现搜索结果页面（SERP）的核心，这个页面上包含了诸多元素：十个蓝色链接的自然搜索结果、广告、图片、谷歌地图视图、“人们也在问” 板块等。</p><p></p><p>为了优化这些元素在有限页面空间内的布局，谷歌采用了 Tangram 系统。该系统负责计算每个元素所需的空间大小，并智能决定在给定的 “框架” 内能容纳多少结果。紧接着，Glue 系统会将这些元素精确无误地安置到它们应有的位置上，确保页面既美观又高效。</p><p></p><p>目前，我们的 “铅笔” 文档在自然搜索结果中排名第五，但值得注意的是，CookBook 系统拥有在搜索结果展示前的最后一刻进行微调的能力。这个系统内部集成了 FreshnessNode、InstantGlue（能在 24 小时内快速反应，但通常会有约 10 分钟的延迟）和 InstantNavBoost 等组件。这些组件如同 “幕后英雄”，在最终页面呈现之前，迅速生成与搜索结果时效性紧密相关的信号，并可能据此对排名进行动态调整。</p><p></p><p>想象一下这样的场景：一档关于 Faber-Castell 品牌 250 周年纪念以及 “铅笔” 这一关键词的德国电视节目突然热播。在节目播出的几分钟内，成千上万的观众可能会迅速拿起他们的智能手机或平板电脑进行搜索。这时，FreshnessNode 便会敏锐地捕捉到 “铅笔” 搜索量的激增，并智能地分析出用户的搜索意图是寻求信息而非直接购买。基于这一判断，系统会相应地调整搜索结果的排名。</p><p></p><p>具体来说，InstantNavBoost 会立即采取行动，将所有与交易相关的结果暂时移除，转而用更加信息丰富、与当前热点紧密相关的结果来替代。同时，InstantGlue 也会迅速更新 “蓝色环” 内的结果排序，导致我们原本可能以销售为导向的文档因为不够相关而被更合适的结果挤出前列。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7e/7e35991f5946e6ec8885cf5c4483905c.jpg" /></p><p></p><p>图 8：一档关于 “铅笔” 一词起源的电视节目，以庆祝德国知名铅笔制造商 Faber-Castell 成立 250 周年。</p><p></p><p>尽管我们假设的排名故事以遗憾暂告段落，但它深刻揭示了一个核心真理：获得并维持高排名，绝非仅凭出色的文档或高效的 SEO 策略就能一蹴而就。</p><p></p><p>排名是一个多因素交织的复杂结果，它受到搜索行为波动、新文档信号的融入以及外部环境不断变化等多重影响。因此，认识到高质量文档与优化的 SEO 策略仅是排名动态系统中的一环，且至关重要，这一点尤为重要。</p><p></p><p>搜索结果的生成过程犹如精密的机械运作，背后涉及数以千计的信号和复杂算法。SearchLab 通过 Twiddler 进行的实时测试，甚至可能间接影响到文档的反向链接权重，从而引发连锁反应。</p><p></p><p>这些文档的命运可能因此发生转折，它们可能被从 HiveMind 这一核心存储系统迁移到优先级较低的存储层级，如 SSD 或 TeraGoogle，这一变动将直接削弱或消除它们对排名的正面影响，即便文档内容本身并未有丝毫改动。</p><p></p><p>谷歌的 John Mueller 曾明确指出，排名的下滑并不总是意味着你的策略有误。用户行为模式的转变、新兴趋势的涌现或是其他外部因素，都可能成为影响排名表现的不确定因素。</p><p></p><p>比如，当用户开始倾向于寻求更详尽的信息或偏好简短明了的文本时，NavBoost 系统便会迅速响应，调整排名以匹配这些新的搜索偏好。然而，值得注意的是，这种调整并不会触动 Alexandria 系统或 Ascorer 中的 IR 评分，后者更多地是基于文档本身的固有质量进行评估。</p><p></p><p>这一切都向我们传达了一个重要启示：SEO 工作应当置于更广阔的视角下进行审视。如果文档内容与用户的搜索意图存在偏差，那么即便是再精妙的标题优化或内容调整，其效果也会大打折扣。</p><p></p><p>更为关键的是，Twiddler 和 NavBoost 等系统对排名的干预力度，往往超越了传统的页面优化手段，包括页面内、页面上以及页面外的优化措施。一旦这些系统对文档的可见性进行了限制，那么无论我们在页面上如何努力优化，都可能难以扭转乾坤。</p><p></p><p>但请放心，我们的故事并不会就此陷入低谷。关于铅笔的电视节目效应终究只是短暂的喧嚣。随着搜索热度的逐渐退却，FreshnessNode 的临时影响也将烟消云散，我们的排名有望重新回升至第五位。</p><p></p><p>当我们重新开始收集点击数据时，根据 SISTRIX 的 Johannes Beus 的预测，第五位的平均点击率（CTR）大约在 4% 左右。只要我们能够稳定保持这一 CTR 水平，我们就有信心继续稳坐前十的宝座。未来可期，一切都将朝着更好的方向发展。</p><p></p><p>SEO 的关键要点</p><p></p><p>流量来源多元化：确保你的网站流量不仅仅依赖于搜索引擎，而是从多种渠道汇聚而来，包括社交媒体平台等非传统渠道，这些都能带来宝贵的访问量。即便谷歌的爬虫无法触及某些页面，它依然能通过 Chrome 浏览器或直接 URL 追踪到你的网站访客数量。强化品牌与域名认知：不断提升你的品牌或域名知名度至关重要。品牌越为人熟知，用户在搜索结果中点击你网站的几率就越大。通过优化针对多种长尾关键词的排名，可以有效提升域名的可见度。据透露，“站点权威性” 可能是影响排名的一个关键因素，因此增强品牌声誉对提升搜索排名大有裨益。深入理解搜索意图：为了更好地满足访客需求，深刻理解他们的搜索意图及路径至关重要。利用 Semrush、SimilarWeb 等工具分析访客来源及其行为，审视这些域名是否提供了你页面所缺失的信息，并据此逐步补充，使你的网站成为访客搜索路径上的 “终极目的地”。谷歌能够追踪相关搜索会话，精准把握搜索者的需求与历史。优化标题与描述，提升点击率：审视并调整当前标题与描述的吸引力，通过大写关键词汇使其在视觉上更为突出，可能有助于提高点击率。标题在决定页面排名中扮演关键角色，因此应优先考虑其优化。评估隐藏内容效果：若采用手风琴等形式隐藏重要内容，需留意这些页面的跳出率是否偏高。当访问者无法迅速定位所需信息，需多次点击时，可能产生负面点击信号。精简无效页面：对于长期无人问津或排名不佳的页面，应考虑删除，以避免对邻近页面造成不利影响。新文档若发布在 “劣质” 页面群组中，其表现机会将大打折扣。“deltaPageQuality” 指标用于衡量域名或页面集群中单个文档的质量差异。优化页面布局：清晰的页面结构、流畅的导航以及令人印象深刻的首页设计，对于跻身排名前列至关重要，这往往得益于 NavBoost 等系统的助力。增强用户互动：延长访客在网站上的停留时间，能发出积极的域名信号，惠及所有子页面。致力于成为访客的 “一站式” 信息源，提供全面信息，减少其他搜索需求。深化而非泛化内容：更新并丰富现有内容往往比不断创建新内容更为有效。“ContentEffortScore” 评估文档创作难度，高质量图片、视频、工具及独特内容均对此有正面贡献。标题与内容一致：确保标题准确概括后续内容，利用文本向量化等先进技术进行主题分析，较单纯词汇匹配更为精准地判断标题与内容的一致性。利用网页分析工具：借助谷歌 Analytics 等工具，有效追踪访客互动情况，及时发现问题并予以解决。特别关注跳出率，若异常偏高，需深入调查原因并采取措施改善。谷歌通过 Chrome 浏览器获取这些数据，实现深度分析。聚焦低竞争关键词：初期可优先针对竞争较小的关键词进行优化，更易于建立正面用户信号。构建高质量反向链接：重视来自 HiveMind 中最新或高流量页面的链接，因其传递的信号价值更高。避免链接至流量稀少或参与度低的页面。同时，来自同国别且内容相关的反向链接更具优势。警惕 “有毒” 反向链接，以免损害评分。关注链接上下文：在评估链接价值时，不仅要考虑锚文本本身，还需关注其前后文本的自然流畅性。避免使用 “点击这里” 等通用短语，因其效果已被证实不佳。理性看待 Disavow 工具：该工具用于屏蔽不良链接，但据泄露信息显示，它并未被算法直接采用，更多用于文档管理和反垃圾邮件工作。强调作者专业性：若使用作者引用功能，应确保其在外界享有良好声誉并具备专业知识。少数高资质作者往往优于众多低信誉作者。谷歌能根据作者的专业知识评估内容质量，区分专家与非专家。创作独特、实用、全面的内容：对关键页面尤为重要，展现你的专业深度，并提供有力证据支持。尽管可以聘请外部人员填充内容，但若缺乏实质质量和专业知识支撑，则难以企及高排名目标。</p><p></p><p>原文链接：</p><p></p><p><a href="https://searchengineland.com/how-google-search-ranking-works-445141">https://searchengineland.com/how-google-search-ranking-works-445141</a>"</p><p></p><p>声明：本文为 InfoQ 翻译，未经许可禁止转载。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ba2e971f73b94be4959ea71a1</id>
            <title>AICon 全球人工智能开发与应用大会参会有感</title>
            <link>https://www.infoq.cn/article/ba2e971f73b94be4959ea71a1</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ba2e971f73b94be4959ea71a1</guid>
            <pubDate></pubDate>
            <updated>Wed, 21 Aug 2024 14:58:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><h1>目录</h1><p></p><p>引言大会背景大会议程参会体验会后感想结束语</p><p></p><h1>引言</h1><p></p><p></p><blockquote>在数字化浪潮席卷全球的今天，人工智能开发与应用已成为推动社会进步和产业升级的关键力量。作为一名对AI技术非常感兴趣的开发者，在 8 月 18 日至 19 日这两天，我有幸参加了由极客邦科技旗下 InfoQ 中国主办的 AICon 全球人工智能开发与应用大会，个人觉得这场盛会就如同科技领域的璀璨星辰，吸引了众多行业精英和技术开发者齐聚上海，共同见证人工智能相关的又一盛会。在人工智能的浪潮中， AICon 全球人工智能开发与应用大会无疑是技术领域的一次盛会，在这次大会上我不仅了解到了AI技术的最新发展，还深入了解了AI在各行各业的实际应用。在这篇文章中，将分享我的参会体验和AI技术的前沿动态，以及它如何影响我们的未来。</blockquote><p></p><p><img src="https://static001.geekbang.org/infoq/82/821d9837ddc41f82cdbd724def5f7f49.png" /></p><p></p><h1>大会背景</h1><p></p><p>先来了解一下本次大会的背景， AICon 全球人工智能开发与应用大会是一个专注于AI技术的国际性盛会，是由极客邦科技旗下InfoQ中国主办的技术盛会，旨在为各行业的AI技术爱好者提供一个交流和学习的平台。大会聚集了来自世界各地的AI领域专家、学者、企业家以及开发者，共同探讨AI技术的发展趋势、应用实践和未来挑战。</p><p><img src="https://static001.geekbang.org/infoq/3b/3b729ea10567776ece148fd9ba4162c6.png" /></p><p></p><h1>大会议程</h1><p></p><p>再来分享一下本次大会的议程，大会的议程丰富多样，涵盖了AI领域的多个方面，具体如下所示：</p><p>AI基础理论研究机器学习算法创新深度学习与神经网络自然语言处理（NLP）计算机视觉AI在金融、医疗、教育等行业的应用AI伦理与法规</p><p><img src="https://static001.geekbang.org/infoq/43/43250663d8b5e92ebe4c17ceeeff1679.png" /></p><p></p><h1>参会体验</h1><p></p><p>当我踏入大会现场的那一刻，便被现场的氛围所感染，会场内人头攒动，来自各个领域的技术开发者怀揣着对人工智能的热情和期待，交流着彼此的见解和经验，作为一名开发者，我对AI技术的应用实践和技术创新特别感兴趣。</p><p><img src="https://static001.geekbang.org/infoq/16/16db377a7be1e0c51b810714f6638633.png" /></p><p>大会的第一天，众多顶尖企业与研究机构的资深专家纷纷登台，分享了他们在人工智能领域的最新研究成果和实践经验：</p><p>来自字节跳动的专家深入探讨了人工智能在内容推荐系统中的应用，他们通过巧妙地运用深度学习算法，实现了对用户兴趣的精准预测，从而为用户提供了更加个性化、精准的内容推荐。华为的专家则分享了他们在人工智能与 5G 通信技术融合方面的研究成果。通过利用人工智能的智能优化算法，实现了对 5G 网络资源的高效分配和管理，大大提升了网络的性能和覆盖范围。这让我看到了人工智能在通信领域的广阔应用前景，以及其对推动整个通信行业发展的巨大潜力。阿里巴巴的专家带来了关于人工智能在电商领域的创新应用案例。他们利用图像识别和自然语言处理技术，实现了商品的智能识别和搜索推荐，极大地提高了消费者的购物效率和体验。这使我明白了人工智能在电商行业的深度融合，能够为企业带来显著的竞争优势和业务增长。微软亚洲研究院的专家展示了他们在人工智能基础研究方面的最新突破，特别是在强化学习和生成对抗网络方面的研究成果。这些前沿的研究为人工智能的未来发展提供了坚实的理论基础和技术支持，让我对人工智能的未来充满了信心。智源研究院的专家分享了关于人工智能伦理和社会影响的思考。他们强调了在人工智能快速发展的背景下，我们需要关注技术带来的伦理问题，如算法偏见、数据隐私等，并积极探索相应的解决方案。这让我意识到，人工智能的发展不仅要追求技术的进步，还要注重其对社会和人类的影响，确保技术的发展是有益和可持续的。上海人工智能实验室的专家介绍了他们在城市智能管理方面的应用实践。通过利用人工智能技术，实现了对城市交通、环境、能源等方面的智能监测和优化管理，提升了城市的运行效率和居民的生活质量。这让我看到了人工智能在改善城市生活方面的巨大潜力，以及其对未来智慧城市建设的重要意义。蔚来汽车的专家分享了人工智能在自动驾驶领域的最新进展。他们通过融合多种传感器数据和深度学习算法，实现了车辆的自动驾驶和智能决策，为未来的出行方式带来了革命性的变革。这使我对自动驾驶的未来充满了期待，同时也让我认识到在实现自动驾驶的过程中，还需要解决许多技术和法律方面的挑战。小红书的专家讲述了他们在内容创作和社交互动方面的人工智能应用。通过利用图像生成和自然语言处理技术，为用户提供了更加丰富和有趣的内容创作工具，同时也提升了用户之间的社交互动体验。这让我感受到了人工智能在社交媒体领域的创新应用，以及其对用户参与和内容传播的积极影响。零一万物的专家展示了他们在人工智能芯片研发方面的最新成果。他们研发的高性能人工智能芯片，为人工智能算法的高效运行提供了强大的硬件支持，大大提升了人工智能系统的性能和效率。这让我认识到硬件的创新对于推动人工智能发展的重要性，以及芯片研发在人工智能产业链中的关键地位。</p><p><img src="https://static001.geekbang.org/infoq/df/dfb26b0b6ef3d95ce77c6aa61122e017.png" /></p><p>在第一天的会议中，我不仅了解到了人工智能在各个领域的最新应用和技术突破，还深刻感受到了人工智能技术的快速发展和广泛应用给我们的生活和社会带来的巨大变革。同时，我也意识到在人工智能的发展过程中，我们需要关注技术的伦理和社会影响，确保技术的发展是有益和可持续的。在大会上，我有机会听到了来自业界领袖的精彩演讲，参与了深入的技术研讨会，还与来自不同领域的专家进行了交流，这让我深刻认识到，人工智能不仅能够提升用户体验，还能为企业创造巨大的商业价值。</p><p><img src="https://static001.geekbang.org/infoq/61/61239e1fc2ae2cad5a8257afd22bfd3e.png" /></p><p>大会的第二天，这一天的分享更加侧重于人工智能的产业化和商业化动态，以及在实际落地场景中的挑战和解决方案，同样的，来自不同企业的专家们分享了他们在将人工智能技术从实验室推向市场的过程中所面临的困难和挑战：</p><p>数据质量和数据标注的问题成为了许多企业共同面临的难题。高质量的数据对于训练有效的人工智能模型至关重要，但获取、清洗和标注大量的数据需要耗费大量的时间和资源。此外，模型的可解释性和透明度也是一个亟待解决的问题，尤其是在一些对安全性和可靠性要求较高的领域，如医疗和金融。在产业化方面，一些企业分享了他们如何构建人工智能团队和建立有效的研发流程。他们强调了跨学科合作的重要性，包括数据科学家、工程师、产品经理和业务专家之间的紧密协作。同时，建立敏捷的开发流程和持续的优化机制也是确保项目成功的关键因素。在商业化方面，专家们探讨了如何将人工智能技术转化为实际的商业价值。他们分享了一些成功的案例，如通过人工智能优化供应链管理，降低成本并提高效率；利用人工智能进行精准营销，提高客户满意度和销售额。同时，他们也提到了在商业推广过程中面临的挑战，如客户对新技术的接受程度、法律法规的限制等。</p><p><img src="https://static001.geekbang.org/infoq/44/44993ae13f9e8fa1711dfed9fd9a160a.png" /></p><p>除了主题演讲和案例分享，大会还设置了互动环节和小组讨论。在这些环节中，我有机会与其他参会者深入交流，分享彼此的经验和见解。我结识了许多来自不同行业的技术大佬，与他们的交流让我拓宽了视野，获得了许多新的思路和灵感。</p><p></p><p>尤其是在小组讨论中，我们围绕着“人工智能在医疗领域的应用与挑战”这一话题展开了热烈的讨论。医疗行业对准确性和安全性的要求极高，因此在应用人工智能技术时需要格外谨慎，我们交流了如何确保人工智能诊断系统的准确性和可靠性，如何解决数据隐私问题，以及如何让医疗机构和患者更好地接受和信任人工智能技术。通过参与这些互动环节，我不仅加深了对会议内容的理解，还建立了宝贵的人脉资源。这些人脉将为我未来在人工智能领域的学习和工作提供有力的支持。</p><p><img src="https://static001.geekbang.org/infoq/24/24e09697cc3734c2e7516c60de090c31.png" /></p><p></p><h1>会后感想</h1><p></p><p>回顾这两天的参会经历，我深感收获颇丰，个人觉得AICon 全球人工智能与机器学习技术大会不仅是一个技术交流的平台，更是一个激发创新思维、促进合作的机会，我不仅接触到了最前沿的技术动态，了解到了行业的发展趋势，同时也结识了许多志同道合的朋友。</p><p>人工智能作为引领未来的关键技术，将继续在各个领域发挥重要作用，我相信通过不断的学习和实践，我们能够更好地驾驭这一强大的技术，为人类创造更美好的生活。</p><p>在今后的工作中，我将把在大会上学到的知识和经验应用到实际项目开发中，然后不断探索和创新，而且我也将继续关注人工智能领域的发展动态，积极参与相关的技术交流活动，与其他开发者共同成长，争取在人工智能领域占一席地！</p><p><img src="https://static001.geekbang.org/infoq/50/50c413f2f4bd966b662e60320025949b.png" /></p><p></p><h1>结束语</h1><p></p><p>参加 AICon 全球人工智能开发与应用大会是一次宝贵的学习经历，我不仅学到了对AI技术的新的认识，还对AI的未来发展有了更深的理解。AI技术正以前所未有的速度改变着世界，作为开发者，我们需要不断学习新技术，探索新应用，从而推动AI技术的健康发展。在AI的征途上，我们既是探索者，也是建设者。让我们携手共进，用AI技术创造更美好的未来。最后，我要感谢极客邦科技旗下 InfoQ 中国主办了这样一场精彩的大会，为开发者们提供了如此宝贵的学习和交流机会，非常期待下一次的 AICon 大会能够带来更多惊喜！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/5Jx040lfFB5d1ioIRtK5</id>
            <title>《黑神话：悟空》太狠了！Steam 瘫痪、多家公司放假，英伟达老黄又要躺在新的印钞机上数钱了</title>
            <link>https://www.infoq.cn/article/5Jx040lfFB5d1ioIRtK5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/5Jx040lfFB5d1ioIRtK5</guid>
            <pubDate></pubDate>
            <updated>Wed, 21 Aug 2024 12:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><h1>《黑神话：悟空》太狠了！Steam 瘫痪、多家公司放假，英伟达老黄又要躺在新的印钞机上数钱了</h1><p></p><p><img src="https://static001.geekbang.org/wechat/images/66/66d8817eb95f472a457ad1cc44627f34.png" /></p><p></p><p>作者｜小褚、华卫、冬梅</p><p></p><p></p><h1>挤爆 Steam 服务器，多家公司给员工放假玩游戏</h1><p></p><p></p><p>8 月 20 日，互联网上似乎所有的热搜都集中在了《黑神话：悟空》上线这件事上。</p><p></p><p>上午 10 点，备受期待的游戏《黑神话：悟空》正式上线，然而，一瞬间挤进上百万人后，这款游戏背后的服务器 Steam 不堪重负，遭遇了短暂的崩溃。</p><p></p><p>众多玩家进入游戏时遭遇了阻碍，无法顺利启动游戏。有玩家在社交媒体上调侃称：“服务器爆了，Steam 好久没被干爆过了吧。”幸运的是，这一问题在大约十分钟内得到了解决，玩家得以继续他们的游戏体验。</p><p></p><p>网友纷纷摩拳擦掌时，却遭遇“漫漫解压路”：“八十一难第一难，开始解压”“比下载时间都长”“解压打断了我的大圣梦”……但依旧挡不住大家的热情。</p><p></p><p>据悉，《黑神话：悟空》的上线人数位居 Steam 同时在线人数历史第四位，这一人数仅次于 PUBG、幻兽帕鲁、CSGO。</p><p></p><p>此外，游戏玩家社区平台小黑盒也报告了崩溃情况。许多玩家通过小黑盒购买了《黑神话：悟空》，此次崩溃事件可能对他们的购买体验造成了一定影响。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b4/b4d8cbe00ff2dc1ab411271afdf4cf48.png" /></p><p></p><p>《黑神话：悟空》到底火到什么程度？在游戏上线首日，甚至出现了不少公司给员工放假去体验游戏的情况。</p><p></p><p>四川木子杨科技有限公司 8 月 19 日发布通知，决定在《黑神话：悟空》上线的当天 8 月 20 日给全体员工放假一天，让员工尽情体验《黑神话：悟空》带来的视觉盛宴和游戏乐趣，与同事、朋友一起分享这款国产大作的精彩瞬间。公司表示，这次放假是对国产游戏行业的一份支持。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/0f/0ff0c0cf10df7f6ed900cd7e1ce9a13c.png" /></p><p></p><p>游戏发行商 Gamera Game 宣布 8 月 20 日放假，公司还表示，“为了避免各位同事因临时暂停工作而感到不知所措，将送给每位同事一份《黑神话：悟空》数字豪华版”。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b4/b4ff5439e76c0872e760b1c61ab1816b.png" /></p><p></p><p>行业媒体“游戏茶馆”也公告，为让员工们更好地体验《黑神话：悟空》，决定 8 月 20 日放假半天，同时为大家提供一份报销该游戏费用的机会。</p><p></p><p>“作为苦逼打工人，也是靠加班赚来的年假 + 病假凑足的请假时间来体验《黑神话：悟空》，我的初体验是‘绝对值得’”有网友评价道。“要不是我现在已经是一个成熟稳重分得清轻重缓急的成年人，我现在也去玩游戏了！”还有网友提到。正式上线后的《黑神话：悟空》好评如潮。</p><p></p><p>那么，这款游戏为何能火到如此程度？</p><p></p><p></p><h1>《黑神话：悟空》为啥这么火？</h1><p></p><p></p><p>4 年前的今天，也是 8 月 20 日，《黑神话：悟空》发出第一条宣传预告，自此之后这一游戏的热度便一直居高不下。据 Steam 商店的官方介绍，《黑神话：悟空》是由游戏科学制作的以中国神话为背景的动作角色扮演游戏，游戏中玩家将扮演一位“天命人”，为了探寻昔日传说的真相，踏上一条充满危险与惊奇的西游之路。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8b/8bdfef422204c751f50d7ebaa5dba588.png" /></p><p></p><p>今年 6 月 10 日，《黑神话：悟空》实体版正式开启全款预售。Steam 中国大陆区标准版售价 268 元，数字豪华版售价 328 元。目前这款游戏的全球媒体评分已解禁，IGN 中国给予 10 分评价、IGN 海外给予 8 分评价。截至 8 月 17 日凌晨，52 家全球媒体平均给出了 82 分的评价。</p><p></p><p>而论《黑神话：悟空》获得如此高市场反响的原因，首先不得不提的是西游记的 IP 加成，基于西游记而产生的影视作品层出不穷，最近一部的《大圣归来》也唤起了大家记忆中那个踏碎凌霄的齐天大圣，孙悟空更是许多人童年时候心目中的英雄。</p><p></p><p>其次便是同行的“衬托”了。《黑神话：悟空》并不是第一个做西游记 IP 的游戏，此前市面上有不少同类型的游戏，但其仅从宣传片画面便赢得了众多玩家对其的期待和青睐。据介绍，《黑神话：悟空》游戏里的取景，几乎都有现实存在的原型，能够让玩家感受到从游戏到现实的无缝切换。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/18/1863774968b2642250c7fb3d7e63bcff.gif" /></p><p></p><p>更重要的是，《黑神话：悟空》寄托了许多游戏玩家对第一个国产 3A 大作的希望。中国的游戏单机市场寂寥已久，一众玩家对于国内游戏的宽容度不低。有专业博主分析认为：“国人渴求中国文化背景的游戏，平心而论尽管国外一些游戏的制作水平比《黑神话：悟空》高，但它们的情感冲击绝对没有它强。”</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8f/8f01473d8992120c88551acf3beb6761.gif" /></p><p></p><p>此外，自首个宣传片放出之日起，《黑神话：悟空》便不断对外同步演示视频，从开场动画到游戏表现形式、人物动作流畅度，玩家们都见证着其在这 4 年中的进步。</p><p></p><p>有市场分析显示，《黑神话：悟空》的预售表现超出预期，一个月内销量达到 120 万份，销售额近 4 亿元，最后的收入肯定更远不止如此。随着《黑神话：悟空》的终极 PV 发布，玩家纷纷喊话，“这次我要做自己的齐天大圣。”</p><p></p><p></p><h1>强大的英伟达 AI 作支撑</h1><p></p><p></p><p>《黑神话：悟空》基于虚幻引擎 5，并采用新的 RTX 技术。在 PC 端，《黑神话：悟空》的视觉效果经过全景光线追踪技术的增强，成为迄今为止发布的沉浸感更强、技术更先进的游戏之一。</p><p></p><p>为了迎接这款游戏到来，NVIDIA 英伟达上周宣布为《黑神话：悟空》推出 Game Ready 驱动，这是 NVIDIA 首次为中国游戏做专属优化。此次 Game Ready 驱动重点是优化 RTX 40 系列的性能。</p><p></p><p>全景光线追踪技术对硬件要求更高，但可以高度准确地渲染光线及其在场景中的效果。这种先进的光线追踪技术也被称为路径追踪，视觉效果艺术家们可利用它打造以假乱真的电影和电视画面。《黑神话：悟空》中的全景光线追踪技术提高了光照、反射和阴影的保真度和质量。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/cb/cb57c378272f3c49a7ea25b06ac1cfc2.png" /></p><p></p><p>借助多次反射光线追踪间接照明，自然色彩光线可反射多达两次，营造出更逼真的间接光照和遮蔽效果。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/45/456d955536ab06bc197a9a92f84a40c4.png" /></p><p></p><p>为了提高质量并增强沉浸感，特效通常包含大量的单体粒子，而使用传统光线追踪方法会对性能带来巨大的压力。《黑神话：悟空》采用一种新技术，使用两级光线追踪为大量面片系统进行画序无关的透明渲染，从而在实时反射中高效地渲染游戏的粒子系统。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/44/44821f2a0b7af3641af8abcf9efdbc9c.png" /></p><p></p><p>要获得此效果，玩家可以在游戏中开启“Full Ray Tracing (全景光线追踪) ”设置，然后将“Full Ray Tracing Level (全景光线追踪水平)”设置为“Very High"。</p><p></p><p>NVIDIA DLSS 是英伟达的 AI 渲染技术，可通过 GeForce RTX GPU 上的专用 Tensor Core AI 处理器提高游戏和应用的图形性能。</p><p></p><p>在《黑神话：悟空》中，通过支持全景光线追踪技术并将每项设置为最大值，DLSS 3 可带来性能的成倍提升。这使得 GeForce RTX 4080 SUPER 的用户能够在《黑神话：悟空 》的基准测试中达到每秒近 74 帧的帧率。此外，GeForce RTX 4070 Ti SUPER 的用户可以在每秒 66 帧的帧率下享受 4K 的乐趣。</p><p></p><p>此前，官方公布的推荐配置显示，玩家在 1080P 高画质下需要至少 RTX 2060 或 RX 5700XT 级别的显卡，而中画质则可选择 GTX 1060 或 RX 580。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b9/b91017444854a18dc9b73e2b95f3b8f2.png" /></p><p></p><p>值得注意的是，这些配置均是在启用了 DLSS/FSR/Xess 等图形优化技术的前提下给出的。若玩家希望体验原生 1080P/ 高画质的游戏效果，可能需要具备更高性能的显卡，如 RTX 3060 及以上。目前已经有玩家表示，3060 使用起来还会有卡顿出现。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/55/5506b4d3713356c2c6d8a6bd02db2719.png" /></p><p></p><p>此外，微星、技嘉、七彩虹、索泰、影驰、映众、万丽、耕升等企业陆续推出了与《黑神话：悟空》联名的显卡产品。</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/400hg9QcIGebLvg0BdbV</id>
            <title>AI 与大模型如何助力金融研发效能最大化？</title>
            <link>https://www.infoq.cn/article/400hg9QcIGebLvg0BdbV</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/400hg9QcIGebLvg0BdbV</guid>
            <pubDate></pubDate>
            <updated>Wed, 21 Aug 2024 05:58:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在金融行业，技术创新与严格合规的需求并行存在，推动着研发团队不断寻求更高效的解决方案。面对日益增长的市场竞争和技术进步，金融机构必须迅速适应变化，同时确保所有创新措施都符合监管要求。这种需求催生了对高效研发流程和先进技术应用的追求。</p><p></p><p>在日前的 InfoQ《超级连麦. 数智大脑》x FCon 直播中，我们邀请到 微众银行研发效能负责人余伟， 以及 数势科技数据智能产品总经理岑润哲，深入探讨了在金融研发中提升效能的有效策略，如何选择合适的技术栈和架构设计，以及如何利用 AI、大模型和低代码等技术优化研发流程、加速产品交付。</p><p></p><p>8 月 16-17 日，FCon 全球金融科技大会于上海举办，2 位老师在 「金融研发效能提升路径与实践」专题论坛中与大家进行了深入的交流和分享。点击链接可获取PPT下载链接：<a href="https://ppt.infoq.cn/list/149">https://ppt.infoq.cn/list/149</a>"</p><p></p><p>以下内容根据对话整理，篇幅有删减：</p><p></p><h3>金融研发效能提升：关键因素与平衡策略</h3><p></p><p></p><p>余伟：金融行业在提升研发效能方面哪些因素最为关键，以及如何平衡这些因素？</p><p></p><p>岑润哲： 在金融机构的项目研发过程中，我认为有三个关键因素对提升研发效率有着显著影响。首先，也是最重要的，是人才素质。一个团队如果拥有既懂金融知识又精通技术和产品的复合型人才，那么无论是项目推广还是产品研发都将更加迅速和敏捷。这样的人才能够作为产品团队和研发团队之间的桥梁，将复杂的业务逻辑转化为技术语言，从而极大提升研发迭代和测试的效率。</p><p></p><p>其次，技术架构的选择至关重要。金融机构需要处理大量数据并应对高并发情况，因此，一个先进的技术架构能够确保系统在面对未来数据量增长时仍能稳定运行。在项目初期，预判未来几年内数据量的增长，并以此为基础设计架构，可以避免未来需要进行系统重构的情况，从而节省时间和资源。</p><p></p><p>第三，流程管控和项目管理同样关键。一个有效的项目管理机制，如敏捷开发的 Scrum 框架，以及一个负责任的项目经理，能够确保产品的设计和开发过程更加高效。项目经理可以引入短期迭代和持续反馈机制，提高资源利用效率，加强产品与技术人员的协作。</p><p></p><p>余伟： 在微众银行，有一个岗位，称为 科技产品经理，他们既懂技术又懂产品。这个角色在金融科技领域尤为重要，因为他们需要与业务团队紧密合作，理解业务需求并将其转化为技术解决方案。业务团队可能清楚自己需要什么，但不一定知道如何通过技术手段来实现这些需求。科技产品经理不仅要了解业务团队的需求，还要与技术团队沟通，确定如何利用现有架构或进行架构调整来实现这些功能。</p><p></p><p>敏捷和持续迭代 的理念并非所有人都能一致理解。过去金融机构可能需要数月才能推出一个版本，但现在通过敏捷开发，迭代周期缩短至两周甚至每周。这种转变需要团队成员对敏捷开发有清晰的认识，并接受持续学习的文化。此外，明确的责任划分和系统架构设计对团队的敏捷落地至关重要。</p><p></p><p>流程工具的自动化和基础建设，如持续集成 / 持续部署（CI/CD）、容器技术、测试环境和泳道，尤其是金融机构复杂的测试环境，以及与 AI 结合的自动化输出，对提升组织效率极为重要。</p><p></p><p>IT 能力的共享也是金融机构提高效率的关键。例如，一个风险模型可能在一个产品中得到应用，也可以作为公共服务被其他产品共享。这种能力共享对于避免资源浪费和提升研发效能至关重要。</p><p></p><p>金融机构与其他机构最大的区别在于风险管控和合规性。风险管控和合规性要求对研发效能有反向作用，高风险管控意味着需要做更多的工作来解决或降低风险，可能会影响研发效率。因此，在设计架构和产品时，需要提前考虑风险管理和监管要求，避免后期返工带来的损失。</p><p></p><p>敏捷文化、流程工具自动化、IT 能力共享以及风险管控和合规性，这四点对金融机构的研发项目集成有着显著影响，需要在实践中不断平衡和优化。</p><p></p><p>岑润哲： 在金融机构中，IT 能力的共享至关重要，特别是在数据分析领域。以我们数势科技的指标平台 SwiftMetrics 为例，这些产品的核心目标是帮助金融机构以更低的门槛进行数据分析。数据分析本身是可以在不同业务线之间共享的，例如风控模型在信用卡场景中有效，在保险代销或财务管理中可能就不再适用。我们希望我们的产品能够在金融机构的各个业务线中低成本复用，从而减少冗余开发和烟囱式架构的成本。</p><p></p><p>监管在金融机构中扮演着特殊的角色。在监管严格的环境下，金融机构可能不会尝试使用 AI 或大模型进行深入探索。例如，在风控领域，模型的可解释性至关重要，因为需要向借款人解释为何被拒贷或额度设置的原因，这就是为什么许多银行和金融机构仍然使用决策树，尽管它们可能不是最优模型，但具备可解释性。</p><p></p><p>在金融机构，尤其是监管严格的业务领域，先进技术的使用需要谨慎，因为风险合规的存在。如何在大模型的创造性和监管的规则性之间找到平衡，是未来需要重点关注的挑战。</p><p></p><h3>利用 AI、大模型与低代码，优化研发流程与加速产品交付</h3><p></p><p></p><p>余伟： 在银行业务中，用户与资金和征信相关的事务至关重要，准确性是关键，任何微小的差错都不允许。在这样的背景下，我们接下来要讨论的是 AI 与低代码技术在金融研发中的应用，这些技术是如何具体应用在金融研发中的，以及它们是如何帮助提升研发效能的？</p><p></p><p>岑润哲：AI 技术在金融领域的应用已经相当广泛，从智能客服、人脸识别、OCR 到风控等。随着大模型技术的发展，金融机构正在探索更深层次的应用场景。</p><p>数据分析：金融机构拥有庞大的数据资产，数据分析成为关键应用之一。传统上，业务方提出报表需求，数据团队进行开发，这不仅效率低下，而且成本高昂。数势科技公司擅长数据分析，通过结合大模型和指标语义层，使业务人员能够用自然语言进行数据分析，从而提高效率并减少数据团队的冗余开发工作。智能客服：在大模型技术的支持下，智能客服可以提供更高效的服务。金融机构拥有大量政策和产品文档，通过大模型的自然语言处理能力，可以提升客服的响应速度和质量，甚至构建完全自动化的智能客服系统。知识库管理：金融机构作为知识密集型行业，需要有效管理和调用历史知识文档。大模型可以帮助将非结构化文档转化为向量数据库中的信息，供业务方和客户经理使用。但这也带来了挑战，如文档版本管理和观点差异，可能导致大模型输出的不稳定性。代码辅助：在研发领域，大模型可以辅助编写代码，提供基础代码框架供开发人员修改和完善。此外，大模型还能将代码转换为流程图或泳道图，帮助产品经理理解代码逻辑，从而提高产品和研发团队之间的沟通效率。</p><p></p><p>余伟： 在微众银行，我们对数据分析的实践采取了一种新的方法，利用大模型技术来提高效率和精确度。例如，业务团队在策划营销活动时，可能需要了解在深圳存款超过 1000 元的人数。过去，这需要向科技团队提出需求，由他们编写 SQL 查询并开发相应的页面功能，这个过程耗时长且需要排期。</p><p></p><p>现在，我们通过大模型技术，业务团队能够直接与模型交互。他们可以向模型描述所需的条件和营销活动的目的，模型会生成类似 SQL 的查询结果。这些结果首先在准生产环境中进行基准分析，确保它们符合业务团队的预期和产品理解。如果结果符合预期，再将其转化为正式的需求，交由科技部门开发。</p><p></p><p>这种方法解决了几个问题：首先，它减少了业务团队因需求不明确而导致的资源浪费；其次，它允许业务团队在小范围内低成本尝试，验证产品特性的可行性；最后，它确保了业务团队的需求与科技团队实现的功能之间有良好的匹配。</p><p></p><p>岑润哲： 大模型在生成代码方面的能力确实为快速原型开发提供了显著优势。以往，业务方提出需求后，设计师需要花费一两天时间来设计图纸，而现在，通过大模型，我们可以迅速提炼出多个版本的设计方案。业务方可以直接从这些版本中挑选，这大大提升了产品开发和研究的效率。</p><p></p><p>无论是将文本转换为图像还是代码，大模型都具备这样的能力。尽管这些能力可能尚未达到完美，但在原型制作过程中，AI 技术的加速作用非常明显。这不仅加快了业务方的选择过程，还为 AB 测试等提供了便利，从而在整体上提高了业务效率。</p><p></p><p>余伟： 金融机构正在探索 AI 技术在多个方向的应用，包括利用 AI&nbsp;分析研究报告 来辅助投资顾问作出决策。研究报告来源广泛，有些容易获取，有些则相对分散。以往依赖有经验的分析师手动阅读和分析，但这种方法存在局限，尤其是对于缺乏经验的分析师，他们可能无法准确把握报告的重点。通过将这些报告交给 AI 大模型进行分析，可以生成标准化的输出，确保分析的全面性和准确性。</p><p></p><p>智能投资顾问也是 AI 技术应用的一个重要领域。通过算法，智能投资顾问能够提供个性化的投资建议和投资组合，满足不同客户的长期或短期需求。微众银行的 App 已经集成了这样的智能投资顾问服务，为客户提供定制化的产品推荐。</p><p></p><p>除了 AI 技术，低代码开发平台 也在金融机构中找到了其应用场景。低代码平台允许用户通过少量编码或零编码来快速构建功能，特别适用于产品管理台的开发。金融机构的每个产品都需要后台管理台，业务人员可以在管理台上执行查询、审批等操作。通过低代码平台，可以快速生成这些标准化功能，并与应用系统集成，实现快速开发。在移动端应用开发方面，低代码工具同样发挥着重要作用。利用 iOS 和 Android 平台提供的组件，低代码工具可以快速开发出功能原型，特别是在产品初期的体验版或演示版开发中。这种方式允许业务团队在没有具体数据支持的情况下，快速验证移动端功能的表现是否符合预期。</p><p></p><p>岑润哲： 在与多家金融机构的接触中，我发现他们已经采购了不少低代码工具，用于风控策略、营销策略等业务场景。低代码工具通常基于工作流（workflow）进行编辑，允许用户通过拖拽组件的方式快速构建应用，如 H5 活动页面。这些工具虽然功能强大，对业务方来说学习门槛仍然存在。一些金融机构的业务人员在使用这些工具时可能会感到复杂，这影响了工具的普及和应用效率。余老师如何看待这个问题？</p><p></p><p>余伟： 低代码平台在金融机构中的应用主要面向两类用户：一是非技术背景的业务人员，二是具备一定代码能力的科技同事。</p><p></p><p>对于业务人员而言，他们通常对代码不太了解，只有产品概念，因此学习使用低代码平台确实存在门槛。为了降低这个门槛，我们需要在易用性和可用性上做出更多努力，使得用户能够像使用普通互联网产品一样，通过简单的拖拉拽操作快速实现所需功能，而无需经过复杂的培训过程。</p><p></p><p>金融机构中使用低代码平台的主要是科技同事，他们利用这个工具来提升研发效率。低代码平台使得需求提出者能够快速看到接近最终形态的产品，从而更准确地确认是否符合预期。这种模式的价值在于金融机构人员结构的特殊性：业务人员面对客户，收集需求并翻译成需求文档，然后交给科技人员实现。业务人员通常只在需求提出阶段和产品即将面向客户的阶段参与，因此 低代码平台在中间起到了桥梁作用，帮助快速实现并验证需求。</p><p></p><p>通过 低代码平台完全组装一个新产品，尤其是涉及前端页面和后端服务的复杂串联，目前还有一定难度。金融机构的产品通常包括前端展示和后端逻辑两部分，如果这些组件能够标准化，前端的页面组装和后端的微服务组装相对容易实现。但中间的逻辑串联过程往往需要人工参与，因此完全自动化的流程编排和快速上线尚未达到成熟阶段。</p><p></p><p>岑润哲： 去年，我们尝试将低代码工具和 API 交给大模型，希望它能进行编排调度。但很快我们发现，这一过程存在困难。Workflow 的编辑具有强烈的业务逻辑性，如果大模型不了解金融机构的业务逻辑，它就无法将这些 API 有效串联起来。</p><p></p><p>目前大模型在自动化生成后端代码，尤其是涉及复杂业务逻辑的代码方面，仍然面临挑战。这需要一个既懂业务又懂技术的中间人来介入，无论是通过低代码平台还是传统编码方式，都需要这样的人来组装业务逻辑，以形成一个完整的产品并交付。</p><p></p><p>余伟： 接下来我们讨论下 如何优化从需求收集到产品交付的整个研发流程，以缩短交付周期并提高质量。</p><p></p><p>在传统的研发流程中，工作通常分为几个主要阶段：首先是需求收集，将需求转化为业务和技术人员都能理解的需求文档；接着是将业务需求转化为系统需求；然后是详细设计、编码、单元测试；最后是移交到系统集成测试（SIT）、用户验收测试（UAT）、回归测试，直至上线发布。</p><p></p><p>为了缩短这一流程，敏捷开发方法被广泛采用。通过将原本可能一个月的迭代周期缩短至两周，一些需求能够更快地上线。例如，某些需求可能仅需 3 天开发加 1 天测试，4 天后即可发布。而更复杂需求可能需要两周开发和一周测试，总共三周才能上线。通过这种方式，大的需求和小的需求可以搭配进行，使得整体的交付时间缩短。</p><p></p><p>在需求量保持不变的情况下，通过缩短交付周期，可以提高整体的研发效率。金融机构普遍采取这种策略，将交付周期进行合理划分，快速上线小需求，而大需求则按照正常流程处理。这对研发管理人员提出了更高的要求，他们需要能够识别哪些需求可以快速交付，并能够将人力资源有效分配到这些快速迭代的开发任务中。随着研发任务的细分，研发人员的管理也变得更加复杂。可能需要将团队分成多个小分队，每个小分队针对不同的需求进行快速响应和服务。</p><p></p><p>岑润哲： 在我们的公司内部，从需求收集、研发设计到单元测试、UAT 和回归测试的整个流程中，我认为 AI 可以在测试环节发挥重要作用。特别是在我们公司开发的 SwiftAgent 产品中，该产品允许用户通过自然语言进行数据分析。在这种情况下，测试的成功率至关重要，因为这是一个基于自然语言交互的产品，与图形用户界面（GUI）不同，它具有不确定性。</p><p></p><p>设计全面的测试用例集对于评估 AI 产品的使用效率和成功率非常关键。例如，在智能客服场景中，不同类型的问题回答率是否达到预期阈值，这就需要精心设计测试问题。AI 产品测试中，如何设计问题和持续优化不良案例是一个重要环节。</p><p></p><p>大模型可以通过提供测试问题来提高测试效率。测试人员可以基于示例问题，让大模型生成多种问法，从而测试 AI 产品的响应能力。例如，提供一个问题示例，大模型可以生成 10 种不同的问法，甚至 100 个问题，这样测试人员就不需要自己构思问题，从而显著提高了测试 AI 产品的效率。此外，大模型还能够通过举一反三的方式，帮助我们从不同角度评估 AI 产品的性能。这种基于大模型的测试方法不仅可以提升测试效率，还能够提高产品的成功率和召回率。</p><p></p><p>余伟：AI 产品的核心期望是输入特定信息后，通过大模型的处理得到正确且符合预期的输出结果。那么我们如何判断 AI 输出的有效性和合理性？</p><p></p><p>岑润哲： 我们的分析产品实质上采用了 Text to API 的逻辑。用户用自然语言表述请求，例如“请帮我分析一下近三个月的账单金额”，我们的系统不会直接将其转换成 SQL 查询语句，因为 SQL 逻辑相对复杂且可能不准确。我们的产品逻辑分为两个阶段。首先是语义理解阶段，系统需要识别用户输入的自然语言中的关键要素，如时间范围（1-3 个月）和指标（账单分析金额）。这可以通过脚本检查来验证大模型是否正确理解并翻译了用户的意图。</p><p></p><p>第二阶段是数据调用阶段，系统将识别出的关键要素转换成 API 调用所需的半结构化数据，如 JSON 格式。我们需要验证大模型生成的 JSON 或半结构化数据结构是否正确，因为这一步骤可能会有不稳定性。即便是先进的模型如 GPT-4，也可能在生成 JSON 时出现随机性或不稳定性。为了提高准确性，我们通过对齐方式来确保生成的数据结构尽可能符合 API 调用所需的 JSON 入参格式。这有助于后续的数据调用过程更加顺畅。</p><p></p><p>最近，GPT-4o 发布了一项新功能，即代码对齐能力，这意味着生成的 JSON 将严格符合语法规范。这一功能如果能够实现，将极大降低代码生成层面的幻觉问题。如果 GPT-4o 的这一新功能能够确保生成符合 API 入参的 JSON 结构，那么在准确性上就能达到 100%，从而提高产品在多种场景下的应用可行性。</p><p></p><p>在产品实现逻辑上，一个关键点是确保大模型不仅能理解用户的话，而且能 将其理解过程反馈给用户。例如，如果用户询问产品的产品经理是谁或产品的起购日期，如果大模型只给出直接答案，用户可能会怀疑其可信度。因此，展示大模型的思考过程，包括它是如何得到这个结果的，即所谓的"白盒化"，对于赢得用户信任至关重要。</p><p></p><p>在数据分析方面，一旦指标定义清楚，只要数据准确，通常不会有问题。但文档查询就更具挑战性，因为可能存在多个版本的文档，观点可能相互冲突。如果用户提出问题，而两个文档的观点相反，大模型可能无法做出判断。在这种情况下，将所有相关文档召回供用户选择可能是一种更好的方法。</p><p></p><p>这种“思维链”（Chain of Thought, COT）的透明化，使用户能够理解大模型是如何得出结论的，从而增加了对结果的信任。这与互联网企业中的推荐系统类似，用户不仅希望获得良好的推荐体验，还希望了解推荐背后的逻辑，尤其是在金融产品推荐中，这一点更为重要，因为金融产品涉及合规性和风险管理问题。例如，如果一个稳健型客户被推荐了一个高风险产品，这显然是不合规的。</p><p></p><p>大模型如果能够作为用户自然语言和技术语言之间的桥梁，将提升 AI 产品的可信度，使用户更愿意使用。确保 AI 具备足够的可解释性，是 AI 产品推广和普及的重要一步。这是 AI 产品能否被广泛接受和使用的关键因素。</p><p></p><p>余伟： 在微众银行，我们探索了多种方法来缩短从需求收集到产品交付的整个流程。我们从统一的研发节奏转变为根据需求和产品类型采用不同的研发节奏，以实现更高效的交互。</p><p></p><p>组织结构上，我们也在尝试引入解决方案层，特别针对 ToB 产品在与金融机构对接时可能遇到的非标准化、研发周期长的问题。如果没有解决方案层，产品直接与金融产品对接，就需要同时满足业务系统和金融机构产品的排期，这可能导致产品研发周期非常长。</p><p></p><p>解决方案层的引入，可以将对接工作分成两部分：一部分是与业务方的对接，另一部分是与银行科技团队的对接。这样的方法可以最小化双方的改动，而解决方案层则承担适配的工作。这种适配相比直接在成熟产品上进行调整要快速得多，从而大大缩短了 ToB 产品的交付流程。</p><p></p><p>当前，许多金融机构不仅做 ToC 业务，也开始大量涉足 ToB 业务，面临提高 ToB 产品交付效率的挑战。我们内部进行了许多讨论，集思广益寻求解决方案。</p><p></p><p>岑润哲： 数势科技为金融机构提供多种软件和产品服务。面对金融机构多样化的需求，我们意识到不能仅仅依赖标准产品。为了更敏捷地交付并满足定制化需求，我们通过组件化和配置化来提高服务的灵活性。这个关键在于构建一个能力中台，它允许我们通过组装和拼接组件来形成满足不同需求的产品。这样，即使面对不同机构的不同需求，我们也无需对 PaaS 层进行大量修改。通过 PaaS 层构建的服务可以被复用，为不同的金融机构提供定制化的解决方案。</p><p></p><p>避免“烟囱式”开发，避免每次开发都从头开始，导致代码重复且难以共享，对我们来说是至关重要的。中台能够高效地重用已有的软件组件和服务平台，从而减少重复工作，加快交付速度，并提升产品的质量和一致性。</p><p></p><p>余伟：研发团队的文化建设对于整个团队的运作至关重要。不同的角色在团队中承担着不同的职责。例如，在编写需求文档的过程中，有些团队可能由业务人员负责，而有些则由产品经理来完成。开发和测试人员在面对不完整的需求时，常常会抱怨需求描述过于简单或频繁变更，这增加了工作的复杂性。这种抱怨并不有助于研发流程的正向发展。每个团队成员虽然有明确的岗位职责，但在需求不完善的情况下，团队成员应更积极地参与到需求的快速转化中。</p><p></p><p>例如，在开发支付产品时，一些基本功能如代收、代付签约是支付渠道必须具备的，这些功能的变化范围有限，团队可以基于这些共通点提前开始开发工作。团队成员在面临依赖问题时，应主动沟通和规划。如果上游工作未能及时提供所需支持，团队成员应提前与依赖方沟通，明确自己的需求和计划，以便双方可以共同协商解决方案。</p><p></p><p>我们鼓励团队成员主动推动产品的落地和面向客户的进程。这种主动性体现在将产品视作自己的责任，展现出主人翁精神，不仅关注自己的任务，还考虑到如何帮助依赖方，甚至在提供支持之前，就给出建议和指导。</p><p></p><p>通过这些实践，我们的团队文化已经从严格的角色划分和等待依赖转变为更加主动和负责任的态度。团队成员开始将产品视为自己的一部分，积极推动产品的发展，并以更加开放和协作的方式与其他团队成员一起工作。这种文化的建设对于提高研发效率和产品质量具有重要作用，我对此深有感触，并且认为这对于团队的长期发展是非常有益的。</p><p></p><h3>打破部门壁垒，传统银行模式如何转向敏捷协同</h3><p></p><p></p><p>岑润哲： 余老师的分享非常中肯，尤其是在互联网银行领域，敏捷文化是其核心特质之一。然而，在与传统金融机构合作进行产品交付时，我们常会发现这些机构可能存在部门间的隔阂，即所谓的“部门墙”。这在一定程度上影响了敏捷迭代和协同工作的效率。对于 ToB 业务，尤其是面对那些可能没有充分采纳互联网思维的传统金融机构时，提升研发效率的关键在于如何打破这些障碍，实现敏捷协同，余老师在这方面有哪些经验可以分享吗？</p><p></p><p>余伟： 在我们团队中，我们经历了从传统银行模式向敏捷文化的转变。这种转变体现在团队成员开始主动与依赖方沟通，提前明确需求和交付时间，以及他们能为对方提供的支持。这种变化不仅提升了团队的交互效率，也使团队成员更加积极地参与到产品的整个生命周期中。我们通过以下几个方面来推动这种文化转变：</p><p>行业最佳实践分享：我们寻找行业内的优秀案例，与团队成员分享，让他们了解其他团队的成功经验，从而激发团队改进的动力。产品驱动：在新产品的研发初期，我们就提出高效率的要求，鼓励团队想象在没有历史包袱的情况下，如何实现更快速的交互和更高效的工作。文化驱动：通过在新产品和服务中尝试敏捷实践，影响那些固守传统研发模式的团队，促使他们认识到改变的必要性。角色融合：在一些团队中，我们尝试减少角色划分，让开发和测试人员承担更多职责，如需求拆解、架构设计、项目管理、培训等，从而提高团队的灵活性和效率。我们鼓励团队成员拓宽自己的职责范围，不再局限于单一角色，而是有机会尝试和体验不同的工作内容，这样不仅提升了个人能力，也为团队带来了新的视角和解决方案。我们在小范围内试验这些新的做法，目前已经取得了超出预期的效果。</p><p></p><p>岑润哲：在我们开发数据分析类的 AI 产品过程中，有时会发现技术团队，特别是算法团队，提出的产品想法可能比产品经理的更出色。这是因为算法人员对算法的潜力和局限有深刻的理解。即便他们提出的 10 个想法中有 9 个不可行，基于对算法底层逻辑的了解，他们仍有可能提出一个非常符合用户需求的产品设计理念。</p><p></p><p>在当前快速迭代的环境中，无论是产品经理、测试人员还是研发人员，如果能够横向扩展自己的能力，成为所谓的“T 型人才”或“π型人才”，即拥有深度专业技能的同时也具备广度的跨领域知识，确实能够提高团队的研发效率，并推动团队文化的发展。这样，测试人员能够更好地理解研发的工作内容，研发人员也能更准确地判断产品需求的合理性。每个成员不仅在自己的专业领域内精益求精，也能够对其他领域有所涉猎和理解，这种多元化的技能组合对于产品创新和问题解决都具有积极的影响。</p><p></p><p>余伟：在微众银行，我们的研发团队与业务团队建立了定期沟通的机制，比如每周或每两周举行一次会议。这些会议的特点是跨部门参与，不仅业务团队的成员会参加，科技团队的成员也会参与进来。</p><p></p><p>最初，这类会议主要由科技产品经理参与，目的是为了理解业务需求和业务目标。但随着时间推移，我们扩展了参与角色的多样性，包括一些核心开发人员也会参与会议。通过这种方式，开发人员能更直接地了解用户和业务的反馈，真实地看到自己开发的产品在业务落地时的效果，并思考在产品开发过程中如何做出改进以支持业务的更好发展。</p><p></p><p>这种长期坚持的做法我们已经实施了两三年，每周都会举行这样的会议。团队成员聚在一起，听取业务团队对产品未来发展的规划和期望。这样的机制不仅促进了科技与业务之间的沟通和理解，而且也帮助项目更加顺利地进行。</p><p></p><p>岑润哲：AI 产品具有一个独特的优势，即能够更直接地理解用户需求。 用户通过自然语言提出的请求直接反映了他们实际的需求，这些请求作为日志记录在后台，为研发团队提供了宝贵的原始数据。通过对这些文本进行分析，研发团队可以清晰地了解用户的疑问和需求，这大大缩短了终端用户与研发团队之间的距离。这种直接从用户输入中获取需求的方式，与传统的 GUI 产品形成鲜明对比。在 GUI 产品中，用户可能需要通过点击多个按钮来完成操作，而他们的真实需求却不一定能够被准确捕捉。这通常需要额外的市场调研。AI 产品通过自然语言处理，能够直接从用户的提问中提取需求，无需额外的调研步骤。</p><p></p><p>此外，如果能够将成千上万条用户的自然语言需求进行抽象和分析，就可以为产品的优化方向提供指导。这种基于用户实际提问的迭代思路，可以更准确地反映用户需求，从而提高技术团队与产品团队之间的协作效率。例如，当前流行的 ChatGPT 以及基于自然语言处理的其他应用场景，都展示了用户输入即需求的直接性。这种直接性不仅提高了产品开发的针对性和效率，也使得产品迭代更加贴近用户的实际使用情况和需求。</p><p></p><h3>减轻金融研发中的技术债务负担</h3><p></p><p></p><p>余伟： 接下来我们继续聊下一个话题：技术债务的管理与研发效能，探讨技术债务对研发效能的影响，以及如何有效管理和减少技术债务。</p><p></p><p>岑润哲： 在我之前所在的互联网大厂，我们确实面临技术债务引发的问题，例如代码可读性差、维护成本高，以及新功能开发受阻。业务方可能会紧急提出需求，要求加入限制条件，而研发团队为了满足这些紧急需求，有时会采用临时技术方案，导致技术债务的积累。这种债务会使得后续的升级和扩展变得非常困难，并需要花费大量时间和资源进行重构。</p><p></p><p>为了应对这些问题，我们采取了以下措施。</p><p>定期技术评审：通过技术评审来检查新上线的功能，判断是否存在临时性功能，以及这些功能是否会对未来技术架构产生影响。执行代码审查流程，确保代码质量和避免技术债务的产生。预留资源：在产品或活动策划中预留一定比例的资源，专门用于偿还技术债务。项目经理需要在项目排期中预留人 / 天资源，用于偿还技术债务。定期复盘：定期回顾项目，分析为何会产生技术债务，以及如何避免临时性开发。进行数据分析，了解技术债务的具体情况，明确净债务量，以便采取措施降低比例。技术债务追踪：建立技术债务台账，定期追踪和分析标准产品迭代需求、定制化开发和临时开发的比例。需求分级：对需求进行分级管理，区分标准产品迭代和临时填坑需求，确保它们的比例合理。使用需求管理工具进行复盘分析，量化评估技术债务的影响。</p><p></p><p>余伟： 技术债务是研发过程中需要重点关注的问题，它涉及产生的原因、量化的方法以及解决的优先级。在管理技术债务时，我们一方面使用管理手段来督促团队成员主动解决技术债务问题，这是一种被动的督促方式。另一方面，我们通过激励措施来鼓励团队成员解决技术问题，尤其是那些对复杂系统有深入影响的技术栈问题。对于成功解决这些问题的个人或团队，我们会量化他们创造的价值，并通过奖项或奖励来给予表扬和鼓励。</p><p></p><p>在金融行业，技术债务可能涉及数据处理、交易处理和安全性问题，尤其是用户数据和合规性安全，这些都是至关重要的。未能及时处理的技术债务可能会导致金融风险，甚至引发危机。例如，生产环境中的慢 SQL 处理、未及时关闭的临时开关、异常用户数据未得到妥善处理等，这些都可能带来法律风险。金融行业的研发团队必须持续面对解决技术债务的问题。我们甚至有时会暂停产品功能上的新交付，专注于清理技术债务，以确保产品的安全性、合规性和稳定性。</p><p></p><h3>如何度量与优化金融研发周期与代码质量</h3><p></p><p></p><p>余伟： 接下来我们讨论下如何度量研发效能，并基于度量结果进行持续改进。</p><p></p><p>岑润哲： 在评估研发效能时，我们通常从两个主要方面来考虑：</p><p>项目交付周期：这是指从项目开始到最终上线的整个时间跨度，是一个非常重要的指标。代码质量：通过代码审查和评分系统来衡量，包括是否遵循了编码规范、是否存在性能问题、以及代码的整体质量。现在，大模型也可以帮助理解代码并识别潜在问题，将代码质量以量化得分形式展现，为优化提供依据。我们也会衡量产品上线后出现的 bug 数量和严重程度，包括前端和后端的问题。通过统计分析，我们可以计算不同级别 bug 的加权平均值，得到一个整体的得分。团队文化和满意度：虽然这不容易量化，但通过问卷调查和团队成员之间的评价，可以评估团队成员的相互满意度和配合程度。团队文化的重要性不容忽视，因为它直接影响团队成员之间的交流和协作。产品经理、研发和测试团队之间的协作默契对于项目成功至关重要。如果团队之间存在分歧或沟通不畅，即使代码质量很高，最终也可能出现问题。</p><p></p><p>综合这些方面，我们可以建立一套完整的指标体系来评估研发效能。这不仅包括项目的周期和代码质量，还包括团队文化的强度和团队内部的协作情况。只有当所有这些因素都达到一定标准时，我们才能全面提高研发效能。</p><p></p><p>余伟： 在度量研发效能时，我们采用的指标并非固定不变，而是动态的。大约 30% 的度量指标会定期进行滚动式更新。这种动态性是必要的，因为一旦指标被定义，人们总有可能找到方法来规避它们，使自己或团队在指标上的表现不至于太差。这是人的本性，我们不逃避这个事实，在度量指标管理上，我们有以下三个方式。</p><p></p><p>度量指标的动态化：我们建议在制定度量指标时，要考虑其动态化，新的指标可以与其他现有指标相互佐证。如果在某个方向上的指标表现很好，而在另一个方向上表现不佳，这种差异需要检视和分析。重视度量指标：为了让大家重视度量指标，我们开发了度量平台，并公开度量数据。但仅仅公开数据还不够，有些人可能不会关注这些数据。为此，我们采取了一些管理手段，比如定期召开研发效能专题会议，每月将各个产品团队的度量指标数据公开，让大家了解自己的表现，并识别出与预期有偏差的地方。度量指标的反馈机制：度量不仅仅是一个结果的展示，更重要的是建立反馈机制。我们希望团队在看到度量结果后，能够采取行动。如果团队认为自己的表现一般，我们不希望他们满足于现状，而是希望他们能够基于度量结果提出新的需求，深入分析数据，辅助决策，甚至对未来的风险进行预警。对于那些表现不佳或未达预期的团队，我们不仅仅通过度量指标来反馈问题，而是通过更多的方式来告诉他们需要改进和调整。我们的目标是让研发效能度量真正推动团队在质量、效率上实现滚动式提升和发展。</p><p></p><p>岑润哲： 指标的深入分析对于研发团队至关重要，因为不同产品形态和需求导致单一的缺陷率指标，如 1% 或 20%，并不能直观反映研发的实际表现。我们需要根据产品的不同维度进行细化分析，并将分析结果与产品的实际价值直接呈现给研发团队，这不仅有助于他们了解自己的工作效果，也是一种激励。</p><p>我们开发的工具如果能够帮助业务团队提升分析效率，那么将具体的用户故事反馈给研发团队，如他们研发的产品如何帮助金融机构的客户经理节省时间，可以显著提升研发人员的成就感。这种成就感来源于他们能够直观地看到自己工作的成果和对实际业务的影响。</p><p></p><p>当前，许多研发人员在编写代码后，并不十分清楚自己的代码如何被使用以及产生了哪些实际效果。如果能够让他们参与到产品的实际应用中，了解他们的工作如何帮助解决具体问题，那么这种正面的用户反馈和成功案例可以极大地提高研发人员的积极性和主观能动性，进而推动他们在未来的工作中更加投入和创新。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/e2wELuusIV51kgdpSyRr</id>
            <title>2024 FCon全球金融科技大会精彩回顾，汇集前沿视野与落地实践｜附PPT下载</title>
            <link>https://www.infoq.cn/article/e2wELuusIV51kgdpSyRr</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/e2wELuusIV51kgdpSyRr</guid>
            <pubDate></pubDate>
            <updated>Wed, 21 Aug 2024 05:46:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>8 月 16 日 -17 日，由极客邦旗下 InfoQ 中国主办的<a href="https://fcon.infoq.cn/2024/shanghai/">第二届 FCon 全球金融科技大会</a>"在上海成功举办。本次大会以“科技驱动，智启未来——激发数字金融内生力”为主题，由中国信通院铸基计划作为官方合作机构，数势科技、未来智能、selectDB、枫清科技 Fabarta、亚马逊云科技和英特尔赞助支持。</p><p></p><p>在“十四五”即将收官之际，本届大会特别邀请了行业内各领域专家，共同审视当下的数字化转型现状，为数字化大考“查缺补漏”。同时，紧跟当下技术热点，众多企业也分享了近一年多以来金融行业在 AI 大模型方面的落地实践成果。</p><p></p><p>2 天大会期间，共举行了 1 个 Keynote+12 个并行专题论坛，聚集了 60+ 顶尖专家，来自龙盈智达、平安证券、度小满金融、汇丰科技、工商银行、交通银行、工银科技、华夏银行、中信银行、广发银行、北京银行、苏州银行、渤海银行、富滇银行、人保寿险、平安产险、蚂蚁集团等银行、保险、证券和金融科技企业的专家分享了各自领域的经验探索。</p><p></p><p>去年底，中央金融工作会议提出了做好“五篇大文章”的要求，成为今年金融机构工作布局的重点方向。然而，经过数月来的探索和实践，仍有不少机构对于其中涉及的核心概念和关键抓手不是非常明晰。对此，中国信通院泰尔终端实验室数字生态发展部主任王景尧围绕金融“五篇大文章”及数字化成熟度路径进行了拆解，他表示，“五篇大文章”的目标是提升金融服务实体经济质效，以金融高质量发展助力强国建设。在这个过程中，中国信通院主要发挥支撑“有为政府”建设，服务“有效市场”的作用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4e/4ea2c9d868f6d672813a605810ec9450.jpeg" /></p><p>中国信通院泰尔终端实验室数字生态发展部主任 王景尧</p><p></p><p>而随着数字化迈入新阶段，金融机构开展数字化的核心价值诉求转向深刻的经营变革。当前，金融科技需要解决的是金融机构何以成功、如何思考决策的问题，革新的是金融机构经营的内核。龙盈智达副总裁宫小奕在主题演讲中介绍了龙盈智达如何以场景驱动业技融合，实现自身在金融科技领域的实践和成果。她指出，顶层战略混沌、路径依赖严重、价值共识缺失、人才结构错配、忽视生态协作这 5 项核心挑战是阻碍机构构建新时代核心竞争力的主要因素。对此，金融业创新需要遵循以场景为驱动，坚持业务场景的准确把握、数据价值的有效释放、新兴技术的合理选择、割裂能力的整合拉通四个关键原则。</p><p></p><p><img src="https://static001.geekbang.org/infoq/67/6741fafc8275ceb98a4be9d797d89f72.jpeg" /></p><p>龙盈智达副总裁 宫小奕</p><p></p><p>那么，在这个过程中，数字化转型是 IT 还是业务牵头？平安证券公司首席信息官张朝晖给出了他的答案——可以 IT 先行，完成数字化转型，产出最佳实践，启发业务数字化转型灵感。在平安证券数字化转型过程中，其技术部门通过 “微卡片”组装式无边界应用开发模式，改变了传统研发模式难以满足数字化需求的困局。作为一个容器，微卡片是技术部门为业务搭建的众多系统中的每一个服务对应的前端业务呈现，它们既可以作为独立的模块独立使用、分享或嵌入其它页面，也可以和不同卡片灵活组装到不同的应用场景，一次创作，多次复用。基于 OPTIMAL 数字化转型方法论，平安证券内部目前的微卡片数量越来越多，也积累了越来越丰富的应用场景案例，已经成为公司的现象级应用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a8e0f0f2b87da22d46d5d746bc92b7e.jpeg" /></p><p>平安证券公司首席信息官 张朝晖</p><p></p><p>此外，人工智能也已经成为企业在数字化转型过程中不可或缺的一个技术，那么具体而言，人工智能如何实现在金融场景的落地呢？作为 FCon 连任联席主席，度小满金融技术委员会执行主席、数据智能应用部总经理杨青分享了“人工智能，助力书写数字金融大文章”的主题。他表示，生成式 AI 正在以嵌入、辅助和共生的形式重塑金融业未来格局。比如，在基础能力嵌入层面，多模态大模型驱动通用文档智能，解决了狭义文档智能框架可处理输入单一、提取流程繁琐和定制化成本高的痛点；在智能助手辅助层面，大模型智能理财投顾 Agent 能够模仿人类理财师工作流程，提供个性化、普惠化的专业投资服务；在人机共生层，生成式 AI 还将智能客服升级到 3.0，能够集成多种模态交互方式，端到端减少中间环节，促成更连贯的用户对话体验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7c/7c097e669640bcb40cca6f771b7131a0.jpeg" /></p><p>度小满金融技术委员会执行主席、数据智能应用部总经理 杨青</p><p></p><p>但是从新技术诞生、发展到应用，整个过程也需要直面其中的各种挑战。尤其是对于金融行业而言，在持续进行创新的过程中，必须守住合规和安全的底线。在演讲中，汇丰科技创新实验室量子和 AI 科学家朱兵介绍了金融中面临的新兴技术风险。拿 AI 大模型来说，朱兵认为，金融机构应该在引入新能力与自身的风险框架、风险承受能力和市场接受度之间进行权衡。在某些情况下，AI 技术有潜力通过改善的数据驱动带来的洞察以降低风险。再以量子技术为例，尽管量子技术在近些年取得了显著进展，但在大规模应用之前仍然面临许多重大挑战，这既有技术本身的高门槛和高度不确定性，也由于人们还未做好准备去面对具有如此强大能力的技术。金融企业和组织必须在新兴技术如人工智能、区块链和量子计算等的好处与安全、伦理和治理相关的风险之间取得平衡。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a171150b87d329e354d8b02344e9708.jpeg" /></p><p>汇丰科技创新实验室量子和 AI 科学家 朱兵</p><p></p><p>无论如何，金融创新的步伐仍将持续加速。在本次大会上，中国信通院铸基计划联合 InfoQ 研究中心还发布了《AGI 在金融领域的应用实践洞察》报告，其中显示，2030 年金融 AGI 市场将达到 887.3 亿元人民币， 增长率从 2024 年起未来 4 年保持 100% 以上。然而，在具体落地应用中还存在技术、数据、资源、合规性及监管四大挑战。对此，InfoQ 研究中心研究总监、首席分析师姜昕蔚指出，金融机构可以分 3 步化解：第一，根据投入产出比进行应用评估思考；第二，根据自身情况设定清晰的中间业绩指标和过程指标；第三，选择一个具体场景作为试点，设定一个“破冰期”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/11/11bec5f69ae6cfccf8d8ed6ebf549cfb.jpeg" /></p><p></p><p>除了 Keynote 主题演讲之外，本次大会还策划了多元化的专题论坛内容，大会现场气氛热烈，不少与会者反馈，此次大会的话题从宏观政策、中观场景、微观技术等多维度出发，兼具行业深度、技术视野与落地实践，为其日常工作开展提供了具有价值的参考。</p><p></p><p>其中，10 余个 AI 大模型相关的演讲议题关注度最高，在新技术持续更迭向前的背景下，寻找价值场景成为众多金融机构当下关注的重中之重。“同行交流可以佐证自己的观点”，“如果智能化不能尽快完成赋能，不出 3 个月就会掉队”，“之前觉得大模型离场景落地挺远的，听完思路变得清晰了，打算先从小而美的场景做起来”，与会者表示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a0/a05d910c63e526051972c5c95ee16689.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/15/156cfed57a7715a97ce5d104060b7cdb.jpeg" /></p><p></p><p>而在大模型烈火烹油的氛围中，我们仍然策划了多个与业务场景数字化以及数字化价值杠杆相关的专场，如数字化管理和运营、数字化风控、数字化营销、数字化人才培养，以及低成本高杠杆的数字化实践。对于金融机构技术从业者而言，既要有看见未来的深谋远虑，也要关注当下的落地实践，业务技术的融合和效果闭环，仍任重道远。</p><p></p><p>与此同时，作为金融数字化转型的技术基底，研发效能提升、IT 架构智能化、现代化核心系统建设等专场也吸引了大量听众的参与。在金融业务创新以及 AI 大模型技术的变革背景下，企业技术基础的夯实和持续迭代升级，无法快进更无法跳过。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f2/f2c8e4a7183a6ff7b0b2b6a2e7af4626.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/c6/c6dee45fd89b133089e9956f5efdb142.jpeg" /></p><p>经统计，大会现场听众累计超过 500 人次。我们深感荣幸与欣慰，感谢每一位专家贡献的知识和智慧，感谢每一位参与者的支持与鼓励。正因为有了大家的热情参与，我们才能不断前行，继续努力成为技术传播领域的佼佼者，持续提升内容质量，打造更加优质的交流平台，共同推动技术领域的创新与突破。</p><p>至此，今年 InfoQ 中国已圆满落幕 5 场技术盛会，随后还将于 10 月 18 -19 日举办上海站的 QCon 大会。如您感兴趣，可点击「阅读原文」查看更多详情。</p><p></p><p>大会 PPT 获取通道已开启（由于讲师所在企业限制，部分 PPT 仍在审查或不对外公布，详情见大会官网日程）：<a href="https://ppt.infoq.cn/list/149">https://ppt.infoq.cn/list/149</a>"</p><p></p><p>期待下一场大会再见！</p><p><img src="https://static001.geekbang.org/infoq/95/953dd5ff61bc3e6855dda5b86aa7f383.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ruTj57meeSg8SMsXKdcQ</id>
            <title>AICon 上海站精彩回顾，从大模型变革之路到高效“炼丹”指南，超 60 位大模型先锋输出最前沿干货！| 附PPT下载</title>
            <link>https://www.infoq.cn/article/ruTj57meeSg8SMsXKdcQ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ruTj57meeSg8SMsXKdcQ</guid>
            <pubDate></pubDate>
            <updated>Tue, 20 Aug 2024 12:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>8 月 18 日 -19 日，由极客邦旗下 InfoQ 中国倾力打造的 <a href="https://aicon.infoq.cn/2024/shanghai/">AICon 全球人工智能开发与应用大会 2024（上海站）</a>"圆满举办，盛况空前！与会嘉宾阵容强大，既有行业领军人物深入探讨大模型带来的变革及其深远影响，也有技术大咖剖析最新的落地思考和实践案例，到场的每一位观众都受益匪浅。</p><p></p><p>大会现场， 60 多位来自字节跳动、华为、阿里巴巴、微软亚洲研究院、智源研究院、上海人工智能实验室、蔚来汽车、小红书、零一万物等顶尖企业与研究机构的资深专家汇聚一堂，全方位剖析大模型的训练与推理机制、多模态融合技术、智能体 (Agent) 的前沿进展、检索增强生成 (RAG) 策略以及端侧人工智能应用的最新动态，并带来 AI 和大型模型在各种落地场景下的应用案例和最佳实践分享，帮助与会者提升技术视野、获得有价值的实践指导。</p><p></p><p>在本次大会的开幕环节，我们荣幸地邀请到了上海市邮政管理局党组书记、局长冯力虎为大会带来开场致辞。冯力虎表示，上海是开放之都，鼓励和欢迎与前沿科技相关的探讨，希望本次 AICon 大会能够成为一个新的起点，激发更多的创新火花。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/0f/0fe7202e87613e179804495c03cd2220.jpeg" /></p><p></p><p></p><p>当前 AIGC 大模型主要是文字、语音、图片等模态为主，在内容创作、辅助设计、知识内容创作辅助设计问答等场景不断出现创新应用。以供应链和物流为核心的运营和决策优化环节中，如何能有效利用大模型能力及其背后的技术？顺丰科技副总裁唐恺在题为《揭秘顺丰物流决策大模型》的主题演讲中，深入介绍了顺丰在物流领域的技术创新与应用。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/e7/e7d891cf9efa7f3a8e88d1e9f0bcc71f.jpeg" /></p><p></p><p></p><p>唐恺介绍，供应链运营是一个专业程度很高且非常严谨的领域，但当前大模型的一些缺点限制了其发挥。为此，顺丰结合大模型和传统小模型来构建供应链业务专家 + 技术专家多智能体，并通过 RAG 召回供应链知识库和数据检索来改善幻觉，同时利用多模态信息进一步提升传统领域模型效果，通过物流决策模型突破模态限制、直接作用于核心决策问题。</p><p></p><p>随后，上海市邮政管理局党组书记、局长冯力虎，顺丰集团副总裁龚威、顺丰科技副总裁唐恺、零一万物联合创始人祁瑞峰、智谱 AI 副总裁吴玮杰、华为云盘古大模型 CTO 李寅、浙江大学管理学院副院长杨翼，以及极客邦科技创始人兼 CEO 霍太稳，共同登台联合发布顺丰物流决策大模型，并一齐见证这一物流行业创新的重要时刻。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/e3/e3aa402896224756d395db9b6fa85270.jpeg" /></p><p></p><p></p><p>在接下来的主题演讲中，蔚来创始人、董事长、CEO 李斌深入介绍了蔚来近年在智能电动汽车和 AI 方面的思考与实践。李斌表示，“AI 将成为智能电动汽车企业的核心基础能力，车是大模型最佳的落地场景。”据介绍，在蔚来智能电动汽车的技术全栈中，AI 和所有的技术栈都有交集。其中， 智能驾驶无疑是汽车 AI 综合能力的反映，而智能驾驶的技术发展史就是算法空间理解和处理能力的进化史，因此蔚来决定直接走向基于视频的端到端世界模型，这一路径的信息损耗最小。李斌表示，蔚来的智能驾驶世界模型 NWM（NIO World Model）能在 0.1 秒内基于全量数据模拟出 216 种可能轨迹，评估后找出最优解。从 NWM 的技术角度来讲，其本身就是一个多元自回归时空生成模型。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/ef/efba22a21102c9007c433f7d6a19369d.jpeg" /></p><p></p><p></p><p>今年内，蔚来将利用 NWM 实现端到端上车。此外，李斌谈到 AI 在车上的另一个重要应用：智能座舱。他认为，车在未来会成为人的情感伙伴，今年蔚来的 NOMI GPT 大模型全量上线，目前具备 2000 项技能，累计用户聊天互动次数达 15680260 次。李斌在演讲最后称，“一个成功的智能电动汽车公司，一定是一家成功的 AI 公司。”</p><p></p><p>英特尔院士、大数据技术全球 CTO 戴金权在题为《大模型的异构计算和加速》的演讲中，分享了英特尔过去一两年在大模型的异构计算和加速方面所做的工作。戴金权指出，大模型在做推理和训练的过程中，存在内存带宽、计算、显存大小和分布式计算多方面的瓶颈。随着大模型被部署在客户端、边缘端、服务器等不同的系统，除低比特计算的方法外，推理算法的各种优化都能够更好地提升其在 XPU 上的计算效率。他表示，高效的异构计算是生成式 AI 发展的核心能力之一。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/caa2fbab6d2aab0e855cafb988de3cd0.jpeg" /></p><p></p><p></p><p>如何高效地训练大模型、做大模型的推理优化？现场，面壁智能联合创始人兼 CEO 李大海指出， 2018 年以来，行业内不断见证大模型规模法则（Scaling Law），工业界也在尽可能地保证摩尔定律有效，持续改进芯片制造工艺、提升芯片制程，核心是提升芯片电路密度、实现计算设备小型化。“制程”不断提高的事情同样发生在大模型领域，根据过去几年在大模型领域的深耕和实践，对大模型的发展趋势进行观察总结，面壁智能提出了大模型时代的面壁定律：大模型的知识密度不断提升，平均每 8 个月提升一倍。”其中知识密度 = 模型能力 / （参与计算的）模型参数。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/8e/8ea4aeda300ae34bd13a6b851d2a71fe.jpeg" /></p><p></p><p></p><p>李大海表示，大模型数据驱动技术方向大致确定，而模型架构 - 算法 - 数据技术方案仍高速迭代，需持续改进模型制程，极致提升知识密度。据他观察，在过去四年，大模型的知识密度平均每 8 个月就提高一倍，相比摩尔定律更加高效，这也是面壁做端侧模型的原因。芯片制程带来终端算力持续增强，模型制程带来模型知识密度持续增强，两者交汇揭示了端侧智能的巨大潜力。此外，李大海认为，更高知识密度带来更高效模型，要构建模型风洞，在小模型高效寻找最优数据和超参配置并外推至大模型，让模型成长摆脱“炼丹”窘境。</p><p></p><p>最后，字节跳动研究科学家、豆包大模型视觉基础研究团队负责人冯佳时分享了字节跳动基于 LLM 的视频生成和图像理解实践。冯佳时表示，无论是在自动驾驶还是具身智能上，业内往往把大语言模型视作机器人大脑，并希望其在做推理时能够参考周围环境的信息，能够具有一定的定位能力，与物理环境进行可靠的交互。为此，字节在 PixelLM 方案中引入多个 token 来完成多个物体的分割，并将分割模型 SAM 替换成轻量的 MLP，计算量比之前的模型 LISA 减少一半，分割精度也显著提升。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/08/0853ef5b844c2bcd64ec9be680b284ef.jpeg" /></p><p></p><p></p><p>此外，冯佳时指出，目前的视频生成模型在交互上有很多不便之处，存在一致性、创作界面与可控性、视频表现力等方面的问题。字节在其 StoryDiffusion 模型提出一致性模块和运动生成模块两个关键技术，来提升角色一致性和表现力。</p><p></p><p>除了 Keynote 主题演讲之外，本次大会还策划了多元化的专题论坛内容，包括大模型训练以及推理加速、RAG 落地应用与探索、大模型产品应用及构建、多模态大语言模型的前沿应用与创新、大模型与企业工具集成的提效实践、大模型产学研结合探索、端侧模型落地探索等十多个高质量话题专场。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/72/723f20a960944292bfc421d627ff3b90.jpeg" /></p><p></p><p></p><p>大会现场气氛异常热烈，不仅吸引了大量听众的积极参与，还赢得了在场参会人员的一致好评。许多与会者纷纷表示，这次大会紧密围绕当下的 AI 和大模型热点话题，从多个角度进行了深入的技术架构专业解读和商业化实践分享，为其日常工作和探索带来了宝贵的启示和具有实际应用价值的参考，有助于他们在各自领域内更好地推动 AI 技术的创新和发展。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/e4/e4252d296a8c7ebb5f5c18637e2e7bc3.jpeg" /></p><p></p><p></p><p>AICon 上海的圆满举办，离不开赞助商们贡献的力量。感谢英特尔、亚马逊云科技、Google Cloud、矩阵起源、百道数据、Optiver、数势科技、未来智能、UCloud优刻得、钛动科技、零一万物、快递 100、快手、昇腾对本届大会的倾情赞助以及蔚来汽车为大会展区带来的特别策划。在大家的共同助力下，我们得以持续推动技术的传播与发展，为行业创新注入不竭源泉。</p><p></p><p>经统计，AICon 上海站现场听众累计超过 1000 人次。我们深感荣幸与欣慰，衷心感谢每一位参与者的鼎力支持与不断鼓励。正是因为有了大家的热情参与和积极贡献，我们才能坚定不移地追求目标，致力于成为技术传播领域的佼佼者。我们将持续不断地提升内容的质量，致力于打造更加优质、更具包容性的交流平台，让每一个人都能在这里找到启发和灵感，一齐推动技术领域的创新与突破，为未来的科技进步贡献力量。</p><p></p><p>大会 PPT 获取通道已开启，关注 AI 前线 公众号，后台回复“PPT”，即可获取 PPT 下载地址！（由于讲师所在企业限制，部分 PPT 仍在审查或不对外公布，详情见大会官网日程） &gt;&gt;&gt;</p><p></p><p>至此，今年 InfoQ 中国已圆满落幕 5 场技术盛会，随后还将于 10 月 18 -19 日举办 QCon 上海站。如您感兴趣，可点击<a href="https://qcon.infoq.cn/2024/shanghai">官网</a>"查看更多详情。</p><p></p><p>期待下一场大会再见！</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/c9/c919f5a113b14883202eec12906fc7e3.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/12dCYzJcokxXEcTMvc9Y</id>
            <title>“从头开始训练模型，几乎没有意义”</title>
            <link>https://www.infoq.cn/article/12dCYzJcokxXEcTMvc9Y</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/12dCYzJcokxXEcTMvc9Y</guid>
            <pubDate></pubDate>
            <updated>Tue, 20 Aug 2024 10:25:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>我们<a href="https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-i/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">之前分享过</a>"我们在运营大型语言模型应用程序时磨练出的战术方面的见解。战术是细粒度的：它们是为实现特定目标而采取的具体行动。我们还<a href="https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-ii/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">分享了</a>"我们对运营的看法：为支持战术工作并实现目标而建立的更高层次的过程。</p><p>&nbsp;</p><p>但这些目标从何而来？这就属于战略的范畴。战略解答了战术和运营中“如何”背后的“什么”和“为什么”。</p><p>&nbsp;</p><p>我们将分享我们的见解，比如“在产品市场契合之前避免使用GPU”和“专注于构建系统而非模型”，以指导团队如何高效地分配稀缺资源。我们还提出了一个通往卓越产品的迭代路线图。这些宝贵的经验教训汇集起来，回答了以下这些问题：</p><p>&nbsp;</p><p>自建还是购买：何时应该自行训练模型，何时应该使用现成的API？答案总是“视情况而定”。我们分享了决定因素是什么。迭代至卓越：如何创造长期竞争优势，而不仅仅是依赖最新的模型？我们讨论了基于模型构建健全系统的重要性，并专注于提供令人难忘的、有粘性的体验。以人为中心的AI：如何有效地将大模型融入人类工作流程中，以提升生产力和幸福感？我们强调了构建支持和增强人类能力的AI工具的重要性，而不是试图完全取代人类。入门指南：团队开始构建大模型产品的基本步骤是什么？我们概述了一个基本的流程，从提示词工程、评估和数据收集开始。低成本认知的未来：大模型的成本迅速降低和能力增加将如何塑造AI应用的未来？我们审视了历史趋势，并通过一个简单的方法来估计某些应用何时可能在经济上变得可行。从演示到产品：从一个引人注目的演示到一个可靠、可扩展的产品需要做些什么？我们强调了严格的工程、测试和持续改进的必要性，以缩小原型和生产之间的差距。</p><p>&nbsp;</p><p>为了回答这些难题，让我们来一步一步地思考。</p><p>&nbsp;</p><p></p><h2>战略：在不失去先机的情况下利用大模型</h2><p></p><p>&nbsp;</p><p>成功的产品需要深思熟虑的规划和严格的优先级安排，而不是无休止的原型迭代或盲目追逐最新的模型或潮流。在本文中，我们将放眼四周，深入探讨构建卓越AI产品的战略考量。我们还将审视团队在开发过程中可能面临的主要权衡问题，比如决定是自主构建还是外部采购，并为早期大型语言模型应用的策略开发提供一个指导“蓝图”。</p><p>&nbsp;</p><p></p><h3>在产品契合市场之前不要使用GPU</h3><p></p><p>&nbsp;</p><p>要实现卓越，你的产品不应该只是在供应商提供的API之上构建一层薄弱的包装层，但走向相反的极端可能带来更大的代价。过去一年，我们目睹了大量的风险投资涌入，包括令人瞠目结舌的60亿美元A轮融资，用于训练和定制模型，而没有清晰的产品愿景或目标市场。在这一部分，我们将解释为什么急于投入模型训练是一个错误，并探讨自托管模型的定位。</p><p>&nbsp;</p><p></p><h4>从头开始训练模型（几乎）总是没有意义</h4><p></p><p>&nbsp;</p><p>对于大多数组织来说，从头开始训练大模型是一种不切实际的分心，它分散了构建实际产品的精力和资源。</p><p>&nbsp;</p><p>尽管这么做很令人兴奋，尽管似乎业界都在追随这一趋势，但开发和维护机器学习基础设施需要大量的资源投入，包括收集数据、训练和评估模型以及部署它们。如果你还处在验证产品市场契合度的阶段，这些可能会从核心产品开发中抽走宝贵的资源。即使你拥有计算能力、数据和技术能力，预训练的大模型也可能在几个月内就变得过时。</p><p>&nbsp;</p><p>以<a href="https://arxiv.org/abs/2303.17564?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">BloombergGPT</a>"为例，这是一个专门为金融任务训练的大模型。这个模型在363B个token上进行了预训练，耗费了<a href="https://twimlai.com/podcast/twimlai/bloomberggpt-an-llm-for-finance/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">九名全职员工</a>"的辛勤劳动，包括四名AI工程师和五名机器学习产品和研究人员。尽管付出了巨大的努力，BloombergGPT在那些金融任务上的表现在一年内就被<a href="https://arxiv.org/abs/2305.05862?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">gpt-3.5-turbo和gpt-4超越了</a>"。</p><p>&nbsp;</p><p>这个故事以及其他类似案例揭示了一个事实，对于大多数实际应用来说，在领域特定数据上从头开始预训练大模型并不是资源的最佳利用方式。相反，团队应该考虑对可能满足他们特定需求的最强大的开源模型进行微调。</p><p>&nbsp;</p><p>当然也有例外。<a href="https://blog.replit.com/replit-code-v1_5?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">Replit的代码模型</a>"就是一个鲜明的例子，这个模型专门为代码生成和理解而训练。通过预训练，Replit超越了其他大型模型，如CodeLlama7b。然而，随着其他越来越有竞争力的模型的发布，要保持其竞争力，还需要持续不断的投入和更新。</p><p>&nbsp;</p><p></p><h4>在证明必要性之前不要进行微调</h4><p></p><p>&nbsp;</p><p>对于大多数组织来说，进行微调更多是受FOMO（错失恐惧症）的驱使，而不是基于清晰的战略思考。</p><p>&nbsp;</p><p>许多组织过早地进行微调，试图避开“只是一层包装”的指责。实际上，微调是一项重装备操作，只有在你收集了大量示例并确信其他方法均不足以解决问题时才考虑使用。</p><p>&nbsp;</p><p>一年前，许多团队向我们表达了他们对微调技术的热情。然而，很少有人找到产品市场契合度，大多数人最终对他们的决定感到后悔。如果你要进行微调，应该非常确信自己已经做好了反复进行这项工作的准备，因为基础模型自身也在不断进步——见下面的“模型不是产品”和“构建LLMOps”。</p><p>&nbsp;</p><p>微调在以下场景中可能可以成为恰当的选择： 特定应用需要的数据并未包含在用于训练现有模型的开放数据集中。 你已经开发了一个最小可行产品（MVP），并证明现有的模型无法满足需求。但请务必谨慎：如果连模型开发者都难以获得高质量的训练数据，那么你又是如何获得这些数据的呢？</p><p>&nbsp;</p><p>最后请记住，大模型驱动的应用程序不是科学展览会上的项目，对它们的投入应当与其对企业战略目标的贡献及所带来的竞争优势相匹配。</p><p>&nbsp;</p><p></p><h4>从推理API开始，但不要拒绝自托管</h4><p></p><p>&nbsp;</p><p>有了大模型API，初创公司能够以前所未有的便捷性采用和集成语言建模能力，无需从头开始训练自己的模型。像Anthropic和OpenAI这样的供应商提供了通用API，只需几行代码就可以将智能嵌入到你的产品中。利用这些服务，你可以大幅减少开发方面的劳动投入，从而将更多的精力集中在为客户提供真正的价值上——这有助于你更快地验证想法并加快产品与市场契合度的迭代。</p><p>&nbsp;</p><p>但是，就像数据库一样，托管服务并不适用于所有场景，特别是在规模扩大和需求增长的情况下。事实上，在一些受严格监管的行业，如医疗保健和金融行业，或在有合同义务、保密要求约束的情况下，自托管可能是唯一能够确保在使用模型时不泄露敏感或私有数据的方式。</p><p>&nbsp;</p><p>此外，自托管能够避开供应商可能设置的限制，如速率限定、模型弃用以及一些使用上的限制。此外，自托管还赋予你完全的控制权，使得构建一个具有差异化优势和高质量标准的系统变得更加容易。最后，自托管，特别是在进行了微调的情况下，可以在大规模应用中显著降低成本。例如，<a href="https://tech.buzzfeed.com/lessons-learned-building-products-powered-by-generative-ai-7f6c23bff376#9da5?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">BuzzFeed就分享了他们如何通过微调开源模型将成本降低了80%</a>"。</p><p>&nbsp;</p><p></p><h2>迭代至卓越</h2><p></p><p>&nbsp;</p><p>为了确保长期的竞争优势，你需要超越模型，想想是什么让你的产品脱颖而出。虽然执行速度很重要，但它不应成为你唯一的优势。</p><p>&nbsp;</p><p></p><h4>模型不是产品，围绕它的系统才是</h4><p></p><p>&nbsp;</p><p>对于那些不自行构建模型的团队而言，快速的创新步伐无疑是一大优势。他们能够灵活地从一个最先进的模型转移到另一个，不断追求上下文理解、推理能力以及成本效益比方面的提升，从而打造更优质的产品</p><p>&nbsp;</p><p>这一进展既令人振奋又具有可预见性。从整体来看，这暗示了模型可能成为系统中最具易变性的部分。</p><p>&nbsp;</p><p>相反，将你的精力专注在那些能够提供长期价值的东西上，例如：</p><p>&nbsp;</p><p>评估基线：确保你的任务在不同模型间具有一致的可靠性能评估；安全护栏：建立机制以确保无论模型如何变化，都能防住不恰当的输出；缓存：利用缓存策略来减少对模型的依赖，从而降低延迟和成本；数据飞轮：为上述的迭代改进提供动力。</p><p>&nbsp;</p><p>这些组件构建了一个更为坚固的产品质量护城河，超越了模型本身的能力。</p><p>&nbsp;</p><p>但这并不意味着应用构建就完全没有风险。不要期望OpenAI或其他模型供应商会提供完全相同、无需额外处理的解决方案。</p><p>&nbsp;</p><p>例如，一些团队构建自定义工具来验证专有模型的结构化输出。在这方面进行适度的投入是明智的，但过度投入则可能不是最佳的时间利用策略。OpenAI需要确保当用户请求一个函数调用时会得到一个有效的函数调用——因为所有用户都想要这个。在这种情况下，采用“战略性拖延”是明智的，即只构建你绝对需要的东西，然后等待供应商能力的进一步提升。</p><p>&nbsp;</p><p></p><h4>建立信任，从小处开始</h4><p></p><p>&nbsp;</p><p>追求成为“万能钥匙”产品往往会导致平庸。要打造引人注目的产品，需要专注于创造独特且令人难以忘怀的用户体验，让用户不断回头。</p><p>&nbsp;</p><p>设想有一个旨在应对用户可能提出各种问题的通用性RAG系统。缺乏针对性的专业化导致系统无法优先获取最新资讯，解析特定领域的数据格式，或深入理解特定任务的复杂性。因此，用户得到的体验往往是表面化的、不可靠的，难以满足他们的实际需求。</p><p>&nbsp;</p><p>为了解决这个问题，需要专注于特定领域和用例。通过深入挖掘而非广泛覆盖，可以开发出与用户产生共鸣的专业工具。专业化还让你能够清晰地界定系统的能力和局限。坦诚地展示系统的优势和局限，不仅体现了自我认知，也帮助用户明白在哪些方面系统能发挥最大效用，从而建立信任并增强对输出结果的信心。</p><p>&nbsp;</p><p></p><h4>打造LLMOps：为了更快的迭代</h4><p></p><p>&nbsp;</p><p>DevOps本质上并非只关注可重复的工作流、左移策略或团队授权——更只不是关于编写YAML文件。</p><p>&nbsp;</p><p>DevOps关注缩短工作流与结果反馈之间的周期，从而促进持续改进而非累积错误。它的理念源于精益创业运动，可以进一步追溯到精益制造和丰田生产系统，这些理念强调的是快速响应变化和持续改进。</p><p>&nbsp;</p><p>MLOps将DevOps的理念和实践应用到机器学习中。它带来了可重复的实验流程，提供了一站式的工具套件，使得模型构建者能够更便捷地将模型推向生产。当然，在这个过程中，YAML文件扮演了不可或缺的角色。</p><p>&nbsp;</p><p>但从行业来看，LMOps尚未完全实现DevOps的核心功能。它没有有效地缩短模型开发与在生产环境中推理和交互之间的反馈周期。</p><p>&nbsp;</p><p>令人振奋的是，LLMOps领域已经从关注那些看似琐碎的问题，如提示词管理，转向解决阻碍迭代的难题：在生产环境中进行有效监控并实现持续改进。</p><p>&nbsp;</p><p>我们已经拥有了中立、众包的评估平台，这些平台专门用于聊天和编程模型的互动——它们构成了一个集体迭代改进的外循环。像LangSmith、Log10、LangFuse、W&amp;B Weave、HoneyHive等工具不仅可用于收集和整理生产系统中的结果数据，还通过与开发流程紧密结合，利用这些数据来不断改进系统。你可以尝试拥抱它们，或者构建属于自己的工具。</p><p>&nbsp;</p><p></p><h4>如果可以买，就不要自己构建</h4><p></p><p>&nbsp;</p><p>大多数成功的业务并不是基于大语言模型的业务，但大多数业务都存在通过大模型进行改进的可能性。</p><p>&nbsp;</p><p>这有时会误导领导者急于将大模型技术应用于系统改造，导致成本上升和产品质量下降，甚至可能将这些技术作为虚假的“AI”特性匆忙推向市场，还带上<a href="https://x.com/nearcyan/status/1783351706031718412?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">那些令人眼花缭乱的星星图标</a>"。更好的做法是：专注于那些真正与你的产品设计目标相契合并能增强你核心运营的大模型应用。</p><p>&nbsp;</p><p>让我们重新审视一下那些可能消耗团队宝贵时间的错误尝试：</p><p>&nbsp;</p><p>尝试为企业开发定制的文本到SQL转换功能；构建能够与文档进行互动的聊天机器人；将公司的知识库与客户支持系统的聊天机器人集成。</p><p>&nbsp;</p><p>虽然上述的大模型应用属于入门级别，但对于大多数产品公司来说，并不适合自己从头开始构建。这些是许多企业面临的普遍问题，演示和实际存在巨大差距——而这正是软件公司的专长所在。当前的Y Combinator孵化器已经在集中解决这些问题，因此在这些问题上投入宝贵的研发资源是一种浪费。</p><p>&nbsp;</p><p>如果说这听起来像是陈词滥调的商业建议，那是因为在当前热潮和泡沫般的兴奋中，人们很容易将带有“大模型”标签的事物误认为是尖端的增值差异化手段，而忽略了哪些应用实际上已经是司空见惯的。</p><p>&nbsp;</p><p></p><h4>AI辅助，以人为本</h4><p></p><p>&nbsp;</p><p>目前，由大模型驱动的应用程序是很脆弱的，它们需要精心设计的保护措施和防御策略，即便如此，依然存在很多不确定性。然而，当这些应用程序的应用范围有了明确限定，可能会变得极为有用。这说明大模型是加速和优化用户工作流的有力工具。</p><p>&nbsp;</p><p>尽管人们可能会被基于大模型的应用程序完全替代工作流或工作职能的想法所吸引，但目前最有效的模式是人机协作——计算机半人马模式（类似<a href="https://en.wikipedia.org/wiki/Advanced_chess?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">国际象棋半人马模式</a>"）。当有才能的人类与大模型能力相结合，完成任务的效率和满足感可以得到显著提升。GitHub Copilot，作为大模型的主要应用之一，已经展示了这种协作工作流程的强大潜力：</p><p>&nbsp;</p><p></p><blockquote>“总的来说，开发者告诉我们，他们感到更有信心，因为编码变得更容易、错误更少、代码易读性更强、可重用性更高、更简洁、更易于维护，并且系统弹性比没有GitHub Copilot和GitHub Copilot Chat时更强。”——<a href="https://resources.github.com/learn/pathways/copilot/essentials/measuring-the-impact-of-github-copilot/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">Mario Rodriguez，GitHub</a>"</blockquote><p></p><p>&nbsp;</p><p>对于那些长期从事机器学习工作的人来说，可能会立刻联想到“HITL”，但不要急于下结论：HITL机器学习是一种人类专家确保机器学习模型能够按照预期的方式运行的范式。尽管两者相关，但我们今天要讨论的是一个更为微妙的概念。大模型驱动的系统不应成为大多数工作流的主要驱动力，而应当被视为一种辅助资源。</p><p>&nbsp;</p><p>将人类置于核心位置，并探索如何让大模型来辅助他们的工作流，这种方法将引导我们做出截然不同的产品和设计决策。最终，这将促使我们打造出与那些急于将所有职责转嫁给大模型的竞争对手不同的产品——更好、更有用、风险更低的产品。</p><p>&nbsp;</p><p></p><h2>从提示词、评估和数据收集开始</h2><p></p><p>&nbsp;</p><p>前面的章节阐述了一些技术和建议，内容相当丰富，可能需要一些时间来消化。让我们来概括一下最核心的建议：如果一个团队想要构建基于大模型的产品，应该从哪里开始？</p><p>&nbsp;</p><p>在过去的一年里，我们已经看到了足够多的例子，这让我们对大模型应用取得成功的轨迹有了清晰的认识。在本节中，我们将通过一个基础的“入门”指南来梳理这些经验。核心理念是保持简单，只在必要时才引入复杂性。根据经验，每增加一个复杂度级别，通常至少需要比前一个级别多一个数量级的努力。</p><p>&nbsp;</p><p></p><h4>提示词工程先行</h4><p></p><p>&nbsp;</p><p>我们从提示词工程开始。在试图从较弱的模型榨取性能之前，先使用我们在战术部分讨论的技术。思维链、n-shot以及结构化输入和输出通常都是明智的选择。在转向较弱的模型之前，先用大的模型进行原型设计。</p><p>&nbsp;</p><p>只有在提示词工程无法达到所需的性能时才考虑微调。如果有非功能性方面的需求（例如，数据隐私、完全控制权和成本考量），并且这些要求阻碍了使用专有模型，不得不使用自托管方案，那就需要进行微调。只是你要确保数据隐私需求不会阻碍你使用用户数据进行微调！</p><p>&nbsp;</p><p></p><h4>评估并启用数据飞轮</h4><p></p><p>&nbsp;</p><p>即使团队是在初始阶段，也需要进行评估。否则，你将无法确定你的提示词工程是否有效，或者微调模型何时能准备就绪替换基础模型。</p><p>&nbsp;</p><p>有效的评估应针对<a href="https://twitter.com/thesephist/status/1707839140018974776?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">具体的任务</a>"，并反映预期的使用场景。我们<a href="https://hamel.dev/blog/posts/evals/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">建议</a>"的评估起点是单元测试。这些基础的断言用于检测已知或假设的故障模式，有助于推动早期的设计决策。此外，还可以看看其他<a href="https://eugeneyan.com/writing/evals/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">特定于任务的评估方法</a>"，例如用于分类、摘要等任务的评估。</p><p>&nbsp;</p><p>尽管单元测试和基于模型的评估很有用，但它们不能完全替代人类的评估。让人们使用你的模型或产品，并收集反馈，这至关重要。这不仅可以衡量产品在现实世界中的表现和缺陷，也能收集可用于微调模型的高质量标注数据。这样可以形成一个正向反馈循环（也叫数据飞轮），随着时间的推移，可以产生复合效应：</p><p>&nbsp;</p><p>使用人类评估来评估模型性能和/或发现缺陷；使用标注数据来微调模型或更新提示词；持续这一过程。</p><p>&nbsp;</p><p>例如，在评审大语言模型生成的摘要时，我们可能会对每个句子进行细致的反馈，识别出事实错误、不相关性或风格问题。然后，我们可以使用事实错误标注来<a href="https://eugeneyan.com/writing/finetuning/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">训练幻觉分类器</a>"，或使用相关性标注训练<a href="https://arxiv.org/abs/2009.01325?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">奖励模型来评估相关性</a>"。 另一个例子是，LinkedIn在其播客中分享了使用<a href="https://www.linkedin.com/blog/engineering/generative-ai/musings-on-building-a-generative-ai-product?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">基于模型的评估器</a>"来评估幻觉、AI违规行为、连贯性等问题的成功经验。</p><p>&nbsp;</p><p>通过构建随时间增值的资产，我们将评估工作从单纯的运营成本转变为战略投入，并在这个过程中加速数据飞轮效应。</p><p>&nbsp;</p><p></p><h2>低成本趋势</h2><p></p><p>&nbsp;</p><p>1971年，施乐帕克研究中心的研究人员预测未来是一个由网络个人电脑主导的世界。他们发明了一系列关键技术，如以太网、图形渲染、鼠标以及窗口界面，为他们预测的未来成为现实奠定了基础。</p><p>&nbsp;</p><p>他们还进行了一项基础实践：观察那些非常有实用价值但成本较高的应用（例如，视频显示器），然后分析这些技术的历史价格趋势（如摩尔定律），并预测了这些技术何时会变得经济实惠。</p><p>&nbsp;</p><p>我们同样可以在大型语言模型技术方面进行同样的分析，尽管我们没有像像晶体管成本那样直观的衡量标准。以一个被广泛认可且持续更新的基准测试为例，比如Massively-Multitask Language Understanding数据集，以及一种一致性的输入方法（five-shot提示词）。然后，我们可以比较在不同时间用各种性能水平的语言模型在该基准测试上运行的成本。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/53/531433da42738663441944d60056011d.png" /></p><p></p><p>在成本固定之下，能力正迅猛提升。在能力水平固定之下，成本正急剧下降。</p><p>&nbsp;</p><p>自OpenAI的davinci模型作为API发布以来的四年里，在该任务上运行具有等效性能的模型的成本已经从20美元降到了不到10美分——成本减半的时间仅为六个月。同样，截至2024年5月，通过API供应商或自行运行Meta的LLama 3 8B模型的成本仅为每百万个token 20美分，这个模型的性能与OpenAI的text-davinci-003相当，后者曾以其卓越的表现震惊了世界。值得注意的是，当LLama 3 8B在2023年11月末发布时，其成本大约为每百万个token 20美元。成本在短短18个月内降低了两个数量级，与摩尔定律预测的时间不谋而合。</p><p>&nbsp;</p><p>现在，我们来探讨一个极具潜力但目前还不具备经济效益的大模型应用（<a href="https://arxiv.org/abs/2304.03442?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">生成视频游戏角色</a>"，成本估计为<a href="https://arxiv.org/abs/2310.02172?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">每小时625美元</a>"）。自2023年8月该论文发布以来，成本已经下降了一个数量级，降至每小时62.5美元。基于这一趋势，我们可以预期在下一个九个月内，成本会降至每小时6.25美元。</p><p>&nbsp;</p><p>当吃豆人（Pac-Man）游戏在1980年首发时，现在的1美元可以兑换一个信用点，允许玩家享受几分钟到几十分钟的游戏乐趣——如果以每小时六场游戏来估算，相当于每小时6美元。按照这种粗略计算，一个引人入胜的大模型增强型游戏体验将在2025年的某个时候变得经济可行。</p><p>&nbsp;</p><p>这些趋势虽然还很新，仅有几年的历史，但我们没有理由认为它们在未来几年会有所减缓。尽管在算法和数据集方面，我们可能已经摘取了容易获得的成果，比如超越了“Chinchilla比率”的每参数约20个token，但数据中心内部更深层次的创新和投入以及硅芯片层面的进展有望弥补这一潜在的不足。</p><p>&nbsp;</p><p>这可能是最关键的战略洞见：那些今天看似完全不可能的演示或研究论文，在未来几年内将逐渐演变为高级的功能，并成为普通商品。我们应当基于这一视角来构建我们的系统和组织架构。</p><p>&nbsp;</p><p></p><h2>从0到1已经够多了，是时候从1到N了</h2><p></p><p>&nbsp;</p><p>构建大模型演示应用非常有趣，只需要几行代码、一个向量数据库和一条精心设计的提示词，我们就能创造出令人惊叹的“魔法”。在过去的一年里，这种“魔法”被比作是互联网、智能手机，甚至印刷机般的创新。</p><p>&nbsp;</p><p>不幸的是，在现实的软件项目中摸爬滚打的人都知道，演示中的完美表现与大规模稳定运行的产品之间存在着巨大的差异。</p><p>&nbsp;</p><p>以自动驾驶汽车为例。第一辆由神经网络驱动的汽车在<a href="https://proceedings.neurips.cc/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">1988年</a>"问世，二十五年后，Andrej Karpathy<a href="https://x.com/karpathy/status/1689819017610227712?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">在他的Waymo上进行了第一次演示</a>"。十年之后，这家公司获得了<a href="https://x.com/Waymo/status/1689809230293819392?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">无人驾驶许可</a>"。从原型到商业产品，经历了三十五年严格的工程、测试、改进和合规监管过程。</p><p>&nbsp;</p><p>在过去的一年，不管是工业界还是学术界，我们都看到了大模型应用的起伏：这是大模型应用的“1到N”年。我们希望我们所学到的经验——从严格的战术性操作技术，再到内部需要构建哪些能力的战略性视角——能帮助你在接下来的一年乃至更长远的未来，更好地参与这项激动人心的新技术的共同建设。</p><p>&nbsp;</p><p>【声明：本文由InfoQ翻译，未经许可禁止转载。】</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-iii-strategy/">https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-iii-strategy/</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/6yYr6GPSULaZQhgJ9fTv</id>
            <title>卷模型还是做平台？落地企业AI，用友这样做！</title>
            <link>https://www.infoq.cn/article/6yYr6GPSULaZQhgJ9fTv</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/6yYr6GPSULaZQhgJ9fTv</guid>
            <pubDate></pubDate>
            <updated>Tue, 20 Aug 2024 10:21:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>由大模型掀起的 AI 热潮持续了两年时间，行业已经从关注大模型技术的创新突破，转变为思考和实践如何利用基于大模型的 AI 能力来赋能业务、创造价值。正如用友网络副总裁罗小江所说：这个时代不缺技术，缺的是方法体系，缺的是让技术真正意义上融入业务。</p><p></p><p>可以预见的是，企业将更加亲睐针对特定行业或应用进行训练优化的大模型，以及能够深入业务场景，带来实际经济效益的 AI 解决方案。</p><p></p><p>8 月 9-10 日，由用友主办，以“AI+ 成就数智企业”为主题的“2024 全球商业创新大会”在北京召开, 在企业数智化技术峰会上，用友围绕 YonGPT 2.0 大模型与用友 iuap 智能平台 YonAI 给出了更加满足企业需求的 AI 落地解法。在用友看来，AI 能力并非孤立的烟囱，想要充分释放 AI 潜能，一定要将 AI 能力融入企业的平台能力建设，以平台为载体，在各个业务环节中挖掘 AI 能力的适用场景，加速发展新质生产力、重塑企业核心竞争力。</p><p></p><h2>大模型落地并非易事，YonGPT 2.0 如何破局？</h2><p></p><p></p><p>目前，大模型在大多数行业中仍然很难深入到企业实际业务层面，要想切实为企业赋能，往往面临多重挑战：首先是数据挑战，很多企业缺乏数据准备，无法为模型训练和微调提供充足的高质量数据，也没有建立与 AI 时代相适应的大数据基础设施；其次是安全挑战，普遍运行在云端的大模型让企业担忧数据和隐私泄露风险，他们更偏向运行在本地，或者自身有更高掌控力的小模型产品；大模型的应用场景偏少也让很多用户头疼，花费大量投资建立的技术栈在实践中少有用武之地，降本增效也就无从谈起。另外，大模型在行业领域应用时，频繁出现的幻觉现象让问题更是雪上加霜，这也是垂类大模型崛起的重要因素；最后，由于 IT 技术较为薄弱，传统企业面对大模型和 AI 技术栈的持续运维也往往力不从心。</p><p></p><p>以上这些问题，都让大模型技术的落地之路变得更加坎坷不平。面对这样的局面，用友在去年发布了业内首个企业服务大模型 YonGPT，专注于助力企业降低使用 AI 的门槛，解决企业 AI 赋智赋能时面临的一系列难题。过去一年来，YonGPT 先后发布了六大场景，上线了问答应用、Agent 和应用生成等能力，并在今年 2 月份通过了网信办备案。在此基础上，用友此次又升级了 YonGPT 2.0 全新版本，包括了多项专业能力增强、一个大模型平台和两个应用框架。</p><p></p><p><img src="https://static001.geekbang.org/infoq/59/5933b38b81f9704543c495920b3a45a5.png" /></p><p>通用大模型所关注的领域范围往往非常广泛，而 YonGPT 2.0 的提升完全专注于企业常见的业务领域，包括 PPT 分析报告生成、合同智能审核与生成、业务对象和表格理解、代码生成、财务和人力等领域知识增强、安全拒识等能力。这一设计的最大优势在于 YonGPT 2.0 可以充分利用用友数十年来服务各行业的经验和数据积累，同时不需要像 ToC 的通用大模型一样扩展更大的规模，节省了大量训练、微调和运维成本。</p><p></p><p>虽然 YonGPT 2.0 大模型已经专门为企业应用量身定制，但各个行业在实际部署模型时仍需要微调和优化才能获得更好的使用效果。对此，用友提供了一个一站式的大模型平台即服务，覆盖了数据管理、模型训练、评估优化、推理服务的全流程。平台内置了多个专业数据集，还支持百川、通义千问等多种大模型的微调训练。用户训练完成后，还可以在平台上直接评估效果，进行可视化展示。通过这一平台，没有大模型实践经验的企业也能快速上手，将 YonGPT 2.0 调整为更加适合自身业务需求的状态，为接下来的应用开发做好准备。</p><p><img src="https://static001.geekbang.org/infoq/03/03f407d4f9cd6054849cc4dbaf69c271.png" /></p><p>企业服务大模型的最终目标还是解决实际的业务问题，对此，用友汇总了经营中常见的八大问题场景，包括人、财、物、服、供、产、销、研，各个场景又总结出八种业务运营和知识生成的问题类型。对于这些问题，用友基于 Agent、RAG 应用框架， 帮助企业实现业务运营、人机交互、知识生成和应用生成等应用能力。</p><p></p><p>Agent 应用框架主要负责将用户的自然语言需求转换分解成模型能够识别的子任务集，并基于这些子任务输出模型 API 可以调用的参数。面对复杂问题时，用友的多智能体自主协同框架可以调度多个大模型模块，用友还结合专家知识和错误反馈学习解决了模型的幻觉和可靠性问题，并优化了模型的时效性和安全性表现。</p><p></p><p>知识生成问答无疑是大模型落地倍受瞩目的应用场景。但基于企业自身数据积累的知识生成高度依赖数据处理框架，处理不好很容易“答非所问”甚至输出误导、错误结果。用友结合流行的 RAG 框架开发了智能大搜产品，并提出了多语义向量技术，对每个知识片段都生成了向量和问题来增强索引，显著提升了知识搜索的精确度。用友还解决了索引搜索的权限问题，防止低权限用户搜索到高权限内容。该框架对表格、图片、视频、代码的理解也更加准确。企业员工使用自然语言提出问题，智能大搜不仅可以给出准确的文本回答，还能输出关系图、汇总图、相关图片和视频，甚至可以帮助员工扩写论点、整理文稿等。而基于 Code RAG 应用框架，应用开发人员甚至业务人员都可以快速生成企业应用代码，简化应用生成流程。所有生成内容都能无缝对接员工使用的各类应用，帮助企业实现全流程、全场景提效。</p><p></p><p>为了深化大模型在行业的场景应用，用友在本次大会上还联合来自公共资源交易行业、工业装备行业、交通建设行业的代表客户，发布了三大行业的垂类大模型，加速了 AI 在千行百业的落地进程 。</p><p></p><h2>挖掘智能场景应用，YonAI 为大模型落地构建平台基础</h2><p></p><p></p><p>YonGPT 2.0 的能力升级，为企业在业务中运用大模型提效增速铺平了道路。YonGPT 是用友为企业持续输出 AI 服务能力的核心工具。如前文所述，企业在 AI 落地过程中面临着一系列挑战，这些挑战仅靠大模型技术本身是不足以应对的。正因如此，用友将过去数十年帮助企业数智化转型取得的技术成果与 YonGPT 大模型创新结合起来，推出了用友 iuap 智能平台 YonAI。</p><p><img src="https://static001.geekbang.org/infoq/49/498a9726bd76471c08cf6a544cd041c9.png" /></p><p></p><p>YonAI 平台由包含 YonGPT 大模型的智能基础平台层、智能算法层、包含 Agent、RAG 和智能服务的智能框架层，以及最顶层的智能入口层构建而成，外部对接用友云技术、应用和数据平台，从而为企业提供全方位、全场景的平台化 AI 能力支撑。基于 YonAI 平台，企业员工在日常业务中随处开启智友智能助理和智能大搜服务，就可以轻松调用 YonGPT 大模型等 AI 能力来提升工作效率，启发创新灵感。用友也在服务企业客户的过程中与用户共同探索，挖掘出了一些企业在当下可以快速引入 AI 技术的应用场景。</p><p></p><p>合同审核是业务运营中常见而关键的环节之一。业务人员将合同草稿输入审核应用，即可自动提取关键字段，根据知识库内拟定的业务规则和敏感词审查合同违规情况。智友助手还能帮助业务人员查询合同相关数据，计算合约背后的经济和财务数据，乃至辅助补充合同条款、润色文本等。合同审核智能化大大缩短了业务合约的审批周期，加快资源周转，提升了业务运营效率。</p><p></p><p>在企业人力资源领域，员工面试是人资部门的日常工作。用友为企业打造了 AI 面试平台，通过对面试视频记录的 AI 分析为候选人进行多维评价，绘制人才画像。平台覆盖 140 多个评分项、600 多个评价标准，并能在面试结束后自动输出面试纪要和综合建议。在典型客户的实践应用中，用友 AI 面试可以帮助人力资源部门提升 30% 的面试效率。</p><p></p><p>用友智能大搜产品也有着丰富的使用场景。例如，出差人员可以通过简单询问快速了解差旅报销标准；新人入职后，可以在智能大搜服务中点播各类企业培训课程自主学习；管理人员组建团队时，可以使用智能大搜寻找符合所需人员属性、匹配岗位的员工人才；营销人员则能利用智能大搜查找企业营销知识库的详细内容等等。员工使用搜索功能查找到所需资料后，可以直接使用这些资料智能生成文档、报告、知识图谱，节约大量文书工作的时间和精力投入。</p><p></p><p>企业员工还能使用手机遥控桌面打开企业应用进行展示。业务人员可以利用大模型代码生成框架，将自然语言自动转化为所需的代码脚本，开发人员在前端开发过程中也能受益于代码自动补全能力。最后，企业交流群中的群组机器人能够随时响应员工的问题，给出准确、实时的回答。</p><p></p><h2>AI 赋能，平台建设才是标准解法</h2><p></p><p><img src="https://static001.geekbang.org/infoq/9c/9c437b44081476bbc408b69fc587ef71.png" /></p><p>如今，通过 AI 技术重构应用，将 AI 算法模型深度融入行业与领域场景，成为颠覆传统业务模式，创新商业形态的最佳路径。</p><p></p><p>用友 iuap 智能平台 YonAI，再一次证明任何创新技术想要真正落地到企业业务层面，为企业带来看得见的收益，都不能仅靠技术本身的孤立应用来达成目标。尤其对于 AI 大模型这样具备颠覆性能力的创新，更要融入企业数智平台建设才能发挥更大效应。</p><p></p><p>YonAI 作为用友历时多年建设的企业数智化底座用友 iuap 的 AI 能力引擎，帮助企业升级数智底座，实现智能运营。在用友 iuap 平台中，云技术、应用、数据、开发和连接集成平台共同为 YonAI 智能平台的能力提供支撑。而 YonAI 的智能能力则通过这些平台延伸到企业业务的十大领域和每一个具体场景中。通过平台化建设，用友解决了 AI 赋能企业的最大挑战，使 AI 落地过程“润物无声”，也为行业给出了一套标准解法。</p><p></p><p>用友 iuap 通过融合六大平台、YonGPT 大模型以及工程化体系和运营体系能力，构建起完整的数智化平台能力，帮助企业搭建起智能运营、数据驱动、敏捷创新、开放连接和全球化支撑等核心能力，加速企业数智化进程！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/dsjckWx5uVJZBH00xGpx</id>
            <title>星尘智能发布新一代AI机器人Astribot S1，煮饭泡茶打拳投篮...样样都能干？</title>
            <link>https://www.infoq.cn/article/dsjckWx5uVJZBH00xGpx</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/dsjckWx5uVJZBH00xGpx</guid>
            <pubDate></pubDate>
            <updated>Tue, 20 Aug 2024 09:54:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>整理 | 华卫</p><p></p><p>8月19日，星尘智能发布新一代AI机器人助理Astribot S1，并展现了其基于面向AI（Design for AI）的软硬件一体化系统架构的泛场景通用操作能力。8月21日，S1将于在北京举办的世界机器人大会上对公众亮相。</p><p></p><p>S1是具备全能操作的具身人形机器人，在今年四月首次技术展示中，执行了熨叠衣物、分拣物品、颠锅炒菜、吸尘清洁、竞技叠杯等多项复杂任务，引发广泛关注。此次S1以整机形态亮相，完成了一系列高难度、长序列、可泛化任务。</p><p></p><p>在1倍速（业界常见为3到10倍速）的视频展示中，S1可谓智能又全能，在食物制作、泡功夫茶、乐器演奏等长序列任务展现了智能规划与最强操作，在模仿咏春拳、定点投篮等特技上展现了媲美专家的敏捷、灵巧与丝滑度。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/f1/f11835c78d8e6892c88a072669906892.jpeg" /></p><p></p><p></p><p></p><p>据介绍， S1将“AI智能”与“最强操作”强耦合，让机器人高度仿人，能像人一样学习、思考和劳动，与人流畅智能地交互，使用人的工具和设备、帮人完成枯燥、困难或危险的任务。AI智能方面，S1具备在复杂环境中的感知、认知、实时决策能力，及智能理解和多模态交互执行能力，实现物体、任务和环境级别通用操作泛化。</p><p></p><p>值得注意的是，星尘智能在具身智能数据获取上取得关键性突破，S1能低成本利用现有的真实世界视频数据和人体动作捕捉数据，并通过第一人称视角收集触觉、力觉、视觉、听觉等多维度的高质量数据。综合这些数据进行更高效的规模化训练，降低了机器人高质量数据采集的成本、数据量级和新任务的训练难度，提升了泛化能力的潜力。</p><p></p><p>机器人硬件方面，S1能以低成本实现同规格机器人中的“操作”。其独特的刚柔耦合传动机构设计，通过传感器实时监测力的传输，不再依赖轨迹估算，而是像人一样，通过感知力的大小来精准控制控制力的输出，显著提升操作精度。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/e5/e535ec734feb48762f114a583bddc034.jpeg" /></p><p></p><p></p><p></p><p>S1具备“高价值的上半身，可落地的下半身”，可用于科研、商业和家庭等广泛场景，预计于2024年完成商业化。其关键零部件自研，具备明显的成本优势。通过刚柔耦合硬件设计和创新力规划算法，S1具备极高安全性，能在交互中精确控制力度，在运动中不伤人、不伤物、不伤自己。</p><p></p><p></p><p>星尘智能 CEO 来杰表示：“我们的愿景是让数十亿人拥有 AI 机器人助理。无论是照顾家庭还是到工厂工作，机器人在学习、决策和执行上越像人，越能帮人做得更多和更好，因此欢迎大家给S1提需求，让它的能力能从55%、85%成长到99.99%，无限接近人类水平。也希望未来五年到十年，AI机器人就能走进千家万户。”</p><p></p><p></p><p>公开资料显示，星尘智能（Astribot）于2022年底在深圳成立，公司已完成数千万美元Pre-A轮融资，由经纬创投领投，道彤投资及清辉投资等产业资本跟投，老股东云启资本跟投。创始人来杰拥有16年机器人研发经验，曾是腾讯机器人实验室1号员工、百度“小度机器人”负责人等，持续推动机器人与人工智能技术结合，让AI机器人从梦想变为现实。团队来自腾讯、谷歌、华为、大疆等企业，及国内外顶尖高校和人工智能研究院。</p><p></p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/oFNB1gBmecXEox4q38sN</id>
            <title>国产大模型超越Llama3！岩芯数智RockAI重新定义端侧智能</title>
            <link>https://www.infoq.cn/article/oFNB1gBmecXEox4q38sN</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/oFNB1gBmecXEox4q38sN</guid>
            <pubDate></pubDate>
            <updated>Tue, 20 Aug 2024 09:02:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>8月18-19日，AICon全球人工智能开发与应用大会在上海举办，以“智能未来，探索 AI 无限可能”为主题，聚焦大模型开发与应用领域。RockAI CEO刘凡平应邀出席并发表《非Transformer架构的端侧大模型创新研究与应用》主题演讲，重新定义端侧智能，引发了行业对端侧AI落地方向的全新思考。</p><p>&nbsp;</p><p>众所周知，端侧AI通常指在终端设备上直接运行和处理人工智能算法的技术，具有减少云端算力依赖、保证用户数据安全等优势。目前，行业普遍将算力限制和数据匮乏视同端侧AI技术发展的拦路虎。而RockAI则认为，基础架构和核心算法的创新才是突破端侧AI发展局限的关键。基于对算法和架构的创新，即使面临算力限制，端侧AI仍可在终端设备上实现流畅的智能多模态运用。</p><p>&nbsp;</p><p>这一观点也在RockAI关于Yan架构大模型的创新实践上得到了证明。其推出的国内首个非Attention机制的Yan架构大模型，可在主流消费级CPU等端侧设备上无损运行，达到其他模型GPU上的运行效果。全面升级后，Yan1.2多模态大模型，已经可以在树莓派、机器人、手机等低功耗计算平台无损流畅运行，将端侧应用场景拓宽至智能家居、物联网等领域。而最新数据显示，3B参数的Yan1.3&nbsp;preview大模型在各项测评中的平均得分甚至超越了8B参数的Llama3，达到极高的知识密度。</p><p></p><p><img src="https://static001.geekbang.org/infoq/24/244def87f3af46930572ec29941ce5a3.png" /></p><p></p><p>论坛现场，刘凡平深入剖析了当前端侧AI技术的发展现状及局限性。他指出，目前大多数“狭义端侧模型”的核心目标在于为用户提供大语言模型推理服务，受限于模型参数、算力、软件生态、功耗控制等诸多难题，往往会通过压缩、分割等软硬件协同优化实现大语言模型在终端设备上的本地化应用。但端侧AI的未来不仅仅在于推理能力的提升，更在于能够实现模型的自我学习和优化，以适应不断变化的应用场景和愈发广阔的用户需求。而通过以上处理手段，模型是无法在端侧进行训练和微调的，更不必说实现自我学习。</p><p>&nbsp;</p><p>刘凡平强调，RockAI不做“狭义的端侧模型”，而是着眼于更广泛意义上的端侧智能，即让世界上每一台设备都拥有自己的智能。这要求端侧模型除了语言理解及生成能力外，还应该具备抽象思考、因果推理、自我反思以及跨领域迁移学习等更复杂的认知功能。因此，端侧模型需要至少支持“理解表达、选择遗忘、持续学习”三种基础能力。</p><p>&nbsp;</p><p>为达成这一目标，RockAI在基础架构创新和实现消费级终端无损部署外，首创了“同步学习”机制。该机制可以使大模型在推理的同时进行知识更新和学习，建立自己独有的知识体系，实现模型的边跑边进化。同时，通过跨模态关联学习，增强模型在多场景下的应用能力，实现秒级实时反馈的人机交互，真正做到端侧模型的自我学习、类人感知和实时交互，推动端侧AI向自适应智能进化阶段演进。</p><p>&nbsp;</p><p>RockAI基于Yan架构大模型的技术突破和创新实践，打破了当前端侧AI发展的技术壁垒，不仅为整个行业的发展提供了新的思路和方向，也预示着端侧AI正朝着更广泛的应用场景稳步前进。待同步学习+全模态+实时人机交互落地后，Yan2.0的诞生将重新定义端侧智能，真正赋予机器自主学习与自我优化能力，构建持续进化乃至群体智能涌现的AGI智慧生态。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ASUQJv0DT6JQuFlxZLLA</id>
            <title>未来智能CTO王松：会议中的AI Agent，从小任务到全场景的技术突破</title>
            <link>https://www.infoq.cn/article/ASUQJv0DT6JQuFlxZLLA</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ASUQJv0DT6JQuFlxZLLA</guid>
            <pubDate></pubDate>
            <updated>Tue, 20 Aug 2024 03:05:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>近期，前谷歌CEO施密特在斯坦福大学的一场闭门演讲引发了广泛争议。无论其观点的正确与否，他的观点确揭示了一个事实：人工智能竞赛似乎是一场只有中美两国能参与的“游戏”。然而，两国在人工智能发展路线上的差异又十分显著。美国侧重于平台的研究与开发，而我国则更注重场景的应用与商业闭环的构建。我国的人工智能发展更加强调实用性，而非单纯的能力展示。</p><p></p><p>8月18日-8月19日，在上海举办的AICon 2024全球人工智能开发与应用大会进一步印证了这一差异。该大会以"智能未来，探索AI无限可能"为主题，探讨了 AI 商业洞察和 AI 原生产品的探索路径，以及大模型和多模态技术的实践和成功应用案例。其中的解决方案专场，则以“大模型在多场景下的部署与应用”为专题，邀请国内人工智能明星企业分享了当下的技术实践。其中人工智能硬件公司未来智能CTO王松受邀参加了解决方案专场，向业界全面展示了AI Agent在个人会议领域的探索和应用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7a/7a148b20c80c001f3cd32af476715814.png" /></p><p>未来智能CTO 王松</p><p></p><p></p><h2>创新与实用结合：未来智能会议Agent引领AI应用趋势</h2><p></p><p></p><p>未来智能是办公会议耳机赛道的领军企业，自创立之来就以AI为基础，聚焦办公会议场景，致力于用AI解决用户办公会议痛点，成功打造了一系列将AI做到实用的人工智能硬件产品。在AI Agent领域，未来智能依托强大的数据基础，早在行业初期就开始布局相关技术研发和探索。</p><p></p><p>在大会上，王松详细介绍了未来智能会议Agent如何通过“感知”、“推理”、“记忆”、“执行”四大模块，精准识别用户场景，并在不同场景下解决用户痛点，提升用户效率。</p><p></p><p>未来智能会议Agent的技术探索始终以办公会议场景为核心，致力于解决用户在办公会议中的痛点。对于职场办公人群来说，大量的时间被各式各样的会议占据，而这些会议中有的充斥着无效信息，有的则需要会前准备大量资料，会后还需进行会议纪要总结。如何提高会议效率，正是职场人士面临的一大难题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/35/352e56bc5c0d3e828b9f21f7b02c6a37.png" /></p><p></p><p>作为一个为用户打造全链路的会议智能体，未来智能会议Agent的每个模块都有着明确的职责和具体的功能设计。在感知模块中，未来智能会议Agent能够自动收集会议通知并创建会议日程，从会议开始就帮助用户提高效率。由于会议信息主要来源于线上和线下两大信息源，因此会议Agent在获得系统或硬件的授权后，可通过技术手段获取相关信息，自动完成任务创建。</p><p></p><p>在推理模块，当下的LLM大模型依旧存在着能力不足等问题，未来智能则通过自研垂直模型，依托人类处理不同问题时的经验、知识，自适应选择合适的解题思路。通过工程化的方式，未来智能不断提升会议Agent“大脑”的能力，并为未来更高级的LLM铺路，不断积累训练数据。</p><p></p><p>在记忆模块，未来智能会议Agent则是在场景之下强化数据的嵌入，向模拟人脑的记忆工作进化，让Agent具备长期和短期记忆，能够实现高准确度和命中率，还能快速的访问和存取。而在执行模块，未来智能会议Agent则是通过LLM来实现任务的落地和最终执行。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b6/b6b7c3c4bfb719cc32db76d8f411afd9.png" /></p><p></p><p>未来智能会议Agent的技术进步迅速。在大会上，王松就会议Agent的技术水平与新能源汽车智能驾驶相类比，预计未来智能会议Agent将在明年基本实现L2.5-L3水平，用户能够通过端到端的解决方案，让AI自动完成用户在会议中的相关任务，就像当下新能源汽车的高阶智驾一样，用户仅需手扶方向盘即可。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1e/1ee53d5b9f4afa9d5933368ce674da91.png" /></p><p></p><p></p><h2>从小场景小任务做起，让AI在使用中不断进化</h2><p></p><p></p><p>未来智能会议Agent的技术探索并没有停留在理论层面，而是从更小的场景和更小的任务出发，通过一个个功能点的创新，让用户先受益起来。</p><p></p><p>例如，面对冗长繁琐的会议内容，讯飞会议耳机内置的viaim AI，能够智能分析记录内容，自动提取记录中的重点，2小时会议可一键生成「摘要总结」，大幅简化会后总结难度，让会议核心内容一目了然。viaim AI还能提取记录中的关键任务，一键生成「待办事项」，帮助用户轻松跟踪会后内容。</p><p></p><p>viaim AI还拥有「快速问答」功能，用户只需语音/文字输入问题，viaim AI就能回答用户关于当前记录内提到的问题和扩展问题，让用户快速获取记录内容中需要的信息。随着未来智能AI技术的不断进化，viaim AI也会常用常新，不断为用户带来更多优秀的体验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/65/657d07a368010a6c1e05e3c62d0776d0.png" /></p><p></p><p>除此之外，未来智能不仅在通用场景中取得了进展，还深入细分行业领域，展开了广泛的探索。例如，在金融领域，未来智能正在探索如何让讯飞会议耳机自动识别金融相关的会议内容，耳机会在会后调用常用的开源或商业化的金融领域大模型，自动生成专业的会议报告。这些报告能够涵盖投研、ESG、财经、财报等多个投研相关细分领域，提供一系列专业的AI支持能力。</p><p></p><p>这种针对具体行业的会议技术解决方案，展示了未来智能技术的深度和实践能力。均给参会的行业人士带来了深刻的印象。</p><p></p><p>未来智能在AI Agent领域的探索，不仅展示了中国企业在技术路线上的独特优势，还体现了中国企业对实用性和场景适用性上的深刻理解。这种在技术发展与商业闭环之间的平衡，或许正是推动人工智能行业健康发展的关键所在。可以说，中国的人工智能企业正在以自己的方式引领全球人工智能发展的新趋势。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Gx8XXk8OF8j05DTfbiis</id>
            <title>智慧海淀：率先构建人工智能全场景赋能的创新生态</title>
            <link>https://www.infoq.cn/article/Gx8XXk8OF8j05DTfbiis</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Gx8XXk8OF8j05DTfbiis</guid>
            <pubDate></pubDate>
            <updated>Tue, 20 Aug 2024 02:58:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>今天，2024 北京人工智能生态大会在北京海淀成功举办。会议以“智绘新篇 算赢未来”为主题，聚焦数据、算力、算法等关键要素与核心技术，围绕大模型、具身智能、可信 AI 等国内外前沿热点话题展开深入交流，搭建合作平台，凝聚人工智能、数字经济与实体经济融合发展的强劲动能。</p><p></p><p>工业和信息化部科技司副司长赵超凡、北京市人民政府副秘书长许心超、北京市海淀区委书记张革以及中关村数字经济产业联盟轮值理事长、北京能源集团有限责任公司党委书记、董事长姜帆，华为技术有限公司副总裁、华为中国政企业务总裁吴辉等政府部门、企业代表受邀出席活动。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ae/ae82a152be2a0d6e02ef577e275e80f9.webp" /></p><p></p><p>会议伊始，中国工程院院士、中国工程院原副院长、中关村数字经济产业联盟顾问邬贺铨围绕人工智能产学研最新趋势与成果等内容做了精彩的主题分享。</p><p></p><p>紧接着，中关村科学城管委会副主任、海淀区副区长唐超发布《中关村科学城人工智能全景赋能行动计划》，市发改委、市科委中关村管委会、市经信局、市科协、海淀区等单位领导共同启动海淀区人工智能创新应用加速器“格物社区”。</p><p></p><h2>场景加速、创新领航，《中关村科学城人工智能全景赋能行动计划》正式发布</h2><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/c8/c8ce468ced1fa7cfa8fef33f726465a2.webp" /></p><p></p><p>为全面落实国家发展新一代人工智能的决策部署，海淀区抢抓人工智能大模型技术革新机遇，以场景应用为牵引，加速人工智能在千行百业赋能赋智赋力，推动科技创新势能转化为高质量发展新动能，助推新质生产力示范区建设，特制定本行动计划。</p><p></p><p>《中关村科学城人工智能全景赋能行动计划（2024-2026年）》（以下简称《行动计划》）的发布，标志着中关村科学城在人工智能领域的进一步深化和拓展。《行动计划》提到，将以人工智能创新街区为“主阵地”，以实施十大应用示范工程为“主平台”，以国产全栈技术创新迭代为“主引擎”，把握人工智能发展“主动权”，加快探索人工智能和千行百业的双向赋能路径，率先在全国建成首个人工智能创新街区、首个人工智能应用加速器，将中关村科学城打造成为人工智能全景赋能第一城。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f9/f97e403a4d7909aebca2b1d35d5736b9.webp" /></p><p></p><p>对于区内企业而言，《行动计划》不仅为人工智能产业提供了明确的发展方向和目标，更为企业提供了实实在在的支持和机遇。《行动计划》提出，到 2026 年，力争产出 100 个标杆行业模型产品，形成 100 个示范引领典型案例，形成千家企业千亿集群引领带动万亿经济发展新局面。《行动计划》还提到，将大力支持各行业优势主体组织形成创新应用联合体，依托联合体建设人工智能创新应用加速器，面向未来催生一批民生、科研和产业领域的新产品、新场景、新服务，提高从技术研发到产品落地和商业应用的效率和成功率。</p><p></p><p>值得注意的是，随着《行动计划》的实施，企业能够快速参与到具身智能、医药健康、自动驾驶等十大人工智能前沿领域的研发和应用中，加速产品和服务的创新。同时，依托创新应用加速器与示范工程新机遇，企业能够获得技术验证、市场对接和示范推广的机会，促进科技成果的快速转化。此外，面向人工智能应用创新的共性需求，海淀区将构建多维度、系统化的支持体系，形成统一完备的支撑服务力量。这将为企业提供包括数据支持、算力资源、标准制定和知识产权保护等在内的多方面的助力。</p><p></p><p>事实上，素有“中国硅谷”之称的海淀，在人工智能领域深耕布局已久，也取得了一系列丰硕成果。数据显示，北京发布大模型、备案上线大模型数量占全国半数以上，百川智能、智谱华章等优质的通用大模型企业皆出自于海淀。科技部发布的《中国人工智能大模型地图研究报告》显示，北京在大模型学者指数、模型开源数量和影响力等指标上，均为全国首位。而海淀作为北京人工智能产业发展的“领头羊”，拥有以清华、北大为代表的 37 所高校、96 家科研院所以及 31 个国家工程研究中心，吸引了北京市超过八成以上的人工智能学者扎根。同时，海淀为了发挥其在人工智能等科创领域的优势力量，计划全面打造一个 53 平方公里的人工智能创新街区，该街区将串联起 37 所高校、12 个新型研发机构、52 个全国重点实验室、106 个国家级科研机构、1300 家人工智能企业的科技成果，汇聚起 1.23 万人工智能学者和 101 位“AI2000”全球顶尖学者。</p><p></p><p>政策方面，从《关于加快中关村科学城人工智能大模型创新发展的若干措施》到《中关村科学城通用人工智能创新引领发展实施方案（2023—2025 年）》再到《行动计划》，一盘围绕“人工智能 +”的大棋正徐徐展开，一系列的政策支持正在持续为海淀企业构筑一个营商环境良好、鼓励科研创新、吸引尖端人才的优质生态环境，同时也将进一步巩固海淀在全球科技竞争中的领先地位。</p><p></p><p><img src="https://static001.geekbang.org/infoq/63/6379dac801c80d31ae0e45adc8917bee.webp" /></p><p></p><p>现场启动的“格物社区·人工智能创新应用加速器”是海淀区为加快推进人工智能全景赋能的重要举措之一。据悉，格物社区加速器由北京海新智能人工智能科技有限公司负责运营和管理。团队的主要人员长期从事人工智能领域算法模型和架构设计研发，包括视觉，感知模型训练，主导研发视觉感知、大模型、多模态、机器人 / 自动驾驶等前沿技术算法方向，行业涉及智慧城市 / 社区、智能创作、自动驾驶以及智能交通等多个领域。</p><p></p><p>格物社区一方面由 AI 龙头企业提供各类支持、建设 AI 软硬件开源平台，为创业者提供封装好的数据、模型调度、多模态组件和硬件、以及推理算力，实现从创意到产品的加速，另一方面设计面向未来的重点产业领域的各类模拟真实世界的场景，各类主体可以开展用户调研、产品设计验证平台，用户反馈良好的设计，由工程师团队协助转化。协助产业龙头企业围绕场景创新和中小企业构建生态，形成大企业为核心的技术创新体系，实现创新联合体共赢。</p><p></p><p>随后，华为、智谱华章、京能、工商银行、京东、昆仑智算、中国移动、百川智能、清昴智能、第四范式、蚂蚁集团等企业分别带来了其在“人工智能 +”方面的生态探索与产业实践，展示了人工智能在千行百业中的前沿应用与未来想象。</p><p></p><p>值得一提的是，潞晨昇腾超级工作站、中科闻歌优雅大模型 3.0、瑞莱智慧生成式人工智能内容检测一体机等一批人工智能领域优质项目产品也在大会上进行了集中发布。</p><p></p><p></p><p></p><h2>应用先锋、百花齐放，多家企业发布“人工智能 +”最新成果</h2><p></p><p></p><p>从“数字化”到“数智化”，"人工智能 + 场景"的结合，正在重新定义技术与行业的融合方式，进而成为推动社会进步和产业升级的关键力量。人工智能技术的引入，不仅仅是技术的叠加，更是一种全新的生产力和创新模式的体现，通过深度学习和数据分析，能够优化决策过程，提高效率，降低成本，并在各个行业中创造出前所未有的价值。</p><p></p><p></p><h4>中数联盟发布《北京首批“人工智能 +”应用场景典型案例研究报告》</h4><p></p><p>会上，中关村数字经济产业联盟轮值理事长、华为北京总经理张东亚发布了《北京首批“人工智能 +”应用场景典型案例研究报告》。</p><p></p><p>这份报告汇集了人工智能领域的前沿案例和对场景融合、未来趋势的深刻洞察等。不仅代表了当前人工智能技术的最新成果与应用实践，也展现了北京在推动人工智能与实体经济深度融合、促进产业升级转型方面的积极探索与显著成效。</p><p></p><p>多年来，北京一直高度重视人工智能产业发展，致力于打造人工智能技术创新策源地和产业发展排头兵，人工智能产业也一直处于国内领先水平，由此孵化出了相当多的优秀企业、优秀案例，通过持续的技术创新与应用示范，不仅解决了行业痛点，提升了生产效率与服务质量，还带动了相关产业链的协同发展，为人工智能产业的繁荣注入了强劲动力。此类经验将为更多企业提供宝贵的启示与借鉴，进一步激发人工智能技术在各领域的广泛应用与深入探索。</p><p></p><p></p><h4>国家区块链技术创新中心发布“高价值语料可信安全基础设施”</h4><p></p><p>人工智能三要素“算力、算法、数据”，缺一不可。尤其是在产业实践中，高价值的语料数据是技术落地的关键。现实情况是：高价值语料数据存在跨单位、跨行业、跨地区“烟囱式”孤立分布的问题。由于缺乏足够的隐私安全保障和有效的激励机制，语料数据拥有者往往“不敢分享”“不愿分享”，造成大量高价值语料数据“供给难、流通难、使用难”，已成为我国人工智能进一步发展的瓶颈。</p><p></p><p>值得一提的是，以区块链、隐私计算为代表的新一代信息技术，凭借着可信存证、不可篡改、易确权、充分保护数据隐私安全等优异性能，可以保障语料数据可信安全地流通、使用和管理，成为了解决难题的一把金钥匙。</p><p></p><p>会上，国家区块链技术创新中心相关负责人宣布——国家区块链技术创新中心高价值语料可信流通基础设施启动。据悉，国家区块链技术创新中心由北京微芯区块链边缘与计算研究院牵头建设，其成员单位将运用我国自主可控、性能领先的区块链软硬件一体基础设施，搭建起覆盖全国的分布式语料数据互联互通桥梁，链接语料供给方、加工方、需求方，实现全国分布式语料数据可信接入，跨地域可发现、可访问，形成高质量语料数据集。同时，运用区块链智能合约技术，实现语料数据流通全链路透明、自动“计量结算”。运用创新隐私计算技术，保障大模型高价值语料数据在处理加工和模型训练过程中无法二次传播。</p><p></p><p>“高价值语料可信安全基础设施”的启动，是海淀区在人工智能生态建设与高质量发展方面的一步大棋，将为国内高价值语料的互通、互信、安全奠定基础，从而加速推动我国人工智能领域通用大模型与行业大模型的蓬勃发展。</p><p></p><p></p><h4>潞晨科技发布潞晨昇腾超级工作站</h4><p></p><p>除了数据，算力同样是制约人工智能技术发展与落地的关键。其中国产化算力是一个绕不开的话题。会议现场，潞晨科技与华为携手，宣布推出潞晨昇腾超级工作站。</p><p></p><p>据悉，潞晨昇腾超级工作站包含潞晨昇腾训推一体机和潞晨昇腾办公一体机。其中，潞晨昇腾训推一体机，以其卓越性能和无缝迁移特性，显著提升了大模型的训练效率，降低了企业的 AI 大模型部署门槛。同时，潞晨昇腾训推一体机提供零门槛迁移方案，为企业从其他硬件过渡到昇腾平台提供了无缝衔接，显著降低了迁移的经济和技术成本。</p><p></p><p>潞晨昇腾超级工作站的发布，让企业能够更加轻松地拥抱 AI，从而加速数字化转型的步伐，也体现了海淀区在构建自主可控、安全可靠的智能计算平台方面的持续探索与贡献。</p><p></p><p></p><h4>中科闻歌发布优雅大模型 3.0 及最新成果</h4><p></p><p>算法层面，随着大模型技术的持续火热，多模态算法逐渐成为了“兵家必争之地”。多模态技术通过整合视觉、语言、声音等多种数据类型，实现了对复杂信息的更深层次理解和处理，极大地拓宽了人工智能的应用范围和能力。</p><p></p><p>在多模态技术方面，海淀区同样也是技术创新与交流的高地。不久前结束的“2024 年多模态大模型高峰论坛”落子海淀，来自国内知名高校、研究机构和企业的专家，现场分享了多模态大模型的最新技术进展和行业成果。另外，在中科院自动化研究所的带领下，海淀区在多模态与具身智能、面向 OCR（光学字符识别）领域的多模态大模型构建应用等方面取得了显著的成果。</p><p></p><p>会议现场，同样孵化于中科院的科创企业——中科闻歌也携优雅大模型 3.0 亮相并发布了最新成果。</p><p></p><p>据悉，优雅大模型 3.0 具备 15 种以上图、视频编目和高阶语义检索能力，准确率达 90% 以上。这一成果的发布，不仅是技术层面的突破，更是对内容创作行业的一次革命性推动。模型多智能体总参数超过 40B，全面赋能包括素材清洗、故事文案生成、文生数字人、视频生成、智能运镜、智能剪辑以及专业化成片等内容创作的各个环节，可将专业化视频内容创作的成本降低 80%。</p><p></p><p>通过优雅大模型 3.0 的应用，无论是媒体公司、广告制作团队还是独立创作者，都能够以更低的成本、更快的速度生产出高质量的视频内容，满足市场对于个性化、多样化内容的需求。不仅极大地提升了内容生产的效率和质量，也为创作者们提供了更多的想象空间和创作自由。</p><p></p><h4>瑞莱智慧发布生成式人工智能内容检测一体机</h4><p></p><p>当然，科技也是一把双刃剑，AIGC 大行其道所带来的信息造假等安全问题同样层出不穷，如何甄别并确保安全成为了 AI 技术落地之后不得不面对的问题。</p><p></p><p>在 AI 安全方面，海淀区同样未雨绸缪，在《中关村科学城通用人工智能创新引领发展实施方案（2023—2025 年）》中，特别强化了人工智能伦理安全规范及社会治理实践研究，并致力于建设科技伦理治理公共服务平台，以服务政府监管，促进行业自律。此外，还计划通过开展科技伦理审查及相关业务培训，加强各责任主体的科技伦理规范意识，并推动科技伦理教育和宣传，从而构建一个良好的人工智能科技伦理氛围。</p><p></p><p>会议现场，人工智能安全领域领先企业——瑞莱智慧发布并展示了他们在“人工智能 + 安全”方面的最新成果：瑞莱智慧 AIGC 检测一体机 DeepReal。</p><p></p><p>据介绍，DeepReal 支持多合成类型的图片、视频、音频、文本的真伪检测，合成疑似特征分析，以及可解释性报告的自动输出等功能，具备技术领先、高准确率、高鲁棒性、运算高效的特点。</p><p></p><p>瑞莱智慧 RealAI 联合创始人、算法科学家萧子豪表示，DeepReal 依托第三代人工智能技术，通过辨识伪造内容和真实内容的表征差异性、挖掘不同生成途径的伪造内容一致性特征，能够快速、精准地对图像、视频、音频、文本内容进行真伪鉴别，有效打击 AI 诈骗、色情黑产、虚假宣传、证据造假等违法违规行为。</p><p></p><p>大会还为入选北京首批“人工智能 +”应用场景十佳案例及北京首批“人工智能 +”应用场景典型案例颁发了荣誉证书，以表彰获奖的企事业单位在推动人工智能技术发展和产业落地方面的卓越贡献。</p><p></p><p><img src="https://static001.geekbang.org/infoq/99/99170aafbede04236b9fea50058ce4a7.webp" /></p><p></p><p>透过此次人工智能生态大会，不难看出：“人工智能 + 场景实践”是海淀区关注的焦点和支持的重点。领先的技术只有通过具体的行业应用，才能展现出其真正的价值和潜力。海淀区正通过一系列政策扶持和资源整合，为人工智能的场景应用提供肥沃的土壤。在这里，企业、院校、科研机构等能够获得从优惠政策到技术研发再到市场应用推广等全方位支持，加速技术的创新和产品的迭代。同样，这些受益者们也正在用一个个领先的研发成果去丰富海淀“人工智能 +”的场景应用创新生态和科创土壤。</p><p></p><p>随着《行动计划》深入实施，海淀区的人工智能产业将更加贴近市场需求，更进一步推动技术与实体经济的深度融合，从而实现更高效的产业升级和经济增长。当然，务实的策略和持续创新的生态建设，也将为区域发展注入源源不断的新动力，全景赋能人工智能技术的创新与场景落地，进而加速海淀区成为 AI 时代的人工智能场景应用示范区，并在全球科创高地百尺竿头、更进一步。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/8wQok08Kk6JVuulQCq81</id>
            <title>250+ AI新创意！百度黑客马拉松大赛“专攻”智能体</title>
            <link>https://www.infoq.cn/article/8wQok08Kk6JVuulQCq81</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/8wQok08Kk6JVuulQCq81</guid>
            <pubDate></pubDate>
            <updated>Mon, 19 Aug 2024 05:25:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>8月19日，据百度官方公众号显示，在百度内部举办的黑客马拉松比赛时上，参赛选手共提出256个AI创意，其中约70%与智能体有关。今年以来，百度正在加速推动智能体发展，上线百度文心智能体平台，并在该平台上免费开放了文心大模型4.0。</p><p></p><p>据悉，今年百度黑客马拉松以智能体和AI原生应用为主题，共计创作、娱乐、提效工具、信息获取、行业智能、探索未来6大等六大创新赛道。在今年的参赛创意中，有超过60个智能体创意进入了“复赛”的集市展示环节，包括“记忆帮”认知训练助手、“咔嚓！人人都是摄影师”AI相机、AI文创打印机、体育赛事观看AI助手、“拜电子神仙攒赛博功德”许愿顾问、“攻程宝典”面试助手、“卡皮巴拉很担心你”电子宠物等各类创意。</p><p> </p><p>以获得一等奖的“记忆帮”认知训练助手为例，这款智能体目的是让老人获得专业、便宜、易得的专属AI认知训练师，能对老人进行针对性训练，为老人提供亲和性更强、在家就能完成训练的AI助手。</p><p></p><p>资料显示，黑客马拉松是是百度面向全员的内部技术创新大赛，迄今为止已持续12年、共计28季，累计产生8000多个创意和300多项专利。多年来，黑客马拉松已不仅是内部比赛，也是诸多百度技术产品创新的孵化地，例如智能翻译机、景区热力图、夜莺（智能客服）、以及最新上线的免费AI律师“法行宝”，均诞生于这场大赛。</p><p>﻿</p><p>多方资料显示，百度正在加速推进智能体的落地和生态。今年4月，百度上线百度文心智能体平台，不久之后，还在该平台上免费开放了文心大模型4.0，进一步利好开发者。截至目前，该平台已汇聚20万开发者、累计6.3万家企业入驻。</p><p></p><p>不久前，百度创始人、董事长兼首席执行官李彦宏在演讲中表示，智能体是最看好的AI应用方向，这是开发最简单的AI应用，未来，将会有数百万量级的智能体出现，形成庞大的智能体生态。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/WftWfDRVcIPQugIT6g6i</id>
            <title>朱啸虎押注的AI公司遭前员工围攻，“去死”成创始人口头禅；小红书取消R职级，将激励和职级晋升解绑；谷歌前CEO：AI创业可先“偷”后“处理”｜AI周报</title>
            <link>https://www.infoq.cn/article/WftWfDRVcIPQugIT6g6i</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/WftWfDRVcIPQugIT6g6i</guid>
            <pubDate></pubDate>
            <updated>Mon, 19 Aug 2024 02:57:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p></p><h2>行业热点</h2><p></p><p>&nbsp;</p><p></p><h4>AI公司FancyTech遭前员工围攻声讨：压榨员工，“去死”成创始人口头禅</h4><p></p><p>&nbsp;</p><p>据报道，朱啸虎曾在公开场合多次称赞，甚至连续3次投资押注的AIGC公司——FancyTech，在一场创始人的访谈播客发布后，迎来了众多前员工的围攻。前员工的声讨主要有三方面：</p><p>&nbsp;</p><p>一、最大化压榨员工，倾向于低薪招聘实习生，公司200多名实习生“扛大旗”；</p><p>二、领导人暴躁易怒，情绪不稳定，让员工“去死”多次出现在聊天记录中；</p><p>三、人工智能被质疑技术底色薄弱，技术不行人力凑，被指公司是用网线连接了人脑，“AI是用来吹牛的”。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/e0/e099faa76b9abfdad4df0d470ee6e215.png" /></p><p></p><p>对于FancyTech被指缺乏员工关怀一事，有FancyTech前员工表示：“这个老板，不知道让多少员工‘去死’了。”该人士透露称，自己正是因为受不了公司缺乏员工关怀的文化，才选择离职。甚至有FancyTech前员工直言“不明白为何朱啸虎会力捧它（指FancyTech）”“离开才是最正确的选择”。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/62/627d0aee4145680b68866fd11b5e5d1a.png" /></p><p></p><p>&nbsp;</p><p>而对于公司被指“AI不够，人力来凑一事”，FancyTech创始人William近期曾公开表示：“Fancytech的AI内容产出在初期对接需求和创意构思方面仍离不开人工和广告公司的参与。”此外，公司注重给客户交互AI成品内容而不是技术的路径选择，也为这样的操作留下了空间。</p><p>&nbsp;</p><p></p><h4>秘塔AI下架知网相关文献信息，此前收到28页侵权告知函</h4><p></p><p>8月16日，秘塔公司在公众号上发文表示，他们昨日收到了《中国学术期刊（光盘版）》电子杂志社有限公司（下称“知网”）长达28页的侵权告知函，知网方面要求秘塔停止提供其数据库内的文献信息。</p><p>&nbsp;</p><p>对此，秘塔公司表示，作为全网首个设置了“学术”搜索板块的AI搜索引擎，他们仅收录了论文摘要和目录信息，而且团队认为知识发现是科学研究进步的重要环节。但目前他们选择尊重知网的诉求，下架了知网相关的文献信息，以后会用其他的中英文献库代替。</p><p>&nbsp;</p><p></p><h4>小红书取消R职级，将激励和职级晋升解绑</h4><p></p><p>&nbsp;</p><p>8月16日，小红书发布全员信宣布调整组织职级，其中包括：不再设置R职级；简化管理层级，不再设置L0；各级Leader采取任命制。</p><p>&nbsp;</p><p>小红书在邮件中说明，“从组织调研看到成长所带来的熵增。比如组织层级有变深的倾向，信息在传递的过程中层层折损，决策效率变慢，不够敏捷；比如职级体系更容易带来论资排辈而不是更快地发现一线人才，也没有最好地做实‘让自驱有战场，让成事有回报’的组织理念。”</p><p>&nbsp;</p><p>此前，R职级可对标阿里P序列，是小红书职位层级的重要参考坐标。小红书的R5、R6、R7是三个常见的职级，5是骨干员工，6一般是小组长，7常为一个小/大部门的负责人。小红书曾在招聘中表示，R5的最大年龄不超过32岁，R6的最大不能超34岁。</p><p>&nbsp;</p><p>此外，职级体系调整后，薪酬更直接地挂钩工作难度，奖金挂钩工作结果。据业内猎头人士分析，互联网公司调薪主要和职级晋升相关，小红书此举将激励和职级晋升解绑，可以更及时充分地激励拿到结果的人才。此前，小红书已提拔了一批新业务负责人。</p><p>&nbsp;</p><p></p><h4>扫地机被曝成偷窥工具，相关产品已下架，科沃斯回应</h4><p></p><p>&nbsp;</p><p>近日，被称为“扫地机器人第一股”的科沃斯面临隐私安全的质疑。两位安全研究人员在参加Def Con安全大会时表示，他们发现科沃斯（Ecovacs）旗下的扫地机器人产品存在安全问题，通过蓝牙连接科沃斯机器人后，黑客可以通过产品自带的WiFi连接功能对其远程控制，并访问其操作系统中的房间地图、摄像头、麦克风等功能和信息。</p><p>&nbsp;</p><p>8月13日，针对旗下产品存在安全漏洞的质疑，科沃斯回应称，这些安全隐患在用户日常使用环境中的发生概率极低，需要专业的黑客工具且近距离接触机器才有可能完成，故用户不必为此过虑。公司将使用限制第二账户登录、加强蓝牙设备相互连接的二次验证等技术手段强化产品在蓝牙连接方面的安全性。</p><p>&nbsp;</p><p>同日，有媒体围绕此事咨询电商平台的科沃斯旗舰店，客服回复表示，新闻中提及的Ecovacs Deebot 900系列、Ecovacs Deebot N8/T8、Ecovacs Airbot ANDY等多款产品在店铺均已下架。针对下架的原因，以及可能存在的漏洞，科沃斯未回应。</p><p>&nbsp;</p><p></p><h4>6个算法岗位争夺1个候选人，顶尖大模型校招生年薪超200万</h4><p></p><p>&nbsp;</p><p>伴随人工智能(AI)日渐火热，“百模大战”激烈开打，AI人才掀起招聘热潮。据求职招聘平台数据显示，今年一季度，AI相关职位同比增加321.7%，投递该领域的人才数量同比增长946.84%。目前最紧缺的大模型算法岗位，人才供需比仅为0.17，大概相当于6个岗位争夺1个人才。</p><p>&nbsp;</p><p>“研究生普遍给到70万就算高价，对于博士而言，毕业薪资可以达到百万元。”深耕AI行业的猎头倪悦透露，面对国内C4(清华、北大、复旦、交大)重点实验室博士毕业生，大厂给出超过200万的年薪很常见。</p><p>&nbsp;</p><p>不过，如此高薪只局限于核心技术骨干，“国内做基座类模型的人才90%都出自清华，真正会调模型、训练模型的甚至不超过200个人。”</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>思科对销售预测持乐观态度，但计划进一步裁员超6300人，遣散费10亿美元</h4><p></p><p>&nbsp;</p><p>8月15日消息，据外媒报道，全球最大计算机网络设备制造商思科对当前季度的营收情况做出了乐观的预测，因为订单出现回升。但与此同时，思科也宣布了裁员计划，并声称这是“战略转型”的一部分。该公司在当地时间14日发布的一份声明中表示，在截至10月份的（2025财年）第一财季，销售额将达到137亿至139亿美元。分析师此前的估计仅达到了这一区间的最低端。</p><p>&nbsp;</p><p>据报道，此次裁员将使思科公司90400名员工的人数减少约 7%，即减少6300多个工作岗位。声明称，这将有助于公司转移工作重点，节省开支，尽管裁员会带来“短期成本”。不仅如此，思科还一直在消化今年早些时候收购的 Splunk。根据声明，裁员将使公司“投资于关键增长机会并提高效率”。作为该计划的一部分，思科预计将记录最多10亿美元（当前约71.56亿元人民币）的税前费用，包括遣散费和其他一次性终止福利，以及其他相关成本。</p><p>&nbsp;</p><p></p><h4>TCL中环CEO辞职后：内部开始震荡，人事“换血”，产量下调，员工上12天休息24天</h4><p></p><p>&nbsp;</p><p>近日，TCL中环宣布CEO沈浩平辞职，由TCL老板李东生暂代CEO职责，并将依照相关规定完成新任CEO聘任相关流程。据悉，沈浩平辞去CEO职务至今已接近两周。有消息人士透露，中环内部已有震荡迹象，酝酿中高层人事“换血”，并改变之前开足马力生产的姿态。TCL中环内部人士也表示，开工上确实有些变化，与此前相比开工率降低了5%-10%，但属于正常排产调整下的操作。</p><p>&nbsp;</p><p>有TCL中环内蒙工厂的员工称，自己已经被安排调休，上12天休息24天。接近TCL中环的人士表示，公司已要求降低硅片开工率到75%，以尽快降低库存，或意味着其经营策略调整。此外，据接近TCL中环的人士透露，近期已有来自TCL的人员进入中环，并和一些人进行了面谈，有人被暗示可以主动辞职。</p><p>&nbsp;</p><p>另有光伏行业人士认为，沈浩平的离职可能是因为今年1-2季度市场判断错误，前期安排中环激进满负荷生产，造成了数十亿片硅片库存，同时硅片市场的大幅度价格波动也造成了巨额的跌价损失。</p><p>&nbsp;</p><p></p><h4>微软又崩了！GitHub全球宕机、Copilot也瘫痪</h4><p></p><p>&nbsp;</p><p>8月15日消息，全球最大代码托管平台GitHub发生了全球性宕机事件，Copilot也一并瘫痪。据最新报道，此次宕机影响了GitHub网站及其多项服务，包括pull requests、GitHub Pages和GitHub API等。不过根据GitHub在美东时间8月13日晚上发布的状态消息显示，公司已经回滚了导致服务中断的数据库基础设施变更，并宣布服务已全面恢复运行。</p><p>&nbsp;</p><p>在宕机期间，访问GitHub主网站会显示错误消息，提示没有服务器可用于响应请求。Downdetector的数据显示，超过1万名用户报告受到了影响，互联网监控服务BetBlocks也发布消息，确认GitHub经历了跨国服务中断。有用户调侃称，应用开发者可以借此机会“光明正大摸鱼了”。</p><p>&nbsp;</p><p>GitHub自2018年被微软以75亿美元收购后，用户数已从不到4000万增长到7300多万，但一些用户反映，被收购后的GitHub在服务稳定性方面似乎有所下滑。</p><p>&nbsp;</p><p></p><h4>公开抱怨谷歌员工“不够拼命”才落后AI竞赛，元老CEO火速道歉</h4><p></p><p>&nbsp;</p><p>8月14日消息，斯坦福大学在流媒体平台上传了一个包含施密特参与的课堂讨论活动。期间，这位曾与两位创始人组成谷歌“三驾马车”的前CEO在讨论谷歌与OpenAI竞争时，开始抨击谷歌的员工不够拼命。意识到自己的“大嘴”闯祸后，前谷歌CEO兼执行董事长埃里克·施密特迅速对自己的前东家公开致歉。</p><p>&nbsp;</p><p></p><h4>谷歌重磅发布AI加持系列手机， 5717元起步</h4><p></p><p>&nbsp;</p><p>8月14日消息，谷歌在景山城总部召开新品发布会，除了正式介绍Pixel 9系列手机外，也着力于解答一个更重要的问题——AI还能为使用者做些什么？谷歌Pixel 9系列一共有3款全面屏手机——Pixel 9、Pixel 9 Pro和Pixel 9 Pro XL，以及一款折叠屏手机Pixel 9 Pro Fold。</p><p>&nbsp;</p><p>首先，谷歌宣布购买Pro系列手机的用户，都能获取一年的Gemini Advanced订阅，这也是使用Gemini Live功能（类似于ChatGPT的新语音模式）的前置条件。另一项重要更新，则是谷歌新推出的Pixel Screenshots应用，调用设备端AI模型Gemini Nano分析和整理手机截图里的内容。</p><p>&nbsp;</p><p>谷歌还发布了一款文生图软件Pixel Studio，基于设备端模型和云Imagen 3文本到图像模型，作为新手机的预装软件。还有一个非常有趣的功能更新——AI合影功能。通过这个叫做“加上我”（Add Me）的新功能，一同出游的伙伴可以分开拍照，然后让AI集成到一张照片里，从而无需自带三脚架或向陌生人寻求帮助。</p><p>&nbsp;</p><p>作为现在AI手机的标配，谷歌新手机也有AI通话记录（Call Notes）功能，在完成通话后，用户可以收到软件发来的通话内容摘要，和完整的语音转写文档。为了保护隐私，这款应用完全使用端载算力运行。与苹果类似，一旦用户激活该功能，所有参与通话的人都会收到通知。</p><p>&nbsp;</p><p>售价方面，Pixel 9价格为799美元（约合人民币5717元）起步，而Pixel 9 Pro和Pixel 9 Pro XL的起售价分别为999美元和1099美元。</p><p></p><p><img src="https://static001.geekbang.org/infoq/91/9132dbf5dd1e82a93bda0787fbde0283.png" /></p><p></p><p></p><h4>&nbsp;</h4><p></p><p></p><h4>半年不到新车变“旧车”，极氪新车发布会直播全员禁言</h4><p></p><p>&nbsp;</p><p>8月13日晚，极氪举行新车发布会，正式发布2025款极氪001、极氪007，新车在外观、内饰、电池、智能座舱、智驾系统全面升级。而2024款极氪001今年2月底才发布，距今也不到半年，不少网友在直播评论区表达不满。无奈极氪官方只能关闭了直播评论，但也有人通过改昵称送礼物的方式继续刷屏。据悉，除了直播评论区，极氪官方微博也设置了评论精选。</p><p>&nbsp;</p><p></p><h4>曾经的“自动驾驶第一股”图森未来宣布进入生成式AI应用领域，与三体公司达成合作</h4><p></p><p>&nbsp;</p><p>8月15日，曾经的“自动驾驶第一股”图森未来宣布，与上海三体动漫有限公司达成合作，共同开发基于刘慈欣创作的国际知名科幻小说《三体》系列的动画长篇电影和视频游戏。图森表示，这一项目标志着公司“生成式AI”新业务部门正式成立。该业务已获得集团董事会的一致批准。</p><p>这也是图森自今年1月份宣布退市以来，针对业务的首次发声。随着此次转型，图森也将自身定位切换为全球人工智能科技公司。另外，对于公司整体业务发展，图森未来总裁兼CEO吕程表示，公司并无计划退出交通运输行业，将通过技术合作和授权，继续推动自动驾驶技术实现商业化。</p><p>&nbsp;</p><p>2021年4月，图森未来正式登陆美国纳斯达克挂牌上市，成为全球自动驾驶第一股。不过，上市之后几年时间，图森频频遭受监管机构审查、管理层频繁变动以及多次裁员重组等事件。今年1月17日，图森未来宣布从纳斯达克退市，并终止在美国证券交易委员会的注册。</p><p>&nbsp;</p><p></p><h4>马斯克裁员邮件被判违法：前推特员工获赔60万美元</h4><p></p><p>&nbsp;</p><p>8月14日消息，爱尔兰劳工监管机构周一裁定，埃隆・马斯克在2022年收购推特后，给员工发送的要求在24小时内“点击同意”保留工作否则视作自愿离职的邮件是违法的。该机构认为，该邮件不仅没有给员工足够的时间考虑，而且员工不点击“同意”也不能构成法律上的辞职行为。相反，法院认为该邮件旨在迫使员工要么不看条款就同意新的雇佣条件，要么在推特大规模裁员期间自愿离职。</p><p>&nbsp;</p><p>爱尔兰劳资关系委员会（WRC）裁定，推特（现改名为X）必须向鲁尼支付超过60万美元（当前约429.3万元人民币），而不是最初拟定的不到2.5万美元的遣散费。据多家媒体报道，这是WRC的最高赔偿纪录，其中包括约22万美元的“未来预期收入损失”。</p><p>&nbsp;</p><p>此次事件并非孤例。马斯克收购推特后，大量被裁员工提起诉讼。而鲁尼的胜诉可能引发更多类似诉讼。推特可以对爱尔兰劳工委员会的裁决提出上诉。</p><p>&nbsp;</p><p></p><h4>斥资6.65亿美元，全现金支付，AMD完成收购欧洲最大私人AI实验室Silo AI</h4><p></p><p>&nbsp;</p><p>8月13日，AMD正式宣布，已完成对欧洲最大私人AI实验室Silo AI的收购，交易金额约为6.65亿美元（当前约47.73亿元人民币），采用全现金支付。至此，Silo AI的科学家和工程师正式加入AMD大家庭。</p><p>&nbsp;</p><p>AMD加速计算事业部（AIG）高级副总裁Vamsi Boppana表示：“AI是我们的首要战略目标。我们将持续加大对人才和软件能力的投入，以支持不断增长的客户部署和路线图。”他还强调，Silo AI团队拥有丰富的AI模型开发和集成经验，尤其在大型语言模型方面表现出色，这些能力将显著提升客户在AMD平台上构建高性能AI解决方案的体验。</p><p>&nbsp;</p><p>Silo AI的客户包括安联保险、飞利浦、劳斯莱斯和联合利华等行业巨头。此次收购不仅确保Silo AI继续使用AMD芯片和技术，还将助力AMD推进开源生成式AI训练和应用软件的开发。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>“全世界的丈夫都在颤抖”，扎克伯格为妻子立像引热议，网友锐评：像阿凡达</h4><p></p><p>&nbsp;</p><p>据报道，“脸书”创始人扎克伯格14日在Instagram上发布了一条帖文，在社交媒体上引发热议。帖文中展示了一座约2米高的雕像，该雕像是为他的妻子普莉希拉·陈量身定制的。扎克伯格称对此举非常满意，表示自己是为了“复兴罗马人为妻子制作雕像的传统”。这座雕像是由美国艺术家阿沙姆创作，以银色和蓝绿色为主色调，展现了陈穿着银色斗篷享受微风的姿态，斗篷的后半部分被制作成了翅膀的形状。</p><p></p><p><img src="https://static001.geekbang.org/infoq/17/17db27053e01fdc9d097b42c801f5366.jpeg" /></p><p></p><p>一些网民称赞扎克伯格的举动很是浪漫，“全世界的丈夫都在颤抖”，还有人评论“这下我得重新考虑送给我妻子的礼物了”。然而，也有一些人对此不太认同，一位网民评论道：“这是最典型的亿万富翁行为。”还有网民锐评称像阿凡达，另有网民直呼“尴尬”，称“雕像是用来纪念死去亲人的”。</p><p>&nbsp;</p><p></p><h2>大模型一周大事</h2><p></p><p>&nbsp;</p><p></p><h3>大模型发布</h3><p></p><p></p><p></p><h4>可生成政治人物图像，马斯克旗下xAI发布Grok-2“手撕”OpenAI</h4><p></p><p>&nbsp;</p><p>当地时间8月14日，埃隆·马斯克预告已久的新一代AI大模型Grok-2终于面世。马斯克对该模型寄予厚望，并且未对其生成内容范围过多限制，希望能借此进一步追赶OpenAI的领先地位。</p><p>马斯克旗下人工智能初创公司xAI本次共推出两款型号的产品，包括Grok-2早期预览版及Grok-2 mini。据xAI介绍，Grok-2相较于上一代大模型Grok-1.5取得了“重大进步”，在推理检索到的内容和工具使用能力方面表现突出。</p><p>&nbsp;</p><p>在官方博客文章中，xAI公布了Grok-2的各项评测结果。大语言模型评测平台LMSYS将Grok-2的早期版本列为全球排名前五的聊天机器人模型，位列OpenAI的ChatGPT-4o、谷歌的Gemini 1.5 Pro之后。此外，Grok-2在多个推理、阅读理解等评测集上的表现都能比肩其他前沿模型，但在代码生成、数学等方面仍略微落后于GPT-4o。</p><p>&nbsp;</p><p>xAI还与一家8月1日刚刚成立的AI图像和视频创企Black Forest Labs达成合作，在Grok-2中引入其FLUX.1模型，为用户提供图像生成服务。其他竞争对手也在类似的AI聊天助手中推出了生图功能，ChatGPT基于OpenAI文生图模型DALL·E 3，谷歌Gemini曾支持调用Imagen 2模型创建图像，但上线不久就因生成错误的历史人物图像，涉嫌种族歧视而被迫撤下，至今仍未重新推出。</p><p>&nbsp;</p><p>不同于现有聊天机器人，Grok-2对生成图像的限制似乎并没有那么严格，特别是在政治人物或真实公众人物方面。不少用户在社交媒体上晒出了使用Grok-2生成的图像，包括美国前总统、共和党总统候选人特朗普举着两把手枪发射，或是特朗普坐着SpaceX火箭飞向天空等图片。</p><p>&nbsp;</p><p>Grok-2和Grok-2 mini已率先在社交平台X上开启测试，向付费用户开放使用。xAI表示，本月晚些时候将推出这两个型号的企业API。</p><p></p><h4>&nbsp;</h4><p></p><p></p><h4>首个全自动科学发现AI系统，Transformer作者创业公司Sakana AI推出AI Scientist</h4><p></p><p>&nbsp;</p><p>一年前，谷歌最后一位 Transformer 论文作者 Llion Jones 离职创业，与前谷歌研究人员 David Ha共同创立人工智能公司 Sakana AI。</p><p>&nbsp;</p><p>8月13日消息，Sakana AI 宣布推出 AI Scientist，这是世界上第一个用于自动化科学研究和开放式发现的 AI 系统。从构思、编写代码、运行实验和总结结果，到撰写整篇论文和进行同行评审，AI Scientist 开启了 AI 驱动的科学研究和加速发现的新时代。</p><p>&nbsp;</p><p>原则上，它可以不断重复科学研究过程，以开放式的方式迭代开发想法，就像人类科学家一样。</p><p>研究人员通过将其应用于机器学习的三个不同子领域来展示它的多功能性：扩散建模、基于 Transformer 的语言建模和学习动力学。每个想法都会被实施并发展成一篇完整的论文，每篇论文的成本不到 15 美元。为了评估生成的论文，研究人员设计并验证了一个自动审阅器，它在评估论文分数方面的表现接近人类。据悉，AI Scientist 已经可以撰写出超过顶级机器学习会议接受门槛的论文。</p><p>&nbsp;</p><p></p><h4>昆仑万维：发布全球首个AI流媒体音乐平台Melodio</h4><p></p><p>&nbsp;</p><p>据昆仑万维集团官微，昆仑万维正式发布全球首个AI流媒体音乐平台Melodio，并同步推出AI音乐商用创作平台Mureka。两款产品均搭载昆仑万维新款自研DiT（Diffusion Transformer）架构音乐大模型Skymusic 2.0，这是业内首个能够持续稳定生成特定风格歌曲的AI音乐大模型。</p><p>&nbsp;</p><p></p><h4>XTransfer自研外贸金融大模型TradePilot成功落地</h4><p></p><p>&nbsp;</p><p>据悉，XTransfer自研的外贸金融大模型TradePilot宣布成功落地。据介绍，在风险识别和管理方面，TradePilot通过其上下文推理和自然语言处理能力，能准确预测并防范潜在的交易风险，极大地提升了中小微外贸企业的市场竞争力。</p><p>&nbsp;</p><p></p><h4>硅基智能推出“AI情绪放大器”硅秀emoji，海外版DUIX.Snap全球上线首月用户破10万</h4><p></p><p>&nbsp;</p><p>硅基智能全新推出的 AI 视频神器硅秀emoji现已正式上线。据了解，这款创新产品凭借一张照片就能瞬间生成高能情绪视频，其海外版本DUIX.Snap，上线首月就吸引了超过10万的全球用户体验，火爆TikTok等平台。DUIX.Snap被称作“AI情绪放大器”，不仅能赋予照片灵魂，更可以DIY出独一无二的爆笑视频。</p><p>&nbsp;</p><p>“DUIX.Snap”的核心技术基于硅基智能的 EMOTE-X 深度学习模型，通过对数千万张人类面部表情图像的深度学习，EMOTE-X 能够精准模拟和复刻人类的情绪与动作。据悉，该模型在情感模拟的准确率上超过 95%，无论是开心、搞笑、还是紧张难过，每一个细微的表情都能被完美捕捉并呈现在视频中。</p><p>&nbsp;</p><p></p><h3>企业应用</h3><p></p><p>&nbsp;</p><p>8月15日，火山引擎“AI创新巡展”第二站在厦门举办。活动中，火山引擎首次发布了大模型文旅解决方案，以字节豆包大模型和火山引擎AI全栈云基础设施为底座，结合抖音内容生态，助力以厦门为代表的旅游城市重塑文旅形态，打造更加新奇智能的旅行和消费体验。8月14日，Anthropic公司宣布为其Claude系列大型语言模型推出一项名为"提示缓存"的新功能，该功能允许用户存储并重复使用特定的上下文信息，包括复杂指令和数据，而无需额外成本或增加延迟。8月14日，百度文库全新产品“橙篇”正式上线App端。作为行业首个查阅创编一站式AI自由创作平台，除了超长图文创作等行业首创功能，橙篇APP独家上线多图成片、今日热点等AI功能，全面覆盖学习办公、日常娱乐、内容检索等多元场景。8月14日，蚂蚁集团在京正式宣布成立新公司“数字蚂力”，发力AI to B市场，将以人工智能技术服务企业经营。据悉，“数字蚂力”主要提供三类企业服务：一是智能客服与营销服务，通过客服领域大模型为客户提供“AI云客服”及智能营销、智能培训质检等各类服务，帮助企业降低经营成本提升经营效率；二是智能运营服务；三是智能技术服务。8月12日，1688面向产业带的源头厂商推出“提效增收”计划，并发布免费的“AI 经营助理”。1688承诺，保障新商家获得确定性订单量、客户数和合理利润，同时面向商家的AI产品全部免费。8月12日，科大讯飞宣布旗下智能文档产品——讯飞智文2.0全新版本正式上线。新版基于讯飞星火V4.0 大模型底座，引入全新的PPT文本生成大模型、AI PPT编排创作引擎和PPT在线编辑模组、实时联网搜索和长文本解析、AI Word和AI读写。</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/YYs93V0SitjSDZp1f6MA</id>
            <title>港科大联手思谋新作：Defect Spectrum 数据集重新定义AI工业质检</title>
            <link>https://www.infoq.cn/article/YYs93V0SitjSDZp1f6MA</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/YYs93V0SitjSDZp1f6MA</guid>
            <pubDate></pubDate>
            <updated>Fri, 16 Aug 2024 10:49:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在“生产制造 - 缺陷检测 - 工艺优化 - 生产制造”的智能制造闭环链条中，基于 AI 的智能缺陷检测扮演着“把关者”的角色。但这个“把关者”长期以来却缺少样本量大、精度高、语义丰富的缺陷数据集。</p><p></p><p>近日，港科广和专注于智能制造领域的人工智能独角兽思谋科技联合发布了一篇论文，该论文提出了 Defect Spectrum 缺陷数据集及 DefectGen 缺陷生成模型，主攻工业智能检测，可解决模型无法识别的缺陷类别和位置问题，有效提升 10.74% 召回率，降低 33.1% 过杀率。</p><p></p><p>据悉在去年，该合作团队提出的《Ref-NeuS: Ambiguity-Reduced Neural Implicit Surface Learning for Multi-View Reconstruction with Reflection》被选为 ICCV 最佳论文候选。</p><p></p><p>Project Page: <a href="https://envision-research.github.io/Defect_Spectrum/">https://envision-research.github.io/Defect_Spectrum/</a>"</p><p></p><p>Arxiv Page: <a href="https://arxiv.org/abs/2310.17316">https://arxiv.org/abs/2310.17316</a>"</p><p></p><p>Github Repo: <a href="https://github.com/EnVision-Research/Defect_Spectrum">https://github.com/EnVision-Research/Defect_Spectrum</a>"</p><p></p><p>Dataset Repo: <a href="https://huggingface.co/datasets/DefectSpectrum/Defect_Spectrum">https://huggingface.co/datasets/DefectSpectrum/Defect_Spectrum</a>"</p><p></p><p>突破传统限制，</p><p></p><p>更贴近落地生产</p><p></p><p>高质量的数据集对 CV 技术和人工智能的发展起着至关重要的作用。如 ImageNet 不仅推动了算法的创新，还促进产业发展和进步。</p><p></p><p>在工业界，MVTec、VISION VI、DAGM2007 等数据集帮助视觉学习算法更接近工业生产实际场景，但由于样本量、精度、语义描述的不足，始终限制着 AI 工业检测的发展。</p><p></p><p>Defect Spectrum 数据集带着突破传统缺陷检测界限的任务而来，为工业缺陷提供了详尽、语义丰富的大规模标注，首次实现了超高精度及丰富语义的工业缺陷检测。</p><p></p><p>相比其他工业数据集，“Defect Spectrum”数据集提供了 5438 张缺陷样本、125 种缺陷类别，并提供了像素级的细致标签，为每一个缺陷样本提供了精细的语言描述，实现了前所未有的性能突破。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/9b/9b2a5629712f3f1ab3b5f4df71c8d058.jpeg" /></p><p></p><p>相比其他工业数据集，Defect Spectrum 精准度更高、标注更丰富</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c8/c8c7d71bbac515b3d6b8c957a5f46271.png" /></p><p></p><p>Defect Spectrum 与其他数据集的数量、性质对比</p><p></p><p>从实际的工业生产来看，工厂对缺陷检测的要求细致，需要在控制缺陷件的同时保证收益率。然而，现有缺陷检测数据集常常缺乏应用所需的精确度和语义丰富性，无法良好支持实际生产。</p><p></p><p>例如，一件衣服的拉链齿出现了错位，虽然缺陷尺寸不大但却影响衣物功能，导致拉链无法正常使用，消费者不得不将其退回工厂进行修复。然而，如果缺陷发生在衣物的面料上，比如轻微的钩丝或颜色略有差异，这时就需要仔细权衡其尺寸和影响。小规模的面料缺陷可被归类在可接受的范围内，允许这些产品通过不同的分销策略销售，比如以打折价格进行销售，在不影响整体质量的同时保有收益。</p><p></p><p>传统数据集如 MVTEC 和 AeBAD 尽管提供了像素级的标注，但常常局限于 binary mask，无法细致区分缺陷类型和位置。Defect Spectrum 数据集通过与工业界四大基准的合作，重新评估并精细化已有的缺陷标注，对细微的划痕和凹坑进行了更精确的轮廓绘制，且通过专家辅助填补了遗漏的缺陷，确保了标注的全面性和精确性。</p><p></p><p>通过 Defect Spectrum 数据集这个强大的工具，工厂缺陷检测系统能够迅速识别、立即标记，并采取相关修复策略。</p><p></p><p></p><h4>革命性生成模型，专攻缺陷样本不足</h4><p></p><p></p><p>港科大和思谋科技研究团队还提出了缺陷生成模型 Defect-Gen，一个两阶段的基于扩散的生成器。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/63/63c41531339762ab0de60ddf847afdb1.png" /></p><p></p><p>Defect-Gen 两阶段生成流程示意图</p><p></p><p>Defect-Gen 专门解决当前数据集中缺陷样本不足的问题，通过利用极少量的工业缺陷数据生成图像与像素级缺陷标签，即使在有限的数据集上也能工作，为 AI 在复杂工业环境中的应用开辟了新的可能。</p><p></p><p>Defect-Gen 具体通过两个关键方法提高图像的多样性和质量：一是使用 Patch 级建模，二是限制感受野。</p><p></p><p>为弥补 Patch 级建模在表达整个图像结构上的不足，研究团队首先在早期步骤中使用大感受野模型捕捉几何结构，然后在后续步骤中切换到小感受野模型生成局部 Patch，可在保持图像质量的同时，显著提升了生成的多样性。通过调整两个模型的接入点和感受野，模型在保真度和多样性之间实现了良好的平衡。而生成数据同样可以作为数据飞轮的一部分，并加速其运转。</p><p></p><p>目前，Defect Spectrum 数据集的 5438 张缺陷样本中，有 1920 张由 Defect-Gen 生成。研究团队对应用 Defect-Gen 生成模型的 Defect Spectrum 数据集进行了全面的评估，验证了 Defect Spectrum 在各种工业缺陷检测挑战中的适用性和优越性。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/73/73ec567221a5284c01a56b8cf95389c1.png" /></p><p></p><p>部分缺陷检测网络在 Defect Spectrum 数据集上的测评结果</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/de/dee38e2bdc882d6e63ff6f82064b4570.png" /></p><p></p><p>Defect Spectrum 数据集上的实际评估标准</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/3e/3e7326bf37269dbcec3d7dc4b26c9e35.png" /></p><p></p><p>Defect Spectrum 在实际评估中的优异表现</p><p></p><p>比起原有的数据集，在 Defect Spectrum 数据集上训练的模型召回率 (recall) 提升 10.74%，过杀率 (False Positive Rate) 降低了 33.1%。</p><p></p><p>据介绍，Defect Spectrum 数据集的引入可以让缺陷检测系统更加贴近实际生产需求，实现高效、精准的缺陷管理，同时为未来的预测性维护提供了宝贵的数据支持，通过记录每个缺陷的类别和位置，工厂可以不断优化生产流程，改进产品修复方法，最终实现更高的生产效益和产品质量。</p><p></p><p>目前 Defect Spectrum 数据集已应用于思谋科技缺陷检测视觉模型的预训练中，未来将与 IndustryGPT 等工业大模型融合，深度落地并服务于工业质检业务。</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ez1o7ukGmyfWeN613OTL</id>
            <title>成本直降90%、延迟缩短80%！Anthropic将API玩出了新花样，网友：应该成为行业标配</title>
            <link>https://www.infoq.cn/article/ez1o7ukGmyfWeN613OTL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ez1o7ukGmyfWeN613OTL</guid>
            <pubDate></pubDate>
            <updated>Fri, 16 Aug 2024 10:38:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Anthropic在其API上引入了新的提示词缓存机制，可将长提示的成本降低多达90%，并将延迟降低80%。</p><p>&nbsp;</p><p>提示词缓存功能能够记住API调用之间的上下文，并帮助开发人员避免输入重复提示内容。目前该功能已经在Claude 3.5 Sonnet以及Claude 3 Haiku当中以beta测试版的形式开放，但对Claude旗下最大模型Opus的支持仍未交付。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/e7/e7423d45cef2b66e0161a0f2182f36f5.jpeg" /></p><p></p><p>&nbsp;</p><p>提示词缓存的概念源自2023年的研究论文，其允许用户在会话中保留常用的上下文。由于模型能够记住这些提示词，因此用户可以添加额外的背景信息而不必重复承担成本。这一点对于需要在提示词中发送大量上下文，并在与模型的不同对话中多次引用的使用场景非常重要。它还允许开发人员及其他用户更好地对模型响应作出微调。</p><p>&nbsp;</p><p>Anthropic表示，早期用户“已经在多种用例中观察到，使用提示词缓存后速度及成本都出现了显著改善——测试范围从完整知识库到100个样本示例，再到在提示词中包含对话的每个轮次。”</p><p>&nbsp;</p><p>该公司表示，提示词缓存的潜在效果包括降低对话智能体在处理长指令及上传文档时的成本和延迟、加快代码的自动补全速度、向智能体搜索工具提交多条指令，以及在提示词中嵌入完整文档等等。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/94/94f0dc8040078924e9781c57db7505a8.jpeg" /></p><p></p><p></p><blockquote>Anthropic刚刚公布了一项改变其API游戏规则的功能：提示词缓存。大家可以这样理解提示词缓存的概念：你选中了一家咖啡厅。第一次光顾时，我们需要逐个挑选出自己喜欢的品类。而下次到店时，直接说“老样子”就好。这就是提示词缓存......&nbsp;</blockquote><p></p><p></p><h2>提示词缓存价格</h2><p></p><p>提示词缓存的主要优势在于每token的价格较低，Anthropic表示使用这项功能要比“直接输入token便宜得多”。</p><p>&nbsp;</p><p>以Claude 3.5 Sonnet为例，初次输入提示词时每100万token（MTok）的成本为3.75美元，但随后调用缓存提示词的每百万Token成本仅为0.30美元。Claude 3.5 Sonnet模型的基础提示词输入价格为每百万个3美元，也就是说只要预先多付一点钱，那么在下次使用缓存提示词时就能将成本压低至十分之一。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/e6/e6ee7e36a5d65c5f22a64226f5ec1b12.jpeg" /></p><p></p><p>&nbsp;</p><p></p><blockquote>我们刚刚在Anthropic API中推出了提示词缓存功能。它能够将API的输入成本降低90%，并将延迟降低80%。</blockquote><p></p><p>&nbsp;</p><p>说到成本，尽管初始API调用会稍贵一些（毕竟需要将提示词存储在缓存当中），但一切后续调用都只是正常输入价格的十分之一。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/bb/bb41226476ad0bbb3a77347b7087d052.png" /></p><p></p><p>&nbsp;</p><p>Claude 3 Haiku用户使用提示词缓存时每百万token时需要额外支付0.30美元，而在调用已缓存提示词时每百万token价格仅为0.03美元。</p><p>&nbsp;</p><p>虽然Claude 3 Opus尚未提供提示词缓存，但Anthropic已经提前公布了具体价格。写入缓存的价格是每百万token 18.75美元，而访问已缓存提示词的每百万token价格为1.50美元。</p><p>&nbsp;</p><p>然而，正如AI意见领袖Simon Willison在X上发帖所言，Anthropic的缓存只有5分钟的生命周期，而且每次使用时都会刷新。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/35/354fc5f6bc896ef1dab17f43c513bf47.jpeg" /></p><p></p><p>&nbsp;</p><p></p><blockquote>这看起来跟Gemini的上下文缓存功能类似，只是Anthropic提出了独立的定价模式。Gemini为百万个token每小时收取4.50美元的费用，即可保持上下文缓存。Anthropic直接对缓存输入量收费，而且“缓存的生命周期只有5分钟，且每次使用缓存内容时都会刷新”。</blockquote><p></p><p>&nbsp;</p><p>当然，这也绝不是Anthropic第一次尝试通过定价手段跟其他AI平台竞争了。在发布Claude 3系列模型之前，Anthropic就曾大幅下调过其token的计费标准。</p><p>&nbsp;</p><p>在当初为自家平台上的第三方开发商提供低价选项之后，现如今他们再次针对谷歌和OpenAI等竞争对手展开一场“比比谁价低”的烈性对抗。</p><p>&nbsp;</p><p></p><h2>功能本身确实备受期待</h2><p></p><p>&nbsp;</p><p>为Claude模型引入提示缓存代表了AI交互效率的重大飞跃。尤其是在考虑诸如检索增强生成（RAG）或其他长上下文模型等替代方案时，其重要性不容忽视。</p><p>&nbsp;</p><p>虽然RAG一直是通过外部知识增强AI模型的一种流行方法，但Claude的提示缓存提供了几个优势：</p><p>简单性：不需要复杂的向量数据库或检索机制一致性：缓存的信息始终可用，确保一致的响应速度：所有信息都可以立即访问，响应速度更快</p><p>&nbsp;</p><p>与具有扩展上下文窗口的模型（如谷歌的Gemini Pro）相比，Claude的提示缓存提供了以下优势：</p><p>成本效益：只需为使用的部分付费，而不是为整个上下文窗口付费灵活性：可以轻松更新或修改缓存信息，而无需重新训练可扩展性：潜在的无限上下文大小，不受模型架构的限制</p><p>&nbsp;</p><p>其他平台也开始提供类似的提示词缓存版本。Lamina是一套大语言模型推理系统，尝试利用KV缓存来降低GPU使用成本。而随意浏览一下OpenAI的开发者论坛或者GitHub，就会发现大量跟提示词缓存相关的话题。</p><p>&nbsp;</p><p>提示词缓存跟大语言模型自己的提示词记忆并不是一回事。例如，OpenAI的GPT-4o就提供记忆机制，模型可以借此记住用户的某些偏好或详细信息。但其无法像提示词缓存那样存储具体提示词及响应结果。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/2c/2c92bdb13ace8ad4689e37158fd15f77.jpeg" /></p><p></p><p>&nbsp;</p><p>X平台上对此的讨论也很多，有网友评价“提示词缓存”有100%的颠覆性，应该作为标准被每家大模型厂商采用。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/9a/9a87f322a299c968661a4299b6be1a6a.jpeg" /></p><p></p><p>&nbsp;</p><p>还有网友对AnthropicAI 提示缓存进行了独立评估——结果简直令人震惊，Claude 3.5 Sonnet能做到90%的成本节省，而在Claude 3 Haiku上甚至能做到97%的成本节省。</p><p>&nbsp;</p><p>展望未来，Claude的提示缓存在推动更高效、更具成本效益的AI交互方面迈出了重要的一步。通过减少延迟、降低成本，并简化复杂知识的整合，这一功能为各行业的AI应用开辟了新的可能性。</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href="https://venturebeat.com/ai/anthropics-new-claude-prompt-caching-will-save-developers-a-fortune/">https://venturebeat.com/ai/anthropics-new-claude-prompt-caching-will-save-developers-a-fortune/</a>"</p><p><a href="https://towards-agi.medium.com/how-to-use-claude-prompt-caching-and-ditch-rag-1837add5a733">https://towards-agi.medium.com/how-to-use-claude-prompt-caching-and-ditch-rag-1837add5a733</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/B6KJhHug3NG3PmQlipwT</id>
            <title>用友亮剑AI，新技术、新能力Buff</title>
            <link>https://www.infoq.cn/article/B6KJhHug3NG3PmQlipwT</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/B6KJhHug3NG3PmQlipwT</guid>
            <pubDate></pubDate>
            <updated>Fri, 16 Aug 2024 08:27:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>人工智能(AI)技术的迅猛发展已驱动AI在企业的应用进入普及化阶段，大大加速了数智化的进程，企业数智化由此前侧重数字化，进入到数字化和智能化并举的新阶段。</p><p></p><p>8月9-10日，由用友主办，以“AI+成就数智企业”为主题的“2024全球商业创新大会”在北京召开。会上，用友宣布：用友BIP的智能和数据服务能力再升级，发布用友BIP3 R6，实现6大领先技术突破、6大应用架构和服务创新，具备更强数智能力、更高运行性能、更低资源消耗以及更加安全可靠的特性。同时，作为用友BIP赋能企业AI应用的新引擎，用友企业服务大模型YonGPT重磅升级，发布YonGPT2.0及100多项智能应用，引领企业AI应用创新发展。</p><p></p><p>承载了用友BIP底层平台与技术能力的用友iuap实现领先技术突破帮助企业构建和运行强大、统一的数智化底座的能力。用友iuap平台以AI为核心引擎，持续进化，全面升级，赋能集开发、数据、集成、架构等革新，推出懂业务（应用）+有工具（paas）+有方法（工程化体系）的数智体系，为大型企业升级数智底座提供集智能运营、数据驱动、敏捷创新、开放互联、全球化支撑、工程化六大能力。</p><p></p><p>以下将通过一张图，全面展示用友BIP3 R6以AI为引擎的新技术新能力！</p><p></p><p><img src="https://static001.infoq.cn/resource/image/6a/e7/6a2f8ff9e0e4c4db2b4cdc093b7180e7.jpg" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/6kFzyZPhYf7H9k3Qyikq</id>
            <title>研发近3年，Linux发行版开源操作系统deepin V23 终于发布！</title>
            <link>https://www.infoq.cn/article/6kFzyZPhYf7H9k3Qyikq</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/6kFzyZPhYf7H9k3Qyikq</guid>
            <pubDate></pubDate>
            <updated>Fri, 16 Aug 2024 04:10:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>在历经近3年的努力、迭代了9个版本、经历了51次内测后，8月15日，知名开源社区deepin（深度）正式发布开源操作系统deepin V23，该版本带来了全新DDE视界、 AI For OS、“如意玲珑”应用生态、“deepin IDE”集成开发环境等诸多重磅更新。</p><p>&nbsp;</p><p>“我们不认为增删几个上游的应用软件，修改一下语言、壁纸，或者调整下应用布局等等，就是一个操作系统的版本更新。我们希望每一次的大版本更新，都有大量真正用户需要和创新性的内容，去突破Linux桌面发行版的能力边界，能让Linux桌面与Windows、MacOS 这两个商业操作系统一样强大。”deepin（深度）社区创始人刘闻欢说道。</p><p>&nbsp;</p><p></p><h3>操作系统全栈自研矩阵，适配多款国产芯片</h3><p></p><p>为了真正掌握操作系统发展权、上游社区主导权、供应链安全主动权，2022年，由开放原子开源基金会旗下的欧拉社区所代表的中国服务器操作系统根社区，以及由统信软件主导运营的deepin深度社区所代表的中国桌面操作系统根社区先后投入建设。</p><p>&nbsp;</p><p>注：Linux操作系统根社区是指从Linux kernel和其他开源组件构建，不依赖上游发行版，有大量的外部个人贡献者与企业参与共建的开源社区。deepin 社区用户超540万，其中近300万为海外用户，其在中国知名下游商业发行版统信UOS目前国产装机量已超600万台。</p><p>&nbsp;</p><p>deepin 社区的第一步是独立构建全新的仓库、自主研发基于deepin根社区的开发工具，以便开发者可以更便捷、更有效地参与贡献。</p><p>&nbsp;</p><p>作为首个基于根社区推出的发行版本，deepin V23实现了操作系统的每个层级均有自研模块，为全球开源操作系统爱好者提供了源自中国的开发工具。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/2e/2eb9ab831350aa0c68e162a57f7d7248.png" /></p><p></p><p>&nbsp;</p><p>deepin V23 搭载Linux 6.6 LTS内核，从仓库到应用层，针对操作系统核心组件，采用了大量自研方案：</p><p>&nbsp;</p><p>独立构建的仓库beige-V23采取独立选型、独立更新策略和精细化的仓库维护模式，对8000+核心包进行升级，有效提升了系统的稳定性和安全性，并且能够更好地支持ARM64、RISC-V、LoongArch64等新硬件和新架构。</p><p>&nbsp;</p><p>服务层，deepin研发了AM应用程序统一管理框架，不仅极大地便利了从应用层对相关进程进行更为细致的资源与权限管控，还实现了统一的调度策略,解决了以往资源管控纷乱无章、后台进程杂乱无序的难题，更为未来的发展预留了扩展空间。</p><p>&nbsp;</p><p>SDK层，基于Qt开发的通用开发框架DTK，可满足研发人员“一次研发，多平台、多架构复用”的需求，提升开发效率。目前已完成6个版本迭代，110+次更新，累计提交代码近20万行，已被迁移至超过10个Linux发行版。在V23中，浏览器、音乐、邮件等40余款原生应用全部使用DTK开发。</p><p>&nbsp;</p><p>桌面环境层，首个由中国社区主导、备受全球Linux爱好者喜爱的DDE迎来全面升级。全新的任务栏、启动器以及更丰富的个性化主题，在保留V20用户习惯的同时，显著提升了系统的管理能力与交互体验。展示形式进行了精心设计，保持统一的风格和节奏，用户得以在进行不同操作之间，视觉始终流畅而连贯。</p><p>&nbsp;</p><p>应用层，deepin为开发者提供了完善的原生应用开发矩阵：</p><p>&nbsp;</p><p>集成开发环境deepin IDE，集成AI能力，支持多种软硬件架构、多种编程语言；具备全量基础功能，可以实现一站式多场景开发，从底层服务到上层开发工具实现垂直安全，真正做到掌握自主发展权；综合型自动化测试框架“YouQu”，由统信软件主导研发，以其简便的环境部署、强大的功能特性脱颖而出，不仅支持UI、WEB、接口及命令行等多种自动化测试场景，还极大地提升了测试效率与质量，为Linux操作系统上的开发测试工作带来了前所未有的便捷与高效；始于2017年、现已捐赠给开放原子开源基金会的新型独立包管理工具“如意玲珑”，凭借对跨发行版的强大支持，可有效解决传统包管理系统强依赖导致的兼容性问题，以及权限松散导致的安全问题。目前，“如意玲珑”千帆竞发，已有400多位开发者贡献了超2000款如意玲珑应用，其中1000余款已上架deepin V23应用商店。</p><p>&nbsp;</p><p>在发布的同时，Intel、龙芯、飞腾、玄铁等CPU厂家日前也纷纷宣布与deepin V23完成适配，这代表着deepin V23成为首个支持X86、ARM64、LoongArch64、RISC-V等全部主流通用计算架构的开源桌面操作系统。此外，社区项目deepin-m1</p><p>（<a href="https://github.com/deepin-community/deepin-m1">https://github.com/deepin-community/deepin-m1</a>"）能够支持苹果M1芯片的设备。这意味着deepin社区全球用户都可以在第一时间体验到deepin V23。</p><p>&nbsp;</p><p>据刘闻欢介绍，deepin V23原计划在2023年发布，延期到现在才最终发布的原因是，因为自2022年启动规划时设立了4个雄心勃勃的目标：</p><p>&nbsp;</p><p>独立仓库：在deepin V23 以后，根据自己的需求、理解和判断，自主选择上游各软件的版本和软件包构建规则，以提高系统的稳定性、安全性和创新特性。行云设计：以现代化设计理念对deepin桌面环境进行新一轮的打磨，打造美观、流畅、细节丰富的视觉界面和方便快捷的交互体验。原子更新：采用ostree技术实现操作系统基础的不可变和系统A/B分区机制，提升操作系统基础的稳定性、安全性和自由回滚机制的特性，为deepin的未来发展打下坚实的基础。非依赖软件包：deepin V23的如意玲珑软件包格式，采用Linux沙箱隔离技术，实现不同于传统deb和rpm的非依赖性包格式，避免应用软件包对操作系统的耦合和侵入，让系统更安全和稳定，同时也能让deepin的应用生态更加方便地在其他Linux发行版上使用。</p><p>&nbsp;</p><p>此外，deepin团队还规划了一系列旨在提升用户体验与兼容性的小目标，如自主研发基于wayland协议的treeland窗口管理器，从而补齐自研桌面环境 DDE 的最后一个版块等。</p><p>&nbsp;</p><p>“实际上每一个小目标背后的工作量一点也不小。”刘闻欢表示，“正因如此，虽然发布了多个中间版本并多次延期，但在最终版本发布的时候，仍然留有不少遗憾。上面的每个目标都有很多的工作，最终没有能100%完成。”</p><p>&nbsp;</p><p>比如，虽然已经经过验证，但由于测试覆盖不够，为了稳妥起见，deepin V23仅实现了基本的原子更新能力，而没有最终发布不可变系统；Treeland窗口管理器虽然已经基本成型，但是没有达到期望的稳定目标，因此deepin V23目前还是使团队自己维护的KWwin分支版本等。</p><p>&nbsp;</p><p></p><h3>将AI 集成到桌面操作系统</h3><p></p><p>统信软件自2023年推出UOS AI以来，就在上游社区版deepin中持续验证和迭代。</p><p>&nbsp;</p><p>“在Linux发行版中首次引入了AI能力，这是我们最初规划中没有包含，但在去年额外增加的目标。deepin自带了UOS AI助手的第一个版本，并且在图像处理、邮件客户端等应用中引入了 AI 能力。除此之外，我们还跟Intel合作，实现了在Intel平台上端侧模型的推理优化。这虽然只是我们探索AI+OS 融合路上的第一步、我们期望的效果还未完全实现，但也已经是Linux世界中少有能够对标Windows AI能力的Linux发行版。”刘闻欢表示。</p><p>&nbsp;</p><p>自UOS AI赋能deepin以来，在应用层，UOS AI已支持自然语言命令调用20余个操作系统设置能力、40余个使用场景，已适配60余款应用；芯片层支持国内主流CPU芯片和英伟达等国内外主流GPU芯片；大模型层开放接口，支持接入所有OpenAI接口格式的大模型，用户可根据自身需求，自行适配专属模型。</p><p>&nbsp;</p><p>对于deepin这一中国首个接入大模型的开源操作系统，海外杂志 Linux Magazine 评价道：“deepin 已经将人工智能集成到桌面操作系统上，开始向微软 Copilot 发起挑战”，并称“这可能只是 deepin V23 融合人工智能的开始”。</p><p>&nbsp;</p><p>Intel 开源技术高级研发经理田俊表示，deepin的Intel SIG小组集中支持了最新的Meteor Lake与deepin的深入适配。作为Intel Ultra平台的重要组成部分，deepin带来了前所未有的性能提升和丰富的功能支持。</p><p>&nbsp;</p><p>“通过CPU、NPU、GPU的协同运算，deepin V23能够胜任各种实用性的AI应用，特别是本地推理能力。GPU的高吞吐和图形处理能力、NPU的低功耗专用AI算法能力以及CPU的低延迟逻辑运算能力，共同构成了deepin V23强大的AI计算能力。”田俊表示。</p><p>&nbsp;</p><p>国民级办公应用WPS日前也公布了双方联合开发AI办公解决方案的进展，基于deepin V23的 WPS Office For Linux 个人版将于8月下旬上线deepin应用商店。用户不仅可在该版本中体验到融入AIGC的三款WPS拳头产品，更能感受到UOS AI与WPS AI在本地个人知识库建设方面的功能联动。</p><p>&nbsp;</p><p>“回溯到现代计算的诞生，我们一直在追求制造出能够理解人类的计算机，而如今我们正在进入一个新时代，就像摩尔定律推动了信息革命一样，深度神经网络的扩展定律也将推动智能革命。”张磊表示，deepin将加速构建AI与操作系统的融合，从AI FOR OS 到 OS FOR AI，引领开源操作系统创新发展。</p><p>&nbsp;</p><p>而在deepin V23发布后，社区正积极准备下一个100%实现V23规划目标的版本，“计划在一年内，弥补未完成的遗憾。”</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/7f1173ac8bf71a5802f91da43</id>
            <title>大模型在资源全生命周期的应用探索</title>
            <link>https://www.infoq.cn/article/7f1173ac8bf71a5802f91da43</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/7f1173ac8bf71a5802f91da43</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Aug 2024 07:19:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><h1>资源全生命周期管理的传统价值</h1><p></p><p></p><p>运营商的网络涉及接入网、数据网、承载网、核心网、传输网、无线网、光缆网、云专网、动力网、业务平台等十数类大专业。网络资源的全生命周期体现在以下六大生产活动环节：网络规划→网络设计→网络工程建设→网络资源的投入使用→网络的运行维护→网络资源的退网。</p><p></p><p><img src="https://static001.geekbang.org/infoq/93/939ee8b1b211da4368388976092b5aa7.webp" /></p><p></p><p>在网络资源从设计到退网的整个生命周期中，资源系统与现实网络的断点无处不在，流程缺失，数据质量不高，系统使用范围狭窄，不能有效提升企业的运营效率，系统建设的投资回报率不高。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e0/e0ba201c284fbb14eaa66a01b6abb187.webp" /></p><p></p><p>建立端到端的全生命周期流程管理系统是一个很好的做法，可以有效解决网络资源管理中存在的断点、流程缺失等问题。通过这样的系统，可以实现以下好处：</p><p>跨部门协作：在线上拉通各部门和专业的工作内容，促进跨部门协作和信息共享，确保各环节之间的衔接和协同。明确业务流程：明确管理的边界和业务流程，避免交叉管理和工作脱节，提高管理资源的衔接性和整体效率。提升工作效率：优化工作流程，减少重复工作和信息传递中的误差，提升工作效率和质量。实时监控和反馈：实现对整个生命周期的实时监控和反馈，及时发现问题并进行调整，提高问题解决的效率和及时性。数据一致性：确保数据在各个环节的一致性和准确性，避免数据质量问题影响决策和运营效率。持续改进：建立持续改进机制，通过系统记录和分析，不断优化流程和提升管理水平。提高管理透明度：使管理过程更加透明，管理者可以清晰了解整个生命周期的进展和问题，有针对性地进行管理和决策。</p><p></p><p>因此，建立全生命周期流程管理系统是推动企业管理现代化和提升运营效率的重要举措，有助于实现网络资源的优化配置和高效利用，提升企业智能化水平。</p><p></p><p>但是，当前存在一些全生命周期管理的业务流程，例如业务使用频率非常高的OBD入网流程，在业务流程发起的操作页面上会有较多的信息填报、复杂的入网配置操作以及相应的资源校验规则约束，完成这些资源录入工作往往需要资源维护人员对资源数据非常熟悉，也需要花费大量填写与资源确认的时间，实际生产过程中也是会经常出现一次入网配置失败，需要多次入网配置的情况，业务发单耗时耗力现象比较普遍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bd41c20d099bf2cdb21ccd9e854b3764.webp" /></p><p></p><p>通过借助资源助手大模型探索与实践，解决资源维护人员在OBD入网这类全生命周期管理业务流程中遇到的棘手问题，达成高效率高质量的资源入网配置，准确快速完成流程发单，从提升系统操作能力上赋能生产，最终提升一线资源维护人员的工作效率和对系统的使用感知。</p><p></p><p></p><h1>AI+资源助手大模型介绍</h1><p></p><p></p><p>资源大模型应用：将资源现有的业务、服务、数据进行组织、加工，转化成大模型知识库，通过大小模型的协同工作，构建功能丰富的资源管理大模型应用，赋能于资源管理的端到端过程和业务全生命周期过程，提高生产作业支撑的效率，实现资源自智等级的不断提升。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6d/6d7ae84752269ed2c4cb0ecc1cc208e7.webp" /></p><p></p><p>数据飞轮，持续进化：通过持续的数据收集、模型训练、应用部署和反馈循环，形成一个自我增强的过程，从而不断提升模型性能和服务质量的机制。通过这个过程，数据飞轮促进了模型自身的持续进化，不断提升投诉处理的判断准确性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/53/535693de0bcc9825d181c9f3b5d55790.webp" /></p><p></p><p></p><p>数据收集：这是整个流程的起点，涉及到从各种来源搜集大量的原始数据。这些来源可以包括网络资源数据、工单处理数据、各类知识文档等。模型训练：使用收集到的数据训练大模型，包括但不限于深度学习模型、语言模型等。大模型在这个阶段学习数据中的模式和规律。应用部署：将训练好的模型持续更新，部署到生产应用场景中。反馈迭代：模型在应用过程中会接收到用户的直接或间接反馈，以及通过模型表现监测得到的数据。这些反馈成为新的数据输入，再次进入飞轮。优化增强：基于反馈数据，对模型进行调整优化，可能涉及微调、参数调整或增加训练数据等。重复循环：优化后的模型重新部署，开始新一轮的数据收集，如此循环往复，形成一个不断加速优化的“飞轮”。</p><p></p><p></p><h1>AI+大模型在资源全生命周期的应用实践</h1><p></p><p></p><p>资源维护人员在资源全生命周期各业务流程的申请发起和派发过程中存在对人员经验要求高，操作费时费力的问题。例如，针对OBD设备的批量入网，需要维护人员一个设备、一个设备的进行录入，同时单设备操作过程做所需要填写的信息也非常多。通过大模型来简化操作，通过对话方式，自动从中分析出OBD入网所需的各类参数，大幅提升一线资源维护人员的工作效率和使用感知。</p><p></p><p><img src="https://static001.geekbang.org/infoq/62/62df4775191a614984c412416d3f2a19.webp" /></p><p></p><p></p><h2>方案举措：</h2><p></p><p></p><p>基于大模型的语言理解和场景识别能力 + DocChain的知识问答体系，提供当前使用量最多的OBD入网等业务场景的智能化发单功能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/40/40650667fcec8de64ad9c89311ac0022.webp" /></p><p></p><p></p><h2>对话模式发单：</h2><p></p><p></p><p>根据用户描述，自智化编排后端原来的多个操作步骤，通过GPT方式一句话完成批量OBD入网流程的发起。</p><p></p><p>AI能力-图片识别：通过图片、电子标签、二维码等方式快速识别出资源设备，自动关联出资源的使用情况、维护状态等。AI能力-文字识别：通过输入关键词（例如“设备入网”、“OBD入网”等）、同义词（例如“GJ”、“光交”、“光交接箱”）检索大模型知识库，提供相应的服务能力。AI能力-语音识别：用于手机APP、AR应用中语音方式的系统操作，解决小屏幕操作不便的问题。自然语言处理NLP：采用NLP自然语言大模型技术对用户输入的参数进行识别，解析所属设备名称或编码，把解析到的参数，如“仁恒江湾城1幢1单元8层”，作为所属名称和所属编码的查询条件，因为不知道是名称还是编码，所以用“or”进行匹配，只要名称或者编码任一个查询到就行，找到这些设备下未发起过入网的OBD。先进行精确查询，如果能够查询到，直接返回结果列表；如果精确查询不到，再进行模糊查询，返回查询结果列表。如果都查询不到，提示无法找到“仁恒江湾城1幢1单元8层”的OBD。</p><p></p><p><img src="https://static001.geekbang.org/infoq/99/99c9367bab2d0dbbf59b8493c518ebd5.webp" /></p><p></p><p></p><h2>建设效果/收益：</h2><p></p><p></p><p>操作提效：智能化地申请单参数初始化，减少人工输入工作量，用户原来需要多步操作完成的工作一句话完成，支持根据业务关联查询进行批量派单，大幅提升派单效率。</p><p></p><p>降低使用门槛：降低人员对资源数据熟悉程度要求，智能化地推荐可接入的上联资源。利用资源助手大模型开发智能辅助工具，帮助资源维护人员快速填报信息、配置资源，减少操作复杂性和错误率。</p><p></p><p>实时反馈和监控：建立实时反馈和监控机制，与资源助手大模型结合，及时发现问题并提供解决方案，减少配置失败和耗时情况。</p><p></p><p>持续学习和优化：资源助手大模型具备持续学习的能力，可以不断优化算法和模型，提高辅助工具的智能化水平，进一步提升操作效率和质量。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>