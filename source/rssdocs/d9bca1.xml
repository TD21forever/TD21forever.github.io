<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/FuZrECsytJmcvGAxU9Cp</id>
            <title>腾讯大模型APP真实测评！七家国产大模型“battle”，元宝顶得住吗？</title>
            <link>https://www.infoq.cn/article/FuZrECsytJmcvGAxU9Cp</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/FuZrECsytJmcvGAxU9Cp</guid>
            <pubDate></pubDate>
            <updated>Fri, 31 May 2024 09:53:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词:  腾讯元宝, 混元大模型, AI搜索, AI写作
<br>
<br>
总结:  腾讯元宝是基于混元大模型的App，提供AI搜索和AI写作等核心能力，旨在服务于普通人的生活。在文生图和AI写作方面，腾讯元宝展现出了提升潜力，与其他大模型进行对比评估，效果不俗。AI搜索方面，腾讯元宝接入了微信搜一搜、搜狗搜索等搜索引擎，提升了效率和准确性。 </div>
                        <hr>
                    
                    <p>作者 | 华卫</p><p>&nbsp;</p><p>“腾讯做大模型不争一时之先。”</p><p>&nbsp;</p><p>5 月 30 日，腾讯基于混元大模型的App“腾讯元宝”正式上线，苹果及安卓应用商店均可下载。腾讯云副总裁、腾讯混元大模型负责人刘煜宏表示：“过去的一年，我们持续推进腾讯混元大模型的能力爬坡，希望腾讯元宝最终服务于每个普通人的生活。”</p><p>&nbsp;</p><p>相比此前测试阶段的混元小程序版本，面向工作效率场景，腾讯元宝提供了AI搜索、AI总结、AI写作等核心能力；面向日常生活场景，元宝提供了多个特色AI应用，并新增了创建个人智能体等玩法。</p><p>&nbsp;</p><p>那么，这些功能的实际表现到底如何呢？</p><p>&nbsp;</p><p>拿到腾讯元宝的体验资格后，我们马上逐一试用了它的亮点功能，并特别针对文生图和AI写作方面，通过同题多解的方式，将其与多个国内知名的大模型进行了对比和评估。</p><p>&nbsp;</p><p></p><h2>创作效果：有提升潜力</h2><p></p><p>&nbsp;</p><p>首先，来直击一下元宝与各大模型“battle”的实况。</p><p>&nbsp;</p><p>文生图</p><p>&nbsp;</p><p>在这一功能上，我们选择了百度文心一格、阿里通义万相、讯飞星火、美图 MiracleVision 4.0 、字节跳动豆包五个模型对比效果，对他们的理解和内容生成能力逐一进行了测评。另外，考虑到涉及的这些大模型都源自国产，我们特意选取了中国文言文作为输入素材，以此来考察它们在处理本国语言古典文本上的能力。</p><p>&nbsp;</p><p>从生成图中所涵盖各实体元素的完整程度和整体画面的协调性来说，元宝的表现是排在前列的。</p><p>&nbsp;</p><p>提示词为：林中有寿鹿仙狐，树上有灵禽玄鹤。瑶草奇花不谢，青松翠柏长春。仙桃常结果，修竹每留云。一条涧壑藤萝密，四面原堤草色新。</p><p>&nbsp;</p><p>生成结果如下：</p><p><img src="https://static001.geekbang.org/infoq/44/44c886249db068571bc948559d535c79.jpeg" /></p><p>（从左到右分别是元宝、豆包、讯飞星火、美图 MiracleVision 4.0、文心一格的输出结果）</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bdce8068798d7af3511dcf3aa6362660.png" /></p><p></p><p>（通义万相的输出结果）</p><p>&nbsp;</p><p>AI写作</p><p>&nbsp;</p><p>据介绍，在AI写作方面，元宝不仅支持多轮问答，还能够将对话的内容整理成报告，按照要求进行结构化输出。这一功能上，我们将其与文心一言、通义千问、Kimi、豆包四个模型对比效果。</p><p>&nbsp;</p><p>以测评案例来看，相较而言，腾讯元宝的生成内容展现出了包含起承转合的完整情节，故事框架也已具备雏形，但语句间的逻辑衔接略显生硬、人物描写也较为生涩。</p><p>&nbsp;</p><p>提示词：唐僧师徒四人穿越到现代后的第一天，会发生什么故事？写一个300字左右的小故事。</p><p>&nbsp;</p><p>元宝的生成结果如下：</p><p>&nbsp;</p><p>可以小夸一下的是，元宝留意到了唐僧师徒四人需要吃素的人物细节，在人设和故事设定方面理解得还不错。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6c/6c93ffbd3c79acfc26a4a3cbff437cf4.jpeg" /></p><p></p><p>&nbsp;</p><p>再看豆包的生成结果，其语句结构明显更为成熟了，各个段落环节之间衔接得也比较自然。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/46/46a422e4fd58cbd06cadc795f3632ef5.png" /></p><p></p><p>&nbsp;</p><p>到文心一言这里，无论语句组织还是文字逻辑，都展现出不错的效果。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ecd423a78fee7f5cef56b16772330317.png" /></p><p></p><p>&nbsp;</p><p>而Kimi和通义千问的生成结果，开始呈现出更显著的变化。除语句构造外，整个故事的人物设定、叙事角度、情节架构都更加立体，并都在结尾处给读者构建了一个引人入胜的虚构世界。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/06/06737b6e9a5777f6c72bdacd8b798721.png" /></p><p></p><p>（Kimi的输出结果）</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/cc/ccae0d6aa2348bc17ce68083be19d3b6.png" /></p><p></p><p>（通义千问的输出结果）</p><p>&nbsp;</p><p></p><h2>效率、娱乐方面：表现不俗</h2><p></p><p>&nbsp;</p><p>当前，大模型仍在快速发展期，从模型能力到应用落地都存在较大“时延”。数据显示，当前人们使用大模型相关产品时，有超过 65%的需求，集中在工作/学习效率场景，但相关的AI产品解决方案尚不成熟。针对效率场景的三大核心需求：信息获取、处理和生产，腾讯元宝均进行了产品化探索。</p><p>&nbsp;</p><p>AI 搜索</p><p>&nbsp;</p><p>AI 搜索方面，腾讯元宝接入了微信搜一搜、搜狗搜索等搜索引擎，并通过AI搜索增强，提升时新类和知识类问题效果，比传统搜索更有效率；同时，内容覆盖微信公众号等腾讯生态内容及互联网权威信源，答案准确性更高；此外，元宝还会提供所引用的参考资料，并给出相关推荐，方便快速溯源及延伸阅读。</p><p>&nbsp;</p><p>我们输入一个近日引发热议的美国AI禁令问题：如何看待中国人被限制在美从事 AI 相关工作？</p><p>生成结果如下：</p><p><img src="https://static001.geekbang.org/infoq/b1/b1a00d5888bfd780c585ab89630e45b5.jpeg" /></p><p>&nbsp;</p><p>AI总结</p><p>&nbsp;</p><p>AI总结方面，无论是希望快速了解一本书或是一个新领域，还是处理复杂繁冗的报告、文献，元宝都能帮上忙。据介绍，元宝可上传最多10个PDF、word、txt等多种格式的文档，并能够一次性解析多个微信公众号链接、网址，支持256K的原生窗口上下文。</p><p>&nbsp;</p><p>例如，我们输入：请总结一下AI前线公众号这一年来的内容输出亮点。</p><p>&nbsp;</p><p>生成结果如下：</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/35/35ef172c42abc7d68cefc8b422c5232a.jpeg" /></p><p>&nbsp;</p><p>除了满足效率需要，腾讯元宝在日常生活场景，也提供了丰富的应用及玩法，包括百变AI头像、口语陪练、超能翻译官等，均免费开放。同时，元宝也支持用户根据个性化需求，快速创建个人专属的智能体，赋予角色设定，或让AI自动生成智能体相关信息，并复刻自己的音色。结合腾讯生态场景，元宝还将于近期推出腾讯新闻哥、《庆余年》主题等特色智能体。</p><p>&nbsp;</p><p>视频、3D生成功能后续上线</p><p>&nbsp;</p><p>腾讯元宝产品能力升级的背后，是混元底层模型的持续迭代。</p><p>&nbsp;</p><p>自 2023 年 9 月首次亮相以来，腾讯混元大模型的参数规模已从千亿升级至万亿，预训练语料从万亿升级至7 万亿tokens，并率先升级为多专家模型结构（MoE），整体性能相比Dense 版本提升超50%。除不断提升通用大模型能力外，腾讯混元也支持角色扮演、FunctionCall、代码生成等领域能力，数理能力提升 50 %。</p><p>&nbsp;</p><p>在多模态方面，腾讯混元文生图大模型是业内首个中文原生DiT架构模型，采用了Sora、Stable Diffusion 3等行业顶尖产品的同款架构，生成效果相比上代提升超 20%。目前，该模型已经全面开源，在Github获得 2000+star，相关能力也全面融入腾讯元宝。</p><p>&nbsp;</p><p>此外，腾讯混元大模型在视频、3D生成等方面也持续探索，目前已经支持16s视频生成，单图仅需30秒即可生成3D模型，相关能力也将于后续在元宝中上线。</p><p>&nbsp;</p><p>目前，腾讯内部有超 600 个业务及场景都已经接入腾讯混元，腾讯广告、微信读书、腾讯会议、腾讯文档、腾讯客服等，都已经基于混元实现了智能化升级。据了解，为了满足开发者及企业客户对于通用模型能力的需求，腾讯混元大模型已通过腾讯云对外开放，可通过API调用，也可以作为基底模型，为不同产业场景构建专属应用。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/NMIPCuy1ctaID4CQ3bJN</id>
            <title>AI 是低代码的“福”还是“孽”？</title>
            <link>https://www.infoq.cn/article/NMIPCuy1ctaID4CQ3bJN</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NMIPCuy1ctaID4CQ3bJN</guid>
            <pubDate></pubDate>
            <updated>Fri, 31 May 2024 09:04:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI, 低代码, 大模型, 编程
<br>
<br>
总结: 在AI发展史上，AI大模型的出现引发了对低代码平台的讨论，AI编程的概念挑战了传统的代码开发方式，行业内存在关于AI与低代码未来发展方向的争论。在相关峰会上，专家们分享了AI与低代码的结合实践和思考，探讨了AI时代软件架构的设计和应用。 </div>
                        <hr>
                    
                    <p>取代论，在 AI 几经起伏的发展史上的每一个高点，都会被拿出来重新审视和热议。这种职业危机感，在生成式 AI 的这一波浪潮中，也很快蔓延到了技术圈中。“自己的饭碗被自己的工作干掉，这可能不是一句玩笑话。”一位开发者在近期与 InfoQ 交流时感叹。</p><p></p><p>在此之前，低代码 / 无代码作为软件提效的平台和工具已经逐步流行起来。顾名思义，其价值在于通过图形化界面和简单点击、拖拽、配置，能够大大降低代码开发的门槛，减少开发人员工作量的同时，针对一些简单开发需求，业务人员也能“自给自足”，更快地响应业务侧的需求。</p><p></p><p>而就在低代码概念开始被市场普及接纳走向落地，行业的商业模式刚刚跑通时，半路“杀出”了 AI 大模型。</p><p>相较于低代码，AI 大模型的可能性更为激进：不需要任何的编码，只通过自然语言交互就可以直接生成应用。换言之，代码开发的门槛不是降低了，而是直接没有了。</p><p></p><p>一场争论不可避免。一种声音是“低代码将被 AI 彻底颠覆”，比如，一些低代码起家的公司，去年火速切换到了 GPT 赛道，背后的考量不难臆测。另一种声音则认为，二者将双向奔赴，AI 能力将成为低代码的标配，有不少企业正在试图将二者融合。</p><p></p><p>在 6 月 14 日 -15 日即将举办的 <a href="https://archsummit.infoq.cn/2024/shenzhen/">ArchSummit 全球架构师峰会深圳站</a>"上，阿里巴巴研究员 / 阿里云云原生应用平台负责人丁宇（叔同）将带来<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5718">《AI 编程如何颠覆生产力》</a>" 的 Keynote 主题演讲，详细介绍在 AI 大模型快速发展的背景下，AI 编程的发展实现了哪些突破，以及 AI 编程助手的引入如何为软件开发带来质的飞跃。从开发者的视角出发，他将展开分享 AI 编程工具基于大模型的设计要点、难点、改进思路，帮助开发者从自身的生命力出发，学会用 AI 激活开发效率，提升生产力，而不是与之对抗。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8c/8cc3b180566afd334adb450c1a57fba6.webp" /></p><p></p><p>为进一步探讨 AI 与低代码的关系，ArchSummit 深圳还进一步策划了<a href="https://archsummit.infoq.cn/2024/shenzhen/track/1643">《低代码与 AI 结合》专题</a>"，深入研究低代码平台如何与人工智能技术相结合，提高开发效率。探讨在低代码环境中集成智能决策、自动化流程，以及构建灵活、高效的应用系统。</p><p></p><p><img src="https://static001.geekbang.org/infoq/03/03baaaff951079ab3734a289ead8e5df.webp" /></p><p></p><p>腾讯 PCG 前端技术专家苑宗鹤将在专题演讲中分享《无极低代码 UI 可视化的 AIGC 落地与实践》。在他看来，大模型的提效能力加上低代码的易用性相辅相成，让低代码开发效率更高，更大的降低了用户的使用门槛。他将基于无极低代码平台介绍 AI 搭建低代码布局、AI 辅助代码生成，以及对平台的 AI 功能进行自动化测试的实践路径。</p><p></p><p>网易 CodeWave 技术团队作为国内较早采用大模型技术并将其应用于产品的技术团队，其团队负责人姜天意也将在专题演讲中从低代码产品的挑战出发，分析大模型的机遇和实施难点。同时，从 AI 友好的语言设计出发，结合大模型的 Agent 能力，介绍融合自然语言生成、辅助编程、D2C 等 AI 能力的产品设计和实施方案，以及相关的模型训练方案。</p><p></p><p>针对“AI 都能编程了，低代码平台会被消灭吗？”这样的灵魂拷问，蚂蚁集团支付宝体验技术部 / 高级前端技术专家江凯将在其演讲中给出他的答案。他将详细介绍《云凤蝶在 AI 与 LowCode 结合上的思考与实践》，分享 AI Native 的低代码产品形态如何设计、如何实际应用 LLM 和 AIGC 技术、如何看待对话式 UI、生成式 UI 的发展？AI 原生应用的 LowCode 研发有市场吗等一系列行业普遍的困惑问题。</p><p></p><p>除此之外，本次大会还策划了 10 余个 AI 系列相关的专题，顺丰集团 CTO 耿艳坤、Thoughtworks CTO Scott Shaw、百度飞桨总架构师于佃海等国内外 100+ 顶尖专家齐聚，将从底层基础到顶层应用深度探索大模型时代软件架构的最佳设计，共探 AI 时代的无限可能。</p><p></p><p>点击链接可查看更多详情：<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">https://archsummit.infoq.cn/2024/shenzhen/schedule</a>"</p><p>会议 9 折购票倒计时1天，如您感兴趣，可以联系票务经理 17310043226，锁定最新优惠。</p><p><img src="https://static001.geekbang.org/infoq/d9/d9b861049ccebcbdec69ed036e705118.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/IN98I1kFWJKwEA1aGZKg</id>
            <title>剥离几百万行代码，复制核心算法去美国？TikTok最新回应来了</title>
            <link>https://www.infoq.cn/article/IN98I1kFWJKwEA1aGZKg</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IN98I1kFWJKwEA1aGZKg</guid>
            <pubDate></pubDate>
            <updated>Fri, 31 May 2024 08:34:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 代码剥离, TikTok, 美国用户, 算法
<br>
<br>
总结: TikTok正着手为其1.7亿美国用户开发克隆版推荐算法，剥离数百万行代码是一项繁琐的工作，需要一年多时间才能完成。这项工作的目标是为面向美国用户的TikTok推荐算法创建一套新的源代码库，但可能导致美国TikTok失去母公司字节跳动的工程开发支持。 </div>
                        <hr>
                    
                    <p></p><blockquote>剥离几百万行代码，是一个繁琐的“脏活”，需要一年多时间才能完成。</blockquote><p></p><p>&nbsp;</p><p>路透社5月30日消息，据直接掌握内情的消息人士透露，TikTok正着手为其1.7亿美国用户开发克隆版推荐算法。这可能催生出一个独立于其中国母公司运行的版本，因此更容易被拟发布禁令的美国立法机构所接受。</p><p>&nbsp;</p><p>就在TikTok中国母公司字节跳动去年年底下令拆分源代码之前，美国方面已经提出一项拟议的强制出售TikTok在美业务的法案，且此项法案今年初在国会得到支持。今年4月，该法案正式被签署为法律。由于未获授权公开谈论这款短视频分享应用，消息人士拒绝透露姓名。但其表示一旦代码被拆分，即可为剥离在美资产奠定基础，从而打开一条免受法律强制执行的可能之路。</p><p>&nbsp;</p><p>TikTok公司此前曾经表示并无出售在美资产的计划，甚至强调绝不可能采取此类措施。该公司最初拒绝发表评论。但在本篇报道发表之后，TikTok在X上的帖子中表示，“路透社方面日前发布的报道存在误导性，且与事实有所出入”。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/4d/4d2d551608dc0513c79bf068230f6055.jpeg" /></p><p></p><p>&nbsp;</p><p>路透社表示，该“辟谣”没有具体说明哪里不准确。TikTok还发布了一段来自其联邦诉讼材料的内容：“无论是从商业、技术还是法律角度来看，该法案提出的「获准剥离」以允许TikTok继续在美运营的建议都根本不可行。而即使可行，法案给出的270天时间表也远远不够。”</p><p>&nbsp;</p><p>路透社方面的发言人则回应称，“我们坚持报道内容属实。”</p><p>&nbsp;</p><p></p><h2>剥离数百万行代码的“脏活”</h2><p></p><p>&nbsp;</p><p>路透社称，过去几个月以来，来自美国和中国的数百名字节跳动和TikTok工程师被要求着手剥离数百万行代码，旨在筛选该公司用于将用户与其偏好的视频相匹配的算法。据两位直接了解该项目的消息人士说，工程师们的任务就是建立一套独立的代码库，其独立于字节跳动的中国版TikTok、即抖音的系统之外，同时将消除其中与中国用户相关的信息。</p><p>&nbsp;</p><p>路透社此前曾报道称，TikTok将应用程序连同算法一同出售的可能性极低。而且内容推荐算法在我们国家的出口管制名单当中，因此对TikTok算法的剥离或者出售必须经过审查。</p><p>&nbsp;</p><p>根据相关法律文件，TikTok推荐引擎的源代码最初由字节跳动工程师在中国开发完成，并针对包括美国在内的TikTok全球各市场运营情况进行了定制。</p><p>&nbsp;</p><p>在字节跳动看来，TikTok之所以在全球范围内大受欢迎，首先要归功于其推荐引擎的出色表现。该引擎能够根据每位用户与其观看内容的交互方式来提供更多内容来源。</p><p>&nbsp;</p><p>在向路透社描述这项任务的复杂性时，消息人士们将其称为繁琐的“脏活”，这也凸显出将TikTok业务与其母公司底层代码剥离开来的难度。据消息人士们介绍，这项工作预计需要一年多时间才能完成。</p><p>&nbsp;</p><p>TikTok及字节跳动已经明确表示会以美国宪法第一修正案为依据，在法庭上对抗这条新法律。尽管如此，消息人士称工程师们已经在依命令开展工作，着手将TikTok推荐引擎与字节跳动的整体网络拆分开来。</p><p>&nbsp;</p><p>消息人士们提到，TikTok高管曾一度考虑开源部分TikTok算法，或者允许其他人访问并修改该算法，借此展示技术透明度。</p><p>&nbsp;</p><p>据一位出席团队全体会议的消息人士、以及另一位翻阅过相关材料的消息人士所言，高管们在会上通过内部规划文件及内部通讯系统Lark传达了剥离计划，并就代码拆分项目做出了更新说明（但路透社无法独立核实这些内部消息的真伪）。</p><p>&nbsp;</p><p>据一位消息人士透露，目前这项工作的复杂之处，在于确定TikTok具体代码迁移部分所带来的合规性与法律问题。消息人士还补充称，必须审查每一行代码以核实其是否可以被剥离至独立代码库。</p><p>&nbsp;</p><p>这项工作的目标，是为面向美国用户的TikTok推荐算法创建一套新的源代码库。工作完成之后，TikTok美国版将独立于其他地区的TikTok版本以及中文版抖音，采取专门的一套推荐算法运行和维护体系。消息人士称，此举将导致美国TikTok失去母公司字节跳动强大的工程开发支持。</p><p>&nbsp;</p><p>消息人士还补充称，如果TikTok最终完成美版推荐引擎与中国版本的拆分工作，管理层承认后续TikTok美国版在性能上恐怕达不到现有TikTok的水平。因为目前TikTok推荐算法库仍高度依赖字节跳动中国工程师们的更新和维护。换句话说，TikTok在美国市场上的用户吸引力可能将因此被削弱。</p><p>&nbsp;</p><p></p><h2>TikTok推荐算法神秘吗？</h2><p></p><p>&nbsp;</p><p>在TikTok风波中，其推荐算法一直是大家争相讨论的话题中心。</p><p>&nbsp;</p><p>2022年6月，有媒体报道，TikTok宣布将美国境内的所有流量转移到甲骨文云服务的基础设施上，同时这项托管服务也给甲骨文带来高达10 亿美元收入。而后，甲骨文于这一年的8月份启动了对TikTok 算法和模型的审查，甲骨文希望确保 TikTok 上的内容显示“符合用户的期望”，并且推荐算法不会受到操纵。TikTok 还专门设有一个“专用透明度中心”的区域，供甲骨文员工审查该应用程序的源代码。</p><p>&nbsp;</p><p>然而一年之后，据福布斯报道，字节跳动和甲骨文之间的关系就已经变得非常不信任和敌对。消息人士将甲骨文对字节跳动的立场描述为“反情报行动”，而不是正常的客户关系。与此同时，一些字节跳动员工怀疑甲骨文是否只是想增加他们的账单。TikTok 托管服务合同在甲骨文内部被称为 Project Telesis，使字节跳动成为甲骨文最赚钱的客户之一。</p><p>&nbsp;</p><p>如今看来，甲骨文的审查并没有让美国政府放松对TikTok的仇恨和警惕。</p><p>&nbsp;</p><p>实际上，TikTok 之前发表过一篇博客文章，主要描述解释了他们的 FYP 算法工作机制，相信大家只要是做软件技术的，看过之后都知道其中并没有什么新鲜的创造。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/29/295ecc271aa2b22c4c00ed3e4b8ae723.jpeg" /></p><p></p><p>&nbsp;</p><p>另外，前亚马逊产品经理（同时也是亚马逊战略规划部第一位分析师）Eugene Wei 也曾发过专门分析文章，阐述TikTok 的算法本身并没有特别的突破性创意。</p><p>&nbsp;</p><p></p><blockquote>当大家谈及 TikTok 的算法是其成功的关键时，便会认为该公司的秘密武器是一些神奇的代码。但该领域的大多数专家持怀疑态度，TikTok 在机器学习推荐算法方面并未取得外界未知的突破性进展。事实上，他们中的大多数人认为，TikTok 很可能就是基于标准方案解决的问题，跟其他方案无异，没有什么特殊性。&nbsp;不过机器学习算法的有效性并不仅仅取决于算法本身的函数，还取决于数据集训练后的算法函数。GPT-3 并不是新创意，但是通过大量数据训练和大量的参数设置，它的输出结果往往是令人惊讶的。&nbsp;同样道理，基于自身数据集训练过的 TikTok FYP 算法，在将视频与觉得该视频有趣的人进行匹配方面做的非常精确和高效（而且，反向匹配做的也很精确，对某些视频不感兴趣的人就不会接收到这些视频） 。</blockquote><p></p><p>&nbsp;</p><p>他认为，TikTok 产品真正的价值点在于 TikTok 的设计和流程里面的每一个元素是怎么互相关联到一起，从而创建出一个数据集，再通过这个数据集，把算法训练成最佳性能的。</p><p>&nbsp;</p><p>“这就是 TikTok 设计的神奇之处：它是一个反馈的闭环，这种设计能够激发并实现视频的创作和观看，产生的数据进而通过其算法进行训练，之后再反过来激发创作和观看。为了让 TikTok 的算法变得像现在这样有效，TikTok 成为了它自己的训练数据来源。”</p><p>&nbsp;</p><p>但多数人还是非常费解，为什么很多公司想要收购 TikTok，另一方面，字节跳动是否应该将 TikTok 这一备受欢迎的 App 卖掉。对此，Eugene Wei 评论说：“围绕 TikTok 算法大肆的炒作已经开始变的异化了，这也是如今西方对中国科技领域项目的普遍套路。”</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.reuters.com/technology/tiktok-preparing-us-copy-apps-core-algorithm-sources-say-2024-05-30/">https://www.reuters.com/technology/tiktok-preparing-us-copy-apps-core-algorithm-sources-say-2024-05-30/</a>"</p><p><a href="https://newsroom.tiktok.com/en-us/how-tiktok-recommends-videos-for-you">https://newsroom.tiktok.com/en-us/how-tiktok-recommends-videos-for-you</a>"</p><p><a href="https://www.infoq.cn/article/38dKguZxeyz2vx2dAR4S?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">https://www.infoq.cn/article/38dKguZxeyz2vx2dAR4S</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/meMm4MXomT8GZMYvDzbo</id>
            <title>全球首款PC原生的AI编程与软件智能研发助手驭码CodeRider正式发布！</title>
            <link>https://www.infoq.cn/article/meMm4MXomT8GZMYvDzbo</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/meMm4MXomT8GZMYvDzbo</guid>
            <pubDate></pubDate>
            <updated>Fri, 31 May 2024 02:56:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AIGC, AI产品发布会, 驭码 CodeRider, 极狐GitLab
<br>
<br>
总结: 5月28日下午，极狐GitLab在上海成功举办了基于AIGC技术且完全自研的AI产品发布会，正式推出驭码CodeRider，PC原生的AI编程与软件智能研发助手。CEO柳钢介绍了驭码CodeRider的含义和优势，强调AI赋能程序员而非取代。产品具有PC原生、私有化部署、与GitLab深度融合等优势，受到企业用户好评。发布会还宣布了驭码CodeRider的三个版本，开启了试用通道。 </div>
                        <hr>
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/c3/c3e154ed850988f8ff90ce95496aa795.png" /></p><p></p><p>5 月 28 日下午，极狐GitLab 在上海成功举办了基于 AIGC 技术且完全自研的 AI 产品发布会，正式推出驭码 CodeRider —— PC 原生的 AI 编程与软件智能研发助手。发布会采取线上线下相结合的方式，线下有包括企业高管、高校教师、高校学生、媒体人员在内的 80 多位嘉宾共同出席参加了此次发布会，线上观看量超过 10万+。 </p><p></p><p>发布会伊始，极狐GitLab CEO 柳钢以“PC 原生全球首发 AI 编程与软件智能研发助手”为主题，全面介绍了驭码 CodeRider。 </p><p></p><p><img src="https://static001.geekbang.org/infoq/40/40e343e6a832e4f6098a3baf0b60536d.png" /></p><p>极狐GitLab CEO 柳钢 </p><p></p><p>柳钢表示，软件定义世界已经成为了不争的事实，而软件的打造者、代码的创造者正是程序员这一群体，在中国就有 1000万程序员。而极狐GitLab就是一家专为中国程序员服务的公司，也是目前国内唯一一家将赋能中国程序员写进员工手册的公司，极狐公司的使命是——让中国程序员的技术潜能与业务价值得到最大化发展。极狐GitLab 致力于通过借鉴全球领先的技术，同时基于国内现状进行完全自主研发、自主创新的方式来赋能中国1000万程序员。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c1/c115d21986defd5f342fcde2bb393d21.png" /></p><p>赋能 or 取代？ </p><p></p><p>柳钢进一步表示，在 AI 时代赋能中国程序员的最好方式就是将 AI 能力应用到软件研发领域，为程序员打造出为程序员所有、被程序员所用、让程序员认可的产品，用 AI 赋能程序员，绝非取代程序员 。但是在这个过程中必须解决当前大模型的三个关键问题：数据安全、个性化以及成本。只有成功解决这三个问题，才能够真正打造出程序员心目中的绝佳好产品——覆盖软件研发全生命周期、企业统一部署 &amp; 用户一键安装、功能丰富 &amp; 触手可及以及私藏独有。</p><p></p><p></p><p>“极狐GitLab AI 团队完全自研的驭码 CodeRider 正是这样一款产品。而且驭码 CodeRider 是全球首款 PC 原生的 AI 编程与软件智能研发助手。”柳钢强调。 </p><p></p><p><img src="https://static001.geekbang.org/infoq/24/24c56680c152f342663a416854148fd6.png" /></p><p>极狐GitLab 驭码CodeRider 正式发布 </p><p></p><p>接着，柳钢解读了驭码 CodeRider 的含义。Code 是程序员心目中最神圣的词，意为编码，而 Rider 有骑兵、驾驭者的意思。为此，特意为该产品起了一个朗朗上口的 Slogan“为 AI（爱）奔腾，驭码当先”。从表面看，驭码 CodeRider 是一个 AI 编程助手，看似是又一款 Copilot，但是驭码又绝不仅仅只是一款 Copilot，相比市面上其他所有的 Copilots，驭码 CodeRider 要比他们优秀三点，而这也是驭码 CodeRider 得天独厚的优势：PC 原生、私有化部署、GitLab 合璧。PC 原生是指驭码 CodeRider 是端侧部署，和笔记本电脑是天然契合的，而且产品在设计之初就考虑了离线运行；私有化部署是指可以将驭码 CodeRider 部署在企业内部的安全环境中，这从根本上解决了数据安全问题，而且能够更好地满足和实现企业智能化、个性化问题；最后一点：驭码 CodeRider 和 GitLab 深度融合、浑然一体，这意味着驭码 CodeRider 将 AI 技术完全应用到软件研发的全生命周期中，不仅可以编写代码，还能够帮助程序员进行议题（Issue）和合并请求（MR）的处理，甚至和 CI/CD 结合起来，做到端到端的 AI 赋能。这些都是其他 Copilots 无法做到的！ </p><p></p><p><img src="https://static001.geekbang.org/infoq/a3/a34bc5dd659e0c563ed255a78769b9fd.png" /></p><p>驭码CodeRider 比 Copilot 好三点 </p><p></p><p>柳钢表示，自夸不算好，要用户说了才算好，目前已经有很多企业用户在体验试用驭码 CodeRider，他们的反馈是对驭码 CodeRider 最好的认可。随后，现场播放了来自嘉宾的祝福视频，包括联想集团、通义实验室、零一万物、PingCAP、无问芯穷、LigaAI、重庆市政府九龙坡区、未尽研究、哪吒汽车、足下科技、PingCode、Sipingsoft、青岛职业技术学院、南京大学、厦门大学、重庆交通大学、广州华商学院的多位嘉宾纷纷表达了对驭码 CodeRider 即将发布的热烈期盼，同时预祝发布会圆满成功！ </p><p></p><p>分享最后，柳钢正式揭晓了驭码 CodeRider 的三个版本：驭码团队版（CodeRider Team）、驭码企业版（CodeRider Enterprise）以及驭码混合算力版（CodeRider Hybrid）。三个版本对应不同的功能以及不同的使用场景。驭码CodeRider 同步开启了申请试用通道，会有专业的顾问为大家解答关于驭码CodeRider 的相关情况。</p><p></p><p><img src="https://static001.geekbang.org/infoq/07/0723ffc88faa9bccf42476d76f49a1c3.png" /></p><p>驭码 CodeRider 专业顾问联系方式 </p><p></p><p>随后，极狐GitLab AI 团队负责人、驭码CodeRider 研发负责人邹雨竹上台深入介绍了驭码 CodeRider 的相关情况。他提到，“驭码 CodeRider 是一款 AI 驱动的 PC 原生应用，是研发人员的智能编程助手，同时跟 GitLab 的深度集成，也让驭码 CodeRider 变成了一款智能 DevOps 工具，真正做到了“一款工具，双重助力”。 </p><p></p><p><img src="https://static001.geekbang.org/infoq/67/6780c895939065b4083d7d041665f79a.png" /></p><p>极狐GitLab AI 产品负责人邹雨竹 </p><p></p><p>邹雨竹进一步解释道，打造这样一款具备 AI 能力的 PC 原生应用，必须考虑三个核心要素算力、模型、引擎。在算力方面，驭码 CodeRider 从设计之初就决定要做 PC 原生的“AI 编程与软件智能研发助手”，为此对 20 多款 AI PC 进行了详尽测评；在模型方面，采取采众家之所长的策略，对 30 多款大模型进行了测评，最后决定驭码 CodeRider 根据不同功能选择最佳模型，比如代码补全使用补全模型、对话采用对话模型，以实现在高效量化的同时达到性能和与体积最佳平衡的目的；在引擎方面，采用了基于 C++ 的桌面推理引擎，而且对不同的硬件指令集和框架做了适配。而这样做也获得了惊艳的效果，同样也造就了驭码 CodeRider 这样一款同时适用于企业、个人的 AI 产品。企业可以通过私有化部署实现数据安全并节省超过 50% 的成本，而开发者则能享受便携性和极佳的响应速度。 </p><p></p><p>邹雨竹还在现场演示了驭码 CodeRider 的相关功能，包括智能编程部分的代码补全、代码生成、技术问答等，以及智能 DevOps 工作流中的 Issue、MR 处理等。“驭码 CodeRider 绝对是一款让企业受益的 AI 工具，因为其具备私有化部署的特性，有着模型组合的优越性，而且与 GitLab 浑然天成”。邹雨竹说到。 </p><p></p><p><img src="https://static001.geekbang.org/infoq/57/57f4a2d7111a97b0a83e660fba224d83.png" /></p><p>驭码CodeRider 产品功能图 </p><p></p><p>接着，来自联想中国的段勐、英特尔中国的张智勇、浙江省特级教师谢作如，三位嘉宾分别进行了分享，他们从企业、高校的角度分享阐述了与 AI 相关的内容和观点。 </p><p></p><p>发布会最后，极狐GitLab CEO 柳钢再次登台，正式揭晓了驭码 CodeRider 三个版本的价格：驭码团队版（CodeRider Team）的价格为499/人/年、驭码企业版（CodeRider Enterprise）的价格为899/人/年。与此同时，也透露了，目前驭码 CodeRider 团队正在积极打造企业级的混合算力私有化架构，智能终端+智能边端会让驭码 CodeRider 变得更强大，这样强大的产品将在 90 天以后与大家见面！ </p><p></p><p><img src="https://static001.geekbang.org/infoq/d4/d4e4a7198909b9d667a42a123ca49d0c.png" /></p><p>驭码CodeRider 价格 </p><p></p><p>最后，柳钢表示，全球首款 PC 原生的 AI 智能编程与软件智能研发助手——驭码CodeRider 发布会宣告结束！到场嘉宾纷纷走向极狐GitLab 团队提前准备好的驭码 CodeRider 体验区，现场体验驭码 CodeRider 的 AI 功能并与技术人员进行了深入交流。 </p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ff74da88bd54726eb9c1496759ae3106.png" /></p><p>驭码CodeRider 现场体验区</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/hPED1Wk71CIt3RHIl2e8</id>
            <title>网易有道CEO周枫：模型即应用的时代到来，Super App随时会诞生</title>
            <link>https://www.infoq.cn/article/hPED1Wk71CIt3RHIl2e8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/hPED1Wk71CIt3RHIl2e8</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 May 2024 07:43:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, AI创新应用, RAG引擎, 有道小P
<br>
<br>
总结: 有道分享了子曰教育大模型的最新技术进展和三大AI创新应用，其中包括新一代知识库问答引擎QAnything、AI全科学习助手“有道小P”APP和虚拟人口语教练Hi Echo 3.0。有道CEO表示大模型需要结合应用场景去优化，并介绍了自研的RAG引擎QAnything。该引擎支持多语种内容提问，已在多个领域取得显著进展。同时，有道还展示了有道小P和Hi Echo 3.0在教育场景中的最新应用成果。 </div>
                        <hr>
                    
                    <p>作者 ｜ 华卫</p><p>&nbsp;</p><p>“大模型的Super App尚未出现，但随时可能会诞生。”</p><p>&nbsp;</p><p>5月29日，网易有道分享了子曰教育大模型最新技术进展及三大AI创新应用：新一代知识库问答引擎QAnything、AI全科学习助手“有道小P”APP和虚拟人口语教练Hi Echo 3.0。</p><p>&nbsp;</p><p>现场，网易有道CEO周枫表示：“当前已经是‘模型即应用’的时代，但大模型也不是万能的，关键是抓住场景。”他认为，大模型的发展需要结合应用场景去迭代优化，在‘产模一体’的框架下去同步提升模型与产品。</p><p>&nbsp;</p><p>据介绍，目前在AI+在线营销、AI+效率工具、AI+娱乐等多个领域，有道都取得了显著进展。有道子曰大模型已经落地到了LLM翻译、AI作文指导、文档问答、语法精讲、虚拟人口语教练等一批应用上。</p><p>&nbsp;</p><p>截至目前，有道AI在线营销业务已连续六个季度实现超过50%的同比增长，AI翻译功能的使用人数已超过500万，使用次数达到2000万次。今年一季度，有道AI订阅服务收入持续增长，一季度会员销售额约5000万，同比增长140%。</p><p>&nbsp;</p><p></p><h2>自研RAG引擎</h2><p></p><p></p><h2>或突破文档问答模态</h2><p></p><p>&nbsp;</p><p>交流会现场，有道首席科学家段亦涛介绍了有道自研的RAG引擎——QAnything。此次升级，QAnything在私有化部署和智能体生成方面进行了一系列的提升，从单纯的文档问答进化成“企业AI大脑”。</p><p>&nbsp;</p><p>段亦涛指出，其原理是利用外部知识来辅助大模型的生成，提高正确性。虽然原理简单，但要落实到生产和生活中真正起到作用，其实还有很大的距离。为此，在开发QAnything之初，有道定下多个目标。</p><p>&nbsp;</p><p>首先是跨语种，QAnything支持中、英、日、韩四种语种的内容提问；其次要求检索质量足够高，有道在这个环节里优化所有环节的模型；另外是好用，QAnything支持一键安装和本地部署，同时支持各种格式文件，形成智能问答的形式。</p><p>&nbsp;</p><p>“QAnything的下一个发展目标是，突破文档问答单纯形式模态，变成真正能够理解企业业务逻辑、领域知识，融入到业务链条驱动决策提供智能能力的AI大脑。”段亦涛透露，为达成目标，现在他们已对QAnything进行三方面升级，包括领域适配能力、增加Agent的支持和内容生成能力“AI写手”的升级，使得文章分类准确度达到95.9%。</p><p>&nbsp;</p><p>通过领域定制适配，QAnything现在已经突破教育领域，拓展到医疗、互联网、智慧企业等行业；引入Agent能力后，QAnything允许用户根据自己的需求和业务特点来去定义整个系统，每个用户可以用这套能力来去定制个性化机器人。现在，该功能已经在有道领世业务落地。</p><p>&nbsp;</p><p>今年年初，QAnything正式对外开源，四个月在GitHub上获得了近1万个星标。据悉，在此期间，有道平均1-2周升级一次内核版本，不断优化和迭代算法的效果、稳定性和质量。目前，QAnything已经服务了20多个不同行业的上百家客户，超过3万用户将其用于各自的业务领域。</p><p>&nbsp;</p><p>另外，段亦涛表示，大模型向产业化发展后，国产大模型是否够用，取决于期望和要求。现在，大模型能够表现出来的能力，在实际应用中还没有完全被激发出来。“大家都在探索激发模型能力的方式，比如通过RAG等方式辅助其扬长避短，能够在应用中体现价值。”</p><p>&nbsp;</p><p></p><h2>“有道小P”和Hi Echo 3.0</h2><p></p><p>&nbsp;</p><p>会上，有道还展示了子曰教育大模型在教育场景中的最新应用成果，即可以在手机端便捷使用的“有道小P”和在内容、功能、教学等方面进行突破的Hi Echo 3.0。</p><p>&nbsp;</p><p>据悉，有道小P基于大模型知识问答能力，此前已在家庭辅导和语言学习两大教育场景落地。在此基础上，有道此次正式推出小P独立APP，可以在手机端便捷使用，解决随时答疑的需求。</p><p>&nbsp;</p><p>“小P所代表的大模型知识问答能力，有非常大的场景拓展可能性和增长潜力。” 有道高级副总裁吴迎晖表示，有道小P集成了多种场景下的互动答疑与交互功能，同时在知识记忆、多模态理解和逻辑推理等方面实现了提升。</p><p>&nbsp;</p><p>他介绍到，有道对小P基座模型定向优化的同时，还进行了知识库的100%扩容，新增了很多模态以及高质量的语料数据，此外还有很多细节的迭代和优化。</p><p>&nbsp;</p><p>目前，小P有免费和付费两种模式，基础功能是免费的，同时对外提供订阅服务。谈到现在的大模型“价格战”，周枫表示，“我们应用下来，大模型的成本是下降的，基本上一年至少下降一半。对于做云端服务的公司来说，目前降价是市场行为。”</p><p>&nbsp;</p><p>此外，会上亮相的新一代虚拟人口语教练Hi Echo 3.0，在功能、教学模式、虚拟人等方面进行创新，并携手雅思官方上线口语练考服务。此次升级还通过搭建真实的对话场景，重新设计了“背单词”的过程，未来还将推出“儿童模式”，并新增两个全新的“语伴”角色，打造陪伴式的教学环境。</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/H9cy5L42CkYKtrDaWFUr</id>
            <title>都白学了！Mistral 的首个“开放”编程模型，精通Python、C等 80+ 语言，用220 亿参数赢了 GPT-4</title>
            <link>https://www.infoq.cn/article/H9cy5L42CkYKtrDaWFUr</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/H9cy5L42CkYKtrDaWFUr</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 May 2024 07:32:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 微软支持, AI初创公司, Codestral, 编码模型
<br>
<br>
总结: 5月29日，由微软支持、估值60亿美元的法国AI初创公司Mistral发布了其有史以来的第一个用于编码的“开放式”生成式AI模型，称为Codestral。该模型旨在帮助开发人员编写代码并与之交互，具备广泛的语言基础，可以在各种编码环境和项目中为开发人员提供帮助。Codestral在多个基准测试中名列前茅，性能优越，受到开发者社区的积极反馈。JetBrains研究员Mik... </div>
                        <hr>
                    
                    <p>整理&nbsp;|&nbsp;华卫</p><p>&nbsp;</p><p>5月29日，由微软支持、估值&nbsp;60&nbsp;亿美元的法国&nbsp;AI&nbsp;初创公司&nbsp;Mistral&nbsp;发布了其有史以来的第一个用于编码的“开放式”生成式&nbsp;AI&nbsp;模型，称为&nbsp;Codestral。</p><p>&nbsp;</p><p>与其他代码生成模型一样，Codestral&nbsp;旨在通过共享指令和完成&nbsp;API&nbsp;端点，帮助开发人员编写代码并与之交互。由于精通代码和英语，它还可用于为软件开发人员设计高级人工智能应用程序。值得一提的是，&nbsp;Codestral的参数要求很高，还受到一些许可证方面的使用场景限制。</p><p>&nbsp;</p><p>虽然该模型刚刚推出，尚未进行公开测试，但&nbsp;Mistral&nbsp;声称，Codestral&nbsp;在大多数编程语言上已经优于现有的以代码为中心的模型，包括&nbsp;CodeLlama&nbsp;70B、Deepseek&nbsp;Coder&nbsp;33B&nbsp;和&nbsp;Llama&nbsp;3&nbsp;70B。此外，Codestral&nbsp;在Kotlin语言的表现上似乎还超过了GPT-4-Turbo&nbsp;和&nbsp;GPT-3.5-Turbo。</p><p>&nbsp;</p><p></p><h2>精通&nbsp;80+&nbsp;编程语言</h2><p></p><p></p><h2>多个基准测试中名列前茅</h2><p></p><p>&nbsp;</p><p>首先，&nbsp;Codestral具备广泛的语言基础，可以在各种编码环境和项目中为开发人员提供帮助。据悉，Codestral&nbsp;在&nbsp;80&nbsp;多种编程语言的不同数据集上进行了训练，其中包括Python、Java、C、C++、JavaScript&nbsp;和&nbsp;Bash等流行语言。在&nbsp;Swift&nbsp;和&nbsp;Fortran&nbsp;等更特殊的语言上，Codestral&nbsp;也表现出色。</p><p>&nbsp;</p><p>而且，Codestral&nbsp;可以完成编码函数、编写测试和“填写”部分代码，以及用英语回答有关代码库的问题，可为开发人员节省时间和精力。与&nbsp;Codestral&nbsp;的互动，将有助于提高开发人员的编码水平，减少错误和&nbsp;bug&nbsp;的风险。</p><p>&nbsp;</p><p>性能方面，相比之前其他用于编码的模型，Codestral&nbsp;作为&nbsp;22B&nbsp;的模型，在代码生成的性能/延迟空间方面树立了新的标准。Mistral&nbsp;介绍，Codestral&nbsp;拥有&nbsp;32k&nbsp;的较大上下文窗口（竞争对手为&nbsp;4k、8k&nbsp;或&nbsp;16k），在代码生成的远程评估&nbsp;RepoBench&nbsp;中优于所有其他模型。</p><p>&nbsp;</p><p>同时，Mistral&nbsp;将&nbsp;Codestral&nbsp;与硬件要求更高的现有特定代码模型进行了比较。针对Python，其使用了四个基准测试：通过HumanEval&nbsp;pass@1、MBPP&nbsp;sanitised&nbsp;pass@1来评估&nbsp;Codestral&nbsp;的&nbsp;Python&nbsp;代码生成能力；CruxEval来评估&nbsp;Python&nbsp;输出预测能力；RepoBench&nbsp;EM来评估&nbsp;Codestral&nbsp;的远程存储库级代码完成能力。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/c2/c2cd8bf14b8091529981965456e44356.png" /></p><p></p><p>&nbsp;</p><p>在远程存储库级&nbsp;Python&nbsp;代码的完成情况上，Codestral&nbsp;以&nbsp;34%&nbsp;的准确率优于所有三个模型。同样，在评估&nbsp;Python&nbsp;代码生成的&nbsp;HumanEval&nbsp;和测试&nbsp;Python&nbsp;输出预测的&nbsp;CruxEval&nbsp;上，该模型分别以&nbsp;81.1%&nbsp;和&nbsp;51.3%&nbsp;的分数击败了竞争对手。它甚至优于&nbsp;HumanEval&nbsp;上用于&nbsp;Bash、Java&nbsp;和&nbsp;PHP&nbsp;的模型。</p><p>&nbsp;</p><p>为评估在&nbsp;SQL&nbsp;方面的性能，Mistral&nbsp;使用了&nbsp;Spider&nbsp;基准，Codestral&nbsp;以&nbsp;63.5%&nbsp;的得分位居第二。除了&nbsp;Python&nbsp;之外，Mistral&nbsp;还评估了&nbsp;Codestral&nbsp;在六种不同语言的&nbsp;HumanEval&nbsp;pass@1&nbsp;中的表现：&nbsp;C++、bash、Java、PHP、Typescript&nbsp;和&nbsp;C#，并计算了这些评估的平均值。值得注意的是，该模型在HumanEval的C++，C和Typescript上的表现不是最好的，但所有测试的平均得分最高，为61.5%，仅次于Llama&nbsp;3&nbsp;70B的61.2%。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/fb/fb0c4cbb8344ddfb9d1aad9b9d608044.png" /></p><p></p><p>此外，Mistral&nbsp;使用&nbsp;Python、JavaScript&nbsp;和&nbsp;Java&nbsp;中的&nbsp;HumanEval&nbsp;pass@1&nbsp;评估了&nbsp;Codestral&nbsp;的中间填充性能，并将其与&nbsp;DeepSeek&nbsp;Coder&nbsp;33B&nbsp;进行了比较，后者的中间填充能力可立即使用，而&nbsp;Codestral&nbsp;的得分比它更高。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/21/21a3e6c46e1f08dc174826936985712f.png" /></p><p></p><p>&nbsp;</p><p>在开发者社区的反馈中，JetBrains&nbsp;研究员Mikhail&nbsp;Evtikhiev还表示，“我们使用&nbsp;Codestral&nbsp;对&nbsp;Kotlin-HumanEval&nbsp;基准进行了测试，结果令人印象深刻。例如，在&nbsp;T=0.2&nbsp;的通过率方面，Codestral&nbsp;获得了&nbsp;73.75&nbsp;分，超过了&nbsp;GPT-4-Turbo&nbsp;的&nbsp;72.05&nbsp;分和&nbsp;GPT-3.5-Turbo&nbsp;的&nbsp;54.66&nbsp;分。”</p><p>&nbsp;</p><p></p><h2>220&nbsp;亿个参数</h2><p></p><p></p><h2>并不完全对外开放</h2><p></p><p></p><p>根据&nbsp;Mistral&nbsp;的官方介绍，Codestral&nbsp;是一个&nbsp;22B&nbsp;的开放式模型，采用Mistral&nbsp;AI&nbsp;新推出的非生产许可证&nbsp;（MNPL），允许开发人员将其技术用于研究和测试目的，在&nbsp;HuggingFace&nbsp;上可以下载。该公司通过两个&nbsp;API&nbsp;端点提供该模型：codestral.mistral.ai&nbsp;和&nbsp;api.mistral.ai。</p><p>&nbsp;</p><p>前者专为希望在其&nbsp;IDE&nbsp;中使用&nbsp;Codestral&nbsp;的&nbsp;Instruct&nbsp;或&nbsp;Fill-In-the-Middle&nbsp;路由的用户而设计，它带有一个在个人级别管理的&nbsp;API&nbsp;密钥，没有通常的组织速率限制，并且可以在八周的测试期间免费使用。后者则是更广泛的研究、批量查询或第三方应用程序开发的常用端点，查询按令牌计费。</p><p>&nbsp;</p><p>但该模型是否真的“完全开放”，还有待商榷。这家初创公司的非生产许可证禁止将&nbsp;Codestral&nbsp;及其产出用于任何商业活动，虽然有&nbsp;“开发&nbsp;”的例外，但也有注意事项：&nbsp;许可证明确禁止&nbsp;“员工在公司业务活动的背景下进行任何内部使用”。</p><p>&nbsp;</p><p>原因可能是&nbsp;Codestral&nbsp;部分训练内容受版权保护，Mistral&nbsp;在官方博文中没有证实或否认这一点，但这并不奇怪；有证据表明，这家初创公司以前的训练数据集包含受版权保护的数据。</p><p>&nbsp;</p><p>今年3月，由前&nbsp;Meta&nbsp;研究人员创立的&nbsp;AI&nbsp;模型评估公司&nbsp;Patronus&nbsp;AI&nbsp;发布了一项研究，展示了AI&nbsp;模型制作受版权保护内容的频率，测试的四个模型是&nbsp;OpenAI&nbsp;的&nbsp;GPT-4、Anthropic&nbsp;的&nbsp;Claude&nbsp;2、Meta&nbsp;的&nbsp;Llama&nbsp;2&nbsp;和&nbsp;Mistral&nbsp;AI&nbsp;的&nbsp;Mixtral。当时，Patronus&nbsp;AI&nbsp;的联合创始人兼首席技术官&nbsp;Rebecca&nbsp;Qian&nbsp;表示，“我们几乎在评估的所有模型中都发现了受版权保护的内容，无论是开源还是闭源。”</p><p>&nbsp;</p><p>不过无论如何，Codestral&nbsp;的这一问题可能也不值得太麻烦地讨论。据介绍，该模型有&nbsp;220&nbsp;亿个参数，需要一台强大的&nbsp;PC&nbsp;才能运行。(参数从本质上定义了人工智能模型处理问题的能力，比如分析和生成文本）。从参数规模的使用门槛来说，&nbsp;Codestral&nbsp;对大多数开发人员来说或许并不实用，在性能提升方面也是渐进式的。</p><p>&nbsp;</p><p></p><h2>代码模型的使用争议</h2><p></p><p>&nbsp;</p><p>Codestral&nbsp;的出现，可能会引发“关于依赖代码生成模型作为编程助手是否明智”的争论。</p><p>&nbsp;</p><p>至少在某些编码任务中，开发人员肯定会采用生成式AI工具。在&nbsp;2023&nbsp;年&nbsp;6&nbsp;月的一次&nbsp;Stack&nbsp;Overflow&nbsp;民意调查中，44%&nbsp;的开发人员表示，他们现在在开发过程中使用AI工具，26%&nbsp;的开发人员计划不久后使用。然而，需要注意的是，这些工具有明显的缺陷。</p><p>&nbsp;</p><p>今年1月，GitClear&nbsp;收集并分析了&nbsp;2020&nbsp;年&nbsp;1&nbsp;月至&nbsp;2023&nbsp;年&nbsp;12&nbsp;月期间编写的&nbsp;1.53&nbsp;亿行更改的代码。其发现，生成式AI开发工具正在导致更多错误代码被推送到代码库中，且这些助手并没有重构代码，而是提供了一键式重复现有代码的“诱惑”。当时，GitClear&nbsp;指出，2024&nbsp;年的问题是：谁来收拾残局？“对于代码的长期可维护性而言，也许没有比复制/粘贴代码更大的祸害了。”</p><p>&nbsp;</p><p>2月，Snyk&nbsp;的一项新研究警告说，生成式&nbsp;AI&nbsp;驱动的编码助手，如&nbsp;GitHub&nbsp;Copilot等通常会放大用户代码库中现有的错误和安全问题。“简单地说，当Copilot建议代码时，它可能会无意中复制邻居文件中存在的现有安全漏洞和不良做法。这可能导致不安全的编码实践，并为一系列安全漏洞打开大门。”Snyk的开发者关系和社区主管Randall&nbsp;Degges表示，大多数开发人员可能没有意识到AI编码助手可以很容易地从用户的代码库和开源项目中复制现有的安全问题。</p><p>&nbsp;</p><p>在2024计算机-人机交互会议（CHI&nbsp;2024）上，普渡大学的一项研究显示，OpenAI&nbsp;的&nbsp;ChatGPT&nbsp;对编程问题给出的答案，有52%包含错误信息，77%的答案比人类答案更冗长，78%与人类答案存在不同程度的不一致。</p><p>&nbsp;</p><p>但这些研究结果，或许并不能阻止Mistral&nbsp;等公司试图用他们的代码模型来赚钱。</p><p>&nbsp;</p><p>现在，Mistral&nbsp;已经在其&nbsp;Le&nbsp;Chat&nbsp;对话式人工智能平台上推出了托管版&nbsp;Codestral&nbsp;及其付费&nbsp;API。Mistral还表示，将致力于把Codestral构建到LlamaIndex、LangChain、Continue.dev和Tabnine等应用框架和开发环境中。“从我们最初的测试来看，Codestral是代码生成工作流程的绝佳选择，速度快、具有有利的上下文窗口，且&nbsp;instruct&nbsp;版本支持工具使用。”LangChain首席执行官兼联合创始人Harrison&nbsp;Chase在一份声明中表示。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://techcrunch.com/2024/05/29/mistral-releases-its-first-generative-ai-model-for-code/?guccounter=1">https://techcrunch.com/2024/05/29/mistral-releases-its-first-generative-ai-model-for-code/?guccounter=1</a>"</p><p><a href="https://mistral.ai/news/codestral/">https://mistral.ai/news/codestral/</a>"</p><p><a href="https://venturebeat.com/ai/mistral-announces-codestral-its-first-programming-focused-ai-model/">https://venturebeat.com/ai/mistral-announces-codestral-its-first-programming-focused-ai-model/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/6gYD53yUW8bxMXu5Djpk</id>
            <title>让智能设备更懂你，主动式AI正在崛起 | 大模型一周大事</title>
            <link>https://www.infoq.cn/article/6gYD53yUW8bxMXu5Djpk</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/6gYD53yUW8bxMXu5Djpk</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 May 2024 01:58:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能大模型, 主动式AI应用, 大模型持续更新, AGI概念
<br>
<br>
总结: 大模型的快速发展使了解最新技术动态和积极学习成为从业者的必修课。本周人工智能大模型在应用方面取得进展，主动式AI应用集中涌现。科研领域也在积极行动，为大模型透明度与可控性提供重要研究基础。同时，AGI概念引发热议，各行业对AGI的应用程度也备受关注。 </div>
                        <hr>
                    
                    <p>大模型的快节奏发展，让了解最新技术动态、积极主动学习成为每一位从业者的必修课。InfoQ研究中心期望通过每周更新大模型行业最新动态，为广大读者提供全面的行业回顾和要点分析。现在，让我们回顾过去一周的大模型重大事件吧。</p><p></p><h2>一、重点发现</h2><p></p><p>本周，人工智能大模型在应用方面迎来了一系列令人瞩目的进展，特别是主动式AI应用的集中涌现。微软首发的Copilot+PC、小鹏汽车的AI天玑系统、美的发布的主动式全屋智能解决方案、联想集团推出的联想Yoga&nbsp;Slim&nbsp;7x和联想ThinkPad&nbsp;T14s&nbsp;Gen&nbsp;6，都标志着主动式AI展现出强大的应用潜力，智能科技将更加深入地渗透到人们的日常生活中。此外，科研领域也在积极行动，Anthropic为大模型透明度与可控性的提升提供了重要研究基础，“CCF-阿里妈妈科技袋基金”为学术界和工业界的融合交流提供了重要平台，有望促进更多创新成果的诞生。</p><p></p><h2>二、具体内容</h2><p></p><p></p><h3>大模型持续更新</h3><p></p><p>5月22日，百川智能发布最新一代基座大模型Baichuan&nbsp;4，其中通用能力提升超过10%，数学和代码能力分别提升14%和9%。</p><p></p><h4>多模态领域</h4><p></p><p>viva公司推出了一款Sora&nbsp;同架构视频生成模型，该模型对用户免费开放，具备文本生成视频、图片生成视频，4K分辨率放大，提示词自动优化功能。在视频生成方面表现出色，尤其是竖屏视频的质量，文生视频单次可生成&nbsp;5&nbsp;秒视频，图生视频为&nbsp;4&nbsp;秒视频，但在一致性和物理特性模拟上仍有提升空间。</p><p></p><h4>科研领域</h4><p></p><p>5月21日，Anthropic&nbsp;宣布成功从&nbsp;Claude&nbsp;3&nbsp;中提取数百万特征，在理解人工智能模型内部运作机制方面取得进展。该项研究对于提升大模型透明度与可控性方面意义重大。</p><p></p><h3>应用探索</h3><p></p><p></p><h4>新产品新应用/功能</h4><p></p><p>5月21日，在微软Build&nbsp;2024开发者大会上，GitHub&nbsp;推出首套由微软和第三方合作伙伴开发的&nbsp;GitHub&nbsp;Copilot&nbsp;扩展，允许任何人通过自然语言和更广泛的功能来提高代码开发速度。5月21日，微软宣布GPT-4o在Azure&nbsp;AI上普遍可用，还引入了一系列由知名合作伙伴和开源社区开发的大模型，进一步丰富了Azure&nbsp;AI平台的模型库。5月22日，百川智能推出成立之后的首款AI助手“百小应”。百小应不仅能够即时响应用户提出的各类问题，还具备快速阅读文件、整理资料、辅助创作、多轮搜索、定向搜索等功能，并能够在用户问题的基础上通过一系列更细致的提问来明确用户需求，给出更精准的答案。5月22日，腾讯云对外宣布旗下AI代码助手全面对外开放，开发者、开发团队、企业客户都可通过腾讯云官网进行在线体验。5月23日，天猫精灵在新品品鉴会上推出了天猫精灵X6智能音箱。该产品搭载Genie&nbsp;OS，通过人工智能大模型，为用户提供丰富的服务。</p><p></p><h4>智能体</h4><p></p><p>5&nbsp;月&nbsp;21日，微软宣布推出Team&nbsp;Copilot，将Copilot从个人助手扩展到团队助手。Team&nbsp;Copilot在团队协作中能够扮演任何角色，并扩展出Agent能力，成为不同领域的专家。</p><p></p><h4>端侧AI</h4><p></p><p>5月20日，小鹏汽车在520&nbsp;AI&nbsp;DAY发布会上宣布将向用户全面推送AI天玑系统，该系统推送将覆盖小鹏汽车所有在售车型。5月20日，美的正式发布主动式的全屋智能解决方案，并推出五大智慧场景及悦家全屋智能套系新品，全新升级美的Pro会员体系。5月21日，微软首发Copilot+PC。这是一款专为AI设计的新型Windows&nbsp;PC，将旗下AI助手Copilot全面引入了Windows系统，并且内置了OpenAI的GPT-4o模型。Copilot+PC的新功能Recall能够回忆并查找曾在显示屏上出现过的内容，使用Cocreator能够实时翻译约40种语言，实时生成或优化AI图像。Copilot+&nbsp;PC还配备了AI&nbsp;Agent，具备充分的实时交互能力。5月21日，联想集团推出首款搭载高通骁龙X&nbsp;Elite的下一代Copilot+&nbsp;PC——联想Yoga&nbsp;Slim&nbsp;7x和联想ThinkPad&nbsp;T14s&nbsp;Gen&nbsp;6，产品允许用户即使离线也可以使用大型语言模型功能。</p><p></p><h3>其他</h3><p></p><p>5月18日，CCF联合阿里妈妈正式发布“CCF-阿里妈妈科技袋基金”，致力于面向全球高校学者搭建产学研合作平台，增强学术界和工业界的融合交流，为社会和企业带来世界领先的创新成果。</p><p></p><p>报告推荐</p><p>AGI&nbsp;概念引发热议。那么&nbsp;AGI&nbsp;究竟是什么？技术架构来看又包括哪些？AI&nbsp;Agent&nbsp;如何助力人工智能走向&nbsp;AGI&nbsp;时代？现阶段营销、金融、教育、零售、企服等行业场景下，AGI应用程度如何？有哪些典型应用案例了吗？以上问题的回答尽在《中国AGI市场发展研究报告&nbsp;2024》，欢迎大家扫码关注「AI前线」公众号，回复「AGI」领取。</p><p></p><p><img src="https://static001.geekbang.org/infoq/69/69f5f30dc6564327e46c59d969be2524.jpeg" /></p><p></p><p></p><p>报告预告</p><p>金融行业是否找到了AGI应用的最佳路径？取得了哪些具体应用成果?&nbsp;又存在哪些难以逾越的挑战与桎梏？金融机构一定要做AGI建设吗？如何考量金融AGI应用产品的效果？欢迎大家持续关注InfoQ研究中心即将发布的《AGI在金融领域的应用实践洞察》。</p><p></p><p><img src="https://static001.geekbang.org/infoq/59/593f81e592f22792c23938ef704be173.jpeg" /></p><p></p><p></p><h4>活动推荐</h4><p></p><p>FCon&nbsp;全球金融科技大会将于8月16日正式开幕，本次大会主题为「科技驱动，智启未来——激发数字金融内生力」。如您感兴趣，可点击「阅读原文」查看更多详情。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fd1cae14cf901553aa4a65d29376cc26.png" /></p><p></p><p>咨询其他问题请联系票务同学：13269078023，或扫描上方二维码添加大会福利官，可领取福利资料包。</p><p>阅读原文链接：<a href="https://sourl.co/bpxhuz">https://sourl.co/bpxhuz</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/zKfV26EFh09DPoDTAqo8</id>
            <title>奥特曼突然变身OpenAI “安全卫士”！网友：刚被实锤不关心安全还“心理虐待”，谁信啊</title>
            <link>https://www.infoq.cn/article/zKfV26EFh09DPoDTAqo8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/zKfV26EFh09DPoDTAqo8</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 May 2024 09:39:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI公司, 安全小组, GPT-4, 安全与保障委员会
<br>
<br>
总结: OpenAI公司成立了新的安全小组，旨在开发GPT-4的继任模型，并应对离职雇员的批评。该安全小组被称为安全与保障委员会，由公司高层领导和团队负责人组成，将向董事会提出关键安全建议，影响GPT-4模型的开发。新安全委员会的成立可能是为了回应重要人物的离职，公司内部存在安全文化和流程的变化，引发了外界质疑和讨论。资深专家认为应该先关注可见的风险。 </div>
                        <hr>
                    
                    <p>OpenAI公司已经成立新的安全小组，致力于开发GPT-4的继任模型，同时也是为了应对近期多位离职雇员对其商业意图的严厉批评。</p><p>&nbsp;</p><p>该团队被称为安全与保障委员会（Safety and Security Committee，简称SSC），领导层包括OpenAI公司CEO Sam Altman、委员会主席为Bret Taylor，外加Adam D’Angelo与Nicole Seligman等董事会成员。</p><p>&nbsp;</p><p>其他委员会成员则是来自OpenAI下辖各团队的负责人，包括曾经取代公司联合创始人Ilya Sutskever并担任了13天首席科学家的Jakub Pachocki。</p><p>&nbsp;</p><p>OpenAI公司表示，从现在起，该安全团队将就“关键的安全与保障决策”向董事会提出建议。这些决定可能会影响GPT-4继任模型的开发，即OpenAI在公告中提到的“下一个前沿模型”。</p><p>&nbsp;</p><p>在一个名“OpenAI 董事会成立安全委员会”的公告里，插入这样一条重要信息，着实很容易让人联想OpenAI是不是在借此暗暗转移大众视线，毕竟大家对GPT-5的期待是可以盖过对安全的关注的。</p><p>&nbsp;</p><p>该公司解释称，“我们很自豪能够构建并发布在行业拥有领先能力及安全水平的模型，也同样欢迎在这个重要时刻开展激烈的辩论。”但OpenAI并没有介绍具体讨论内容。</p><p>&nbsp;</p><p>这支安全团队的首要任务，就是在90天时间内制定出可供董事会审议的安全建议，不过Altman及其他董事对于建议内容仍拥有最终决定权。当然，OpenAI&nbsp;CEO及其他四位负责人同样可以在提交董事会之前对建议内容施加影响。</p><p>&nbsp;</p><p></p><h2>得到更多质疑：对谁安全？</h2><p></p><p>&nbsp;</p><p>新安全委员会的成立，很可能是为了回应本月早些时候Sutskever与Jan Leike两位重量级人物的高调离职。随着他们离开OpenAI，公司内负责评估长期AI安全问题的超级对齐小组也宣告解散。</p><p>&nbsp;</p><p>在离职之前，Leike一直担任超级对齐小组的负责人。几乎在OpenAI发布通告的同时，Leike宣布加入了Anthropic。Anthropic 由前 OpenAI 工程师创立，创始人出走就是因为双方安全理念存在差异。Leike 在 Anthropic 依旧负责超级对齐。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/e5/e51e9e8df678a3f9f11e4cafc09c7f6f.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>Leike在超级对齐团队解散的前一天曾经表示，“过去几年以来，安全文化和流程已经让位于公司对快速发布新品的坚持。我们早就应该认真思考通用人工智能（AGI）的影响了……OpenAI必须成为一家以安全为先的AGI厂商。”</p><p>&nbsp;</p><p>该发言还引来马斯克的“补刀”：言外之意就是，安全并不是OpenAI现在的首要任务。</p><p>&nbsp;</p><p>但这个新部门的成立并没有扭转网上OpenAI一直以来的负面安全舆论，反而引来了网友更多质疑。“好吧，我想 OpenAI 的产品现在对于 Sam Altman 和他的目标来说是安全的。”有网友略显无奈地说道。</p><p>&nbsp;</p><p>“利益冲突。这样的安全团队从定义上来说难道不应该是独立的吗？”有人质疑道。对此网友调侃成：“是的，应该有一个治理架构，确保首席执行官遵守以下原则……哎呀，他们已经摧毁整个组织架构了。”</p><p>&nbsp;</p><p>也有网友称：“至少 OpenAI 现在有了一个‘安全’团队。”显然还是觉得OpenAI 有些敷衍。</p><p>&nbsp;</p><p>当然，也有人期待这个安全委员会未来会做出什么成绩，毕竟Altman的信徒大有人在。“我仍然可以让 ChatGPT 告诉我如何制造炸弹。所以，是的，我迫不及待地想看到安全进展。”</p><p>&nbsp;</p><p>对此，行内资深专家告诉“AI 前线”，这更多是公司内部资源分配的问题。OpenAI 一直讲闭源才安全，有人认为AGI要来了、机器要毁灭人类，所以安全太重要了，要赶紧把安全做好，需要投入一定比例的资源进去。但是从一个商业公司的角度看，企业不可能停下工作去做各种安全方面的事情，更多还是要不停开发布新的模型，然后满足客户的需求，跟其他公司竞争。</p><p>&nbsp;</p><p></p><h2>前董事会成员“插刀”</h2><p></p><p>&nbsp;</p><p>同样在今天，OpenAI前董事会成员Helen Toner和Tasha McCauley 的联名文章，再次将Altman不关心安全的问题推上浪尖。</p><p>&nbsp;</p><p>“由于Altman个人长期以来的行为模式，董事会维护公司使命的能力受到了越来越大的限制。据我们了解，这些行为不仅削弱了董事会对关键决策和内部安全协议的监督能力，还引发了其他问题。”</p><p>&nbsp;</p><p>根据爆料，多位高层领导私下向董事会表达了深切的担忧，他们认为Altman营造了一种“撒谎的有毒文化”，并涉嫌“心理虐待”行为。Toner 还表示，Altman“多次”向董事会撒谎，并且“隐瞒信息”，她甚至是在 Twitter 上知道 ChatGPT 发布的消息的。</p><p>&nbsp;</p><p>当董事会意识到 Altman 需要被换掉时，Toner 表示，如果Altman 发现了这个，很明显他会“竭尽全力”阻止董事会反对他。她声称他“开始对其他董事会成员撒谎，试图将我从董事会中赶出去。”</p><p>&nbsp;</p><p>“我们非常小心，非常慎重地选择通知谁，除了我们的法律团队之外，几乎没有任何人提前通知过我们，所以这才把消息拖到了 11 月 17 日。”Toner谈及去年的OpenAI政变时说道。</p><p>&nbsp;</p><p>两人指出，自从Altman重返公司以来，一些发展动态令人担忧，包括他重新加入董事会，以及OpenAI一些专注于安全领域的高级人才的离职。这些情况对于OpenAI在自我治理方面的实验来说，似乎预示着一些不利的影响。</p><p>&nbsp;</p><p>有趣的是，刚刚成为亿万富翁不久的Sam Altman 承诺捐出自己大部分财富，表示将继续专注于“支持有助于为人们创造富足的技术”。</p><p>&nbsp;</p><p></p><h2>资深专家：应该先关注看得见的风险</h2><p></p><p>&nbsp;</p><p>没有什么比Sutskever和Leike等人扮演的重要角色更能表明OpenAI致力于其使命的了。Sutskever和Leike是技术专家，他们长期致力于安全，并明显真诚地愿意在必要时要求OpenAI改变方向。</p><p>&nbsp;</p><p>Sutskever 在2019年的采访中当记者刚刚说道，“你们说，‘我们要建立一个通用人工智能，’”时，Sutskever 立即插话强调：“我们将尽一切可能朝这个方向努力，同时确保以一种安全的方式做到这一点。”</p><p>&nbsp;</p><p>随着他们的离职，很多人问他们在OpenAI看到了什么，但没有得到答案。</p><p>&nbsp;</p><p>不同于 Sutskever、Leike等人坚决捍卫AI安全的态度，有些大佬并不那么重视，比如图灵奖得主Yann LeCun。</p><p>&nbsp;</p><p>当时，在LeCun在对Jan Leike的回贴中，他表示当前对AI安全担忧的紧迫感是过度夸张的，类似于在涡轮喷气发动机发明之前，急于解决跨洋飞行的安全问题。所以难怪OpenAI 解散对齐团队。在LeCun看来，智能系统的进化需要多年时间，应该通过反复的工程改进逐步提高其智能和安全性，而不是过度担忧未来可能的风险。</p><p>&nbsp;</p><p>同时，上述专家也告诉“AI 前线”，从开源角度讲，我们离“AGI 来了、毁灭人类”这些还很远，他并不认可这些说法。</p><p>&nbsp;</p><p>该专家表示，目前，AI 安全上的风险更多来自大家看得见、摸得着的地方，比如数据集的偏见和毒化给使用模型带来很多挑战：让ChatGPT 画一个剥了皮的荔枝，由于ChatGPT 根本不知道荔枝剥皮了什么样，所以它就是随便画；又如让Stable Diffusion 等海外模型画北京城市，它会画一个破破烂烂的四合院。</p><p>&nbsp;</p><p>“目前，像这种数据集的 bias 其实没有得到很多关注。但这种可能是更重要的，与超级对齐不是一回事儿，”该专家说道。</p><p>&nbsp;</p><p>该专家也分析称，从更大层面来说，美国也在渲染 AI 安全问题，比如AI自动生成恶意软件、自动攻击各种网站，但实际上我们都知道，代码生成的能力远远没有到这种程度，所以这种渲染也是为了防止模型出口，不让非常厉害的模型技术扩散出去。美国炒作这个事情，也有想要得到中国类似“不利用这个技术开发武器”承诺的意图。</p><p>&nbsp;</p><p>“安全是一个非常大的叙事，每个人在不同的立场都会有不同的看法。”当前应该把主要精力投入到哪个方面？显然OpenAI当前掌门人有自己的答案，其他公司也有自己的答案。但答案正确与否，还需要时间验证。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href="https://openai.com/index/openai-board-forms-safety-and-security-committee/">https://openai.com/index/openai-board-forms-safety-and-security-committee/</a>"</p><p><a href="https://www.theregister.com/2024/05/28/openai_establishes_new_safety_group/">https://www.theregister.com/2024/05/28/openai_establishes_new_safety_group/</a>"</p><p><a href="https://www.businessinsider.com/openai-board-member-details-sam-altman-lied-allegation-ousted-2024-5">https://www.businessinsider.com/openai-board-member-details-sam-altman-lied-allegation-ousted-2024-5</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0CF4oJYiP21jXOVjvkeo</id>
            <title>如何降低数据消费门槛，让非技术用户也能成为数据分析专家？</title>
            <link>https://www.infoq.cn/article/0CF4oJYiP21jXOVjvkeo</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0CF4oJYiP21jXOVjvkeo</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 May 2024 09:05:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据分析, 企业发展, 大模型, SwiftAgent
<br>
<br>
总结: 在数字化时代，数据分析已成为企业发展的重要驱动力。企业需要克服数据获取困难和高阶分析难等挑战，形成基于数据的决策文化，打破数据孤岛，促进跨部门协作。SwiftAgent 的出现实现了数据分析的民主化，让人人都能成为数据分析专家，提高了数据分析的效率和价值。 </div>
                        <hr>
                    
                    <p>在数字化时代，数据分析已成为企业发展的重要驱动力。对于企业而言，数据价值毋庸置疑，更重要的是如何对其进行分析和利用。从客户运营到战略决策，各行各业都离不开数据分析。本期《极客有约》栏目邀请到了数势科技数据智能产品总经理岑润哲，与他一起探讨大模型时代下，如何以 SwiftAgent 革新企业数据分析范式，让人人都能成为数据分析专家。</p><p></p><p>数势科技是行业领先的数据智能产品提供商，为全球大金融、泛零售和高科技制造企业提供大模型增强的智能产品体系，促进企业数字化转型。</p><p></p><p>如何让企业的非技术人员跨越数据的门槛，成为数据分析专家？数据分析的现状和未来发展趋势又如何？SwiftAgent 的出现如何帮助非技术用户？它的技术特点和优势是什么？精彩观点总结如下：</p><p>企业在进行数据分析时面临的主要挑战包括数据获取困难、高阶分析难等，企业数据分析的效率和准确性极具挑战。非技术人员应从业务需求出发，逐步引入数据分析工具和理论，并通过实际操作提升分析思路。企业从组织层面进行变革，形成基于数据的决策文化，打破数据孤岛，并促进跨部门协作。SwiftAgent 通过引入指标语义层和大模型能力，实现了数据分析的民主化，让不同岗位的人员都能实现实时、灵活、精确的数据分析，缩短从数据获取到决策的链路，提高了数据分析的效率和价值。</p><p></p><p>完整视频参看：</p><p></p><p></p><p></p><p></p><h2>企业数据分析的机遇与挑战并存之道</h2><p></p><p></p><p>&nbsp;InfoQ：润哲老师，您在数据分析领域有着非常丰富的经验，您认为企业用户在进行数据分析的时候，通常会遇到哪些困难和挑战？这些挑战对于企业的发展有哪些影响？</p><p></p><p>岑润哲：在我们服务的企业客户中，数据分析一般分为四个步骤。首先是数据收集与获取（Data Query）。企业用户、包括分析师和业务人员在分析前需从数据仓库或业务系统库中提取数据，由于他们通常缺乏数据分析技能，不熟悉 SQL 或底层数据表，这在提取中造成了最大的不便，这是许多客户的痛点。</p><p></p><p>数据分析师或商业分析师具备一定的数据分析技能，会编写 SQL，但底层数据仓库的表结构混乱，熟悉的表有限。当他们完成一个领域的指标分析后，转向另一领域时，需重新梳理表逻辑，这是数据获取阶段的一个难点。</p><p></p><p>第二步是数据获取后，使用专业分析工具或方法进行高阶分析（Data Analytics）。难点不在于选择工具，而在于根据不同场景选用合适的分析范式。例如，销量分析时可能需要同环比分析、排序分析；指标异常时需归因分析能力；转化率异常时需漏斗分析能力。工具学习不难，难的是找到适合业务板块的分析范式。</p><p></p><p>第三步是数据获取和高阶分析后，如何快速解读数据。传统 BI 工具难以直接将数据转化为洞察（insight）。我们的产品结合大模型的语义理解能力，可快速挖掘商业洞察。例如，从几千行 Excel 数据中迅速识别产品或客户群的问题。我们希望结合指标语义层和大模型辅助，优化数据洞察和解读，提供业务方所需的分析解读。</p><p></p><p>最后是数据权限和安全性问题。 若数据权限开放给所有分析师或业务人员，可能引发数据安全性和隐私问题。在数据复盘过程中，需确保不同角色和用户能获取适当权限的数据集。</p><p>总体来说，数据获取、高阶分析、数据解读理解、数据权限管控，是不同行业客户在数据分析中面临的主要问题。</p><p></p><p>&nbsp;InfoQ：这些非技术用户在数据分析的过程中，他们最大的困惑是什么呢？</p><p></p><p>岑润哲：对于非技术人员，我们将其定义为偏业务人员，他们擅长业务流程和合规性，但在将业务思维与底层数据表关联时存在难题。以零售行业为例，非技术人员如门店店长或督导需要分析经营数据、客户画像和商品销售数据，但若直接提供数据表，他们难以进行分析。</p><p></p><p>非技术人员不仅需要工具，更需要将数据分析与业务场景结合。我们认为可以通过指标语义层和大模型的生成能力，帮助客户提出更精准的问题。 例如，构建好指标体系后，大模型能生成结构化问题，如从经营视角分析门店流水和毛利，或从服务水平视角分析大众点评评分，以及客群画像等。</p><p></p><p>当企业内部的指标和维度体系建立完善，结合大模型，能够输出标准的分析思路。这与传统 BI 工具相比，是一个双向过程：大模型不仅能响应问题，还能提供分析思路。比如，大模型可以根据公司的指标和维度生成批量问题，用户再从中筛选最关心的问题进行分析。</p><p></p><p>将大模型分析助手与用户请求结合后，我们从单向的 BI 分析模式转变为可交互模式，用户可以向大模型提问，大模型也能反问用户，提供分析视角。这种模式优化了非技术人员在分析思路上的痛点，大模型的出现增强了分析思路扩展的能力。</p><p></p><p>大模型不仅提供分析思路，还能激发用户的分析欲望，形成良好的交互形式，相当于由顾问提供建议。这是大模型在智能分析领域带来的最大改变。</p><p></p><p>&nbsp;InfoQ：您认为企业在数据分析的过程中还有可能会遇到哪些管理上或者流程上的障碍或者挑战吗？</p><p>岑润哲：结合我们服务过的客户，我总结了三种主要障碍。</p><p></p><p>首先是组织文化的障碍。许多组织尚未形成基于数据的决策文化，决策更多由高层领导凭经验作出，这会影响分析工作的价值。如果组织文化不以数据驱动决策，即使分析质量再高也难以发挥作用。</p><p></p><p>第二是企业内部数据体系的孤岛问题。例如，在泛零售行业，线上线下渠道的数据可能未打通，或埋点数据与交易数据之间存在隔离。这导致无法进行跨部门或跨领域的分析，如无法评估营销活动的效果。</p><p></p><p>第三是跨部门协作问题。不同部门之间可能存在边界和利益问题，例如活动运营部门需要客户运营部门的数据时，可能难以获得必要的支持。这种跨部门协作的障碍，使得进行复杂的分析或关联分析变得困难。</p><p>&nbsp;InfoQ：数势科技就作为数据智能产品的提供方，您认为帮助企业解决这些问题的核心思路是什么呢？企业应该重点关注哪些方面？</p><p></p><p>岑润哲：我们公司的核心理念和使命是更新现行的数据分析范式，从集中式转变为民主化。目前，企业的分析逻辑多是粗犷或集中式的，业务方需向数据团队提出需求，然后等待数据提供。这种模式下，数据解读和高阶分析强烈依赖商业分析师或数据分析师团队，存在较大的隔阂。</p><p></p><p>引入指标语义层和大模型能力后，我们希望企业内部的每个员工能够成为“数据公民”，这意味着他们即使不懂数据，也能基于业务分析思路，获取和探索企业内部的数据资产。大模型的出现有助于每个数据公民进行大规模自定义数据分析，极大缩短从数据获取到决策的链路。</p><p></p><p>未来数据分析的主要方向是从研发与业务割裂的形式，转变为业务方在研发设定标准后，自行利用大模型辅助获取和挖掘数据。</p><p></p><p>&nbsp;InfoQ：您刚刚提到数据公民的概念非常有趣，它会对决策层的思维导向和人才培养产生积极影响吗？</p><p></p><p>岑润哲：是的。例如我们曾为一家鞋类企业提供了 基于指标语义层的完整分析框架， 他们可以分析不同客群的偏好。通过 Know Your Customer 标签，发现 25 至 29 岁女性对 PVC 材质、鞋跟高度在 3 到 5 厘米的鞋子有很高的偏好。这些信息在之前是无法获取的，因为他们不知道公司内部有这些客群和商品标签。</p><p></p><p>现在，借助大模型工具，业务方可以提出更有针对性的问题，并驱动分析过程。他们更了解产品的销售情况，能够通过数据分析找出哪些客群对特定类型或特征的鞋子有更高的转化率，进而讨论投放策略或营销策略，形成一个正循环。</p><p></p><p>传统仅从技术角度分析数据表可能无法获得这样的洞察。但现在，业务方有能力自行分析不同的标签和指标，这使他们能够更好地理解企业内部如何提升销售，实现业务驱动的数据分析和决策。</p><p></p><p>&nbsp;InfoQ：前不久数势科技在 AICon 大会现场发布了 SwiftAgent 2.0 版本，是否可以现场演示？</p><p></p><p>岑润哲：好的，以下是产品的 demo 视频，供大家了解，可留言或点击“阅读原文”申请产品试用。</p><p></p><p></p><p></p><p>InfoQ：在现场发布的时候，数势科技也提到了大模型和 Agent 将会颠覆企业数据分析与决策范式，我想请问为什么这样来表达呢？</p><p></p><p>岑润哲：我们可以回顾一下大模型和 Agent 架构出现之前的数据分析流程。传统上，数据分析链路较长，从提出需求到数据团队获取数据集、配置 BI 工具并搭建驾驶舱，整个过程耗时且复杂。核心问题在于，所有工具的使用都需要人工配置和梳理，工作量较大且重复，效率较为低下。</p><p></p><p>Agent 架构结合大模型后，展现出其优势，尤其是 Agent 在工具调用方面的能力。Agent 不仅能理解用户的自然语言需求，还能自动规划任务执行步骤。</p><p></p><p>例如，用户提出数据分析请求，Agent 可理解用户意图、获取所需地区的销售明细、进行排序和高阶分析、对比 TOP3 产品。这不仅涉及任务拆解和规划，还包括与知识库的协同和工具串联。</p><p></p><p>这种架构带来四个好处：首先，用户不再需要学习工具配置，因为大模型已经掌握了工具调用的方法；其次，通过 Agent 统一规划，提高了效率，避免了在不同工具间切换的繁琐；第三，交互性得到改善，用户通过自然语言与系统交互，降低了使用门槛；最后，简化了操作，将复杂逻辑留给程序处理。</p><p></p><p>企业可通过 Agent 机制调度内部不同工具，形成有效串联，降低了业务方学习和使用工具的时间与门槛，这是我认为它会颠覆企业分析决策范式的原因。</p><p></p><p></p><h2>SwiftAgent 开启智能数据分析新篇章</h2><p></p><p></p><p>InfoQ：SwiftAgent 为什么能够在众多的大模型和数据分析产品中脱颖而出？</p><p></p><p>岑润哲：SwiftAgent 被定义为由大模型 Agent 机制驱动，并结合指标标签语义层的智能分析产品。它让企业非技术人员——如企业管理者和业务人员可准确、即时、个性化地进行数据査询和业务洞察，提升决策能力，实现数据价值普惠化。</p><p></p><p>其核心技术亮点主要分为三个层面：通过构建指标标签语义层，统一了数据和业务语言，避免了大模型的幻觉；结合 Agent 架构，赋予产品反思、推理和规划的能力；通过自研的加速引擎，提升了前端问询的响应速度。</p><p></p><p>指标标签语义层： 我们采取的技术路线不是直接将用户自然语言请求转化为 SQL。因为企业内部数据标注和治理程度不一，直接转化的准确率很低。我们构建了指标和标签语义管理层，统一了数据语言和业务语言，解决了大模型的幻觉问题，提高了准确率，并帮助企业建立了一套指标和标签体系，解决了数据统一问题。</p><p></p><p>Agent 产品架构设计：Agent 架构能够进行思考、推理和反思，解决复杂任务执行问题。自然语言形式的灵活性让用户可能提出不可预测的问题，我们设计了合理的 Agent 架构，使用户能够以自然语言形式灵活、高效地获取数据。我们在 Agent 架构层面做了大量的调研和研发，提升处理复杂问题的能力。</p><p></p><p>数据加速引擎： 我们自研了 Hyper Computing Acceleration Engine，提升对话式分析的响应速度。例如，针对用户常问的商品品类、城市等维度的销售额或毛利，进行预聚合和预计算，使得即使面对百亿级数据量的订单表，也能快速响应用户查询。</p><p></p><p>&nbsp;InfoQ：SwiftAgent 的产品优势 / 壁垒是什么？</p><p></p><p>岑润哲：除了上述提到的 Agent 机制和数据加速引擎，SwiftAgent 还拥有结构化与非结构化数据联动分析的能力。我们将非结构化信息（如用户评论、直播数据）抽象化，转化为结构化数据，并与企业内部指标进行关联分析，提供更全面的分析。</p><p></p><p></p><h2>智能数据分析市场的发展前景</h2><p></p><p></p><p>InfoQ：除了零售行业以外，还有哪些行业已经上线 SwiftAgent？成效如何? 可以分享几个案例吗？</p><p></p><p>岑润哲：除了零售行业，我们也在金融行业如银行、证券公司，以及高端制造行业实现了应用落地。</p><p>以某知名城商为例，分行行长通常关注贷款余额、不良率等指标的波动。传统上，他们需要向分析团队提出需求，由团队提供分析结果。现在，通过上线智能分析产品，领导可以直接通过自然语言查询获取信息，同时经营分析团队可以利用沉淀的分析模板和思路，加速从数据到分析报告的转化。</p><p></p><p>我们也与头部证券机构合作，帮助客户经理分析他们管理的高净值客户。例如，理财顾问或投资经理管理 200 个客户时，可以通过自然语言查询，快速了解哪些客户存在流失风险，或关注行业政策变动，以及持仓标的的变化。这样的分析能力，如果依靠传统 CRM 工具，可能需要花费大量时间。而通过 SwiftAgent 与客户标签、指标联动，构建了从数据洞察到决策的完整链路，效率提升 80%。此外，我们还将优秀客户经理的 SOP 沉淀在知识库中，帮助新员工快速了解如何应对不同情况，比如客户亏损时的安抚策略。这样，数据分析不仅帮助业务人员理解数据，还指导他们基于数据采取行动。</p><p></p><p>&nbsp;InfoQ：对于金融和央国企而言，信创和数据安全是重点关注的方向。数势科技在这两方面有些认证或适配？</p><p></p><p>岑润哲：数势科技是北京信创工委会会员单位，已经完成国家高新技术企业认证、中关村高新技术企业认证、ISO9000 质量管理体系认证、信息安全管理体系认证、信息技术服务管理体系认证、信息系统安全登记保护三级、麒麟操作系统信创认证、达梦数据库信创认证、人大金仓信创认证和 CMMI 等资质认证。产品充分满足金融企业和国央企的部署需求。目前合作的国产大模型都已完成算法备案。另外，SwiftAgent 已首批通过中国信通院针对大模型驱动的数据分析工具的专项测试，获得权威认可。</p><p></p><p>&nbsp;InfoQ：在您看来未来智能数据分析市场规模是将会是怎么样的？</p><p></p><p>岑润哲：智能数据分析市场将是大模型落地的重要场景。数据分析智能化能够充分利用大模型的规划和拆解能力，并与企业内部数据联动，产生化学反应。企业不仅希望提升数据分析体验，还希望降低开发需求，将数据智能化作为核心战略。</p><p></p><p>在客户需求层面，大金融、泛零售和高科技制造是我们的重点服务领域。这些行业的企业对大模型的应用已经从观望学习阶段过渡到试点实施阶段。许多头部企业，尤其是金融、国央企、零售和能源企业，已经开始大量招标，希望在数据分析、知识库、营销和 RPA 等多个场景中应用大模型。数据分析场景特别受到重视，占企业需求的 80% 以上。</p><p></p><p>另外，国家层面也在推动数据资产入表，鼓励企业将数据资产作为无形资产量化并反映在财务报表中，这也将促进大模型在数据管理和分析领域的结合。</p><p></p><p>&nbsp;InfoQ：您作为资深专家，请问对于想要提高数据分析能力的非技术用户，有哪些建议呢？</p><p></p><p>岑润哲：首先，非技术用户应以业务需求为出发点，学习基础统计学和数据分析概念，构建分析能力的基础。例如，零售客户，可从业务场景切入，如教店长如何分析门店数据以提升业绩。</p><p></p><p>其次，非技术人员应先理解自身管理的业务逻辑流程，再逐步引入数据分析工具和理论。 建议通过实际操作小项目来提升分析思路，如门店经理、财务经理或 HR 可以分析与自己工作相关的数据。</p><p></p><p>最后，还可以加入专业论坛或群体，关注不同领域的分析博客，以帮助构建人脉并提升对数据的理解力。 业务人员转为分析师往往潜力巨大，因为他们对业务流程有深刻理解，具备在数据分析和业务洞察领域的天然优势。</p><p></p><p>数据分析不仅限于互联网公司或运营、财务风控等领域，数据分析将持续渗透到企业各个部门，提升决策效率，这是未来的大趋势，也是我们数势科技的愿景和使命。</p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/v0Y1tppWvfGPvtpMqgTY</id>
            <title>干货下载 | 腾讯云ES RAG如何支持微信读书实现“AI问书”？</title>
            <link>https://www.infoq.cn/article/v0Y1tppWvfGPvtpMqgTY</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/v0Y1tppWvfGPvtpMqgTY</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 May 2024 06:19:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/95/95248354db49afa8d5efac3f71431c10.webp" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/e8/e8076563d8941632e9a8f79eed8fcc96.webp" /></p><p></p><p>干货下载页面点击：<a href="https://qdrl.qq.com/TJBspHYw">https://qdrl.qq.com/TJBspHYw</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/fXOqLr5i5lGx8mqJacPf</id>
            <title>浪潮信息发布 “源2.0-M32” 开源大模型，大幅提升模算效率</title>
            <link>https://www.infoq.cn/article/fXOqLr5i5lGx8mqJacPf</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/fXOqLr5i5lGx8mqJacPf</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 May 2024 02:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 源2.0-M32, 门控网络, 专家模型, 模算效率
<br>
<br>
总结: 源2.0-M32是一个包含32个专家的混合专家模型，采用了门控网络结构来调度专家，实现高效计算。在模型训练和推理过程中，源2.0-M32表现出色，主要通过建模专家之间的协同关系来提升模型精度和模算效率。 </div>
                        <hr>
                    
                    <p>5月28日，浪潮信息发布“源2.0-M32”开源大模型。“源2.0-M32”在基于“源2.0”系列大模型已有工作基础上，创新性地提出和采用了“基于注意力机制的门控网络”技术，构建包含32个专家（Expert）的混合专家模型（MoE），并大幅提升了模型算力效率，模型运行时激活参数为37亿，在业界主流基准评测中性能全面对标700亿参数的LLaMA3开源大模型。</p><p></p><h3>大模型技术解读</h3><p></p><p></p><p>在算法层面，源2.0-M32提出并采用了一种新型的算法结构：基于注意力机制的门控网络（Attention Router），针对MoE模型核心的专家调度策略，这种新的算法结构关注专家模型之间的协同性度量，有效解决传统门控网络下，选择两个或多个专家参与计算时关联性缺失的问题，使得专家之间协同处理数据的水平大为提升。源2.0-M32采用源2.0-2B为基础模型设计，沿用并融合局部过滤增强的注意力机制（LFA, Localized Filtering-based Attention），通过先学习相邻词之间的关联性，然后再计算全局关联性的方法，能够更好地学习到自然语言的局部和全局的语言特征，对于自然语言的关联语义理解更准确，进而提升了模型精度。</p><p></p><p><img src="https://static001.geekbang.org/infoq/77/7749b68f0ae25cc003fe38680e899857.png" /></p><p>Figure1-&nbsp;基于注意力机制的门控网络（Attention Router）</p><p></p><p>在数据层面，源2.0-M32基于2万亿的token进行训练、覆盖万亿量级的代码、中英文书籍、百科、论文及合成数据。大幅扩展代码数据占比至47.5%，从6类最流行的代码扩充至619类，并通过对代码中英文注释的翻译，将中文代码数据量增大至1800亿token。结合高效的数据清洗流程，满足大模型训练“丰富性、全面性、高质量”的数据集需求。基于这些数据的整合和扩展，源2.0-M32在代码生成、代码理解、代码推理、数学求解等方面有着出色的表现。</p><p></p><p>在算力层面，源2.0-M32采用了流水并行的方法，综合运用流水线并行+数据并行的策略，显著降低了大模型对芯片间P2P带宽的需求，为硬件差异较大训练环境提供了一种高性能的训练方法。针对MoE模型的稀疏专家计算，采用合并矩阵乘法的方法，模算效率得到大幅提升。</p><p></p><p>基于在算法、数据和算力方面全面创新，源2.0-M32的性能得以大幅提升，在多个业界主流的评测任务中，展示出了较为先进的能力表现，在MATH（数学竞赛）、ARC-C（科学推理）榜单上超越了拥有700亿参数的LLaMA3大模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/18/18c12d432b7f6f84dcb3fc48081ae17a.png" /></p><p>Figure2 源2.0-M32业界主流评测任务表现</p><p></p><p>源2.0-M32大幅提升了模型算力效率，在实现与业界领先开源大模型性能相当的同时，显著降低了在模型训练、微调和推理所需的算力开销。在模型推理运行阶段，M32处理每token所需算力为7.4GFLOPs，而LLaMA3-70B所需算力为140GFLOPs。在模型微调训练阶段，对1万条平均长度为1024 token的样本进行全量微调，M32消耗算力约0.0026PD(PetaFLOPs/s-day)，而LLaMA3消耗算力约为0.05PD。M32凭借特别优化设计的模型架构，在仅激活37亿参数的情况下，取得了和700亿参数LLaMA3相当的性能水平，而所消耗算力仅为LLaMA3的1/19，从而实现了更高的模算效率。</p><p></p><p>浪潮信息人工智能首席科学家吴韶华表示：当前业界大模型在性能不断提升的同时，也面临着所消耗算力大幅攀升的问题，对企业落地应用大模型带来了极大的困难和挑战。源2.0-M32是浪潮信息在大模型领域持续耕耘的最新探索成果，通过在算法、数据、算力等方面的全面创新，M32不仅可以提供与业界领先开源大模型相当的性能，更可以大幅降低大模型所需算力消耗。大幅提升的模算效率将为企业开发应用生成式AI提供模型高性能、算力低门槛的高效路径。</p><p></p><h3>技术创新点剖析：</h3><p></p><p></p><p>Llama系列模型的精度从Llama1到Llama3显著提升，Llama3的精度处于领先地位，特别是其700亿参数的模型在每个Token的推理和算力上达到140GFLOPS。尽管如此，Llama3在推理时的算力开销较大，也就是说单位算力下的精度表现较差。</p><p></p><p>在采访环节，吴韶华回答了记者问，关于32个专家的优势及挑战，吴韶华解释道，当前很多大模型工作采用8个专家的架构，但浪潮信息选择32个专家，核心原因是模算效率。实验表明，在他们的LFA加上Attention Router架构中，专家数量从8增加到32时，精度显著提升，而算力开销保持不变。这是因为激活专家的数量仅为2个。此外，单个专家参数量为2B，这样控制模型参数量有利于企业应用的模算效率。结果显示，这一选择在相同精度下实现了低算力消耗。</p><p></p><p>同时，由于激活的专家数量为2个，通过Attention Router机制考虑专家间协同，专家数量的增加使得每个专家或专家组能够学习更多有针对性的信息。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/20/9d/20109819dd1cb3e6794a287d3648e49d.jpg" /></p><p></p><p>模算效率与成本控制也是此次大模型发布的关键讨论点。吴韶华强调，算力是当前大模型发展的核心瓶颈。MoE结构模型通过扩展专家数量，在固定算力下获得更高精度。在多元芯片的使用上，浪潮信息的EPAI软件提供相关工具，支持多元算力架构，降低用户迁移设备的难度和成本。这些创新措施有助于降低用户试错成本，实现应用落地。</p><p></p><p>高模算效率意味着在单位算力投入下获得更高的精度回报，这对于大模型训练和推理都非常有利。“源2.0-M32”模型旨在通过创新算法提升精度并降低同等精度下的算力开销，大幅提升基础模型的模算效率。“源2.0-M32”是一个包含32个专家的混合专家模型，采用了Attention Router结构来调度专家，实现高效计算。在模型训练和推理过程中，“源2.0-M32”表现出色，Attention Router结构主要是通过建模专家之间的协同关系来提升模型精度。</p><p></p><p>M32模型的训练数据筛选与优化也是核心技术点，吴韶华详细介绍了浪潮信息在训练数据方面的策略。浪潮信息从源1.0开始构建了互联网自然语言文库，并开发了一套数据清洗平台。对于稀缺数据（如中文数学数据），通过数据合成工具补充。M32模型引入了大量代码数据和互联网数据，提升数据的多样性和质量。代码数据不仅对模型的代码能力有益，还能帮助解决数学问题和推理问题。最终，源2.0-M32模型在精度和算力开销方面优于Llama3。</p><p></p><p>在应用落地方面，源2.0-M32增强了小样本学习能力，通过少量样本就能显著提升模型能力。相较于微调而言，这是一种轻量化支撑大模型应用落地的有效技术。</p><p></p><p>MoE模型对企业开发应用和大模型普惠的影响也逐渐展现，吴韶华向大家介绍说，MoE模型除了提升算力效率外，还能提高精度，降低使用成本，增强模型能力。MoE模型通过激活少量专家，保持算力开销低，同时允许训练更多Token，进一步提升精度。对于终端用户来说，关键在于解决实际问题和降低使用成本。例如，在智能客服等应用中，用户更愿意花费较少的钱解决具体问题，而不会购买高成本的大模型。</p><p></p><p>最后，吴韶华补充了大模型落地与微调的观点，大模型在应用落地时需要进行微调，这是由于预训练阶段的数据和模型能力存在局限性。微调能有效应对不同的行业需求，但算力需求较大。同时，推理阶段也是算力开销大户，因此高效的模型结构和更强的能力在实际应用中具有优势。浪潮信息通过内部实际应用场景，如客服、软件研发、运维等，不断积累经验，提升模型能力，满足更多用户需求。</p><p></p><h3>回顾与展望：</h3><p></p><p></p><p>回顾大模型的发展历史，我们可以看到，2020年GPT-3的发布点燃了大模型的热潮。从2020年到2022年，业界在大模型能力上进行了广泛的探索。例如，2022年推出了GPT强化学习方法，使大模型与人的意图对齐，建立了良好的发展思路。同年末，ChatGPT问世，引发了大模型应用的热潮，成为增长最快且被广泛接受的大模型应用。此后，Llama系列模型陆续推出，2024年大模型的发展速度进一步加快。</p><p></p><p>浪潮信息的大模型研究始于2020年GPT-3发布后。2021年，他们发布了第一个大模型“源1.0”，拥有2457亿参数。2022年，进行了应用落地探索，运用了检索类技术和RAG技术。2023年，发布了“源2.0”，并推出了“源2.0-M32”混合专家结构模型。</p><p></p><p>关于大模型推广及触达用户，吴韶华介绍了浪潮信息大模型落地的两个方向：外部客户和内部需求。对外，浪潮信息通过与合作伙伴在EPAI平台上合作，提供开源模型支持，增强用户体验。对内，浪潮信息在多个业务场景中应用大模型，解决内部需求问题的同时积累经验，提升算法和工具性能，从而更好地服务外部客户。</p><p></p><p>未来，M32开源大模型配合企业大模型开发平台EPAI（Enterprise Platform of AI），将助力企业实现更快的技术迭代与高效的应用落地，为人工智能产业的发展提供坚实的底座和成长的土壤，加速产业智能化进程。</p><p></p><p>最后，吴韶华宣布，浪潮信息已在GitHub和Hugging Face上开源了代码和模型，并发表了相关论文。</p><p></p><p>源2.0-M32将持续采用全面开源策略，全系列模型参数和代码均可免费下载使用。</p><p>代码开源链接：<a href="https://github.com/IEIT-Yuan/Yuan2.0-M32">https://github.com/IEIT-Yuan/Yuan2.0-M32</a>"</p><p>模型下载链接：<a href="https://huggingface.co/IEITYuan/Yuan2-M32-hf">https://huggingface.co/IEITYuan/Yuan2-M32-hf</a>"</p><p><a href="https://modelscope.cn/models/YuanLLM/Yuan2-M32-hf/summary">https://modelscope.cn/models/YuanLLM/Yuan2-M32-hf/summary</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/205tMHA6eOSsVyS7jOla</id>
            <title>别再危言耸听！大多数被评为“严重”的Bug评级具有误导性</title>
            <link>https://www.infoq.cn/article/205tMHA6eOSsVyS7jOla</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/205tMHA6eOSsVyS7jOla</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 May 2024 13:39:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: CVSS评级, 安全团队, 软件供应链, AI/ML工具
<br>
<br>
总结: JFrog发布的调查结果显示，大多数CVSS评级在实际情况下并不适用，但安全团队仍花费大量时间修复漏洞。报告指出安全问题会影响工作效率，同时揭示了软件供应链安全和AI/ML工具在安全领域的应用不成比例。JFrog提供的安全方案聚焦于统一平台管理，为企业提供了高性价比的解决方案。 </div>
                        <hr>
                    
                    <p></p><blockquote>74%被列为“高”或“严重”的CVSS评级在大多数常见情况下并不适用，但有60%的安全和开发团队仍花费25%的时间修复这些漏洞。</blockquote><p></p><p>&nbsp;</p><p>近日，流式软件公司、JFrog软件供应链平台背后的公司JFrog&nbsp;发布了其 《2024年全球软件供应链发展报告》的调查结果，指出了新兴的发展趋势、行业风险以及保障企业软件供应链安全的最佳实践案例。</p><p>&nbsp;</p><p>JFrog首席技术官兼联合创始人Yoav Landman表示：“软件安全领域变幻莫测，全球的DevSecOps团队都在探索前行，在AI迅速普及的时代，更需要创新来满足需求。我们的数据涵盖了迅速发展的软件生态系统，为安全和开发组织提供了一个更为全面的介绍，包括值得关注的CVE评级错误、使用生成式AI进行编码所带来的安全影响相关洞察、允许组织用于开发的高风险软件包等信息，以便相关人员做出更明智的决策。”</p><p>&nbsp;</p><p>JFrog的《2024年全球软件供应链发展报告》结合了超过7000家企业的JFrog Artifactory开发者使用数据、JFrog安全研究团队原创的CVE分析、以及委托第三方对全球1200名技术专业人士进行的调查数据，旨在为快速发展的软件供应链领域提供信息参考。主要研究结果包括：</p><p>&nbsp;</p><p>并非所有CVE都如表面所见：传统的CVSS评级仅关注漏洞利用的严重性，而非其被利用的可能性，后者需要结合具体情境才能做出有效的评估。JFrog安全研究团队在分析了2023年发现的212个高知名度CVE后，平均将85%的“严重”CVE和73%的“高危”CVE的重要性评级下调。此外，JFrog发现，在报告的前100个Docker Hub社区镜像中，74%的CVSS评级为“高危”和“严重”的常见CVE实际上是无法被利用的。</p><p>&nbsp;</p><p>拒绝服务（DoS）攻击盛行：JFrog安全研究团队分析的212个高知名度CVE中，有44%存在发起DoS攻击的潜在威胁；17%存在执行远程代码（RCE）的潜在威胁。这对于安全组织来说是个好消息，因为RCE由于能够提供对后端系统的完全访问权限，与DoS攻击相比，其危害性更大。</p><p>&nbsp;</p><p>安全问题会影响工作效率：40%的受访者表示，通常需要一周或更长时间才能获得使用新软件包/库的批准，这延长了新应用程序和软件更新的上市时间。此外，安全团队大约耗费25%的时间用于修复漏洞，即使这些漏洞的风险在当前情况下可能被高估或甚至无法被利用。</p><p>&nbsp;</p><p>在软件开发生命周期（SLDC）中采用安全检查方式的差异性&nbsp;——当涉及到决定在软件开发生命周期中的哪个阶段采取应用安全测试时，行业内存在明显分歧，这突显了同时进行左移和右移的重要性。42%的开发人员表示，最好在编写代码过程中执行安全扫描，而41%的开发人员认为最好在新软件包从开源软件（OSS）库引入企业之前执行扫描。</p><p>&nbsp;</p><p>安全工具的过度使用现象仍在持续&nbsp;——&nbsp;近半数IT专业人士（47%）表示他们部署了四到九种应用安全解决方案。然而，有三分之一的调查对象和安全专业人士（33%）表示，他们正在使用十种乃至更多的应用安全解决方案。这一现象反映出市场对于安全工具整合的需求趋势，同时也表明人们正逐渐放弃单一的点对点解决方案，转而寻求综合性更高的安全工具集成。</p><p>&nbsp;</p><p>AI&nbsp;/&nbsp;ML工具在安全领域的应用不成比例&nbsp;——尽管有90%的受访者表示，他们的企业目前以某种形式使用AI&nbsp;/&nbsp;ML驱动的工具来协助安全扫描和修复工作，但只有三分之一的专业人士（32%）表示他们的组织使用AI&nbsp;/&nbsp;ML工具来编写代码。这反映出业内大多数人对AI生成的代码可能会为企业软件带来的潜在安全隐患仍持审慎态度。</p><p>&nbsp;</p><p>尽管新发布的报告揭示了被列为“高”或“严重”的CVSS评级在大多数常见情况下并不适用，但企业对于软件供应链的安全意识，一刻也不应放松。以JFrog为例，其提供的安全方案聚焦于以统一的平台去实现管理，且不限制用户数，顺应了很多企业的需求。同时，作为JFrog的一大产品特点，JFrog Xray和制品库是进行统一绑定的，即企业使用了JFrog的制品库，就无需额外购买JFrog Xray，会自动获得安全扫描的能力。这进一步帮助企业的安全团队减少了工具安全扫描维护和采购的成本对的同时，还能够帮助企业在安全扫描、制品管理、供应链管理上提供统一的高性价比解决方案。&nbsp;</p><p></p><p>JFrog安全研究高级总监Shachar Menashe表示：“虽然安全漏洞的数量每年都在增加，但这并不意味着其严重性也在同步上升。显然，IT团队愿意投资于新工具以提升安全性，但了解如何部署这些工具、如何有效利用团队时间以及简化流程，对于确保软件开发生命周期（SDLC）的安全至关重要。我们编制这份报告的目的不仅仅在于分析趋势，更是为了当技术业务领导者在针对AI导航、恶意代码或安全解决方案等方面制定决策时，能够为其提供清晰的指导和专业的技术咨询。”</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/WoY69us6292NWvmx99Ot</id>
            <title>谷歌刚刚更新了算法，顺便搞毁了几家公司</title>
            <link>https://www.infoq.cn/article/WoY69us6292NWvmx99Ot</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/WoY69us6292NWvmx99Ot</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 May 2024 13:29:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 谷歌更新算法, 公司毁灭, AI功能, 搜索引擎
<br>
<br>
总结: 谷歌最近的算法更新对一些公司造成了毁灭性的影响，特别是那些依赖谷歌搜索引擎的公司。谷歌的更新带来了更强大的AI功能，但也导致了一些原本排名靠前的网站被挤出搜索结果页面，给他们的业务带来了巨大打击。受影响的公司包括HouseFresh和Ready Steady Cut等。这些变化引发了对谷歌算法更新是否真的有助于网络的质量和用户体验的质疑。 </div>
                        <hr>
                    
                    <p></p><h2>谷歌更新算法，毁了多家公司</h2><p></p><p>&nbsp;</p><p>过去两年以来，谷歌搜索的一系列更新为这款互联网上最强大的工具带来了巨大变革，更配备了前所未有的AI功能。但最近，互联网上越来越多声音质疑，谷歌的一系列变化是在拯救网络，还是会将其推向毁灭？</p><p>&nbsp;</p><p>如果大家在谷歌引擎中输入过“空气净化器测评”，那想要获取的很可能是HouseFresh.com上的内容。该网站由Gisele Navarro和她的丈夫于2020年建立，整理了过去十年间改善室内空气质量的所有产品使用感受。他们在地下室里装满了各种净化设备，开展严格的科学测试，并撰写文章来帮助消费者们厘清思路、辨别炒作。</p><p>&nbsp;</p><p>HouseFresh就是由独立内容发布方推动建立活跃行业的典型案例。这些发布方所产出的原创内容，也正是谷歌长期以来号称应当推广的核心价值。实际上，就在该网站上线后不久，这家科技巨头就开始在搜索结果顶部显示HouseFresh。这让该网站迅速发展成一家欣欣向荣的企业，拥有15名全职员工。Navarro自己也为公司设定了颇具雄心的发展规划。</p><p>&nbsp;</p><p>但在不久后的2023年9月，谷歌对其搜索引擎算法展开了一系列重大更新。</p><p>&nbsp;</p><p>Navarro坦言，“这直接毁掉了我们的业务。一夜之间，本来指向HouseFresh的搜索词开始将人们引导至各大生活方式杂志，可这些杂志明显没有实际测试过产品。那里的文章中充斥着我一望而知的错误信息。”</p><p>&nbsp;</p><p>谷歌又在今年3月再次更新算法，这次造成的影响更大。HouseFresh的访客数量由每天数千人次锐减至数百人次。Navarro表示“我们完全被压垮了。”过去几周以来，HouseFresh网站不得不解雇掉大部分团队成员。她承认，如果后续情况没有好转，这家网站将唯有关闭一途。</p><p>&nbsp;</p><p>受影响的不止HouseFresh一家公司。</p><p>&nbsp;</p><p>英国娱乐新闻网站Ready Steady Cut的主编Daniel Hart也控诉谷歌改变搜索算法带来的影响可谓立竿见影。</p><p>&nbsp;</p><p>Hart解释道，“自从谷歌去年9月的更新之后，我们的流量当场减半，而且情况正变得越来越糟。我们不仅受到大网站内容的冲击，同时也正在被窃取我们内容的垃圾网站所取代。这样的整改毫无意义。”在接下来的几个月间，收入缩水已经迫使该网站将原本20人的作家与编辑团队裁撤至4人。</p><p>&nbsp;</p><p>谷歌方面的发言人则表示，该公司最近的更新已经给垃圾邮件和非原创内容造成了重大打击。谷歌也一直在密切关注导致搜索信息质量低下的滥用行为。</p><p>&nbsp;</p><p>谷歌算法更新之后，该公司向网站所有者发布了提示，号称能帮助其维持住搜索流量。但Hart指出，他们的网站聘请了顾问、重点关注谷歌的建议，而且不眠不休地更新网站。但经过近一年的努力，还是没有什么帮助。他表示，“过去8个月间，我浪费掉了宝贵的生命来努力遵循谷歌的建议。谷歌声称他们希望让网络用户能从掌握第一手经验和具备相关背景的人们那获取内容，可我们完全符合这样的标准。总之如今的情况实在让人心碎。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/b3/b323b0d0bb06674cb3e1d7df09ce40c7.png" /></p><p></p><p>部分案例显示，谷歌搜索近期的变化正在对各类网站产生惊人的影响。</p><p>&nbsp;</p><p>谷歌一位发言人则在采访中强调，该公司的所有搜索算法调整都是在经严格测试验证、确认对用户有所帮助之后才会落地，而且谷歌方面也为各网站所有者提供了协助、资源和机会，允许其就搜索排名问题提出反馈。</p><p>&nbsp;</p><p>但批评人士认为，实际情况可能恰恰相反。随着谷歌重新调整其算法并使用AI将搜索引擎转化为搜索与回答引擎，不少人担心对于那些专司产出用户喜爱内容的企业来说，造成的冲击恐怕不亚于物种灭绝级别的事件。</p><p>&nbsp;</p><p>谷歌坚定认为这些变化将给整个网络带来好处，而搜索算法的调整只是个开始。</p><p>&nbsp;</p><p>至少有一点可以肯定：谷歌在AI上所做出的努力，将对大部分网民在网络上看到的内容产生深远影响。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/d3/d3f93cd99c3a60b293dd2f2f0fc66fce.png" /></p><p></p><p>过去两年以来，所谓能让搜索变得更加“有用”的一系列更新，正在令努力遵循谷歌最佳实践的网站所有者感到沮丧。</p><p>&nbsp;</p><p></p><h2>谷歌AI工具“已读乱回”，工程师无奈手动删除</h2><p></p><p>&nbsp;</p><p>对搜索算法“动刀”后，谷歌最近又将注意力放到了AI工具上。这不，日前谷歌正因为社交媒体上充斥着的AI工具闹出的乌龙而忙得不可开交。</p><p>&nbsp;</p><p>这款谷歌新推出的AI综述（AI Overview）经常会胡言乱语，有时候让用户往披萨上抹胶水、有时候建议他们吃石头。面对这款匆忙上线产品搞出的麻烦，再加上互联网用户们用各种表情包大加嘲讽，谷歌正忙于手动禁用AI综述上的特定搜索内容。也正因为如此，很多表情包在被发上社交网络后不久就神奇地消失了。谷歌公司承认正“迅速采取行动”，旨在清除AI工具给出的一些奇怪答案。</p><p>&nbsp;</p><p>这样的现状着实令人摸不着头脑。毕竟谷歌测试其AI综述功能也有一年之久了——该功能早在2023年5月就以搜索生成体验的名号推出了beta版。谷歌CEO桑达尔·皮查伊更放出豪言，称该公司在测试期间已经支持了超过10亿条查询。</p><p>&nbsp;</p><p>而且皮查伊也提到，“在硬件、工程和技术突破的共同推动下”，谷歌同期将AI回答的交付成本降低了80%。看起来似乎是成本优化来得太早，而生成技术本身并没有做好准备。</p><p>&nbsp;</p><p>事实上，这款AI工具（AI Overview）是前不久皮查伊在公司年度开发者大会上，向众人宣布了其搜索引擎发展历史上最重大的举措之一。皮查伊表示，展望未来，谷歌搜索将针对诸多问题给出自己的AI生成答案。这项名为“AI综述（AI Overviews）”的功能已经面向美国用户推出。皮查伊指出，“这是一款能切实服务用户的产品。谷歌搜索由此成为建立在人类好奇心之上的生成式AI成果。”</p><p>&nbsp;</p><p>谷歌方面还表示，其AI综述产品旨在向用户输出“高质量信息”。谷歌发言人Meghann Farnsworth在采访邮件中回应称，“我们看到的许多案例都不属于常见查询，而且发现了不少被篡改或者无法重现的案例。”Farnsworth同时证实，谷歌方面正在“迅速采取行动，在符合内容政策的前提下适当删除某些AI综述查询，并利用这些案例对我们的系统进行广泛改进。部分改进结果已经在实际使用中得到体现。”</p><p>&nbsp;</p><p>由此可见，其实谷歌也承认了AI工具可能会提供不准确信息，但表示正在不断努力改进结果。谷歌公司发言人指出，AI综述的内容通常整理自多个网页，而非单一来源，而且响应结果会突出显示相关链接。这位发言人还提到，内容发布方可以在网页上使用特殊标签来控制AI综述是否列出相关网站链接。但需要注意的是，一旦AI模型抓取了创作者的内容，该数据可能将无法被删除。</p><p>&nbsp;</p><p>AI综述只是过去两年以来，谷歌对其核心产品做出的一系列重大改变中的一环。该公司表示，其最近针对搜索算法做出的改进努力将开启一个令人兴奋的技术新时代，并有助于解决困扰网络世界的诸多问题。</p><p></p><h2>成也搜索，败也搜索</h2><p></p><p>之所以要推动这些变化，是因为谷歌意识到此前的网络一直存在弊端。如果大家使用过搜索引擎，对此肯定也有切身体会。互联网的运作长期由所谓“搜索引擎优化（SEO）”所主导，这项技术旨在调整文章及网页内容，以便更好地被谷歌搜索发现并优先显示。谷歌甚至在为网站所有者提供SEO技巧、工具和建议。对于数百万将业务建立在机械化搜索体系之上的企业来说，SEO就是一笔他们不得不承受的“技术税”。</p><p>&nbsp;</p><p>问题在于，搜索引擎优化可能会被滥用。抱有野心的网站所有者也越来越多地意识到，相较于服务人类用户，专门制作适合谷歌筛选算法的内容才是增加经济收益的不二法门。</p><p>&nbsp;</p><p>谷歌针对垃圾搜索结果的战争已经愈演愈烈。2022年，该公司对其算法发布了“实用内容更新”，旨在淘汰纯为提升搜索排名而创建的内容。谷歌随后一截2023年9月发布后续更新，并在今年3月再次出手调整算法。谷歌方面表示，结果是“搜索结果中低质量、非原创内容减少了45%。”这似乎代表着一次巨大的成功。</p><p>&nbsp;</p><p>谷歌一位发言人在采访中表示，“我们最近的更新，希望将人们与来自网络的各类不同网站上的实用、令人满意且原创性的内容联系起来。在努力改进搜索服务的同时，我们还将继续专注于为网站提供有价值流量，以支持健康、开放的网络环境。”</p><p>&nbsp;</p><p>但谷歌的一系列改变，包括算法的更新和近期AI工具的出现，都没有博得什么好印象。</p><p>&nbsp;</p><p>一位不愿透露姓名的AI业务创始人在采访中表示，“谷歌曾经是一家以引领前沿、提供高质量产品的行业龙头，如今却不断发布各种质量低下的产品，甚至沦为整个互联网的笑柄和玩梗对象。”</p><p>&nbsp;</p><p>AI专家、纽约大学神经科学名誉教授Gary Marcus则在采访中表示，不少AI厂商都是在“兜售梦想”，希望更多人相信这项技术的正确率终将从80%提升至100%。Marcus强调，初步实现80%的正确率相对简单，因为其中涉及大量人类数据，其正确率天然就在这个区间。但弥合这最后20%的差距却极具挑战。实际上，Marcus认为这最后20%很可能是条死胡同。</p><p>&nbsp;</p><p>Marcus坦言，“对于很多问题，必须要经过相应的推理步骤才能判断当前事件是否可信、信息来源是否合法。而要想像人类审核员那样解决问题，恐怕首先要真正实现通用人工智能（AGI）。”Marcus本人和Meta公司的AI负责人Yann LeCun也都认定，为当前AI系统（包括谷歌Gemini和OpenAI GPT-4）提供支持的大语言模型并不是实现AGI的正确答案。</p><p>&nbsp;</p><p>这对谷歌来说，现在面临的处境无疑是十分艰难的。毕竟微软已经抢先一步，通过Bing大力推广生成式AI技术。另据报道，OpenAI正在开发自己的搜索引擎。而TikTok，正在为年轻一代用户提供最能满足其喜好的AI推荐体验。各方角逐之下，老牌巨头谷歌明显是感受到了竞争压力，最终导致整个生成式AI市场乱成了一锅粥。Marcus指出，2022年Meta曾发布名为Galactica的AI系统，但该系统在推出后不久即遭下架，因为它居然建议用户吃玻璃。吃玻璃、吃石头，看来Meta和谷歌的大模型倒是很有共同语言。</p><p>&nbsp;</p><p>谷歌倒是对其AI综述颇有信心并制定了宏伟计划，而目前已发布的功能只是其上周官定量内容的一小部分。针对复杂查询的多步推理、利用生成式AI组织结果页面，通过Google Lens实现视频搜索——谷歌的雄心壮志绝对不容小觑。但回归现实，谷歌的商业声誉无疑取决于其AI功能的实际表现，而目前来看其正确性实在堪忧。</p><p>&nbsp;</p><p>Marcus直言，“（这些模型）本质上无法对自己的输出进行健全性检查，而这样的现实正在拖累整个AI技术产业。”</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.theverge.com/2024/5/24/24164119/google-ai-overview-mistakes-search-race-openai">https://www.theverge.com/2024/5/24/24164119/google-ai-overview-mistakes-search-race-openai</a>"</p><p><a href="https://www.bbc.com/future/article/20240524-how-googles-new-algorithm-will-shape-your-internet">https://www.bbc.com/future/article/20240524-how-googles-new-algorithm-will-shape-your-internet</a>"</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/BciKq80BAwfAeJuZ3k7b</id>
            <title>禁令再升级！拜登政府已不想让中国人在美从事AI工作了，套壳大模型的公司也危险了</title>
            <link>https://www.infoq.cn/article/BciKq80BAwfAeJuZ3k7b</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/BciKq80BAwfAeJuZ3k7b</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 May 2024 13:25:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 美国, AI产业, ENFORCE法案, 中国
<br>
<br>
总结: 美国通过《ENFORCE法案》收紧AI大模型出口，限制中国员工在美从事AI相关工作，旨在保护美国技术优势和国家安全。该法案一旦生效，可能对中国AI产业造成阻碍，包括数据供给和技术合作方面的影响。然而，中国也在加速自主研发步伐，将面临挑战但也有机遇。 </div>
                        <hr>
                    
                    <p></p><blockquote>美国已经不止一次提议从科技上对中国发起制裁，如果此次法案生效，又将对我国AI产业带来哪些影响？听听专家们的观点。</blockquote><p></p><p></p><h2>美国立法收紧AI大模型出口，连AI人才在美工作也受限</h2><p></p><p>&nbsp;</p><p>北京时间上周四，美国众议院外交事务部委员会以显著的多数票数，成功通过了一项旨在严格管控AI技术出口的法案。这项法案被正式命名为《加强海外关键出口国家框架法案》，通常简称为《ENFORCE法案》。</p><p>&nbsp;</p><p>值得一提的是，在该法案不仅限制了AI系统和大模型的出口，一旦法案通过，持有H1b 签证的中国员工或留学生可能需要特殊许可才能在美从事AI/ML相关工作。也就是说，这是明晃晃在限制中国人在美从事AI相关工作。</p><p>&nbsp;</p><p>《ENFORCE法案》由美国众议员共和党议员迈克尔·麦考尔（Michael McCaul）、约翰·莫伦纳尔（John Molenaar）、马克思·怀斯（Max Wise）和民主党议员拉贾·克里希纳莫西（Raja Krishnamoorthi）提出。其主要目标在于，通过强化美国商务部的权力，使其能够更加便捷地对AI模型实施出口管制，并进一步限制美国与外国实体在开发可能威胁国家安全的AI系统方面的合作。</p><p>&nbsp;</p><p>立法者表示，此举意在确保美国的技术优势和国家安全不受外部威胁。该法案的共同作者、众议院议员Michael McCauln (R-TX)&nbsp;表示：“人工智能引发了一场技术革命，它将决定美国是否能继续保持世界领先超级大国地位，还是会被中国超越。”</p><p>&nbsp;</p><p>McCauln表示他最担忧的是，“虽然美国政府的工业和安全局 (BIS) 有权限制人工智能加速器的出口——拜登政府曾多次利用这一点来扼杀中国在该领域的创新——但它缺乏监管人工智能模型出口的权力。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/27/2774a88c92338eebf9842c873338e7e7.png" /></p><p></p><p>《ENFORCE法案》于5月9日首次公布，由众议院的跨党派AI工作组提出，该法案旨在修订2018年出台的《出口管制改革法案》，这一年美国将14类新兴技术纳入出口管制。&nbsp;</p><p>&nbsp;</p><p>据悉，《ENFORCE法案》还需要通过众议院、参议院的全体表决以及总统拜登签署，才能落地生效。一旦此法案生效，将授予美国工业和安全局限制人工智能模型出口的权利，也促使白宫能够要求美国公司或个人只有获得出口许可证才能出口AI大模型。</p><p>&nbsp;</p><p>McCauln称道：“这项立法为 BIS 提供了灵活性，使其能够制定对封闭人工智能系统适当的控制，而不会扼杀美国的创新或影响开源模型。”</p><p>&nbsp;</p><p>值得一提的是，目前该法案实际上并不包含任何明确的保护或对开源模型的豁免，并且基本上涵盖了所有的人工智能系统、软件或硬件。</p><p>&nbsp;</p><p>需要明确的是，该法案的实际措辞意在模糊，并具体要求在法案通过后一年内更新“涵盖的人工智能系统”的定义。</p><p>&nbsp;</p><p>众议院议员玛德琳·迪恩 (D-PA) 在投票前解释道：“我们还在法案中对人工智能和人工智能系统的定义进行了临时修改，以便政府可以采取通常的监管程序并征求公众意见，从而对最终的定义进行适当的范围界定。”</p><p>&nbsp;</p><p>就在签订该法案的同一周，据彭博社报道，美国国会计划立法减少人工智能的潜在风险和危害，并每年至少投入320亿美元于人工智能研究，以促进美国经济和国家安全。美国参议院多数党领袖舒默（Chuck Schumer）表示，这项资金将让美国公司、大学和人才“保持在人工智能产业最前沿的地位”。</p><p></p><h2>如果法案生效，将对我国AI产业带来哪些影响？</h2><p></p><p>&nbsp;</p><p>据《21世纪经济报道》报道，清华大学人工智能国际治理研究院的李依栩向撰文对比了两部法案，表示本次法案主要是将AI技术纳入了《出口管制改革法案》的管制框架内，通过补充AI相关定义、赋予总统管制权、增加美国人从事AI模型出口相关活动的许可义务，对AI模型进行管控。</p><p>&nbsp;</p><p>比如，法案第三条扩展了总统管制权，管制对象是特定受限的AI系统、对美国国家安全至关重要的新兴和基础技术相关活动；</p><p>&nbsp;</p><p>法案第四条增加了一项额外权利，如果美国人在出口、再出口被确定为对美国国家安全至关重要的新兴和基础技术，包括设计、开发、生产、维修、翻新这些技术，美国总统有权要求他们申请并获得商务部许可。</p><p>&nbsp;</p><p>那么，这项比此前《出口管制改革法案》更加严格的《ENFORCE法案》一旦生效，对我国AI产业将带来怎样的影响？</p><p>&nbsp;</p><p>某头部电商技术总监Micheal Yan在接受《AI前线》采访时表示：</p><p>&nbsp;</p><p></p><blockquote>“短期内对于我国AI大模型发展会产生一些阻碍。尤其是在数据供给方面，国内开发者可能会面临数据短缺的问题。由于AI大模型的训练需要大量的数据集，如果美国限制对华出口数据集，这将直接影响中国AI模型的训练和性能提升。&nbsp;另外，在技术合作方面，中美两国在AI大模型开发过程中有着广泛的技术合作。如果该法案生效，这将会阻碍新技术与新模型的开发，因为中美顶尖开发人才的技术交流将受到较大影响。&nbsp;然而，需要指出的是，实施对华出口禁令的负面影响是双向的，对美国自身也存在不利之处。例如，在数据收集问题上，禁止对华AI模型出口同样可能让美国公司错失借助中国数据进行AI迭代的机会。从长远来看，中国有充足潜力自主发展、突破限制，而美国的这种限制措施可能会激发中国加快自主研发和创新步伐。美国限制AI大模型出口的政策不仅会对中国AI产业的发展带来挑战，同时也会对美国自身在某些方面产生一定的负面影响。”</blockquote><p></p><p>&nbsp;</p><p>面对一波接一波的科技制裁时，我国也在加速自主研发的步伐，这对于国产AI技术的发展来说也是一种机遇。</p><p>&nbsp;</p><p>平安集团前CSO、广东省CIO联盟会长李洋表示：</p><p>&nbsp;</p><p></p><blockquote>“如同过往的芯片、系统软件、应用软件等对中国的限制一样，美国对AI的限制出口接踵而至，但是AI的限制在目前中国大力推行国产替代的大前提下，是机遇大于挑战的。在这样的大背景下更加便于中国的科研工作者，丢掉幻想，重新布局。&nbsp;现实情况下是，AI所依赖的算力、算法很多都依赖于美国，当然也包括其他国家的算法、数据。但实际上中国的AI场景和应用体量非常大，在数据和算法层面不久后将不再受制于人。现代AI的发展不过几十年，即使是当前美国处于领先地位，也未必就永远处于领先。即便是如今众星捧月般的明星独角兽OpenAI也无法保证其大模型架构是未来AGI的正确技术发展路线，所以这个时候美国的限制对中国来说未必是坏事。&nbsp;希望中国的科研工作者，尤其是AI工作者们能够沉下心，在基础算法和模型研究中能够走出一条中国特色之路。</blockquote><p></p><p>&nbsp;</p><p>李洋还表示，“对于AI人才赴美的限制，我觉得也不是什么大问题，中国的AI基础研究和应用市场相对于美国来说都还是蓝海，立足于中国的产业，将AI人才留在中国服务，利大于弊。况且，美国所谓的这些限制，在当今时代下，也不会是一揽子的全封闭，所以我们应当审时度势，抓住这个机遇。”</p><p>&nbsp;</p><p>法案公布以来，除了担忧对于AI人才和大模型的限制外，外界还会担忧对于开源大模型的限制会让许多国内“套壳”大模型企业很难受。</p><p>&nbsp;</p><p>对此，李洋表示，“开源模型虽然现在美国比较领先，但是其他欧美国家也不乏相应的开源产品，我们在这个阶段还可以多方面借鉴和研究。并且，基于我们的自主创新体系，我们还是要沉下心来研发自己的AI基础底座，包括硬件基础设施、开发平台和大模型及其应用，所以短期内会对一些套壳的中国公司产生一些影响（包括模型的演进、应用和商业化等等都存在相应的限制）。”</p><p>&nbsp;</p><p>“但是从技术和应用上来讲，进口的开源模型也不是完美和最终的AGI的路标和唯一标准，所以在这个时候，也希望我国AI企业能够立足于自身的能力打造，丢掉幻想，与中国生态和国际生态一道打造出中国的AI开源版本，为中国和国际做出自己的贡献”。李洋说道。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.21jingji.com/article/20240527/herald/5f7b347c2787de4bf776584f95950075.html">https://www.21jingji.com/article/20240527/herald/5f7b347c2787de4bf776584f95950075.html</a>"</p><p><a href="https://www.theregister.com/2024/05/23/us_lawmakers_advance_bill_to/">https://www.theregister.com/2024/05/23/us_lawmakers_advance_bill_to/</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/xLryHtsN1PFPMuhquhAr</id>
            <title>Agent 还没出圈，落地先有了“阻力”：进入平台期，智力能否独立担事？</title>
            <link>https://www.infoq.cn/article/xLryHtsN1PFPMuhquhAr</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/xLryHtsN1PFPMuhquhAr</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 May 2024 10:12:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AutoGPT, AI Agent, 具身智能, 大模型
<br>
<br>
总结: 作者介绍了AI Agent的当前能力和应用潜力，讨论了在企业场景中有效利用Agent的重要性，以及未来发展趋势。同时，还探讨了AI Agent在不同领域的具体应用场景，以及与大模型的区别和发展方向。 </div>
                        <hr>
                    
                    <p>作者 ｜ 华卫</p><p></p><p>去年出圈的AutoGPT，让AI Agent来到大家的视线中并迅速爆火，大家都对Agent抱有极高的想象力与期待值。那么，Agent现在到底有多大的应用潜能？企业要如何抓住？同时在具体的落地实践方面，也有不少悬而未决的挑战。</p><p></p><p>带着这些问题，InfoQ《极客有约》特别邀请了阅文集团 AIGC 技术负责人马宇峰担任主持人，与机器姬CTO&amp;具身智能一百零八讲主讲人刘智勇、华为云aPaaS首席架构师陈星亮，一同探讨AI Agent的当前能力、应用落地情况以及未来发展趋势。部分亮点如下：</p><p></p><p>Agent不仅仅是一个玩具，而可以改变现实世界。在企业场景中有效利用Agent，合理选择业务场景非常重要。具身智能领域最大的挑战在于操作层面，瓶颈在于如何泛化地执行物理世界中的各种操作。未来使用Agent和大模型将成为企业员工需要掌握的技能。人类仍然拥有最终的评价权和评估权，这种能力是大模型无论如何发展都无法达到的。具身AGI的到来会为人类社会带来新的篇章，即从碳基生命到硅基生命的延续。</p><p></p><p>以下为访谈实录，经编辑。完整视频参看：</p><p><a href="https://www.infoq.cn/video/DOPpG6NjCHcJKDzCsAFT">https://www.infoq.cn/video/DOPpG6NjCHcJKDzCsAFT</a>"</p><p></p><h2>AI Agent当前的能力</h2><p></p><p>马宇峰：首先要谈的就是AI Agent现阶段的能力，大家现在是如何应用AI Agent的？具体落地场景有哪些？</p><p>刘智勇：最近大家可能已经注意到了一个名为“Figure”的机器人，演示中，工作人员向该机器人表达了饥饿感之后，Figure成功地将苹果递给了他；这一过程展示了AI Agent在物理世界中进行任务推理、规划并最终转化为实际行动的能力。在具身智能领域，AI Agent的应用场景非常广泛，AI Agent可以大致分为以下四个方面。</p><p>工业场景：在工厂中，具身智能机器人可以应用于3C生产线或汽车总装线，提高生产效率和自动化水平。商业服务场景：在商业环境中，具身智能机器人可以提供接待、讲解、导览、巡逻和配送服务，改善客户体验，提升服务质量。家庭场景：在家庭环境中，具身智能机器人可以承担清洁服务或家务工作，减轻人们的负担，提高生活质量。火星建设：在未来的火星探索和建设中，具身智能机器人有望发挥重要作用，帮助人类在恶劣的外星环境中进行建设和研究。</p><p>对于这些应用场景，具身智能都展现出了巨大的潜力和希望，为未来的技术发展和应用提供了广阔的前景。</p><p>陈星亮：针对企业场景进行AI Agent能力创新时，多数是从IT场景开始的，因为该场景拥有较为完善的信息化基础。在这一过程中，我们遵循两个主要原则：一是先易后难，我们首先从普遍性场景开始，然后逐步向专业化场景演进；二是保障效果，无论开发哪种场景的AI应用，都必须确保其有效性。</p><p>办公和编码领域被广泛认为是AI Agent应用的切入点，因为这些场景相对通用，容易实现。随着技术的进步，我们将AI Agent的应用延伸到更复杂的场景，例如：</p><p>办公领域：AI Agent可以用于自动生成会议纪要或设计文档，这些任务比简单的代码生成或文本创作更具挑战性，需要更深层次的场景理解和更高级的语言处理能力。销售或服务领域：AI Agent可以用于合同审核或法律条文的辅助生成，这要求AI Agent不仅要理解法律术语，还要能够处理复杂的逻辑关系。网络设备监控：在对网络设备进行监控的基础上，AI Agent可以执行自动巡检任务。这要求AI Agent不仅要处理专业数据，还要能够理解并应用信息化积累的知识。</p><p>马宇峰：大家首次接触到AI Agent大概是在什么时候？从本质上讲，AI Agent与大模型的区别究竟体现在哪些场景上？最核心的区别是什么？</p><p>陈星亮：Agent 这个概念，实际上在大模型出现之前就已经存在了。在进行 IT 系统集成或设计某些自动化流程时，其实已经有Agent 这一层了，尤其是在设备与外界交互的环节，而那时还没有将大模型技术整合进来以实现更广泛的泛化能力和生成式能力。</p><p>大模型技术引入后，起初我们并没有考虑将其应用于设备控制或高度交互性的 IT 系统交互中，而主要看中其在创作和生成内容方面的潜力。之前我们在设备代理方面的工作与 AI Agent 的概念思路颇为相似，只是随着大模型的加入，AI Agent 的能力和应用场景都发生了变化。当我们将这些结合起来后，认识到了 AI Agent 的真正面貌。因此，如果仅从 IT 系统的能力角度来看，AI Agent 这个概念并不神秘，不过是通过引入大模型为 AI Agent 带来了更多能力，从而丰富了其功能。</p><p>刘智勇：无论是ChatGPT还是Agent、具身智能，本质上都是在以下三个方面进行发展。</p><p>文本世界：在文本领域，大语言模型展现出了强大的生成和理解能力，这主要体现在ChatGPT等应用中。数字世界：数字世界中，我们需要利用规划、循环和反思的控制机制，实现任务从开始到结束的全流程控制，并调用数字世界里的外部工具进行执行。物理世界：物理世界中，Agent的能力落地体现在具身智能上，即通过具身智能技术将规划形成的任务序列转化为物理世界中的实际操作。</p><p>马宇峰：我分享一下第一次接触Agent的经历，去年夏天OpenAI开发了一项名为“Function Call”的能力，虽然看起来仍然是文本的输入和输出，但当函数作为一个字符串被输出并被精确调用时，我确实看到了Agent的不同之处。以前我们认为创作和创意不确定性是大语言模型最人性化的特征，但同时它们也有机器的一面，能够在有限的范围内唤醒某些函数。这项能力让我意识到Agent应该被独立考虑，其围绕工具使用、规划和执行的能力，可以帮助大模型结合现实世界中的数字和物理能力，形成一个更完整、更通用的解决方案。这是我对Agent概念的一次认知冲击。</p><p>然而，随着时间的推移，我发现Function Call可能并不像我最初想象的那么好。它演示的技能是查询天气，虽然可以很好地执行，但许多场景要复杂得多，可能不只有10个或20个函数可供调用，会出现完全不确定的函数，下一步该执行哪个函数也会是未知的。不过，Agent的主流能力，如浏览器的唤起、搜索引擎的查询结果以及一些生成能力的唤起，确实有效地让它从概念走向实际。当然，在实际应用过程中，我们也发现了许多不确定因素，但Agent的能力已经让我感到惊讶，它不仅仅是一个玩具，而可以改变现实世界。</p><p>回到Agent 的适用场景，我分享一下个人自身在探索中使用的直观感受。使用Agent能力可以批量生成自媒体文章，也可以像模像样地讲一个故事，从创建角色、制定纲到将角色和情节融合，再逐步生成内容，它的成文速度非常快，也有一些优点，比如生成过程中，可以将角色单独抽象出来去形成可视化的元素，可以使用多个角色和情节引导来发展内容片段，且在逻辑框架内是可控的。</p><p>但深入研究后我们发现另一个问题：Agent输出的内容，还是没有达到人类所能达到的逻辑性、创意性相结合。业内也做了很多尝试，这方面却似乎一直停留在中等或中上水平，整体表现平庸，所以这确实是长期困扰我们的问题。虽然我们最初认为Agent很有用，但在商业化和变现能力上似乎没有那么强。</p><p>想问一下陈老师，在代码和办公场景，Agent 可以从哪些方面提升效率？有哪些bad case？</p><p>陈星亮：我先谈谈Agent 给一些稳定场景带来的效率提升作用，如设计文档生成和合同中法律文本的生成等。在一些应用场景相对明确、法律条文引用也相对模式化的特定领域，如可靠性设计或安全威胁设计，Agent的表现在业务用户看来感知和体验都非常好，准确度也相当高，显著提升了工作效率。目前，我们也在将Agent应用于网络设备巡检等生产场景。尽管巡检过程中会遇到各种意想不到的问题，但对于那些已有案例库和解决方式库的巡检，Agent 都能够发挥作用，并帮助提高巡检效率、简化人力的工作。</p><p>然而，也有一些不尽如人意的地方。Agent刚推出时，大家对它寄予厚望，导致在选择应用场景时没有过多限制，业务团队提出了许多要求较高的场景，想要用Agent去解决未知的问题。这些要求的实际难度很大，而Agent在处理未知问题时的能力有限。因此，如果要在企业场景中有效利用Agent，合理选择业务场景非常重要。否则，Agent的效果可能不会达到预期，甚至可能非常差。</p><p>马宇峰：如果人类都做不到的事情，期望Agent达到超越人类的水平是非常困难的。相反，那些人类已经重复做了很多遍且已经规范化的工作，确实可以将人类的判断力解放出来，完全交给Agent来自动化处理。在具身智能的Agent应用上，哪些方面是可行的？可能存在什么挑战？</p><p>刘智勇：首先，具身智能的输入需求依赖于视觉语言模型，这意味着需要处理整个环境的三维数据信息，而不仅仅是二维图像。它需要的输入包括深度数据、RGB图像等，可能还要结合触觉、反馈力以及编码器数据等，这些数据共同构成了具身智能的全面输入。因此，在数据输入的方式上，具身智能与传统Agent存在显著差异，这些差异带来了巨大的挑战。</p><p>其次，在数字世界的Agent中，无论是什么类型的Function Call，基本上都是可执行的动作，操作层面通常不会遇到问题。然而，具身智能中存在一个可供性问题，即是否能够真正执行某个动作。尽管存在这些挑战，但也有一系列方法可以解决这些问题，如具备泛化能力的视觉语言模型、迭代细化的机制、自我反思的机制等。目前来说，具身智能领域最大的挑战在于操作层面，即具身操作。感知、决策和规划虽然重要，但真正的瓶颈在于如何可泛化地地执行物理世界中的各种操作。</p><p>马宇峰：Agent目前的发展状况如何？是否已经达到了一个平台期，还是仍然有很大的提升空间？是否依赖于某些特定的背景？</p><p>我认为Agent主要依赖于大模型的Function Call能力，需要准确地识别出当前调用哪个模型来完成当前任务，并提供相应的结果，以便大模型进行下一步操作。而瓶颈可能在于读取上下文的长度，上下文长度决定了能够识别多少个函数。Agent在执行过程中受限于场景，只能在有限的函数中进行选择，其执行也不完全精确；如果执行不精确，就需要获取更多的环境信息或反馈信息来执行函数，过程中可能会出错。Agent是一个精妙但不够鲁棒的系统，如果它返回到上一级并根据错误信息重新执行，可能会带来更大的资源消耗和时间延迟。</p><p>陈星亮：在企业场景中实施Agent时，我们首先需要考虑的是技术的可实现性。在挑选场景的过程中，就要考察技术是否可行；一旦场景确定，接下来需要考虑的是如何提高Function Call的准确度，如果准确度不够高，需探索其他工程手段来提升API的识别准确率，甚至在语义理解之后通过额外的工程能力进行调整、校验生成的API并通过查询方式进行补充。企业面临的最大挑战之一就是需要重复性地进行这类工作。目前我们也在探索长序列处理、记忆的短、长期存储以及上下文空间的扩展等技术，以期在未来实现更多的技术突破。</p><p>在具身智能领域，企业场景中也在逐渐引入多模态技术，尤其是当与操作技术领域（OTA）的设备关联时。多模态技术的引入包括传统的视觉识别等，将进一步增加系统的复杂性。如果大模型在这些领域取得显著进展，那么在企业IT融合场景中的工程难度将大大减少。目前，我们在工程实践中仍需进行大量技术工作，这些工作的管理复杂性甚至超过了传统的微服务架构。</p><p>我相信，随着技术的进步，未来将有很大的空间来改进现有的工程能力，减少人工干预，让大模型承担更多的工作。无论是让大模型自行处理，还是让Agent框架沉淀出更多稳定的框架性技术，都是未来技术发展的趋势。我对大模型在未来的迭代和改进抱有很高的期待，相信它们将带来更好的效果，并减轻当前工程化实践中的一些负担。</p><p>刘智勇：从阶段性的角度来看，我们认为具身智能目前处于技术起步期，未来的发展空间仍然非常广阔。之所以称之为技术起步期，是因为目前还存在三个方面的挑战：</p><p>任务类型的泛化性：这涉及到Agent能否理解各种类型的指令，并能够完成具体的规划而不产生幻觉，抑制Agent在理解上的偏差，对齐人类意图的二义性和潜在偏好，确保其能够准确执行任务。环境的泛化性：即Agent快速与环境对齐，对齐环境的规律、动态性和随机性。操作的泛化性：这是更为复杂的挑战，涉及如何利用多种数据源采集更多的线下数据，并据此训练出能够泛化到不同情境的具身操作模型，目前行业中还没有一个非常好的解决方案。</p><p>从这三个方面的挑战中，我们看到了未来的发展机会。尽管目前还存在许多问题需要解决，但这同时也是推动技术进步的动力。</p><p>观众提问：是否可以认为大模型做好了就不需要 Agent 了呢？</p><p>刘智勇：大语言模型的主要功能是处理和生成文本，核心在于将文本信息进行向量化处理，并通过Transformer架构以及监督学习机制，实现技术上的范式转变。这些技术基础的迭代，再结合大量的数据和强大的算力，促成了ChatGPT等大语言模型的诞生，它们在文本生成和回复方面表现出色。</p><p>尽管大语言模型在文本领域取得了显著的成就，但本质上只具备基于零样本提示词的文本回复的能力，而不具备执行实际任务的能力。这意味着，无论大模型在文本处理上多么先进，它们仍然需要Agent的介入来实现从文本到行动的转变和全流程的处理。</p><p>因此，大模型和Agent是两个不同的概念，前者专长于文本交互，而后者则涉及到任务的执行和落地能力。简而言之，大语言模型缺乏将文本回复转化为实际行动的能力，是典型的缸中之脑。</p><p>马宇峰：如果大语言模型发展到某个瓶颈无法提升，那也可以像两个人类合作思考能更高效地完成工作一样，使用两个大模型实际上可以进一步提升当前水平。哪怕提升的幅度不大，但考虑到大模型的较高的基础表现，即便是小幅提升也可能带来非常显著的回报，并且能够有效地增强现有能力。至于这些能力是否会直接集成到大语言模型中，我认为在相当长的一段时间内，我们仍然可以将大语言模型视为一个智能体，主要从智能逐步思考的角度来使用它。</p><p>陈星亮：aPaaS主要是基于行业内现有的资产或经验，实现程度化代码开发，降低开发门槛，通过拖拉拽的方式快速构建简单的应用程序。随着大语言模型代码生成能力的出现，零/低代码平台受到了较大的冲击。曾经有观点认为，大模型的出现可能会使得低代码或零代码的开发方式变得不再必要。实际上，我认为情况并不会如此。</p><p>零/低代码平台可以有效地融合大语言模型的能力，让大模型直接参与代码生成。以前需要通过拖拉拽来实现的功能，现在可以通过自然语言处理（NLP）的方式进行交互，提供更直观、友好的用户体验，并帮助理解业务用户原始的语意，以更好地生成低代码或零代码应用。我认为零/低代码平台和大模型之间更多的是一种合作关系。低代码平台上已经积累了大量的业务资产，而大模型可以将其作为插件调用，两者结合将发挥出更大的潜力。</p><p></p><h2>AI Agent的落地挑战</h2><p></p><p>马宇峰：在大语言模型不提升或通用大语言模型更新周期较长的情况下，如何利用现有工具和能力取得良好成果？有哪些方法或策略？</p><p>尽管当前AI Agent面临许多瓶颈和困境，限制了其应用范围，但仍有一些方法可以提升其驱动能力，如可以通过垂直领域的强化训练、特殊训练技术或更巧妙的方法，在不提升大语言模型本身能力的前提下改善Agent的表现。Agent在当前大语言模型框架下的表现，不仅取决于模型本身，还受到其他多个环节的影响。即便大语言模型不是限制因素，其他环节的优化也能提升整体Agent的效果。以Kimi为例，它之所以能够脱颖而出，可能确实在大模型的某些方面做了针对性强化，但重要的是它对文档类型的解析能力有效提升了实际操作中的使用体验。Kimi能够在处理长文档时进行分块，并采用迭代检索的方式输出答案，这大大增强了Agent在特定场景下的应用体验。</p><p>我相信，即使在大语言模型能力不变的情况下，只要充分提升检索能力，就能显著提高最终的可用性和准确率。很多时候未能获得准确答案，是因为没有找到正确的信息片段。如果知识库足够丰富，片段足够多，那么作为一个智能整合的搜索引擎，Agent将具有巨大的应用潜力。在大语言模型能力不完整的情况下，只要把某个小模型、小工具或阶段（如检索阶段）做得足够好，也能显著提升Agent的整体表现。</p><p>刘智勇：要提升AI Agent的能力，首先需要充分挖掘并利用长期记忆，通过RGBD摄像头读取的数据，结合视频语言模型，形成丰富的语义信息。在特定场景中，这些语义信息往往是重复出现的元素，关键在于如何有效地保存信息，为后续的规划提供坚实的基础。随着时间的推移，语义信息不断积累，AI Agent的长期记忆能力将变得更加强大。</p><p>其次，进行迭代细化是提升AI Agent能力的另一个关键点，这意味着需要不断结合当前的模糊指令和新获得的语义信息，形成新的提示词。通过不断的迭代询问，AI Agent能够逐步细化和精确化其理解和响应，通过不断反思，最终达到更加精准的结果。</p><p>陈星亮：企业内部考虑事务时主要关注两点，都与数据紧密相关。首先是文档处理的问题，在企业中，非结构化文档往往是承载信息的主体，处理这些文档不仅要识别文档类型，还包括对复杂文档的解析，如图文混排和包含复杂表格的文档。这些内容在原有的基础上，需要对文档类型识别的范围进行扩展，但在企业内部对这种复杂文档的解析仍是一个较大的挑战。</p><p>其次关于原有数据的利用问题，特别是在生产场景中，一般都具备专业领域的背景。以设备巡检为例，它与设备的领域知识密切相关，这种情况单靠企业自身的私域数据积累可能不够，需要在行业内去做垂域模型。目前，我们期望通过Agent技术的发展，能够让更多企业在通用场景中体会到Agent带来的好处，从而愿意将自己内部的结构化数据进行区分，将企业机密数据与可对外开放的数据分离，并逐步开放一些行业公共数据，这将有助于构建每个行业的垂直领域模型，为未来企业场景和Agent的发展带来巨大的好处。</p><p>马宇峰：初期部署Agent的成本是否高昂？是否能够带来相应的收益？能否实现成本的回收和价值回报？</p><p>陈星亮：企业部署Agent时，成本问题是一个必须考虑的重要因素，并且需要结合业务团队的期望以及对目标的评估来共同考量。初期企业主要探索通用场景时，成本通常是较低的。随着业务场景的成熟，以及越来越多的用户和业务团队成员开始使用这些场景，成本就会开始上升。特别是当场景全面开放并开始构建更多场景时，就可能需要多套模型和版本，模型也需要不断地做飞轮进行迭代和优化，成本可能会指数级增长。</p><p>因此，在正式对外放开并大规模使用Agent之前，与业务团队进行充分沟通和期望管理是非常重要的，需要让业务团队明白，业务场景真正对外开放并吸引大量用户使用后将会涉及到哪些成本。同时，业务团队也需要评估这些成熟场景能够带来的价值，如对客户满意度和内部效率提升的贡献。当业务团队获得这些信息并进行综合评价后，他们对预算和投入的决策将会更加明智，这样的过程有助于确保Agent部署的成本得到合理评估和控制，并带来相应的价值回报。</p><p>刘智勇：Agent部署的成本考量包括云端的调用成本、机器人本体的计算成本以及整体的部署成本这三个主要方面。</p><p>云端数据成本。这涉及调用模型的频率，如果实时观察环境中的语义信息，就需要频繁且快速地调用模型，这样会耗费大量的计算资源，从而产生高额成本。因此，必须考虑调用频次和计算资源消耗的问题，实现具身智能体和自身限制的对齐。机器人本体成本。在具身智能场景中，机器人本体通常需要具备一定的计算能力。为了使机器人能够在不同场景中应用，无论是商业、工业还是家庭环境，都希望能够在端侧部署大模型，尤其是本地部署，而这在没有高端GPU和显寸的支持的条件下尤为关键。部署成本。将设备分布式放置在不同地方会产生额外的成本，此外还需要考虑是否能够通过启发式方法或其他手段，让设备快速启动并投入使用，这也是降低部署成本的一个重要方面。</p><p>马宇峰：部署成本确实是一个值得讨论的话题。在实验性质的探索阶段，对时间的考量可能并不严格，但一旦考虑到响应速度，比如每秒需要处理多少个token来执行动作，成本问题就变得尤为突出。因为模型需要很长时间才能完成一个Agent的输出，这意味着直接使用大模型进行推理的成本和时间的耗费都是非常高的。对此，我个人建议可以利用一些框架，如Dify或Coze，它们可以帮助构建Agent框架，并提供了丰富的工具来逐步检查生产和输出的结果。</p><p>企业部署Agent时， 可以先验证整个流程是否可行，并确保其能带来业务价值。之后，可以考虑用一些专门训练的小模型来替代流程中的关键部分，以降低成本。初期可以利用现有的资源进行探索，长期来看，通过特定化的方式进行优化和部署可以平衡成本。</p><p>观众：在部署Agent时可能会遇到哪些安全方面的问题？目前是否有一些比较成熟的工具可以用于保障Agent的安全性？同时，是否可以认为Agent的安全性主要取决于其底层大模型的安全性？</p><p>陈星亮：首先，Agent的安全性并不仅仅由大模型决定，模型安全只是一部分，还涉及应用安全和数据安全。对企业来说，对安全性的投入无论多少都不为过。无论在引入模型时，还是实际使用过程中，包括Agent框架都需要进行安全检查。例如，使用开源框架组件时需要进行安全审查，运行时需要对模型的输入输出内容进行监控，以及对应用框架进行访问控制，防止调用越权等。</p><p>在企业原有的安全体系内构建Agent的安全性会更好一些，在华为云内部，我们基于AI原生应用引擎等平台，当Agent对外提供服务或与模型进行交互时，利用内部原有的数据安全、应用安全和内容安全方面的技术，对内容进行检查和过滤。Agent的安全性需要在现有基础上，结合Agent之间的技术组件交互以及场景特有的安全要求来综合考虑和实施。</p><p>观众：面对多智能体协同框架的开源与闭源发展，应该如何选择合适的技术路线和框架，以减少试错过程并确保系统不会被行业不断的更新迭代所淘汰？</p><p>陈星亮：我认为应该分开考虑。对于Agent的开发框架，目前开源的选择比较多，都有很多可用的资源。鉴于Agent领域本身正在快速发展，选一个团队成员熟悉且操作顺手的框架，然后跟随其发展进行使用。而对于Agent的运行时环境，进入企业生产环境后，我建议使用闭源解决方案。理想的状态是，在企业现有的基础设施基础上进行必要改造，以便将Agent的运行时环境纳入统一管理和运维体系中，确保运行时的稳定性和安全性。</p><p>刘智勇：我更倾向于观察一个技术方案是否展现出成熟和收敛的迹象，如果开始趋于稳定和收敛，那可能是着手开展相关工作的更适宜时机；如果尚未成熟，还在快速迭代和变化之中，那可能面临开发速度跟不上开源社区更新速度的问题。</p><p>马宇峰：在选择开闭源路线时，实际上需要根据所处的发展阶段来决定。不管选择何种路线，实际投入使用比纠结于何时开始尝试和如何减少错误更为关键。毕竟，随着时间的推移，技术本身会更新迭代，这些更新往往也会朝着更优化的方向发展，对业务发展带来积极的价值。</p><p></p><h2>AI Agent的未来前瞻</h2><p></p><p>马宇峰：从长远来看，企业中Agent的落地是否会对某些现有的职业造成冲击？比如普通员工、现有商业模式、提供API服务的SaaS公司以及供应商等。Agent的普及和应用会带来怎样的影响？</p><p>陈星亮：对于员工而言，随着技术的发展，未来使用Agent和大模型将成为他们需要掌握的技能，尤其是提示词。员工至少需要学会如何使用Agent，就像现在进行零代码应用开发一样，将其作为日常办公工具的一部分。对于企业，尤其是传统SaaS公司来说，Agent和大模型的引入已成为明显趋势。一些大型SaaS公司，已经开始将大模型集成到平台中，将Agent框架和集成外部大模型的能力嵌入到二次开发和应用中。传统SaaS公司如果不加入到这个发展潮流中，可能会影响产品体验，建议一定要去拥抱大模型和Agent。</p><p>刘智勇：我从两个不同的角度来探讨Agent的运用及其对未来的影响。对于企业而言，利用大模型或Agent的主要目的是提升工作效率和减少对人工的依赖。有时员工的工作效率确实无法与Agent相比，特别是在一些技术性任务上，初级工程师的编码能力可能远不及代码Agent。对于工程师来说，积极利用Agent不仅是为了保持个人竞争力，也是为了适应未来工作的需求。Agent可以作为一个强大的工具，帮助工程师完成更高效和更复杂的任务。</p><p>而具身智能特别是人形机器人，预计它们对未来世界的冲击将是巨大的，会在商业、工业和家庭三个领域中体现出来。在商业领域，许多展示和演示类的工作岗位可能会被智能机器人所取代，因为它们可能表现得更好。在工业场景中，很多重复性或技术性工作实际上可以由机器人来完成，提高生产效率和安全性。在家庭环境中，未来也可能会出现更多类型的服务机器人，帮助处理日常家务。同时，我们也应保持谨慎乐观的态度，认识到技术发展和应用普及的速度可能没有想象中那么快。机器人和Agent的发展旨在辅助人类，使我们能够专注于更有价值和创造性的工作。</p><p>马宇峰：关于Agent如何影响我们的现实世界，尤其是在工作场景中，我的感觉是需要先拥抱这些变化，然后学会适应和改变。现在可能是小企业创业的绝佳时机，因为借助大语言模型这样的“万能胶水”，不再需要像以前那样协调大量资源来进行服务能力的交付，只需要尝试不同的组合，就可以高效地为客户提供解决方案。这样，小企业的服务能力从完全定制化转变为可以大规模扩展的模式，这对于二线市场可能是一个深远的改变。</p><p>观众：英伟达使用虚拟环境训练智能体机器人的方法是否可以拓展到所有应用场景？虚拟环境数据能在多大程度上替代现实场景数据？</p><p>刘智勇：我们实际上已经使用过英伟达的Isaac Gym来训练智能体，主要是进行强化学习的训练。这种方法涉及合成不同的仿真环境，并基于此来进行强化学习的训练。这种方法的主要优势是数据是免费的，但存在一个从仿真到现实（sim-to-real）的转换gap。在应用拓展方面，特别是在本地运动（locomotion）即行走部分，使用强化学习和虚拟环境的训练模式效果是不错的。对于一些操作类的任务，也有一些积极的应用特点。但对于更广泛的操作任务，可能更倾向于使用采集到的真实数据，并利用transformer架构来训练大型的transformer模型。因为在英伟达的仿真环境中，很多物理引擎的细微数据是无法被完美仿真的，如一些非常精细的触觉反馈。</p><p>马宇峰：在内容行业，我们对AI技术的发展有着深刻的感受。有人认为，Sora的成功是因为学习到了物理世界的真实性，但随后有人指出，Sora可能只是选择了一些优秀的片段来展示。Sora的训练采用了虚幻引擎，但这种方法还是单一的，并不一定能够真正理解物理世界。这与刘老师提到的英伟达的反馈机制可能有所不同，它们的输入输出机制存在差异。目前，Sora虽然理念上很先进，但实际上还没有达到通过虚拟化的输入输出来获得真实物理引擎的效果，可能是因为模型的参数规模不够大、训练数据不够丰富，或者受虚拟数据本身的限制，还需要进一步深入观察和研究。</p><p>观众：请介绍一下目前单智能体落地的情况，以及它与公司当前技术架构的结合方式。多智能体的具体架构是如何建设的？</p><p>陈星亮：在原有的技术架构体系中，目前大家使用的较多的是Web应用、微服务，有时还会使用函数技术体系。我们可以将Agent和大模型引入进来，先进行隔离，用于特定的场景。这些场景必然会与现有的微服务体系或函数体系进行交互。这时可以采用集成的方式进行，而不是直接使用大模型的Function Call方式。这样实施难度会小一些，而且也能让Agent发挥作用。当技术团队逐渐掌握了Agent和大模型这套技术，就可以开始取代一些现有的应用。这样的过渡不仅有利于架构的演进，也有助于技术团队的能力培养。</p><p>马宇峰：多智能体协同是一个复杂而富有挑战的领域。项目中，多智能体的协同运作被分解为不同的角色，如项目管理员、编码者、产品经理等，各自承担不同的职责。然而，如果单个智能体（单Agent）的运作还没有完全搞明白，就急于发展到多智能体（多Agent）协同，其实是存在很大风险的。</p><p>在实际应用中，比较常见的模式是有两个智能体协同工作，一个负责生产，一个负责评估，但目前还没有看到这种模式带来特别显著的提升。举一个例子，情感陪伴场景中有大量的对话交互，如果一个人与一个IP进行对话，输入输出的比例可能不太理想，引入第三个智能体会带来信息量的显著提升，这在满足用户情感需求和具体任务需求时非常有用。</p><p>接下来，我们继续研讨AI Agent的未来。目前，AI Agent的进展可能在技术行业内比较流行，但还没有真正“出圈”。当AI Agent被充分使用时，哪个场景会是理想中的未来？</p><p>刘智勇：图灵测试是一个经典测试，用于评估机器是否具备人类智能，即在背靠背的情况下，判断对方是计算机还是人类。而我曾经提出过一个“面对面测试”，特别适用于人形机器人，尤其是高端的女性机器人。测试中，高端女性仿真机器人被指派到一个地点，与10位单身男性分别进行相亲，如果其中有9位男性最终发出了下一次约会的邀请，该机器人就通过了测试。这不仅考验机器人是否能够理解并执行任务，还考验它是否能够与人进行有效沟通和交流。如果机器人能够在这种面对面的互动中展现出高度的智能和亲和力，那么无论在用户交互、情感表达、行动能力还是外观颜值上，其都达到了非常高的标准。</p><p>通过这样的测试，机器人将展现出巨大的商业价值。因为当机器人在面对面互动中具有很好的亲和力时，就可以在各种职业领域中得到应用，包括教师、律师、前台接待、演艺、直播等各个领域。我认为，这种更泛化的Agent通过“相亲测试”的事件是一个标志性的里程碑，意味着AI Agent的能力和人机交互能力已经达到全新的水平。</p><p>马宇峰：Agent未来的发展趋势将是怎样的？当它们发展到一个成熟的阶段后，将会呈现出什么样的形态？</p><p>陈星亮：在企业场景中，Agent目前主要扮演辅助角色。我们正在考虑的是，Agent是否能够从围绕特定场景服务转变为围绕特定人员服务。随着这一趋势的逐步发展，我们可以设想，未来某些人的工作是否会逐渐被Agent取代，这可以在分工上进行明确划分。我认为，当Agent真正能够在企业中提升效率并降低成本时，就达到了真正的成熟阶段。</p><p>马宇峰：Agent落地过程中可能会遇到一些难以实现的场景，这就需要我们在筛选场景和逻辑执行上进行深入的思考，最终的理想状态是以人的方式来看待Agent：作为智能体能够取代当前人类的多少工作。Agent概念并非是大模型出现后才产生的，但确实又是一直存在的。智能体这个词，最常见的体现可能就是人类自己。人类可以作为Agent 选择任务难度的度量，同时也可以作为Agent的驱动方向。</p><p>如果有一天Agent真的取代了所有的工作，人类应该干什么？我想，这时人类最重要的价值就是发挥自己的需求。Agent服务的目标永远是人本身，人类有需求，才会有Agent去做这件事情。人类是需求的发起方，Agent只是去满足需求的一方。因此，人类仍然拥有最终的评价权和评估权，这种能力是大模型无论如何发展都无法达到的，除非Agent拥有像人一样的肉身，有自己的激素欲望和生理限制。</p><p>陈星亮：首先，我认为Agent的未来是充满无限可能的。无论是在各个行业，还是在ToB或ToC的体系中，人类社会有各式各样的场景需要Agent来提供支撑，因此它的发展前景是极其广阔的。其次，我相信Agent将是一个多样化的存在，无论是在技术实现还是在业务场景的应用上。目前Agent技术的发展呈现出百家争鸣的局面，这对技术行业来说是一件好事，意味着有更多的行业场景愿意尝试采用Agent，并进行投资。在这样的投入下，技术可以快速发展，进而更好地探索未知领域。</p><p>最后，在Agent向前发展的过程中，我们也需要正视现实情况。当前无论大模型还是Agent框架本身的发展，下一步的方向似乎还不是很清晰。我相信未来还会有更多新技术不断涌现，将推动Agent的发展，使企业和个人的诉求和场景得以实现。</p><p>刘智勇：从具身智能的角度来看，商业落地是一个重要议题。目前，Agent或具身智能体主要扮演的是辅助角色。以它们当前的智力水平，还不能承担替代型的角色。它们能够提升生产力，但并不能真正改变生产关系。我们应该从最大程度提升人的生产力的角度出发去寻找落地场景，这是比较实际和可行的视野。</p><p>另外是从更宏观的层面来看待Agent和具身智能的发展，这与AGI息息相关。在经历了Transformer模型、ChatGPT以及机器人的Transformer模型等重要时刻之后，我们可能在不久的将来迎来AGI的时代。具身AGI的到来会为人类社会带来新的篇章，即从碳基生命到硅基生命的延续。在具身智能领域，如果具有AGI的通用人形机器人能够实现，那么在某种程度上将实现仿生或永生的概念。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/EOxM7cd2GeS7pECMjxra</id>
            <title>527蚂蚁技术日：AI应用矩阵集体“同框”</title>
            <link>https://www.infoq.cn/article/EOxM7cd2GeS7pECMjxra</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/EOxM7cd2GeS7pECMjxra</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 May 2024 09:03:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 蚂蚁集团, 技术日, AI创新应用, 开放创新
<br>
<br>
总结: 蚂蚁集团每年5月27日举办技术日活动，展示AI创新应用产品，强调让AI像扫码支付一样便利生活，同时推动开放创新，探讨技术领域的多种可能性。 </div>
                        <hr>
                    
                    <p>每年的5月27日，是蚂蚁集团的技术日，用以勉励蚂蚁技术人保持敬畏和创新之心，到今天技术日已发展成为技术周，一个包含技术论坛、技术集市、编程大赛、技术沙龙等活动的技术嘉年华。今年，第九届蚂蚁技术日首次对外开放，开放日上展示了诸多蚂蚁AI创新应用产品，向外界透传了“让AI像扫码支付一样便利每个人的生活”的技术主张和面貌。</p><p></p><p></p><h2>AI是开放日关键词</h2><p></p><p></p><p>据了解，蚂蚁集团持续投入AI领域的研发，基于大规模业务场景的需求，布局了包括知识图谱、运筹优化、图学习、可信AI、大模型等在内的AI技术，其中蚂蚁百灵大模型已于去年底完成备案。蚂蚁集团CTO何征宇在开放日现场表示，一直以来蚂蚁集团致力于用最前沿的技术做最普惠的服务，近两年蚂蚁重投AI和数据要素技术，致力于实现AI规模化应用，让AI像扫码支付一样便利每个人的生活。</p><p></p><p><img src="https://static001.geekbang.org/infoq/62/62093c4002f76e1330c69590a689887b.jpeg" /></p><p></p><p>记者注意到，在此次开放日的技术集市上，蚂蚁AI创新应用展位前尤其热闹。</p><p></p><p>三个智能管家，是蚂蚁百灵大模型的核心创新应用产品。生活管家“支付宝智能助理”连接了400万商家机构小程序和8000项数字生活服务，食住行游购娱等生活服务均可问可办；金融管家“支小宝”能通过文字对答提供行情分析、持仓诊断、保险配置、投教科普等理财保险服务，目前使用过支小宝的用户达4300万；蚂蚁也推出了一套医疗解决方案，在不久后会正式推出服务大众的医疗健康管家。目前这套解决方案已经服务了很多医院。譬如，浙江省卫健委与支付宝联合推出了可陪诊的数字健康人“安诊儿”。浙江省内用户就医时，可感受全流程的陪伴、指引、互动，就医体验进一步提升。目前，“安诊儿”已在浙江大学医学院附属第一医院、浙江大学医学院附属邵逸夫医院等浙江省内近百家医院中应用，服务超百万人次。</p><p></p><p><img src="https://static001.geekbang.org/infoq/42/4239f10cfd69871911e01e593f9f85f2.jpeg" /></p><p></p><p>记者注意到，多模态大模型技术也在蚂蚁内部有诸多创新应用，如AI智绘、AI智乐、AIGC数字人等，用户输入一句话、一张图、一段语音，就能生成一张设计图、一段音乐作品、一段动态人像视频。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b3269137664bac2bbbd54e0350739530.jpeg" /></p><p></p><p>安全可信，是蚂蚁AI创新应用的一个特色。在集市现场，记者见到了大模型一体化安全解决方案“蚁天鉴”，可实现对大模型本身安全性的检测和防御；加速隐私计算关键硬件“乾坤卡”，能解决密码学运算慢的问题，提升全同态计算性能100倍。</p><p></p><p>蚂蚁集团CTO何征宇在交流会现场介绍，“我们相信AI不为替代人而生，是为每一个人而生。”为实现AI人人可享的目标，蚂蚁一直在努力优化和提高AI的可靠性、经济性和易用性。可靠性是用AI监督AI，以解决控制和“对齐”比人类聪明得多的模型；经济性是把大模型做“小”，这决定了大模型应用能否成为主流；易用性则是将智能“傻瓜化”，这决定了生成式AI是否能规模化落地应用。</p><p></p><p>据了解，在蚂蚁内部，AI已经成为重要生产工具。譬如，基于蚂蚁百灵大模型开发的智能研发平台CodeFuse已支持一半以上蚂蚁工程师的日常开发工作，他们提交的代码中10%由AI生成。</p><p></p><p></p><h2>开放创新是内核</h2><p></p><p></p><p>在技术开放日现场，来自学界、AI企业、开源界等诸多领域的人士共同探讨了当下最热门的话题。</p><p></p><p>在AI主题论坛上，来自智谱等AI企业和蚂蚁热烈讨论AI的多种可能性“the‘N’one”、“wherearethe‘angrybirds’？”……</p><p></p><p>蚂蚁开源嘉年华活动中，全球最大开源软件基金会之一，Apache的开源爱好者杭州本地群组走进蚂蚁，和开源爱好者、开发者们一起分享开源故事。</p><p></p><p><img src="https://static001.geekbang.org/infoq/05/0594997f0f179bb5d0d00321a0c0c4c5.jpeg" /></p><p></p><p>在接下来持续一周的技术日里，20场关于数据技术的论坛，围绕大模型、支付技术、安全科技、技术风险、隐私计算、图计算等技术领域，从主题设计上看都是直指技术工业级应用中的挑战。据了解，直面规模化应用场景中的真问题也是蚂蚁技术日一直以来的特色。</p><p></p><p><img src="https://static001.geekbang.org/infoq/57/5741d52cad8bab79d79a970f2054884c.jpeg" /></p><p></p><p>48小时黑客松是技术日期间举办的极客编程挑战赛，48小时内完成一个创意的实现，主题既可与工作、业务相关，也可以放飞奇想，做有趣、好玩的创意。参加今年黑客松的创意，大多聚焦生成式AI与给AI降本相关，如绘画AGENT、基于AI构建无障碍世界、本地化大模型工作智能提效工具等。</p><p></p><p><img src="https://static001.geekbang.org/infoq/dd/ddc825844be408b848ac77f9776d5dcb.jpeg" /></p><p></p><p>T-STAR是蚂蚁技术人年度颁奖仪式，也是蚂蚁内部最高技术荣誉，颁发给过去一年的表现杰出的技术人和技术项目。今年的十大创新奖项涵盖安全科技和分布式计算存储，AI也首次成为大赢家。在颁奖现场，蚂蚁集团CTO何征宇说，“蚂蚁技术日因敬畏而生，但敬畏不代表畏手畏脚，公司正处于全力发展的大阶段，不发展才是最大的风险。我们把T-STAR放在每年的527揭晓，就是要向大家表明，蚂蚁技术不仅要牢记历史，敬畏风险，更要用于创新、敢于突破。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/f8/f88905ade6bf48aa210cbdad4ce6ba14.jpeg" /></p><p></p><p>设置技术日，体现了一家公司对技术的重视。2015年5月27日，因光纤被挖断，部分支付宝用户两个小时无法登录账户，这次事件也成为蚂蚁历史上次重大技术事故，此后，这一天被定为蚂蚁技术日。2018年，支付宝实现了宕机后分钟级可恢复，如今，支持系统稳定安全的分布式技术已经成为蚂蚁的核心和领先的技术赛道。今年，第九届技术日首次对外开放，让外界看到了在人工智能时代，蚂蚁技术的新赛道和新可能。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/mbWEukF4hvAGPcMRloIA</id>
            <title>6个必须参加ArchSummit深圳的理由，错过等1年</title>
            <link>https://www.infoq.cn/article/mbWEukF4hvAGPcMRloIA</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/mbWEukF4hvAGPcMRloIA</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 May 2024 06:26:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: ArchSummit, 智能进阶, 大模型时代, 人工智能
<br>
<br>
总结: ArchSummit全球架构师峰会将在深圳举办，围绕“智能进阶. 架构重塑”主题讨论企业架构在大模型时代的适应性和成本效益，探讨人工智能在架构中的应用，以及架构师如何规划职业道路。BAT等企业的顶尖专家将共探AI时代的无限可能。 </div>
                        <hr>
                    
                    <p>6月14日-15日，ArchSummit全球架构师峰会将在深圳举办。本次大会围绕“智能进阶. 架构重塑”主题，将探讨在 AI 浪潮下，企业架构如何适应大模型时代趋势，同时寻找具有成本效益的问题解决方案，帮助参会者更好地了解如何以及何时可以在架构中使用人工智能，同时探讨在架构师技术知识更新换代速度快的行业常态下，如何规划职业道路，保持自身的竞争力。</p><p></p><p>届时，头部AI大模型将同台竞技，BAT实力PK。顺丰CTO耿艳坤、阿里巴巴丁宇、Thoughtworks CTO Scott Shaw等来自国内外前沿企业的顶尖专家将与参会者面对面，共探AI时代的无限可能。</p><p></p><p>大会6大看点已备好，更多参会攻略可点击链接可查看详细日程：<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">https://archsummit.infoq.cn/2024/shenzhen/schedule</a>"</p><p></p><p><img src="https://static001.geekbang.org/infoq/a8/a8efeed0960c87e2ae91cdce14eb6b86.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/RIVAfAuacRahUG9MhJkp</id>
            <title>王小川：不烧钱推AI应用、大模型价格战是云厂商的游戏</title>
            <link>https://www.infoq.cn/article/RIVAfAuacRahUG9MhJkp</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/RIVAfAuacRahUG9MhJkp</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 May 2024 10:14:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 价格战, Baichuan 4, AI助手, AGI发展路径
<br>
<br>
总结: 王小川发布了最新一代基座大模型Baichuan 4和首款AI助手“百小应”，强调了百川智能的AGI发展路径，明确不参与价格战，Baichuan 4在模型能力上达到国内第一，具备行业领先的多模态能力。王小川表示关注文本能力，发布了多款API接口，推出全新的MaaS+AaaS服务，以及针对Agent构建的Assistants API接口。同时强调了要有自己的超级应用，差异化是市场竞争力的关键。 </div>
                        <hr>
                    
                    <p>明确不参与“价格战”的王小川，在5月22日做了两个重要发布：最新一代基座大模型Baichuan 4 和成立后的首款AI助手“百小应”，同时强调了百川智能的AGI发展路径：超级模型+超级应用。</p><p>&nbsp;</p><p>“模型还没有到超级模型，应用上还没有到超级应用。”王小川在发布会开始就很明确地说道，“我们在通往AGI的路上。”但这次发布仍然对百川来说意义非凡，Baichuan 4的模型能力达到了国内第一，也正式入局了 C 端市场。</p><p>&nbsp;</p><p></p><h2>模型能力，国内第一</h2><p></p><p>&nbsp;</p><p>相较1月底发布的 Baichuan 3，Baichuan 4 在各项能力上均有极大提升，其中通用能力提升超过10%，数学和代码能力分别提升14%和9%。</p><p>&nbsp;</p><p>榜单成绩已经是各家发布大模型时的必备项目了。这次，百川分享了国内权威大模型评测机构SuperCLUE的评测结果。在最新的测试结果中，Baichuan 4的模型能力达到了国内第一。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/aa/aa1f6ed330a84bacc9809f53737893e4.png" /></p><p></p><p>&nbsp;</p><p>与国外主流大模型对比，Baichuan4在知识百科、长文本、生成创作等文科类中文任务上明显优于国外大模型。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/c0/c037fff5834687120dc87a15af5d9134.png" /></p><p></p><p>&nbsp;</p><p>此外，Baichuan 4还具备行业领先的多模态能力，在MMMU、MMBench-EN、CMMMU、MMBench-CN、MathVista等评测基准上表现优异，大幅领先Gemini Pro、Claude3-sonnet等多模态模型。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/63/633320d70ccdf66d3b267fd358839821.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>不过王小川也表示，相较多模态，百川更关注于文本能力。“多模态没有智力”，王小川解释道，多模态只是建立了一个分类的体系。而在像“1+2=3”这个等式里，智力不体现在1+2如何等于3的计算里，而是在等号里面。</p><p>&nbsp;</p><p>Baichuan 4能在较短的时间内取得突破，源于其在训练过程中引入了诸多领先技术优化手段：</p><p>&nbsp;</p><p>数据方面，在预训练阶段采用了基于Model-based+Human-based的协同数据筛选优化方法，以及对长文本建模位置编码科学的Scaling Law，有效提升了模型对数据的利用；SFT对齐方面，重点优化了模型Reasoning、Planning、Instruct following能力，通过loss驱动的数据选取与训练，多阶段爬坡，多模型参数融合等方式有效提升了模型的关键指标和稳定性；突破RLHF和RLAIF融合的RLxF强化学习对齐技术，大幅提升模型的指令遵循等能力；成本方面，提出了新的投机采样方案clover，通过将序列知识与并行解码结合，使得投机采样的命中率提升至60％，成本降低30％以上。</p><p>&nbsp;</p><p>此外，百川智能还宣布开放Baichuan 4、Baichuan3-Turbo、Baichuan3-Turbo-128k、Assistant API 四款API ：</p><p>&nbsp;</p><p><a href="https://platform.baichuan-ai.com/playground">https://platform.baichuan-ai.com/playground</a>"</p><p>&nbsp;</p><p></p><h4>“价格战是云厂商的游戏”</h4><p></p><p>&nbsp;</p><p>王小川坦言，当前 API 收入对百川而言并不多，也构不成收入重点。</p><p>&nbsp;</p><p>对于最近大模型市场打得火热的“价格战”，王小川表示，一方面，这表明了大家太看好大模型前景，不愿意失去任何机会，宁愿零价格也要进来入场；另一方面，降价的核心是要看商业模式的，如果是做ToB服务，最后售卖的不是模型本身而是整套云服务。云厂商是比较偏传统的服务模式，进到一个新的战场可以降价，但降价仅限于云厂商。在有限场景打价格战，已经走出了创业公司的射程。</p><p>&nbsp;</p><p>“这波降价跟之前的滴滴美团还不一样，因为那会儿的价格战或补贴背后带有网络效应，是双边网络，那个商业模式在改变生产关系，比如司机和乘客的关系、外卖员和用户之间的关系。而这次降价更像当初四小龙的降价方式，而不是像滴滴美团这样的价格竞争，这次不是生产关系的改变，而是直接做生产力供给。我觉得这件事情对我们而言，就是别掺和进去。”王小川说道。</p><p>&nbsp;</p><p>除了大模型发布，百川智能还推出了全新的MaaS+AaaS服务。MaaS版块由基座模型组成，分为旗舰版和专业版。旗舰版将全量开放Baichuan 4的各项能力，专业版包含Baichuan3-Turbo和Baichuan3-Turbo-128K两款模型，价格比旗舰版Baichuan 4更实惠，且对企业高频场景做了针对性优化，综合测试相比GPT-3.5整体效果提升8.9%。百川智能表示，MaaS 的新用户即日起可以获得1000万免费token。</p><p>&nbsp;</p><p>Assitants API则是百川智能在Baichuan 4基础上针对Agent构建推出的API接口，不仅支持Code interpreter、RAG内建工具，还支持自定义工具调用，方便企业接入各种丰富复杂的API。据悉，百川智能未来还将推出零代码Agent创建平台产品。</p><p>&nbsp;</p><p></p><h2>超级应用</h2><p></p><p>&nbsp;</p><p>“之前我就提到一定要有自己的超级应用，如果只是学OpenAI一开始有个模型做API服务，在中国的创业公司而言，是走不通的。”王小川说道。</p><p>&nbsp;</p><p>对于这一点，王小川的考虑有三：首先，虽然在美国做ToB是好生意，但在中国的商业环境中，C端市场就比B端大10倍；其次，做To B，收的是人民币，花的是美金；最后，大厂都会卷这件事情，这是大厂射程范围内的。因此，百川智能一定要做差异化，低价作为市场竞争力是不够的。</p><p>&nbsp;</p><p>这次，百川智能终于发布了自己的首款C端应用：“百小应”。当被质疑这款应用是不是发晚了的时候，王小川直言，“我正好觉得相反，发早了。”他认为，之前的各种应用更多是为了展示大模型能力，而到现在这个领域还没有到一个成熟的状态。</p><p>&nbsp;</p><p>王小川理想中的AI助手是有温度的，“作为伙伴，而不是工具”。他不吝惜对Kimi的赞赏，“Kimi打赏的理念，就是不是把它当工具，而是当成一个伙伴、一个人。”</p><p>&nbsp;</p><p>“懂搜索、会提问”是百川给百小应总结的有别于其他AI助手的地方：</p><p>&nbsp;</p><p>“懂搜索”意味着不是像其他搜索一样做汇总，而是让模型掌握专业的搜索技能，用户在搜索的时候理解意图、可以拆词作多轮搜索。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/77/77a416611b4f0dc130706d819cf26ac3.png" /></p><p></p><p>&nbsp;</p><p>“会提问”则是让模型能引导用户表达自己的需求。这是由于当前很多人在搜索时表达只有关键词、不够准确，机器只能去猜，现在百川则是通过让机器反问的方式让用户表达出来，比如：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/0e/0e44e938f621a835783a00c4c22ef6a9.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>除了强大的搜索和提问功能以外，用户还能在百小应中上传PDF、word文档，或者直接输入网页链接（URL），阅读并分析书籍、报告、学术论文等长篇内容，百小应一分钟就能读完上市公司财报。</p><p>&nbsp;</p><p>在Baichuan 4多模态能力的支持下，用户提问的同时还可以同步上传图片，百小应对图片内容进行解读，或者将图片作为补充材料，获取更精准的回答。此外，它还支持用户通过语音的方式进行交互。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/32/3235c04e1c07d8300470f186f525a483.jpeg" /></p><p></p><p>&nbsp;</p><p>王小川表示，百小应发布后不会为了获客疯狂投广告，“这并不是健康的行为。”“我们手上有足够的资金，不会通过立马发布一个产品去寻找下一轮融资。我们的精力会放在产品价值上。”</p><p>&nbsp;</p><p>对于商业模式，王小川认为现在还不到时候。“以Kimi为例，虽然现在有足够的用户体量，不管是走流量模式还是广告模式都有意义，但其实没有本质的区别。”具体的变现时机，要看一个场景下多少用户有付费意愿。</p><p>&nbsp;</p><p>当然，现在的百小应并不是王小川心目中的超级应用，“今天百万级DAU都远远称不上一个超级应用至少要再提升两个数量级。”王小川预计，百小应今年内会有大的升级变化。</p><p>&nbsp;</p><p>“同一代模型中，百川更有机会把超级应用做出来。”王小川说道。显然，百川智能对超级模型和超级应用两个赛道都非常有信心，“百川未来可能不再按月发布，而是按季度发布，团队要把时间调到长线来做事情。”王小川说道。</p><p>&nbsp;</p><p>未来百川智能将如何实现自己的AGI，我们拭目以待。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/wl9bMYS5WRCUFr9ZsjqA</id>
            <title>大语言模型加持，是智能运维架构的未来吗？</title>
            <link>https://www.infoq.cn/article/wl9bMYS5WRCUFr9ZsjqA</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/wl9bMYS5WRCUFr9ZsjqA</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 May 2024 08:59:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 智能运维架构, 大语言模型, SRE-Copilot框架, AIOps挑战赛
<br>
<br>
总结: 在QCon北京2024大会上，字节跳动技术专家王宁分享了基于大语言模型的智能运维架构SRE-Copilot框架的演讲。该框架结合了大语言模型和AIOps实践，通过模拟不同团队的运维知识来解决技术问题，降低训练成本，提升故障处理能力，并支持自然语言交互，为智能运维带来新的可能性。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>演讲嘉宾&nbsp;|&nbsp;王宁&nbsp;字节跳动技术专家策划&nbsp;|&nbsp;蔡芳芳整理&nbsp;|&nbsp;Penny编辑&nbsp;|&nbsp;褚杏娟、傅宇琪</blockquote><p></p><p></p><p>在&nbsp;QCon&nbsp;北京&nbsp;2024&nbsp;大会上，字节跳动技术专家王宁，根据自己在字节的实践经历，发表了题为《<a href="https://qcon.infoq.cn/2024/beijing/presentation/5740">SRE－Copliot：基于大语言模型的智能运维架构</a>"》的演讲。</p><p></p><p>本文由InfoQ整理，经王宁老师授权发布。以下为演讲实录。</p><p></p><p>随着大语言模型的广泛应用和能力的提升，许多团队都在尝试利用大语言模型来改进他们所在行业的应用，我们团队在字节跳动内部也在探索如何将大语言模型与&nbsp;AIOps&nbsp;实践相结合。</p><p></p><p>去年，我们带着&nbsp;SRE-Copilot&nbsp;框架参加了&nbsp;AIOps&nbsp;2023&nbsp;挑战赛，并荣幸地获得了冠军。在比赛中，我们设计了一套更为开放和富有想象力的框架，进行了初步的探索。</p><p></p><p>比赛的题目是开放性的，旨在鼓励大家尽可能地探索大语言模型在智能运维领域的应用潜力。选手面临的挑战是帮助企业运维团队应对日益庞大的系统规模、复杂的系统结构以及日益增多的数据量。</p><p></p><p>比赛所用的数据来自中国建设银行“建行生活”APP中的真实模拟数据。比赛的架构设计模拟了从入口负载均衡集群到中间的基础集群，如抢券集群和订单集群，这些集群之间相互依赖。此外，还包括了出口负载均衡集群，并且依赖许多复杂中间件如&nbsp;Redis、MySQL、Kafka&nbsp;等的各个集群。</p><p></p><p>我们在比赛中获得的数据包括调用链数据、业务黄金指标，例如订单成功率、抢券成功率，系统交易的每个订单的延迟时间，以及集群的性能指标，不仅涵盖了基础集群，还有中间件集群的监控数据，如&nbsp;CPU&nbsp;负载、常见的出入网流量，系统日志数据等。</p><p><img src="https://static001.geekbang.org/infoq/08/08436d00c6950fe9cbaff7d3c9890161.png" /></p><p></p><p></p><h2>为什么需要SRE-Copilot&nbsp;框架</h2><p></p><p></p><p>随着系统日益复杂和数据量的增加，即使是专业的运维团队也难以全面掌握所有技术细节。在大型企业中，每个组件，如计算、存储、数据库，都可能需要专门的运维团队。当出现大规模故障时，通常需要所有团队协作来定位问题根源。而&nbsp;SRE-Copilot&nbsp;框架可以通过大语言模型的能力，学习和模拟不同团队的运维知识，以解决整个链路上的技术问题。</p><p></p><p>传统的&nbsp;AIOps&nbsp;在异常检测和根因分析上严重依赖于标注数据，这限制了算法的泛化能力，因为它们需要在有监督的环境下进行训练。而大语言模型能够学习更多的通用知识，减少对标注数据的依赖，从而降低训练成本。</p><p></p><p>运维团队积累的专家经验很难编码到算法模型中。通常，这些经验会被简化为阈值或复杂的规则，不仅难以维护，也难以传承。SRE-Copilot&nbsp;框架通过大语言模型，将专家经验转化为模型可以理解和推理的形式，从而提升了故障处理的能力。</p><p></p><p>传统&nbsp;AIOps&nbsp;的接入和维护成本较高，需要业务和算法团队深入理解业务逻辑和算法模型。此外，私域数据的处理和定制化开发也增加了成本。SRE-Copilot&nbsp;框架采用集成学习的概念，通过模块化设计，使得系统能够像搭积木一样动态编排。</p><p></p><p>在传统&nbsp;AIOps中，未遇到过的故障很难被解决，因为它们超出了模型的训练范围。大语言模型展现出了强大的推理能力，能够基于通用知识和训练中学到的关键字，推断出未知故障的性质，即使没有相似的训练数据。</p><p></p><p>传统的&nbsp;AIOps&nbsp;解决方案需要用户理解模型并精确地传递参数，而&nbsp;SRE-Copilot&nbsp;框架支持自然语言交互，使得非技术用户也能轻松地与系统交互，提高了用户体验，并有潜力开放给更广泛的用户群体。</p><p></p><p>SRE-Copilot&nbsp;框架，采用了基于&nbsp;function&nbsp;call（函数调用）的方法来实现多功能的智能运维。</p><p></p><p>我们首先将&nbsp;SRE&nbsp;在日常运维中可能遇到的多种场景进行了统一收敛，通过大语言模型来理解用户的意图。无论是进行运维可视化还是故障分类，我们的目标是编排不同的工具，以实现一个多场景的智能运维解决方案。</p><p></p><p>在比赛上，我集成了一些简单的场景，虽然每个场景本身并不复杂，但我们尽可能探索了许多新的场景，以测试和展示&nbsp;SRE-Copilot&nbsp;的多功能性。比如我们实现了故障分类功能，并让框架能够生成故障自愈的代码，以自动化处理常见的问题。</p><p></p><p></p><h2>实践效果如何？</h2><p></p><p></p><p>我利用大语言模型生成了一个排查故障的工作流，并以自然语言的形式呈现。</p><p></p><p>根据提前设定的&nbsp;Agent，模型会匹配并确定哪些&nbsp;Agent&nbsp;能够在排障工作流程中发挥作用，然后将这些&nbsp;Agent&nbsp;编排成一个可执行的工作流。我可以通过自然语言向模型提问，比如询问特定时间段内的问题。模型会提取相关参数，并动态地将任务分配给相应的子&nbsp;Agent。每个子&nbsp;Agent&nbsp;会检查自己的数据，寻找故障迹象。</p><p></p><p>例如，调用链&nbsp;Agent&nbsp;会检查调用链是否存在问题。如果检测到&nbsp;CMDB（配置管理数据库）中的信息，模型可以进行下钻操作，比如定位到具体的集群，然后触发下一轮更深入的检测。如果没有更多的信息，模型会开始进行根因诊断，检索历史故障和专家经验，以此来判断当前的故障类型，比如磁盘写满。</p><p></p><p>在诊断过程中，我会将相关指标进行可视化展示，帮助理解故障的性质。根据诊断结果，我会生成清理磁盘的简单代码。如果是线上执行，模型会匹配预设的自愈方案。每次故障诊断完成后，会自动生成告警总结和复盘报告。诊断的故障信息会自动积累下来。如果用户对诊断结果进行确认，那么这个故障案例就会被记录到历史故障库中，供模型未来诊断时参考。</p><p></p><p>在比赛的复现过程中，我们在建行云的&nbsp;8&nbsp;核和&nbsp;16G&nbsp;内存的堡垒机上进行了演示，并使用了一张&nbsp;V100&nbsp;显卡。实际上，我们仅使用了开源的&nbsp;ChatGLM&nbsp;6B&nbsp;的小模型，就实现了上述效果。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b12c49c627aefbd1ebb7f1bdbb33a961.png" /></p><p></p><p></p><h2>SRE-Copilot&nbsp;架构</h2><p></p><p></p><p></p><h4>Tool&nbsp;calling</h4><p></p><p></p><p>在&nbsp;SRE-Copilot&nbsp;架构中，Tool&nbsp;calling&nbsp;是指将大语言模型与外部工具有效结合或交互的能力。这种机制允许模型解决它自身无法直接回答的问题，因为它可以调用外部工具来获取所需的信息。</p><p></p><p>以询问天气为例，大语言模型本身无法提供实时天气信息，因为它缺乏对外部环境的感知。为了解决这个问题，需要设计一个封装用户问题的方法，并提供一个能够获取当前天气的接口工具，再通过一些描述指导模型使用接口。</p><p></p><p>例如，接口可以根据给定的地点查询天气，它是一个&nbsp;function，需要两个参数：地点（location）和温度单位（如摄氏度或华氏度）。成熟的大语言模型，如&nbsp;ChatGPT&nbsp;或文心一言，通常都是通过相似的训练方法来实现这一机制。模型会调用适当的函数，并且能够从用户的问题中提取出必要的参数。假设用户询问的是伦敦的天气，模型会自动将“伦敦”作为&nbsp;location&nbsp;参数补全，并指导调用天气查询接口，从而提供准确的天气信息。</p><p></p><p></p><h4>RAG</h4><p></p><p></p><p>SRE-Copilot&nbsp;架构中的另一个重要概念是检索增强生成（Retrieval-Augmented&nbsp;Generation，简称RAG）。RAG&nbsp;结合了搜索技术和大语言模型，旨在解决几个关键问题。</p><p></p><p>首先，它可以帮助处理私域知识，例如公司内部服务器的状态或特定订单的详情，这些信息是大语言模型无法直接感知的。</p><p></p><p>RAG&nbsp;能够解决新知识的问题。以电影为例，如果模型的知识截止日期是&nbsp;4&nbsp;月，那么它不会知道&nbsp;5&nbsp;月上映电影的主演是谁。同样，对于当天发生的新故障或新闻，模型也无法感知。</p><p></p><p>RAG&nbsp;也适用于长尾问题，即那些在模型训练时不常见或非常具体的问题。例如，如果用户需要编写某个小众语言的代码，或者询问关于特定数据库连接的问题，尤其是当这个数据库是公司内部改造过的，传统的大语言模型可能无法提供答案。为了解决这些问题，可以利用传统的检索方法，比如搜索公司内部文档，查找是否有关于特定语言连接数据库的&nbsp;QA&nbsp;文档。一旦检索到相关文档，我会将文档中的相关内容与用户的问题一起输入给大语言模型，以便模型能够提供一个更准确的答案。</p><p></p><p></p><h4>Reason+Act</h4><p></p><p></p><p>ReAct&nbsp;概念针对的是那些无法仅通过一步查询或大语言模型自身直接解决的任务。这些任务通常需要多步骤的执行，并且每一步都需要模型提供其思考过程，以减少模型产生幻觉现象的风险。</p><p></p><p>通过&nbsp;ReAct，我们可以将任务的每一步规划和执行可视化。例如，如果任务是让大语言模型去厨房做菜，模型可能会首先思考需要哪些调料，并预测它们可能存放的位置。比如，模型可能会推断出胡椒可能在&nbsp;1&nbsp;到&nbsp;6&nbsp;号柜子里，或者&nbsp;1&nbsp;到&nbsp;3&nbsp;号台面上。模型首先会选择检查第一个柜子，如果没有找到胡椒，它会根据这一步骤的结果，决定下一步检查其他柜子。</p><p></p><p>再比如，当用户询问&nbsp;GitHub&nbsp;上某位开发者获得的点赞数并想要得到该数值除以&nbsp;2&nbsp;的结果时，模型的第一步是通过&nbsp;GitHub&nbsp;的&nbsp;Open&nbsp;API&nbsp;获取点赞数，然后使用计算器工具进行除法运算，通过这两步操作来得到最终结果。</p><p></p><p>ReAct&nbsp;概念旨在通过分步骤地规划，并在每一步中进行合理的推理和决策，来执行越来越复杂的任务。</p><p></p><p></p><h4>Agent&nbsp;智能体</h4><p></p><p></p><p>智能体主要由三个部分组成：</p><p>大脑：最核心的部分是大语言模型，它扮演着智能体的大脑角色，负责处理和生成语言输出，同时整合和利用知识库中的信息。感知：智能体的第二部分是它的感知能力，这涉及到使用外部工具来感知图片、声音等信息。通过这些工具，智能体能够与外部世界进行交互，获取必要的数据。行动：智能体还可以通过计算&nbsp;API、查询&nbsp;API&nbsp;等工具进行实际的交互操作，执行真实世界中的行动。这使得智能体能够处理更为复杂的任务，如根据图片内容推断位置并查询当地天气。</p><p></p><p>我们期望实现的是多个智能体之间的复杂交互，让智能体不仅能够独立工作，还能够相互协作，共同解决更加复杂的问题。</p><p></p><p></p><h4>架构设计思路</h4><p></p><p></p><p>在&nbsp;SRE-Copilot&nbsp;框架的具体实现上，我根据比赛时提供的数据，在最底层为每个数据源设计了相应的&nbsp;Agent。这些数据源包括日志、调用链、交易类型数据、主机监控数据以及&nbsp;CMDB&nbsp;数据等多元数据类型。对于每种类型的数据都设计了一个&nbsp;Agent，使其能够进行异常检测、数据可视化，以及查询历史故障和返回故障描述等操作。</p><p></p><p>在底层&nbsp;Agent&nbsp;之上，我定义了功能型&nbsp;Agent，它包括知识库问答、工作流规划、故障报告编写和代码生成等能力。这些功能型&nbsp;Agent&nbsp;能够基于&nbsp;ReAct&nbsp;框架，调用底层的子&nbsp;Agent，并动态地编排它们的执行顺序和结果。</p><p></p><p>在框架的最顶层，我设计了一个名为&nbsp;Copilot&nbsp;的智能体，它作为与人类用户交互的核心。Copilot&nbsp;负责识别用户的意图，判断用户是想要获取答案还是进行故障诊断，并从用户的问题中提取出相关参数。</p><p></p><p>例如，用户可能询问某个具体时间段的情况，Copilot&nbsp;可以将这个时间段作为参数传递给后续的处理流程。随着大语言模型的智能化，它甚至能够理解并处理如“过去&nbsp;15&nbsp;分钟”这样模糊的时间段，并将其转换为具体的参数。顶层&nbsp;Copilot&nbsp;还能够进行任务分配和&nbsp;Agent&nbsp;之间的协调工作，确保整个框架能够高效地响应用户的需求。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/4e/4e989cfcbd307002b6b49ee8341b83e3.png" /></p><p></p><p></p><p>在设计&nbsp;SRE-Copilot&nbsp;框架时，我借鉴了&nbsp;GPT&nbsp;背后的集成学习思想。我们的目标并不是开发一个庞大而全面的模型来解决所有问题，而是通过集成多个专精于特定领域的子&nbsp;Agent&nbsp;来实现。每个子&nbsp;Agent&nbsp;都在其专业领域内表现出色，我们采用混合专家系统（Moe,&nbsp;Mixture&nbsp;of&nbsp;Experts）的形式，使整个系统的效果更加完善和高效。</p><p></p><h2>SRE-Copilot&nbsp;技术细节</h2><p></p><p></p><p></p><h4>主要运维能力：异常检测</h4><p></p><p></p><p>首先介绍一个异常检测场景，以下是我们在这个场景中定义的角色：</p><p>Copilot&nbsp;主持人：作为核心，Copilot&nbsp;负责解析用户需求，制定运维计划，并安排不同&nbsp;Agent&nbsp;执行具体任务。多数据源&nbsp;Agent：针对不同的数据源，如日志、调用链、交易类型数据、主机监控数据以及&nbsp;CMDB&nbsp;数据，我们设计了专门的&nbsp;Agent。这些&nbsp;Agent&nbsp;利用合适的算法对各自模态的数据进行异常检测和检索。RCA&nbsp;Agent：该&nbsp;Agent&nbsp;负责收集其他&nbsp;Agent&nbsp;的检测结果，并执行最终的根因分析推理。</p><p></p><p>以&nbsp;Copilot&nbsp;为起点，例如，当收到用户关于&nbsp;9&nbsp;月&nbsp;18&nbsp;日下午&nbsp;4&nbsp;点大量交易失败的请求时，Copilot&nbsp;会识别这是一个根因诊断问题，并将任务交给&nbsp;RCA&nbsp;Agent。</p><p></p><p>RCA&nbsp;Agent&nbsp;会关注用户请求中的关键信息，如交易类数据，并将其传递给交易类型&nbsp;Agent进行初步检查。该&nbsp;Agent&nbsp;将返回关于交易量同比下降的结果，但不提供额外信息。</p><p></p><p>根据初步检查结果，系统会动态调度其他&nbsp;Agent&nbsp;检查各自负责的组件是否存在问题。例如，当轮到第五个&nbsp;Agent，即调用链&nbsp;Agent&nbsp;时，它发现了调用链上的异常情况。调用链&nbsp;Agent&nbsp;的反馈将引导&nbsp;CMDB&nbsp;Agent&nbsp;在&nbsp;CMDB&nbsp;中查询接口的调用记录，上游下游关系，以及接口的具体问题。</p><p></p><p>通过这种方式，我们模拟了真实线上云平台中多个组件之间的协同定位过程。整个异常检测过程完全自动化，没有一个固定的流程，而是根据每个&nbsp;Agent&nbsp;的检测结果动态地调度其他&nbsp;Agent，共同完成异常检测任务。</p><p></p><h4>主要运维能力：根因定位</h4><p></p><p></p><p>有了上一步的异常检测结果，接下来需要对这个异常检测的结果做根因定位，或者说故障的分类。</p><p></p><p>首先，获取异常检测组件的结果，并对其进行了一些调整，使其返回的不仅是传统的&nbsp;true/false&nbsp;或&nbsp;JSON&nbsp;结构体，而是大语言模型和人类都容易理解的自然语言描述。例如，异常检测结构体可能会返回：“本次故障持续了&nbsp;10&nbsp;分钟，CPU&nbsp;指标飙升，内存也被打满，某某接口出现了大量失败”。</p><p></p><p>然后，这个故障工单会转换成向量，并在提前构建的向量数据库中检索。向量数据库包含两部分内容：一部分是提前配置的专家经验，另一部分是历史上遇到的相似故障记录。例如，如果历史上遇到过内存打满的问题，通过重启服务解决了问题，这样的专家经验会被记录在系统中。</p><p></p><p>在当前故障检测时，系统会同时检索出相关的专家经验和历史故障信息。通过&nbsp;RAG，将当前故障描述、专家经验和历史故障信息一起输入给大语言模型，进行根因推断。大模型会根据提供的信息推理，例如，这是否是内存打满的问题，是否需要通过重启服务来解决。</p><p></p><p>RAG&nbsp;方法可以帮助大语言模型不断地更新其知识库，适应新的故障情况，同时减少模型的幻觉现象。通过引入历史故障数据和反思机制，模型的准确率得到提升，幻觉现象得到降低。即使在没有提前配置专家经验或历史故障数据的情况下，使用的&nbsp;6B&nbsp;大小的模型（如ChatGLM&nbsp;3）仍能够对某些类型的故障做出准确的推理，如磁盘写满或&nbsp;Java&nbsp;GC&nbsp;问题。</p><p></p><h2>字节的实践探索</h2><p></p><p></p><p></p><h4>基于&nbsp;LLM&nbsp;的&nbsp;RCA-Agent&nbsp;构建</h4><p></p><p></p><p>字节跳动内部的目标是先将基于大语言模型的根因诊断（RCA）Agent&nbsp;框架落地应用，因为根因诊断是&nbsp;SRE&nbsp;团队面临的一个主要挑战，它占用了大量的时间和精力，日常的&nbsp;On&nbsp;Call&nbsp;问题定位也给团队成员带来了沉重的负担。我们希望专注于解决这些实际问题，真正缓解&nbsp;SRE&nbsp;同事的痛点。</p><p></p><p>我们定义了一些工具和插件，是在出现故障时用来进行检测的工具。除了工具和插件，我们还设计了工作流编排，以自动化和优化故障处理流程。我们构建了一个知识库，它包含了历史故障数据、专家经验和故障处理策略，这些都是进行有效根因分析的关键资源。</p><p></p><p></p><h4>知识库的构建</h4><p></p><p></p><p>构建知识库方面所做的工作主要包括以下几个部分，并且我们计划未来会引入更多用户原始文档、历史&nbsp;On&nbsp;Call&nbsp;记录等不同类型的数据。</p><p></p><p>排障专家经验：这部分是根据根因诊断的场景特别设计的，目的是让业务团队的成员能够管理和记录他们的知识和经验。我们定义的每一个经验都是一组根因故障，包括故障发生时的描述和一些止损措施的组合。这些信息将被用来训练大语言模型推理。</p><p></p><p>例如，流量突增导致的故障，其根因可能是用户&nbsp;QPS&nbsp;的突增。故障的表现可能是流量首先突增，随后内存和&nbsp;CPU&nbsp;使用率也跟着上升，最终导致服务不可用。这种描述将帮助模型理解故障模式。对于上述故障，可能的止损措施包括重启服务或进行扩容操作。</p><p></p><p>故障场景&nbsp;SOP&nbsp;文档：我们希望用户输入的是一些&nbsp;SOP&nbsp;文档。这种方式给组件团队提供一种灵活管理知识的方法。我们选择这种半规范化文档的形式，是因为当前大语言模型的能力还有局限，需要通过文档梳理来帮助模型更好地理解。</p><p></p><p>历史故障信息：我们还维护了一个历史故障信息库，记录每一次通过大语言模型检测到的故障，这些记录会用来对组件团队进行训练和打标。</p><p></p><p></p><h4>基础工具的构建</h4><p></p><p></p><p>在构建&nbsp;SRE-Copilot&nbsp;框架的基础工具方面，我们参考了&nbsp;OpenAI&nbsp;GPTs&nbsp;将工具集成到平台时所遵循的规范。我们将运维场景中的一些关键指标和基础工具进行了统一管理，把传统的异常检测方法统一成一个工具，用户只需要维护他们需要进行异常检测的指标即可。</p><p></p><p>用户可以自定义检测项，包括指标名称、指标的标签或指标描述，以及定义何为异常表现。因为是用户自定义的工具，所以可以根据具体需求设置检测标准。</p><p></p><p>我们实现了一个变更事件查询工具，当出现故障时，用户可以通过调用这个接口来确定是否由线上变更导致。我们在平台上部署的组件配置了一些工具，例如异常检测、变更和事件查询等，还包括了自然语言的意图理解和大语言模型的根因推理功能。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d193671ffeaa6714378964ace886acc9.png" /></p><p></p><p></p><h4>核心工具：LLM&nbsp;根因推理</h4><p></p><p></p><p>关键的根因推理功能基于大语言模型。在实践中，我采用了一种新的方法，不再将故障数据压缩成向量空间进行聚类分析，而是利用大语言模型将故障映射到自然语言空间中，从而简化了故障分类过程。</p><p></p><p>例如，流量突增、内存升高和接口不可用等信息可以自然地描述为&nbsp;QPS&nbsp;问题。对于新出现的具有相同描述的故障，可以直接使用大语言模型进行分类，而无需计算向量空间中的相似度。</p><p></p><p>分类过程中会使用用户配置的专家经验和当前故障的检测结果。此外，工具还利用预定义的模板生成描述，并让大语言模型进行推断，而用户可以根据需要调整这些模板、专家经验和检测结果。</p><p></p><p>为了提高根因推断的准确性，确保检测项插件的描述足够具体，比如详细描述指标曲线的形态变化，以及变更检测组件提供的详细信息，如变更的范围和性质。这样的方法不仅提高了故障分类的效率，还能让大语言模型的推理过程更加精确。</p><p></p><p></p><h4>工作流的构建</h4><p></p><p></p><p>下一步是构建工作流，目前这一过程仍然需要用户自行配置，这主要是由于大语言模型当前能力的限制所做出的妥协。</p><p></p><p>不过，我们正在探索一种新的方法，即允许用户在其&nbsp;SOP&nbsp;文档中预先设定工作流，例如，文档中可以指明首先需要检查哪些指标，以及根据这些指标的结果接下来应该检查哪些指标。</p><p></p><p>我们希望能够训练大语言模型，使其能够直接根据用户的&nbsp;SOP&nbsp;文档生成工作流。最终，SRE&nbsp;团队能够向大语言模型提供一个简单的文档，甚至是未经格式化的文本，而模型能够根据文档中的指标或检测项动态地编排诊断步骤，并根据每一步的检测结果，智能地调度后续的执行流程。</p><p></p><p></p><h4>Agent&nbsp;的应用与调试</h4><p></p><p></p><p>我们在一些组件上进行了&nbsp;RCA&nbsp;的试点工作，实现了一些改变传统交互方式的效果。现在，用户可以通过提出模糊的问题来与系统交互，例如询问某个集群存在哪些故障，系统会自动识别并调用相应的集群诊断工作流。</p><p></p><p>完成诊断后，系统不仅能够将结果进行美化或封装，还能以卡片或自然语言的形式向用户直观展示。此外，在面对大规模故障时，系统能够自动解析故障群中的告警卡片信息或历史消息，判断需要诊断的参数，如特定集群和时间段，自动提取所需参数，并触发相应的诊断流程。</p><p></p><p></p><h2>未来展望</h2><p></p><p></p><p>针对我们遇到的瓶颈，我认为未来需要继续在下面几个方向增强。</p><p></p><p>第一，我期望有越来越大、能力越来越强的通用大语言模型，因为随着这些模型能力的不断提升，在动态根因推理和每一步的动态决策上，它们的准确率和效果都将得到显著提升。</p><p></p><p>第二，我期待能够开发出更强大、更专业的模型，例如时序大模型或日志大模型，并将这些专业模型作为工具，供核心Agent调用。</p><p></p><p>第三，我正在探索是否能有更灵活的Agent框架，以支持多轮和更灵活的对话，比如让用户在排障过程中随时打断并提供关键信息，以此缩短故障排查的平均时间。实现多人或多Agent之间的真正协作。</p><p></p><p>第四，我期望Agent能在更丰富的场景中应用，目前它们可以像实习生一样帮助进行简单的监控数据处理，未来随着模型和工具能力的增强，我希望它们能够发展到像初级员工或应届生那样进行一些简单的决策，甚至最终成为一个资深专家，能够自动诊断问题并执行高级决策。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/QwgwNwRZTX4o10fh64yU</id>
            <title>老黄急了！为跟华为抢业务，英伟达也得低头降价，但大家已经不买账了？</title>
            <link>https://www.infoq.cn/article/QwgwNwRZTX4o10fh64yU</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/QwgwNwRZTX4o10fh64yU</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 May 2024 07:08:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 芯片产品, 价格下调, 竞争压力, 制裁限制
<br>
<br>
总结: 英伟达在中国市场下调芯片产品价格，面临竞争压力和制裁限制，导致销售疲软，需应对中国市场挑战。 </div>
                        <hr>
                    
                    <p>整理&nbsp;|&nbsp;华卫</p><p></p><p>5&nbsp;月&nbsp;24&nbsp;日，根据路透社的报道，英伟达下调了中国特供版芯片产品的价格，H20&nbsp;芯片现在的售价比华为的Ascend&nbsp;910B便宜&nbsp;10%&nbsp;以上。</p><p></p><p>知情人士透露，英伟达在与华为的中国市场芯片争夺战中销售疲软，性能问题、充足的供应和制裁限制，迫使其定价低于华为的芯片产品。中国日益增长的竞争压力，给这家美国半导体设计公司的投资者也敲响了警钟。</p><p></p><p>在美国对人工智能芯片出口实施制裁和竞争加剧的背景下，价格下行凸显了英伟达中国业务面临的挑战，为其在中国市场的前景蒙上了一层阴影，而这一市场对英伟达2024财年营收的贡献率高达17%。</p><p></p><h2>价格从略低到更低，但买家大多倒向华为？</h2><p></p><p></p><p>去年年底，英伟达推出了三款为中国量身定制的芯片，分别是HGX&nbsp;H20、L20&nbsp;PCle和L2&nbsp;PCle。三款新产品都是从H100的基础版修改而来，包含了英伟达用于人工智能工作的大部分最新功能，但一些计算能力被削减以符合美国新规。</p><p></p><p>三款芯片中，H20最受关注，因为它是英伟达可在中国销售的性能最强大的产品。但三位供应链消息人士透露，市场上该芯片的供应充足，反而表明对其需求的疲软。</p><p></p><p>据悉，H20芯片刚开始接受分销商的预订时，定价还与华为的竞争产品相当。短短几月，情况就发生了变化。今年&nbsp;2&nbsp;月，H20芯片每张定价约在1.2万~1.5万美元区间（约8.6~11万人民币），略低于华为Ascend&nbsp;910B约12万元的售价；分销商提供的预装了&nbsp;8&nbsp;块&nbsp;AI&nbsp;芯片的&nbsp;H20&nbsp;服务器售价为&nbsp;140&nbsp;万元人民币。</p><p></p><p>有消息人士表示，在某些情况下，H20芯片价格会以低于华为Ascend&nbsp;910B（华为公司最强大的AI芯片）售价的10%以上。</p><p></p><p>中国的服务器分销商正在以每张卡约10万元人民币的价格出售H20，八卡服务器每台售价约为&nbsp;110&nbsp;万元~130&nbsp;万元人民币。相比之下，分销商以每张卡12万元以上的价格出售华为910B，而其八卡服务器的起价为&nbsp;130-150&nbsp;万元/台。“H20&nbsp;和华为&nbsp;910B&nbsp;的价格也会根据订单的规模而波动。”消息人士补充说。</p><p></p><p>当时，一位消息人士指出，就规格而言，H20&nbsp;在&nbsp;FP32&nbsp;性能方面似乎落后于&nbsp;910B，而&nbsp;FP32&nbsp;是衡量芯片处理普通任务速度的关键指标，H20&nbsp;的&nbsp;FP32&nbsp;性能还不到其竞争对手的一半。但&nbsp;H20&nbsp;在互联速度方面似乎比&nbsp;910B&nbsp;更有优势，互联速度是衡量芯片间数据传输的速度。“这意味着在需要将大量芯片连接起来作为一个系统工作的应用中，H20&nbsp;仍能与&nbsp;910B&nbsp;竞争。”</p><p></p><p>有分析师表示，H20&nbsp;芯片的表现将是影响英伟达中国业务的一个重要因素，而长期前景则取决于其如何与本土科技巨头华为竞争。虽然华为去年才开始与英伟达竞争，但消息人士称，今年华为将大幅增加其Ascend&nbsp;910B芯片的出货量，该芯片在某些关键指标上优于H20。</p><p></p><p>根据路透社对现有政府采购数据的核查显示，在过去六个月中，仅有五家国有或国有附属企业买家表示有兴趣购买H20芯片，而同期有十多家买家表示有兴趣购买华为的910B芯片。不过，这些数据并不详尽，可能无法反映市场需求的全部范围。</p><p></p><p>对比在中国的市场处境，英伟达在国外仍然保持发展势头。5&nbsp;月&nbsp;25&nbsp;日，据The&nbsp;Information报道，埃隆·马斯克本月告诉投资者，其初创公司&nbsp;xAI&nbsp;计划在&nbsp;2025&nbsp;年秋季之前建造一台超级计算机，它将依靠数万个英伟达&nbsp;H100&nbsp;芯片，以为未来更智能的&nbsp;Grok&nbsp;聊天机器人提供动力。马斯克将这台超级计算机称为“计算超级工厂”，耗资数十亿美元建造。</p><p></p><h2>制裁所带来的利润挤压</h2><p></p><p>由于美国旨在限制中国成为科技强国的制裁，英伟达的&nbsp;H800&nbsp;和&nbsp;A800&nbsp;在中国被禁售，包括&nbsp;H100&nbsp;和&nbsp;B100&nbsp;在内的其他先进产品线也已被禁。之后，英伟达针对中国市场推出了新产品，并调整了其人工智能芯片战略以应对制裁令。</p><p></p><p>在英伟达的第一季度财报中，该公司的高管们警告称，受制裁影响，他们在中国的业务成绩已“大幅”低于过去。英伟达的首席财务官Colette&nbsp;Kress&nbsp;指出，“相比10月实施新的出口管制许可之前的水平来说，我们在中国的数据中心收入大幅下降，预计未来中国市场的竞争仍将非常激烈。”</p><p></p><p>消息人士称，H20&nbsp;芯片上个月开始在中国广泛销售，一个多月后就交付给了客户。中国的一些科技公司已经下了订单，其中阿里巴巴订购了3万多块H20芯片。</p><p></p><p>研究机构SemiAnalysis的创始人Dylan&nbsp;Patel表示，到2024年下半年，将有近100万颗H20芯片运往中国，英伟达必须在定价上与华为竞争。</p><p></p><p>“尽管H20&nbsp;的售价是&nbsp;H100（2022&nbsp;年禁止出口到中国英伟达人工智能芯片）的一半，但由于更高的内存容量，其制造成本高于&nbsp;H100。这是利润率的急剧下降。”Patel&nbsp;表示。</p><p></p><h2>英伟达：做最坏的打算</h2><p></p><p></p><p>在英伟达降价之际，其&nbsp;H20芯片或许还迎来在中国取得成功的另一大“绊脚石”：除了制裁和由此产生的微薄利润外，英伟达还必须应对中国政府鼓励企业支持国产芯片发展的指令问题。尽管上述三位消息人士中的两位表示，近几个月来这些指令有所放松。</p><p></p><p>5&nbsp;月&nbsp;14&nbsp;日，据The&nbsp;Information援引知情人士报道称，中国监管机构已建议字节跳动、腾讯、阿里巴巴和百度等大型科技公司减少购买外国制造的AI芯片，转而购买更多国产芯片，尤其是英伟达GPU一直是大多数国内科技科技公司的首选。但监管机构希望科技公司为新的互联网数据购买同等数量的国产和外国制造的AI芯片，这是中国监管机构首次为企业在购买AI芯片制定具体指引。</p><p></p><p>根据监管要求，如果国内公司选择订购更多的外国芯片而不是本土芯片，它们必须以书面形式详细说明订购美国芯片进行部署的数量，并证明其合理性。但据悉，该指令尚未得到严格执行，目前尚不清楚是否会对不合规公司施加任何处罚。</p><p></p><p>根据中国市场研究公司赛迪咨询（CCID&nbsp;Consulting）的一份报告，预计到2035年，中国在全球人工智能行业的份额将超过30%。有分析师表示，尽管英伟达正在努力抢占中国的芯片市场份额，但前景越来越不确定。</p><p></p><p>“英伟达正在走一条微妙的路线，试图在维持中国市场和应对美国紧张局势之间取得平衡。”IG市场分析师Hebe&nbsp;Chen表示，“从长远来看，英伟达肯定在为最坏的情况做准备。”</p><p></p><p>前不久，还有媒体爆料称，华为Ascend&nbsp;910B&nbsp;在&nbsp;2024&nbsp;年的出货量将达到40万片以上，单卡价格在7万元左右。这对英伟达来说，恐怕将造成更大的冲击。有网友这样评价，“英伟达最怕的不是少了中国市场，而是中国的市场体量可以培育出一个可怕的竞争对手。”</p><p></p><p>参考链接：</p><p><a href="https://www.reuters.com/technology/nvidia-cuts-china-prices-huawei-chip-fight-sources-say-2024-05-24/">https://www.reuters.com/technology/nvidia-cuts-china-prices-huawei-chip-fight-sources-say-2024-05-24/</a>"</p><p><a href="https://www.yahoo.com/tech/nvidias-ai-chip-sales-china-170128954.html">https://www.yahoo.com/tech/nvidias-ai-chip-sales-china-170128954.html</a>"</p><p><a href="https://www.theinformation.com/articles/chinese-regulators-tell-local-tech-firms-to-buy-fewer-nvidia-chips">https://www.theinformation.com/articles/chinese-regulators-tell-local-tech-firms-to-buy-fewer-nvidia-chips</a>"</p><p><a href="https://www.reuters.com/technology/nvidias-new-china-focused-ai-chip-set-be-sold-similar-price-huawei-product-2024-02-01/">https://www.reuters.com/technology/nvidias-new-china-focused-ai-chip-set-be-sold-similar-price-huawei-product-2024-02-01/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/1HSU228X61i7sId0r75U</id>
            <title>MatrixOne → MatrixOS：矩阵起源的创业史即将用“AI Infra”和“AI Platform”书写新章程</title>
            <link>https://www.infoq.cn/article/1HSU228X61i7sId0r75U</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/1HSU228X61i7sId0r75U</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 May 2024 06:12:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数字化浪潮, MatrixOne, AI 技术, 数据处理能力
<br>
<br>
总结: 在数字化浪潮的推动下，MatrixOne 以构建能支撑新一代数字世界的操作系统为目标，从处理数据到整合 AI 能力，不断升级发展，最终推出了 AI-Native 数据智能全域操作系统 MatrixOS，满足市场对算力和数据管理的需求。MatrixOne 的发展历程体现了对 AI 技术和数据处理能力的持续迭代和创新。 </div>
                        <hr>
                    
                    <p>在数字化浪潮的推动下，MatrixOne 的故事就像一部科技界的创业史诗，它始于一个简单而宏伟的梦想——构建一个能够支撑起新一代数字世界的操作系统。想象一下，在 AIGC 时代，数据流动如同“血液”，算法运转如同“心跳”，用户界面如同“呼吸”，新一代操作系统像指挥家一样，精准处理多元化的海量数据、操控着算力分配，这个数字世界将变得多么有活力。</p><p></p><p>故事的起点是在 2021 年，MatrixOne 的创始团队站在数字世界的入口，他们看到了一个巨大的机会——数据，这个新时代的石油正等待着被开采和利用。想要让数据发挥出真正的潜力，就必须要有强大的存储和处理能力。于是，MatrixOne 诞生了，它最初的定位是超融合异构数据库，能够处理各种类型的数据，满足不同行业的需求。</p><p></p><p>然而，随着时间的推移，MatrixOne 的团队发现，仅仅处理数据是不够的。在这个由人工智能和机器学习驱动的时代，算力成为了新的战场，企业和开发者们不再满足于单一的数据存储解决方案，他们需要的是一个能够整合 AI 能力、满足算力需求，完成 AI 基础设施建设同时，还能实现业务成本、效率与技术先进性平衡的平台。于是，MatrixOne 的研发团队矩阵起源决定将在 MatrixOne &nbsp;的基础上，扩展业务至 &nbsp;AI Infra (人工智能基础设施) 和 AI Platform(人工智能平台) 领域，并与世纪互联的 AIDC(Artificial Intelligence Data Center，人工智能数据中心) 业务进行深度融合和紧密协作。对此，InfoQ 对矩阵起源 CEO 王龙进行了特别专访，围绕矩阵起源的业务拓展规划，聚焦数据库和 AI 技术的未来发展展开了讨论。</p><p></p><p></p><h2>MatrixOne 升级至 MatrixOS，是 AI 浪潮下市场需求的“响应”</h2><p></p><p></p><p>AI 技术高速发展，已经成为了推动各行各业变革的核心技术力量，而 AI 技术的发展依赖于强大的算力和高效的数据处理能力。目前围绕这两者的软件平台众多，但这一繁荣背后隐藏着一个挑战——标准化的缺失。GPU 的云化、虚拟化和资源池化等领域尚未形成统一标准，这与已经成熟的 CPU 公有云服务形成对比。在数据管理方面，传统数据库解决方案和新兴的 AI 驱动的数据技术都在探索如何更有效地管理和使用数据。AI 技术的加入为数据的使用和管理带来了新的维度，许多公司正在尝试创新的解决方案，如向量数据库、RAG 模型和 AI 代理技术等，都是在适应这一变化。</p><p></p><p>矩阵起源认为，“在数据和算力之间建立有效的连接是至关重要的。”所以他们在开发 MatrixOne 云原生数据库时，就专注于连接“未来数据”和“过去数据”，解决传统数据库分裂带来的数据割裂问题，目标是将结构化、非结构化、半结构化等不同类型的数据融合起来，以支持 AI 在不同行业中的落地应用，这种数据融合能力，和算力、数据的兼容策略，为他们提供了与市场上其他数据平台和算力平台厂商相比的领先优势。</p><p></p><p>于是，当矩阵起源在近日完成千万美元 Pre A 轮融资后，便决定将这笔资金用于开发极简统一、开源开放的 AI-Native 数据智能全域操作系统 MatrixOS。据悉，该系统将由大规模异构算力纳管调度平台 MatrixDC、超融合异构数据管理平台 MatrixOne 和 AI 智能体应用开发平台 MatrixGenesis 三部分组成，目标是打造链接算力、数据、知识、模型与企业应用的 AI Native 软件平台。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ce/ce04e808a663546f8c5a75d6db2ec2c2.webp" /></p><p></p><p>Matrix OS 提供了一个统一的算力和数据管理平台，不同用户可以根据自己的需求进行不同的组合使用。对于 AI 科学家来说，Matrix OS 解决了他们需要在不同系统间移动数据和算力的问题，他们可以将自己的训练数据放在算力平台上进行训练，训练完成后，模型可以托管在同一平台上。对此，王龙表示，“Matrix OS 提供了一个统一操作平台，简化了数据和算力之间的连接和管理。”</p><p></p><p>同时，对于开发者而言，他们可能不需要进行模型训练，但需要使用 API 调用大模型或传统模型进行推理，所以 Matrix OS 同样提供了简化数据连接问题的、性价比高的工具，使开发者能够将 API 调用与传统应用和工作流程结合。而对于最终用户来说，他们能够更切实地感受到应用优化速度变快、应用延迟降低以及性价比提高。例如，AI 服务的成本（如 token 价格）和数据库服务的成本（如节点价格）都变得更低，应用变得更加流畅，延迟更低，使用成本也更低。</p><p></p><p>从媒体的角度来看，MatrixOne 升级至 MatrixOS，这是一个非常值得关注的行业动态，这是矩阵起源对现有服务的持续改进和对新兴技术的积极拥抱。看得出，矩阵起源在保持其核心业务的连续性和稳定性的同时，也在持续积极地做技术创新探索，以满足市场和用户不断变化的需求。MatrixOne 的这次升级是对现有服务的补充而非替代，此次矩阵起源的业务拓展，不仅是对现有业务的一次强化，更是对未来趋势的一次前瞻性布局。</p><p></p><p></p><h2>AI Infra 和 AI Platform，是一站式 AI 解决方案的必要条件</h2><p></p><p></p><p>对于想投入 AI 建设的企业来说，底层的 AI Infra 提供必要的硬件和软件基础，支持大规模数据处理和 AI 模型训练，而顶层的 AI Platform 提供易于使用的服务和工具，可以帮助企业快速开发和部署 AI 应用。对于做数据库起家的厂商来说，从底层到顶层，提供一站式 AI 解决方案，可以更好地增强客户黏性，扩大技术服务优势。</p><p></p><p>所以当我们从宏观视角去看 MatrixOS，大规模异构算力纳管调度平台 MatrixDC、超融合异构数据管理平台 MatrixOne 和 AI 智能体应用开发平台 MatrixGenesis 这三个子产品之间的技术能力是非常相关联的，功能上有所重叠，但技术能力上相互互补，共同构成了一个强大的数据智能平台。</p><p></p><p>过去，MatrixOne 本质上可以说是一个湖仓一体的数据平台，但比 Hadoop 要简单得多。技术上，MatrixOne 的架构独特，实现了多层管理，包括数据的冷热分层、读写分层和存算分离，这种架构允许灵活调度计算资源和数据资源，让 MatrixOne 像变形金刚一样可以适应银行、互联网、IoT 等不同的应用场景，比如为了适应大模型技术的发展，MatrixOne 已经具备了向量数据库的能力，便可以支持 AI 需求。如今，MatrixOne 能够非常自然地管理来自智能物联网和 AI 的新数据源，这将是“未来数据”的主要来源，也是 MatrixOS 运转的基础燃料，MatrixOS 据此能够更好地与 AI 世界连接。</p><p></p><p>在 AI Infra 领域，想要实现颠覆性技术变革，就必须要解决算力和电力这两个核心问题。目前像英伟达的 stargate 计划、中国政府建设的智算中心都在硬件层面上积极提升算力，在软件方面则主要在围绕 GPU 进行技术升级，当前行业内需要一个软硬结合的解决方案来完成未来的 AI 基础设施建设。为此，矩阵起源把 MatrixDC 作为 MatrixOne 和 MatrixGenesis 的资源底座，利用云原生技术帮助企业构建 AI 原生的资源管理和调度平台。</p><p></p><p>最近世纪互联领投的 Pre A 轮投资让矩阵起源可以在万卡集群上测试软件，为他们提供了难得的实践机会。拥有全球人才团队的矩阵起源集中优势，通过在万卡集群上的实践，快速迭代，构建了技术壁垒，创造出竞争力强的 MatrixDC ——提供大规模异构 GPU，能够快速满足 AIGC 应用场景算力需求，在算力集群管理方面，可以很好地帮助数据中心进行异构芯片和 GPU 服务器的统一纳管及资源池化，实现算力资源的共享和动态调度，提高资源利用率。</p><p></p><p>不仅如此，MatrixDC 还灵活支持 CPU 计算资源，可以高效运行结构化交易 / 分析型业务应用；拥有 AINet 高速网络架构，能够很好地支撑千卡乃至万卡集群训练工作负载，全托管企业级容器服务，企业可以充分享受容器和微服务架构的便利。同时，MatrixDC 采用 Serverless 按量计费的商业模式，能够帮助企业大幅降低 AI 创新应用的探索成本。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c6/c6a0d6f761ba49320931fd50f7a4a7ec.webp" /></p><p></p><p>而在 AI Platform 领域，目前的主要技术挑战是要解决大模型幻觉问题，以及大模型和传统行业的应用的集成问题，现在行业内亟需一个平台来整合和连接所有相关的能力，同时将不同的技术和解决方案汇集在一起。这种整合和协作，将催生出新的 AI 开发范式，将大大降低 AI 技术应用的开发门槛。于是，矩阵起源将通过 MatrixGenesis 与 MatrixOne ，为 AI 开发者和用户提供一站式的数据共享、模型训练以及大模型应用开发和运维平台，帮助 AI 技术在各行各业快速落地，典型的应用场景包括企业知识库管理、智能客服、投诉分类、智能报告生成、政策咨询以及办公助理等。</p><p></p><p>其中，MatrixGenesis 作为一个 AI 智能体应用开发平台，“Model as a Service”（MaaS），不仅仅关注软件层面，还从数据到算力进行整体优化，通过提供全链路开发支持及高性价比的大模型服务，以帮助企业 0 门槛搭建 AI 应用。它支持大模型的对比选择、部署和推理，与结构化系统数据和知识库实时打通，提供 AI 应用工程流编排、应用对接集成的全生命周期开发支持， 企业可以一键部署开源和闭源大模型，快速完成模型精调和推理加速。同时，MetrixGenesis 平台允许客户以更高的性价比访问数据和算力资源，大幅降低业务使用成本。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f8/f8333508caf85625b133247ab4afcfe7.webp" /></p><p></p><p>在应用落地阶段，MetrixGenesis 利用 MatrixOne 的支持，为用户提供了完整的数据管理和使用能力，这包括 RAG、知识库和 Agent 等组件，它们通过 0 代码的配置方式，共同作用于应用的部署和运行，确保应用能够高效地利用数据资源。王龙形容 MetrixGenesis 时说道，“用户可以在这个平台上实现从数据到算力再到应用的全面优化，确保 AI 智能体应用的快速构建。”</p><p></p><p></p><h2>MatrixOS 与世纪互联 AIDC 的合作，是“中国版 CoreWeave”构建的开端</h2><p></p><p></p><p>在这个数字化迅速发展的时代，AI 技术如同一股不可阻挡的潮流，席卷了整个商业世界，众多企业纷纷开设 AIDC 部门，致力于构建和管理人工智能基础设施，提供算力、数据存储、模型训练等关键服务，以期在这场技术革新的浪潮中占据一席之地。大家深知，AI 技术的崛起不仅能够为企业带来新的增长点，更能在激烈的市场竞争中提升企业的核心竞争力。</p><p></p><p>世纪互联作为国内领先的第三方数据中心提供商，其 AIDC 策略具有鲜明的特点，具有非常坚决的“All In AI”的决心，将所有资源投入到 AI 算力的发展中，将传统的以机房机柜为中心的服务转变为“以 AI 算力为核心”。这一转变，反映了世纪互联对未来数字世界的深刻洞察——算力将成为未来 IT 世界最重要的核心资产。矩阵起源与世纪互联的合作初衷，也正是基于双方对算力和电力重要性的共识以及共同推进 AI 领域发展的愿景。</p><p></p><p>矩阵起源以其开源的 MatrixOS 软件，与世纪互联的 AIDC 产品相结合，共同打造了 GPU 平台服务——Neolink.AI，并将在今年的第三季度上线。这个平台不仅解决了企业在 AI 算力、软件连接能力以及成本控制上的问题，更是定位于成为一个开源、共享的 AI 算力平台，旨在降低推理成本，吸引更多用户，从而实现从成本中心到价值创造中心的转变。</p><p></p><p>世纪互联认为 AI 的上限是算力和电力，而矩阵起源则认为算力和数据是 AI 的瓶颈，所以他们不谋而合地提出了打造“中国版 CoreWeave”的想法，希望通过 Neolink.AI 平台，推动 AI 技术的本土化发展，提升国内 AI 产业的创新力和竞争力。这个想法的实现，将对中国的 AI 领域产生深远的影响，为中国在全球 AI 领域的发展奠定坚实的基础。</p><p></p><p>在对王龙的采访中，我们也了解到，世纪互联即将获得一批合规的 H20 芯片，这批芯片的到来将是一个重要的里程碑，标志着世纪互联在算力资源方面迈出了坚实的一步，将会通过 Neolink.AI 开放给大家。</p><p></p><p></p><h2>拥抱开源的 MatrixOS，是赋能千行百业 AI 落地的加速器</h2><p></p><p></p><p>在当下这场澎湃的 AI 浪潮中，企业急需一艘能够驾驭风浪的航船。MatrixOS，便是一艘非常有潜力的快船。MatrixOS 不仅仅是一个操作系统，它是企业 AI 构建的加速器，它通过提供高效、稳定的数据处理能力和灵活的 AI 技术支持，让企业能够轻松跨越技术门槛，无需投入巨额资源和精力去构建复杂系统，便能迅速实现 AI 应用构建。从基础设施层，到数据库层，再到应用开发层，MatrixOS 为千行百业提供 AI 时代的工具全家桶，助力生态打造高效智能的 AIGC 方案，如制造业的智能化生产、金融业的精准风控……</p><p></p><p>展望未来，MatrixOS 的技术创新之路充满了无限可能。矩阵起源在未来将会持续努力寻找新数据、新场景。随着计算能力的不断突破，MatrixOS 将能够处理更庞大、更复杂的数据集，为 AI 技术的发展提供坚实的算力支持和模型服务。</p><p></p><p>据王龙表示，MatrixOS 的探索不止于此。它还将与物联网、5G 等前沿技术紧密结合，打造全新的技术解决方案。在这个物联网设备日益普及、5G 网络商用化的时代，数据正以前所未有的速度增长，MatrixOS 将通过融合这些技术，实现更高效的数据管理能力，为企业提供更及时、更准确的数据服务、AI 开发服务，推动千行百业迈向更高层次的智能化和数字化。</p><p></p><p>同时，MatrixOS，作为一个开源平台，其生态建设的重要性不言而喻。它将与世纪互联等战略投资者携手，吸引更多企业和个人加入到开源社区中来，共同推动 AI 技术的进步和普及。MatrixOS 还将举办线下交流活动、技术大赛，加强社区成员之间的交流和合作，共同营造一个开放、共享、创新的 AI 技术生态。</p><p></p><p>MatrixOS，不仅是技术的革新，更是智慧的传承，让我们共同见证并参与这场伟大的变革。MatrixOS 的故事才刚刚开始，希望更多的企业和个人关注并支持 MatrixOS 的发展，共同推动 AI 技术的升级和普及，为各行各业的“全面 AI 化”的数字化转型贡献智慧和力量，迎接一个更加智能、更加互联的 AI 未来。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0dc98342868f3aca8a539dcf7</id>
            <title>大模型应用之基于Langchain的测试用例生成</title>
            <link>https://www.infoq.cn/article/0dc98342868f3aca8a539dcf7</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0dc98342868f3aca8a539dcf7</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 May 2024 05:54:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型生成测试用例, LangChain, 测试用例生成方案, 技术细节说明
<br>
<br>
总结: 文中介绍了使用大模型生成测试用例的实践效果，探索了基于LangChain的方法，解决了现有工具的痛点，提高了测试用例生成的效率和质量。详细介绍了基于LangChain的测试用例生成方案和实现细节，包括整体流程、技术细节说明、代码框架和部分代码展示。整体内容涵盖了使用大模型生成测试用例的全过程。 </div>
                        <hr>
                    
                    <p></p><h1>一 用例生成实践效果</h1><p></p><p>在组内的日常工作安排中，持续优化测试技术、提高测试效率始终是重点任务。近期，我们在探索实践使用大模型生成测试用例，期望能够借助其强大的自然语言处理能力，自动化地生成更全面和高质量的测试用例。</p><p></p><p>当前，公司已经普及使用JoyCoder，我们可以拷贝相关需求及设计文档的信息给到JoyCoder，让其生成测试用例，但在使用过程中有以下痛点：</p><p></p><p>1）仍需要多步人工操作：如复制粘贴文档，编写提示词，拷贝结果，保存用例等</p><p></p><p>2）响应时间久，结果不稳定：当需求或设计文档内容较大时，提示词太长或超出token限制</p><p></p><p>因此，我探索了基于Langchain与公司现有平台使测试用例可以自动、快速、稳定生成的方法，效果如下：</p><p><img src="https://static001.geekbang.org/infoq/43/4328f52343c11804154293baa39b28a9.png" /></p><p>（什么是LangChain？ 它是一个开源框架，用于构建基于大型语言模型（LLM）的应用程序。LLM 是基于大量数据预先训练的大型深度学习模型，可以生成对用户查询的响应，例如回答问题或根据基于文本的提示创建图像。LangChain 提供各种工具和抽象，以提高模型生成的信息的定制性、准确性和相关性。例如，开发人员可以使用 LangChain 组件来构建新的提示链或自定义现有模板。LangChain 还包括一些组件，可让 LLM 无需重新训练即可访问新的数据集。）</p><p></p><h1>二 细节介绍</h1><p></p><p></p><h2>1 基于Langchain的测试用例生成方案</h2><p></p><p></p><p>因3种方案使用场景不同，优缺点也可互补，故当前我将3种方式都实现了，提供大家按需调用。</p><p></p><h2>2 实现细节</h2><p></p><p></p><h3>2.1 整体流程</h3><p></p><p>​</p><p></p><p><img src="https://static001.geekbang.org/infoq/c0/c0228566441a94e1b2543d97d8d6bebb.png" /></p><p>​</p><p></p><h3>2.2 技术细节说明</h3><p></p><p>•pdf内容解析： ：Langchain支持<a href="https://python.langchain.com/docs/modules/data_connection/document_loaders">多种文件格式的解析</a>"，如csv、json、html、pdf等，而pdf又有很多<a href="https://zhuanlan.zhihu.com/p/352722932">不同的库</a>"可以使用，本次我选择PyMuPDF，它以功能全面且处理速度快为优势</p><p></p><p>•文件切割处理： 为了防止一次传入内容过多，容易导致大模型响应时间久或超出token限制，利用Langchain的<a href="https://python.langchain.com/docs/modules/data_connection/document_transformers/split_by_token">文本切割器</a>"，将文件分为各个小文本的列表形式</p><p></p><p>•Memory的使用： 大多数 LLM 模型都有一个会话接口，当我们使用接口调用大模型能力时，每一次的调用都是新的一次会话。如果我们想和大模型进行多轮的对话，而不必每次重复之前的上下文时，就需要一个Memory来记忆我们之前的对话内容。Memory就是这样的一个模块，来帮助开发者可以快速的构建自己的应用“记忆”。本次我使用<a href="https://python.langchain.com/docs/modules/memory/types">Langchain的ConversationBufferMemory与ConversationSummaryBufferMemory</a>"来实现，将需求文档和设计文档内容直接存入Memory，可减少与大模型问答的次数（减少大模型网关调用次数），提高整体用例文件生成的速度。ConversationSummaryBufferMemory主要是用在提取“摘要”信息的部分，它可以将将需求文档和设计文档内容进行归纳性总结后，再传给大模型</p><p></p><p>•向量数据库： 利用公司已有的向量数据库[测试环境Vearch]，将文件存入。 在创建数据表时，需要了解向量数据库的检索模型及其对应的参数，目前支持六种类型，IVFPQ，HNSW，GPU，IVFFLAT，BINARYIVF，FLAT（<a href="https://github.com/vearch/vearch/wiki/Vearch%E7%B4%A2%E5%BC%95%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%8F%82%E6%95%B0%E9%80%89%E6%8B%A9">详细区别和参数可点此链接</a>"），目前我选择了较为基础的IVFFLAT--基于量化的索引，后续如果数据量太大或者需要处理图数据时再优化。另外Langchain也有很方便的<a href="https://python.langchain.com/docs/integrations/vectorstores/vearch">vearch存储和查询的方法可以使用</a>"</p><p></p><h3>2.3 代码框架及部分代码展示</h3><p></p><p>代码框架：</p><p></p><p><img src="https://static001.geekbang.org/infoq/cc/cc4beb2a38f97d3ea492c03462913e12.png" /></p><p>​</p><p>​代码示例：</p><p></p><p><code lang="text">    def case_gen(prd_file_path, tdd_file_path, input_prompt, case_name):
        """
        用例生成的方法
        参数:
        prd_file_path - prd文档路径
        tdd_file_path - 技术设计文档路径
        case_name - 待生成的测试用例名称
        """
        # 解析需求、设计相关文档, 输出的是document列表
        prd_file = PDFParse(prd_file_path).load_pymupdf_split()
        tdd_file = PDFParse(tdd_file_path).load_pymupdf_split()
        empty_case = FilePath.read_file(FilePath.empty_case)

        # 将需求、设计相关文档设置给memory作为llm的记忆信息
        prompt = ChatPromptTemplate.from_messages(
            [
                SystemMessage(
                    content="You are a chatbot having a conversation with a human."
                ),  # The persistent system prompt
                MessagesPlaceholder(
                    variable_name="chat_history"
                ),  # Where the memory will be stored.
                HumanMessagePromptTemplate.from_template(
                    "{human_input}"
                ),  # Where the human input will injected
            ]
        )
        memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
        for prd in prd_file:
            memory.save_context({"input": prd.page_content}, {"output": "这是一段需求文档，后续输出测试用例需要"})
        for tdd in tdd_file:
            memory.save_context({"input": tdd.page_content}, {"output": "这是一段技术设计文档，后续输出测试用例需要"})

        # 调大模型生成测试用例
        llm = LLMFactory.get_openai_factory().get_chat_llm()
        human_input = "作为软件测试开发专家，请根据以上的产品需求及技术设计信息，" + input_prompt + ",以markdown格式输出测试用例，用例模版是" + empty_case
        chain = LLMChain(
            llm=llm,
            prompt=prompt,
            verbose=True,
            memory=memory,
        )
        output_raw = chain.invoke({'human_input': human_input})

        # 保存输出的用例内容，markdown格式
        file_path = FilePath.out_file + case_name + ".md"
        with open(file_path, 'w') as file:
            file.write(output_raw.get('text'))
</code></p><p></p><p><code lang="text">    def case_gen_by_vector(prd_file_path, tdd_file_path, input_prompt, table_name, case_name):
        """
        !!!当文本超级大时，防止token不够，通过向量数据库，搜出某一部分的内容，生成局部的测试用例，细节更准确一些!!!
        参数:
        prd_file_path - prd文档路径
        tdd_file_path - 技术设计文档路径
        table_name - 向量数据库的表名，分业务存储，一般使用业务英文唯一标识的简称
        case_name - 待生成的测试用例名称
        """
        # 解析需求、设计相关文档, 输出的是document列表
        prd_file = PDFParse(prd_file_path).load_pymupdf_split()
        tdd_file = PDFParse(tdd_file_path).load_pymupdf_split()
        empty_case = FilePath.read_file(FilePath.empty_case)
        # 把文档存入向量数据库
        docs = prd_file + tdd_file
        embedding_model = LLMFactory.get_openai_factory().get_embedding()
        router_url = ConfigParse(FilePath.config_file_path).get_vearch_router_server()
        vearch_cluster = Vearch.from_documents(
            docs,
            embedding_model,
            path_or_url=router_url,
            db_name="y_test_qa",
            table_name=table_name,
            flag=1,
        )
        # 从向量数据库搜索相关内容
        docs = vearch_cluster.similarity_search(query=input_prompt, k=1)
        content = docs[0].page_content

        # 使用向量查询的相关信息给大模型生成用例
        prompt_template = "作为软件测试开发专家，请根据产品需求技术设计中{input_prompt}的相关信息:{content},以markdown格式输出测试用例，用例模版是:{empty_case}"
        prompt = PromptTemplate(
            input_variables=["input_prompt", "content", "empty_case"],
            template=prompt_template
        )
        llm = LLMFactory.get_openai_factory().get_chat_llm()
        chain = LLMChain(
            llm=llm,
            prompt=prompt,
            verbose=True
        )
        output_raw = chain.invoke(
            {'input_prompt': input_prompt, 'content': content, 'empty_case': empty_case})
        # 保存输出的用例内容，markdown格式
        file_path = FilePath.out_file + case_name + ".md"
        with open(file_path, 'w') as file:
            file.write(output_raw.get('text'))
</code></p><p></p><h1>三 效果展示</h1><p></p><p></p><h3>3.1 实际运用到需求/项目的效果</h3><p></p><p>用例生成后是否真的能帮助我们节省用例设计的时间，是大家重点关注的，因此我随机在一个小型需求中进行了实验，此需求的PRD文档总字数2000+，设计文档总字数100+（因大部分是流程图），结果效率提升50%。</p><p></p><p>本次利用大模型自动生成用例的优缺点：</p><p></p><p>优势：</p><p></p><p>•全面快速的进行了用例的逻辑点划分，协助测试分析理解需求及设计</p><p></p><p>•降低编写测试用例的时间，人工只需要进行内容确认和细节调整</p><p></p><p>•用例内容更加全面丰富，在用例评审时，待补充的点变少了，且可以有效防止漏测</p><p></p><p>•如测试人员仅负责一部分功能的测试，也可通过向量数据库搜索的形式，聚焦部分功能的生成</p><p></p><p>劣势：</p><p></p><p>•暂时没实现对流程图的理解，当文本描述较少时，生成内容有偏差</p><p></p><p>•对于有丰富经验的测试人员，自动生成用例的思路可能与自己习惯的思路不一致，需要自己再调整或适应</p><p></p><h1>四 待解决问题及后续计划</h1><p></p><p>1.对于pdf中的流程图（图片形式），实现了文字提取识别（langchain pdf相关的方法支持了ocr识别），后续需要找到更适合解决图内容的解析、检索的方式。</p><p></p><p>2.生成用例只是测试提效的一小部分，后续需要尝试将大模型应用与日常测试过程，目前的想法有针对diff代码和服务器日志的分析来自动定位缺陷、基于模型驱动测试结合知识图谱实现的自动化测试等方向。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/W63jFI4t7PT6ZM5LVglH</id>
            <title>甲骨文副总裁：只会SQL也可以搞定AI，但对 DBA 的要求将更高</title>
            <link>https://www.infoq.cn/article/W63jFI4t7PT6ZM5LVglH</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/W63jFI4t7PT6ZM5LVglH</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 May 2024 02:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, Oracle Database 23ai, AI功能, 向量搜索
<br>
<br>
总结: 甲骨文公司发布了Oracle Database 23ai，其中加入了突破性的AI技术，包括AI for Data、简化数据中的AI使用等功能，其中的AI Vector Search功能可以根据概念内容搜索文档、图像和关系数据。这一举措使得即使只懂得SQL的用户也能够全盘搞定AI应用，展示了数据库AI化的必然趋势。 </div>
                        <hr>
                    
                    <p>“最重要的不在于拥有多少大模型，而在于如何使用它们。”甲骨文公司副总裁及中国区董事总经理吴承杨说道。&nbsp;</p><p>&nbsp;</p><p>在全民探索大模型应用的现在，甲骨文也积极尝试。Oracle 融合数据库中的最新版本&nbsp;<a href="https://www.oracle.com/cn/database/">Oracle Database 23ai</a>"&nbsp;作为广泛的云技术服务正式发布，标志着甲骨文在AI领域的重大进展。</p><p></p><h2>“只会SQL，也可以全盘搞定AI”</h2><p></p><p>&nbsp;</p><p>由于这个版本主要加入了突破性的 AI 技术，因此甲骨文将 Oracle Database 23c 重命名为 Oracle Database 23ai。</p><p>&nbsp;</p><p>甲骨文公司中国区技术咨询部高级总监李珈介绍，23ai 专注AI主要体现在三个方面：一是AI for Data，在数据应用在AI的层面上做深入加持；二是针对应用开发者，在使用层面上能够更简单；三是针对关键任务赋予AI 能力。</p><p>&nbsp;</p><p>此长期支持版本包含了&nbsp;Oracle AI Vector Search 、300多个主要新功能和数千项增强功能，专注于帮助用户简化数据中的&nbsp;AI 使用。</p><p>&nbsp;</p><p>AI Vector Search（AI向量搜索）&nbsp;是 Oracle Database 23ai的一项重要功能，用户可以借此根据概念内容（而不是特定的文字、像素或数据值）来搜索文档、图像和关系数据，同时用户可以使用自然语言界面查询私有业务数据，并帮助&nbsp;LLM 提供更准确和更相关的结果。</p><p>&nbsp;</p><p>为什么不是一个单独的向量数据库或在Database引擎之上再拓展一个向量引擎？李珈表示，这样做的最大好处就是可以把业务数据和向量数据整合在一起。比如，原来用大量的业务数据做AI应用需要把数据拷贝出去，AI Vector Search则可以用一个SQL直接查找业务数据和向量数据及其他数据类型的数据。</p><p>&nbsp;</p><p>在向量化过程中，原来的做法是调一个大模型或一个嵌入的算法，这对有GPU资源的用户比较友好，对于多数没有设备资源的用户，Oracle提出了新的SQL嵌入函数，可以把符合标准的嵌入模型放到Oracle里面，用Oracle数据库的引擎来帮用户做向量化，这样意味着即便没有AI方面的经验、只会SQL，也可以全盘搞定这个过程。</p><p>&nbsp;</p><p>此外，Oracle Database 23ai 可以在客户数据中心本地部署，也可以在云上部署。甲骨文还提供了Oracle Digital Assistant，这是一个嵌入在Oracle应用中的数据助手，可以帮助回答客户问题。</p><p>&nbsp;</p><p>“300多个新功能，对于23ai的这个版本来讲非常有战略意义，是真正的 Game Changer。” 李珈说道。</p><p>&nbsp;</p><p>甲骨文投入了大量的资金用于人才培养、科技研发和基础设施建设，以确保用户能够获得高质量的服务。Oracle Database 23ai的研发周期通常为五年一个大版本的更新，但在每个季度都会有季度性的版本发布。</p><p>&nbsp;</p><p>费用方面，吴承杨表示，甲骨文数据库的定价一直是以使用的CPU量来计算的，不会因为23ai的发布而改变定价规则。Oracle Database 23ai 中 AI 功能的使用不需要额外付费，因为它是作为一个新功能添加到现有版本中的。</p><p>&nbsp;</p><p>“生成式AI用得好不好，只有一个标准，就是可以给用户带来什么效果，用户是否得到了本质性上的改变。”吴承杨强调，AI的功能应该像用电一样简单易用，用户不需要成为AI科学家，只需懂得SQL即可。</p><p>&nbsp;</p><p>李珈介绍，甲骨文开始做Vector DB以来，金融、电信、制造业等用户也积极跟进，应用场景包括欺诈分析，各种各样的智能体、AI 助手，长视频检测等。</p><p>&nbsp;</p><p></p><h2>“数据库 AI 化是一个必然趋势”</h2><p></p><p>&nbsp;</p><p>“随着生成式AI的出现，融合数据库将变得更加重要。”吴承杨说道。</p><p>&nbsp;</p><p>融合数据库能够处理结构化数据、非结构化数据、图数据、JSON和空间数据等多种数据类型，因此能够简化应用和分析的生成与运行。</p><p>&nbsp;</p><p>企业在使用生成式AI ，大模型应用时，更希望将企业内部数据放在本地。用户可以使用任何一个国产或国外的模型，比如Llama-3，可以使用各种数据库来做RAG。没有融合数据库，这些事情也可以做到，但需要一个很强大的开发商和复杂的技术架构。。</p><p>&nbsp;</p><p>吴承杨认为，简化企业大模型应用的逻辑是“四个任何”：任何时候、任何地方、任何人、任何数据，都可以使用。把这四个加起来以后，就真正地解决了很多的问题。甚至用户自己内部的工程师、以前的DBA就可以解决。</p><p>&nbsp;</p><p>吴承杨提到，随着AI进入数据库，未来对DBA的要求将会有所不同。“对未来的DBA来说，像数据库的管理、打补丁等能力基本不太需要或者需要比较少，但是对整个架构、整个数据应用方面的要求，会比以前的要求高很多。虽然仍被称为DBA，但是要求不一样、工作范畴更大。”</p><p>&nbsp;</p><p>甲骨文还强调了其向量数据库产品完全继承了Oracle数据库的企业级特性，包括安全、稳定、可靠和可扩展性。Oracle Real Application Clusters (RAC) 保护实例，Active Data Guard (ADG) 保护容灾。甲骨文还提供了强大的备份功能，以及Exadata数据库一体机，以满足高性能需求。</p><p>&nbsp;</p><p>“Oracle AI 使我们的融合数据库上了一个非常大的台阶。”吴承杨说道。</p><p>&nbsp;</p><p>吴承杨表示，数据库的AI化是一个必然趋势。对于一个数据库先进性的衡量，对AI的支持是必选项。而对于甲骨文来说，现在只是迈出了第一步，未来的想象空间是非常大的。</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/8mvby5h6dglDsO7akkti</id>
            <title>大模型应用商业化落地关键：给企业带来真实的业务价值</title>
            <link>https://www.infoq.cn/article/8mvby5h6dglDsO7akkti</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/8mvby5h6dglDsO7akkti</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 May 2024 10:39:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, ToB 领域, 商业化落地, 数据分析 Agent
<br>
<br>
总结: 2024 年被称为大模型应用的元年，大模型已成共识，但如何在 ToB 领域落地商业化仍是关键问题。ToC 市场发展迅猛，ToB 市场潜力巨大，大模型结合 Agent 在企业经营分析领域有望落地。数势科技推出的 SwiftAgent 2.0 在数据分析和决策方面具有突破性，为大模型商业化提供了实际落地路径。 </div>
                        <hr>
                    
                    <p>2024 年被很多人称为大模型应用的元年，毫无疑问，大模型已经成为共识，下一步更急迫的问题也摆在了大家的面前——大模型到底能够用在哪？有哪些场景能落地？怎么做才能创造真正的价值？</p><p></p><p>在刚刚过去的 AICon 全球人工智能开发与应用大会上，InfoQ 采访了在大模型应用领域的领跑企业数势科技创始人兼 CEO 黎科峰博士，交流大模型商业化落地的可行性路径，为行业提供启发。</p><p></p><p></p><p></p><p>大模型在 ToB 领域蕴藏巨大机遇，企业出海或将成为落地加速器</p><p></p><p>当前，许多传统企业对于如何将大模型技术整合到现有业务中感到迷茫。大模型应用产品提供企业难以找到合适的变现方式。大模型应用企业究竟该如何突破商业化之困？ToB 和 ToC，呈现出两种不同的路径。</p><p></p><p>ToC 市场似乎拥有一种天然的魔力，在中国的互联网领域中，它催生了一批又一批的杀手级应用，并不断吸引着更多企业加入这个生态系统。ToC 市场如火如荼，ToB 市场发展因为投资回报率、数据安全性等问题导致相对发展缓慢。但纵观中国科技发展史， ToB 市场经历了 20 多年的从信息化到数字化再到智能化等多个技术阶段的演变与发展，每一次技术革命都经历了探索和融合期，最终帮助企业更好地为用户提供个性化产品、满足特定需求。</p><p></p><p>因此黎科峰认为，大模型在 ToB 市场同样蕴藏着更大的机遇。从用户的角度来看，广大用户，包括企业管理者、技术人员和业务人员，已经开始熟悉并运用大模型进行日常的数据处理、知识获取等，当企业公民使用率提升后，将反向推动 To B 软件的发展，真正实现软件产品“从管到用”的进化。从市场选择来看，企业出海也许将加速大模型商业化落地。一方面，由于海外企业用户更倾向选择公有云部署，能够为大模型商业化提供更加全面的场景数据积累，不断丰富和优化场景应用；另一方面，SaaS 付费模式在海外接受程度高于国内市场，有利于大模型应用企业更轻量、高效地实现商业化落地。</p><p></p><p>大模型 +Agent，将在企业经营分析领域落地</p><p></p><p>大模型具备知识、智商、学习能力和推理能力，能够总结和生成新的见解，叠加 Agent，让企业应用具备了记忆、反思和学习能力，能够调用企业内部工具并不断迭代反思，真正实现业务价值的落地。据行业调研表明，数据分析和决策被视为这一结合最重要的应用方向。</p><p></p><p>在企业考虑引入数据分析 Agent 产品时，成本、数据安全以及实际落地价值将是关注重点。部署千亿级别参数的大模型成本高昂，目前或许只有头部企业才能承担。同时，数据安全问题也不容忽视，特别是在受严格监管的金融等行业。再者，企业也需要确保大模型投入后能对业务产生实质性的帮助。</p><p></p><p>黎科峰表示，为解决这些问题，数势科技为企业提供了规模适中、符合企业经营分析场景的 Agent 产品，来降低企业成本和使用门槛。同时，通过云端私有化的专属数据空间以及支持私有化和本地化部署的方式，确保数据安全。</p><p></p><p>SwiftAgent 2.0 发布，实现企业数据分析与决策的范式变革</p><p></p><p>自大模型技术发布之初，数势科技便迅速拥抱这一变革，并在此基础上推出了 SwiftAgent 的新一代产品。黎科峰在采访中介绍，SwiftAgent 2.0 版本与 1.0 相比，主要新增了五大亮点功能，包括增加了指标和标签语义层、多模态和多源异构数据链接、用户可干预、可持续性学习和数据加速引擎，这些亮点体现了产品如何针对企业数据分析的现有问题提供解决办法。</p><p></p><p>“统一语义层的构建”，SwiftAgent 2.0 通过添加指标和标签语义层，解决了大模型对底层业务语义理解的难题，并统一了企业各部门的数据口径，从而避免了数据的混乱。</p><p></p><p>“多模态和多源异构数据链接”使得结构化数据和非结构化数据能够结合，提供更丰富的分析结论。</p><p></p><p>“用户可干预机制”允许用户参与到数据分析过程中，通过反馈和确认来训练和优化 Agent。</p><p></p><p>“数据加速引擎”则确保了实时数据处理的能力，实现了秒级数据查询，真正达到了实时人机交互的水平。底层选用了性能优异的数据分析引擎，如 StarRocks、Doris，并结合对数据加工和使用场景的优化，提供了基于视图的预计算能力和基于预计算结果的查询优化能力。</p><p></p><p>此外，数据虚拟化技术的应用，使得数据定义与物理数据解耦，实现了指标 / 标签的灵活加工使用，无需排期开发，进一步提高了数据处理的效率。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/2y/02/2yyafd33566da283d3d0835f654a8a02.png" /></p><p></p><p>大模型应用商业化落地的关键：找到产品的业务价值</p><p></p><p>在商业化方面，大模型仍然属于新兴事物。为了让更多行业看到大模型在真实业务场景中的应用价值，数势科技也在致力于创造实际的落地标杆。“只有通过口口相传和业务价值的真实感知，才能让有价值的产品被更多人接受和使用。”黎科峰认为。</p><p></p><p>目前，SwiftAgent 已经与行业头部客户签订了合同，并实现了真正的付费使用。这标志着数势科技在大模型领域的领先地位，其产品已经在金融、零售、消费和高端制造等多个行业中得到了应用，帮助企业解决实际的业务问题。</p><p></p><p>以一家头部茶饮连锁企业为例，黎科峰介绍说明，SwiftAgent 通过在集团层面部署语义层和数据分析框架，并将 Agent 提供给每位店长，使得他们能够通过对话式查询快速获取数据和分析结果，从而提升门店运营效率。这种数字化工具的应用，为企业提供了一个经营闭环，改变了过去门店靠经验经营的困境，极大地促进了业务发展。</p><p></p><p>数势科技 SwiftAgent 2.0 实现了大模型技术在企业经营场景下的价值验证，也提升了产品的智能化程度，正在颠覆企业企业数据分析与决策范式。黎科峰表示，数势科技的愿景便是以科技创新实现数据价值的普惠化，让每一个企业、每一位员工都能体会到数据带来的价值。</p><p></p><p>未来，我们期待看到更多像数势科技这样的企业，通过创新的技术和产品，推动大模型商业化进程，为各行各业带来价值提升。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/61JsqmVQbZA0pqoOpbeK</id>
            <title>价格战，并不是大模型厂商的初衷</title>
            <link>https://www.infoq.cn/article/61JsqmVQbZA0pqoOpbeK</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/61JsqmVQbZA0pqoOpbeK</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 May 2024 10:30:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 商战, 降价, 大模型, MaaS模式
<br>
<br>
总结: 本文讨论了大模型厂商之间的价格战，阿里云降价引发了国内外大模型厂商的跟风降价，降幅高达80%。文章指出降价背后的价值和影响，探讨了MaaS模式对大模型应用的推动作用，以及如何通过公有云、API调用和应用工具实现大模型应用的落地。文章强调了技术的价值在于解决实际问题，而大模型的降价将促进创新和推广。 </div>
                        <hr>
                    
                    <p>商战，往往是朴实无华的。</p><p></p><p>OpenAI 吹响了降价号角之后，国内外大模型厂商陆续发布降价消息。直至 5 月 21 日，阿里云发布降价公告，将旗下通义千问的多款商业化及开源模型进行大幅降价，彻底将这场降价狂欢推向高潮。</p><p></p><p>大模型“价格战”的厮杀也将愈演愈烈——不管是头部厂商还是中小大模型厂商都在搞降价，降价幅度逐渐走高，降幅 80% 都变得不那么出人意外，这一系列操作给科技圈和经济圈带来了大大震撼，谁也不想错过这么好的流量。一时间互联网上充斥着无数实时更新的资讯，深度浅度的分析，莫名其妙的预测。风起云涌，硝烟四起……</p><p></p><p>请暂停一下。</p><p></p><p>虽然一场以降价为起点的互联网狂欢会在一年当中出现很多次，但国内大模型的降价还是第一次。降价带给我们的，难道仅仅是短期的优惠和吃瓜的快乐吗？在这场价格战背后，我们究竟能获得什么？为了解答这些问题，我们对最近的这波“模型厂商的降价狂欢”做了全面观察，笔者发现阿里云的此次降价有点不一样，不仅全线 9 款模型降价，且主力模型 Qwen-Long 作为对标 GPT-4 的国产大模型是降幅相当大，高达 97%。这不禁让笔者陷入思考，下文以期从技术角度拆解。</p><p></p><p>入场：MaaS 模式开启，阿里云领跑大模型应用新时代</p><p></p><p>随着 MaaS 理念、模型开源开放技术发展理念被行业里越来越多人推崇，阿里云已经站在大模型应用新时代的领跑位置。自去年下半年起，行业风向明显转变，模型厂商们开始深刻认识到，技术的真正价值在于解决实际问题，而不仅仅是技术本身的堆砌。根据国家数据局今年 3 月发布的数据，中国已拥有超过 100 个参数超过 10 亿的大型模型，这些模型在多个行业中形成了上百种应用模式，有效推动了各行业的数字化转型。</p><p></p><p>MaaS 模式应运而生，它标志着大型模型技术从“拼业务”转向“拼市场”的新阶段。通过 MaaS，企业可以按需访问云平台上的大型模型，无需承担高昂的硬件投资和长期维护成本，从而更加灵活、高效地使用这些先进技术，加速业务创新和市场响应。在阿里云看来大模型将更像AI时代的操作系统，大多软件都将基于大模型调用GPU，而不再会直接调用GPU。</p><p></p><p>阿里云敏锐地捕捉到了这一市场趋势，率先推出了 MaaS 模式。通过高性能 AI 集群“灵骏”，阿里云为万卡规模的 AI 集群提供了无拥塞、高性能的集群通讯能力，为大模型应用的快速落地提供了坚实的算力基础。</p><p>阿里云 MaaS 的独特优势还体现在其对大模型应用快速落地的助推作用。通过提供一站式的大模型应用开发平台「百炼」，为需要在内部进行大模型训练的企业提供了一个安全的数据使用环境。</p><p></p><p>市场竞争加剧也对企业的商业模式和造血能力提出了更高要求，行业或加速洗牌。一些产品成熟度不高、资金实力不强的企业可能在价格战中被淘汰。大模型创业公司月之暗面创始人杨植麟表示，过度的价格战可能会伤害那些专注于技术创新但资金实力有限的中小型企业，希望行业能回归理性竞争。</p><p></p><p>不过，在任何一场价格战中，无论商家如何“搏杀”，用户至少短期来看都能从中受益。大模型的调用成本大幅降低，有利于创业公司加快创新步伐，开发出更多商业化的大模型应用，促进大模型技术的普及与推广。</p><p></p><p>站在变革的十字路口，如何从技术实力、生态建设、应用创新等方面厚积薄发，是目前对于模型厂商来说，比价格战更重要的事情。</p><p></p><p>亮剑：公共云 +API 调用 + 应用工具，是大模型应用最好的落地范式</p><p></p><p>如果大模型不能实现有效的落地应用，那么无论其技术多么先进，其实际意义也将大打折扣。技术的价值一定是在于其能够解决现实问题、提高效率、创造新的可能性。而阿里云百炼的业务主张一直是“公有云 +API+ 应用的落地范式”，从他们整个发展过程来看，这个主张行之有效。</p><p></p><p>得益于“公有云 +API”的边际效应，阿里云的 MaaS 服务体系让按需进行算力“租借”模式延续到企业 AI 创新领域。加之各模型厂商 API 的持续降价，最直接的影响是推理成本的普惠，毕竟所有的模型应用过程都需要推理。但基础模型本身并不能决定任何一家企业的 AI 应用能否成功落地。模型只是基础，真正的关键在于如何将这些工具应用到实际业务场景中，并解决具体问题。而要实现这一点，使用门槛的降低就显得尤为重要。</p><p></p><p>使用门槛的降低能够释放大模型的红利，让更多的企业和开发者能够轻松接入和使用这些先进技术。这正是阿里云推出 MaaS 模式和百炼大模型服务平台的核心思想。百炼作为一站式的大模型应用开发平台，其独特的架构和优势为企业提供了强大的支持。</p><p></p><p>「百炼」凭借创新的开放架构设计，为企业提供了一站式的 AI 解决方案。该平台整合了模型服务，配备了全面的模型开发工具和应用工具，实现了从大模型输出到智能体发布的无缝对接。利用智能体构建、应用广场及自定义画布流程编排等先进工具，企业得以迅速构建并部署大模型应用。</p><p></p><p>百炼平台拥有全面的模型服务。它提供从数据管理、模型训练、评估到部署的全链路服务，支持多种大模型和框架，以满足不同业务场景的需求。平台通过智能调度和优化算法，实现 GPU 资源的最大化利用，有效降低用户算力成本。此外，百炼平台支持多种开源和商业化模型，用户可轻松接入和使用，加速 AI 应用的开发进程。无论是云上部署还是私有化部署，百炼平台都能提供灵活的解决方案，同时保障用户数据的安全和隐私。用户友好的界面和工具使得开发者能够更高效地进行工作。</p><p></p><p>在模型功能及生态方面，百炼平台展现了其强大的实力。它支持多种模型的训练和微调，包括预训练模型、全参微调和 PEFT（参数高效微调）等，以满足不同场景下的模型优化需求。全面的模型评估指标和工具帮助用户快速了解模型性能并进行优化。百炼平台支持模型的快速部署和推理，同时开放 API 和插件接口，支持用户自定义模型和工具，进一步丰富模型生态。</p><p></p><p>针对 AI 应用落地场景对模型平台的需求，百炼平台同样表现出色。它支持基于 Open AI 的 Assistant API 架构，方便开发者集成。支持 Llamaindex、langchain 等开源框架，使得模型能够在实际业务场景中发挥作用，实现智能交互和决策。Prompt 工程支持帮助用户构建高质量的指令和回答，提升模型的准确性和稳定性。在数据管理与预处理方面，百炼平台提供高效的数据处理功能，以提高模型训练的质量和效率。</p><p></p><p>以朗新科技为例，该公司基于「百炼」成功训练出电力专属大模型，并开发出多款产品，如“电力账单解读智能助手”，显著提升了客户接待效率并降低了投诉率。</p><p></p><p>在最近的 AI 智领者峰会上，阿里云智能集团资深副总裁刘伟光强调：“作为中国领先的云计算公司，阿里云此次大幅降低大模型推理价格，旨在加速 AI 应用的广泛普及。我们预见，未来大模型 API 的调用量将实现成千上万倍的增长。”</p><p></p><p>阿里云的「百炼」战略核心在于，通过降价降低企业使用 AI 技术的门槛，实现技术的普及与惠民。从技术、时间和费用三个成本维度来看，这一策略不仅减轻了企业的技术投入压力，还通过优化服务流程，如简化模型部署和应用集成，降低了企业在技术学习和应用上的时间成本。同时，后付费结算模式和免费额度的提供，进一步减少了企业的费用支出，使企业能够以更低的风险尝试和采纳 AI 技术。</p><p></p><p>阿里云的「百炼」战略核心在于，通过持续打磨的千问大模型，提升企业及用户的使用效果，带来体验的飞升。通过技术升级带来降价，降低企业使用 AI 技术的门槛，实现技术的普及与惠民；通过便捷开放的应用构建工具，提高企业集成大模型的效率，打通大模型到业务落地最后一公里。从技术、时间和费用三个成本维度来看，这一策略不仅减轻了企业的技术投入压力，还通过优化服务流程，提升集成及开发效率，降低了企业在技术学习和应用上的时间成本。同时，后付费结算模式和免费额度的提供，进一步减少了企业的费用支出，使企业能够以更低的风险尝试和采纳 AI 技术。</p><p></p><p>创新战：降价策略助力企业普惠，推动 AI 产业共创共赢</p><p></p><p>商战，价格不是唯一，所谓价格战的本质其实是技术创新之战。</p><p></p><p>这一策略背后隐藏着深层次的商业逻辑和市场考量。通过降价，厂商不仅能够利用技术领先优势快速占领市场份额，还能加速产品迭代，根据用户反馈和数据持续优化产品。同时，降价有助于构建和扩大以大模型为核心的生态系统，吸引更多开发者和合作伙伴加入，共同推动技术和应用创新。厂商通过降价适应市场变化和用户需求，使高端技术更加普及，从而加速技术的广泛应用。</p><p></p><p>在这场 AI 技术的浪潮中，回看大模型厂商之间的这场价格战，商业竞争更是表象，它其实是更深层次地反映了企业对 AI 技术普惠的追求和对行业共赢未来的承诺。阿里云大模型服务平台百炼的降价策略，正是基于这样的理念，通过降低使用门槛，使更多的企业能够享受到 AI 技术带来的变革。</p><p></p><p>当模型数量和价格不再是竞争的唯一标准时，如何使模型更加实用、易用，成为了新的竞技场。阿里云凭借其在算力资源、数据管理、研发人才以及数据中心管理等方面的优势，已经具备了成为国内顶尖模型基础设施提供者的条件。通义千问开源模型的高下载量和企业客户的快速增长，也正是阿里云在模型领域领导地位的有力证明。</p><p></p><p>对于中小科技企业而言，AI 技术的降价策略带来了更广阔的的时长和机遇。他们能够以更低的成本接入先进的 AI 技术，从而在市场中获得竞争优势，甚至有机会再次颠覆传统的互联网格局。</p><p></p><p>此次的降价浪潮也让我们我们能够预想到大模型技术的应用场景将不断拓展，对社会和行业的积极影响也将日益显著。</p><p></p><p>总结来说就是，百模大战，已经结束。</p><p></p><p>但这并非 AI 发展的终点，而是技术新征程的起点。随着大模型技术的普及和成熟，AI 产品技术之间的真正角逐才刚刚拉开帷幕，未来一定是一个以创新、应用和用户体验为核心的全新时代。企业之间的竞争将不再局限于模型的规模和能力，而是转向如何将 AI 技术转化为实际产品，解决现实问题，创造商业价值。至于未来谁能厮杀出圈，就让我们拭目以待。</p><p></p><p><img src="https://static001.geekbang.org/infoq/24/24b7ddc110c8b78027dc054cb9740fa0.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/RpUU6cNNZtHaMecxqBmr</id>
            <title>又翻车！微软一次更新引爆大规模连锁反应，Bing、Copilot等多个软件集体宕机五小时！</title>
            <link>https://www.infoq.cn/article/RpUU6cNNZtHaMecxqBmr</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/RpUU6cNNZtHaMecxqBmr</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 May 2024 08:36:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 微软, 服务中断, Bing, Copilot
<br>
<br>
总结: 微软突然的大规模中断影响了Bing.com、Copilot等服务，主要影响了亚洲和欧洲用户，部分版本仍处于离线状态。中断导致Bing搜索页面出错，DuckDuckGo等搜索引擎也受影响。微软确认是一次软件更新出错所致，经过数小时后服务逐渐恢复上线。 </div>
                        <hr>
                    
                    <p>昨晚，微软突然的大规模中断影响了 Bing.com、网页和移动版 Copilot、Windows 版 Copilot、ChatGPT 互联网搜索和 DuckDuckGo等。</p><p>&nbsp;</p><p>微软的服务中断大约在美国东部时间凌晨 3 点开始，似乎主要影响了亚洲和欧洲的用户。目前，部分版本在欧洲仍处于离线状态。</p><p>&nbsp;</p><p><a href="https://x.com/mayank_jee/status/1793560213498581389">根据</a>"网友们的反馈，当打开 Bing.com时，会看到空白页或带有 429 HTTP 代码错误的页面，但是如果直接访问，Bing 搜索仍然有效。出于某种原因，这次中断只对主页产生了影响。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/9a/9a0182923dd1b381afb4fde5e2218fc8.png" /></p><p></p><p>&nbsp;要使用 Bing，可以使用这个链接（<a href="https://www4.bing.com/">https://www4.bing.com/</a>"），但是网站、应用程序和 Windows 中的 Copilot都是完全离线的。</p><p>&nbsp;</p><p>据悉，这次中断影响了 Bing 的 API，因此依赖该 API 的服务也被波及，包括 ChatGPT 互联网搜索、DuckDuckGo 和<a href="https://x.com/ecosia/status/1793549809141625247">Ecosia</a>"，DuckDuckGo 和<a href="https://x.com/ecosia/status/1793549809141625247">Ecosia</a>"两个替代搜索引擎都依赖 Bing 的搜索结果。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bdf2bf53c03a83295229bcd2a9609af9.png" /></p><p></p><p>DuckDuckGo 拒绝加载搜索页面，显示：“搜索结果出错。请重试。”</p><p>&nbsp;</p><p>有网友指出，在这次中断的大约一个小时之前，DuckDuckGo就出现了加载结果很慢、需要多次刷新或重新输入查询的情况。而最终显示的加载结果则是当前查询的各种答案的混合，以及来自其他最近查询的关键字。假设输入"LG 显示器型号"等，按回车键后，每个结果都是"导电性错误铝制 LG 显示器更换部件"或“可以使用洗洁精清洁铝制 LG 显示器吗？月光骑士第 2 季传闻……”依此类推，对于每一个结果都是如此。</p><p>&nbsp;</p><p>事件似乎在太平洋时间上午 7 点 20 分左右结束，经过五个多小时的中断后，服务开始恢复上线。</p><p></p><h2>一次更新引起大范围的宕机</h2><p></p><p>&nbsp;</p><p>虽然微软并没有对引发中断的原因做出解释，但微软确认， UTC 时间5月23日 08:46出现问题，并指出“用户可能无法访问 Microsoft Copilot 服务。”</p><p>&nbsp;</p><p>互联网平台经常出现故障，这已是司空见惯。然而，当像微软这样的大型供应商的技术受到影响时，后果可能极其严重，并波及广泛。尽管此次故障似乎并未影响所有平台的使用，但大量用户的投诉表明，其影响范围可能相当广泛。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/0c/0cc997ffe9ff0123a8c2714fe34735d0.jpeg" /></p><p></p><p></p><blockquote>GitHub Copilot 坏了吗？我真的得自己输入代码了？呃。</blockquote><p></p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/44/44684a8af9c8314f64201d1405e728e5.jpeg" /></p><p></p><p></p><blockquote>我于美国东部时间凌晨 2:30 起床，发现使用ChatGPT-4的Copilot不工作了。</blockquote><p></p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/77/777f458ced20fcff497f3d8b6f86c887.jpeg" /></p><p></p><p></p><blockquote>Copilot停止工作，这意味着我今天的工作效率将受到严重影响。</blockquote><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/35/35c36a6210cf2686169666f9e0602b3d.jpeg" /></p><p></p><p></p><blockquote>印度的Bing不工作了。</blockquote><p></p><p>&nbsp;</p><p>此前，微软在其<a href="https://twitter.com/MSFT365Status/status/1793564067191259271">@MSFT365Status X 页面</a>"上表示：“我们正在调查用户可能无法访问 Microsoft Copilot 服务的问题。我们正在努力找出问题的原因。更多信息可以在管理中心的 CP795190 下找到。”OpenAI 也<a href="https://status.openai.com/">证实了这个问题</a>"，并表示正在调查。</p><p></p><p><img src="https://static001.geekbang.org/infoq/06/064b004cfb12a49c8a7c32acc8b35d2b.jpeg" /></p><p></p><p>&nbsp;</p><p>微软将这次事故原因归于“一次软件更新出错”，他们随后将请求转移到备用服务组件，想尽快让服务恢复。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/9c/9cf7481a36cde17494041c5ab394c575.jpeg" /></p><p></p><p>数小时后，Bing 和 Copilot 恢复上线，但 Android、iOS 和 Windows 应用程序仍然无法加载。</p><p>&nbsp;</p><p>另一方面，由于Bing以及 API 正成为网络底层基础设施的重要组成部分，因此这次故障中调用Bing搜索结果的DuckDuckGo也受到影响。</p><p>&nbsp;</p><p>但问题貌似比API调用更为复杂一点，有网友表示，微软在几个小时前就设法修复了 Bing 前端，但依赖于Bing搜索结果的DuckDuckGo仍然没法正常工作。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e1/e1856d49fdde0dac9dd46364c706acc9.jpeg" /></p><p>&nbsp;</p><p>这次事故，除微软系的搜索引擎几乎全部没法提供服务，有网友指出他并不想因此转投Google搜索。&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ef/ef1f3e406e55e76111f67727723e918a.jpeg" /></p><p></p><p>尽管微软投入大量资金，并在该服务中加入了人工智能，但 Bing 的市场份额其实仍然不算高。今天问题的主要影响可能是现有用户纷纷转向其他平台，而这是微软无法承受的。</p><p>&nbsp;</p><p></p><h2>搜索看起来完全被垄断了</h2><p></p><p>&nbsp;</p><p>这次故障确实凸显了服务的相互依赖性。“有趣的是，DuckDuckGo坚持声称他们使用的搜索结果‘远不止 Bing’，然而 Bing 一旦瘫痪，他们的搜索功能就会完全瘫痪，无法显示任何结果。”网友评价道。</p><p>&nbsp;</p><p>虽然到东部时间上午上班时，Bing服务已经基本恢复，但人们在庆幸之余又感到一丝担忧。</p><p>&nbsp;</p><p>长期占据市场主导地位的搜索平台Google上周刚刚宣布并首次发布AI概览（AI Overviews）功能，将其作为整体搜索服务的默认添加项。如果大家不想要AI响应但仍想使用Google，则可以在菜单中寻找新的“Web”选项，或者按照相关说明在搜索中添加“&amp;udm=14”，通过这条神奇的秘技继续留在前生成式AI时代。</p><p>&nbsp;</p><p>相信很多朋友都曾对AI技术的幻觉、电力消费或者怪异的披萨配方感到失望，包括担心谷歌一方的更多问题，例如隐私、跟踪、新闻、搜索引擎优化乃至垄断权力等。但看似靠谱的大多数替代方案，都因今天早上的这次API中断而现出了原形。要想避免这种“牵一API而动全身”的尴尬局面，整个行业和关注此事的个人都需要好好费一番工夫。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/f2/f271708e567828ad29fbacff4ed06cdf.png" /></p><p></p><p>2023年4月至2024年4月，StatCounter统计的搜索引擎市场份额</p><p>&nbsp;</p><p>绝大多数以Google“替代方案”自居的搜索工具，其实都依赖于Google、Bing或者Yandex，也就是说这三大搜索引擎正维持着太大的全球索引体系。</p><p>&nbsp;</p><p>Yandex总部位于俄罗斯，其服务范围目前还没有覆盖到世界上的每个角落。Bing广泛为合作方提供服务，特别是DuckDuckGo，但其基于广告的收入模式和隐私方面的记录曾经引起过一些争议。因此在找到阻止微软跟踪脚本的办法之前，DuckDuckGo公司CEO兼创始人Gabriel Weinberg在Reddit的回复中，解释了为什么像他们这样的厂商无法走通完整的DIY路线：</p><p>&nbsp;</p><p></p><blockquote>……我们的大部分传统链接和图像都是从Bing那边下载获取的……实际上只有两家公司（谷歌和微软）拥有高质量的全球网络链接索引（因为我相信他们在这方面的年度开销超过了10亿美元），所以任何其他搜索引擎都需要借助其中一方或者两方才能提供主流搜索产品。&nbsp;顺带一提，地图业务也是如此——只有体量最大的巨头才能负担得起部署卫星、并派出地面车辆为各个社区拍摄街景照片的成本。</blockquote><p></p><p>&nbsp;</p><p>Bing确实在给微软赚钱，虽然还没有完全盈利。尽管目前的业务重点几乎完全集中在自家AI聊天机器人版Bing之上，保持搜索索引储备及API开放仍旧符合微软的利益定位。然而，如果微软决定取消API访问，或者这项服务的可靠性持续下滑，那么谷歌的市场地位将进一步提升。</p><p>&nbsp;</p><p>面对这样的现实，想要搞点动静出来的搅局者们究竟该何去何从？</p><p>&nbsp;</p><p>当前，许多非GBY搜索引擎都依赖于Common Crawl，或者至少是此为起点。Common Crawl属于公共资源，包含过去17年间超过2500亿个网页，且每月新增网页数量高达数十亿。从某种意义上讲，我们需要的全部信息都在其中，这也让Common Crawl成为众多大语言模型训练中的基本组成部分。</p><p>&nbsp;</p><p>换句话说，接下来的重点在于弄清如何以用户喜欢的方式组织、排序并显示这些结果，同时找到一条可行的盈利路径。</p><p>&nbsp;</p><p>DuckDuckGo和Kagi的创始人已经亲自证明，如果想让业务超越Crawl之外，就必须投入大量资金和基础设施。谷歌有广告可供销售，还有浏览器、Android手机及一众受益于其引擎的捆绑服务。微软这边同样有广告业务，外加一款浏览器和捆绑有Bing引擎的主导级操作系统。如果想要彻底脱离这两者（或者说也算上Yandex），其他厂商该如何交付搜索结果？这是个问题。</p><p>&nbsp;</p><p>现在大模型成为主要的探索方向，至少AI 能以搜索助手的形态帮助大家摆脱典型的GBY产品。</p><p>&nbsp;</p><p>“我已经无法忍受网络搜索那粗糙的结果，甚至宁愿直接上Reddit查找爱好者们的意见。”这应该是很多人的心声。当然，Reddit的帖子也在被纳入ChatGPT、谷歌乃至其他AI解决方案的关注范围。</p><p>&nbsp;</p><p>不过，尽管基于GBY来源的其他引擎往往会显示不同的结果，甚至偶有亮眼表现，但全球搜索被两到三个信息源绑架的现实已成定局。&nbsp;</p><p></p><h2>小众搜索：只能是“第二辆车”</h2><p></p><p>&nbsp;</p><p>在被大厂“垄断”的主流搜索之外，也有很多小众搜索。</p><p>&nbsp;</p><p>个人开发者<a href="https://seirdy.one/posts/">Rohan Kumar</a>"一直在用自己的索引更新一份带有大量注释的搜索引擎列表，借此我们也可以了解到更多关于“GBY”（即Google、Bing和Yandex）三巨头之外的搜索世界及其运转逻辑。</p><p>&nbsp;</p><p>Kumar三年来统计出了一份榜单，该英文榜单洋洋洒洒数千言，但列举的引擎中只有少部分拥有自己的通用索引。</p><p>&nbsp;</p><p>Kumar最喜爱的两个分别是在市场竞争中保持住生命力的Mojeek，以及为各类主要引擎提供良好补充的Stract。Right Dao的搜索结果“速度很快且质量很高”，部分原因在于其爬虫选择了维基百科作为起点。Yep的覆盖范围更广，会显示与查询相关的网站链接与返回结果，并承诺与创作者们分享广告收入。</p><p>&nbsp;</p><p>这些搜索都在某种程度上表现出了实力，但仍给人一种强烈的“家庭第二辆车”的感觉——不足以成为“出行首选”。</p><p>&nbsp;</p><p>Kumar 还分享了其他更为小众的引擎。对于大家偶尔听说过的这类搜索引擎，其很可能被列入了“半独立索引”的部分。这是因为当其自身的搜索结果不够强大时，它们都会借用GBY索引。比如推崇加密货币、备受争议的Brave引擎，以及不少直接“转载”GBY结果或者将其相关链接塞进自身显示内容的引擎。</p><p>&nbsp;</p><p>Kagi 还要求使用者注册账户，并将其索引Teclis与Google、Bing、Yandex、Mojeek乃至其他索引（包括Brave）配合使用。Kagi的创始人对于基于AI的搜索未来有着鲜明的观点，认为这将以“不可扩展”的方式危害整个搜索世界。</p><p>&nbsp;</p><p>也许各位有不同观点，但需要注意的是，如果GBY三巨头的服务出现问题，Kagi这样的搜索工具也同样会受到波及。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://x.com/MSFT365Status">https://x.com/MSFT365Status</a>"</p><p><a href="https://arstechnica.com/gadgets/2024/05/bing-outage-shows-just-how-little-competition-google-search-really-has/">https://arstechnica.com/gadgets/2024/05/bing-outage-shows-just-how-little-competition-google-search-really-has/</a>"</p><p><a href="https://www.bleepingcomputer.com/news/microsoft/microsoft-outage-affects-bing-copilot-duckduckgo-and-chatgpt-internet-search/">https://www.bleepingcomputer.com/news/microsoft/microsoft-outage-affects-bing-copilot-duckduckgo-and-chatgpt-internet-search/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/dE6w34Ke2az1QqFtGmPz</id>
            <title>快来加入 NVIDIA 初创加速计划，获得产品折扣、技术指导、投融资对接等全方位助力！</title>
            <link>https://www.infoq.cn/article/dE6w34Ke2az1QqFtGmPz</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/dE6w34Ke2az1QqFtGmPz</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 May 2024 07:46:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 科技创业, NVIDIA 初创加速计划, 技术支持, 全球生态项目
<br>
<br>
总结: 科技创业是充满挑战与未知的领域，NVIDIA 初创加速计划为技术创业公司提供全球生态项目支持和加速发展服务。 </div>
                        <hr>
                    
                    <p>科技创业无疑是当下最热门的领域之一，但对于投身其中的创业者而言，这条路或许没有想象中好走。从产品的孵化、打磨到落地，从团队的组建到市场的开拓，每一步都充满了挑战与未知。资金的压力、激烈的市场竞争、人才的渴求、技术的风险、管理的难题以及法规政策的变化，这些都是科技创业公司必须正视的困难。</p><p></p><p>作为一家创业公司，如果你想找到解决上述问题的最快路径，欢迎加入由 NVIDIA 发起的加速创业公司发展的全球生态项目——NVIDIA 初创加速计划。该项目联合了国内外知名的投资机构，创业孵化器，创业加速器，行业合作伙伴以及科技创业媒体等，打造创业加速生态系统，能够提供产品折扣，技术支持，市场宣传，融资对接，业务推荐等一系列服务，加速创业公司的发展。</p><p></p><p>目前该计划接受所有技术创业公司（AI 和深度学习、数据科学、高性能运算、网络、图形、AR/VR 和游戏等）的申请。与传统加速器不同，NVIDIA 初创加速计划将为创业企业生命周期的各个阶段提供支持。NVIDIA也会与会员密切合作，并为其提供更好的技术工具、最新的资源以及与投资者交流的机会。</p><p>无论您当前所处的融资阶段为何，NVIDIA 初创加速计划都欢迎您的加入！</p><p></p><p>NVIDIA 初创加速计划 (NVIDIA Inception)&nbsp;是 NVIDIA 为初创企业所提供的一个加速平台，目前全球已有超过 15,000&nbsp;名成员。作为全球最大的初创企业生态系统之一，NVIDIA 初创加速计划成员遍布 100 余个国家。今年，NVIDIA 初创加速计划中国会员企业数量正式突破 2,000 家！伴随这一里程碑的到来，2023 NVIDIA 初创企业展示的序幕也将拉开。</p><p></p><p>NVIDIA 初创加速计划于 2016 年落地中国，数以千计的初创企业受益于 NVIDIA 强大的技术实力和开放的合作生态。NVIDIA 为会员企业提供了一系列全面而有针对性的资源，从技术指导到软硬件支持，从市场宣传到投融资及大企业对接，在各个方面助力会员企业发展。8 年来，已经有数十家会员公司上市，上百家成为独角兽企业。在 NVIDIA 领先技术的助力下，会员能够更快地推进产品开发、优化算法、提高性能，在市场上取得竞争优势，同时他们也正在通过诸如 GTC 这样的国际化平台走向世界，拥抱更多未知的机会和可能性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/58/5845e7790d424328ea9d405c2a8fa10b.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/GKslHaQfg1pbw32IpOp4</id>
            <title>“数字中国建设峰会”首次结合 AI 大模型，打造“智慧观展”新体验</title>
            <link>https://www.infoq.cn/article/GKslHaQfg1pbw32IpOp4</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/GKslHaQfg1pbw32IpOp4</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 May 2024 05:02:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数峰会, AI技术, 支付宝, 智能助理
<br>
<br>
总结: 本文介绍了第七届数字中国建设峰会首次应用AI技术，通过支付宝智能助理为参展公众提供多种服务和互动，包括了展前、展中、展后的全流程服务，同时推出了官方一站式会务服务平台——“数字峰会”支付宝小程序，为参会人员提供智能服务和数字化便捷服务，展示了数字科技服务于民的创新探索。 </div>
                        <hr>
                    
                    <p>“数峰会如何报名？会展中心哪里有充电宝？帮我做个福州一日游攻略”5 月 24 日，第七届数字中国建设峰会正式开展，峰会通过引入人工智能、数字人、官方小程序、AR等技术，极大提升展会的智能化体验，引领科技办会新风潮。据悉，这是数字中国建设峰会首次应用AI大模型技术创新。</p><p>&nbsp;</p><p>参加本届峰会的公众，下拉支付宝首页唤起AI智能助理，就能通过自然对话，了解本届峰会的议程设置、参展须知、活动亮点等信息，也能实时查询附近的充电宝、厕所、餐馆等服务信息，让 AI 为参展公众提供多种服务和互动。此外，通过AI技术，公众可以一键连接官方一站式会务服务平台——“数字峰会”支付宝小程序，还能快速唤起展会服务数字人，通过可看可听的播报形式解答常见问题，带来峰会最新资讯动态。</p><p><img src="https://static001.infoq.cn/resource/image/bb/ed/bb66yy843ff4006a9a157dfd0b3039ed.png" /></p><p></p><p></p><h2>跟随支付宝AI 智能助理，轻松逛 44 万平米展厅</h2><p></p><p></p><p>本届数字中国建设峰会，深度结合支付宝智能助理打造“智慧观展”新体验，在“展前、展中、展后”全流程中，为公众提供峰会的AI咨询应答及全流程服务。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/f3/53/f34aee67631e008ec8f0e9f18cd06c53.png" /></p><p>图：支付宝智能助理提供智能服务和问答</p><p>&nbsp;</p><p>（1）展前——观展注册与路线计划服务等</p><p>基于公众参展前的需求，智能助理可提供本届峰会举办信息和背景介绍、看展的注册入口、天气查询、路线换乘方案查询、呼叫网约车等，便于公众出发前进行出游规划的制定。</p><p></p><p>（2）展中——峰会新闻播报与生活类服务提供等</p><p>当公众来到海峡会展中心或烟台山AI街区，可询问智能助理有哪些展区具有特色亮点、如何用餐、哪里借用充电宝、现场游玩攻略，还能快速观看3D建模的数字展馆等。</p><p></p><p>（3）展后——城市本地游服务等</p><p>观展后，公众可通过智能助理询问附近美食店铺、领取店铺优惠、查询福州当地景点及特色打卡地、制定福州游攻略、查询并预定返程火车票或机票等。</p><p>&nbsp;</p><p>“通过一场展，链接一座城”，本届峰会首次结合AI大模型能力，通过支付宝智能助理及背后链接的商家服务生态，更深层地提升了公众的看展体验，并进一步解决了公众在展会之外的城市游玩等周边需求，是展会利用创新科技打造智能展会的典型例证。</p><p>&nbsp;</p><p></p><h2>上线“数字峰会”支付宝小程序做峰会“百科全书”</h2><p></p><p></p><p>本届数字中国建设峰会也在近日首度推出官方一站式会务服务平台——“数字峰会”支付宝小程序。该小程序由福州市大数据发展委员会、福州市会展会务集团与支付宝共同打造，用AI+数字化的形式，助力参会人员高效参展逛展，定制化的“&nbsp;AI 峰会助理”和“展会服务数字人”，在线上提供“百科全书”般服务能力，在支付宝搜“数字峰会”即可体验。</p><p>&nbsp;</p><p>此外，峰会官方支付宝小程序还提供国际支付、出行等数字化便捷服务，为侨胞提供贴心的参会服务。峰会小程序由四大服务板块组成：</p><p>（一）“展览服务”：数字展馆、团体报名、出行码、支付码、AR活动；</p><p>（二）会务服务：参观注册、峰会助理、数字主播；</p><p>（三）场馆导览；</p><p>（四）新闻中心。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/55/bc/5578c248611abc82798510f559c761bc.png" /></p><p>图：“数字峰会”支付宝小程序提供的智能服务和数字人信息播报</p><p></p><p>“场馆导览”中的数字展馆通过 3D 建模技术复刻线下展馆实际布局，共有峰会 11 个类型板块及其数百个展商可从“数字峰会”支付宝小程序上点击打开，线上就可了解展商的展厅样式、公司背景资料以及联系方式等，促进各商家快速交流合作。此外，为满足老年人群便利使用，还支持一键开启“长者模式”，更大的字号和页面布局优化，让老年朋友使用体验更好。峰会还采用蚂蚁灵境数字人平台，以福建广播电视台主持人为原型，复刻了AI展会服务数字人，用于现场接待引导、常见问题咨询答疑等。</p><p>&nbsp;</p><p>得益于中国“数字基建”高水平发展，出现了如支付宝这样涵盖了“食住行游购娱”等多方面服务的数字生活服务平台，而繁荣的数字生态，是中国打造“全民AI”的必备土壤。“一场展会+一个AI技术+一个APP”，即可畅行展会、畅行全城，为未来更多展会提供了更好的“智慧办展、智慧观展”思路借鉴，这是本届数字中国建设峰会突破性的创新探索，也是中国数字科技服务于民的又一案例。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/1LCxaEy6CHZQekWoM6E6</id>
            <title>第一波收割完的AI创企要跑路了？6年来仅做了一款产品，问世30天就彻底失败，Ai Pin公司10亿美元求“卖身”</title>
            <link>https://www.infoq.cn/article/1LCxaEy6CHZQekWoM6E6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/1LCxaEy6CHZQekWoM6E6</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 May 2024 05:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Humane, 初创公司, 可穿戴设备, 融资
<br>
<br>
总结: 美国加州旧金山的初创公司Humane考虑接受收购，成立5年未公开产品但融资数额巨大，联合创始人来自苹果公司，公司目标是开发AI定制软件平台和消费设备，最新产品Ai Pin具有投影显示和人工智能功能，但被批有严重缺陷，导致公司开高价求收购。 </div>
                        <hr>
                    
                    <p>当地时间5月22日，据外媒报道，总部位于美国加州旧金山的可穿戴设备初创公司Humane正考虑接受收购。彭博社援引消息人士称，该公司的定价在 7.5 亿至 10 亿美元之间，出售过程正处于早期阶段。</p><p></p><h2>成立5年未公开过任何产品，拿着PPT融了10多亿元？</h2><p></p><p>&nbsp;</p><p>据公开信息显示，Humane一家是由苹果前设计和工程团队成员 Imran Chaudhri 和 Bethany Bongiorno 于2018年创立的AI硬件初创公司，公司管理层层包括来自苹果公司的关键人物，Bongiorno 担任CEO，Chaudhri 担任董事长兼总裁。这家初创公司的CTO Patrick Gates 也来自苹果。</p><p>&nbsp;</p><p>创办Humane之前，Chaudhri 曾在苹果公司担任设计师长达20 年，据报道于 2017 年被苹果公司解雇，Bongiorno在苹果公司工作了8年，担任 iOS 和 macOS 的软件工程总监，并于 2016 年离职。两人很可能都知道 Vision Pro 的长期开发过程。</p><p>&nbsp;</p><p>此外，据报道，在这家初创公司成立的5年间，约有 90 名前苹果员工在这家 200 人的团队中工作或曾为其效力。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/8a/8a34f25101dce550ee382cb968296953.png" /></p><p></p><p>Humane 联合创始人 Bethany Bongiorno 和 Imran Chaudhri</p><p>&nbsp;</p><p>从成立之初，Humane就一直笼罩在神秘之中。公司成立的前5年时间里，关于Humane公司产品的相关报道寥寥无几，其向外界释放的为数不多的产品信号就是公司的目标是开发专为人工智能（AI）量身定制的突破性软件平台和消费设备。</p><p>&nbsp;</p><p>但出人意料的是，即使没有任何一款产品问世品，Humane仍然在过去几年完成了多轮大额融资。</p><p>&nbsp;</p><p>2019年6月，有消息透露Humane完成了一轮种子融资，但融资的具体金额并未公开。</p><p>&nbsp;</p><p>2021年9月1日，Humane 宣布其 B 轮融资已筹集 1 亿美元。此轮融资由 Tiger Global Management 领投，软银集团、BOND、Forerunner Ventures、Qualcomm Ventures LLC 等参投。此轮融资将使 Humane 能够扩大业务规模，并继续执行和拓展其使命，以实现人类与计算之间的下一次转变。</p><p>&nbsp;</p><p>当时，Humane 联合创始人 Imran Chaudhri 和 Bethany Bongiorno 表示：“Humane 是一个人们可以通过真正的设计和工程合作实现创新的地方。我们是一家体验式公司，致力于创造造福人类的产品，打造以人为本的技术——一种超越我们今天所知的更个性化的技术。我们都在等待新事物，一种超越我们一直生活的信息时代的东西。在 Humane，我们正在为所谓的智能时代打造设备和平台。我们致力于打造一种不同类型的公司，以信任、真理和快乐为价值观。在合作伙伴的支持下，我们将继续扩大团队规模，吸纳不仅对彻底改变我们与计算交互的方式充满热情，而且对构建方式也充满热情的个人。”</p><p>&nbsp;</p><p>Tiger Global 合伙人 Chase Coleman 表示：“Humane 的员工素质令人难以置信。这些人为全球数十亿人打造并交付了变革性产品。他们打造的产品具有开创性，有可能成为未来计算的标准。”</p><p>&nbsp;</p><p>在2023年完成的最新一轮融资（C 轮融资）中，Huamne又筹集到了1亿美元，使总融资额达到 2.3 亿美元，投资者包括 Kindred Ventures（领投）、SK Networks、LG Technology Ventures、微软、沃尔沃汽车科技基金、Tiger Global、高通风险投资公司和 OpenAI 首席执行官兼联合创始人 Sam Altman等。</p><p>&nbsp;</p><p>Humane 与微软的合作主要是体现在其利用微软的云基础设施搭建技术平台，同时，Humane 也将OpenAI 的技术集成到其设备中。与 LG 和沃尔沃的合作也暗示了其在家庭技术和汽车产品方面的潜在应用。</p><p></p><h2>首款产品被批有严重缺陷，Huamne开高价求被收购</h2><p></p><p>&nbsp;</p><p>而就在一个月前，该公司刚刚推出了经过大肆宣传、售价699美元的AI Pin产品。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/84/84a4cc2630c7591f921dd238dac13e19.png" /></p><p></p><p>图片来源：Humane 官网</p><p>&nbsp;</p><p>事实上，Humane&nbsp;于去年6月就亮相了 Ai Pin，这是一款具有投影显示和人工智能功能的可穿戴设备。该消息在美国开启了一段预购期，但 Ai Pin的最终上市时间被推迟到了今年的4月中旬。</p><p>&nbsp;</p><p>这款可穿戴设备配有运动摄像头与传感器，可帮助识别周遭环境，它由高通芯片提供支持并利用人工智能。它有点像Narrative Clip，一款命运多舛的生活记录相机。这款方形设备Ai Pin配有摄像头和麦克风，以及深度和运动传感器，用于收集数据，并由 Snapdragon 处理器进行处理。语音控制是该产品的核心，这似乎是 Siri 等智能手机助手的合乎逻辑的下一步。该设备通过“声波扬声器”或配对的蓝牙耳机与佩戴者进行通信。</p><p></p><p><img src="https://static001.geekbang.org/infoq/24/24fd6906916077d828d7e61c512ed7ee.gif" /></p><p></p><p>颇具争议的点是，这款产品没有屏幕——实际上，但有一个触摸板。这款产品还能对手势做出反应。Humane公司称，与无数其他承诺让我们摆脱屏幕依赖的产品不同，Ai Pin 的设计目的是为了不依赖智能手机。这需要通过基于 T-Mobile 的 Humane 品牌无线网络来实现。</p><p>&nbsp;</p><p>据介绍，Ai Pin 产品另一个吸引人的视觉元素是激光墨水显示屏，它可以将来电等文本投射到用户的手掌上，代替触摸屏。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1d/1d919fd6692ebf75ba83bd94630e6181.png" /></p><p>&nbsp;</p><p>该设备附带一个“电池增压器”，因此用户可以热插拔电源。Pin“采用独特的两件式设计，由主计算机和电池增压器组成。它们通过磁性连接并通过衣服和服饰无线供电，使您可以以多种方式佩戴 Ai Pin。凭借其永久电源系统，用户可以在旅途中热插拔电池增压器，确保不间断使用和全天电池寿命。”</p><p>&nbsp;</p><p>但在发布之后，AI Pin被广泛批评为存在缺陷且未能兑现承诺。</p><p>&nbsp;</p><p>由于标价699 美元，外加每月 24 美元的定期订阅服务（该服务会为用户提供一个电话号码和无限流量，以便用户进行尽可能多的查询），Ai Pin似乎在资金紧张的消费市场很难销售。从某些方面来看，这款设备似乎是一个在寻找问题的解决方案，许多最初的评论都表示，Ai Pin的功能实际上并不比智能手机的功能多多少。</p><p>&nbsp;</p><p>如今，Huamne正希望以10亿美元价码寻求收购。彭博社则在关于收购新闻的报道中表示，Humane已经为此事聘请了一名财务顾问，但能否收购成功尚未确定。</p><p>&nbsp;</p><p>Humane方面的代表没有立即回应置评请求。</p><p>&nbsp;</p><p>据报道，结合Humane推出AI Pin一个月后即寻求收购的情况，对于这家希望把握住全球AI技术热潮的设备制造商来说，恐怕是已经感常见到业务或将迎来低谷。</p><p>&nbsp;</p><p>自从初创公司OpenAI向公众推出ChatGPT AI聊天机器人以来，AI技术的应用在过去两年间呈现出爆发式的增长态势。短短两个月内，全球过亿用户涌入这项技术，也掀起各大企业巨头的踊跃投资与战略转变，包括苹果、亚马逊、微软，以及谷歌母公司Alphabet和Facebook母公司Meta等。</p><p>&nbsp;</p><p>但与大多数着眼软件的厂商不同，Humane希望通过一款方形、配有光滑边缘设计的产品来推销其AI成果，让消费者把设备佩戴在衬衫或包包上并保持摄像头和传感器朝外。Humane表示，这款设备代表继智能手机之后的下一形态，人们可以通过AI技术与应用软件和服务进行交互，例如联系网约车或听音乐。</p><p>&nbsp;</p><p>尽管Humane的承诺野心勃勃、产品演示也令人印象深刻，但这款产品还是引发了一波又一波的负面评论。来自技术媒体CNET的Scott Stein更是直言不讳地表示，这款产品“有着疯狂的概念，但在日常使用体验上则令人沮丧。”Humane公司表示正在努力改进产品设计。</p><p>&nbsp;</p><p>与此同时，三星、谷歌和微软等公司已经稳步将AI技术添加至自家设备当中，包括通过专门的软件以及在计算机键盘上添加新的AI功能按钮。预计苹果公司也将宣布对其iPhone、iPad和Mac设备支持的软件进行重大AI变革。</p><p>&nbsp;</p><p>尽管其首款产品存在严重的软件缺陷和硬件问题，但这家初创公司显然认为其价值在 7.5 亿至 10 亿美元之间。</p><p></p><h2>Humane这是割完韭菜要跑路了？</h2><p></p><p>&nbsp;</p><p>不难发现，随着AIGC技术的高歌猛进，人工智能（AI）与穿戴设备的结合日益紧密，这一新兴市场吸引了众多厂商和投资者的目光。然而，在这波AI穿戴设备的热潮中，一些厂商似乎过于追求短期的市场利益，而忽视了产品研发和用户体验的长期价值，这种现象很难不被认为是在“割韭菜”。</p><p>&nbsp;</p><p>从苹果Vision到Meta Quest，都曾被寄予厚望，认为是通往元宇宙的第一步。但刚发布产品这款产品却因粗糙、收费高被广泛诟病，可以说与元宇宙基本不沾边。此外，产品发布一个月就寻求出售的做法多少有些让人难以捉摸，因此被质疑割韭菜也就不足为奇了。</p><p>&nbsp;</p><p>不只AI穿戴设备领域如此，放眼全球市场，不乏一些A硬件厂商在资本的助推下迅速崛起，但又在短时间内因融资烧光而倒闭，Airware就是一个典型的例子。</p><p>&nbsp;</p><p>这家公司曾试图生产自己的商用无人机硬件，并开发了一套云端软件系统，帮助建筑、钻探和保险行业的企业客户使用无人机的航拍图像来评估破坏情况。然而，随着市场竞争的加剧和商用无人机硬件公司附带软件的逐渐成熟，Airware逐渐被边缘化，最终因融资烧光而倒闭。</p><p>&nbsp;</p><p>AI硬件厂商“割韭菜”现象的出现，一方面是由于市场竞争的激烈和资本的逐利性。在资本的推动下，一些厂商为了迅速占领市场份额，不惜采取低价策略、夸大宣传等手段来吸引消费者。然而，这种做法往往忽视了产品研发和用户体验的长期价值，导致产品质量参差不齐、售后服务不到位等问题。另一方面，一些厂商缺乏对市场的深入了解和精准定位，盲目跟风、一哄而上，导致产品同质化严重、市场竞争无序。</p><p>&nbsp;</p><p>技术越热，市场和资本越需要冷静，消费者更需要冷静，否则一不小心，谁成了韭菜真说不定。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.cnet.com/tech/mobile/humane-maker-of-wearable-ai-pin-is-exploring-a-sale-report-says/">https://www.cnet.com/tech/mobile/humane-maker-of-wearable-ai-pin-is-exploring-a-sale-report-says/</a>"</p><p><a href="https://humane.com/media/humane-completes-series-b-funding-round">https://humane.com/media/humane-completes-series-b-funding-round</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/IOOKB3tw2u3EQkyEAT9K</id>
            <title>10+AI系列专题，拆解从0到1构建大模型架构平台的实现路径｜ArchSummit</title>
            <link>https://www.infoq.cn/article/IOOKB3tw2u3EQkyEAT9K</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IOOKB3tw2u3EQkyEAT9K</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 May 2024 01:46:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI大模型, ArchSummit深圳站, 专题, 数据驱动
<br>
<br>
总结: AI大模型正在颠覆传统架构设计思路和路径，6月14-15日ArchSummit深圳站举办10大AI系列专题，涵盖AI运维、AI大模型中台、AI安全与风控等热门话题，解析企业如何搭建大模型时代的架构平台。专题内容涉及大模型的集成与部署策略、低代码与AI结合、数据与人工智能相互驱动、AIOps业务场景实践、AI大模型中台实践探索、AI时代的安全与风控、高效算力基建与性能优化、LLM作为新一代‘OS’的探索、大模型基础框架等。 </div>
                        <hr>
                    
                    <p>AI大模型正在颠覆传统架构设计思路和路径，<a href="https://archsummit.infoq.cn/2024/shenzhen/">6月14-15日ArchSummit深圳站</a>"，10大AI系列专题，从底层基础到顶层应用多角度，覆盖AI运维&nbsp;、AI大模型中台、AI安全与风控及大模型算力等热门话题，解析企业如何从0到1搭建大模型时代的架构平台。</p><p><img src="https://static001.geekbang.org/infoq/09/091d7d757337a339278c136ee290423e.png" /></p><p></p><p><a href="https://archsummit.infoq.cn/2024/shenzhen/track/1636">《基于大模型应用层的探索》专题</a>"将关注大模型的集成与部署策略，深入探讨如何从应用层面充分发挥大模型的优势，挖掘其潜在的巨大价值。从选择适合的大模型，到对这些模型进行精细化的性能调优等内容，我们将带领大家一步步理解大模型的运作机制和应用技巧。</p><p></p><p><img src="https://static001.geekbang.org/infoq/39/3992517f3ba2d92ce941582f25dca58b.png" /></p><p></p><p></p><p><a href="https://archsummit.infoq.cn/2024/shenzhen/track/1643">《低代码与AI结合》专题</a>"将深入研究低代码平台如何与人工智能技术相结合，提高开发效率。探讨在低代码环境中集成智能决策、自动化流程，以及构建灵活、高效的应用系统。</p><p></p><p><img src="https://static001.geekbang.org/infoq/61/614646b100f95d04aab6d8ad56ce683c.png" /></p><p></p><p><a href="https://archsummit.infoq.cn/2024/shenzhen/track/1640">《Data&nbsp;4&nbsp;AI和AI&nbsp;4&nbsp;Data方面的探索和实践案例》专题</a>"，将深入探讨数据与人工智能相互驱动的关系。分享在构建数据驱动AI系统时的最佳实践，包括数据质量管理、特征工程、数据增强等。同时涉及数据结构和数据治理、数据保护、数据管理优化等具体实践，以及超融合数据架构，在AI应用上所做的设计及优化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3c/3c41333f2188350fd525493128d9fa5f.png" /></p><p></p><p></p><p><a href="https://archsummit.infoq.cn/2024/shenzhen/track/1641">《AIOps&nbsp;业务场景最佳实践》专题</a>"将聚焦AIOps在不同业务场景中的实际成效，比如如何通过AIOps推动可量化的业务价值增长和效率提升？邀请来自可观测性、监控运维技术等领域的资深专家分享他们的AIOps实战经验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4e/4e98ba75a76ef1addd35511431aa29d5.png" /></p><p></p><p><a href="https://archsummit.infoq.cn/2024/shenzhen/track/1653">《AI大模型中台实践探索》专题</a>"将聚焦机器学习和深度学习模型的管理、部署、运维和监控的话题，帮助构建大模型中台的团队了解如何提升机器学习和深度学习、软件工程、大数据处理、模型部署和服务化、数据治理和隐私保护、业务理解等能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d9/d9ccc968cb104af7ee2fcb9ba09c6876.png" /></p><p></p><p><a href="https://archsummit.infoq.cn/2024/shenzhen/track/1651">《AI时代的安全与风控》专题</a>"将深入研究在人工智能时代如何构建安全可靠的系统。从AI引发的安全挑战到创新的风险管理策略出发，探讨在技术和业务层面如何应对安全和风险问题，包括互联网安全、业务风控等相关技术。</p><p></p><p><img src="https://static001.geekbang.org/infoq/af/aff03639baaaf397932fe63b366bfd1a.png" /></p><p></p><p><a href="https://archsummit.infoq.cn/2024/shenzhen/track/1639">《高效算力基建与性能优化》专题</a>"将深入挖掘构建高效算力基础设施的技术策略，包括云原生技术、容器化、微服务、GPU虚拟化、在离线混部等架构模式的实际应用案例，推进计算领域基建的高质量发展和创新性探索。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b9/b97429ca55a8d0e7412b21ea61fe63bb.png" /></p><p></p><p></p><p><a href="https://archsummit.infoq.cn/2024/shenzhen/track/1649">《LLM作为新一代‘OS’的探索》专题</a>"将深度剖析大型语言模型（LLM）在构建新一代创新应用过程中所发挥的作用（这里所说的OS是代指一种核心技术），介绍在未来系统构建、架构设计等角度中融入LLM的潜在益处，以及在系统层面解决的技术挑战。</p><p></p><p><img src="https://static001.geekbang.org/infoq/38/38f6016b5bb2707d7a503ed813bf358b.png" /></p><p></p><p><a href="https://archsummit.infoq.cn/2024/shenzhen/track/1646">《大模型基础框架》专题</a>"将深入探讨行业中大模型训练和推理的基础架构和关键技术，包括训练加速、多维并行、万卡集群、高性能算力等技术焦点，关注训练性能优化、推理部署策略等重要领域，助力人工智能领域进一步发展。</p><p></p><p><img src="https://static001.geekbang.org/infoq/86/8631d512bd54e8718cfc517c7995562e.png" /></p><p></p><p>与传统的云计算平台不同，智能计算平台有独有的网络架构（如&nbsp;IB，RoCE&nbsp;等），独特的虚拟化方式（基于虚拟机/容器的GPU&nbsp;虚拟化），独特的算力调度平台，海量的并行存储系统等。<a href="https://archsummit.infoq.cn/2024/shenzhen/track/1637">《智算平台建设与应用实践》专题</a>"将分享构建智算平台的技术要点，以及在实践和落地过程中所作的优化和踩过的坑。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2d/2d0e33209524e6b1ce89b63ae46eb340.png" /></p><p></p><p>6月14-15日ArchSummit深圳站，不见不散！</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>