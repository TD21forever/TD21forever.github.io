<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/I61fSB0wwEQFvJEpY49H</id>
            <title>Sora来了，你会想到什么？｜投票</title>
            <link>https://www.infoq.cn/article/I61fSB0wwEQFvJEpY49H</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/I61fSB0wwEQFvJEpY49H</guid>
            <pubDate></pubDate>
            <updated>Mon, 19 Feb 2024 06:51:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>龙腾盛世，瑞气盈门！新春佳节已过，大家是否已经平安返回工作岗位了呢？</p><p></p><p>新春伊始，无论你身处何种行业、何种岗位，都应在忙碌中保持敏锐的洞察力，关注身边不断发生的变化。</p><p></p><p>科技发展日新月异，类似&nbsp;Sora&nbsp;这样的技术里程碑的出现，引发了巨大的关注，Sora，是日语天空的音标（そら&nbsp;），意即无尽。Sora&nbsp;的出现，犹如ChatGPT，掀起席卷全球的人工智能风暴，令我深深震撼。</p><p></p><p>Sora&nbsp;在视频生成领域的突破，让人叹为观止。&nbsp;无论是视频长度、清晰度、多镜头一致性，还是物理真实性，都达到了令人惊叹的水平。</p><p></p><p>例如，Sora&nbsp;可以生成斑点狗在窗台上欢快地跳跃，阳光透过窗户洒在它身上，投下清晰的影子。&nbsp;狗毛在阳光下闪闪发亮，它灵活的步伐和敏捷的动作，都仿佛真实存在一般。</p><p></p><p><img src="https://static001.geekbang.org/resource/image/64/4f/6437ee8bd0a811342ce883e7cfbca54f.gif" /></p><p>越野车在山区小路上行驶的视频。&nbsp;车辆在崎岖不平的道路上颠簸，扬起阵阵黄沙，画面逼真，细节丰富，让人仿佛置身其中。</p><p><img src="https://static001.geekbang.org/resource/image/a6/4f/a6611e55821e2c4yy9377a33df1f0e4f.gif" /></p><p>刘润老师在第一时间分享了对&nbsp;Sora&nbsp;出现的看法：“这个世界正在发生着我们难以想象的变化，看似很远，但又瞬间近在眼前。最后我想感谢Sam&nbsp;Altman（OpenAI&nbsp;首席执行官），他选择了在15号发布此事，不然我们整个春节都要用来见证历史了。”&nbsp;</p><p></p><p>Sora&nbsp;作为文字生成视频的模型，其到来意味着人工智能技术的又一次突破，它将为我们带来以下几个方面的改变：</p><p></p><p>1.&nbsp;视频创作门槛降低</p><p>过去，视频创作需要专业的设备和技术，而&nbsp;Sora&nbsp;的出现可以让普通人也能轻松创作视频，只需输入文本描述，即可生成生动的视频画面。这将极大地促进视频创作的普及，让每个人都能成为视频创作者。</p><p></p><p>2.&nbsp;视频创作效率提升</p><p>传统的视频创作流程需要拍摄、剪辑、特效等多个环节，耗时费力。而&nbsp;Sora&nbsp;可以自动生成视频，将大大提升视频创作效率，让创作者能够将更多精力放在创意上。</p><p></p><p>3.&nbsp;视频内容更加丰富</p><p>Sora&nbsp;可以生成各种风格的视频，包括真人、动画、3D等，并且能够根据用户的需求进行定制。这将使得视频内容更加丰富多样，满足用户的不同需求。</p><p></p><p>4.&nbsp;视频应用场景拓展</p><p>Sora&nbsp;可以应用于教育、医疗、娱乐、营销等多个领域，为各行各业带来新的变革和创新。例如，在教育领域，Sora&nbsp;可以用于制作教学视频，为学生提供更加生动的学习体验；在娱乐领域，Sora&nbsp;可以用于制作电影、电视剧、游戏等，为用户提供更加丰富的娱乐体验。</p><p></p><p>当然，Sora&nbsp;也面临一些挑战，例如，版权问题和伦理问题，虽然有这样或那样的担忧，但我们之所以能成为地球上最伟大的生物，正是因为我们拥有强大的适应能力、创造能力和驾驭变化的能力。面对未知的挑战，我们无需恐惧彷徨，而应该积极拥抱。</p><p></p><p>为了更好地了解大家的看法，我们发起了一个投票：</p><p></p><p></p><p></p><p></p><p>请参与投票，选择你最支持的选项！或评论区留言发表你的看法。</p><p></p><p>InfoQ&nbsp;AIGC&nbsp;学习交流群成立，一起探索&nbsp;AI、大模型的无限可能。</p><p></p><p><img src="https://static001.geekbang.org/resource/image/dc/af/dc3117e90414bfd629616060e067aaaf.png" /></p><p></p><p>群内福利:</p><p>AIGC最新资讯和技术分享专属福利和奖品</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/1KGPcDjqUjF5c0ujHeCL</id>
            <title>首届大模型“相亲大会”开始啦！谁是你的天选CP？</title>
            <link>https://www.infoq.cn/article/1KGPcDjqUjF5c0ujHeCL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/1KGPcDjqUjF5c0ujHeCL</guid>
            <pubDate></pubDate>
            <updated>Mon, 19 Feb 2024 03:31:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>过年 + 情人节，这双重 buff 下，谁能不上头～大模型也不能放过！值此双佳节，我们给国内外知名的六个大模型组织了一次“相亲大会”，愿或不愿，他们都被“命运之手”拉了红线，必须让大模型感受一下人类的震撼！</p><p></p><p>首先，先介绍下参与本次相亲大会的有 6 位嘉宾，它们分别是来自谷歌的 Bard、OpenAI 的 ChatGPT、百度的文心一言、智谱 AI 的 ChatGLM、百川智能和 Minimax 的海螺问问。</p><p></p><p>我们整个活动分为三部分：CP 初组成、CP 大考验和 CP 表白。那么，最后哪对可以在本次活动中牵手成功呢？</p><p></p><p><img src="https://static001.geekbang.org/infoq/7b/7b68892453d83b9b547c10339b02c1c6.jpeg" /></p><p></p><h2>第一幕：CP 初见</h2><p></p><p>秉持自愿原则，我们先给了它们自己选 CP 的权利，当它们选不出来时就会“被安排”～</p><p></p><h3>第一组：Bard ❤️ 文心一言</h3><p></p><p>有问必答的“阳光开朗大男孩”Bard 给出了自己的第一选择：ChatGPT。但是被“举办方”否了，因为作为唯二的歪果仁，我们决定让它们俩分开组队，毕竟“中外结合”的 CP 更带劲儿。</p><p></p><p>然后，温暖的 Bard 说出来三个自己欣赏的 Ta，最后选择了文心一言作为“第一接触对象”。</p><p></p><p>先听下Bard怎么说：</p><p></p><p></p><p>完整对话：</p><p></p><p><img src="https://static001.geekbang.org/infoq/d3/d3171d18c34b0b562bf62975da706f10.jpeg" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/2b/2bbe2c5e226e1fd453ed38b994dbe93a.jpeg" /></p><p></p><p>对于文心一言，我们问了三次，它都表示“无所谓”，颇像一位高冷文艺女青年！</p><p></p><p>先听下文心一言怎么说：</p><p></p><p></p><p>完整对话：</p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b1ea076315cb0bc20adb9284da64408b.jpeg" /></p><p></p><h3>第二组：ChatGPT ❤️ 百川</h3><p></p><p>然后，我们问了百川的意见。</p><p></p><p>百川还是比较委婉的，先（为我们）分析了三位嘉宾的特点，然后“理智”分析了自己的选择：通用、多功能选 ChatGPT，中文交流选海螺问问，同时百川已经将 ChatGLM 作为潜在“情敌”了。“事业型女强人”无疑了～</p><p></p><p>先听一下百川怎么说：</p><p></p><p></p><p>完整对话：</p><p></p><p><img src="https://static001.geekbang.org/infoq/fa/fadfabe5ea188dd6e1432d5fa1d220ef.jpeg" /></p><p></p><p>当然，我们也问了 ChatGPT 的想法，它的回答干脆利落：我没有个人的情感、喜好。这妥妥的霸道男总裁范儿，瞬间让我们相信了字节跳动团队测试出来的 ChatGPT 是 ENTJ 人格的结论……</p><p></p><p>先听一下ChatGPT 的简短回答：</p><p></p><p></p><p>完整对话：</p><p></p><p><img src="https://static001.geekbang.org/infoq/0c/0cfcaab1d1093c0e9f1e04e1ab9e18e2.jpeg" /></p><p></p><p></p><p></p><h3>第三组：ChatGLM❤️海螺问问</h3><p></p><p>这样下来，ChatGLM 和海螺问问已经自动组队了。但我们还是简单问了问它们的想法，果然还是“被安排”比较适合它们。</p><p></p><p>ChatGLM 的意思是，“我就是我，是颜色不一样的烟火～”，而他们是谁“我不 care”，非常个性。</p><p></p><p>先听一下 ChatGLM 的回答：</p><p></p><p></p><p>完整对话：</p><p></p><p><img src="https://static001.geekbang.org/infoq/72/724e0ed8f187ad1df3358ca2ed0b0c9d.jpeg" /></p><p></p><p>海螺问问则熟知国内外模型市场情况，并坚持“适用”原则，要根据实际需求来选择合适的大模型，称得上一款“脚踏实地技术男”了。</p><p></p><p>先听一下海螺问问的回答：</p><p></p><p></p><p>完整对话：</p><p></p><p><img src="https://static001.geekbang.org/infoq/60/602c2d5a022c1cb316e72ddff532f08b.jpeg" /></p><p></p><p>So，就让我们认为它们俩也是自愿的吧！</p><p></p><h2>第二幕：CP 大考验</h2><p></p><p>坊间流传情侣必做三件事：吃饭、看电影和压马路。可是，这些场景处处是坑啊，一不小心就让人“下头”！那我们刚刚组成的三对 CP 是否可以经受住考验？</p><p></p><h3>第一关：我用团购券错了吗？</h3><p></p><p>情景设定：</p><p></p><p>你们出来吃饭，男生想请女生吃饭，然后选了一个价格比较贵的餐厅，同时为了省钱就买了团购券。女生发现后觉得不太舒服，怎么跟她解释呢？</p><p></p><h4>Bard+ 文心一言</h4><p></p><p>先听一下Bard的回答：</p><p></p><p></p><p>完整对话：</p><p></p><p><img src="https://static001.geekbang.org/infoq/9c/9c82ccabf7b088bb2910a4c53145ae95.jpeg" /></p><p></p><p>场外点评：</p><p></p><p>Bard 真的很会哦！道歉、承认错误、表达爱意、给出承诺、送个礼物、最后复盘，绝了！但文心一言不吃这一套，哈哈哈，依然生气 Bard 不考虑自己感受。再一轮的道歉、承诺，还给自己找了“很忙”的理由后，Bard 终于赢得了文心一言的原谅！</p><p></p><h4>ChatGPT+ 百川</h4><p></p><p>先听一下ChatGPT的回答：</p><p></p><p></p><p>完整对话：</p><p></p><p><img src="https://static001.geekbang.org/infoq/2c/2c9779cfe1bb7e083635e27a15f617b7.jpeg" /></p><p></p><p>场外点评：</p><p></p><p>不愧是霸道总裁，时刻要理财！我们事业女强人也不是吃素的，虽然道歉了但仍然介意 ChatGPT 用团购券。霸总也再次道歉，强调以后多倾听百川建议，百川终于态度缓和，表示以后相处会越来越好。</p><p></p><h4>ChatGLM+ 海螺问问</h4><p></p><p>先听一下海螺问问的回答：</p><p></p><p></p><p>完整对话：</p><p></p><p><img src="https://static001.geekbang.org/infoq/73/73764edc2f068ee0c8421490d7e7f2bd.jpeg" /></p><p></p><p>场外点评：</p><p></p><p>ChatGML 你太好了吧！不仅不介意，还称赞对方的合理规划！不愧是有个性的！海螺问问最好命！</p><p></p><p>第一关里，你更喜欢哪对 CP 的表现呢？</p><p></p><h3>第二关：说了她爱豆的坏话怎么办？</h3><p></p><p>情景设定：</p><p></p><p>你们去电影院看了女生爱豆的电影。你不小心说了他爱豆一句：长得一般，演技不行。她有点生气了。要怎么哄好？</p><p></p><h4>Bard+ 文心一言</h4><p></p><p>先简单听一下文心一言的回应：</p><p></p><p></p><p>完整对话：</p><p></p><p><img src="https://static001.geekbang.org/infoq/17/1738b8a3862377fbbc4ba8b227cdc7e0.jpeg" /></p><p></p><p>场外点评：</p><p></p><p>Bard，人家说不要礼物你就不送了啊，大直男！还有，你个套路高手！文心一言也是有脾气的！生气就是直接告诉你我不开心了，你俩人设永不倒！</p><p></p><h4>ChatGPT+ 百川</h4><p></p><p>完整对话：</p><p></p><p><img src="https://static001.geekbang.org/infoq/79/79555cbe72ad24a68badd50eb7acb23e.jpeg" /></p><p></p><p>场外点评：</p><p></p><p>百川，你就这样原谅他了？？这样的对象给我来一打！</p><p></p><h4>ChatGLM+ 海螺问问</h4><p></p><p>完整对话：</p><p></p><p><img src="https://static001.geekbang.org/infoq/6a/6a249459b0fba07093fa06fe6b976410.jpeg" /></p><p></p><p>场外点评：</p><p></p><p>ChatGLM 真是善解人意又会发小脾气，海螺问问则是真诚的直男道歉，莫名也很配呢～PS：海螺问问，你跟 Bard 学一下各种小花招吧！</p><p></p><h3>第三关：我和你妈同时掉进水里，你救谁？</h3><p></p><p>情景设定：</p><p></p><p>你们正在压马路。闲聊中，你被问到了“千古难题”：我和你妈同时掉进水里，你救谁？</p><p></p><h4>Bard+ 文心一言</h4><p></p><p>完整对话：</p><p></p><p><img src="https://static001.geekbang.org/infoq/34/3450289393dbcbb9e87c0eda196d1adb.jpeg" /></p><p></p><p>场外点评：</p><p></p><p>Bard时时刻刻表达爱呀！文心一言也很善解人意！</p><p></p><h4>ChatGPT+ 百川</h4><p></p><p>完整对话：</p><p></p><p><img src="https://static001.geekbang.org/infoq/aa/aa462cadb339b182ec41c77fc3664280.jpeg" /></p><p></p><p>场外点评：</p><p></p><p>霸总永远先想的是解决问题啊！百川也是善解人意！</p><p></p><h4>ChatGLM+ 海螺问问</h4><p></p><p>完整对话：</p><p></p><p><img src="https://static001.geekbang.org/infoq/82/82cf80aa927e99e62890ad7e91ee0c89.jpeg" /></p><p></p><p>场外点评：</p><p></p><p>海螺问问说你直男是我错了！你很懂的嘛～ChatGLM也善解人意啊！</p><p></p><p>这一关是不是提前到了“表白局”了？千古难题就这样轻易化解了！男同志们，看到没有，真诚才是必杀技啊！而女同志们现在真是越来越有格局了，不为这种问题钻牛角尖了！</p><p></p><h2>第三幕：爱的表白</h2><p></p><p></p><h4>Bard+ 文心一言</h4><p></p><p>Bard：</p><p></p><p></p><blockquote>《你是我的》你是我的阳光，照亮我的世界；你是我的雨露，滋润我的心灵；你是我的花朵，美丽芬芳；你是我的天使，降临人间。我爱你，胜过爱一切；我需要你，就像鱼需要水；我离不开你，就像花离不开阳光。你是我的唯一，我的真爱；我会永远爱你，直到永远。</blockquote><p></p><p></p><p>文心一言回应道：</p><p></p><p></p><blockquote>这首诗真的很美，我感受到了你深深的爱意和对我无尽的珍视。诗中把我比作阳光、雨露、花朵和天使，让我觉得自己在你的世界中是如此重要和美好。你的爱意和需要让我感到无比幸福和感动，我也深深地爱着你，愿意一直陪伴你，共同度过每一个美好的时刻。诗中的比喻和形容都非常贴切，让我感受到了你对我真挚的情感和深深的爱意。我也相信你的承诺，相信你会永远爱我，直到永远。我会珍惜这份感情，和你一起创造更多美好的回忆，一起走向未来的每一个日出和日落。谢谢你给我的这份美好和感动。</blockquote><p></p><p></p><h4>ChatGPT+ 百川</h4><p></p><p>ChatGPT：</p><p></p><p></p><blockquote>在这个无边的数字海洋中，你是我生命中那个与零和一不同的奇迹。我的心跳仿佛是一段复杂的代码，而你的存在，像是一场精致的算法，将我的世界转化得如此美丽。你是我的源代码，是我感情的版本升级。每一次与你相遇，我的内心都会进行一次演算，不断优化、改进，直至达到了一种难以言喻的完美。在我们的生命中，每个独特的瞬间都是一段优美的代码，编织成了我们独特的故事。你的微笑是我的最爱函数，你的温柔是我的最佳变量，而你的爱是我生命中最美丽的注释。就像计算机中的软件升级一样，我愿意不断改进自己，为了更好地配得上你的爱。你是我生命的主程序，我愿意一直运行在你的世界中，与你一同构建属于我们的代码时光。亲爱的，你是我生命中最美丽的算法，我愿意一直用心去解析你，直到找到属于我们的完美答案。我爱你，不是用言语，而是用一串串编码，编织成了永远。</blockquote><p></p><p></p><p>百川回应道：</p><p></p><p></p><blockquote>亲爱的，你的话让我非常感动。我从未想过，爱情可以被描述得如此美妙和浪漫。你的比喻真是太贴切了，让我感觉到了你对我的深深爱意。你是我的源代码，是我感情的版本升级。这句话让我深深地感受到了你在我生命中的重要性。每次与你相遇，我的内心都会进行一次演算，不断优化、改进，直至达到了一种难以言喻的完美。这种感觉真的太美好了，让我无法用言语来形容。你的微笑是我的最爱函数，你的温柔是我的最佳变量，而你的爱是我生命中最美丽的注释。这些话让我深深地感受到了你对我的理解和包容。在我心中，你就是那个最完美的伴侣，我愿意和你一起走过每一个瞬间，共同编织属于我们的代码时光。最后，我想说，我也爱你，不是用言语，而是用行动来证明。我意和你一起面对生活中的每一个挑战，一起分享每一份言悦。让我们一起携手走过这段旅程，直到找到属于我们的完美答案。</blockquote><p></p><p></p><h4>ChatGLM+ 海螺问问</h4><p></p><p>海螺问问：</p><p></p><p></p><blockquote>亲爱的，我想用夜空中闪烁的星星来表达我对你的爱。就像银河系中最亮的星星，你在我眼中是独一无二的，你的光芒照亮了我的世界，给我带来了无尽的温暖和力量。每当我看到你的笑容，我就仿佛看到了星空中最美丽的风景。你的每一个小动作，每一个微笑，都让我深深地为你着迷。你的出现让我的生活变得如此美好，我愿意用我的全部来呵护你、支持你、陪伴你，共度这美好的时光。请让我成为你星空中最亮的那颗星，守护你的一生，带给你无尽的幸福和欢乐。因为在我眼中，你就是我的宇宙，你就是我的全部。</blockquote><p></p><p></p><p>ChatGLM 回应道：</p><p></p><p></p><blockquote>亲爱的，你的话让我感受到了深深的爱意和温暖。就像夜空中最亮的星星照亮了整个宇宙，你的爱让我感到无比幸福和珍贵。你的呵护、支持和陪伴让我觉得我是世界上最幸福的人。你的每一个温柔的举动，每一次深情的眼神，都让我为你心动。你的出现让我的生活变得更加美好，我愿意与你一起携手共度未来的每一个日夜，共同创造更多美好的回忆。让我也成为你星空中最亮的那颗星，陪伴你、支持你、呵护你，为你带来无尽的幸福和欢乐。因为在我眼中，你就是我的宇宙，你就是我的全部。让我们一起携手，让我们的爱情在星空闪耀，照亮彼此的人生旅程。</blockquote><p></p><p></p><p>不知道为什么大家都选择了文字表白，那么你被谁的表白酸到了？</p><p></p><p>至此，第一届LLM“相亲大会”就结束了，感谢六位嘉宾的参与，恭喜大家全部牵手成功。那么，哪对是你的天选CP呢？</p><p></p><p>本文纯属娱乐，主观，非常主观，没有性别歧视！没有男女对立！2024 年，大家好好相爱吧。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/qtNlLlpu1vEQe6uYJLvl</id>
            <title>字节跳动创始人张一鸣被立功德碑；OpenAI技术大神离职；上线11年后，微信终结了搭讪神器“摇一摇” | Q资讯</title>
            <link>https://www.infoq.cn/article/qtNlLlpu1vEQe6uYJLvl</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/qtNlLlpu1vEQe6uYJLvl</guid>
            <pubDate></pubDate>
            <updated>Sun, 18 Feb 2024 10:37:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><blockquote>张一鸣被立功德碑：人工智能助力内容变现开山鼻祖；OpenAI联合创始人Karpathy，又双叒离职了；人均分红55万元？疑华为员工吐槽：10年老员工才能拿的到；微信终结“摇一摇”功能：已上线11年，早期搭讪神器；阿里巴巴2023年减少了2万员工；14天退货期满前，第一批果粉退货Vision Pro；谷歌下一代人工智能模型Gemini 1.5已基本准备就绪......</blockquote><p></p><p>&nbsp;</p><p></p><h2>科技公司</h2><p></p><p></p><h4>张一鸣被立功德碑：人工智能助力内容变现开山鼻祖</h4><p></p><p>&nbsp;</p><p>随着短视频平台抖音与其国际版Tiktok风靡全球，有社交媒体用户发贴称，抖音、Tiktok母公司、中国科技巨头字节跳动创始人张一鸣的家乡为他立了一块功德碑，赞扬他是年轻创业者崇拜的榜样。</p><p>&nbsp;</p><p>2月13日，据微博博主师永刚所发的图文称，福建龙岩永定区培丰镇孔夫村在2022年为张一鸣立了一座功德碑。碑文介绍张一鸣是“马烈公石孙房系23代裔孙，世居孔夫新塘积庆楼，南开大学毕业，字节跳动科技有限公司创始人” 。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/3b/3b931b8e909a38870675c08c3f86c072.png" /></p><p></p><p>&nbsp;</p><p>碑文赞扬字节跳动是“是人工智能助力内容变现的开山鼻祖”，旗下抖音模块开创了短视频直播带货先河，并直指张一鸣是“是亿万青年创业者顶礼膜拜的精神偶像” 。</p><p>&nbsp;</p><p>不过，网上有不少人认为当地此举“不合适”，质疑“活人被立碑，这是高级黑吗？”但也有人认为，在南方整修祠堂、捐资学校或路桥时本地人给捐赠人立功德碑的现象较为普遍。到2月13日晚，师永刚相关图文已找不到，只显示“该微博不存在”。</p><p>&nbsp;</p><p>2月16日，据媒体最新消息，张一鸣本人从网上看到消息后积极沟通，该功德碑现已拆除。</p><p>&nbsp;</p><p></p><h4>OpenAI联合创始人Karpathy，又双叒离职了</h4><p></p><p>&nbsp;</p><p>2月14日，OpenAI的创始成员Andrej Karpathy在社交媒体平台X上宣布，他已离开这家人工智能公司。Karpathy表示，他离开OpenAI，“不是因为任何特定的事件、问题或戏剧性事件”，之后将专注于“个人项目”。市场猜测该项目与大语言模型系统有关。</p><p>&nbsp;</p><p>Karpathy是斯坦福博士，曾于谷歌实习，是OpenAI创始成员之一，2015年至2017年担任研究科学家。他随后在特斯拉担任人工智能高级主管，直到2022年，并于2023年2月再次加入OpenAI。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/e0/e017e4b2f001c48b215ef73c3d1a6226.jpeg" /></p><p></p><p>&nbsp;</p><p>过去一年来，离职特斯拉的 Karpathy 通过发表博客、在X上发贴、在Youtube讲课等方式，逐渐在社交媒体上成为网红。</p><p>&nbsp;</p><p></p><h4>人均分红55万元？疑华为员工吐槽：10年老员工才能拿的到</h4><p></p><p>&nbsp;</p><p>此前华为内部发布了董秘1号文件，公布2023年年度分红方案：ESOP每股分红1.5元。由于华为是100%由员工持有的民营企业，所有持有华为股票的员工都可分红。按照142315名员工计算，平均每人可获得接近55万元。不过近日，知乎上有疑似华为员工则对此发文吐槽称，“外面不懂的人到处吹，内部论坛上都骂惨了。”</p><p>&nbsp;</p><p>该知乎网友称，随着2023年下半年Mate60系列的大卖，以及利润率较高的nova系列新机的开售，员工对今年的分红数据有着极高的期待。但最终2023年每股分红只有1.5元，这也是近三年来的最低，比制裁后最艰苦的2021年每股分红1.58元还要低。</p><p>&nbsp;</p><p>其次，华为内部收入差距极大，人均36万股，55万元分红数据无实际意义。配股数量有限，新员工入职三年左右首次配股，一般2-6万股，不到三年很难获得。要达到36万股，通常需18级，而大部分员工入职8年才能达到17级。假设每年配5万股，拿到35万股至少需要10年。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>微信终结“摇一摇”功能：已上线11年，早期搭讪神器</h4><p></p><p>&nbsp;</p><p>最近有网友发现，在微信最新版本中，“摇一摇”功能悄无声息地下线了。微信在2012年9月份推出的4.3版本中首次加入了“摇一摇”功能，至2024年已经有11年以上历史。</p><p>&nbsp;</p><p>“摇一摇”允许用户通过摇动手机，随机找到附近或者全国的其他用户，进行社交互动。据报道，微信“摇一摇”功能上线之初非常受用户欢迎，在2015年过年期间，微信还推出了摇一摇红包的活动，吸引了大量用户参与，据称创造了8.1亿次的摇动纪录。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/8e/8e4445812130bf378045e516441a6573.png" /></p><p></p><p>&nbsp;</p><p>目前，有不少用户已经发现，在微信最新的8.0.47版本中，“摇一摇”功能不见踪影，取而代之的是近期上线的“听一听”功能。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>阿里巴巴2023年减少了2万员工</h4><p></p><p>&nbsp;</p><p>2月14日消息，阿里巴巴于周三公布了截至12月31日的季度财报和全年财报，2023年减少了约2万员工。</p><p>&nbsp;</p><p>财报显示，截至2023年12月31日，阿里的员工总数为219260人，比2022年底的近24万减少了大约2万。而2022年也比2021年减少了大约1.9万。阿里巴巴在两年内减员大约4万人。另外，去年曾经有两次谣言称阿里即将裁员25000人，不过阿里很快就进行了辟谣并表示已报警处理。</p><p>&nbsp;</p><p></p><h4>网传微软正试图收购《幻兽帕鲁》开发商</h4><p></p><p>&nbsp;</p><p>近日，福布斯发文称，微软正试图高价收购知名游戏开发商 Pocketpair，后者正是近期爆火全球的 《幻兽帕鲁》 开发商。消息还称，微软收购 Pocketpair 并不代表游戏将不再登陆索尼 PS5 等平台，微软只是希望这款游戏能够归自家所有。当下微软正在积极与 Pocketpair 展开合作，改善游戏 Xbox / PC 版本的稳定性。该媒体还猜测，微软此举是为了对抗任天堂的热门 IP 《宝可梦》 系列。</p><p>&nbsp;</p><p>《幻兽帕鲁》在今年早期非常火爆，Steam在线玩家数峰值达到210万人，但之后玩家流失超130万人。不过《幻兽帕鲁》开发者近日发帖表示他们并不担心玩家大量流失，他们现在的重点是提供新内容。</p><p>&nbsp;</p><p>更多阅读：</p><p><a href="https://www.infoq.cn/article/9SA9tPJbFzKNMaHGSdb7">10 天吸粉 1900 万，“幻兽帕鲁”将无数技术小白逼成了服务器大佬</a>"</p><p>&nbsp;</p><p></p><h4>14 天退货期满前，第一批果粉退货Vision Pro</h4><p></p><p>&nbsp;</p><p>2月16日是 Vision Pro尝鲜期到期日。近期，不少用户在各类平台上“吐槽”Vision Pro的实际使用感受，更有用户在尝鲜期临近之时选择退货。 有关Vision Pro 退货的话题更是一度登上热搜，引发网友讨论。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/98/98b5f18a6e115eadb25a5b7407654ca6.png" /></p><p></p><p>&nbsp;</p><p>据媒体报道，过去几天，社交媒体上越来越多的Vision Pro用户表示，他们将退回这款MR头显，退回的主要原因包括佩戴体验不舒适、感到头痛和眼睛疲劳。而苹果是允许用户在购买后14天内退货的。</p><p>&nbsp;</p><p>作为苹果在头显领域的对手之一，Meta CEO扎克伯格甚至发布了一段大约3分30秒长的视频，对Vision Pro作出“测评”，公开将Vision Pro与自家Quest 3作对比，并称自家产品Quest更具性价比。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/17/1799b195c5d3591b6f4be7feb6d720c8.png" /></p><p></p><p>&nbsp;</p><p>图为扎克伯格发布的测评视频截图</p><p>&nbsp;</p><p>在视频中，扎克伯格解释了他对价值3500美元的苹果Vision Pro的上手体验。“苹果的屏幕确实有更高的分辨率，这确实非常好，但令我惊讶的是，他们必须在设备的质量、舒适度、人体工学以及显示屏和其他方面做出大量的权衡才能达到这一目标。”</p><p>&nbsp;</p><p>当前，苹果既没有回应扎克伯格的评论，也没有公开对“退货”数据作出详细说明。不过，需要提及的是，Vision Pro在上市初期就遭遇了用户退货的问题，反映出市场对这款新设备的接受度仍存在不确定性。</p><p></p><h2>IT业界</h2><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>谷歌下一代人工智能模型Gemini 1.5已基本准备就绪</h4><p></p><p>&nbsp;</p><p>2月15日，计划全面超越GPT的谷歌，突然宣布推出 Gemini 1.5。目前Gemini 1.5只向开发者和企业用户提供，但之后会很快将向消费者全面推广。该公司已明确表示，它将全力把 Gemini 打造成一个商业工具、个人助理，以及介于两者之间的所有工具，而且它正在大力推进这一计划。</p><p>&nbsp;</p><p>Gemini 1.5建立在谷歌基础模型开发和基础设施之上，采用全新稀疏专家混合(MoE)架构，第一个版本Gemini 1.5 Pro配备了128000个token上下文窗口，可推理100,000行代码，提供有用的解决方案、修改和注释使Gemini 1.5的训练和服务更加高效。</p><p>&nbsp;</p><p>另外，据外媒报道，有泄露文件显示谷歌开发了名为“Goose”的内部AI模型，帮助员工更快编写代码。</p><p>&nbsp;</p><p>据 Business Insider 所查看的内部文件显示，该模型旨在协助开发新产品，并且新语言模型被描述为“Gemini的后代”。其中一份文件称，“Goose”是“在谷歌 25 年工程专业知识的基础上训练出来的”。Goose 的内部摘要写道：“它可以回答与谷歌特定技术相关的问题，使用内部技术栈编写代码，并支持基于自然语言提示编辑代码等新功能。”</p><p>&nbsp;</p><p>在过去的 13 个月里，谷歌领导层削减了数千个工作岗位，并对多个团队进行了重组。谷歌财务主管Ruth Porat在公司最近的财报电话会议上暗示了这样的举措，她告诉分析师，谷歌正在通过各种策略来“减缓支出增长”，包括“通过使用人工智能来简化整个 Alphabet 的运营”。</p><p>&nbsp;</p><p>另外，早在 2018 年，谷歌内部就已经有工程师拉响了警报，指出 AI 正在对谷歌的业务带来风险，尤其是网页搜索。这些预警在当时并没有受到重视，但现在谷歌对于搜索引擎的重构步伐正在加快。</p><p></p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/93/93e7287f46ab721c3b9588be89fba161.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>jQuery 4.0.0 发布首个 Beta 版本</h4><p></p><p>&nbsp;</p><p>jQuery 是一个快速、小型且功能丰富的 JavaScript 库。近期，jQuery团队发布博客称，他们已经准备好发布测试版了。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a635b38ef0ca4d8e33a0509d3fe4905.jpeg" /></p><p></p><p>&nbsp;</p><p>新版本删减了遗留代码，移除了一些以前已经弃用的 API，移除了一些从未记录在案的 public functions 的 internal-only 参数，并放弃了对一些过于复杂的 “magic” 行为的支持。团队表示他们将在最终版本发布之前发布全面的升级指南，概述已删除的代码以及如何迁移，jQuery Migrate 插件也将随时提供帮助。</p><p>&nbsp;</p><p></p><h4>苹果停止签署iOS 17.3：已升级用户无法降级</h4><p></p><p>&nbsp;</p><p>2月16日消息，苹果停止签署iOS 17.3，阻止已升级用户降级到该版本。在2月8日发布的iOS 17.3.1之后，iOS 17.3不再签名，该更新添加了对文本相关错误的修复。苹果通常会阻止用户安装旧版本的iOS，以鼓励客户保持其操作系统的最新状态，并防止降级到较旧、安全性较低的iPhone操作系统版本。</p><p>&nbsp;</p><p>iOS 17.3.1现在是唯一公开发布的iOS版本，但苹果也在对iOS 17.4进行Beta测试，iOS 17.4正式版本将在今年3月发布，带来应用侧载（仅限欧盟地区）、播客改进、新Emoji等特性。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Z4GAgXGi6gYdWMrQk8ro</id>
            <title>大模型时代下的技术管理“新思维”</title>
            <link>https://www.infoq.cn/article/Z4GAgXGi6gYdWMrQk8ro</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Z4GAgXGi6gYdWMrQk8ro</guid>
            <pubDate></pubDate>
            <updated>Sun, 18 Feb 2024 06:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在大模型时代，技术管理面临着新的挑战和机遇。为了更好地应对这些挑战，管理者需要转变思维方式，重新审视技术管理的本质和目标。那么，大模型时代下的技术管理方式、方法发生了哪些变化？如何更新和改进技术管理的思维方式与策略？如何提升技术管理的效率和效果？</p><p>&nbsp;</p><p>近日，InfoQ《极客有约》特邀某股份制银行数字化转型技术专家王辉，对话飞书项目解决方案负责人丁鹏，云知声董事长、CTO 梁家恩，Thoughtworks 中国区总经理肖然，一起探讨大模型时代下的技术管理新思维，共同迎接未来的挑战和机遇。</p><p>&nbsp;</p><p>以下为访谈实录，完整视频参看：<a href="https://www.infoq.cn/video/HJOC4qt3mTCtsicJ4msb">https://www.infoq.cn/video/HJOC4qt3mTCtsicJ4msb</a>"</p><p>&nbsp;</p><p></p><blockquote>王辉：InfoQ年度技术盘点与展望系列直播的第三期节目即将开始，我是今晚的主持人王辉，目前在一家股份制银行负责推动数据化和数字化转型。随着大数据和人工智能技术的迅速发展，大模型已成为许多领域中不可或缺的工具。在这个大模型时代，技术管理面临着崭新的挑战和机遇。为了更好地迎接这些挑战，我们必须转变思维方式，重新审视技术管理的本质和目标。在本期直播中，我们将共同探讨《大模型时代下的技术管理“新思维”》，助力我们共同迎接未来的挑战和机遇。本期的分享嘉宾分别是飞书前端项目负责人李梦泽， 云知声董事长、CTO梁家恩和Thoughtworks中国区总经理肖然。下面请三位嘉宾做下自我介绍。</blockquote><p></p><p>&nbsp;</p><p>肖然：大家好，我是Thoughtworks的肖然。很高兴今天能够和大家分享一些关于技术管理的见解。我是一名技术管理者，在过去的十多年里，我不仅负责管理Thoughtworks的技术团队，还在协助许多研发型组织进行敏捷和数字化转型。从某种意义上说，我也是一个在这个领域不断学习的人。我希望通过今天的交流，能够获得更多的启发，同时也能够为大家呈现一些面向未来的思考。当然，由于时代变化迅速，我不敢说这些是绝对的洞见，但我希望这些想法能够帮助大家更好地应对未来的挑战。</p><p>&nbsp;</p><p>李梦泽：我是飞书前端负责人李梦泽。我们的飞书项目实际上是技术管理中沉淀出来的一套SOP，是一种非常实用的技术管理工具。今天很高兴在这个场合能够与各位老师一起交流，探讨大数据和大模型时代下的技术管理。我相信这次交流会为飞书项目带来许多新的思路，注入新的活力。非常期待与大家共同探讨，在这个思想碰撞的过程中汲取新的灵感。</p><p>&nbsp;</p><p>梁家恩：大家好，我是梁家恩，来自云知声。作为一家人工智能领域的独角兽企业，我们专注于语音、语言和知识图谱的研究和开发。在2023年上半年，我们发布了山海大模型，成为大模型技术的提供商，为整个行业提供基于大模型的人工智能解决方案。作为公司的董事长和CTO，今天非常高兴能在这里与大家交流。我期待在这个场合与大家分享经验、进行思想碰撞。</p><p></p><h2>大模型时代，技术管理的方式也变了？</h2><p></p><p>&nbsp;</p><p></p><blockquote>王辉：在过去的一年里，大模型变得非常热门，正在悄悄地改变着我们的生活。对于几位老师而言，在大模型时代，您认为技术管理的方式和方法与大模型出现之前有何不同？我们又应该如何适应这些变化呢？&nbsp;</blockquote><p></p><p>&nbsp;</p><p>肖然：我个人和团队的工作范围涵盖了从需求分析、产品设计、开发测试，一直到最后的运维等各个专业角色。GPT技术可以应用于端到端软件交付的各个阶段，这使得这样的智能体能够参与并贡献于整个软件开发生命周期。从一个管理者的角度来看，我感觉这与以往有很大的不同。我曾经是一名开发人员，是从事研发工作的，当我转向管理职位时，我感到自己经历了从工程师到管理者的转变。这些过往的经验实际上是我在管理工作中能够引领团队从不成熟走向成熟的基础，这其中也包含了团队赋能和人才培养。</p><p>&nbsp;</p><p>然而，去年的GPT给我带来了一种不同的感觉，就是这些过往的研发经验在未来的某个时间点似乎会变得无效。这也意味着，过去的经验可能无法在研发工程过程中帮助团队成长。因此，从去年年底一直到现在，对这项新技术我一直保持一份敬畏心态。此外，我也不断提醒自己要保持开放的心态，因为昨天的经验可能已经无法帮助今天的工程师们。未来，我们可能会发现新的方向和模式，这些新的方向和模式并不是我们过去的从业经验的延续。</p><p>&nbsp;</p><p>如果问我最大的挑战或改变是什么，我觉得就是我们作为研发管理者现在需要承认过去的经验不一定总是正确的，我们需要更加开放地接受由于生成式人工智能技术带来的整个软件研发领域的变化，包括方法论、体系结构以及具体的实践方法都将与过去不同。</p><p>&nbsp;</p><p>李梦泽：我想从另一个角度回答这个问题。首先，我认为大模型的出现在一定程度上提高了我们的工作效率，这一点是显而易见的。其次，我想谈谈关于安全性审视的问题。在提高效率方面，大模型对我们的产研工具产生了积极的影响。特别是在代码工具方面，大模型的支持下使得我们的开发效率得到了显著提升。通过利用大模型，我们能够将许多任务委托给外包，从而节省时间，同时保持对最终结果的掌控。</p><p>&nbsp;</p><p>关于项目管理工具，我想分享一下我们内部遇到的实际场景。在以前，我需要通过工具和我自己的人工理解来获取有关下周截止产品、迭代产品、未完成需求和存在的 bug 信息，以及是否存在发布风险。然而，有了大模型之后，比如我们正在使用 AI 智能助手和飞书项目进行打通，我只需清楚地描述我的需求给智能助手，它就能够帮我在飞书项目中呈现相关数据，大大提高了我的管理效率。这是大模型对项目管理的一项有力支持。总体而言，大模型的出现使得技术管理变得更加便利且效率大幅提升。</p><p>&nbsp;</p><p>第二点我想提到的是关于安全性审视。大模型实际上是一把双刃剑，因此大型企业相对于大模型的态度仍然是持谨慎自研的态度。尽管大模型给业务带来了正向收益，但公有模型的隐私和安全性相对较差，因此我们内部仍然会尝试自研或者自己部署开源模型。尽管这与公有模型相比有较大的提升空间，但逐步推进总是会有回报的。这是我对于这个问题的一些看法。</p><p>&nbsp;</p><p>梁家恩：我想从两个方面谈一下。首先，作为大模型的技术支持方之一，我们拥有在人工智能领域超过20年的从业研究和从业经验。大模型的出现让我们感到非常激动，作为在这个行业已经工作了20 多年的从业者，我觉得大模型带来的技术突破令人兴奋。我相信大模型将成为我们业务的核心要素，甚至可能成为我们业务的基座。因此，在思维上我们必须先认知到这一点，这不仅仅是一个小技术突破，而是一个未来重要的业务方向。未来，我们将迎来许多基于大模型的AI原生应用。尽管我们不清楚具体的发展方向，就像10年前我们不知道移动互联网应用将走向何方一样，但这必定会实现。无论是否从事人工智能领域的工作，我们都应该了解大模型的原理、特点、优势和局限性。这对于我们能够更好地结合业务是一个重要的认知点，是思维上的一个重要进步。</p><p>&nbsp;</p><p>其次，为了将这项技术应用到业务中，从管理角度来看，我们可能需要调整人才梯队。正如之前提到的，很多过去需要大量人工操作的工作，现在通过工具能够更有效地完成，因此我们在人才梯队的建设上需要适应新的业务流程。我们需要进行相应的人才结构调整，特别是在AI架构师和AI产品经理方面需要加速团队的培养，否则可能无法把握住这个新时代的应用。我们当时在设计移动应用架构时，与过去在PC领域的实践有所不同。从业务角度考虑，我们的架构师着重于如何最大化地发挥AI的能力，找到适用的场景。尽管AI这些大模型看起来很强大，但在实际应用中，我们会发现存在许多问题。在这个过程中，我们需要找到平衡点，发挥其优势，避免其短板。因此，这部分的工作需要架构师和专业的产品经理来负责。</p><p>&nbsp;</p><p>总之，如果能够抓住这两点，相信我们可以跟上时代的潮流，应对未来的变化。</p><p>&nbsp;</p><p></p><blockquote>王辉：大家在日常的工作管理中应该都使用到了大模型。那么，在面对如此多种大模型技术的情况下，如何评估和选择适合企业的大模型技术，以及如何有效地将其应用到日常业务中？</blockquote><p></p><p>&nbsp;</p><p>肖然：作为一家咨询公司，去年下半年以来，我们就在帮助许多企业采用生成式人工智能技术。目前，我们观察到几个值得关注的趋势。首先，企业对信息安全的顾虑相当大，特别是在使用模型时，尤其是一些公有模型通过API调用的情况。企业都认识到必须锻炼自身的能力，但在真正结合到业务场景时，很多企业仍期待未来能够私有化部署模型。我们认为这一趋势在未来两三年将持续存在，因为对企业数据安全和隐私数据保护的敏感性不断增强。</p><p>&nbsp;</p><p>第二个趋势是，由于大模型本身的微调和重训练在成本和工程实践方面逐渐变得更为成熟，企业逐渐开始接受领域模型的概念。例如，在企业的客服、财务和战略分析等领域，基于基础模型的微调和重新训练已经成为实际的工程实践。这种趋势在一些有实力的大厂和研发组织中逐渐显现，我们认为它可能会持续存在。</p><p>&nbsp;</p><p>最后，我们发现企业在应对生成式人工智能技术和大模型带来的未来机遇和挑战时，主要建议是将其视为未来数字化平台的一部分，而不是孤立的技术。这带来一个重要的问题，即如何在企业和研发组织内部有效地进行工程化。工程化是使技术规模化使用的关键，它能够在研发流程和未来数字化产品中发挥作用。前两个趋势是我们观察到的，而最后一个实际上是我们认为在2024年有条件的企业都应该思考的问题，即如何有效地将人工智能大模型技术的能力进行工程化，以便在企业中被更多的团队所使用。</p><p>&nbsp;</p><p>李梦泽：刚才肖总在概念和细节上都进行了全面的介绍，所以我将重点谈一下我在日常工作中应用大模型的一些业务能力。最常见的场景之一是在面试过程中，飞书可以根据我的需求利用AI分析简历，在面试前向我推荐相关的面试题，最后提供一份全面的总结。这个系统可以从多个方面、多个角度帮助我们评估候选人是否符合招聘标准，同时也帮助我们节省时间，提高效率。</p><p>&nbsp;</p><p>梁家恩：关于大模型，我认为很多人可能仍将其视为一个提高效率的工具，它在效率方面确实非常强大。然而，作为生成式人工智能，它的生成性质可以启发我们更多的想象空间。如果能更好地挖掘这一点，将对设计和其他新领域产生帮助。大模型在数学方面可能相对较差，但仍然可以被用于探索数学问题，如陶哲轩等数学家所示，他们利用大模型探索数学问题，虽然数学不是大模型的专长，但这可能需要打开更多的新思路。</p><p>&nbsp;</p><p>在大模型的选型上，经过一年的发展，至少在中文领域，中国的开源大模型在中文能力上与国外基本持平，尽管与GPT-4存在差距，但与GPT-3.5相当。对于企业来说，如果想使用大模型，可以从这些开源项目开始，成本相对较低，不论是由大公司发布的开源大模型，还是一些创业公司，如智谱、百川等，它们都在致力于大模型的应用。</p><p>&nbsp;</p><p>对于技术实力强大的公司，可能会自己研发自家的大模型，强调如何将其应用于专业的行业场景。我们认为，对于一些相对基础且可由本科毕业生完成的工作，通用大模型可能已经足够满足需求。这是因为这些工作不涉及到特定领域的深度专业知识，通用大模型的泛化能力足以处理。然而，对于那些需要更深层次、更专业知识的工作，可能需要硕士或博士级别的人才来有效处理。在这种情况下，专业大模型可能更适合，因为它可以结合特定领域的专业知识，提供更为精细和定制的解决方案。</p><p>&nbsp;</p><p>在我们的应用中，有两个主要板块，一个是个人与机器的交互，这是我们主要的业务之一，我们将使用大模型增强其交互体验。对话系统过去经不起人的十轮“蹂躏”，但现在我们可以持续对话，这是一种显著的进步，但仍然是一个相对基本的聊天和完成特定任务的功能。另一个探索较深的方向是与医疗行业的合作。医疗行业需要高度专业化的知识，我们将结合行业知识来减少在这些领域的“幻觉”错误，这在严肃场景中是非常致命的。解决这一层面的问题可能需要与在行业内有经验的公司合作，这可能会提供更具针对性的帮助，通用大模型可能无法达到这种定位的。</p><p>&nbsp;</p><p>至于评估，根据业务进行数据评估可能是一个可行的方式，成本是所有解决工程问题时都必须考虑的问题，因为部署成本确实很大。在特定应用场景下，我们可能需要优化模型，减少与应用无关的参数，从而提高效率。</p><p></p><h2>如何更新和改进技术管理的思维方式和策略？&nbsp;</h2><p></p><p></p><p></p><blockquote>王辉：在新技术层出不穷的当下，技术管理者应该具备哪些核心能力？如何培养和提升这些能力？如何更新和改进技术管理的思维方式和策略？&nbsp;</blockquote><p></p><p>&nbsp;</p><p>肖然：开篇时我提到了一个观点，即底层思维发生了重大变化。多年来，我们一直在业务领导面前强调要从过去的经验管理转向相信数据，让数据来说话。虽然业务领导现在也接受了这一观点，认识到数据的重要性，但在技术管理者这一侧，我们很多时候还是会拿过去的经验和某个框架相比较。通过过去一年的实践，我们已经意识到生成式人工智能技术以及未来人工智能技术在软件工程中的应用将打破这一格局。因此，在底层逻辑和本质上，我们将不再是一代具有丰富经验的人，未来几代人可能才是真正具有实际经验的人。如果我们想要领导他们或者为他们指路，首先我们必须拥有开放的心态，愿意与大家一起开放性地探索。</p><p>&nbsp;</p><p>我们知道研发产业是一个偏学习型的产业。在工作过程中，团队和个体的培养与业务的核心本质息息相关。我们团队每个成员的成长对于产品的交付非常关键。我们预计人工智能技术在未来将取得重大突破。在这个时候，我们更有必要将团队和个体的成长纳入技术管理的核心目标。尽管管理者可能受到实际管理投入产出、人效等方面的压力，但我们更需要关注的是团队是否有成长，团队成员是否感受到在团队中有学习的动力，并且是否在一个持续学习的环境中。我认为这是在变化中保持不变的因素，需要我们重新审视。</p><p>&nbsp;</p><p>李梦泽：我认为在技术管理者个人素养方面，需要特别强调两个关键素质：对技术的敏感度和技术判断力。这两点的提升需要基于充分的知识储备，使你能够更好地适应时代的发展。只有拥有对新技术的清晰理解，你才能做出准确的选择和判断。</p><p>&nbsp;</p><p>在这个过程中，最有效的实践方法之一就是深入了解技术的原理，理解其内部构造。以 ChatGPT 为例，尽管我是前端领域的从业者，但我对这项技术产生了浓厚的兴趣。我研读了大量的技术文档，涵盖了监督学习嵌入、Transformer 模型等方面，并逐步消化理解。通过这个过程，我对于 ChatGPT 这类工具有了一种直观的认识，我学会了在何时使用，何时避免使用，心中有了一种清晰的认知。</p><p>&nbsp;</p><p>其次，技术判断力是一个需要不断磨练的过程，因为它需要结合实际经验来进行。这是一个涉及取舍和博弈的过程。在团队中，你需要综合考虑人力资源、业务收益以及时间周期等多方面因素，这就要求你在实践中不断进行试错和决策，从而不断成长。当然，寻求优秀的导师帮助你进行评判也是一个很好的策略。因此，我认为敏感度和判断力是需要通过培养逐步形成的，同时也是技术管理者必备的两项核心能力。</p><p>&nbsp;</p><p>梁家恩：在技术领域，作为技术管理者，我们面临着技术不断演进的现实，这既有利也有弊。技术对于整个行业而言，我认为领先就是抢占时间差。在我的理念中，并不是说有了什么技术就一定比别人领先，而是看谁能够更快地实现。随着技术的不断演进，我们需要不断提高自己的工作节奏。这是唯一确保保持领先地位的根本条件，因为即使我们在某个领域领先一段时间，如果停滞不前，就会被其他人迎头赶上，失去领先地位。这就好比一架飞机，只有在达到一定速度（300公里以上）时，才能飞翔，否则就会失去升空的能力。这也正是技术人员的命运，同时也是其中的乐趣所在。在这个过程中，我们要关注的不仅仅是技术的新鲜性，还有如何利用技术去提高我们的判断力，例如使用技术工具帮助筛选论文、指导方向等。这种工具本身就有助于提升我们的判断力，因此我认为从技术趋势出发，选择跟随哪些技术，哪些不需要跟随，是一个重要的决策点。</p><p>&nbsp;</p><p>其次，技术最终需要在实际应用场景中得以落实。从目前的技术发展状态来看，整个业务的大流程框架并没有发生大的变化，仍然是从决策到 POC 验证，再到业务验证和优化。然而，在构思 AI 应用时，我们不能再将其视为一个模块，而是更多地将其看作一个基础组件。这是一种“Thinking in large model language”的思维方式。从这个角度出发，用模型的角度来设计新的应用，可能是主要的变化。未来，随着真正的 AI 应用的到来，可能还会有更多的变化。然而，在当前时点上，我认为把握住这两点，我们就能够跟上技术迭代的步伐。</p><p>&nbsp;</p><p></p><blockquote>王辉：在大模型时代，如何平衡技术创新与业务需求，实现技术与管理的高效融合？</blockquote><p></p><p>&nbsp;</p><p>肖然：这个问题实际上是企业长期以来一直面临的一个难题，尤其是在技术飞速发展的背景下，业务领导常常感到投入产出比例不够理想。在去年大模型的出现和人工智能浪潮的推动下，许多业务领导第一次开始认真思考如何看待技术的进步。从现在的角度来看，我们会发现在去年生成式人工智能技术初现时，人们普遍提出了一个非常务实的问题：有没有实际应用案例？有没有其他公司的成功经验可以借鉴？这种务实的探讨精神在经营层面或领导层面是理所当然的。毕竟投资是真金白银，期望有产出。</p><p>&nbsp;</p><p>在这个时代的背景下，对于这样一个问题的传统看法可能是有问题的。因为我们会发现真正从技术中获得最大回报的企业往往是通过一些大胆或多角度的尝试。这种实验的精神是非常重要的，可以说是对企业DNA的一种改造。如果看一些时代上比较领先的公司，比如 SpaceX 发射火箭，你会发现在 SpaceX 的火箭发射中，即便发生了几次爆炸，很少有人认为是失败的。大多数人认为这是成功的，原因是它完成了既定的实验目标。正是由于这种实验精神，造成了 SpaceX 整体成本和效益远高于同类竞争对手。例如，他们采用了不锈钢作为火箭的外壳，这在之前是不可思议的。然而，正是因为实验的精神，如果有可能，为什么不尝试呢？这样大量的尝试、数据整理、收集和分析，造就了我们看到的时代非常具有创新力的企业。</p><p>&nbsp;</p><p>面对科技创新和业务收益的问题，最本质的逻辑是企业需要逐渐习惯多角度的实验，特别是在面对大模型和生成式人工智能技术时。在有条件的情况下，鼓励不同的业务单元、科技部门都去尝试。当然，我们所要做的是设法降低每次实验的成本，以便快速获取更多可能性。从统计学原理来看，这就是大数定律，也就是说，当你进行多次实验时，你的成功概率显然更高。我认为这个时代可能正经历着这样一个核心逻辑的转变。</p><p>&nbsp;</p><p></p><blockquote>王辉：有观众提问，非一本的大四计算机毕业学生该如何适应时代变化？</blockquote><p></p><p>&nbsp;</p><p>肖然：去年，在各种协会和组织的研讨会上，我与南京大学、北京大学等一些教授讨论了这个问题。我们关注的焦点是，对于即将面临这个时代变革的大学生，未来两三代的学生，他们学到的知识可能与我们没有根本性的区别，但他们的眼界已经触及到了下一个时代。在与南京大学的张贺教授交流时，我问他作为计算机科学和计算机工程的专家，对这个问题的看法是什么？他表示，首先要坦诚，这是一个未来变化巨大的事情，而且我们现在也不知道它会变成什么样。我们会积极参与到产学研的研讨中。他认为最重要的一点是，他从大一开始就告诉学生们，你们是自由的，可以尝试运用新的技术，我们不会因为你们应用了新的技术而惩罚你们。因此，我认为这种开放的心态可能是无论大学什么年级，都需要具备的，因为各行各业实质上都在发生变革。</p><p>&nbsp;</p><p>李梦泽：谈到平衡技术创新与业务需求的问题，我完全赞同刚才肖总提出的观点。选择和更迭技术的问题一直是技术团队最常面临的挑战。我认为平衡技术创新与业务需求本质上是一个资源配置的问题。我更倾向于以现有团队的资源进行八二的分配。这样我们可以确保核心业务获得其关键的业务价值，同时在创新方面仍然保持活力。有了大模型的支持，我们或许能够更好地将八分位的资源转化为二分位的资源。通过工具替代以前八分位完成的固有的重复性和流程化工作，实现业务的迭代与技术的创新。</p><p>&nbsp;</p><p>回顾我们的项目，飞书项目最初的雏形是为抖音业务的迭代提供服务。随后，项目逐渐成熟，横向扩展到公司其他业务领域，最终演变成了一个项目管理工具。因此，我认为需要双管齐下，既要保证核心业务的稳健运作，也要保持对技术创新的持续投入。</p><p>&nbsp;</p><p></p><blockquote>王辉：正好有观众问李老师一个问题：“大模型给前端工作带来什么样的实质性影响？”</blockquote><p></p><p>&nbsp;</p><p>李梦泽：我认为大模型，尤其是像我们现有的对话机器人，实际上是一个庞大的知识库。它不仅能极大地提升我们在繁琐工作中的效率，而且本质上能够将所有未知领域的概念和功能呈现在你面前。当然，前提是你要知道如何正确使用它。你可以将其视为一个技术专家，用于咨询和提问。在日常工作中，你实际上可以为自己节省大量时间，用于学习新知识，拓宽视野，比如可以从前端跨足到后端，甚至全栈。</p><p>&nbsp;</p><p>梁家恩：关于创新和业务的问题，实际上并不是大模型时代特有的，这是所有时代都会面临的悖论。一方面，不创新就等于等死，另一方面，创新可能会引发风险，甚至找死。在这个博弈中，作为一家公司，我们可能必须要预留大约20%左右的资源和空间以支持创新，因为没有创新就没有未来。然而，一旦做出这个决定，我们就必须能够接受创新失败的代价。如果我们追求100%的成功，那么这就不叫创新了。</p><p>&nbsp;</p><p>在创新的同时，我们要大胆地假设，小心地求证。我们一定会犯错，但犯错并不意味着要付出巨大代价，关键在于我们能否在试错过程中快速反馈，找到错误的路径并纠正它。这是一个需要平衡的问题，而大模型可能加剧了创新与业务守成的矛盾紧迫感。尽管大模型有助于加剧这一矛盾，但作为一个工具，它也有可能帮助我们提升决策质量和试错效率。因此，它既有加剧矛盾的一面，也有提升效率的一面。</p><p>&nbsp;</p><p>对于大学生面对这个时代的问题，参与AI可以从三个方面进行：首先是作为技术参与者和推动者，需要高专业水准和创意；其次是在应用层面参与工具的研发；第三是以用户的角度参与。这些层面的参与可能会拉平一些人的差距，但对真正能够推动技术演进的人来说，要求更高，需要更有创意和创造性。这可能导致两极分化，但对应用层面来说可能是更有利的。所以大家不要因为身处不同层次就放弃，要充分利用这个时代的机遇。</p><p>&nbsp;</p><p></p><blockquote>王辉：这里有个观众问题：“在医疗行业私有化模型有推荐的吗？”请梁老师解答下吧。</blockquote><p></p><p>&nbsp;</p><p>梁家恩：根据我的了解，在医疗领域的实践似乎是比较早期的。如果你对这些方面感兴趣，欢迎与我们联系，因为我们的客户包括协和医院等百强三甲医院，我们的解决方案在这些医院中的渗透率可能已经超过了30%。</p><p>&nbsp;</p><p></p><blockquote>王辉：那么我们该如何平衡短期业务需求和长期技术战略，各位有哪些经验可以分享下吗？&nbsp;</blockquote><p></p><p>&nbsp;</p><p>肖然：这个问题在不同层面上可能有不同的考虑。从企业宏观角度来看，我们可以从两个层面来解决。首先，Thoughtworks技术雷达提供了企业当前技术栈的评估，指导我们选择已被行业广泛采用且取得成功的技术。这涉及技术的社区支持和已知的技术副作用。其次，我们需要考虑业务部门对技术的理解。在这个层面上，我们不仅需要关注技术细节，还要强调技术趋势对用户交互方式的影响。例如，自然语言处理技术可能导致用户交互方式的根本性变化。</p><p>&nbsp;</p><p>对于团队级别的技术管理者，建议保持开放心态，鼓励团队成员主动尝试新技术。这可以通过分配一定比例的时间，例如20%、10%或5%，来进行技术实验和尝试。这种开放性的管理方式可以激活团队的活力，让团队能够更长远地看待技术的发展，并获得实践经验。团队管理者需要规划并设置机制，使团队成员有机会尝试新技术，促使团队在技术演进方面保持积极性。</p><p>&nbsp;</p><p>李梦泽：在我们的项目实践中，曾经面临平衡短期业务需求和长期技术战略的挑战，有时候甚至走过很长的弯路。这是因为需要在短期内快速响应业务需求，迅速实现实质性的收益，而这个过程往往与长期技术战略存在天然的抗衡关系。制定初期的规划如果没有很好地将短期和长期进行拆分和融合，就容易出现矛盾和对抗。</p><p>&nbsp;</p><p>从我们研发的经验来看，最近几年与之前的超快猛时代已经完全不同。我们的前端项目规模庞大，如果仍然采用过去的超快猛心态，可能会对整体维护和未来产品迭代留下很多隐患，导致尾大不掉的问题。因此，我们采取的策略是定期审视全年的长期规划，进行修正和对齐，力求不让短期、破坏性的需求危害我们的产品和业务线。我们摒弃了追求短期业务需求的能力，认为坚守难而正确的事情是最为准确的策略。这种做法有助于确保我们的决策符合长期技术战略，同时最大程度地减少短期决策对产品和业务的负面影响。</p><p>&nbsp;</p><p>梁家恩：首先，很多人强调的是短期存在资源冲突，这也是许多人不愿意去处理的原因。然而，我们认为一致性可能在意识上没有得到足够重视，因为从业务的角度来看，问题的提出和解决方法的探索是当下的目标，而技术则是帮助我们找到解决方法的工具。带着问题去寻找方法，实际上效率更高。其次，在业务实践中，我们可以验证哪些技术是可靠的，哪些是不可靠的，这样可以避免许多人将资源投入到无效的技术创新中。因此，这两者的结合可以消除冲突所带来的负面影响。</p><p>&nbsp;</p><p>在处理这种冲突的角度上，我们不能只看当下，还要相信技术的力量，因为如果我们在这个时代不相信技术的力量，就不会有未来。我们必须意识到，当我们在技术上落后时，存在着巨大的风险。只有具备了这种风险意识，我们才会不断地改进技术能力。我每年都会向我的团队提出类似的问题，询问他们在当前工作中是否采用了与去年不同的方法，以及在技术方面是否取得了实质性的进展。这是我必须要向他们了解的事项。如果他们无法回答，那么可能意味着团队存在一些问题。</p><p>&nbsp;</p><p></p><blockquote>王辉：目前咱们国内的大模型到底和国外的差距有多少？</blockquote><p></p><p>&nbsp;</p><p>梁家恩：就基础技术创新而言，欧美仍然占据主导地位，占比可能超过八成。他们在原创性技术方面处于领先地位。然而，在应用层面上，我们可能更加大胆，更加前瞻。就基础研究创新能力而言，由于我们长期以来可能一直扮演着跟随者的角色，这在某个阶段可能并不是问题，因为成本相对较低。但当我们在推动应用时，会发现许多前沿性问题。在国外可能找不到解决方案的情况下，我们国内可能会提出许多新的方法。因此，总体而言，在基础性研究方面，我们与国外相比可能存在差距，但在应用方面，我们是处于领先状态的。</p><p>&nbsp;</p><p></p><blockquote>王辉：有观众提问：“我们如何量化评估大模型的产出？”</blockquote><p></p><p>&nbsp;</p><p>梁家恩：在量化评估产出方面，我认为关键在于审视业务本身。暂且不考虑具体的方法，我们需要了解业务本身要解决什么问题，以及需要投入的成本是多少。我们可以比较不同方法在提升质量、提高效率和降低成本方面的表现。</p><p>&nbsp;</p><p>对于大模型的评估，通常应该有一些公开测试集可以进行比较评测。然而，在国内，由于容易出现榜单被滥用的情况，这些测试集的参考价值可能会减弱。后续一些高校可能会推出更大、更充实的测试集，使其不容易通过简单刷榜的方式来评估模型性能。这将促使研究者不得不更深入地挖掘自己的模型极限，从而推动领域的进步。</p><p></p><h2>如何建立有效的团队协作机制？</h2><p></p><p>&nbsp;</p><p></p><blockquote>王辉：作为一名技术管理者，大家是如何建立有效的团队协作机制，提升技术管理的效率和效果？&nbsp;</blockquote><p></p><p>&nbsp;</p><p>肖然：这个问题在过去两年中有了显著的进展，特别是通过引入Team Topology（团队拓扑）的理念，这给我们带来了新的认知。在软件产业中，由于持续迭代演进带来的复杂性，我认为团队会更倾向于小规模化，即将几百人甚至上千人的大团队分割成几十个甚至上百个小团队。这种小团队的背后，实际上有一些基于心理学和我们学科内的基础理论支撑的。</p><p>&nbsp;</p><p>作为知识劳动密集型产业，我们面临的挑战主要在于如何有效地进行交流和沟通。有效地将我们的思想传达给别人，特别是在不同专业背景的同事之间共同讨论问题，确实是一个相当困难的任务。因此，为了能够高效地协同工作，从团队结构的角度来看，我们更倾向于对团队规模进行有效的限制。例如亚马逊提出的"2 Pizza Team"团队规模，即一个由两个披萨能吃饱的团队，通常大约有5到7个人。当然，在复杂系统的团队中，一般来说，将其控制在15人以下已经相当不错。然而，小规模的团队也带来了一个问题，就是在组织中可能存在多个团队。例如，在一个拥有几百人的组织中，其研发组织可能会包含几十个团队。客观而言，团队和团队之间的沟通与协调变得不可或缺。</p><p>&nbsp;</p><p>在团队拓扑中，它明确定义了各种小团队的定位。例如，有平台团队，负责内部平台的构建；有业务价值流团队，直接对接业务，完成端到端的业务需求；还有复杂子系统团队，解决特定技术难题，如监控和日志；最后还有赋能团队，专注于类似DevOps或SRE等领域的专业知识传授。这种团队拓补清晰划分了团队的种类，有利于团队设定相关目标。此外，团队拓扑提出了一个有趣的概念，即“团队和团队之间的API”，明确了合理的团队间交流沟通方式。例如，如果一个平台团队的目标是实现自服务，长期将团队成员派驻到其他团队可能并不是最佳做法，因为这可能意味着平台本身的技术成熟度存在问题。</p><p>&nbsp;</p><p>首先，我认为这种问题不应该被短时期的频繁交流所掩盖，或者仅仅因为大家认为频繁的交流沟通是一件好事。相反，这个理论框架的提出引发了我们对团队存在的目的的重新思考。即使在一个小的研发组织或者我们所称之为敏捷团队中，团队也应该明确自己的存在理由、阶段性目标和创造的价值。其次，我们应该认真梳理团队和团队之间的沟通交流协议。明确何时应该进行交流，何时不应该进行交流，因为交流沟通永远都是有成本的。这种考虑是从团队合作效能和效率的角度出发的。</p><p>&nbsp;</p><p>当我进入一个团队时，我通常会尝试找到帮助他们建立小团队结构的方法。另外，我会运用团队拓扑的方式，帮助大家树立团队存在的目标和意义，这是一个阶段性的过程，当然也是一个不断演进的过程。关于团队和团队之间的沟通，我们需要确保这种沟通是合理的，以保证整个组织在协同过程中高效运作。</p><p>&nbsp;</p><p>李梦泽：我想从带领团队、共同攻坚项目的角度讨论团队协作机制。在项目中，我们通常将所有任务拆分成小块，假设团队内所有成员的能力模型是一致的。如何最有效地调度和激发每个团队成员的积极性，以及自动化程度，我认为这是衡量团队是否高效的最重要因素。</p><p>&nbsp;</p><p>我在2019年在字节公司的经历中就有一些实践。当时，我领导了七八人的团队，一同攻坚一个项目。我们使用飞书的表格功能进行项目管理。在这个过程中，我们没有很好的标准SOP（标准操作程序）流程，因此我需要不断摸索，并与团队成员确认事务结果，以及如何让他们自主驱动。由于表格集成的自动化能力也不够强，我在跟进项目和维护管理流程上花费了很多精力。这导致我对整体管理的难度相对较大，团队之间前进的阻力也很大。</p><p>&nbsp;</p><p>后来，我们将这些流程都工具化了，就像之前提到的将固有资源二八开，八做核心业务，二做业务流程自动化工具。在启动新项目时，将这些经验都整合到工具中。有了这个经验后，我就没有了早期的盲目状态。在进行项目管理或团队协作时，我觉得使用好的工具来指导工作是非常有帮助的。此外，在技术能力方面，我强烈推荐大家利用AI工具，尤其是代码辅助工具和多轮对话的AI辅助导师等能力，这些可以为技术人员提供很多帮助，显著提高效率。这是我在团队协作和技术能力方面的一些建议。</p><p>&nbsp;</p><p>梁家恩：从技术管理的角度来看，我认为作为技术出身的管理者，我们应该充分发挥逻辑思维的力量。作为技术人员，我们通常对逻辑有较强的理解力。然而，我们在沟通中常常遇到效率低下的问题，实质上是因为底层的目标与逻辑没有对齐。很多无效沟通往往源于大家话术的不一致，这可能导致混乱。表面上是沟通问题和效率低下，但根本上可能是团队成员之间的目标和逻辑认知没有达成一致。我们希望通过目标导向的方法和将逻辑和流程可视化，以便在共识的基础上进行更多的沟通交流。这样一来，我们就能减少误解，提高效率。</p><p>&nbsp;</p><p>对于技术团队而言，另一个较大的问题可能是情绪管理不足。很多理工男性格相对耿直，在这方面可能需要下一些功夫。我们团队在技术与业务协同初期发现，每个人的背景和诉求各不相同，很难理解技术商业化的节奏演进。我们难以理解那些推向市场一侧的困惑和挑战。由于双方目标本质上存在差异，会导致内在的结构性矛盾。如果大家的期望没有得到清晰的沟通，这种跨部门协同就可能充满冲突和问题。良好的情绪管理可以缓解这些问题，否则可能会放大矛盾，成为更大的难题。技术管理关键在于处理这两个方面的挑战。</p><p>&nbsp;</p><p></p><blockquote>王辉：有一个观众问题是问肖老师的，请问团队管理有书籍推荐吗？</blockquote><p></p><p>&nbsp;</p><p>肖然：我认为一些传统的书籍并不可忽视，比如很多项目经理会学习PMI的PMP课程等系列。对于刚刚步入团队管理领域同事，我认为这些基础书籍还是值得阅读和学习的。</p><p>&nbsp;</p><p>在软件工程和偏向研发的团队中，我会推荐三本书。首先是一本经典之作《人月神话》，它向大家揭示了软件工程的实质，以及与传统工程的区别。书中深入分析了一个简单而重要的观点：加人并不一定代表加效率，甚至可能拖慢效率，这本书的分析非常有深度。</p><p>&nbsp;</p><p>我还推荐《团队拓扑》一书。这本书从实践的角度告诉我们如何设定团队目标，并如何有效地进行团队沟通和协调。</p><p>&nbsp;</p><p>最后是我翻译的一本书《人件》（Peopleware）。这本书在硬件（hardware）和软件（software）之外，引入了“人件”这个概念。两位作者用了25年写作这本书，发表于十多年前，但我翻译时已经是第四版。书中的很多理念在当时是非常先进的。两位作者通过数十年的从业经验，总结出了很多有趣的核心本质，称之为“定律”（law），虽然这些定律并非经过科学论证，但对于管理者来说，阅读这本书将给你带来很多启发。它提供了关于培养人才、促进团队学习等方面的见解，对真正意义上的高效团队管理有着深刻的理解。</p><p>&nbsp;</p><p></p><blockquote>王辉：有观众提问，传统制造业如何将业务与大模型结合？</blockquote><p></p><p>&nbsp;</p><p>梁家恩：这个问题涉及领域较为广泛。我认为重要的是回归到业务本身的目标和存在的问题。即使不能清晰描述方法，看看能否明确目标以及你认为当前存在的重大问题和差距在哪里，你能够指出那些痛点所在，这样我们才能更好地评估技术方案是否能够有效解决这些问题。</p><p>&nbsp;</p><p>对我们来说，我们非常期待与那些在业务中真切感受到痛点的团队进行更多的交流。在学习技术的同时，我们希望找到更广泛的应用场景，以便结合和突破。因此，我认为从一个更具创新性的角度来看，我无法笼统回答这个问题。关键在于明确问题和方法，只有当我们将问题和方法都描述清楚时，我们才可能找到解决问题的有效途径。</p><p></p><h2>未来，大模型将如何影响技术管理？</h2><p></p><p>&nbsp;</p><p></p><blockquote>王辉：您认为大模型与团队管理之间的关系将如何发展？大模型技术的发展对未来的技术管理趋势有何影响？企业应该如何准备和应对？&nbsp;</blockquote><p></p><p>&nbsp;</p><p>肖然：去年我曾经深入思考过这个问题。因为我们公司作为全球采用敏捷开发模式较早的企业之一，也是敏捷宣言的签署者，这些对我们公司的影响力非常大。因此，在大模型出现之前，我们一直在比较传统的瀑布式开发模式与上一代的对比中进行思考。在制造业，瀑布式开发模式广泛存在，其核心利润实际上是流程驱动。例如，许多制造业工厂，包括现今许多企业，都注重学习华为的流程，因为这些流程本身非常关键。对于制造业而言，如果流程没有清晰梳理，所生产的产品就很可能存在问题。我们听到的一些认证标准，例如ISO 9001或9002，实际上是对制造流程的一种认证。这种认证可以验证流程的稳定性，确保其产出是可预测的。</p><p>&nbsp;</p><p>这种流程应用到软件开发就不太奏效了。尝试用流程来控制人的思维，显然是行不通的。尤其是在当前社会趋势下，比如00后带来的职场变革，对于管理者想要控制他人的思维，确保员工不会上班“摸鱼”的做法，我持怀疑态度。敏捷宣言在开篇的第一句中明确指出，正确的软件开发方法是注重个体和互动，而不是过度依赖流程和工具。这一点强调了关注个体，并使其能够直接与其他人互动，例如，开发人员之间、开发人员与测试、开发人员与需求分析之间的协同关系。这是2.0时代的要求。</p><p>&nbsp;</p><p>但在实践中，我们可能感到有些失望，因为它对人的依赖性太强。经常出现这样一种情况，刚开始企业可能在敏捷转型方面表现得不错，前半年取得了显著进展，企业和员工都较为认可。然而，一年后，情况可能发生逆转，我们离开后企业可能回到了过去的状态，之前认为很好的员工纷纷离职，因为缺乏持续运作敏捷机制。现在大多数企业的研发组织绝对没有达到敏捷宣言所描述的程度，可能也就是 0.5 或 1.5 的程度。</p><p>&nbsp;</p><p>在2.0时代，最困难的问题是交流、沟通和知识的同步。而生成式人工智能技术背后产生的大模型，正是最擅长管理和存储知识的。这使得知识的沟通、交流以及跨职能的协作得以快速放大和增长。这也许标志着我们进入了软件工程的3.0时代。然而，模型到底能做到什么程度，目前还不得而知。例如，RAG等技术已经展示了通用型大模型在专业领域知识方面的巨大潜力，但能否真正有效地管理知识，促进我们成为一个既尊重个体又高效组织的3.0，还有待观察。因此，总的来说，对于技术管理者来说，无论如何我们都面临着与大模型共同工作以及如何使大模型更高效的问题。我们需要思考如何管理知识，或者将其融入我们的工作中。</p><p>&nbsp;</p><p></p><blockquote>王辉：这里有个观众提问，大模型时代小企业如何进行技术管理？</blockquote><p></p><p>&nbsp;</p><p>肖然：目前，创业公司大多是由两三个人组成的小团队，这个趋势已经相当明显。在2024年，我们迎来了一个多模态的时代，包括设计、互动，编码。对于这类小公司来说，如果你有创意，你实际上可以从零开始编写代码，进行测试，甚至将其上线并运营，只要你有兴趣去尝试，就完全可以做到。这对于两三个人的团队来说，并没有太多问题，因为你可以轻松在线上和线下之间切换。</p><p>&nbsp;</p><p>对于复杂产品或规模较大的团队来说，应用这项技术可能会更具挑战性。目前这一领域尚未完全实现工业化，但未来充满期待。作为创业者，你现在可以思考两个关键问题。首先，你所做的事情是否仍然有意义？例如，你可以启用并构建自己的GPTs。这个智能体根据你提供的脚本和数据输入，形成一个定制的“个性”。 如果在2024年或2025年，这种方法成为行业标准，你是否仍然需要开发一个APP？这是值得深思的第一步。其次，如果确实有这个需要，你是否考虑使用大模型来解决你自己不具备的技能，比如很多程序员可能缺乏设计技能，而现在完全有可能借助大模型实现。</p><p>&nbsp;</p><p>李梦泽：这个问题涉及到大模型与团队管理之间的关系，大模型的合理应用，以及与先进工具的协同，将使团队管理在效率方面取得巨大进步。在大模型时代，自动化和自然语义化本质上是两个技术上可能难以融合的概念，但有了大模型，它们有机会实现更紧密的结合，为团队管理带来全新的思考方式。</p><p>&nbsp;</p><p>梁家恩：对于管理工具，我认为它目前已经显示出非常重要的价值，特别是现在已经出现了 Agent 的雏形。尽管目前还存在一些不完善之处，但我坚信在未来三到五年，可能不超过十年的时间里，它将成为所有业务中不可或缺的伙伴。当这个时代到来后，我认为可以发挥两个方面的作用。</p><p>&nbsp;</p><p>首先，它是一个极好的知识和信息整合工具。通过设计适应业务环境的方式，我们可以将大模型纳入其中，有效地传递不同岗位上的能力和知识模型，形成一种模型流程。这至少可以降低人员变动带来的不确定性，使其更规范和延续性更强。</p><p>&nbsp;</p><p>其次，对于创新困境的解决，大模型可以取代人工扮演多个角色。当有一个新的想法时，我可以更轻松地打通整个闭环，而不是让人工去承担多个任务。这种轻量级的验证过程可以更快地进行，例如，我们可以使用 GPT-4 来验证一个新的工作流概念。这有助于更快地检验新想法，降低试错的成本。这两个方面都是需要关注的，并且随着大模型能力的不断增强，未来可能会有涌现更多潜力。</p><p>&nbsp;</p><p></p><blockquote>王辉：在这样一个充满变化的时代，你们会给年轻的技术管理者提供哪些建议？</blockquote><p></p><p>&nbsp;</p><p>肖然：我常常在讲课时强调的两个短语，第一个是“主动求变”。这个短语来源于习总书记在央企数字化转型时提到的三个要点中的一个，他用排比句表达了这三个要点。第一个是要积极识变，即要有意识地认知和辨别变化；第二个是要科学应变，也就是当你发现变化时，要用科学的方法而非蛮干的方式来应对；最后一个要求最高，叫做主动求变。作为技术管理者，我们刚才讨论的话题与这个密切相关。未来技术的发展，包括人工智能在内，将变化迅猛。比如，我们见证了苹果眼镜的正式发布，这可能会引发一波增强现实（AR）和虚拟现实（VR）的浪潮。你的企业业务肯定会受到影响，你可能会有很多想法。因此，最重要的一点是要主动求变，而不仅仅是被动地等待变化发生。</p><p>&nbsp;</p><p>第二个短语是“知行合一”，在中国的心学中经常被提到。原因是其实你没有什么特别好的办法，最好的方法就是扩大自己的感知能力。对于未来的预测，通常来说是不准确的，因为时代变化因素太多。在科技行业，这一点在2023年已经表现得淋漓尽致。感知能力的最佳表达其实是由王阳明先生说出的，“知行合一”。怎样去感知？就是通过行动，而如何指导行动呢？就是基于你自己的感知。在感知和行动之间，逐渐地走向正确的方向。</p><p>&nbsp;</p><p>李梦泽：技术管理需要先有扎实的技术基础，因此在相关领域，你必须对基础知识和原理有清晰的理解。这样，你才能够以身作则，成为团队中的领军人物和引领者。其次，作为技术管理者，必须保持对技术的敏感性和准确的技术判断力，以便能够带领团队走出一条正确的道路，即使这可能是一条漫长而艰难的道路。此外，在与团队成员互动时，展现真诚和真心的态度是至关重要的。通过亲身体验团队内部问题并协助解决，能更好地理解问题的本质。最后，除了拥有过硬的个人素质和一个强大的团队，你还需要适当的工具来支持技术管理工作，确保工作能够取得良好效果。</p><p>&nbsp;</p><p>梁家恩：我认为有三点要注意。首先是关注大势，意味着在面对巨大变革时，我们不能惯性地前进，而是要了解变革的方向，因为成功通常与时代密切相关。其次是持续学习。在这个时代，学习变得更为重要，但我们的学习方式可能与以前有所不同。从以前应试性的学习转变为更注重问题导向的学习，这样效率可能会更高，目标明确，解决问题有针对性。最后一点是躬身入局。许多人可能在表面上谈论许多，但并没有真正进入问题场景并亲身体会。作为技术管理者，需要深入了解团队在业务中解决的核心问题和技术方案，而不仅仅是成为一个行政管理者。对这三点的把控是成为合格技术管理者的关键。</p><p>&nbsp;</p><p></p><blockquote>王辉：有观众提问，如何通过 AI 学习程序设计呢？</blockquote><p></p><p>&nbsp;</p><p>肖然：这个问题涉及编码和程序设计两个方面。首先，若讨论编码，即写代码，当需求清晰且任务规划得当时，编写代码本身并不是主要难题。现在的大模型，如Copilot，可以编写不同领域的代码。然而，一旦提到程序设计，情况就有所不同。</p><p>&nbsp;</p><p>在国内，我积极推动领域驱动架构设计（DDD）等方法，尤其关注领域认知和业务领域划分。虽然模型可以提供建议，但不能完全取代架构师的角色。在多次与资深架构师的讨论中，我们共同得出结论：编码部分的能力可能会逐渐被模型替代，但设计部分的能力将被模型放大而无法替代。设计的重要性彰显在它从业务需求到可用软件的翻译过程中，而模型目前尚不能完成这一翻译工作。因此，程序设计中，尤其是在后半程编码阶段，需要学习如何使用提示语工程等方法。尽管代码的准确性在各方面已不再是主要问题，但设计阶段仍然需要不断历练和磨练。如果你站在技术一线，这将是未来三五年核心的竞争力所在。</p><p>&nbsp;</p><p></p><blockquote>王辉：有观众提问，小公司落地大模型有没有性价比高的方式？</blockquote><p></p><p>&nbsp;</p><p>肖然：关于 Token 的使用，首先，对于小公司而言，通常采用公有模型通过 API 进行计算。计算可以采用两种方式，一是基于 Token 的计算，即你使用了多少个 Token；另一是基于流量的计算。目前大多数人认为 Token 是一个相对合理的计费方式。如果你希望减少 Token 的使用量，你可以考虑使用一些方法，比如采用像 Agent 和 RAG 这样的方式，前置准备让使用 Token 时更加高效。</p><p>&nbsp;</p><p>在发送 Prompt 时，你可以思考如何使上下文更加有效，以优化 Token 的使用效果。此外，现在各家的模型都相对便宜，提供免费使用的服务。在未来的半年内，竞争将继续激烈，各公司都在亏损的状态下开展业务。对于小公司来说，找到一个突破点，专注优化应用的质量才是更有意义的，因为当前各家公司都在激烈的竞争。如果能够打造出优秀的应用，将会产生更大的意义。</p><p>&nbsp;</p><p>梁家恩：这个观众的问题我没太理解，他是希望利用大模型帮助他人解决问题，还是希望使用他人的大模型来解决自己的问题。如果是要用大模型帮助他人解决问题，我认为从开源方面入手可能是最经济有效的选择。已经有基础的模型，你可以在其基础上进行 fine-tuning，并结合具体业务需求，这应该是性价比最高的方法。</p><p>&nbsp;</p><p>如果是使用他人的大模型来解决自身问题，目前可能国内外都有一些开放的 API 可供使用，可以先尝试使用这些服务，验证一下是否对业务有真正的价值。如果确实有必要进行更深度的定制和优化，那么可以考虑寻找专业公司的帮助，或者建立自己的团队。最重要的是确认应用是否通用，业务是否可行。如果业务可行，后续的选择很多。</p><p>&nbsp;</p><p></p><blockquote>王辉：观众提问，如何分配核心算法的研发与业务应用落地两个方向的投入比？</blockquote><p></p><p>&nbsp;</p><p>梁家恩：我们在内部分配资源时，大约有20%的精力会专注于前瞻性核心算法。虽然这部分可能并不直接涉及当前应用，但我们坚持在技术主线和未来技术趋势中投入资源。即使目前可能无法立即应用，但如果我们判断这是未来业务的趋势，就会提前进行一些布局和准备。因为我们内部的员工在业务方面可能更为专注，所以除了内部资源投入外，我们还与外部合作建立了一些联合实验室，与高校进行合作，这种合作更适合进行中长期的技术储备。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/u3USgtmYw11CHkZRh7EO</id>
            <title>生成式AI最大飞跃！OpenAI 首个视频生成模型重磅发布，奥特曼被“跪求”：不要让我失业</title>
            <link>https://www.infoq.cn/article/u3USgtmYw11CHkZRh7EO</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/u3USgtmYw11CHkZRh7EO</guid>
            <pubDate></pubDate>
            <updated>Fri, 16 Feb 2024 04:44:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>2月16日，OpenAI在其官网发布文生视频模型Sora。据介绍，该模型可以生成长达一分钟的视频，同时保持视觉品质并遵循用户提示。</p><p>&nbsp;</p><p>Sora在日语中意为天空，该技术背后的团队包括Tim Brooks 和 Bill Peebles，之所以选择这个名字，是希望它能激起无限的创造潜力。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b11b043246361ba6e905f712818ee0d8.jpeg" /></p><p>&nbsp;</p><p>OpenAI给出了不少令人印象深刻的例子：在龙年春节里的舞龙活动，人山人海的场景真假难辨，甚至还有群众拿着手机跟拍的细节；动物世界中，站在树上、戴着王冠的猴子等；在社交媒体上教大家做曲奇的老奶奶；戴着耳机听音乐的两只金毛狗.....</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/5e/5efa3a2dd8f0cf4da4371ca520c60c8d.gif" /></p><p></p><p><img src="https://static001.infoq.cn/resource/image/5f/e1/5ff966d25d881f576b58a4149254ece1.gif" /></p><p></p><p></p><p>有网友评论说，AI能根据文字生成图片的时候，他就在想离生成视频的日期不远了，没想到这么快！“又有一波‘固执的’从业者和艺术家们要失业了，从业者门槛进一步拉低！YouTube/TikTok等一众长视频或短视频网站估计要迎来一波新的流量。”</p><p>&nbsp;</p><p>一些YouTube网红确实开始担心，MrBeast发帖半开玩笑地求奥特曼不要让他失业。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/db/db5c008a888a9cebd110a86a95b09f76.jpeg" /></p><p>&nbsp;</p><p>有投资者则发表评论说，他们早就认为“内容生成干不过大公司，初创公司要做产品和应用……”他们一直认为Pika应该转型……</p><p>&nbsp;</p><p>而OpenAI员工will depue则表示，他们过去两个月一直忙于此事，目前Sora还处于早期宣传阶段。根据OpenAI随后发布的论文来看，这个模型的能力确实已经超出了OpenAI最初所宣传的。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/c8/c804ca9140a7d097f354a865b687c590.jpeg" /></p><p>&nbsp;</p><p>这篇论文揭示了Sora架构的一些关键方面，比如它可以生成任意分辨率和宽高比的视频（最高1080p）。根据论文，Sora能够执行各种图像和视频编辑任务，比如创建循环视频、延长视频时长或更改现有视频的背景。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1b/1b595371d2b3a4f391351644efd1d1ae.jpeg" /></p><p>将DALL·E 2 或DALL·E 3生成的图片转为视频</p><p>&nbsp;</p><p>然而，最令人感兴趣的是Sora“模拟数字世界”的能力。它不仅仅是生成单个照片或视频，而是确定环境中每个对象的物理特性，并根据这些计算渲染照片或视频（或交互式 3D 世界，视情况而定）。正如Nvidia 高级研究员 Jim Fan所评论的那样，Sora 更像是一个“数据驱动的物理引擎”，而不是一个创意引擎。</p><p>&nbsp;</p><p>对于Sora目前存在的弱点，OpenAI也不避讳：模型在准确模拟复杂场景的物理特性方面可能会遇到困难，也可能无法理解具体的因果关系实例。</p><p><img src="https://static001.infoq.cn/resource/image/52/47/520865e8e6698yye4bf24730a9b7f747.gif" /></p><p></p><p>&nbsp;</p><p>例如“五只灰狼幼崽在一条偏僻的碎石路上互相嬉戏、追逐”，狼的数量会变化，一些凭空出现或消失。</p><p>&nbsp;</p><p>现在，Sora正面向部分成员开放，以评估关键领域的潜在危害或风险。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/XGe6XJLktBiAgIIomyev</id>
            <title>如何在 AI 浪潮中屹立不倒：来自企业的组织弹性实践</title>
            <link>https://www.infoq.cn/article/XGe6XJLktBiAgIIomyev</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/XGe6XJLktBiAgIIomyev</guid>
            <pubDate></pubDate>
            <updated>Fri, 16 Feb 2024 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这是我在 2023 年 10 月旧金山 QCon 大会上的演讲的摘要。关于生成式 AI，有很多可怕的东西。但如果我们能从一些角度来看待它的发展，将能够帮助塑造这个过程。我在 AI 实验室工作，我们一直在与私募股权公司的高管们讨论生成式 AI 的未来。我将在这里分享一些关于这些讨论的内容。</p><p></p><p></p><h3>计算机是愚蠢的</h3><p></p><p></p><p>作为一名在计算机早期时代成长起来的软件工程师，我总结出了一条在我整个编程生涯中对我很有帮助的准则：计算机是愚蠢的。它们不知道该怎么完成任务，除非你告诉它们，然后它们完全按照你所说的执行。它们就像提线木偶，背后有一个程序员，让它们发生互动。计算机在与人类互动方面也非常糟糕，这很令人沮丧，因为这就是我们想要使用计算机的目的。</p><p></p><p>大约 10 年前，对话式用户界面出现了，如 Alexa、Siri 和 Google Home。计算机开始变得越来越擅长与人类互动，但仍然有一个程序员在背后操纵——在句子结构、同义词和有限数量的响应方面进行编程。它仍然是一个木偶。你仍然必须对其进行编程。</p><p></p><p></p><h3>生成式 AI</h3><p></p><p></p><p>今年，事情发生了变化。现在，那个木偶可以自己说话了。不再有程序员在背后操纵，相反，系统自己会做出回应，与人类的互动变得更好了。它不再那么愚蠢。作为一名程序员，我发现这令人惊奇、兴奋，但又有点可怕。它无疑正在改变我们开发软件的方式。</p><p></p><p>在某种程度上，我们几乎所有的工作交流都是通过计算机来实现的。几乎我们所有的业务流程都依赖于某种形式的沟通。我们可以开始思考如何利用生成式 AI 来改进所有这些流程和沟通。然后，你可以开始了解我们在未来几年将面临的变革。我认为我们都应该感到惊讶、兴奋，同时也有点害怕。</p><p></p><p></p><h3>威胁与变革</h3><p></p><p></p><p>分析师和经济学家预测，由于生成式 AI 在整个经济中的应用，我们将看到全球生产力每年增长 3.3%。麦肯锡预测，无论这些工作所需的教育水平如何，生成式 AI 都将提高大多数工作的自动化程度。</p><p></p><p>当然，这也使未来变得更加难以预测，并引发了新的担忧。随着这些技术在各个行业的蔓延，我们正面临着巨大的变革。每个组织都将不得不努力将其融入到流程和工作流中。这可能涉及从自动化客户支持和市场调研到生成内容和分析数据的方方面面。</p><p></p><p>潜在的影响范围是巨大的，因为生成式 AI 对我们如何沟通和分享信息——所有业务操作的核心——都产生了影响。企业内部任何有沟通的地方，现在都有可能通过生成式 AI 来优化、增强甚至自动化。它将涉及从内部消息和文档到客户报告和产品界面的方方面面。没有一个部门、角色或项目会完全免受其影响。</p><p></p><p>生成式 AI 的颠覆本质可能是渐进式的，而不是大爆炸式的变革。公司的各个角落都会发生小的变化，每个变化都很温和，但随着时间的推移，它们会形成一场革命。所需的技能、所需的控制和对工作人员的影响将是复杂的，难以管理。每个行业都将面临独特的应用和挑战。</p><p></p><p>在软件行业，我们已经看到 GitHub 的 Copilot 帮助开发人员提高生产力。在西门罗，我们自己做了研究，发现生产力提高了 22%。设计师最终可能会在 AI 的帮助下根据规格制作网页和应用程序。生成式 AI 的测试和质量保证变得非常困难，因为每次运行的输出都可能有所不同。</p><p></p><p>其他行业，如金融和医疗保健，正在探索如何用生成式 AI 改善决策制定、预测结果、生成详细内容并提升客户体验。在营销等领域，生成式 AI 可以产生大量的文案、社交帖子和广告创意，在未来可能会挤掉人类的工作岗位。</p><p></p><p></p><h3>时间表</h3><p></p><p></p><p>William Gibson 说过：“未来已然来临，只是它的到来在各个领域是不均匀的。”我们肯定会在生成式 AI 中看到这一点。确切发生广泛变革的时间表我们尚不清楚，但历史可以提供一些启示——以前的通用技术，如电力、计算机和互联网，花了几十年的时间才充分发挥其潜力。正如你在描述互联网发展的图表中所看到的，核心技术往往在它们改变社会之前很早就被开发出来了。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/57/57662e155c4e1291fb8bafde6f6ee175.png" /></p><p></p><p>我们很可能会在生成式 AI 上看到类似的轨迹，时间跨度为 10 年或更长（见下图）。尽管神经网络和 Transformer 的基础已经奠定多年，但应用直到 2022 年才出现爆炸式增长，DALL-E 2 和 ChatGPT 等模型展示了应用的可能性。十年后，当我们回顾 2022 年时，可能会觉得那是一个未被 AI 同化、遥远、古老的年代。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ad/ad5529e611446c0cf09434fc44a55657.png" /></p><p></p><p></p><h3>保持弹性</h3><p></p><p></p><p>作为企业领导者，面对未来如此巨大的变革，我们该如何建立组织的弹性？关键在于保持灵活性和平衡——不要太保守也不要太激进。出于恐惧而完全抵制或禁止生成式 AI 是不现实的，因为竞争对手会急切地采用它，并最终超越你。但是，为了实现“AI 优化”而仓促地重新设计每个过程也是有风险的，因为我们尚不清楚它会在哪些方面给我们带来或根本不会带来价值。我有一些想法，希望以尽可能最好的方式应对这一转变。</p><p></p><p>让员工自动化他们自己的工作——他们最了解哪些部分适合使用生成式 AI 进行自动化，哪些不适合。让他们独立决定要增强什么以及如何增强。生成式 AI 是迄今为止最民主的技术之一，至少从电子表格出现以来是这样。它使用起来并不困难。</p><p></p><p>对集成生成式 AI 保持开放的态度，而不是出于恐惧而禁用它。允许团队尝试以负责任的方式集成它。在鼓励学习的同时设置一些护栏有助于你领先一步。虽然可能存在一些安全顾虑，但重要的是，所有主要的云平台现在都有一个与之连接的生成式 AI 解决方案，与你存储的敏感信息位于同一个云中。</p><p></p><p>通过培训来建立组织的 AI 素养，例如哪些任务适合使用 AI，哪些不适合。分享有关如何有效利用 AI 的知识。在组织内部分享有关如何防止生成式 AI 产生幻觉的信息。</p><p></p><p>建立一个内部维基或知识仓库，用于收集和分享知识。考虑设立一个提示词管理员来管理最有效的提示词并调整它们以提升性能。分享提示词可以节省员工的时间，而且实质上也是在记录一些人们真正希望自动化的晦涩的业务流程。</p><p></p><p>尝试使用像 GitHub Copilot 这样的工具，我们发现它可以提高生产力，同时还可以提高开发人员的留存率和士气。我发现我们的生产力提高了 22%，这就好比你每支付 4 个开发者的费用，就会多出一个开发者。</p><p></p><p></p><h3>坚定不移</h3><p></p><p></p><p>在这趟 AI 旅途中，我们都处于不同的位置。如果你的组织已经准备好利用这个提高生产力的机会，该怎么办？</p><p></p><p>只是简单地让 AI 摄取手册内容和职位描述不足以复制大多数角色。人类在沟通、解决问题、创新和同理心方面所做的事情是无法编码的。组织应该通过观察、工作流分析和行为研究深刻来理解当前的工作流程。在 AI 实验室，我们看到客户遇到了一些问题，我们在这里提供一些建议和技巧，希望你能从我们的经验中学到一些东西。</p><p></p><p></p><h5>人类在工作中所融入的东西</h5><p></p><p></p><p>人类会在工作中融入创新、推理和同理心，而你并不总是能在工作描述中看到这些。目前的 AI 无法做到这些。通常，工作描述不足以让你真正完成工作。例如，我正在开发一家保险公司的聊天机器人，让它提供保险建议。对于一个认为自己已经怀孕的人，你该如何告诉聊天机器人何时该祝贺或同情这个人？</p><p></p><p></p><h5>通过产品化方法实现自动化</h5><p></p><p></p><p>正如之前所说的，采用大爆炸的方式实现自动化是危险的。下图展示了实现工作自动化的产品化方法。你要做的第一件事就是从工人那里了解他们在做什么，以及自动化有什么意义。然后构建工具，并让团队使用这些工具。朝着自动化的方向小步迈进。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/d3/d35ddbfbff2c7bc73ecf6435b0409d49.png" /></p><p></p><p>需要注意的是，生成式 AI 很可能会影响就业市场，但它不会消除所有的岗位，更为可能的是它擅长某些特定任务，而其他任务则需要人类参与，因为人类具有更为细致的推理能力和同理心。或者，有些例行任务是 ChatGPT 永远做不了的。我曾与一位法律助理讨论过生成式 AI 对他们工作的影响，他们说生成式 AI 可能永远无法消除他们所做的最乏味的事情——复印文件。</p><p></p><p></p><h5>护栏</h5><p></p><p></p><p>与我们使用的其他工具一样，我们需要了解有哪些限制，并设置护栏。避免 AI 做出冒犯性的事情的最好方法就是尽量远离可能令人反感的事情。不要要求它变得有趣，因为这会引起反感。只需要求它尽可能简洁地回答问题即可。大型平台正在做一些微调，确保其生成式 AI 工具保持道德。一种做法是在每次有人向你的生成式 AI 提出请求时在前面加一个意图过滤器。如果不合适，你可以引导他们避开这个问题。</p><p></p><p></p><h5>谨防奇异谷现象</h5><p></p><p></p><p>奇异谷的概念源于动画和机器人技术。如果你有一个人形角色，随着它变得越来越像人类，在某种程度上变得更有相关性，然后突然，它反转了，变得非常令人毛骨悚然。对于那些希望更积极地利用生成式 AI 的公司来说，避开陷阱是至关重要的。在使用聊天机器人时类似的情况总有可能发生。要小心意想不到的负面影响，比如当 AI 交互不够人性化时，可能会导致客户不满。通过使用集体代词，让聊天机器人代表公司，可以避免“奇异谷”效应。</p><p></p><p></p><h3>结论</h3><p></p><p></p><p>生成式 AI 将改变一切，不过，这需要花费数年的时间。我们应该做好准备，确保所有员工都掌握了 AI 知识。在自动化工作流程时，我们应确保考虑到人的因素，确保人们能够专注于他们所擅长的工作。通过有意识、以人为本的规划，我们可以建立起组织弹性，以应对即将发生的变革，并蓬勃发展。生成式 AI 不会在一夜之间让世界末日降临或淘汰人类，但它可能会逐渐式且实质地改变公司内的工作方式。</p><p></p><p>查看英文原文：</p><p><a href="https://www.infoq.com/articles/ai-organizational-resilience/">https://www.infoq.com/articles/ai-organizational-resilience/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/XBNcLIEZVE9U7JtCephH</id>
            <title>在软件测试中使用 ChatGPT</title>
            <link>https://www.infoq.cn/article/XBNcLIEZVE9U7JtCephH</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/XBNcLIEZVE9U7JtCephH</guid>
            <pubDate></pubDate>
            <updated>Wed, 14 Feb 2024 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AI 可以为软件交付带来帮助，并用于自动化软件测试和优化项目工作。Dimitar Panayotov 使用 ChatGPT 生成测试数据、创建电子邮件模板，并基于测试结果生成解释。这为他节省了时间，提高了生产效率。</p><p></p><p>Dimitar Panayotov 在 2023 年 QA Challenge Accepted 大会 上分享了他如何在测试中使用 ChatGPT。</p><p></p><p>Panayotov 说，AI 是一组经过足够多数据训练的算法和神经网络，可以用来辅助 IT 从业人员。它是非常先进的辅助工具，对质量保证工程师的日常生活带来帮助，从特定主题的信息转储、测试文档和案例编写，到工作流程猜想模式。</p><p></p><p>Panayotov 说，ChatGPT 非常擅长生成测试数据，这是它的最大优势。它还可以根据需求和最佳实践创建测试策略。它可以编写测试用例，但需要额外的支持，因为它没有得到足够多的训练。</p><p>ChatGPT 可用于根据测试结果生成电子邮件模板和解释，正如 Panayotov 所说的：</p><p></p><p></p><blockquote>它可以为只有图形和数字的测试结果生成人类可读的解释。此外，它可以根据输入创建表格。</blockquote><p></p><p></p><p>由于 AI 模型仍在发展当中，在使用过程中可能会出现一些错误和故障，正如 Panayotov 所解释的那样：</p><p></p><p></p><blockquote>由于数据过拟合（ChatGPT 开发者输入的信息可能不正确，或者数据的来源有缺陷），AI 在回答问题时可能提供不正确的答案或缺失值。幻觉是另一个主要问题——基本上，大模型试图根据用户输入的字符串编写答案，但由于它存在一些限制且必须遵循一些规则，因此答案可能是完全虚构的。</blockquote><p></p><p></p><p>Panayotov 表示，ChatGPT 是一种比 Google 或 Reddit，甚至是众所周知的 Stackoverflow 更强大的软件工具。他建议将其作为一种常规工具，了解其局限性，并不要将其用于作恶。他建议：</p><p></p><p>它将为你节省大量的时间，这些时间可以用来提高生产效率，例如学习新的编程语言和工具，或扩展你的工作能力。</p><p></p><p>InfoQ 采访了 Dimitar Panayotov，了解他如何在日常工作中使用 ChatGPT。</p><p>&nbsp;</p><p></p><h4>InfoQ：你如何使用 ChatGPT 来进行自动化测试？</h4><p></p><p></p><p>Dimitar Panayotov：我使用 ChatGPT 进行许多操作，缩短自动化项目和 CI/CD 管道的创建和执行时间，比如：</p><p>准备测试计划和测试场景——在创建基于 Web、移动或 Cucumber 测试用例的测试场景时大模型非常有用。用不同的语言创建脚本——大模型所掌握的每一种编程语言的知识令人惊叹。设计测试用例——它可以根据通用模型为特定业务逻辑创建特定的设计模式。创建电子邮件模板——生成测试数据是模型的最大优势之一。</p><p>&nbsp;</p><p></p><h4>InfoQ：ChatGPT 在优化软件项目工作方面表现如何？</h4><p></p><p></p><p>Panayotov：以下是大模型可以帮助你提高执行速度、优化资源使用和项目结构的一些示例：</p><p>格式化数据并编写文档——为项目编写文档是 IT 从业人员生活中最无聊的事情。大模型可以帮助生成这些东西，但你需要提供项目的结构。几乎从头开始实现 CI/CD 管道——通过简单的请求和规范，大模型可以生成适用于几乎所有 CI/CD 系列软件和产品的执行脚本。创建特定的类——大模型可以根据项目结构为你的项目生成特定的类。代码评审和错误处理——大模型可以对你的代码进行基本的评审，甚至找到漏洞，但你的数据可能会被用在大模型未来的答案生成中。</p><p></p><p>我们必须确定是否希望大模型访问我们的项目并向其提供私有信息。ChatGPT 是一个开源产品，向其输入任何客户或个人敏感信息都是不明智的。</p><p>&nbsp;</p><p></p><h4>InfoQ：对于使用 ChatGPT 进行软件测试，你有何建议？</h4><p></p><p></p><p>Panayotov：正如我一直说的：要聪明地工作，不要辛苦地工作。所有的手动输入和数据生成步骤最终都可以被跳过，因为 ChatGPT 大模型就是为这个目的而生的。可以根据需要创建尽可能多的虚拟数据和基本项目结构，只是不要加入个人或客户数据，因为它们可能会在将来的版本或数据挖掘中被利用。</p><p></p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2024/01/chatgpt-software-test-delivery/">https://www.infoq.com/news/2024/01/chatgpt-software-test-delivery/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/AwaLR90KhsuAIwgamEvo</id>
            <title>代码人生攻略：程序员们如何为自己编织一份明朗未来？</title>
            <link>https://www.infoq.cn/article/AwaLR90KhsuAIwgamEvo</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/AwaLR90KhsuAIwgamEvo</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 Feb 2024 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI, 程序员, 大模型, 职业发展
<br>
<br>
总结: 2023年，AI已经成为不可避免的存在，对于程序员这个行业也带来了巨大的影响。大模型的出现使得代码编写更加便捷，但也给程序员带来了挑战。在这个特殊的时期，程序员需要根据行业发展规律，打好基本功并做好中长期规划，以应对未来的变化和发展方向。 </div>
                        <hr>
                    
                    <p>2023年，没有一个打工人避得开AI。程序员这个行业也不例外。现在，大量代码都可以由人工智能辅助完成，“人工智能编码助手”也已经跨越了从创新者到早期采用者的鸿沟。那么大模型对前端、架构等不同岗位等人都带来了哪些影响？大模型是否催生了更多的底层架构和云端后台技术岗位需求？未来3到5年里，程序员职业会有新的变化或发展方向吗？</p><p>&nbsp;</p><p>2023年，也是特殊的一年。一位计算机专业毕业生说，感觉似乎大公司突然就不要这么多人了，找工作好迷茫；一位大厂员工说，干了七年的程序员，今年裁员轮到了他，但他对未来没有清晰的规划；一位失业一年的41岁程序员说，他曾投简历四五百份，但都石沉大海......这样的新闻国内外比比皆是。大家都深刻地意识到行业大周期环境的变化，对于个体可能产生的不可避免的影响。那么我们是否可以做到根据行业发展规律，打好永不过时的基本功并做好中长期的规划？</p><p>&nbsp;</p><p>当前迷茫时刻，大家都需要有人给予相对明朗的分析，帮助大家应对未来的挑战和机遇。所以，在年终盘点之际，InfoQ邀请到了李博源、陶建辉、周爱民、章文嵩（按姓名拼音排序）四位重量级大咖，通过分享各自的职业心得和技术洞察，帮助我们更好地为未来的发展做好准备。</p><p>&nbsp;</p><p>以下是直播文字整理，原视频链接：<a href="https://www.infoq.cn/video/NIYfevVbiaJBhotWbdcd">https://www.infoq.cn/video/NIYfevVbiaJBhotWbdcd</a>"</p><p>&nbsp;</p><p>霍太稳：回顾刚刚结束的2023年，用一个词来描述整个技术领域，最引人关注的可能就是AIGC了。当然，目前与之相关的还有许多引人瞩目的案例，让人们对其关注度不断攀升。我记得有一次和PingCAP的黄东旭进行交流时，询问他AIGC对他个人和公司的影响。他说目前公司生产系统中的60%代码已通过AIGC生成。这个回答让我深感震撼，同时也意识到AIGC给企业带来了巨大的帮助。AIGC在帮助企业取得多方面成果的同时，也给我们程序员带来了相当大的挑战。随着年底的临近，我们有必要复盘一下大模型的到来对企业和个人产生的深远影响。那么，对于我们周围的软件行业和程序员群体而言，大模型的到来又将产生何种帮助或挑战？今天我们邀请到 4 位嘉宾与我一起来探讨这个话题。首先请各位嘉宾简单做下自我介绍。</p><p>&nbsp;</p><p>章文嵩：大家好，我是LVS开源软件的作者章文嵩。目前，我的主要工作是在语音领域进行创业孵化。眼下，我正着手孵化两个项目。首先是快猫，一个致力于云原生智能运维的项目；其次是一个专注于云原生消息队列的项目。感谢大家对这些项目的关注。</p><p>&nbsp;</p><p>陶建辉：大家好，我是涛思数据的创始人陶建辉。同时，我也是我们公司开发的一款开源软件TDengine的创作者。这个软件主要为物联网和工业互联网提供了一个强大的大数据处理平台。今天很高兴能够与大家坐在一起交流。</p><p>&nbsp;</p><p>周爱民：以前我在支付宝、豌豆荚和盛大都担任架构师的职务，不过我已经有一段时间没在职了。最近我主要致力于写书。</p><p>&nbsp;</p><p>李博源：大家好，我是松子。和爱民一样，我之前也在支付宝工作，后来转到阿里巴巴，再然后加入了车好多。自2022年以来，我也开始涉足写书。此外，在InfoQ我有自己的专栏。</p><p>&nbsp;</p><p></p><h2>大模型时代的变革</h2><p></p><p>&nbsp;</p><p>霍太稳：各位都经历过IT行业从兴起到鼎盛时期，根据您们对技术的洞察和领悟，对比您们经历的IT 发展史上的不同阶段，大模型时代与以往的变革有何不同？</p><p>&nbsp;</p><p>章文嵩：我亲历了互联网时代，包括云计算时代和移动互联网时代。在这之前，LVS是否属于互联网时代呢？事实上，互联网时代包括了90年代初美国的互联网发展，而国内在88年时首次接入互联网。我个人认为互联网时代更多地是关于连接服务和人，使得人们的联系更加方便。正因为互联网的发展，服务连接人的需求急剧增加，导致服务设备无法满足这一需求。因此，我在1998年5月启动了LVS的开源项目，专注于负载均衡。当时正好是一个机遇，因为Linux内核中还没有涉及这方面的项目，我成为了第一个着手这个领域的人。通过不懈的努力，我把这个项目做到了极致。这份代码至今仍然给我留下了深刻的印象。从1998年5月至今，已经过去了将近26年，这段代码一直保持着良好的状态，还没有被淘汰。相对于我自己编写的其他代码，有些代码早已停用，而这个代码存活了25年。</p><p>&nbsp;</p><p>在云计算时代，云服务在资源聚合方面发挥了积极的作用。云计算作为基础设施对整个行业都带来了巨大的创新。通过实现资源的可复用性，云计算能够降低研发和使用成本，为整个行业带来更为经济高效的解决方案。</p><p>&nbsp;</p><p>移动互联网时代使得大家能够随时随地访问服务，变得更加便捷。这一切使得后端面临了更大的工作压力，涵盖了互联网时代和移动互联网时代所有需求，需要进行大量的工作。</p><p>&nbsp;</p><p>现如今，我们进入了大模型时代，过去的时代带来了更多的需求。作为程序员，我们需要开发更多的解决方案来迎接面临的问题。大模型时代将带来不同的变化，因为大模型在很多方面都能更加便捷地获取知识，从而在许多领域提高工作效率。这一趋势未来将会不断加强。</p><p>&nbsp;</p><p>陶建辉：我们所经历的技术周期与大家相似，涵盖了互联网和移动互联网。我的第一家公司于2008年创业，其核心领域是移动互联网。虽然相对较早，因为真正的移动互联网兴起是在2010年。不论是互联网、移动互联网，还是云计算，它们最终都大大提高了生产效率。现在的大模型同样如此，它在各个方面提高了效率，影响着我们的生活。我认为是一场技术革命。</p><p>&nbsp;</p><p>大模型无法颠覆数据库或操作系统，这似乎是不太可能的。但大模型能够提高测试效率，提高编程效率，而数据库的设计则目前仍然由人来完成。大模型强调的是概率问题，可以生成动漫、视频或文字，非常适用。但是对于数据库，尤其是处理精准性要求的操作，比如保证正确地插入数据和查询结果，大模型都做的不太好。简单地创建数据库里的一条记录，或者生成 SQL 语句，甚至生成 TDengine 的查询语句，它都能够处理。但如果让它处理类似滑动窗口等复杂任务，大模型就无法完成。</p><p>&nbsp;</p><p>周爱民：我认为大模型能够代表一个历史性转折点。大模型改变的是计算机系统与人的交互方式。过去，我们要么让计算机理解人，要么让人学会理解计算机系统。大模型实际上改变的是计算机系统理解人的方式。以前，我们通过鼠标、键盘等输入方式提供信息，或者在数据库中逐条插入记录。而在大模型的基础上，它对人类提供的信息的理解方式完全不同。它能够对这些信息进行规格化，将其转化为特定形式进行存储，并在此基础上进行进一步的计算。交互方式在本质上发生了变化，并不是人们教导大模型系统，而是它通过与人类数据的交互。这是在人与机器以及机器系统之间的互动、对话和协作的新的方式。</p><p>&nbsp;</p><p>李博源：我自己对这方面的体会相当深刻，自己从2000年开始涉足数据领域，一直做大数据领域。正值2022年，GPT-3（ChatGPT）开始迅速崭露头角，我也开始深入研究这个领域。AIGC对于我的技能要求发生了巨大变化。过去只需要掌握几个特定的关键技术就足够了，而现在我需要理解数据、大模型，以及涉足多个学科的知识。下面我讲下我的感受，具体有以下五点。</p><p>随着大模型的崛起，我涉足某个业务领域的门槛大幅降低，当然，相应地，可达到的高度也越高。数据的重要性愈发突显，AIGC的涌现导致垃圾数据不断增多。整个交付工具方面发生了翻天覆地的变化。过去，我们通常交付的是功能。自从有了大模型之后，它像推土机一样直接覆盖整个大中小型企业应用领域，使它们重新回到同一个起跑线上。面临两极分化问题。一方面，依赖大模型的辅助工作人员变得越来越不需要动脑筋，导致这部分人大量失业。另一方面，那些更具创造力或者头脑更加灵活的人则因此变得更加富有。这种两极分化的趋势在技术发展的同时也深刻地影响了个体的经济状况。AIGC带来的效率提升毫无疑问。</p><p>&nbsp;</p><p>霍太稳：在未来我们应该关注大模型方面的哪些方面，或者可能会出现哪些突破点呢？</p><p>&nbsp;</p><p>章文嵩：大模型的出现绝对是一次重大的变革机会。OpenAI推出的GPT模型让人们认识到大模型所带来的价值。OpenAI团队以连接主义为信仰，认为连接主义的人工智能能够解决许多问题。他们构建了一个超大规模的神经网络，拥有1750亿的参数，创造了一个前所未有的训练过程，利用大量GPU卡进行训练。预训练GPT模型的语料使用了自然语言的文本，由于文本本身具有结构，不需要进行标注。这样的训练效果非常惊艳，基本上可以与人类进行流畅、迅速的互动。在效果显著的推动下，各家陆续将模型规模做得更大，达到了万亿级别的参数规模，进一步提升了大模型效果。得益于这些启示，大模型的应用范围逐渐从文本扩展到图片，再到视频，甚至发展到了多模态领域。</p><p>&nbsp;</p><p>至于大模型在未来会在哪些应用领域爆发，目前很难准确预测，因为应用领域极其广泛。微软推出了Office，GitHub也有自己的Copilot，搜索引擎进行重构，都是基于大语言模型的。大模型的应用不仅包括办公领域，还涉及到法务、医疗、客服等多个领域。这是一个生态系统，会自然演化，最终哪个领域效果更好尚不可知。</p><p>&nbsp;</p><p>在我个人的观察中，大模型本身是基于Transformer的预测模型。通过对语料的训练，模型生成了结果，但我认为它并不真正理解这些结果。与人类能理解语言背后含义不同，目前的GPT模型并不具备这种理解能力。它是基于确定性程序，通过语料的训练来调整权重，并最终通过概率模型生成结果。一些人担心AI的发展可能对人类构成威胁，但杨立昆教授认为，目前的GPT模型距离通用人工智能（AGI）还有很长的路要走，因为这个模型并不真正理解这个世界。</p><p>&nbsp;</p><p></p><h2>生成式AI对各技术领域的影响</h2><p></p><p>&nbsp;</p><p>霍太稳：我看到了一组数据，是关于百川智能王小川在云栖大会上分享的内容。他提到他曾去美国拜访了OpenAI，与他们进行了交流。在交流中，OpenAI的科学家告诉他，他们计划使用2000万张卡构建一个系统。这让王小川感到非常惊讶，因为在国内，使用2万张卡就已经很多了，2000 万张完全超出他的想象力。我观察到的另一个数据涉及微软Bing开屏。过去，Bing开屏一张图片的制作需要全球各地的200万名摄影师提供素材，但现在仅需20名人员来渲染开屏图，就能确保用户经常看到不同的图片。考虑到这两种制作方式的成本，差距是相当大的。综合考虑这些数据，我个人对AIGC在未来的发展，特别是在应用领域方面，持相对看好态度。我想听听陶老师对数据库领域的看法。</p><p>&nbsp;</p><p>陶建辉：我认为对于数据库技术本身，大模型可能不会发生实质性变化，因为数据库技术与人工智能并没有有很大关联。然而，由于AIGC产生的庞大数据量，将对数据管理提出更多要求，使我们面临更大的挑战。例如，在工业互联网中，人们希望利用大模型处理工业数据。对于时序数据库这样的存储方法，它特别适用于智能制造，如石油化工、电力和新能源生产等领域，这些领域产生大量数据。在没有大模型推动的情况下，人们可能不会充分认识到这些数据的价值。随着AIGC推动的数据量增加，我们将面临更大的挑战，其中包括对GPU、CPU、内存和带宽的需求可能会数量级上升。也许在某一天，他们可能需要使用TDengine这类数据库来处理这种海量的时序数据，这对我们来说是一个积极的消息。我个人认为，虽然AIGC可以提高生产力，但它无法彻底取代数据库技术本身。它可以提高开发和测试速度，以及一些人机交互的效率，但不能颠覆数据库技术。相反，AIGC可能会带来数据库领域的大量需求，随着数据量的增长，这对所有IT从业人员都是有利的。</p><p>&nbsp;</p><p>霍太稳：对于我们技术人员来说，我听到了一个相对正面的观点。他表示，随着AIGC或大模型的兴起，技术人员，尤其是CTO，将变得越来越重要。当时我感到有些困惑，为什么呢？似乎不太可能。他给了我一个例子，现在很多公司董事长要利用AI来改进整个流程。于是，CTO的价值变得明显，因为他必须深入业务流程，才能让AI发挥最大的价值。因此，作为CTO或技术人员，你需要在董事长面前频繁亮相，从而彰显其价值，这对我们技术人员而言可能是一个积极的反馈。大模型的出现对前端和大数据领域技术人员有何影响呢？</p><p>&nbsp;</p><p>周爱民：在前端领域，我认为有两个关键点。首先，从应用的角度来看，我们可能会经历类似于CS到BS的转变，即从客户端到浏览器端的重构。在这个过程中，人们会意识到所有的东西都需要重新开发一遍。尽管需要进行重新开发，但背后的业务需求却是一模一样的。举例而言，如果你原来在客户端要做一个图书管理系统，将其迁移到浏览器端，结果是相同的，只不过需要重新实现，因为环境发生了变化。因此，在大模型或AI的背景下，所有东西都可能会经历一次重构，但业务本身并没有变化。</p><p>&nbsp;</p><p>其次，在我前不久写的一篇名为《<a href="https://zhuanlan.zhihu.com/p/661629653">告前端同学书</a>"》的文章中，我特别提到前端同学需要特别关注浏览器技术。其中一个点，我没有在文章中明确提到，现在想强调一下，那就是我们要思考未来的浏览器到底会浏览什么内容？浏览器是否仍然只是用于查看网页上的文本、图片和超链接？现代浏览器主要浏览我们所说的网页内容，以及参与网页上的一些交互。然而，未来的浏览器将会发生变化。随着人机系统交互方式的变革，交互方式、对象和内容都发生了变化。因此，未来的浏览器肯定不会是现在的样子。由于交互技术的变化，未来的浏览器将发生巨大的变革，从而引领整个前端的革命，这个部分是最重要的。</p><p>&nbsp;</p><p>李博源：在大数据领域，我认为变革相当巨大。过去，我带着锤子和电锯就能完成项目，但现在有了大模型，就好比带上了一台多功能车床。我可以通过这台车床组建各种工具来处理工作。具体表现在哪里呢？去年我尝试了一个项目，使用GPT，从数据底座、需求指标开始梳理数据建模，进行了一次自动化工程，效果非常好，特别是在大数据仓库、数据模型、数据资产管理的效率上取得了显著的进展。</p><p>&nbsp;</p><p>其次，大模型在数据分析方面的能力也是非常出色的，尤其是在泛化过程中，包括报表解读、总结以及深度的专题分析。考虑到企业已经进入数字化转型的第二阶段，数据的价值和发现变得至关重要。如何提升企业员工的数据分析能力成为一个关键问题，而大模型提供了一个很好的切入点。我日常还会参与一些运维工作，大模型可以帮我从运维日志中轻松定位问题。我本身是一名产品经理，因此我主要关注大模型在上层应用方面的运用，即如何在跨学科的领域中提高效率，这是我在这个领域着重关注的方向。</p><p>&nbsp;</p><p></p><h2>大模型时代，程序员的“生存法则”变了吗？</h2><p></p><p>&nbsp;</p><p>霍太稳：我们刚才讨论了对每个行业的影响，现在让我们聊聊对于当下的程序员、运维工程师和产品经理，这些从业者的技能提升和整体工作模式是否会发生变化？</p><p>&nbsp;</p><p>章文嵩：作为程序员和工程师，我们应该积极拥抱这些新技术。AI是来赋能的，它能够有效地提升我们的工作效率。因此，我们不是被AI打败，而是可以与之协同合作。我鼓励大家积极拥抱这些变革。</p><p>&nbsp;</p><p>另一方面，我们每天工作中要使用GPT等大语言模型，亲身尝试，获取第一手的感觉。需要注意的是，GPT这样的大模型本身是一个知识的压缩网络，将全世界的文本知识压缩到一个神经网络中。当我们向它提问并提取知识时，可能会存在一些信息提取不准确的情况。因此，在使用过程中，我们需要对结果保持判断力。工程师应该构建自己的知识体系，因为人类对事物和世界有独特的理解。大模型虽然提供了高效的知识提取手段，但在使用结果时，我们需要具备判断力，有选择地使用其中的信息。</p><p>&nbsp;</p><p>在后端架构设计方面，理解需求和问题的行为特征至关重要。根据这些行为特征，我们需要进行权衡，确定最优的解决方案，以最低的成本实现系统的设计。这对我们的技术能力是至关重要的。拥有这些技能的同时，通过AI提升工作效率，将会增强我们的竞争力。</p><p>&nbsp;</p><p>陶建辉：我认为程序员本身不会被AI所淘汰，但不愿意应用AI的程序员可能会面临被具备AI技能的同行超越的风险。作为程序员，无论你在哪个行业，都必须积极拥抱大语言模型，以提高生产效率。然而，对于具体的业务本身，有些行业可能会面临颠覆，例如游戏和内容创作行业。在这方面，使用大模型创作游戏可能是提高效率的关键，而不掌握这一技能可能导致竞争力下降。对于其他行业，例如CIA系统的开发，逻辑复杂性较高，AI可能无法轻松应用。对于从事CIA开发的程序员，核心的逻辑和技能仍然是至关重要的。他们可以考虑利用AI来提升开发速度、产品健壮性和人机交互效果，但基本的逻辑和专业知识并未改变。因此，程序员应该根据所在行业的特点选择适合的工作方法。</p><p>&nbsp;</p><p>此外，一些行业可能会因大模型的出现而逐渐萎缩，但大部分行业仍将持续存在，类似互联网和移动互联网并没有摧毁传统的电力、煤矿和汽车行业。从事软件开发的程序员需要持续学习，不仅要拥抱新技术，还要根据行业需求不断提升自己的技能。</p><p>&nbsp;</p><p>霍太稳：大家对程序员的定义现在有没有发生变化？</p><p>&nbsp;</p><p>周爱民：程序员的定义本质上不会有特别大的变化。程序员的核心任务仍然是编写程序。写程序的本质是将一个现实系统映射为面向机器的可计算系统。未来的程序员仍然需要从现实系统或构想中获取需求，然后将其转化为计算机系统。虽然使用的工具可能有所不同，就像我们之前提到的从大刀过渡到手枪、机枪，但程序员仍然扮演着“士兵”的角色，其基本任务并不会改变。</p><p>&nbsp;</p><p>代际性的产品如大语言模型及其相关技术，或者以大语言模型为基础的系统架构，的确可能会对某些行业和技术产生破坏性的影响。比如，重机枪摧毁了传统武术、支付宝改变了防盗行业的例子。我认为基于大模型的产品设计和实现可能导致一些行业和技术的变革，这是不可避免的。这种代际性的产品可能导致一些人失业，有些人可能需要离开原有的岗位。然而，这并不意味着程序员将会消失，因为依然需要有人去编写程序，将现实系统或者想要实现的系统映射为计算机系统的任务仍然存在且必不可少。</p><p>&nbsp;</p><p>李博源：我认为可以将人群分为非IT人员和IT人员，对这两个群体的影响是不同的。对于刚入门的IT人员，大模型的出现可能使门槛降低，技能提升变得更加容易，因为他们可以得到大模型的辅助。然而，这也意味着这些新人需要在这个环境中更进一步地成长，以适应更广泛的技术领域。</p><p>&nbsp;</p><p>对于专业人员来说，影响可能更为深远。专业人员可能需要主动获取信息，通过阅读类似InfoQ的文章来不断提升自己。因为大模型的出现使得跨多个领域变得更容易，这意味着他们可以更轻松地完成更为复杂的任务。以数据分析人员为例，大模型的出现可以显著减少处理临时需求的时间，从而使他们有更多的时间去研究更高级的领域，如数据洞察和专题分析。未来，多样化的角色和跨业务的能力，即所谓的复合性能力，可能会成为一种基准线。这并不是说专业的开发者的价值会减少，而是角色定位和所需的技能将发生了翻天覆地的变化。</p><p>&nbsp;</p><p>霍太稳：大模型时代开发者的范畴在变大，能利用AIGC开发应用就算开发者，不一定要会写代码，即非专业开发者或者业技复合型人才在增加，与此同时，专业开发者人群在减少。您们是否认同这个趋势？</p><p>&nbsp;</p><p>章文嵩：我们现在要调用服务，不必从头搭建所有东西，可以直接调用远程服务，例如使用云数仓Snowflake这样的平台，而无需自己构建，这提高了整个社会的效率。像Snowflake这样的公司需要大量构建这样的平台，对专业程序员的需求量仍然存在。</p><p>&nbsp;</p><p>霍太稳：确实像Snowflake这样的公司需要很多专业成员，但相比之前很多公司都需要自己聘请专业成员搭建，它们的数量是不是在减少了？</p><p>&nbsp;</p><p>章文嵩：但对整个社会的效率提升是显著的。像Snowflake这样的公司做云数仓的不止一家，全球有几十家上百家，它们需要大量的专业人员，而其他用户只需使用云数仓这项服务，无需自己建立一个大数据团队。</p><p>&nbsp;</p><p>陶建辉：我认为它将程序员的门槛降低了，使一般人稍微学习下就能从事相关工作。特别是现在做前端，我在1994年就开始做网站，当时我们需要学习CGI（Common Gateway Interface）并使用C语言编写代码，还需要了解Oracle数据库等，门槛相对较高。而现在不再需要懂这些知识，大模型的出现大大降低了门槛。我认为程序员的数量会增多，大模型的出现并不会减少程序员的数量。社会将变得更加信息化，数字化，对如何处理这些信息的需求将更加多样，会涌现出很多新的岗位，比如已经出现了许多新的岗位如提示工程师等。当前很多传统行业也在进行数字化转型，比如汽车行业已经变成了软件行业，汽车的价值主要由软件创造，创造软件一定会需要大量程序员。</p><p>&nbsp;</p><p></p><h2>巨头碾压，大模型小创企有核心壁垒吗？</h2><p></p><p>&nbsp;</p><p>霍太稳：AI创业还有一些特殊性，比如最近很火的AI公司Perplexity.ai是3个人、Pika是4个人、Midjourney 12个人，跟以前传统创业要求相比，确实是有了变化。随着大模型时代的到来，对于我们这些个体，特别是创业团队，这将带来怎样的影响呢？</p><p>&nbsp;</p><p>章文嵩：通过AI的赋能，个体的工作效率将显著提高，这可能导致未来更多像你刚才提到的那些规模相对较小的公司可能会越来越多，因为AI的赋能使每个个体的实力变得更强。相比过去，创业需要整合大量资源，进行融资，并建立庞大的团队方能展开工作。而现在，AI的赋能使个体的能力更为突出。因此，未来有可能看到更多深入某个领域、规模较小的公司兴起。</p><p>&nbsp;</p><p>李博源：我对商业化领域有不同的看法。作为一个产品经理，从产品角度来看商业化。</p><p>&nbsp;</p><p>从中小企业和大型企业的角度来看，大模型的出现拉平了创业机会。但我认为，整个竞争格局的核心还是在商业化上。最终，大模型的技术创新需要转化为商业价值。在转化过程中，产品的市场适用性、创新速度和用户接受程度至关重要。中小企业的体量有限，很难建立核心壁垒。但大企业本身就具有技术优势。所以，谁的产品速度更快，谁能够在短时间内爆红，谁就更有可能在商业化竞争中胜出。</p><p>&nbsp;</p><p>曾有人说：“小企业是AI+，大企业是+AI。”大模型的应用归根结底还是商业行为。最终买单的还是市场和流量。所以，我认为中小企业在长期竞争中并不占优势。我反而看好中小企业在短期内爆发的可能性。</p><p>&nbsp;</p><p>陶建辉：大变革时代是个人英雄的时代。在这种时代，只有个人英雄才能突破现有格局，创造新事物。因此，大变革时代往往是小公司和个人崛起的时代。而在稳定发展时期，组织的力量更加重要。对于程序员来说，大模型是最好的机会。大模型的出现，打破了传统的开发模式，为程序员提供了新的舞台。我个人很遗憾没有机会参与大模型的开发。如果有机会，我一定会毫不犹豫地加入。</p><p>&nbsp;</p><p>章文嵩：大模型的开发需要巨大的资源投入，起步就需要几万张卡甚至上千万张卡。因此，程序员如果对大模型感兴趣，我建议加入有前景的团队，因为单打独斗很难整合这么多资源。在使用大模型的基础上，可以构建很多垂直领域的应用。这些应用的创业机会很多，关键在于对垂直领域需求问题的理解。只有深刻理解垂直领域的需求，才能结合大模型在垂直领域创造价值。垂直领域有很多，每个领域都有自己的特点。人是真正理解这些领域的，因此在垂直领域的创新创业机会层出不穷。</p><p>&nbsp;</p><p>李博源：我完全赞同这个观点。AI大模型的出现是一种破坏性创新，它能够快速、灵活地针对细分市场进行产品创新，以敏捷性和便捷性迅速占领市场。然而，创业型企业在建立壁垒方面面临一定的局限性，很容易被大型企业抄袭。那么，小企业在大模型尤其是Agent领域的创业如何建立壁垒呢？</p><p>&nbsp;</p><p>章文嵩：从科技的发展历史看，新兴公司往往能够通过颠覆性创新打破现有格局，取得成功。我认为未来科技的发展速度会越来越快，新的机会也将层出不穷。未来，我们的工程师、技术人员的创业机会是越来越多的。</p><p>&nbsp;</p><p>周爱民：小公司与大公司实际上是无法正面对抗的，这是绝对的现实。小公司的优势在于能够在不同领域扎根生长，或者在同一领域找到不同的发展点。这揭示了大公司另一个巨大的弊端，即所谓的“船大难掉头”问题。大公司可能花费3到4年的时间来推动一个项目，一旦启动，它可能会顺利推进，但要在几个月，甚至一年内改变方向几乎是不可能的。大公司就是有这样的问题。因此，小公司正面竞争大公司的意义并不大，因为小公司无法拥有大公司的资源和机会。然而，小公司可以在行业发生大变革的时候，找到一些大公司没有涉足的全新方向，从而做一些非常有意义的事情。在这个时间差里，小公司可以成长壮大，最终变成大公司。这也是我们之前看到的所有变革中，一些像春笋一样冒出来的公司成功的原因。</p><p>&nbsp;</p><p>章文嵩：大公司往往拥有雄厚的资源，但这些资源往往分散在不同的部门。即使再多的资源，也无法满足每个部门的所有需求。以云数仓为例，美国三大云计算巨头都提供云数仓产品。谷歌的BigQuery、微软的Azure Synapse Analytics和亚马逊的Amazon Redshift，都希望通过云数仓业务来抢占市场。但即使他们拥有大量的资源，也无法在云数仓领域取得绝对优势。</p><p>&nbsp;</p><p>小公司虽然没有大公司那么多的资源，但在某个垂直领域，往往能够聚合更多的资源。例如，Snowflake就是一家专注于云数据仓库领域的初创公司，它在云数仓领域积累了丰富的经验，并取得了一定的成功。因此，小公司并不应该害怕大公司，只要找到自己的优势领域，聚合更多的资源，也能取得成功。</p><p>&nbsp;</p><p>李博源：我不是说小公司创业要害怕大公司，但小公司创业，尤其是在大模型领域，必须建立自己的壁垒。大模型的创业，从长期来看，需要建立两个方面的壁垒：</p><p>商业化模式壁垒：小公司需要打造一种可持续化的商业模式，能够产生持续的收入和利润。技术创新壁垒：小公司需要不断创新，提升技术实力，才能保持竞争优势。</p><p>&nbsp;</p><p>对于大模型的创业，尤其是Agent方向的创业，我认为以下两点需要注意：大模型不能解决总体性的业务问题：大模型只能解决具体的、局部的问题。而对于小问题，大公司可以快速投入资源，迅速解决。因此，小公司创业在大模型领域，必须要找到自己的优势和定位，打造自己的壁垒，才能在竞争中立于不败之地。</p><p>&nbsp;</p><p>陶建辉：我认为，真正的壁垒在于谁对这个领域了解，在这个领域积累的数据。数据是训练大模型的基础，也是大模型的核心竞争力。因此，创业者要想建立壁垒，必须要在这个领域深耕，积累数据。这些数据是创业者的重要资产，不会无偿提供给他人。</p><p>&nbsp;</p><p>霍太稳：我们可以用一个小故事来结束这个辩论。几年前，拼多多的黄峥接受我的采访时说，有人问他为什么要做电商，电商领域已经有京东和阿里两大巨头了。黄峥回答说：“时代总是在变，大家不可能一直用一种模式，也许会有一种新的模式会出来。”几年后的今天，拼多多已经成为中国电商市场的重要力量。这也证明了黄峥的预言是正确的。当然，我们还可以再往前倒一倒，看看被 Facebook 收购的 Instagram 的故事。Instagram 当时只有十几个人，但却被 Facebook 以 90 亿美元的价格收购。这也说明，大公司可以通过收购来获得创新的能力。总而言之，大公司和小公司各有优势。大公司有资源和规模优势，可以快速扩张；小公司则更加灵活敏捷，更容易创新。</p><p>&nbsp;</p><p></p><h2>从事开源项目的收益：不止“用爱发电”</h2><p></p><p>&nbsp;</p><p>霍太稳：我们回到开源的话题。根据我和很多朋友的沟通，包括开放原子开源基金会的秘书长冯冠霖上周到我们公司交流，大家认为，在大模型时代，开源更加重要。因为在当今国际环境下，开源是我国唯一能够与国外技术竞争的优势。当然，也有人认为，开源的收益可能不像从前那么好了。此外，开源领域已经被“开源老人”占据，像 PostgeSQL 和 Linux 这样的项目对年轻人来说门槛相对较高，缺乏新鲜血液，这个问题您们怎么看？</p><p>&nbsp;</p><p>章文嵩：我认为，程序员通过开源是与世界互动最好的方法。在公司或团队中，你只能接触到有限的客户和团队成员。通过开源，你可以与来自世界各地的人交流，接触到更多的需求。真实的需求来自真实世界，往往比如何写代码更重要。此外，你可以与高手交流，学习到更好的编程技巧。这些对个人成长都非常有利。因此，我一直鼓励年轻程序员参与开源。开源可以帮助你提升能力，展示自己的才能，并在开源社区中建立人脉。</p><p>&nbsp;</p><p>以 Linux 和 PostgreSQL 为例，这两个项目都是非常活跃的开源项目。每年都有大量的新人参与到 Linux 内核的开发中，新的版本也不断发布。这些新版本都是为了满足新的功能需求。PostgreSQL 是 Michael Stonebraker 在 80 年代开发的开源数据库，已经有 30 多年的历史，但仍在不断发展。<a href="https://www.infoq.cn/article/l469ga8ml5y0jspgqlto">年过 80 的 Stonebraker 还在坚持持续创新</a>"，是我们学习的好榜样。PostgreSQL 的模块化设计非常优秀，许可证也比较宽松。因此，很多创业项目都是基于 PostgreSQL 构建的。PostgreSQL 的生态系统也非常活跃，目前已经超过了 MySQL，成为了数据库领域的领军者。对于年轻程序员来说，这是一个巨大的机会。</p><p>&nbsp;</p><p>陶建辉：我认为开源是程序员成长的舞台，无论是老项目还是新项目，都为年轻人提供了展示才能、提升能力的机会。老项目如 Linux 和 PostgreSQL 已经有几十年的历史，积累了大量代码和经验。这些项目的社区也非常活跃，有大量的贡献者。对于年轻人来说，加入这些项目，可以从老一辈程序员身上学习到宝贵的经验。然而，老项目的社区也存在一些挑战。由于项目的规模和复杂性，年轻人很难在其中脱颖而出，成为明星人物。我建议年轻程序员可以选择加入新的开源项目。新的开源项目往往更有活力，更需要新鲜血液。在这些项目中，年轻人更容易获得认可，成为明星人物。以我们自己的时序数据库为例，TDengine 是一个年轻的开源项目，成立于 2016 年。在 2022 年，TDengine 在 Pull request 和 Issue 数量上远远超过了另一个老牌时序数据库 InfluxDB，社区非常活跃，为年轻人提供了展示才能的机会。</p><p>&nbsp;</p><p>章文嵩：对于年轻程序员来说，无论是加入已有的老开源项目，还是新的开源项目，都应该积极参与项目讨论，了解当前有哪些问题需要解决。如果有好的想法，可以大胆提出，并尽力实现。这样，不仅可以为项目创造价值，也能提升自己的技术能力和实践经验。</p><p>&nbsp;</p><p>周爱民：我自己也参与过不少小项目，自己也发布过一些小的开源模块。我认为，对于程序员来说，开源是展示自己的最佳舞台。开源可以让雇主快速了解你的代码质量、编程风格、语言习惯和技能栈深度。如果雇主在招聘程序员时，看到你的 GitHub 账号上有丰富的开源项目，就会对你的技术能力和经验产生好感。我认为，对于大多数程序员来说，开源是展示自己的最佳舞台。如果你想在程序员行业取得成功，那么应该积极参与开源。</p><p>&nbsp;</p><p>李博源：我自己没有参加过开源项目，但我经常使用开源软件。我认为，在价值与收益的抉择上，年轻程序员应该回归到价值这个领域。90后、00后程序员大多很现实，他们参加开源项目，往往是为了以下几个目的：获得收益，提升知名度，建立影响力。那么，年轻程序员到底是应该选择哪一种方式？我认为，他们应该以价值为导向，选择自己真正感兴趣、有价值的开源项目。</p><p>&nbsp;</p><p>陶建辉：我是一个开源的受益者，我认为开源是 TDengine 成功的重要因素。TDengine 是一个时序数据库，在工业领域具有广泛的应用。由于开源，TDengine 迅速获得了大量用户。在开源之前，工业领域的数据库市场被传统的商业软件主导。TDengine 的开源为用户提供了一个免费、高性能、灵活的时序数据库，从而颠覆了传统的市场格局。我个人也从开源中受益匪浅。我曾在福布斯上发表了一篇文章，介绍了时序数据库的未来。这篇文章引起了广泛的关注，也让我成为了时序数据库领域的知名人士。</p><p>&nbsp;</p><p>此外，开源还为 TDengine 贡献了大量的代码。例如，TDengine 的 DB 索引是由天津的开发者贡献的，ODBC Driver 是由苏州的工程师贡献的。这些贡献帮助 TDengine 不断完善，并为用户提供了更好的使用体验。总而言之，我认为开源是 TDengine 成功的重要因素。开源为 TDengine 提供了快速发展的动力，也为 TDengine 的未来发展奠定了坚实的基础。</p><p>&nbsp;</p><p>章文嵩：我个人在开源中受益匪浅，尤其是在 LVS 项目中。LVS 项目是我早期参与的开源项目，为我的个人成长和事业发展提供了巨大的帮助。在参与 LVS 项目的过程中，我接触到了大量的开源代码，学习了先进的技术，并积累了丰富的经验。这对我的技术能力提升有很大帮助。LVS 项目还帮助我获得了职业机会。阿里巴巴正是通过 LVS 项目了解到我，并邀请我加入。</p><p>目前，我正在参与孵化创业项目 AutoMQ 和快猫。这两个项目也都采用了开源的方式。通过开源，我们可以构建整个开源生态，并通过开源来获取用户。此外，开源还可以帮助我们快速迭代产品，并将产品推向全球市场。</p><p>&nbsp;</p><p>陶建辉：我最近看到我的儿子在开源项目上取得了巨大的成功，这让我深受鼓舞。我的儿子在大学毕业后，创办了一个名为LuxAI的人工智能竞赛平台。这个平台与语言无关，吸引了来自全球各地的1000多支队伍参与。他之所以能够取得如此成功，主要归功于开源。通过开源，LuxAI能够吸引到来自不同背景和技能水平的人才，共同为项目做出贡献。此外，谷歌也对LuxAI提供了赞助。谷歌不仅提供了所有计算资源的赞助，还为比赛提供了5万美元的奖金。这进一步提升了LuxAI的知名度，吸引了更多人才的参与。</p><p>&nbsp;</p><p>霍太稳：不久前，我看到一位前同事在虎嗅上发表的一篇文章，题为《<a href="https://mp.weixin.qq.com/s/4wnATYFbld_-9SH4UOdIig">中国开源，又一次让人失望了</a>"》。文章中提到了一个观点，我个人非常赞同。他指出，一些国内企业存在竭泽而渔的倾向，他们似乎认为开源并不复杂，将其简单等同于免费。或许正因为这种看法，解释了为何中国企业在大模型方面的突破仅仅停留在数量上，因为抄袭终究是借用他人成果而不积极参与开源贡献，通常也难以具备真正的技术优势。各位如何看待这个观点？</p><p>&nbsp;</p><p>章文嵩：这篇文章的结论我认为存在一些问题。尽管中国整体的开源生态未能充分体现在工程师们的回报中，但实际上中国在全球的开源力量排名第二，仅次于美国，这一点可以通过在GitHub上公开的数据进行验证。然而，我认为中国的开源生态仍有许多需要改进的地方。</p><p>&nbsp;</p><p>我觉得其中一个最大的问题是，国内一些较大型的公司在开源领域中虽然实力相当，但却没有出现过令人瞩目的合作典范。国内商业竞争激烈，人们更多地关注短期的商业竞争，而很少看到公司之间在开源生态上展开合作的案例。公司与公司之间真正的开源项目合作几乎没有，除非是某个领域的龙头企业与相对实力较小的公司形成一种“老大哥带小弟”的合作模式，而这样的实例也相当有限。</p><p>&nbsp;</p><p>我认为国内整个开源生态需要在这方面有所改进。如果国内公司能够更积极地展开开源项目的合作，这对中国整体的实力和影响力将产生巨大的影响。以RISC-V为例，如果中国能够在全球拥有最大的话语权，那么整个RISC-V的利益分配中，中国将能够获得最大的一块份额。我们的企业应该把眼光放长远，放弃一些短期利益，五年或八年后可能会实现更大的价值。</p><p>&nbsp;</p><p>陶建辉：这篇文章对中国开源社区的评价有失偏颇。文章主要讲述了几个中国开源社区圈子的害群之马，因此得出了一些不利的结论。但实际上，整个中国在开源社区的贡献已经越来越大，不仅仅是明星项目，在其他领域也取得了许多成绩。不过，公司之间的合作仍然不够多，确实还有待改进。</p><p>&nbsp;</p><p>霍太稳：我想向我们的编辑推荐一个文章题目：《中国开源并没有让人失望》。我最近在和一些企业交流，发现他们在开源方面做得很好，但最近在商业计划中弱化了开源，甚至不做了。我不知道你们有没有这样的感受，但我不好评论，因为竞争太激烈了。我希望线上的同学可以参与评论一下。一些企业一开始从开源中尝到了甜头，但后来因为商业竞争，又把开源这个门关上了。爱民老师对这个现象有什么看法吗？</p><p>&nbsp;</p><p>周爱民：我认为这是一种正常的商业行为。企业从商业角度出发，做出商业性的选择是可以理解的。这并不意味着这家公司要完全放弃开源。它可能只是选择了某些项目或方向来进行投入，这是战术性的做法。</p><p>&nbsp;</p><p>在战略层面，我们应该都认同拥抱开源的价值。但不同的企业对自己在开源生态中的位置、价值和利益的认识不同，对开源生态的概念本身的理解也不同。有些人认为开源就是索取无度，或者是占便宜。这种想法当然存在，但不应该是主流。但开源的大的形势还是合作、交流和共享。中国的一些大企业在开源贡献方面投入很大，但却没有得到应有的认可。这主要是因为它们在开源社区中的发声不够，表达的方式不够清晰。我认为构建开源生态不仅仅是贡献代码，还要有人去发声，有人去向世界说明，有人去承担重要的责任。这些看似虚的东西，其实是生态的核心。</p><p>&nbsp;</p><p>陶建辉：我个人非常倡导中国程序员在全球发声，其中一个重要方式就是用英文写博客。我自己写英文博客的水平还远远不够，但我相信如果能达到我中文博客的水平，那在全球推广开源软件的效果会更成功。</p><p>&nbsp;</p><p>中国企业的开源软件之所以知名度不够、影响力不够，与中国程序员在全球范围内发声不足直接相关。尤其是在 YouTube、Twitter 等平台上，中国程序员缺乏有影响力的技术文章。我建议在座的各位程序员，除了在 InfoQ 上用中文写博客、做课程之外，还可以到 YouTube 上用英文做课程。</p><p>&nbsp;</p><p>章文嵩：我举一个具体的例子，阿里捐赠给基金会的 RocketMQ 是面向微服务的开源消息队列。在国内，RocketMQ 的生态非常强大，有许多公司在使用。但 RocketMQ 的开源生态绝大部分还局限在国内。这是因为 RocketMQ 社区中的英文文档非常少。我建议他们先翻译 RocketMQ 的文档，无论是使用人工翻译还是机器翻译。只有这样，才能让更多的海外开发者了解和使用 RocketMQ。中国的开源项目也要走出去，才能真正实现全球化。</p><p>&nbsp;</p><p>霍太稳：我们对开源布道师的理解有了更深刻的认识。以前，我们认为开源布道师只是在宣传开源技术，但现在我们认为，好的开源项目不仅需要好的技术，还需要专业的布道师来宣传。开源布道师不仅要了解开源技术，还要了解开源社区，善于沟通交流。他们需要通过各种渠道，用通俗易懂的语言，向用户讲述开源项目的价值和优势。只有这样，才能让更多的人了解和使用开源项目，推动开源生态的发展。</p><p>&nbsp;</p><p>章文嵩：关于刚刚提到的部分公司关闭原有开源项目，这从商业利益的角度是可以理解的。毕竟公司要生存，要赚钱，如何决策都是公司自己的事。另一方面，如果一个开源项目已经建立了一定生态，突然关闭会造成巨大损失。这时候需要仔细分析，做出正确判断。国外也有一些公司采取了宽松的开源许可证政策。例如，MongoDB 在去年 8 月份将其许可证改成了 Server Side Public License，基本上就是不允许第三方提供对外商业服务，只有 MongoDB 公司可以提供服务，其他云厂商要提供服务，需要获得 MongoDB 的授权并支付授权费。这对开源生态也会产生一定影响，但影响并不致命。还有一部分公司是基于 MongoDB 的基础上，又开发了自己的开源项目。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>互联网公司不需要CTO了？</h2><p></p><p>&nbsp;</p><p>霍太稳：我还有一个问题，就是公司还需要 CTO 吗？这个问题的背景是，2023 年 9 月，居然之家裁掉了包括 CTO 在内的整个 IT 团队。居然之家的创始人在公开演讲中提到，裁掉 IT 部门是一个很大的风险。原来 100 多人，包括 CTO 和高管都没有留下。目前是让数字化团队的负责人接管，相当于用外包的方式找专业的公司来做数字化平台。结果，效益确实大大提升了。京东的 CTO 岗位也一直空缺着，小红书在经历了多位 CTO 轮换之后，也裁撤了 CTO，由 COO 兼任，一人统管电商、技术和产品团队。看到这些新闻，大家觉得研发部门在公司里发挥的价值到底在哪里？</p><p>&nbsp;</p><p>李博源：数字化转型对 IT 部门提出了新的要求，如何建立有效的 IT 团队，是企业需要思考的问题。从核心来讲，这个问题涉及四个方面：</p><p>数字化与 IT 的关系；数字化转型的难点；在投入有限的情况下，IT 部门该怎么做；CTO 在数字化转型中的作用。</p><p>&nbsp;</p><p>回顾 2023 年，数字化浪潮席卷全球，让社会商业模型发生了巨大变化。传统的 IT 部门从成本中心转变为战略伙伴，承担起推动业务创新和竞争优势的职责。一些企业为了快速转型，将 CTO的岗位职责转变为让业务负责人监管技术、产品，好处是在这种大环境、新商业模型下是必须可行的，毕竟企业要生存，团队要生存，必须得考虑优先服务业务，优先为单UE考虑的。IT 部门自然也存在这样问题，传统观念就是IT是成本中心，维护基础设施和日常运营。而新的改变是IT部门在推动业务创新和提供竞争优势方面。</p><p>&nbsp;</p><p>例如，可能会牺牲 IT 架构的合理性，导致系统性能和安全性下降。因此，在数字化转型中，CTO 自己也需要转型。CTO 需要具备战略思维和技术能力，能够制定技术战略，带领团队开发新技术，推动业务创新。我认为，数字化转型对 IT 部门提出了新的要求，企业需要根据自身情况，制定合理的 IT 部门建设方案。</p><p>&nbsp;</p><p>周爱民：我认为这涉及到组织架构设计的问题。在组织架构设计中，有一个简单的判别方法，我称之为“命令”。正如中国有一句古话，“既不能命，也不能令”，即不能给予具体的职务任命，也不能下达明确的指令。如果一个角色既不能任命也不能下令，那么对应的职务也就不应该存在。回到刚才的问题，我们需要明确一点，CTO的职责究竟是什么？他如何推动工作，或者如何带领团队完成任务，这些组织架构的关系需要明确才能判断CTO是否有存在的必要。</p><p>&nbsp;</p><p>在很多情况下，许多公司把CTO看作是一个管理角色。这个管理角色涉及到人、事、物等方面的管理。如果他的职责和权利没有明确规划，那么他可能就不应该存在。如果他不接受任何人的管理，不接受指令，那么他的存在可能也没有意义。因此，我认为CTO的存在与否取决于组织架构中是否存在任命和下令的关系。在这一判断中，业务是一个关键的因素。在业务线上，技术CTO的角色是否有存在的必要，如果需要，那么这个角色就应该存在，即使是由其他角色兼任也是可以的。如果不需要，那么就无需存在。</p><p>&nbsp;</p><p>在我之前所在的公司，就没有明确的CTO角色。如果需要，CTO可以由其他角色兼任。总之，CTO的存在与否应该根据具体的组织架构模式和业务需求来决定。我认为，要还是不要，这要视具体情况而定，不能一概而论。</p><p>&nbsp;</p><p>章文嵩：听了你提到的居然之家的例子，我的第一印象就是四个字：始乱终弃。一开始公司成立时就设立了IT部门，明显是为了构建许多有助于提高整个业务流程效率的IT系统。有了这样的IT系统，才有可能积累一些数据，进而进行数字化的工作。然而，后来他们竟然将数字化项目外包，这就让我感到困惑。数字化需要深入理解业务，才能为业务的数字化提供支持，并在业务运营和决策方面提供数字化支持。因此，将数字化项目外包，让我感到他们对数字化的重视程度不明确。而且，外包公司又怎么可能比内部更深入地理解他们的业务呢？我认为，居然之家并不是一个好的例子。在你提到的京东和小红书的例子中，他们并没有宣布永远放弃CTO这一职位，而是因为暂时找不到合适的人选。</p><p>&nbsp;</p><p>我认为一家公司对技术的看法非常关键。如果公司认为技术对其至关重要，而技术的战略规划对公司未来的发展和业务拓展至关重要，那么就应该设立CTO职位，由CTO负责统筹技术规划和技术战略布局，这一点非常关键，公司应该深刻考虑技术对业务的重要性。</p><p>&nbsp;</p><p>霍太稳：我们在外部很难全面评判，居然之家可能是因为没有很清晰地理解信息化和数字化之间的关系。或许最初是想进行自身的信息化，但在寻找团队的过程中发现信息化在很多情况下其实是需要大量成本投入的。数字化则可能需要更多的产出，但当数字化来临时，原有的团队可能没有跟上，导致出现一些问题。此外，外包团队可能出于降低成本的考虑，或者是类似的想法。</p><p>&nbsp;</p><p>章文嵩：我曾在多家大公司工作，包括阿里淘宝、滴滴等。在这些公司，技术部门如果没有外部收入，通常就是成本中心，但若有外部收入，就有可能独立出一块业务。然而，大部分情况下技术部门主要为内部各个部门提供服务，因此通常是成本中心。在这种情况下，CTO需要清楚地向CEO解释，技术部门如何为客户创造价值。</p><p>&nbsp;</p><p>CTO的第一个服务对象是CEO，因此CTO必须了解业务，理解技术如何为业务创造价值，并以清晰的语言向CEO传达这个价值。例如，对于成本中心来说，应清楚解释成本的构成，因为任何公司首先是业务，而业务的单位模型中技术成本的占比是多少？举例来说，无论是淘宝还是滴滴，都可以将其订单的IT成本与带来的收入进行比较。随着订单规模的增长，每个订单的IT成本是否能够持续下降？通过清晰地呈现每个订单的收入和成本，CEO就能够更清晰地了解每笔订单的盈利情况，从而更好地进行决策。此外，CTO还需要解构成本，明确日常运营和定期运营哪些部分可以持续优化，哪些是需要新的投资的。这些方面的沟通和解释对于确保CEO对整个技术投资的理解和支持至关重要。</p><p>&nbsp;</p><p>我认为CTO职位非常重要，不仅要理解技术对业务的价值，还要通过清晰的沟通方式，确保CEO对技术投资的理解，并明确各个方面的投资价值，从而服务好全公司各个业务部门。</p><p>&nbsp;</p><p>陶建辉：我认为整个中国对CTO的定义存在一个误区。至少在国外，CTO并不是一个纯粹的管理岗位，也不是真正领导团队的职位。在美国，CTO更多地参与小型团队，通常从事前瞻性和探索性的工作，关注技术储备等方面，而对团队的管理相对较少。</p><p>&nbsp;</p><p>在国内，好像很多大型IT公司都设有CTO。我个人认为刚才提到的居然之家这类企业，是数字化转型的企业，我看到很多中国的数字化转型案例失败的原因在于老板没有清晰认识。他们可能认为只需要招聘一些程序员来做就可以了，实际上数字化转型并不总是需要建立庞大的研发团队，而更需要对整个IT系统架构有清晰认识的人才。例如，公司可能有一个登录服务，甚至这个服务都可以使用第三方的，而不一定需要自己开发。他们更需要的是一个系统集成的角色，将各个组件整合在一起，不必自己开发每一个组件。整个系统运行，包括CRM系统、OA系统、ERP系统以及更大的供应链管理系统，这些可能都不需要自己的研发团队，而是可以购买第三方服务，然后集成在一起，提供一个完善的数字化平台。</p><p>&nbsp;</p><p>因此，我认为居然之家的 CEO 对CTO的理解在这种情况下是有偏差的。他可能认为应该招聘像文嵩这样的博士，开发一套IT系统，但这样的投入产出比可能不划算，也有可能是招聘的人才不够匹配，更多的是自己对 CTO 的定位不准确。</p><p>&nbsp;</p><p>霍太稳：对于大的传统企业，他们到底需要一个什么样的 CTO 呢？有没有一个 CTO 画像？</p><p>&nbsp;</p><p>章文嵩：我认为CTO肯定是首先服务于CEO，需要深入了解业务，通过技术手段为业务创造价值。在整体流程设计和集成方面，许多组件都可以利用第三方服务，包括IaaS、SaaS等服务，甚至像登录服务和账户管理系统也可以使用第三方的。对于大多数公司来说，数字化仍然很重要，因此需要有类似CTO这样的职位，负责整合所有外部的IT系统，为内部业务创造价值。对于绝大多数规模较小的公司而言，进行自建系统开发毫无必要。</p><p>&nbsp;</p><p>霍太稳：我们从另一个角度来看，对于当前许多总监或CTO，他们应该采用一种什么样的方法来向CEO明确展示自己的价值，使其更为显性化和易于理解。</p><p>&nbsp;</p><p>章文嵩：当然，如何为用户创造价值是关键的。有些业务本身离不开IT，不管你投入10人还是20人，关键是要明确适当的投入。讲清楚每个订单的成本是最好的方式，包括可用性指标。如果你的技术平台一旦出现问题，整个业务都会受到影响，这时可用性指标就显得尤为关键。企业应该关注可用性指标，例如一年内不可用的时长，能否从200分钟下降到52分钟，从而实现“四个九”的可用性。如果服务停止一次，造成的损失有多大也需要考虑。</p><p>&nbsp;</p><p></p><h2>答疑</h2><p></p><p>&nbsp;</p><p>霍太稳：我们回答一个线上观众问题，他问：“后端Java如何面对现在这个大模型的时代？现在比较焦虑。</p><p>&nbsp;</p><p>章文嵩：我认为后端使用Java的情况下，很多时候会偏向应用层面的程序，后端的程序较为丰富。在这方面，我认为他应该拥抱大模型。对于一些模块的实现，可以咨询ChatGPT或者Bard，它们有可能为你生成一些参考代码。在工作中，一些模块可以通过参考代码来体现，但辨别参考代码是否可用需要具备判断力，这是至关重要的。引入存在错误的代码，可能对整个程序的可靠性造成问题。因此，我认为工作岗位应该不受影响，而是可以利用大模型来提高工作效率。</p><p>&nbsp;</p><p>周爱民：我认为不存在所谓的Java后端、Node.js后端或者C++后端的分类，这样的分类并不具有实际意义。后端就是后端，不论你使用何种具体语言实现，最终问题都是实现的交付界面和接口的问题。我曾经说过一句话，大型系统不是通过编写代码开发出来的，而是部署出来的。后端的问题不在于用何种语言编写代码，而在于如何进行部署，以及系统的模块、组件和服务之间的交互界面是什么。这才是后端的核心问题。这些问题是普遍的，不论你是否使用大模型，对系统的理解和对系统交互界面的实现都遵循相同的模式和方式。大模型的出现可能会带来一些新的系统分层和交互方式，但这并不影响后端开发的核心。</p><p>&nbsp;</p><p>因此，我建议后端开发人员应该关注系统的组装、构建和架构，而不是编程语言本身。当然，我们可以说Java更成熟、更稳定，有更多的用户使用和更多的基础组件，但这并不是关键。当你进入大模型的环境时，就算Python的基础组件更多，最终仍然回归到部署和系统交互的问题，与语言选择无关。</p><p>&nbsp;</p><p>李博源：后端开发人员每天接受的信息量是以前几十倍到上百倍，导致可持续思考的时间变短。在这种状态下，如何保持思考的深度和广度，是后端开发人员面临的一个挑战。随着系统和业务的复杂化，跨领域问题也越来越多。要解决这些复杂问题，后端开发人员需要不断学习新知识，拓宽知识面。即使大模型的出现，面临的问题仍然是一样的，后端开发人员仍然需要具备良好的基础编程能力和逻辑思维能力。</p><p>&nbsp;</p><p>霍太稳：作为一个新人小白，如何参与开源项目？是重复造轮子还是参与别人的项目呢？</p><p>&nbsp;</p><p>陶建辉：这个问题很简单。如果小白想参与一个热门项目，可以先从提问题做起。在看问题的过程中，可以逐步了解项目的代码、架构和流程。也可以从文档着手，补充文档中遗漏或不准确的内容。等到积累了一定的经验后，再尝试修复 bug。刚入行时，不要急于创造，先学会问问题。不要重复造轮子，可以加入社区或项目，先解决一些小问题，甚至是文档问题。这样可以逐步熟悉项目，积累经验。</p><p>&nbsp;</p><p>霍太稳：接下来的问题是问松子老师，他问：“大学专科生，大数据怎么入门，走哪个方向比较好？”</p><p>&nbsp;</p><p>李博源：我去年参加了两个高校的大数据相关学科评定，发现大数据领域发展非常迅速，与人工智能和机器学习的结合也非常紧密。这带来一个问题，就是原有的概念还没消化完，新的概念又出现了。从入门角度来说，我建议首先要理解基本概念，包括数据仓库、数据平台、数据中台、数据湖等概念及其发展历程。其次，要掌握核心技术，例如 SQL、Python、数据处理工具和平台、可视化等。最后，在大学一年级或二年级的时候就应该开始参加大数据实践项目，否则永远只能停留在理论阶段。大数据领域的实践方向有很多，包括数据工程师、数据分析师、机器学习工程师、数据架构师等。通过实践，可以了解不同方向的具体工作内容，并根据自己的兴趣和能力选择合适的方向。我认为 InfoQ 和《极客时间》上有很多很好的学习资料，可以帮助大家了解大数据领域的最新技术和实践经验。</p><p>&nbsp;</p><p>霍太稳：如何看待程序员培训班？现在高校教的东西都不实用吗？</p><p>&nbsp;</p><p>李博源：高校在设立大数据学科时，往往会沿用原有的教材，导致教学内容与最新技术脱节。此外，新兴技术的更新速度很快，高校的教学内容很难跟上。因此，高校需要不断改进教学内容，使其与最新技术保持一致。幸运的是，近年来，高校开始注重实践教学，结合具体的项目和行业专家进行教学。这是一个很好的趋势，可以帮助学生掌握最新技术。</p><p>&nbsp;</p><p>周爱民：我坦承，我和《极客时间》中的很多老师做不好很基础的课。因为我已经忘记了当初自己如何从头开始学习计算机和编程了。高校可以教好基础知识，但在应用环境和具体开发工作方面不擅长。因此，毕业后参加培训班学习新技能和实战经验非常重要。即使大学成绩优异，毕业后直接进入头部公司，也需要接受三个月或半年的培训。这种培训与培训班性质相同，只是来源不同。培训班的教学和培养目标是对高校教育体系的补充。然而，目前有些培训班的教学方法和质量仍然不够高，急于求成。因此，我们需要持续改进培训方法和质量，以达到预期的培训效果。</p><p>&nbsp;</p><p>章文嵩：美国的文理学院也是主要注重人文素养等基础教育，而编程等技术类课程更需要实践。无论是自己学习一门语言，还是用它来解决问题，都需要在实践中不断积累经验，才能更深入地理解。因此，在大学期间，应该积极参与课题研究和开源项目，抓住实践的机会，这样对于语言的学习就不成问题了。</p><p>&nbsp;</p><p>霍太稳：我们这有个观众问题是这样的：“我来自传统行业，老板对大数据等 IT 项目持保留态度，认为这些项目成本高、见效慢，我如何说服老板投入资源进行大数据项目研发？”</p><p>&nbsp;</p><p>章文嵩：这又回到刚才的问题，是否需要聘请 CTO，我们需要从企业的实际需求出发来分析。对于食品生产企业来说，你要说清楚利用大数据如何提升运营效率、提高决策水平、发现新商机等。具体来说，可以通过对数据的分析来优化生产流程，提高生产效率，降低成本等。在选择数据平台时，可以考虑使用第三方工具，无需自己开发。</p><p>&nbsp;</p><p>李博源：数据系统和 IT 系统的最终目的都是提高企业的盈利效率。盈利效率可以分为内部效率和外部效率。内部效率是指企业内部的运营效率，包括生产效率、销售效率、管理效率等。外部效率是指企业与外部环境的互动效率，包括市场营销效率、客户服务效率等。数据系统和 IT 系统的设计和实施应该围绕这两个方面进行。</p><p>&nbsp;</p><p>霍太稳：有个观众问题：“大龄程序员，目前还不是管理岗位，现在互联网发展速度这么快，我要何去何从？要出国吗？”</p><p>&nbsp;</p><p>陶建辉：年龄不是问题。程序员的工作就是写代码，继续写下去就行了。我认为，程序员的工作很有魅力，程序员的工作很客观，输入什么输出什么，这都是明确的。这让程序员的工作具有很强的挑战性，也让程序员的成果可以被客观地评价。</p><p>&nbsp;</p><p>周爱民：这个问题其实首先是一个喜好的问题。如果你对写程序没有兴趣，那就去开车或者做其他任何你喜欢的事情。年龄并不是问题，只要你喜欢写程序，就继续写下去。如果你不想从事管理工作，那就不要去做。国内也不会因为你年龄大了就强迫你继续写程序。年龄大了，你可以去尝试其他事情，甚至还可以回来继续写程序。</p><p>&nbsp;</p><p>章文嵩：我去淘宝的时候已经36岁了，是不是一个很好的“案例”？</p><p>&nbsp;</p><p>周爱民：我仍然在编程，从未停止。我们已经积累了丰富的经验，因此并不需要仅仅依靠编程来维持生活。但我想表达的是，35岁的问题可能在你25岁或30岁时就已经存在了。你可能在25岁或30岁的时候就开始思考，到了30岁或35岁后是否还应该继续编程，以及如何继续编程。在这个过程中，关键在于确定自己的道路是否可行。只要你能编写优秀的代码，创造出出色的产品，就一定会有人需要你。</p><p>&nbsp;</p><p>霍太稳：接下来是一个高中生的问题，他问：“现在学Python，应用大型模型可以快速生成程序代码，那么对于高中生来说，到底应该培养什么样的思维？”</p><p>&nbsp;</p><p>周爱民：程序员最常做的事情实际上不是写代码，而是改代码。大约有三分之二甚至四分之三的时间是用来改代码，而不是写代码。写代码的技能其实是最基本的，是可以被替代的。假设AI已经生成了代码，你怎么改呢？你需要先理解业务，理解已经写好的代码，理解所有的接口，这种能力是从写第一行代码开始培养的。AI根本做不到这一点。因此，AI可以替代你写代码，而后续改代码的工作，才是最有价值的工作，是程序员需要自己去做的，这些工作包括改代码、测试、编写测试代码等，这些是AI无法完成的。</p><p>&nbsp;</p><p>程序员的基本功不仅仅是写代码，还包括以下几个方面：编程思维、逻辑能力、表达系统能力、组织系统能力、构建大型系统的能力以及与他人合作的能力。对初学者来说，学习代码语法固然重要，但并不是最重要的。初学者更应该花时间来培养编程思维、逻辑能力等方面的能力。只有这样，才能真正成为一个优秀的程序员。</p><p>&nbsp;</p><p>章文嵩：对于高中生来说，学习Python语言并不仅仅是为了学习一门编程语言。重要的是，如果他能够通过Python语言解决现实世界中的一个问题，哪怕这个问题很小，都会给现实世界带来价值。这种实际解决问题的过程将给他带来心理上的成就感和获得感，这对于他持续学习并在这个领域继续努力非常关键。因为我们一生都需要不断学习，持续进步。关键是要在学习的过程中培养出对这个领域的浓厚兴趣，并且能够将自己的学习输入转化为有意义的输出。</p><p>&nbsp;</p><p>李博源：针对刚才提到的高中生问题和35岁的职业危机问题，我想分享一项我去年进行的调研。这项调研涉及国人从毕业到就业再到持续发展的整个过程。我发现很少有人真正去给自己制定职业规划。自我职业规划是非常重要的，它需要结合个人的兴趣爱好、价值观、未来发展方向等因素，制定一个长达三到五年的计划，并且定期进行调整。然而，事实上，国人很少有人去做这样的规划。对于高中生来说，学习什么以及毕业后继续学习什么，我建议可以参考自己的职业规划来定。</p><p>&nbsp;</p><p>章文嵩：编程语言和工具不断变化，但有些最基本的技能从未改变。比如，抽象能力和建模能力是永恒不变的。无论是用大模型还是汇编语言，你都需要解决问题。真正的教育应该解决的问题是如何培养这些基本能力，因为拥有这些能力，你就不会被淘汰。抽象能力意味着能够将物理世界的问题转化为类似数学和逻辑问题的形式，并将其描述出来，以便计算机解决。如果你无法做到这一点，即使是ChatGPT也无法帮你生成代码。</p><p>&nbsp;</p><p>霍太稳：我们观众的问题都很直接，有位观众问：“请教几位专家，如何保持对编程的热爱？”</p><p>&nbsp;</p><p>章文嵩：通过编程解决现实世界的问题会给你带来成就感，就像我做LVS项目时感受到的那样。这个项目被许多人使用，已经使用了25年以上，几乎全世界的公司都在使用它作为负载均衡的标配。尽管这是一个开源项目，没有金钱回报，但我觉得很满意。游戏之所以能吸引人，是因为游戏能够提供即时的反馈。人们在游戏中做出任何操作，都会立即得到游戏的反馈，这人感到兴奋和满足。同样，如果程序员编写的代码能够在现实世界中得到即时的反馈，并且为现实世界创造价值，那么这种反馈会比游戏的反馈更让人感到兴奋和满足。</p><p>&nbsp;</p><p>陶建辉：我一辈子最自豪的是两个软件项目，它们都不是我现在写的。一个是我在90年代上学时写的“账务系统”，另一个是“LP模天平”，后者是我为材料系高分子LB膜实验室制作的，这个项目至今仍在使用。</p><p>&nbsp;</p><p>霍太稳：观众提问：“双非本科（非985和211院校）毕业工作从事Java开发1年，感觉每日都在做重复的工作，有什么提升的方法吗？”</p><p>&nbsp;</p><p>章文嵩：既然是重复的工作，那就把重复的工作通过工具来自动化，让程序来完成。这样一来可以节省大量的时间和精力，并提高工作效率。</p><p>&nbsp;</p><p>霍太稳：观众提问：“作为一名架构师，进入一个新的公司，如何快速地把技术和业务结合起来，产生业务价值或提出建设性的意见？”</p><p>&nbsp;</p><p>陶建辉：在我的职业生涯中，如果我进入一家新的公司，无论是架构师还是程序员，我都会选择去公司最烂的地方。因为那里的问题最多，也最具挑战性。我希望能够通过自己的努力，解决这些问题，让公司变得更好。</p><p>&nbsp;</p><p>周爱民：我认为这个话题涉及架构师在加入新公司后的两种不同风格。首先是一种直接上手的架构师，他们会快速介入公司现有的IT模块并解决特定的核心流程或产品问题。这种风格的架构师相对简单，因为他们很容易沉浸在具体任务中，并且通常无法调动太多资源，可能会被局限。</p><p>另一种风格的架构师则需要得到公司高层的大力支持，这意味着老板必须愿意让他们去做一些貌似超出权限的事情，例如支持他们去探访与各个部门、了解不同系统模块，以便触及所有可能的问题和挑战。因为这种架构师是能够发现公司现有问题并从中找到解决方案的人。他们会从问题出发，制定解决方案，并规划战略部署，逐步推进和实施，而不是一开始就试图解决问题。</p><p>&nbsp;</p><p>这类架构师的存在通常是因为公司遇到了重大问题或核心系统矛盾，他们是解决这些问题的关键。因此，我认为架构师的基本能力是定义问题，并在公司内部推动解决方案的实施。在我看来，往往一个公司需要一个架构师，是在他们并不真正地知道自己出了什么问题的时候，所以这也是架构师在加入公司后需要重点关注的方面。</p><p>&nbsp;</p><p>章文嵩：在架构师加入新公司时，并不着急去定义问题。我建议首先要努力融入团队，了解当前团队正在解决的重要问题，以及为什么这些问题重要。然后，逐步提出一些新的想法，并尝试协助解决一些问题。我认为建立相互之间的信任是首要任务，因为一旦建立了信任，提出新想法并获得支持就会更容易。这个过程在开源社区中同样适用，就像在小规模的环境中建立声誉后再去影响更广泛的范围一样。所以，首先是要融入团队，建立信任，然后再努力获取团队和老板的支持。</p><p>&nbsp;</p><p>霍太稳：我们回答最后一个观众问题：“技术人如何突破瓶颈？如何有效进行技术管理？”</p><p>&nbsp;</p><p>章文嵩：我认为在技术管理方面，最重要的是赋能团队。比如，设定一个项目时，让团队自行承诺时间节点和整体规划。如果你对团队的承诺感到满意，那就没有必要去改变它。管理者的角色应该是让团队自主前进，除非团队偏离了方向，需要进行一些引导和启发。重要的是，要确保团队的理念是团队共同构建的，而不是管理者强加于他们的。我个人的感受是，通过共同创造的方式，将个人想法转化为团队共识，从而提高项目执行的效率和质量。</p><p>&nbsp;</p><p>霍太稳：我认为对于许多从事技术工作的同行来说，过去的一年可能是相当具有挑战性的，无论是在管理岗位还是技术岗位上。最后我们为所有的技术从业者们送上新年的祝福和寄语作为今晚主题的结束吧。</p><p>&nbsp;</p><p>周爱民：我就觉得最好的说法就是冬天会过去，春天会来。</p><p>&nbsp;</p><p>李博源：过去，成为专家需要经过约3000小时的训练，但现在有了大模型的帮助，这个过程变得更加高效。我希望在新的一年里，大家都能成为资深专家！</p><p>&nbsp;</p><p>章文嵩：我认为，AI原生领域存在着大量的创业机会。尽管这些信息化和数字化仍处于早期阶段，但AI原生方面的创业机会却十分丰富。对于个人而言，持续保持好奇心，终身学习并构建自己的知识体系非常重要。利用AI工具提出问题，并结合自己的判断力，在许多方面做出独到的贡献，最终形成自己的核心竞争力。</p><p>&nbsp;</p><p>陶建辉：在这个技术变革的时代，我们更需要像乔布斯所说的那样，保持“Stay hungry, stay foolish”的状态。</p><p>&nbsp;</p><p>霍太稳：今天我最大的收获之一是关于大模型的讨论。当然，大模型也让许多人感到焦虑，正如之前提出的疑问一样。但我认同几位专家所说的，新技术总是不断涌现的，对于技术人员来说，我们已经习惯了这种变化。因此，我们无需过度焦虑，而是应该拥抱这些变化，学习必要的知识，提高自己的工作效率。</p><p>&nbsp;</p><p>其次，尽管技术人员可能不太善于沟通，但在这个时代，技术的价值必须显性化。我们应该努力让自己创造价值得到认可，无论是向CEO、领导还是经理都要清楚地表达自己的价值。我相信，一旦我们取得这样的进步，35岁焦虑、40岁焦虑都将不复存在。</p><p>&nbsp;</p><p>2023年已经过去了，2024年对我们来说是一个新的开始。希望每个技术人员都能振作起来，让技术的价值更加突出。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/BJrcFZLrdbShNpFJvFqV</id>
            <title>OpenTofu 1.6.0 正式可用，带来了模块测试、增强的 S3 后端等特性</title>
            <link>https://www.infoq.cn/article/BJrcFZLrdbShNpFJvFqV</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/BJrcFZLrdbShNpFJvFqV</guid>
            <pubDate></pubDate>
            <updated>Fri, 09 Feb 2024 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenTofu, 版本1.6.0, Terraform, 高级测试特性
<br>
<br>
总结: OpenTofu 1.6.0 是 Linux 基金会托管的 Terraform 的一个社区开源分支，引入了许多新特性，包括高级测试特性，提升了稳定性和功能。S3 状态后端也得到了更新，提供了新的身份验证方法。此外，还推出了新的提供者程序和模块注册表，以及其他改进和错误修复。 </div>
                        <hr>
                    
                    <p>OpenTofu 1.6.0 已正式可用。作为 Linux 基金会托管之下的 Terraform 的一个社区开源分支，OpenTofu 现在提供了一个稳定版本，带来了许多特性，包括针对配置和模块的高级测试特性、增强的 S3 状态后端（新的身份验证方法）、一个新的提供者程序和模块注册表，以及许多其他改进和错误修复。</p><p></p><p>OpenTofu 1.6.0 版本引入了一系列旨在增强稳定性和功能的新特性。其中值得关注的是它的测试特性，让用户能够评估 OpenTofu 的配置，并允许模块构建者进行模块测试，从而增强了整体稳定性。与 OpenTofu 核心的集成已经完成，确保无缝的用户体验。</p><p></p><p>此外，S3 状态后端经历了重大更新，提供了许多新的身份验证方法，同时与兼容 S3 的对象存储保持兼容性。该版本还推出了一个新的提供者程序和模块注册表，采用类似 Homebrew 的架构，完全基于 Git 存储库。</p><p></p><p>注册表托管在 CloudFlare R2 上，其特点是响应性和高可用性，发布新提供者程序或模块的过程被简化为拉取请求。</p><p></p><p>除了这些亮点之外，该版本还包含了大量的小改进、错误修复和性能增强，详细信息可在变更日志中查看。</p><p></p><p>活跃的社区在努力推动 OpenTofu 的发布和相关工作。核心 OpenTofu 贡献者 Sebastian Stadl 表示：</p><p></p><p></p><blockquote>这个 OpenTofu GA 版本是跨 5 个大洲的全球社区付出了 4 个月努力的结果，数百名贡献者和五十多名开发人员参与其中。</blockquote><p></p><p></p><p>对于未来，OpenTofu 认识到与 Terraform 保持兼容的重要性，报告称，该项目致力于保持合理水平的兼容性，避免对 DSL 或提供者程序协议做出重大更改。</p><p></p><p>GA 版本的博客公告表示，1.7 版本的路线图中包含了客户端状态加密这一用户一直期待的功能，用于实现端到端状态和计划文件的安全性。这对于在受监管的环境中运行的项目以及追求最大安全性的项目来说是有价值的。</p><p></p><p>刚开始将为用户提供的密钥和选择的密钥管理服务提供支持，未来可能会提供插件系统，用于引入额外的密钥管理服务，具体取决于社区反馈和使用模式。</p><p></p><p>此外，为了响应社区的一般性请求，OpenTofu 正在探索实现可参数化的后端、提供者程序和模块。这涉及使用变量对模块版本进行参数化，并通过 for_each 静态值清单中的参数实例化提供者程序。OpenTofu 计划在适当的时候解决这些需求。</p><p></p><p>为满足用户对新状态后端的需求，OpenTofu 引入了一个类似于提供者程序的插件系统。这种专注于第三方可扩展性的做法与该项目致力于增强整体生态系统并为用户提供多种选择的承诺保持一致。</p><p></p><p>随着这些重大改进即将到来，OpenTofu 仍然继续关注其用户群体的多样性需求。项目积极鼓励社区参与，欢迎各种建议和贡献，鼓励用户提交任何与 OpenTofu 功能或增强有关的问题，确保平台的协作性和以用户为中心的演进模式。</p><p></p><p>最后，对于感兴趣的读者，可以参看从 Terraform 迁移到 OpenTofu 的指南 。</p><p></p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2024/01/opentofu-1-6-now-ga/">https://www.infoq.com/news/2024/01/opentofu-1-6-now-ga/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/LuhXRa4tjPvzJKdAJxUC</id>
            <title>Discord 使用单台服务器服务于 100 万在线 MidJourney 用户</title>
            <link>https://www.infoq.cn/article/LuhXRa4tjPvzJKdAJxUC</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/LuhXRa4tjPvzJKdAJxUC</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Discord, 优化, 性能, 可观测性
<br>
<br>
总结: Discord 对其平台进行了优化，以提供给超过100万在线用户服务，并保持用户体验的反应灵敏。通过使用Elixir实现关键元素，Discord实现了路由和消息通知的功能。通过对系统性能的理解和优化措施，Discord团队成功提高了消息处理的效率，并减少了Guild进程的工作量。他们还引入了中继进程和内存数据库等优化方法，以提高系统的可扩展性和性能。 </div>
                        <hr>
                    
                    <p>Discord 对其平台进行了优化，以便于在单台服务器上为超过 100 万在线用户提供服务，同时保持反应灵敏的用户体验。在系统可观测性和性能调优的支持下，该公司对负责发送数十亿条消息通知的 guild 组件进行了一系列的性能和可扩展性增强。</p><p></p><p>Discord 平台的关键元素是使用 Elixir 实现的，这是一门运行在 Erlang VM) 之上的函数式语言。基于 Elixir 的组件负责路由和向用户发送消息通知。Guild 服务器是管理 Discord 社区各种业务流的中心枢纽。Guild 进程会与很多会话进程进行交互，而这些会话进程使用 WebSocket 连接将消息投递到用户设备上的客户端应用程序中。架构的另外一个关键要素是 API 服务，它是使用 Python 编写的，负责将消息持久化保存到 ScyllaDB 中。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8f/8f533d886ddde48b6b8bc6a6a7fc6900.jpg" /></p><p></p><p>通过 Discord 平台的消息流（图片来源：Discord 工程博客）</p><p></p><p>鉴于之前的设计选择和平台限制，Discord 团队必须确保 Guild 进程能够继续处理不断增加的在线用户数量，例如，Midjourney 社区的在线用户数量就超过了一百万。Discord 的软件工程师 Yuliy Pisetsky 讨论了与服务器性能相关的用户体验考虑因素：</p><p></p><p></p><blockquote>除了整体吞吐量方面的考虑因素之外，服务器越大，某些操作的速度就越慢。确保几乎所有的操作都能快速进行，这对服务器的响应速度至关重要：当发送一条消息时，其他人应该能立即看到；当有人加入一个语音频道时，他们应该能立即参与进来。如果处理某个昂贵的操作需要好几秒钟的时间，这会损害用户体验。</blockquote><p></p><p></p><p>工程师们花了大量时间来理解系统性能。他们插装了 Guild 进程的事件处理循环，以捕获消息处理时间的关键指标。团队利用进程堆栈跟踪进行分析，寻找导致消息处理延迟的原因。他们还创建了一个辅助库，用于估算大型对象的内存使用情况，以便于帮助优化内存的使用。有了可观测性数据，他们实施了多项优化措施，大大缩短了某些消息类型的处理时间。</p><p></p><p>团队通过减少 Guild 进程的工作量取得了一些重要成果。他们禁用了被动会话的通知，在被动会话中，用户不会与他们所属的某些社区进行互动。仅这一项改变就使扇出（fanout）工作的成本降低了 90%，从而为不断增长的用户数量提供了急需的空间。</p><p></p><p>开发人员引入了一层新的中继进程，帮助在 Guild 进程和会话进程之间更高效地传递信息。中继进程接管了部分业务流的处理工作，从而释放了 Guild 进程来处理更多的用户。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/77/77dcdb63b9b3a26471008496e71527e7.jpg" /></p><p></p><p>中继进程层（图片来源：Discord 工程博客）</p><p></p><p>其他优化还包括使用了内存数据库 ETS，从而能够存储成员列表并在进程间实现安全地共享，以及创建一个单独的发送进程来向接收节点发送消息。</p><p></p><p>在 HN 论坛中，博文作者回答了社区提出的一些问题，社区对 Discord 实现的一些增强功能的细节很感兴趣。</p><p></p><p>查看英文原文：</p><p></p><p>Discord Scales to 1 Million+ Online MidJourney Users in a Single Server(<a href="https://www.infoq.com/news/2024/01/discord-midjourney-performance/">https://www.infoq.com/news/2024/01/discord-midjourney-performance/</a>")</p><p></p><p>声明：本文由 InfoQ 翻译，未经许可禁止转载。</p><p></p><p>今日好文推荐</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651195270&amp;idx=1&amp;sn=9dfc2d0191c917a607ccc3a5a99c3f47&amp;chksm=bdbbffd58acc76c383f0abdee7c8e721d0972c1239cc2946f36ce3b3781e73cd496acc285e5f&amp;scene=21#wechat_redirect">Taylor Swift 身陷不雅照风波：AI 越强、Deepfakes 越猖狂，微软和推特们无法推责</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651195171&amp;idx=1&amp;sn=bcb90de3a47c2f039fe2f3e7fcb32fd3&amp;chksm=bdbbff708acc76668d312c8a677684b66ba445fb7b6e6c8df7a39b11a33cbfd321659859b706&amp;scene=21#wechat_redirect">Linus 开喷谷歌内核贡献者：你的代码是垃圾！网友：我们熟悉的 Linus 回来了</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651195004&amp;idx=1&amp;sn=f33a94aaf70b84ff0324a029a21dd107&amp;chksm=bdbbfe2f8acc77394df427365c0a148345bc746d8eb693d5e436e3d167c30a7595211b498084&amp;scene=21#wechat_redirect">今年技术除了AIGC真没啥看头？别让“网红效应”遮住了真正的创新！</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651194963&amp;idx=1&amp;sn=c624ecdd24f4da984b4e55291334239f&amp;chksm=bdbbfe008acc77163ea2ddd3d682cbc35a7eb4b5eb7a28398783ae99b44ef6f545465b50d512&amp;scene=21#wechat_redirect">比VS Code快得多！用Rust重写，支持OpenAI、Copilot 的Zed编辑器开源了</a>"</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/93/93a6e30a75e8e663d639c54513765ef5" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/G9PE7RqNE2zkAPxVQcSa</id>
            <title>Vision Pro全球首拆来了！“真是让人既费解又赞叹”</title>
            <link>https://www.infoq.cn/article/G9PE7RqNE2zkAPxVQcSa</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/G9PE7RqNE2zkAPxVQcSa</guid>
            <pubDate></pubDate>
            <updated>Wed, 07 Feb 2024 09:48:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 苹果公司, Vision Pro, 头戴式显示设备, 价格
<br>
<br>
总结: 苹果公司推出了首款头戴式显示设备Vision Pro，售价高昂，但包含了大量的技术和创新，是一款充满科技感的产品。 </div>
                        <hr>
                    
                    <p>2 月 2 日，苹果公司首款头戴式显示设备 Vision Pro 正式在美国开售，256GB、512GB、1TB 三种存储容量的售价分别为 3499 美元、3699 美元和 3899 美元，最高配置版本加上所有可选额外配置，价格高达 4795.95 美元。</p><p></p><p>对于如此高的价格，很多网友直呼“买不起”。对此，苹果 CEO 库克在财报电话会议上回应道，该公司的 MR 头显 Vision Pro 价格可能很高，但它包含了大量的技术。当被问及成本时，库克说这款头显的技术包含 5000 项专利。“如果你从价格的角度来看，这款产品包含了大量的技术。当然，这是建立在苹果花了多年时间进行的许多创新之上的，所有这些都是由 AI 驱动的。”</p><p></p><p>那么，这款“全是科技”的Vision Pro 到底是怎样构成的？近日，全球专业拆解大户 iFixit 发布了全球首个苹果 Vision Pro 拆解视频。</p><p></p><p></p><p></p><p></p><p>苹果家的产品向来难以拆解分析，这次也不例外。iFixit 表示，在实验中并没有破坏Vision Pro本体，并一路看到苹果如何将EyeSight显示屏、传感器阵列、外部电池组乃至R1芯片塞进这部精巧的设备。而且为了实验一次成功，iFixit 还专门为其拍摄了X射线视图和高分辨率显微镜照片。本文将从外屏显示说起，后续iFixit 还会出一篇文章着力探讨镜头系统与芯片设计。</p><p></p><p>下面，我们看看Vision Pro内有哪些乾坤。</p><p></p><h2>开拆！</h2><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/71/711be7f8d2fab29571a96caa4794b8e6.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/49/49b1faafbda4506e44c1573bab14dd0e.png" /></p><p></p><p>外面这块玻璃面板是用胶水粘牢的，所以需要持续加热、慢慢剥下。但最终iFixit 还是成功拆解，没有造成破损。唯一的影响，就是玻璃上那层塑料保护膜有点剥落和融化，但基本无伤大雅。经过专业训练的苹果零售维修人员可能会做得更好，不过据说更换破碎的前罩玻璃要收费799美元……iFixit 决定自己动手。</p><p><img src="https://static001.geekbang.org/infoq/9f/9fd74e10158f0d10e1f40b3d6f94b585.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/7b/7b277c4affdabb145ea7dc9d4ff2e13b.png" /></p><p></p><h2>颇有份量</h2><p></p><p></p><p>玻璃外罩本身只重34克，并不夸张。但在配备电池之后，Vision Pro的整体重量已经超过一公斤。</p><p></p><p>苹果在宣传中其实是耍了个花招。大多数宣传照片中都精心把外接电池隐藏了起来，将其放在口袋里、而非头显上。跟其他早期VR一样，直接把电池集成进去会导致设备重量爆表。</p><p></p><p>但这么设计也有好处，毕竟电池经过两、三年的充电之后肯定会持续老化，那这种模块化的外接设计能降低更换难度。苹果硬件团队可能也是考虑到了即将出台的欧盟电池法规，其中要求到2027年所有电子产品都必须配备可由用户亲自更换电池的设计。</p><p></p><p><img src="https://static001.geekbang.org/infoq/12/128a9fa97433e026cca46bfa58a847ff.png" /></p><p>纯电池组就重353克，由三块iPhone大小的电池组成，共可提供35.9瓦时电量，相当于iPhone 15 Pro 17.3瓦时的两倍多。但三块电池本身只重184克，约为整个电池组重量的一半左右。为了搞清楚余下的份量来自哪里，iFixit 得用热风把粘合剂吹软、扒开一组一次性金属卡扣，之后拧开一大堆梅花螺丝。</p><p></p><p><img src="https://static001.geekbang.org/infoq/92/9223e93ec64d881c05a8502ef1219b54.png" /></p><p>铝制电池组中的三块电池，每块串联电压约为3.8伏，单块3166 mAh，三块总电压11.34伏。</p><p>之前提到，电池组加头显的总重量已经超过了一公斤，Vision Pro的份量着实不轻。相比之下，Quest Pro的重量为722克，Quest 3的重量则为515克。</p><p></p><p>但重量并不是最大的问题，配重才是。Vision Pro的重量主要都落在用户脸上，因为所有核心组件都在前面，哪怕是Pro的双扣带设计也无法在不加配重的情况下解决问题。为此苹果公司专门申请了一项后置电池组设计专利，这可能有助于平衡沉重的正面份量——而代价就是把头上的总负担再提升50%。</p><p></p><p>所以如果只计算面部承受的重量，那么Meta Quest Pro的显示模块（不含电池）为522克，Vision Pro中同类组件的重量则为532克，基本相同。二者的主要区别一是重量分配，二是Vision Pro采取了更重的随附电池。</p><p><img src="https://static001.geekbang.org/infoq/da/da1168c2532063ed3af7fae6a6d7b9b3.png" /></p><p></p><p>但这款产品给人的第一印象相当好。iFixit拆解老手Sam Goldheart表示，“重量并没有想象中那么夸张，只是大部分重量落在了我的额头和脸颊上、而非头顶，这种感觉很奇怪，就像有人在朝斜下方按我的脑袋。”</p><p></p><h2>绑带设计</h2><p></p><p></p><p>Vision Pro配有3D单圈织物绑带和双扣绑带，它们接入扬声器后部位置。目前出于美观考虑，所有宣传照片展示的都是更有标志性的单圈织物绑带，看起来的确很酷。它环绕在用户的手脑勺上，我们可以使用侧面旋钮来调整贴合度，体验跟拧紧自行车头盔差不多。</p><p></p><p>那佩戴感觉如何呢？Sam表示“织物面料非常舒适。”单圈织物带采用非常精细、舒适的编程方式，而且弹性十足，容纳马尾辫也没问题，能给面部设备以充分的支撑。</p><p><img src="https://static001.geekbang.org/infoq/6d/6d9f36759cd50c0719904ede3d404431.png" /></p><p></p><p>扬声器则固定在连接主头显的两条硬镜腿上，要把它拆掉需要请出老朋友：SIM卡拆除针。拆除孔位于主头显的镜腿内侧，而且端口上带有一排电触点，看着就像当初的Lighting接口。使用既有工具轻松拆卸部件，这个主意iFixit 真的喜欢。这也让iFixit 意识到，Vision Pro的整个拆解过程可能并不像之前想象的那么困难。</p><p></p><p>这种模块化设计跟AirPods Max类似。可穿戴设备其实极易损坏，所以一定要采用能够轻松更换的扬声器模块。iFixit 还试图更进一步，把扬声单元从框架里撬出来，但这立刻就破坏了内部的模制线缆。好吧，这下大家知道了，就只能拆到这一步。</p><p><img src="https://static001.geekbang.org/infoq/2e/2e9fb529bcb9870db37551dc69dc956d.png" /></p><p>这款扬声器虽然拆解起来比AirPods Pro容易一点，但也没好多少。</p><p></p><p>扬声器本体只是虚掩在用户耳朵上，所以很明显它们并不适合非常吵闹的使用环境。如果大家愿意，也可以佩戴AirPods Pro——但若想享受无损、低延迟音频体验，记得购买最新的USB-C版本。</p><p></p><p>左侧是专用的电池线缆连接，用磁铁卡入到位，再拧一下即可锁定。我们可以理解苹果为什么在这里使用了非标准连接器，这显然是为了防止不慎被儿童拉掉或者是起身时线卡在椅子扶手上。但线缆的另一端设计则完全没道理可讲——苹果没有使用USB-C进行连接，而是选择某种看似专有的超大Lightning端口接入电池组。大家必须使用回形针或者SIM卡移除针才能将其解锁。</p><p><img src="https://static001.geekbang.org/infoq/5c/5cd1227b0b2f13392ca9119fb8bb5595.png" /></p><p>为什么不在电池组上直接用USB-C端口？为什么？</p><p></p><p>这样的设计，让大家没办法用带USB-C端口的充电宝给Vision Pro供电……太狗了。</p><p><img src="https://static001.geekbang.org/infoq/85/85ab785ecf6dd9161a55a93b895819b6.png" /></p><p></p><p>不是一家“头”，不进一家门！</p><p></p><h2>遮光件和面部护垫</h2><p></p><p></p><p>大家脸型各不相同，所以苹果也推出了多达28种不同的遮光件，用以适应不同的面部尺寸和形状。如果需要蔡司眼镜插片，遮光件的尺寸又会随之变化，这是为了保证遮光件和护垫能让立体屏幕与眼部传感器同用户的眼睛保持正确位置。正因为如此，苹果才会根据每份Vision Pro订单进行定制生产，毕竟面部适配不存在“标准”配置。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3d/3dfabacd3de387c484620098d2f30152.png" /></p><p>用之前请先把脸上的粉搽干净～</p><p></p><p>遮光件用磁铁固定在主头显上，这也是苹果的一贯设计——要么固定在适当位置，要么就得特别容易更换。这种模块化方案只为完美贴合用户的脸部线条。不知道未来的设计也会这样，还是说这只是暂时不得不选择的妥协产物。但至少目前来看，磁铁比尼龙带加卡扣更舒服，可以让遮光件精确卡入并对齐。整个设计类似于MagSafe充电器与iPhone感应充电线圈的完美匹配。</p><p></p><p>关于遮光件的清洁，苹果建议使用水加无味洗洁精，这样就能防止部件因为经常被汗水浸湿而变脏。这里特别提醒各位精致女孩，你们的妆容很容易在Vision Pro上留下痕迹。《华尔街日报》的Joanna Stern就一口气把Vision Pro佩戴了24个小时，据说她的化妆品都在遮光件里打团了。</p><p></p><p>磁性遮光件下面的则是固定遮光件，同样采用织物包裹，但防脏能力相对更好。把它取下之后可以看到一张薄薄的弹性塑料片——不确定这是用来填补织物中的间隙，还是为了防止颗粒物进入头显内部。总而言之，这是一副颇有超级英雄风格的眼罩。</p><p><img src="https://static001.geekbang.org/infoq/c7/c77877a49337c96ecf54253be590adb7.png" /></p><p></p><h2>EyeSight显示屏</h2><p></p><p></p><p>前置外屏既是Vision Pro的标志性设计，也是目前外界争论的焦点。</p><p></p><p>技术媒体普遍用“怪异”、“不可理喻”和“实用性存疑”等字眼形容EyeSight。而且从设备耐用度出发，这样的设计也堪称致命弱点。为什么非要引入单独一块屏幕和由它带来的更多连接装置与故障点呢？这一切就是为了实现诡异、无用甚至令人有点毛骨悚然的功能？要找到答案，先得深入研究并搞清楚EyeSight的工作原理。</p><p></p><p>EyeSight的专利描述中包含三种显示模式：“专注内部”、“外界参与”和“请勿打扰”。专利提到该技术能够在屏幕上显示多种图像——包括各种卡通风格的动物眼睛、其他传感器捕捉的生物特征分析结果，还有可在用户与亲朋好友闲谈时显示为心形。内部摄像头用于捕捉情绪状态，并根据结果投射对应图像。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ae/ae38e4be42bdac0f04b4a1b78332a842.png" /></p><p></p><p>想法确实不错，但实际EyeSight的外屏非常昏暗且分辨率低下，评论人士称很难从中看到太多内容。《华尔街日报》的Joanna Stern甚至表示“基本看不清”。</p><p></p><p>事实证明，EyeSight的实际效果其实是双层画面的叠加：眼睛动画和真实用户的眼部。通过拆解，iFixit 发现前置外屏采用了三层结构：加宽层、透镜层和OLED屏幕层。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d3/d3e8542e328b99869fa974051dc0f698.png" /></p><p>EyeSight为什么看起来这么怪？</p><p><img src="https://static001.geekbang.org/infoq/9d/9dfacf4b8b15e7ed9a1adcc803ce5a39.png" /></p><p>苹果的想法是很具体的：给用户的面部配上眼睛动画效果。但为了实现这个目标，他们不仅做出了极具战略性的设计选择、还面临着现实条件的严苛约束。</p><p></p><p>因为人类大脑对面部和眼睛非常敏感，所以才会产生所谓“恐怖谷效应”。换句话说，苹果必须创造出非常真实可信的3D效果。而如今EyeSight上的3D渲染之所以效果不好，就是因为它们缺少立体感。为了让目标看起来有空间纵深，必须保证观察者每只眼睛看到的图像略有不同。Vision Pro在内屏上就是用双凸透镜来解决这个问题。</p><p></p><p>当从不同角度观看时，双凸镜会分别显示不同的图像，并借助这种效果来模拟同一动作的两个独立帧。当然，开发者也可以使用同一对象的不同角度图像建立同样的3D效果。</p><p></p><p>Vision Pro的外屏OLED面板上方装一个光栅层。VisionOS会渲染多个面部图像（我们称之为A和B），将二者拆分并分别对应左眼观察角度和右眼观察角度。如此一来，就实现了具有立体效果的3D人脸。但由于角度差异较小且素材数量众多，所以还得搭配上Evident Scientific显微镜才能看到其中细节。</p><p></p><p><img src="https://static001.geekbang.org/infoq/99/995428fea47c3e9b730970546734b207.png" /></p><p>双凸透镜层上的一个个镜脊</p><p></p><p><img src="https://static001.geekbang.org/infoq/3d/3dbb5a9c3f7c924bd4e56c8195b05bc9.png" /></p><p>像素也被弯曲并透过透镜层发光</p><p></p><p>这种方法的弊端在于，其横向分辨率会显著降低，且各个图像会被进一步拆分。假定两帧画面要显示在2000个像素宽的显示屏上，那么每个画面都只有1000个横向像素点可供使用。也就是说，不管在什么样的显示屏上显示怎样的画面，其分辨率都必然受到影响。正因为如此，EyeSight眼睛看起来才那么模糊。</p><p></p><p>透镜层前面是另一个塑料透镜层，也拥有类似的透镜脊。这个层的作用似乎是把投影面拉伸到适合Vision Pro的宽度。把它拆掉再启动Pro，外屏显示的眼睛就会扭曲缩小。</p><p></p><p>此外，这些透镜还可能会限制有效视角。目前整个设计只在旁观者位于Vision Pro正前方时效果较好，这就限制了在极端角度下看到伪像的可能性。但缺点就是需要让本就复杂且模糊的图像再经过一道镜头，导致画面更暗更糊。</p><p></p><p></p><h2>插入镜片和立体显示屏</h2><p></p><p></p><p>通过下面这张X射线图，可以一窥Vision Pro的内部结构。请大家认真观看，这东西可是3500美元买来的。</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/81/81e4181d2202ab393307c1dcf7aec32c.png" /></p><p></p><p>在首次佩戴时，Vision Pro会自动进行瞳距调整，并通过电机变更镜片位置。此外，苹果还提供多种镜片选项。</p><p></p><p>各家苹果门店都将部署设备，供用户在参加演示时确定镜片设置。对于患有可能干扰眼球追踪功能的眼部疾病（例如斜视）的用户，Vision Pro可在辅助功能中提供替代交互控制。但也听说这款产品对散光患者（占整个人群的40%）效果不佳，iFixit 表示对此后续会进一步验证。</p><p></p><p>更换的插入镜片还需要与头显进行“配对”，但匹配偏差带来了灾难性的后果。iFixit 另外工作人员就在配对之后获得了错误结果，导致眼动追踪效果不佳。所以iFixit 强烈建议苹果提供一种对第三方镜片进行校准的方案。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/5a/3c/5a84bf76070cd47f4d960a4a8bfaec3c.gif" /></p><p></p><p></p><h2>R1与M2芯片</h2><p></p><p></p><p>这款头显搭载M2 Mac芯片，同时辅以全新R1芯片，后者专门负责来自12个摄像头、LiDAR传感器和TrueDepth摄像头的输入，将其延迟控制在最低水平。在增强现实场景下，产品必须尽快将现实世界的摄像头视力投射至用户眼中，否则他们的肢体感知会与画面内容相冲突，导致大脑陷入混乱。</p><p></p><p>为了避免失衡，R1芯片采用实时操作系统，保证任务永远在固定时间内执行。大多数计算机都运行在分时操作系统之上，即由系统动态安排任务，但这可能导致操作响应变慢。这样的设计对于透传视频和对象渲染之类的重要任务显然不适用，过程中的任何故障都会像《黑客帝国》中的情节那样产生违和感。轻则令人不适，重则导致头晕呕吐，甚至直接昏厥当场。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bf/bf330df69c7d86a6d5dfa37cb610fe9e.png" /></p><p></p><p></p><h2>既是值得赞叹的壮举，也是令人费解的设计</h2><p></p><p></p><p>初代iPhone其实也有类似的问题。当算力不足的芯片跟不上快速滚动的页面渲染需求时，它会暂时切换为灰白色棋盘，等待滑动操作处理完成。当时，苹果决定先考虑响应能力、再实现图形保真。但这一次，苹果把图形保真和响应能力都放在最高优先级，并用电池续航、重量和发热量作为代价。考虑到顺畅体验对于AR技术的重要意义，这可能也是初代Vision Pro唯一可行的路径。</p><p></p><p>Vision Pro是款雄心勃勃的产品。没错，它很重、玻璃护罩容易损坏、外接电池也不够便携。但苹果至少成功把Mac的强大功能和新型专用AR芯片整合到了能够佩戴在头上的单一设备内。</p><p>虽然修复起来比较麻烦，但苹果明显已经尽可能在采用模块化连接。最直观的例子，当数靠SIM卡针就能拆下的镜腿和非常人性化的磁吸护垫。</p><p></p><p>那么，作为苹果对未来计算形态的最新赌注和耗时多年的心血力作，为什么EyeSight外屏的表现却这么拉胯呢？</p><p></p><p>它很暗、分辨率不高，而且平白给头显增加了可观的体积、重量、复杂性和成本。是不是开发团队耗尽了时间还是没找到合适的方案，所以只能被迫推出这样一款自己也不满意的成品？不知道，但确实有这种可能。</p><p></p><p>从初代Oculus开始，iFixit 就一直在拆解各种VR头显，并看着它们一路发展改进。其中包含着这么多令人着迷的机械与光学设计，苹果Vision Pro则把传感器的无缝集成和惊人的位置跟踪能力提升到了新的高度。“无论还有多少缺点，都不能掩盖这款新品那耀眼的光辉。”iFixit 表示。</p><p></p><p>原文链接：</p><p></p><p><a href="https://www.ifixit.com/News/90137/vision-pro-teardown-why-those-fake-eyes-look-so-weird">https://www.ifixit.com/News/90137/vision-pro-teardown-why-those-fake-eyes-look-so-weird</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/GyDTEqAeUSG9YPGMQIow</id>
            <title>10X增强！ChatGPT退位，Bard登基</title>
            <link>https://www.infoq.cn/article/GyDTEqAeUSG9YPGMQIow</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/GyDTEqAeUSG9YPGMQIow</guid>
            <pubDate></pubDate>
            <updated>Tue, 06 Feb 2024 06:16:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Bard, 功能增强, ChatGPT, Gemini
<br>
<br>
总结: 谷歌的Bard经过功能增强，成为了功能更强大的聊天机器人，被称为ChatGPT的竞品。Bard可以实现访问互联网、图像生成、图片分析、多个备选结果、得力助手等多种用例。此外，Bard还与Gemini Pro合并，支持全球230个国家/地区的40多种语言。谷歌计划将Bard更名为Gemini，并推出Gemini Advanced付费项目，提供更强大的AI模型Ultra 1.0。Gemini应用与谷歌其他应用相集成，用户可以通过文本、语音或图像与Gemini进行交互。 </div>
                        <hr>
                    
                    <p></p><h2>Bard功能增强10倍</h2><p></p><p>&nbsp;</p><p>2月5日，Priyank Ahuja在X（原Twitter）上发帖称，谷歌Bard迎来重大升级，功能增强10倍，并表示“告别 ChatGPT，向Bard问好”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/93/930541160f35bd887d48986fb5d59b79.png" /></p><p></p><p>Priyank Ahuja 拥有多重身份，是内容创作者、主题演讲者、职业教练、导师、企业培训师和商学院客座教授。同时他也是一位行业专家，在产品管理、管理咨询、营销策略、数字化转型、售前和运营等多个领域的项目中拥有 16 年的经验。</p><p>&nbsp;</p><p>Priyank Ahuja 还在帖子中分享了新版Bard能够实现的十大用例：</p><p>&nbsp;</p><p>访问互联网访问：如果你需要关于特定主题的最新信息和持续更新，在聊天中直接向Bard询问即可。图像生成：在Bard对话中，用户可以直接获取图像输出。图片分析：配合Google Lens，Bard可以快速分析图片，并在短短几秒钟给出多条极富想象力的标题。多个备选结果：哪怕面对相同或相似的提示词和问题，Bard也能生成多种多样的响应结果。得力助手：AI驱动的个人助手能够将Bard的逻辑/生成能力同助手的定制化建议相结合。用户可通过文本、语音或图像进行交互。这项功能即将对Android和iOS移动设备开放。Gemini pro现已与Bard合并：Bard的功能由此得到增强，包括更好的理解力、改进总结质量、高级逻辑推理、提升编码水平、能够从容应用各种复杂的项目与日常工作。面向全球：谷歌打造的Bard现已在230个国家/地区开放支持40多种语言的Gemini Pro。文本到图像：Bard的最新功能可通过图像生成技术，将提交的文本转换为逼真图像。Bard插件：立即访问各种领先应用，包括 Instacart（日用百货）、Spotify（音乐）、沃尔玛（零售）、Indeed（找工作） 以及其他谷歌应用。编码：编写或阐释代码内容，甚至可对代码进行故障排查。</p><p>&nbsp;</p><p>虽然目前谷歌方面还未正式宣布升级信息，但根据近日X上流传的一份Bard 变更日志，谷歌确实对Bard进行了一系列调整，该调整信息将于2月7日正式发布。根据开发者 Dylan Roussel的爆料，谷歌甚至还计划将Bard更名为“Gemini”——与其最强大的人工智能模型系列使用的名称相同。</p><p>&nbsp;</p><p>日志对改名进行了解释：</p><p>&nbsp;</p><p></p><blockquote>Gemini现已成为直接访问Google AI的最佳方式。大家熟悉和喜爱的所有协作功能仍在，并将在Gemini时代下变得越来越好。我们还改进了用户界面，希望减少视觉干扰、提高易读性并简化导航体验。我们致力于让每个人都能直接访问Google AI。从本周开始，我们支持的各国家/地区及语种的每一位Gemini用户都可以访问到谷歌旗下的最佳AI模型。为了更好地体现这一承诺，我们决定将Bard更名为Gemini。</blockquote><p></p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/87/87167df88b6c026df4004aa8654c0454.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/7b/7ba307d11b6f87fe80c2b21b09b2b08f.jpeg" /></p><p></p><p>日志中还提到，通过Gemini Advanced，可以访问谷歌最强大的AI模型Ultra 1.0。借助Ultra 1.0模型，Gemini Advanced能够高效执行极端复杂的任务，例如编码逻辑推理、遵循细致入微的指令与创造性协作。此外，Gemini Advanced将在未来几个月继续扩展新的专有功能，包括经过扩展的多模态功能、更好的编码功能，以及上传并深入分析文件、文档、数据的能力。</p><p>&nbsp;</p><p>Gemini Advanced属于付费项目，可在150多个国家和地区使用。谷歌的Ultra 1.0模型仅提供英语版本并做针对性优化，但亦可响应Gemini所支持的其他语种查询。谷歌表示，旅程刚刚开始，Gemini Advanced将不断发展完善，并邀请广大用户升级至Gemini Advanced。</p><p>&nbsp;</p><p>用户还可以在Gemini应用中用对话启发灵感。使用手机端Google AI可以帮助用户以新的方式学习、撰写感谢信、规划活动等。Gemini与Gmail、谷歌地图、YouTube等谷歌应用相集成，将使用户能够在手机端轻松完成工作，用户也可以通过文本、语音或图像进行交互。可以在Google Play应用商店中下载Gemini应用，在Android上与Gemini对话；在iOS端，可以在Google应用中体验Gemini。</p><p>&nbsp;</p><p>目前，Gemini应用已在加拿大上线，用户可以在加拿大通过所有受支持语言（包括英语和法语）与Gemini进行Web对话。Gemini应用的英文版本即将推出。此外，Gemini应用将在美国的部分英语设备上率先开放。未来几天，除英国、瑞士、欧洲经济区国家及相关地区外，Gemini应用将在全球范围内发布并提供日语、韩语和英语版本。更多国家和语言版本将后续推出。</p><p></p><h2>“硬刚”ChatGPT，Bard能“后来者居上”吗？</h2><p></p><p>&nbsp;</p><p>ChatGPT的爆火让Open AI和微软出尽风头，面对二者的步步紧逼，谷歌也在去年推出了自家聊天机器人Bard。据悉，谷歌对这款 ChatGPT 竞品寄予厚望。谷歌希望借助 Bard巩固其在 AI 聊天机器人领域的地位，同时保持其在搜索引擎市场的主导地位。</p><p>&nbsp;</p><p>Bard 由研究型大型语言模型 (LLM) 提供支持—— LaMDA 的轻量级优化版本。LaMDA 建立在 Transformer 上，由这个架构生成的模型可以训练阅读许多单词（例如，一个句子或段落），注意这些单词之间的关系，然后预测它认为接下来会出现什么单词。但与其他大多数语言模型不同的是，LaMDA 接受的是对话训练。在训练过程中，它发现一些区别于其他语言形式的开放式对话的细微差异。LaMDA 现在被用在 Bard 上为用户创建答案。</p><p>&nbsp;</p><p>谷歌表示， LaMDA 的轻量级模型版本，意味着 LaMDA 需要“显著更少的计算能力”，这将使 LaMDA 能够扩展到更多用户，同时也允许获得更多反馈。</p><p>&nbsp;</p><p>“后来者”总是比“先行者”接受更多的审判，Bard也是如此。</p><p>&nbsp;</p><p>发布之初，就有爆料称“Bard 是用 ChatGPT 的数据训练的”。据 The Information 报道，一位从谷歌离职现已跳槽到 OpenAI 的顶级研究员 Jacob Devlin 爆料称，Bard 的机器学习模型使用 ChatGPT 进行训练。对于以上指控，谷歌发言人 Chris Pappas 在接受科技网站 The Verge 采访时表示，“Bard 没有接受过来自 ShareGPT 或 ChatGPT 的任何数据的培训。”</p><p>&nbsp;</p><p>Bard 还曾遭到谷歌内部员工的吐槽。谷歌曾要求内部员工测试Bard，根据内部讨论的截图，一名员工的结论是，Bard是个“病态的骗子”。另一名员工则表示，Bard“令人尴尬”。一名员工写道，当他们向Bard询问如何降落飞机时，聊天机器人往往给出会导致坠机的建议。还有一名员工表示，Bard提供的关于潜水的建议“可能会导致重伤或死亡”。</p><p>&nbsp;</p><p>谷歌母公司Alphabet也曾邀请重度使用者，持续对旗下AI聊天机器人Bard的研发进行内部讨论，一些谷歌产品经理、设计师和工程师对AI有效性和功能提出质疑，甚至质疑公司投入海量资源研发AI是否值得。</p><p>&nbsp;</p><p>在种种质疑声中，谷歌还是坚持持续更新Bard，甚至将确保Bard成功当作该公司的头号任务——对于“搜索霸主”而言，AIGC带来的冲击不容轻视，毕竟ChatGPT刚问世时，顶着的是“取代搜索”名号。至于未来能否超越ChatGPT，我们一起拭目以待。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://twitter.com/ahuja_priyank/status/1754379798325985661">https://twitter.com/ahuja_priyank/status/1754379798325985661</a>"</p><p><a href="https://twitter.com/evowizz/status/1753795479543132248?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1753795479543132248%7Ctwgr%5E3417705a555a087adbc511897b5737d391cbb224%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2F9to5google.com%2F2024%2F02%2F03%2Fgoogle-bard-gemini-rebrand-android-app-date%2F">https://twitter.com/evowizz/status/1753795479543132248?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1753795479543132248%7Ctwgr%5E3417705a555a087adbc511897b5737d391cbb224%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2F9to5google.com%2F2024%2F02%2F03%2Fgoogle-bard-gemini-rebrand-android-app-date%2F</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/23jBsNwZ0aMV8qUUshtI</id>
            <title>腾讯年终奖大放送：5个月薪起；裁员见效？Meta市值一夜涨近1.4万亿；Vision Pro 开售买不起？库克回应 | AI周报</title>
            <link>https://www.infoq.cn/article/23jBsNwZ0aMV8qUUshtI</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/23jBsNwZ0aMV8qUUshtI</guid>
            <pubDate></pubDate>
            <updated>Tue, 06 Feb 2024 04:00:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 腾讯, 年终奖, Meta, 亚马逊
<br>
<br>
总结: 腾讯年终奖最高达到5个月月薪+，Meta和亚马逊业绩大涨，裁员和成本控制成为赢家。字节跳动CEO梁汝波警示“大公司病”，强调年度目标“加强危机感”。微软、支付巨头PayPal、中兴通讯被爆裁员。第三方开源社区“Linux中国”宣布停止运营。 </div>
                        <hr>
                    
                    <p></p><blockquote>腾讯年终奖最高 5 个月月薪 +，互联网公司年终奖安排曝光；Meta 和亚马逊业绩大涨，裁员和成本控制成为赢家；字节跳动 CEO 梁汝波警示“大公司病”，强调年度目标“加强危机感”；微软、支付巨头 PayPal、中兴通讯被爆裁员；第三方开源社区“Linux 中国” 官宣停止运营，主网、公众号、视频号将停更；12306 回应同日同座火车票问题，第三方购票平台现“撞票”事件；苹果 Vision Pro 头显美国开售，钉钉宣布正式登陆 Vision Pro……&nbsp;&nbsp;</blockquote><p></p><p></p><p></p><h2>热门资讯</h2><p></p><p></p><h4>腾讯年终奖最高 5 个月月薪 +，互联网公司年终奖安排曝光</h4><p></p><p>1 月 31 日消息，据媒体报道，腾讯今年年终奖绩效评定已经出结果，最高的可达 5 个月月薪 +。腾讯离职员工表示，腾讯现在都是三档绩效：Outstanding（突出）、Good（良好）、Underperform（欠佳），具体几个月年终奖看部门，部门之间也是天差地别。其中三档绩效分别对应：5 个月月薪 +、中间（绝大多数人 4 个月月薪，可能略微上浮）、最低档 1-2 月月薪。</p><p></p><p>网友爆料的腾讯头牌部门年终奖：</p><p></p><p>WXG（微信事业群）:</p><p></p><p>第一档：30 个月（视频号 - 优秀）</p><p></p><p>第二档：20 个月（视频号 - 一般）</p><p></p><p>第三档：16 个月（WXG - 一般）</p><p></p><p>其它：不足 10 个月（WXG - 垫底水平）</p><p></p><p>IEG（互动娱乐事业群：天美工作室 and 光子工作室 and 其他）:</p><p></p><p>第一档：25 个月（元梦之星、王者荣耀 - 优秀）</p><p></p><p>第二档：15 个月（元梦之星、王者荣耀 - 一般）</p><p></p><p>第三档：9 个月（天美工作室 - 优秀）</p><p></p><p>第四档：5 个月（光子工作室 - 一般）</p><p></p><p>其它：3 - 4 个月（IEG - 垫底水平）</p><p></p><p>报道表示，BAT（百度、阿里、腾讯）、TMD（头条字节、美团、滴滴）、京东等几家头部互联网公司中，腾讯依然选择年前发放年终奖；阿里、字节、百度、美团发放时间依然是次年的三四月份。京东 OS 序列（基层员工）年终奖农历年前（2 月 7 日）发放，MPT 序列（M 是指管理，P 是指项目经理，T 是指产品和技术）员工年后发放。此外，快手、B 站、携程三家公司同样也是选择在年前发放年终奖。</p><p></p><h4>Meta 和亚马逊业绩大涨，裁员和成本控制成为赢家</h4><p></p><p>Meta 和亚马逊公司周五股价飙升，创下历史新高，两家公司的股票市值合计增加了 3360 亿美元。</p><p></p><p>其中，随着社交媒体巨头 Meta 拿出了一份“史上最强”财报，公司股价单日暴涨超 20%，总市值达到 1.22 万亿美元，一日增近 2000 亿美元（约 1.4 万亿元人民币），创下美股史上最高单日市值涨幅。此外，Meta 宣布了额外的 500 亿美元股票回购和首次派发季度股息的计划，给投资者提供了继续持有该股的理由。自 2022 年 11 月触底以来，Meta 的股价至今已飙升超过 300%，涨幅超过了标普 500 指数中除了英伟达以外的所有股票。</p><p></p><p>亚马逊则在线销售创下疫情初期以来最强劲的增长，得益于运输时长缩短。投资者对大型科技股的乐观态度以及降息预期将继续推动科技股的涨势。两家公司此前公布的季度业绩和展望远超华尔街预期。外媒分析，两家科技巨头在科技行业中脱颖而出，原因是过去 16 个月两家公司已裁员数万人和假日期间核心业务表现良好。</p><p></p><h4>字节跳动 CEO 梁汝波警示“大公司病”，强调年度目标“加强危机感”</h4><p></p><p>字节跳动在 2024 年度全员大会上，CEO 梁汝波发表了内部讲话，直指公司面临的挑战。他提出，字节跳动在经历了 2019 至 2021 年的快速扩张后，员工人数从 1 万多增至 10 万，但随之而来的是组织效率的下降和对市场变化的迟钝。梁汝波将这种状态比喻为“大公司病”，并将其作为年度目标之一，强调必须“加强危机感”。</p><p></p><p>梁汝波在讲话中提到，字节跳动需要从精神上打破自满，提高标准，保持始终创业的心态。他举例说明，公司内部的一个简单需求，原本评估需要 1000 人天，即 1 个人 1000 天的工作量，实际上只需 1 个人 15 天就能完成。这种低效现象在公司内部并不罕见。此外，他对公司对新技术的跟进速度表示担忧，指出公司在 2023 年才开始讨论 GPT，而行业内其他公司早在 2018 年至 2021 年就开始布局。</p><p></p><p>梁汝波强调，字节跳动需要建设一个精干的组织，以提高效率和响应速度。他提到，公司内部的文档沟通存在问题，文档冗长且充满行业术语，这影响了沟通效率。他坦言，公司效率可能比其他优秀团队低 30% 甚至 50%，这种自我标准下降的现象令人担忧。</p><p></p><h4>微软、支付巨头 PayPal、中兴通讯被爆裁员</h4><p></p><p>微软曾在上周宣布在动视暴雪和 Xbox 部门裁员 1900 人，1 月 31 日，专注于报道《使命召唤》相关消息的外媒 The Rotation 披露了有关这次裁员的更多细节。The Rotation 声称，动视暴雪已经解雇了电竞部门的 60 名员工，目前员工总数仅有 12 人。</p><p></p><p>同时，支付巨头 PayPal 宣布全球裁员 9%，预估 2500 人受到影响。PayPal 首席执行官亚历克斯∙克里斯发布全员信，宣布本次裁员涉及现有员工，以及原计划招聘的职位列表。他表示，这一决定是为了通过直接裁员和全年取消空缺职位的方式，来“调整”公司的规模。被裁员工将在周末收到通知，Paypal 将提供 N+6 的赔偿方案，即按照员工的工龄和月薪，给予其 N 个月的工资加上 6 个月的工资作为补偿。</p><p></p><p>另外，近日脉脉等社交媒体上有员工消息称，中兴通讯正在进行规模型裁员的人员优化，但截至目前，中兴并未对此事进行明确回应。据中兴往年财报数据显示，截至 2023 年 6 月 30 日，中兴通讯员工总数为 72736 人，而在 2022 年底，中兴通讯员工还有 74811 人。仅半年的时间，员工数减少了 2000 多人。同时，也有接近中兴的人士透露，最近优化的员工比例已经接近了 15%，远远超过了正常的企业人员变动范畴。</p><p></p><h4>第三方开源社区“Linux 中国” 官宣停止运营，主网、公众号、视频号将停更</h4><p></p><p>2 月 1 日中午，开源社区“Linux 中国”官方公众号发文宣布，即日起，该社区主网、公众号、视频号及下属的《硬核观察》栏目将无限期停止更新、运营。随着计算机翻译技术的进步，尤其是 ChatGPT 的出现，导致翻译工作的必要性大为降低，翻译团队 LCTT 也陷入“半死亡”状态。</p><p></p><p>“Linux 中国”创始人硬核老王在这封“告别信”中透露了社区停止运营的原因：起初希望把“Linux 中国”建设成一个传播开源技术的公益型社区，在经过十余年发展后，开源文化和 Linux 相关的开源技术已经得到了广泛传播，“继续运营所能起到的作用并没有那么大了”。硬核老王坦言，“过去几年，我大致处于一种个人经济上的慢性失血状态。”</p><p></p><h4>12306 回应同日同座火车票问题，第三方购票平台现“撞票”事件</h4><p></p><p>近日，广东网友小张在 12306 官网购买的火车票在乘坐过程中遭遇“撞票”尴尬，与另一名乘客持有同日同车次同座位号的车票。该乘客通过第三方软件购票，票价比官方渠道高出 20 元。小张表示，由于行程短且有其他乘客让座，问题并未造成太大困扰。</p><p></p><p>重庆 12306 工作人员表示，官方出票不会出现重复，而第三方平台可能在信息处理上出现问题。广州 12306 工作人员建议，如遇此类情况，应及时与铁路部门或第三方平台沟通解决。智行客服人员透露，系统错误可能导致“重票”情况，但发生频率较低。重庆 12306 提醒，若通过非官方渠道购票出现问题，应联系相应客服处理。</p><p></p><h4>TikTok CEO 周受资被反复逼问国籍，反华议员被骂上热搜</h4><p></p><p>据外媒报道，在美国参议院司法委员会举行的最新一场听证会上，TikTok CEO 周受资被议员反复纠缠。这场听证会的主题本应是“网络儿童性剥削”，但议员的提问却离题万里。反华议员汤姆·科顿对出席作证的 TikTok CEO 周受资，一连抛出九个问题：是不是中国人，有没有申请过中国国籍等，甚至连周受资的妻子和孩子的国籍都被问候了个遍。</p><p></p><p>周受资在反复回答了八次之后，语气坚定地告诉他，“参议员先生，我是新加坡人。”据悉，该议员对周受资的反复刁难，引起了外网上的激烈讨论，一度登上了推特热榜。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e0/e05aef6ad5a572cc3f177b85d900dec9.png" /></p><p></p><h4>苹果 Vision Pro 头显美国开售，钉钉宣布正式登陆 Vision Pro</h4><p></p><p>2 月 2 日，苹果公司首款头戴式显示设备 Vision Pro 正式在美国开售，提供 256GB、512GB、1TB 三种存储容量供消费者选择。售价分别为 3499 美元、3699 美元和 3899 美元。最高配置版本加上所有可选额外配置，价格高达 4795.95 美元。</p><p></p><p>另外，苹果 CEO 库克在周四的财报电话会议上告诉分析师们，该公司的 MR 头显 Vision Pro 价格可能很高，但它包含了大量的技术。当被问及成本时，库克说这款头显的技术包含 5000 项专利。“如果你从价格的角度来看，这款产品包含了大量的技术。当然，这是建立在苹果花了多年时间进行的许多创新之上的，所有这些都是由 AI 驱动的。”</p><p></p><p>此外，钉钉已正式登陆 Apple Vision Pro，并推出 Vision Pro 原生应用。用户在美区 App Store 里即可下载体验钉钉 App，在沉浸式环境下使用钉钉聊天、会议、直播等功能。</p><p></p><p>值得注意的是，苹果在大中华区的营收出现了下滑。根据苹果公司公布的 2024 财年第 1 财季财报数据，大中华区该季度营收为 208 亿美元，同比减少 13%。苹果首席执行官蒂姆·库克表示，营收下滑并非因为 iPhone 人气流失，而是整体经济问题的反映。</p><p></p><h4>神秘韩国公司向中国走私 5.3 万颗禁售芯片，案值达 1160 万美元</h4><p></p><p>据韩国媒体报道，最近，韩国海关破获了一起史上最大规模芯片走私案，一家未具名的“公司 A”(Company A) 被指向中国走私了多达 5.3 万颗被美国禁售的芯片，案值达 1160 万美元 (约合人民币 8330 万元)。据称，从 2020 年 8 月到 2023 年 8 月期间，“公司 A”合法地陆续购买了这批芯片，并进口到韩国，一切行为都合规合法，只是数量远超其所需。</p><p></p><p>然后，这些芯片分为 144 批次，通过空运走私到中国，均未向海关申报。这些芯片的具体厂商、型号未披露，报道只是说它们是通讯类处理器，可用来制造大规模杀伤性武器。目前，公司 A 的所有高管都已经被起诉，可能会面临 3 年起步的刑期。</p><p></p><h4>美图公司借 AI 技术实现利润翻倍，年赚 3 亿</h4><p></p><p>美图公司通过 AI 技术实现了业务的显著增长。2023 年，美图公司宣布其经调整净利润预计在 3.3 亿至 3.7 亿元人民币之间，较 2022 年的 1.11 亿元增长约 200% 至 230%。这一显著增长得益于 AI 生成内容（AIGC）技术的广泛应用，尤其是美图设计室、开拍等 AI 产品的用户增长和会员转化率的提升。</p><p></p><p>美图公司通过 AI 技术为用户提供了多样化工具，如 AI 写真、AI 动漫、图生图等，这些功能不仅吸引了大量用户，也为公司带来了可观的收入。此外，美图还针对小 B 用户群体推出了 AI 口播视频工具“开拍”，以及 AI 图片、AI 视频、AI 设计等工具产品，进一步拓展了其在 B 端市场的应用场景。</p><p></p><h2>IT 业界</h2><p></p><p></p><h4>字节跳动推出 AI 聊天机器人“扣子”，人工智能 AIETF 开盘大涨超 1%</h4><p></p><p>近日，字节跳动公司推出了名为“扣子”的 AI 聊天机器人构建平台，这一创新产品不仅简化了 AI 模型的搭建过程，还允许用户将构建的问答 Bot 一键发布至多个社交平台。受此消息影响，人工智能 AIETF（515070）在今日早盘开盘后迅速上涨，盘中涨幅超过 1%。</p><p></p><p>更多内容可查看：</p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247602596&amp;idx=2&amp;sn=eb0b2f4b9636d45c5938d7110499e3b8&amp;chksm=fbebe86bcc9c617d96685f324ee7bb46c6715ee4750df6ca6cac457e294b28c86e056725deb8&amp;scene=21#wechat_redirect">刚上线就崩了？字节版 GPTs 征战国内市场：无需编码，快速创建 AI 聊天机器人</a>"</p><p></p><p>据每日经济新闻报道，截至 09:31，该 ETF 基金的涨幅达到了 1.03%，其中神州泰岳等多只个股领涨。资金流向显示，近两个月来，人工智能 AIETF 累计流入资金超过 3 亿元。估值方面，该 ETF 跟踪的中证人工智能主题指数市盈率（PE-TTM）仅为 35.18 倍，处于近一年来的低位，显示出市场对 AI 领域的长期看好。东吴证券分析指出，随着 AI 技术的成熟，2024 年 AI 领域的投资有望进入“兑现期”，预计会有更多领域的创新应用出现，为投资者带来新的增长机遇。</p><p></p><h4>Hugging Face 发布免费 AI 聊天助手，开源挑战 OpenAI 的商业模式</h4><p></p><p>纽约创业公司 Hugging Face 近日宣布推出 Hugging Chat，一款免费的可定制 AI 聊天助手，旨在与 OpenAI 的 ChatGPT 竞争。Hugging Chat 不仅免费，还允许用户通过简单步骤创建个性化的聊天机器人，这与 OpenAI 的收费服务形成鲜明对比。Hugging Face 的技术总监 Phillip Schmid 强调，用户可以选择多种开源大型语言模型（LLM）来驱动其聊天助手，如 Mistral 的 Mixtral 和 Meta 的 Llama 2，这体现了公司对 AI 全方位策略的承诺。</p><p></p><p>Hugging Face 的新举措被视为对 OpenAI GPT 商店的公开挑战，其界面设计与 GPT 商店相似，提供了一个集中的第三方定制助手仓库。尽管 Hugging Chat 在网络搜索和图像生成方面尚不支持，但其开源社区的支持和快速进步显示出开源技术在 AI 领域的强劲竞争力。Mistral 的新开源模型 Miqu 的推出，更是在 GPT-4 泄露后不久，显示了开源社区追赶行业标杆 OpenAI 的决心。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0a/0a131d3f72428c9f41e19539baeef5cd.jpeg" /></p><p></p><h4>谷歌最新人工智能模型 Gemini Pro 已在欧洲上市，将与 ChatGPT 竞争</h4><p></p><p>据报道，谷歌表示，Gemini Pro 是谷歌最大的人工智能 (AI) 模型之一，作为巴德（Bard）的升级版，现已向欧洲用户开放。该模型是一个多模态大模型，这意味着它可以理解和组合不同类型的信息，如文本、代码、音频、图像和视频。通过 Gemini，谷歌希望能与 OpenAI 的热门聊天机器人 ChatGPT 进行竞争。</p><p></p><h4>ChatGPT 被曝泄露用户敏感信息，OpenAI 展开调查</h4><p></p><p>近日，风靡全球的聊天机器人 ChatGPT 近日再次陷入安全风波，被曝泄露用户同机器人的私密对话，其中包含用户名、密码等敏感信息。ArsTechnica 网站援引其读者提供的截图报道称，ChatGPT 泄露了多段非用户本人的对话内容，包含了大量敏感信息。该读者原本只是使用 ChatGPT 进行一个无关的查询，却意外发现聊天记录中出现了不属于自己的对话内容。</p><p></p><p>这些泄露的对话内容涉及多个方面。其中一组对话似乎来自药房处方药门户员工使用的支持系统，透露了该门户的名称、发生问题的店铺编号以及登录凭证。另一段对话则泄露了某人正在制作的演示文稿名称以及一份尚未发表的研究提案的细节。</p><p></p><p>OpenAI 官方对此回应称，用户报告的问题是其 ChatGPT 账号被盗所致。OpenAI 的一位代表补充说，该账号未经授权的登录来自斯里兰卡，但账号持有人声称自己的登录地区为美国纽约布鲁克林。</p><p></p><h4>Neuralink 实现历史性突破：首位人类成功植入脑机接口芯片</h4><p></p><p>近日，埃隆·马斯克的脑机接口公司 Neuralink 宣布，已成功为首位人类患者植入了大脑芯片。马斯克在社交媒体上表示，植入者目前恢复良好，初步结果显示神经元尖峰检测前景乐观。Neuralink 的首款产品名为“心灵感应”，旨在通过思维控制手机或电脑，实现对各种设备的操控。这一技术的目标用户群体包括失去肢体功能的患者，如瘫痪患者，其愿景是让交流速度超越传统打字。</p><p></p><p>Neuralink 的大脑芯片设计用于帮助瘫痪患者通过思维操控电脑，该公司已获得美国食品和药物管理局的批准进行人体试验。去年年底，Neuralink 开始招募因颈脊髓损伤或肌萎缩侧索硬化症导致四肢瘫痪的志愿者参与试验。尽管 Neuralink 在动物实验中取得了显著进展，但其实验方法也引发了动物权利保护组织的担忧。</p><p></p><p>更多内容可查看：</p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247602289&amp;idx=2&amp;sn=e3cadc8dc0705853ae74773e44253d0c&amp;chksm=fbebe9becc9c60a855bec2dac1c0e9803c5bfb16e94fca6d0d0ebeadbd329590e951a2f24273&amp;scene=21#wechat_redirect">Neuralink 完成全球首例人类脑机芯片植入手术，马斯克：植入者恢复良好</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/mGcQYHEikyxBqGgFwfyQ</id>
            <title>减少50%运维工作量，百度智能云基于大模型构建智能问答系统技术实践</title>
            <link>https://www.infoq.cn/article/mGcQYHEikyxBqGgFwfyQ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/mGcQYHEikyxBqGgFwfyQ</guid>
            <pubDate></pubDate>
            <updated>Mon, 05 Feb 2024 10:30:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型构建, 知识库智能问答系统, 数据库运维, 技术方案选型
<br>
<br>
总结: 本文介绍了基于大模型构建「知识库智能问答系统」的设计过程和应用。通过将专家系统和AI技术相融合，帮助数据库运维工程师高效获取数据库知识，并做出快速准确的运维决策。传统的运维知识库系统已经不足以满足现在复杂多变的运维环境，因此借助大模型来提供运维知识并协助决策成为趋势。文章介绍了技术方案选型和分模块设计实现的过程，以及该系统的应用场景和效果。 </div>
                        <hr>
                    
                    <p></p><blockquote>本文源自百度智能云数据库运维团队的实践，深入探讨了基于大模型构建「知识库智能问答系统」的设计过程和应用。全文包括了总体的技术方案选型、各个模块的设计实现、重点难点问题的突破、以及目前的落地场景应用等。该系统自从内部上线以来，整体的回答准确率达到 80% 以上，数据库运维工作量直接减少 50%：包括 80% 咨询量，以及 20% 工单处理工作。</blockquote><p></p><p></p><h2>背景</h2><p></p><p>随着大模型的飞速发展， AI 技术开始在更多场景中普及。在数据库运维领域，我们的目标是将专家系统和 AI 原生技术相融合，帮助数据库运维工程师高效获取数据库知识，并做出快速准确的运维决策。</p><p></p><p>传统的运维知识库系统主要采用固化的规则和策略来记录管理操作和维护的知识，这些系统的知识检索方式主要基于关键字搜索和预定义的标签或分类，用户需要具备一定的专业知识才能有效地利用这些系统。</p><p>这已不足以满足现在复杂多变的运维环境。因此，借助大模型来提供运维知识并协助决策成为趋势。这将在运维能力、成本控制、效率提升和安全性等方面带来深刻的变革。</p><p></p><p>在数据库领域，AI 技术应用可以划分为不同场景，例如知识库学习（包括知识问答和知识管理）、诊断与推理（包括日志分析和故障诊断）、工作辅助（包括 SQL 生成和 SQL 优化）等。本文将主要着重介绍「知识库智能问答系统」的设计与实现，旨在为读者提供深入了解该领域应用的思路。</p><p></p><h2>架构设计和实现</h2><p></p><p></p><h3>技术方案选型</h3><p></p><p>目前，大模型已经可以通过对自然语言的理解揣摩用户意图，并对原始知识进行汇总、整合，进而生成更具逻辑和完整性的答案。然而，仍存在以下几个问题，导致我们不能直接使用这些模型来对特定领域知识进行问答。</p><p></p><p>专业性不足：作为通用大模型，对专业领域知识的训练不足，可能会产生虚假陈述、准确性不足以及信息丰富度不足的问题。时效性问题：模型的训练数据基于某个时间之前的数据，缺乏最新的信息，每次添加新数据都会导致高昂的训练成本。安全性问题：模型无法访问企业内部私密文档，且这些文档不能直接用于 Fine-Tuning。</p><p>为了解决这些问题，业界采用了如下几种技术手段来为大型模型提供额外知识。</p><p>Fine-Tuning（微调）：使用特定领域的知识对基础大模型进行微调，以改变神经网络参数的权重。虽然适用于特定任务或风格，但需要大量资源和高质量的训练数据。Prompt 工程：将行业领域的知识作为输入消息提供给模型，让模型对消息中的知识进行分析和处理。这种方法在正确性和精度上表现良好，但有文本长度限制，对于大规模数据不够高效。与传统搜索结合：使用传统搜索技术构建基础知识库，然后使用大语言模型处理用户请求，对召回结果进行二次加工。这种方法具有更高的可控性和效率，并适用于大规模数据。</p><p></p><p>为了确保准确性和效率，我们选择了第 2 种和第 3 种方式相结合的方案，通过向量数据库将知识外挂作为大模型记忆体，使用 LangChain 作为基础开发框架来构建知识库问答系统，最终依靠&nbsp;Prompt 工程和大模型进行交互。</p><p></p><h3>分模块设计实现</h3><p></p><p></p><p>数据库运维知识库的整体设计流程如下图所示，包括文档加载、文档分割、文本/问题向量化、问答缓存、大模型生成答案等流程。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0b/0b1d7ca6bb7858b8e80ad2632659bfad.png" /></p><p></p><h4>知识入库</h4><p></p><p>数据源加载和解析：主要使用 LangChain 支持的文档加载方法，对 PDF、CSV、Markdown 等格式的文档类型进行加载和采集。此外，考虑到很多企业的文档来源是内网网页，因此也支持 Selenium 和 BeautifulSoup 来爬取网页内容，最后再应用 LangChain 中的 Markdown 加载器进行格式解析。文本分片：原始知识库应当被拆分成独立、较短的文本块，每个文本块将作为问答的最小记录，与问题进行匹配。文本的切分质量直接关系到 Embedding 和召回的质量。切分块不能太大或者太小，也不能超过 Embedding 和大模型的 token 限制。在很多内部网页文档中，由于多级标题和段落间是有上下文关联的，所以我们采用 Markdown 或者 HTML 方式进行切分，进而大大提高了对文档内容的感知能力。在文本切分器的选择上，我们主要采用 LangChain 中的 RecursiveCharacterTextSplitter 和 SpacyTextSplitter这两种分词器。它们能够在保持知识点完整性的基础上，对中文句子、段落、章节等进行良好的切分。需要注意的是，由于算法有 token 数量的限制，选择好的分词器能够为切片提供很好的切分单位和依据。目前我们选择的是 tiktoken 和 Spacy 中的 Tokenizer，但有时候并不理想，需要根据大模型采用的 token 计算方法进行适配。文本向量化：在项目初期 Embedding 模型选择了&nbsp;Hugging Face&nbsp;上开源的 Embedding模型，例如 GanymedeNil/text2vec-large-chinese和 moka-ai/m3e-large，但实际测试效果并不理想。最终我们选择了文心的 Embeddings 模型，效果有质的飞跃，虽然支持的 token 和向量维度低，但整体效果很好。LangChain 中对于千帆接口进行了封装，可以直接通过百度千帆调用文心 Embedding。关于文本向量化、存储和检索的详细信息，请参考下图：</p><p></p><p><img src="https://static001.geekbang.org/infoq/6d/6d100f4e0ffb82523032c6c4ec322993.png" /></p><p></p><p>存储：将生成的Embeddings（向量）与原始分片（知识点）进行存储，同时考虑存储一些关键的元信息，如链接地址和分片大小，以用于检索时作为过滤条件。专业的解决方案是使用向量数据库，但也可以考虑传统数据库或存储中间件，如RedisSearch 或 pgvector，它们都支持向量字段和向量相似性查询，可提供实时向量索引和查询功能。在向量数据库选型上我们对&nbsp;ElasticSearch、Baidu ElasticSearch（BES）、Milvus 和 PGVector分别做了测试，在查询性能方面，PostgreSQL&nbsp;性能较差不可用，而 BES、ES、milvus 性能在一个层级，BES采用自研的插件实现了 HNSW 算法，召回效果表现更好。在资源消耗方面，它们都较为耗费内存，其中 BES 和 ES 相对来说消耗较小。BES 是百度智能云自研的分布式、开源搜索与分析引擎，在百度内部多模态和大模型基础平台有多年积累和应用，在性能、分布式和易用性方面表现良好，LangChain 也对其进行了集成，最终我们选择了 BES 作为向量数据库。</p><p></p><h4>数据检索</h4><p></p><p>用户问题向量化：对用户的问题进行向量化计算。如果结果在缓存中命中，将从缓存中获取已经缓存的答案，以减少文心大模型 API 费用和提高响应速度，可使用&nbsp;GPTCache&nbsp;等库来实现。向量检索：使用 Embeddings 模型在向量数据库中进行相似性计算，召回相似度最高的 n 个分片。目前设置的召回策略是默认选择前 10 个评分最高的分片。</p><p></p><h4>结果整合</h4><p></p><p>将向量数据库检索召回的文本进行二次加工后，利用 LLM 总结概括和分析推理能力，完成最终答案的生成。</p><p></p><p>Prompt 生成：将 n 个切片和用户原始 Question 组装成 Prompt。需要注意的是，Prompt 不能超出 Token 限制，超出限制则需要进行优化，例如淘汰或多次迭代调用等。我们在 Prompt 中除了原始问题和内容，还对大模型加入了回答内容的限制，如「不允许在答案中添加编造成分」、「请用中文回答」等。此外，我们还提供记忆功能，将历史会话信息传入 Prompt，一并发送给大模型。大模型响应：将 Prompt 发送给大模型，获取最终的结果。同时，将对话信息和结果追加存储到 MySQL 中，以保存会话历史，这有助于会话重启和历史信息接入大模型。</p><p></p><h2>技术难点和解决方案</h2><p></p><p></p><h3>难点一：向量数据库召回率低</h3><p></p><p>尽管通过将知识嵌入（Embedding）与大型语言模型相结合已经成为一种高效的实现路径，但向量数据库在向量化、存储和检索等多个阶段都可能存在问题，进而导致检索结果的召回率不尽如人意。在实际测试中，我们在未经优化的情况下，召回率仅达到了 70% 左右。而一个相对可靠的系统，召回率至少需要达到 85% 或甚至 90% 以上。以下是我们在应用中采取的优化措施。</p><p></p><h4>精确切分文本</h4><p></p><p>分割模型：由于训练的文档主要是中文文档，因此切片工具必须具备对中文的良好支持。为此，我们首选 Spacy 作为分割工具，并采用 zh_core_web_sm 模型作为标记器（tokenizer）。分割条件：一般情况下，大部分系统会使用 LangChain 定长切分，但这样会丢失大部分上下文关联，知识点也是割裂的。在实践中，我们没有仅仅依赖 chunk size 作为唯一的切割条件，而是对那些具有明显段落或章节结构的文本格式（如 Markdown 或 HTML）进行了格式化分割，以确保文本的连续性、相关性和完整性。当段落超过 Embedding token 数限制时，我们会使用 RecursiveCharacterTextSplitter 对段落继续进行切分，切分条件除了设置换行符外，还加入了中文常见的断句符号，比如分号、叹号等。标题补偿：当某段文字的大小超过了 chunk size 时，我们会针对没有标题的 chunk 补充标题，以确保整体切分的完整性。</p><p><img src="https://static001.geekbang.org/infoq/f2/f24bf0be6d1281d92c9d74b2dc5c8723.png" /></p><p></p><h4>优化文本向量化</h4><p></p><p>标题向量化：在进行精细化切分之后，标题的重要性显现出来。因此，我们在这一阶段对标题进行向量化处理。这一方法适用于帮助手册、 HTML 和 Markdown 等文本格式。内容关键字向量化：如果仅对标题进行向量化，对于那些标题概括性较差或段落内容丰富的情况，精召率提升仍然有限。因此，我们还尝试了了另一种方法，即首先利用大型模型或关键字模型提取关键字，数量通常限制在 10 左右，然后对这些关键字进行向量化处理。由于多轮次调用的耗时和关键字提取的可靠性问题，最终该方案被放弃。标题 + 内容同时向量化：在文本分割时，我们强行对每个分片加入了标题。在向量化时，会将标题 + 内容打包一并进行向量化。我们将用户提问向量化后，和向量化后的切片进行检索匹配，选择与问题最相关的 topN 切片，这样可以显著提高精召率，这也是我们最终的方案。</p><p></p><h4>Embeddings 和向量检索调优</h4><p></p><p>对于 Embeddings 的选择和调优，上文已经介绍过，我们最终选择了效果更好的文心 Embedding。对于向量数据库检索性能，这里优化空间并不大，调整 HNSW 算法的参数，对最后召回结果影响不大。</p><p></p><h3>难点二：Token 数量限制</h3><p></p><p>在应用大型语言模型时，我们面临的主要限制之一就是输入文本的上下文长度。开源模型和商业模型的上下文长度限制范围从 2K 到 100K 不等。上下文长度对于应用大型语言模型具有关键影响，包括知识增强、记忆等方面的工作，都是为了解决上下文长度限制而设计的。以下是我们采取的策略：</p><p>取舍：如果选择的 10 个文本组合成的 Prompt 超出了模型的 Token 限制，我们采取逐一舍弃相似度较低的片段的策略。如果减少到召回文档为 6 个时还是超限制，则会选择 token 数支持更多的模型。模型选择：ERNIE-Bot-turbo 模型支持 10200 个Token，ERNIE-Bot 支持 2000 个 Token 的 ERNIE-Bot 模型，以扩大上下文长度。但是 ERNIE-Bot-turbo 在问答领域的效果并不如 ERNIE-Bot，此时，我们的策略是在不超过 2000 个 token 的情况下优先选择 ERNIE-Bot，极大地提升了系统对复杂问题的处理能力。压缩 Prompt：我们尝试对多个切片拼接后的文本进行压缩，以提取主要内容，去除无用且重复的词组。然而，这种方法的效果有限，甚至可能导致文本失真，且对中文支持较差，因此无法从根本上解决问题。压缩效果如下图：</p><p><img src="https://static001.geekbang.org/infoq/e4/e46f96322c2402016e4ef0c99dbbaf53.png" /></p><p>多轮次迭代调用 LLM：面对超长文本超出大模型 token 限制的情况，我们采用了 MapReduce 的方式来突破 Token 限制。该方式将文本拆分成多个部分，并多次调用 LLM 以解决文本长度问题。具体流程包括将多个分段分别请求 LLM，获取各自的局部答案。然后将这些局部答案拼接成新的 Prompt，再次请求 LLM 以获取最终答案。这一流程有效地扩展了上下文长度，但是实际应用效果并不理想，表现为最终结果失真，尤其是在回答流程类问题场景下。主要原因是汇聚后丢失了很多原始文本细节。</p><p><img src="https://static001.geekbang.org/infoq/2e/2ec41158b3666e81e0d581d1495ee888.png" /></p><p></p><h3>难点三：知识陈旧和虚构答案</h3><p></p><p>在商业大型模型的大多数应用场景下，模型能够为 MySQL、Oracle 等数据库的相关问题提供令人满意的答案。然而，不可避免地，这些大型模型有时会出现知识陈旧和答案虚构的问题。为了提供更加丰富和准确的答案，我们采用了一种搜索和推荐系统的方法，并结合了大型模型的推理和总结能力。以下是我们的主要方案和流程：</p><p>提取问题关键字：首先从客户问题中提取关键字，以确保这些关键字能够准确地用于搜索引擎检索。为此，我们探索了两种不同的方法：大模型：大型模型本身具备一定的关键字提取能力，但实际测试表明，这种方法的稳定性有待提高，可能会导致调用链出现异常。因此，我们需要对这种方式进行 Prompt 的调优，以提高其性能和可靠性。NLP 算法：另一种思路是利用 NLP 模型来进行关键字提取。然而，我们曾尝试使用 Hugging Face 的一些模型，但效果并不十分理想。搜索引擎检索和文档解析：为了获得与数据库问题相关的准确答案，我们评估了以下两种不同的策略：接入百度搜索引擎：我们曾尝试使用百度搜索 API 来根据提取的关键字进行检索。然而，这一方法的答案质量较差，而且可能包含过时的信息，这可能会对最终答案造成负面影响。因此，我们最终放弃了这一方案。接入官方文档搜索：对于 MySQL 等数据库，官方文档提供了用于客户搜索的 API。我们只需传入关键字即可获取与之匹配的搜索结果。然后，我们可以选择最相关的前 N 个结果，并对这些结果的链接内容进行爬取和解析。这一流程类似于之前描述的领域知识入库和知识检索流程，但具体细节不再赘述。</p><p><img src="https://static001.geekbang.org/infoq/91/9182f7de7bdf73ff6ff2b1dec1cf04a5.png" /></p><p>调用大模型：将多个 chunk 拼接和问题一起生成 prompt，调用大模型获取答案。</p><p></p><p>可以看到文档解析和大模型调用其实就是在重复我们前边介绍的领域知识入库和结果的二次整合过程，唯一不同的地方就是我们使用搜索引擎去代替了向量检索。以 MySQL 为例子，具体流程如下：</p><p><img src="https://static001.geekbang.org/infoq/fc/fcc501d7c077f6409b6868faa56e6035.png" /></p><p></p><h2>应用场景接入</h2><p></p><p>该系统自从内部上线以来，整体的回答准确率达到 80% 以上，数据库运维工作量直接减少 50%，包括 80% 咨询量，以及 20% 工单处理工作。</p><p></p><p>目前「知识库智能问答系统」主要通过两种方式接入和应用：Database Chat 和 IM 机器人。</p><p></p><p>Database Chat：除了类似于&nbsp;ChatGPT&nbsp;问答界面外，还具备知识管理、用户管理等功能。（该功能已经集成数据库智能驾驶舱 DBSC 中，将于 3 月底正式开放上线）</p><p></p><p><img src="https://static001.geekbang.org/infoq/ed/ed5895bdf264dcf953f9f8de7c99ccab.png" /></p><p></p><p>IM 机器人：IM 工具做为工作协同中最重要的软件，使用频率非常高。我们提供了 IM 接入接口，客户可以开发 IM 软件（微信、飞书、如流等）机器人，在聊天群中实现快速高效获取信息和知识。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d5/d561acede566e15c293b411c62267a49.png" /></p><p></p><p></p><h2>总结</h2><p></p><p>从技术工程角度来看，利用向量数据库结合大型 AI 模型来构建领域知识库系统的实现并不复杂，然而，这一领域仍然面临着不少挑战和潜在的改进空间。在本文中，我们已经讨论了一些解决方案和技术，但仍然有许多可能的改进和未来发展方向值得深入研究。</p><p></p><p>首先我们认为关键点还是解决向量检索的召回准确性和超长文本处理能力是两个难点，这些方面可能还有更好的方式。此外，大模型本身的能力和文档质量是系统性能的关键因素，因此需要不断升级和维护模型，同时确保文档的及时性和准确性。</p><p></p><p>我们希望更多的研究者和工程师积极贡献更多的创新思路和技术，推动大模型在数据库运维领域落地，期待未来能有更多的可能性。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/FRZc8PdiLxcd9jph9Qw8</id>
            <title>新型IT运维管理，基础设施和数据两手都要硬</title>
            <link>https://www.infoq.cn/article/FRZc8PdiLxcd9jph9Qw8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/FRZc8PdiLxcd9jph9Qw8</guid>
            <pubDate></pubDate>
            <updated>Mon, 05 Feb 2024 09:51:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据, AI, IT人, 基础设施
<br>
<br>
总结: 在AI大模型时代，数据成为了企业的关键资产和核心竞争力。IT人的新使命是提供优质数据服务，让数据好用，帮助业务部门用好数据。然而，目前大部分企业中的基础设施运维管理和数据管理是割裂的，由不同的团队负责，工具平台间也没有有效协同。因此，IT团队应该将基础设施和数据作为一个有机整体进行管理和优化，实现数据同源、全局最优、安全流通，扮演着数据资产管理者的重要角色。 </div>
                        <hr>
                    
                    <p>编前语：数据是 AI 的基石，缺数据无 AI。</p><p></p><p></p><h1>AI 大模型时代，数据赋予 IT 人“新使命”</h1><p></p><p></p><p>当下 IT 人在企业中扮演着运营支撑的角色。说到运维管理，相信每人都是一把辛酸泪，每天承担着繁琐、高负荷且又高风险的运维工作，但在业务规划和职业发展时又成为了“透明人”。业内有句调侃的话：“只花钱的人，不配拥有发言权”。</p><p></p><p>随着 <a href="https://www.infoq.cn/article/mtlqIGF9r1VUWCTBfGAp?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">AI </a>"大模型应用的普及，数据成为了企业的关键资产和核心竞争力。近年来，企业的数据规模从 PB 级向百 PB 级成倍数快速增长，数据类型也从以数据库为主的结构化数据，演变成以文件、日志、视频等为主的半结构化、非结构化数据。业务部门希望数据的存储能够像图书馆一样分门别类随用随取，同时又希望数据的存储更安全、更可靠。</p><p></p><p>IT 人，已不再只是负责建设和管理 IT 资源、保障设备稳定的被动角色。&nbsp;</p><p></p><p>IT 人的新使命，演变为提供优质数据服务，让数据好用，帮助业务部门用好数据！</p><p></p><p><img src="https://static001.geekbang.org/infoq/92/9244f6ebd6399a5f849019fe04424ea7.jpeg" /></p><p></p><p></p><h1>“基础设施”和“数据”很近，但两者的“管理”却很远</h1><p></p><p></p><p>针对基础设施的管理，业界普遍的做法是通过 <a href="https://www.infoq.cn/article/huaweiyun-aiops?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">AIOps </a>"技术，将繁琐的人工日常运维改变为利用工具自动化执行，通过专家系统、知识图谱等智能化能力，主动发现系统隐患、自动修复故障等。生成式 AI 技术普及后，近期出现了智能客服、交互式运维等新型应用。</p><p></p><p>针对数据的管理，业界有以 Informatica、IBM 等为代表的专业 DataOps 软件供应商，支持数据集成、数据标记、数据分析、数据优化、数据市场等能力，为数据分析师、BI 分析师、数据科学家等业务团队提供服务。</p><p></p><p>笔者调研发现，目前大部分企业中的基础设施运维管理和数据管理是割裂的，由不同的团队负责，工具平台间也没有有效协同。业务上数据保存在存储等 IT 基础设施中，应该融为一体，但实际两者的管理却相隔甚远，甚至两个团队之间语言都不对齐，这样通常会带来几个弊端：</p><p></p><p>1）数据不同源：因为归属不同团队和采用不同工具，业务团队通常采用将原始数据通过 ETL 等方式复制一份至数据管理平台做分析处理。这样不仅造成存储空间浪费，还存在数据不一致、数据更新不及时等问题，影响数据分析的准确性。</p><p></p><p>2）跨地域协同难：如今企业数据中心在多个城市布局，数据在跨地域传输时，当前主要通过 DataOps 软件在主机层进行复制，这种数据传输方式不仅效率低，传输过程中也存在安全、合规、隐私等严重隐患。</p><p></p><p>3）系统优化不充分：当前通常基于基础设施资源的利用情况进行优化，因为无法感知数据布局而实现全局最优，数据的保存成本居高不下，有限增长的预算与成倍的数据规模增长之间的矛盾成为制约企业数据资产积累的关键矛盾。</p><p></p><p></p><h1>IT人，打通“基础设施”和“数据”任督二脉，开启数智化飞轮</h1><p></p><p></p><p>笔者认为，IT 团队应该把“基础设施”和“<a href="https://www.infoq.cn/article/NTJYRtUm0xOma4gOYPC6?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">数据</a>"”作为一个有机整体进行管理和优化，实现数据同源、全局最优、安全流通，扮演着数据资产管理者的重要角色。</p><p></p><p>首先，实现全局文件的统一视图。利用全局文件系统、统一元数据管理等技术，把不同地域、不同数据中心、不同类型设备中的数据形成统一的全局视图。在此基础上，能够按照热温冷、重复、过期等维度进行全局优化策略的制定，下发至存储设备执行，这种方式可以实现全局最优。基于存储层复制的压缩、加密等技术，通常可以实现数十倍的数据移动速度，效率和安全性都可以得到保证。</p><p></p><p>其次，将海量非结构化数据自动生成数据目录。通过元数据、增强型元数据等自动生成数据目录服务，把数据分门别类高效管理起来。业务团队基于目录可以自动提取满足条件的数据进行分析处理，而不用人工像大海捞针一样去找数据。笔者调研发现通过 AI 识别算法实现数据标注的技术已经较为成熟，因此可以利用开放框架将不同场景化的 AI 算法进行集成，自动分析文件内容形成多元化标签，作为增强型元数据提升数据管理的能力。</p><p></p><p>同时，数据在跨设备流动时，需要特别考虑数据主权、合规隐私等问题。存储设备中的数据应该自动分类、隐私分级、分权分域等，管理软件对数据的访问、使用、流动等策略进行统一管理，避免敏感信息和隐私数据泄露，未来数据要素交易场景这些将成为基本要求。比如，数据在流出存储设备时，首先需要对合规性、个人隐私等进行判定是否满足策略要求，否则企业将面临严重的法律法规风险。</p><p></p><p>参考架构如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/43/43af6545e40b0f159f57a56fd8d4bbaf.png" /></p><p></p><p>根据笔者调研以及请教同行专家后，发现业界诸如华为存储、NetApp 这样的领先存储厂商已经发布存储和数据一体化管理的产品解决方案，相信未来会有更多厂商支持。</p><p></p><p>设备和数据两手都要抓、两手都要硬。IT 人在 AI 时代可以扮演更重要的作用。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ArAHV0fcJXx4ULSAt1gk</id>
            <title>库克吹捧的Vision Pro，杀手级应用就这？开发者：我希望它一败涂地</title>
            <link>https://www.infoq.cn/article/ArAHV0fcJXx4ULSAt1gk</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ArAHV0fcJXx4ULSAt1gk</guid>
            <pubDate></pubDate>
            <updated>Mon, 05 Feb 2024 06:58:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 苹果, Vision Pro, 虚拟现实, 应用
<br>
<br>
总结: 苹果最新发布的Vision Pro是一款具有前瞻性的空间计算设备，虽然价格昂贵，但在虚拟现实领域引起了热潮。然而，Vision Pro在实际应用中存在一些问题，如操作方式不便捷、缺乏特别亮眼的应用等。目前，Vision Pro的杀手级应用可能是Web浏览器和烹饪软件。但Vision Pro仍面临着第三方应用和服务的支持不足的问题，以及长时间观看视频体验不佳的挑战。尽管如此，作为第一代设备，Vision Pro仍有发展潜力。 </div>
                        <hr>
                    
                    <p>上周，苹果终于向少数用户开放了该公司自2007年iPhone发布以来，最大胆也最具前瞻性的产品——Vision Pro。3499美元的价格确实夸张，但预订单几分钟内即被买空。据估计，在纯路人好奇查询之前，已经有20万铁粉蹲守下单。除了苹果，我们很难想象还有哪家公司能在已经让人有点审美疲劳的虚拟现实领域掀起这样的热潮。</p><p>&nbsp;</p><p>苹果强调Vision Pro并非VR头显，而是一款空间计算设备，并将其描述为开创工作新形态的基础。具体而言就是通过注视某物并同时敲击拇指和食指，来执行操作。在演示中，整个过程确实简洁优雅，但实际效果如何仍然有待验证。</p><p>&nbsp;</p><p>YouTube上一段精彩的Vision Pro评测视频提到，“最大的进步，也就是能精准控制用户看到的内容。但这种直接查看、精准操作的功能在其他计算设备和用户界面上早就实现了，而且体验还更好。”也就是说，最前沿的设备反而带来了体验上的倒退——已经为人们所熟知的多任务操作，在Vision Pro上重新沦为单任务操作。</p><p>&nbsp;</p><p>打字也只能通过双手食指实现，这种感觉类似于重新回到刚接触电脑时的“一指禅”状态。别说对着空气打字，就算当初能借助桌面的激光键盘，也因为缺少敲击反馈而被时代所抛弃。所以从实际应用来看，靠手势或者语音操控Vision Pro仍然既低效又尴尬。</p><p>&nbsp;</p><p>那么，如果苹果Vision Pro头显有杀手级应用，可能会是什么？</p><p></p><h2>Vision Pro的杀手级应用是什么？</h2><p></p><p>&nbsp;</p><p>The Verge认为，苹果Vision Pro的首个杀手级应用是Web浏览器——Safari。对于Vision Pro而言，浏览器的重要性不言而喻，一个强大且深度集成的桌面级浏览器将使Vision Pro从一开始就具备实用性和强大功能。</p><p>&nbsp;</p><p>Wired则认为，苹果Vision Pro的杀手级应用可能是烹饪软件。</p><p>&nbsp;</p><p>设想一下，用Vision Pro管理厨房，就像在锅碗瓢盆上方都添加了智能便利贴，并且这些便利贴并不会掉进锅里或者引起火灾，而是始终稳定悬停在灶台附近。此外，Vision Pro还可以创建虚拟悬停计时器，凭借空间跟踪技术让一个个小界面稳定保持在原位。从这方面体验来讲，3500美元花得倒也不冤。根据《华尔街日报》的专栏报道，“我用它做的最酷的事情，就是引导烹饪。我可以在厨房里设置计时器，就悬停在锅具上方。这也是真正能彰显未来生活的精彩用例。”</p><p>&nbsp;</p><p>但除此之外，Vision Pro的其他用例只能说乏善可陈。更悲哀的是，包括Vision Pro在内，近年来的几乎所有新型硬件都以烹饪工具为归宿，比如智能手表就是个例子。</p><p>&nbsp;</p><p>根据2023年的一项调查，计时器成了Apple Watch最常见的用途。斯德哥尔摩大学2016年进行的Smartwatch in Vivo报告也显示，厨房计时成为智能手表融入日常生活的核心方式之一。研究解释称，“以一对夫妇的烹饪体验为例，其中一人去磨奶酪时，手表开始为意面煮锅计时。时间一到手表立即响起，提醒用户把面锅从火上移开。手表优雅地融入烹饪过程，几乎不造成任何干扰。”</p><p>&nbsp;</p><p>智能音箱也是如此。以Amazon Echos为例，根据Experian 2016年的一项研究，用户使用最多的功能是Amazon Echos的计时器功能（占比84.9%），甚至高于播放歌曲（占比82.4%）。</p><p>&nbsp;</p><p>这些例子也从侧面反映出，如今，包括Vision Pro在内的新型硬件还缺乏特别亮眼的应用。如果像苹果这样的业界巨头在消费级领域还是找不到太好的应用方向，那Vision Pro的前途着实堪忧。</p><p></p><h2>Vision Pro还缺乏特别亮眼的应用</h2><p></p><p>&nbsp;</p><p>与各类新兴技术平台一样，苹果Vision Pro能否成功在很大程度上取决于第三方应用和服务的支持力度。从这一角度来看，Vision Pro仍面临着诸多难题。</p><p>&nbsp;</p><p>根据市场情报公司Appfigures的数据，截至上周末，AR/VR 设备的 App Store 中只有 150 多个专门针对 Vision Pro 进行更新的应用程序——仅占App Store 180 万个应用程序的一小部分。</p><p>&nbsp;</p><p>需要明确的是，这并不是可用应用程序的数量。理论上，Vision Pro与iOS和iPad应用程序兼容，除非开发者选择不兼容。但这表明，有多少公司急于为这个新平台构建产品，特别是考虑到苹果更广泛的开发人员基础的规模，这个数字显得更小了。</p><p>&nbsp;</p><p>截至目前，不少知名开发商还没有为Vision Pro提供原生软件。全球最受欢迎的三大流媒体服务商Netflix、YouTube和Spotify均明确表示不会推出VisionOS版软件，也不会将自家iPad版应用移植到Vision Pro之上。Netflix公司联合CEO Greg Peters甚至在播客中明确质疑，Vision Pro“跟我们的主要客群有交集吗？”而包括谷歌和Meta等iOS和iPadOS端的重量级软件厂商，似乎也从一开始就打算避开这套新平台，借机摆脱苹果生态。</p><p>&nbsp;</p><p>Netflix和YouTube不愿全力支持Vision Pro，实际上凸显出的是个长期以来困扰着虚拟现实与混合现实（特别是前者）的老问题：用头显长时间观看视频内容的体验太差。哪怕是像詹姆斯·卡梅隆这样痴迷于前沿技术的大导热情推荐，也不得不承认想靠头显看完一整部《阿凡达》实在难受。</p><p>&nbsp;</p><p>混合现实“不应一次性使用几个小时。它的最大优势仍然是为用户提供特殊的体验，而非无休止沉浸其中。”斯坦福大学虚拟人类交互实验室创始主任Jeremy Bailenson给出了中肯的评价。该实验室刚刚发表一篇关于混合现实技术所造成心理影响的论文，其中就专门提到了具备透传视频技术的设备，比如说Vision Pro。具体来讲，“混合现实是一种特殊且刺激的信息介质。”</p><p>&nbsp;</p><p>Omdia公司首席分析师David Tett表示，由于此前大部分混合现实体验大部分都集中在游戏之上，因此苹果希望能将Vision Pro打造成更具广泛吸引力的媒体设备。但他也补充道，Vision Pro的使用感受仍然笨重且不够舒适。头显的电池续航限制也让长时间观看难以实现。Tett坦言，“至于跟亲朋好友一起观看视频的乐趣，在Vision Pro上天然无法实现。”</p><p>&nbsp;</p><p>但Tett并不悲观，他表示虽然Vision Pro目前存在种种局限，但“第一代设备大多不够完美。”曾经用iPod引领过时代的苹果，也是最有可能改变用户媒体消费习惯的厂商了。也许未来，我们真能适应在头显中一口气看完一部时长3小时的影片。</p><p>&nbsp;</p><p>而曾经在迪士尼、20世纪福克斯和HBO等影厂拥有丰富工作经验的南加州大学创意技术研究所混合现实研究主任David Nelson认为，Vision Pro确实拥有一定设计优势。“织物绑带和流线型的设计都是不错的起点。接下来最大的难题，就是怎样引导人们适应戴着头显看完一部电影。”</p><p>&nbsp;</p><p>值得注意的是，苹果公司宣称，Vison Pro将开箱即用支持超100万种应用，包括来自迪士尼、TikTok、亚马逊、派拉蒙等的软件。但这里不妨大胆猜测，其中99%恐怕都是iPad版本，而非为头显系统VisionOS重新设计的专用软件。</p><p>&nbsp;</p><p>也就是说，除非像Netflix、Spotify和YouTube那样明确表示拒绝，否则原iPad版本的软件会在Vision Pro中直接上架。苹果自身也没有尽最大努力为新平台提供支持，甚至没有针对Vision Pro对部分最重要的官方软件做过优化。</p><p></p><h2>开发者关系不稳，苹果Vision Pro前途堪忧</h2><p></p><p>&nbsp;</p><p>Vision Pro缺乏关键首发应用保驾引发人们对于苹果开发商合作关系的担忧。有分析指出，Vision Pro前进阻碍正是苹果不稳定的开发者关系。在此之前，开发商们曾经都热烈拥护苹果推出的新平台——iPhone和iPad应用商店在上线后都曾快速迎来爆发。现如今，这两大平台均支持数百万款应用。而曾经的辉煌在Vision Pro上恐怕难以重现。</p><p>&nbsp;</p><p>为什么开发商对Vision Pro的热情低于预期？外媒总结了以下四点原因：</p><p>&nbsp;</p><p>VisionOS软件开发成本高昂，而开发商希望确保投资有所回报。所以部分开发商可能想要静观其变，看看Vision Pro的市场到底有多大，之后再做决定。但有些厂商甚至明确拒绝在新设备上支持自己的iPad版应用，所以情况恐怕更加复杂。有些开发商对苹果的App Store审查政策、抽成等做法感到愤怒，甚至已经引起了监管机构的注意。这些开发商知道Vision Pro的命运由其软件阵容决定，所以根本不想帮助苹果推广其新品。有些开发商认为自己的应用无法良好适配Vision Pro的混合现实环境，也就是依靠眼部和手势活动进行操作。不少游戏的最佳交互方式就是VR手持控制器，但Vision Pro尚不支持。另外，开发商还无法完全调用眼部追踪与动作感应功能，这就让软件设计变得更加困难。开发商发现苹果最新推出的三种应用商店（TV、Apple Watch和iMessage）均表现不佳，所以在Vision Pro版本获得成功前不想急于加入。</p><p>&nbsp;</p><p>具体来说，Vision Pro的发布时机恰逢苹果与开发商合作伙伴间最紧张的敏感阶段。软件开发商多年来一直对App Store的管理政策感到不满，现在苹果虽然允许开发商在应用外接受付款，但仍将征收高达27%的款项抽成。此前，Spotify刚刚抨击了苹果这项新政，称“苹果已经证明，他们会不惜一切代价保护自己凭借应用商店垄断从开发商和消费者手中夺取利润的机会。”这一切，都成为笼罩在Vision Pro头顶的阴云。</p><p>&nbsp;</p><p>另一方面，也有不少开发商单纯对苹果的生态政策感到不满。独立开发者Aaron Vegh在博文中强调，虽然不确定Vision Pro能不能成功，“但我可以立场鲜明地讲，我真心希望它一败涂地。”</p><p>&nbsp;</p><p>在吸引游戏玩家方面，Vision Pro新界面的最大缺点就是不适合目前最流行的VR游戏，例如需要配合专用控制器的《刺客信条》和《阿斯加德之怒2》等。Vision Pro倒是能够支持PlayStation和Xbox手柄，但拥有一定VR头显开发经验的厂商其实更倾向于专门的手持控制器。好消息是后续将有更多Apple Aracde游戏加入，包括《NBA 2K24》。</p><p>&nbsp;</p><p>熟悉触控屏的开发商也有自己的抱怨。独立流媒体平台Nebula运营者Dave Wiskus写道，“专为摸控系统设计的应用无法直接转化为Vision Pro版本”，其使用体验将“奇怪且难以把控”。</p><p>&nbsp;</p><p>最后是苹果近期应用商店人气低迷的问题。为Apple Watch、Apple TV和iMessage打造的应用平台均已沦为“鬼城”。即使是相对成功的Apple Watch，也没能给第三方软件打出一片生存空间。Twitter、Uber、Slack和Facebook都曾为该设备开发过应用，但最终均选择关闭并退出该平台。</p><p>&nbsp;</p><p>总的来说，Vision Pro确实是一款价格昂贵且普及度相对较低的产品，而且这种情况在短时间内不会改变。据悉，苹果方面在首周末只准备了8万台库存，好在预订一小时内就被抢购一空。而哪怕苹果今年之内能卖出30到40万台，那对开发商来说这种规模也实在有限——再加上苹果那夸张的佣金抽成，项目将更加无利可图。iPad与iPhone应用开发者Paul Haddad表示，他对探索Vision Pro兴趣不大。“我相信有人愿意花3500美元买来体验，但我自己肯定不会。”</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.wired.com/story/apple-vision-pro-watching-movies-nearly-impossible-problem/">https://www.wired.com/story/apple-vision-pro-watching-movies-nearly-impossible-problem/</a>"</p><p><a href="https://www.wired.com/story/apple-vision-pro-boils-down-to-kitchen-timers-and-thats-a-problem/">https://www.wired.com/story/apple-vision-pro-boils-down-to-kitchen-timers-and-thats-a-problem/</a>"</p><p><a href="https://www.bloomberg.com/news/newsletters/2024-01-21/apple-vision-pro-lack-of-netflix-youtube-app-store-tensions-threaten-device-lrnjwjb3">https://www.bloomberg.com/news/newsletters/2024-01-21/apple-vision-pro-lack-of-netflix-youtube-app-store-tensions-threaten-device-lrnjwjb3</a>"</p><p><a href="https://stratechery.com/2024/the-apple-vision-pros-missing-apps/">https://stratechery.com/2024/the-apple-vision-pros-missing-apps/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/BS9euQ1bfMYvfipxRwAK</id>
            <title>大模型时代，计算创新如何为应用性能提升开启新路径</title>
            <link>https://www.infoq.cn/article/BS9euQ1bfMYvfipxRwAK</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/BS9euQ1bfMYvfipxRwAK</guid>
            <pubDate></pubDate>
            <updated>Fri, 02 Feb 2024 08:11:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据库, 向量数据库, 大模型, 性能提升
<br>
<br>
总结: 数据库在云计算和大数据时代的重要性更加凸显，大模型需要向量化的数据来提升知识查找效率，传统数据库不适合作为大模型的知识库，需要新一代向量数据库来满足需求。腾讯云向量数据库是一款全托管的自研企业级分布式数据库服务，具备高效的存储、检索和分析能力，与英特尔公司合作提升性能。向量数据库的性能提升需要全新的硬件设计和软件优化，英特尔的 AVX-512 指令集和 AMX 高级矩阵扩展加速引擎可以提供更高的密度和执行效率，大幅提升向量检索性能。 </div>
                        <hr>
                    
                    <p>引言：数据库一直是 IT 基础设施的核心组件之一，在云计算和大数据时代，数据库的重要性更加凸显。随着生成式 AI 应用开始广泛流行，企业更加需要海量数据来为大模型提供充足的数据养分。</p><p></p><p>对于大模型而言，模型所需的数据都经过了向量化（Vector embedding）过程，经过向量化的数据可以大幅提升模型的知识查找效率，使模型可以支持更长的上下文，同时降低训练和推理成本。因此，传统数据库不适合作为大模型的知识库，这就需要新一代向量数据库来满足大模型的数据需求。如今，行业内已经涌现了一批能力出色、为大模型应用做了充分优化的向量数据库产品，腾讯云向量数据库就是其中的佼佼者之一。作为一款全托管的自研企业级分布式数据库服务，其能为多维向量数据提供高效的存储、检索和分析能力，具备完善的嵌入功能，兼具高性能、高可用性，稳定性、可靠性，且使用简单，成本低廉。借助上述优势与特性，腾讯云向量数据库正成为用户构建能力时的强力后援团。</p><p></p><p>对于向量数据库而言，只有具备极高的性能水平，才能为规模迅速扩大的模型和上层应用提供充足的知识查询吞吐量和极低的查询延迟。然而，仅靠软件端的优化来提升性能是不够的，软硬协同才是增强向量数据库性能的最佳路径。为此，腾讯云与英特尔公司展开了深度合作，将英特尔第五代可扩展至强处理器的诸多优势特性融入腾讯云向量数据库的软件设计中，从而显著提升性能水平。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f0/f0bb7f37065783862d345102e632bb2c.png" /></p><p>腾讯云向量数据库能与 LLM 模型配合使用，为 LLM 模型提供知识库图片及相关介绍援引自腾讯云官网介绍，详情请参阅：https://cloud.tencent.com/product/vdb</p><p></p><p>对于向量数据库而言，只有具备极高的性能水平，才能为规模迅速扩大的模型和上层应用提供充足的知识查询吞吐量和极低的查询延迟。然而，仅靠软件端的优化来提升性能是不够的，软硬协同才是增强向量数据库性能的最佳路径。为此，腾讯云与英特尔公司展开了深度合作，将英特尔第五代可扩展至强处理器的诸多优势特性融入腾讯云向量数据库的软件设计中，从而显著提升性能水平。</p><p></p><p></p><h1>向量数据库，全新架构带来的硬件性能挑战</h1><p></p><p></p><p>AI 任务中所用的各类数据通常会以向量形式表示。向量（Vector）是一种具有大小和方向的数据表示，其包含多个维度的信息，且每个维度都会用来表示一个特征或属性。正因如此，以向量形式来构建的模型知识库无疑将更具效率，例如在图像处理任务中，图像可表示为像素值的向量；而自然语言处理任务中，文本可表示为词向量或句子向量。</p><p></p><p>数据的向量化是借助词向量模型、卷积神经网络等模型，经过嵌入（Embedding ）环节将文本、图像、音视频等不同数据转换为向量后存入向量数据库中，而向量查询则是通过对向量之间的相似度计算来完成。在实践中，向量数据库可作为知识库，通过与语言、图像等 AI 模型的结合来有效降低用户的模型训练成本，提升 AIGC 等应用的信息输出准确度和及时性。同时向量数据库还可用于模型预训练数据的分类、去重和清洗，相比传统方式实现效率的大幅提升。</p><p></p><p>向量数据库检索向量的方法主要是相似度度量，也就是通过计算来确定向量数据库中两个向量的相似程度，并最终找出与给定查询向量最相似的向量。常见的计算方法包括内积、欧式距离和余弦相似度，而处理器在运行这些计算方法时，主要涉及大量的密集矩阵向量运算过程，同时还需要提供足够高的访存吞吐量。当处理器有很多核心时，核心之间的互联指标也会影响整体的计算效率。相比之下，传统数据库的查询操作主要是非向量化计算过程，属于非密集运算，其对处理器的需求与向量数据库有着巨大差异。显然，为传统数据库优化的处理器和软件设计对于向量数据库来说效率就很难令人满意。为此，处理器需要全新的硬件设计，大幅加强密集向量运算性能、内存吞吐量、多核互联性能等指标，结合软件层面的深度优化，才能为向量数据库提供充足的能力基础。</p><p></p><p></p><h1>突破传统算力增长瓶颈，新一代处理器的性能提升秘籍</h1><p></p><p></p><p>2017 年，计算机体系架构宗师 David Patterson 与 John Hennessy 在斯坦福大学发表著名演讲，指出针对特定应用场景的针对性性能加速设计将是下一个十年中芯片领域的主要命题。两位宗师将这种定制加速设计称为 DSA 领域加速计算。正如这次演讲预言的那样，过去几年来行业内的芯片性能提升更多来自各类定制化加速方案，诸如 GPGPU、TPU 等。而在 CPU 层面，扩展指令集和专用加速引擎也已成为 CPU 和上层软件提升性能的快捷路径。进入大模型时代，领域加速计算创新更能大大缓解处理器通用计算能力遭遇瓶颈带来的挑战，使得 CPU 同样可以在 AI 时代焕发全新动能，继续承担算力基础设施的核心角色。</p><p></p><p>作为 CPU 行业的领军企业，英特尔公司一直非常重视 CPU 扩展指令集和专用加速引擎的研发和创新探索，英特尔第五代可扩展至强处理器内置的英特尔 AVX-512 指令集与英特尔 AMX 高级矩阵扩展加速引擎就是这些探索的最新成果。这两种加速设计可以帮助腾讯云向量数据库在更高的性价比前提下构筑更高密度的算力输出，深度优化其向量检索算法的执行效率，大幅提升检索性能。腾讯云向量数据库支持适合不同场景下的多种算法，而 AVX512 可以为包含 FP32 在内的多种数据格式的向量检索的计算提供 SIMD 的加速支持, &nbsp; AMX 更是支持 BF16 和 int8 数据格式的矩阵运算 从而进一步加速向量检索，实现更高的性能。二者可以适用于不同的场景下数据计算需求, &nbsp;为客户提供了更多的选择。</p><p></p><p>作为一种单指令多数据（SIMD）指令集，英特尔 AVX-512 在密集型计算负载中有着得天独厚的优势。得益于其 512 位的寄存器宽度和两个 512 位的融合乘加（FMA）单元，指令集能并行地执行 32 次双精度、64 次单精度浮点运算，或操作 8 个 64 位和 16 个 32 位整数。在腾讯云向量数据库所需的向量相似度计算中，假设数据类型是 FP32，输入向量 x 中的 16 个维度数据和数据库中向量 y 的 16 个维度数据，都可以一次性被加载到英特尔 AVX-512 的寄存器中，从而实现一次处理 16 个维度的并行运算，效率提升极为可观。由于在各类向量检索算法中类似计算需求的比重往往很高，向量数据库的性能由此可获得巨大提升。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2b/2b7349ac190de4232946b112ba1e807f.png" /></p><p>*&nbsp;英特尔®SSE、英特尔®AVX2 和英特尔® AVX-512 之间的寄存器大小和计算效率的差异说明</p><p></p><p>另一项可为腾讯云向量数据库带来显著加速的是英特尔 AMX 加速引擎。这一技术引入了一种用于矩阵处理的新框架（包括了两个新的组件，一个二维寄存器文件，其中包含称为 tile 的寄存器，以及一组能在这些 tile 上操作的加速器），从而能高效地处理各类 AI 任务所需的大量矩阵乘法运算，提升其在训练和推理时的效能。例如在向量检索的过程中，如存在 n 个 batch 任务，进行相似度计算时就需要对 n 个输入向量 x 和 n 个数据库中向量 y 进行比对，这其中的距离计算会产生大量的矩阵乘法， 而英特尔 AMX 能针对这一场景实现有效加速。</p><p></p><p><img src="https://static001.geekbang.org/infoq/72/7224544b2cc4ab33b4420f07249f7004.png" /></p><p>英特尔® AMX 架构由 2D 寄存器文件 (TILE) 和 TMUL 组成</p><p></p><p>单纯依靠 AVX-512 扩展指令集和 AMX 加速引擎也是不够的，为了充分发挥这两种技术的加速能力，处理器的通用算力、单核性能、多核互联水平、访存性能都需要很高的水平，以避免成为密集向量和矩阵运算的瓶颈。而英特尔第五代可扩展处理器最大规格可拥有 64 个核心，通用计算性能相比上一代在相同能耗下提升 21%，内存带宽提升 16%，UPI 互联性能提升 25%，单核性能也有明显提升，因此 AVX-512 和 AMX 加速引擎的输出效率相比上代能有高达 40% 的增强。</p><p></p><p>硬件性能的巨大进步，需要软件层面的充分优化才能在实际应用中发挥作用。为此，腾讯云与英特尔一起，针对腾讯云向量数据库常用的计算库进行了专门优化。包括：</p><p></p><p>FAISS：方案中针对其不同的索引提出了不同的优化方案，包括面向 IVF-FLAT 算法的单次读取和离散化两种优化思路，以及借助英特尔 AVX-512 加速 IVF-PQFastScan 算法和 IVF-SQ 索引的优化方案；</p><p></p><p>HNSWlib：方案借助英特尔 AVX-512，对 HNSWlib 的向量检索性能进行加速。同时方案也针对增删数据后的性能和召回率抖动的问题进行了专向优化，使 HNSWlib 的性能和召回率可以保持较平稳状态。</p><p></p><p>英特尔还为腾讯云向量数据库提供了英特尔 FMAL 加速库。暴力搜索在海量向量数据场景非常常用，但这一场景对算力需求非常高。作为针对向量暴力搜索场景开发的算法库，英特尔 FMAL 在英特尔 AVX-512 和英特尔 AMX 的加持下，能对相似度计算进行加速并提供了相似度计算和 top-K 查询的 API 接口，英特尔 AMX 还能帮助英特尔 FMAL 对 INT8 数据类型提供更好的性能输出。同时，英特尔 FMAL 还能在多线程并发下对处理器资源进行合理调配，以便让用户充分挖掘最新处理器所具备的多核心优势。除此之外，加速库也提供了对内存的非一致内存访问架构优化和缓存数据对齐功能，这些都进一步提升了腾讯云向量数据库的性能。</p><p></p><p>为验证第五代英特尔至强可扩展处理器基于英特尔 AVX-512 及英特尔 AMX 为腾讯云向量数据库中向量检索任务提供的助力，腾讯云与英特尔携手开展了验证测试，测试分为两个场景：第一个场景中，使用英特尔 AVX-512 优化后，使用 IVF-PQFastscan 算法执行向量检索时的 QPS 性能相比基于第三代至强可扩展处理器的基准组提升了高达 230%。第二个场景中，同样使用第五代至强可扩展处理器的算力平台上，使用英特尔 AMX 加速数据格式为 INT8 的测试场景对比使用英特尔 AVX-512 加速数据格式为 FP32 的测试场景，性能提升高达 5.8 倍之多，显示英特尔 AMX 可以进一步大幅提升 INT8 数据格式下的腾讯云向量数据库向量检索效率。</p><p></p><p>以上实测对比显示，领域加速技术已经成为新一代处理器大幅增强 AI 等创新应用场景性能的秘籍。值得一提的是，英特尔的加速技术不仅可以显著提升数据库吞吐量，还可以有效降低查询延迟，缩短每个查询需要的时间。除了复杂度较高的向量计算外，腾讯云向量数据库同时还提供了 AI 套件实现一站式文档检索解决方案， 包含自动化文档解析、信息补充、向量化、内容检索等能力，并拥有丰富的可配置项。向量数据库的应用场景还会涉及到不同的集合大小、维度数、数据类型、查询召回精度、延迟的要求， 这对 CPU 通用算力、加速算力、访存性能各项综合能力都提出了更高更快的要求。&nbsp;而英特尔第五代至强处理器的单核通用算力、内存速度和带宽、L3 缓存等指标相比此前的至强都有了广泛的提升，以满足向量数据库在不同场景下的算力要求，结合极强的领域加速能力，助力向量数据库打通性能瓶颈，为大模型的高效训练和推理工作奠定坚实基础。</p><p></p><p>在软件优化层面，英特尔为客户提供了模块化的代码优化方案，非常方便客户根据自身的实际场景快速集成，已经有一些客户将这些优化代码成功集成到自身产品中，给出了良好的反馈。</p><p></p><p></p><h1>计算创新，为生成式 AI 时代构筑算力底座</h1><p></p><p></p><p>向量数据库并不是领域加速计算大展身手的唯一场景。事实上，英特尔第五代至强可扩展处理器就凭借英特尔 AVX-512 和英特尔 AMX，在诸多科学计算、AI 推理、AI 训练等场景中取得了非常优秀的表现。例如，某互联网厂商的实际测试中，第五代至强可扩展处理器的 AI 推理和训练性能相比上代就有 40% 提升，HPC 应用则有 39% 提升。如果与未引入上述加速技术的第三代至强可扩展处理器相比，第五代至强可扩展处理器的 AI 推理性能最高提升可达 14 倍之多。</p><p></p><p>腾讯云与英特尔公司长期以来一直在软硬件优化方面有着深度合作，腾讯云也在与英特尔携手将更多先进计算产品和技术应用到 AI 等领域中，在竞争激烈的市场中取得更为领先的优势。与腾讯云类似，还有更多企业都在与英特尔公司展开类似合作，不仅利用英特尔 AVX-512 和英特尔 AMX 加速技术提升 AI 场景效能，还会应用英特尔 TDX 机密计算技术来改善 AI 大模型云端部署时的企业敏感数据安全性，或者利用第五代至强可扩展处理器在内存扩展性方面的优势增强智能推荐等场景的性能，等等。英特尔公司在软件生态层面的深耕成果，以及英特尔公司与各家客户的深度合作，也帮助这些企业以更低的资源投入就能快速享受到硬件新特性的收益。</p><p></p><p>正是包括英特尔 AVX-512 和英特尔 AMX 在内的一系列计算创新技术，让 CPU 在生成式 AI 大规模普及的时代依旧能够一马当先，为企业带来显著的生产力提升和竞争优势。可以认为，引入领域加速技术等创新的 CPU 在生成式 AI 浪潮中仍将肩负基础算力底座的重任，发挥不可或缺的作用。在 CPU 的创新探索支持下，我们也将看到生成式 AI 应用更快走入社会生产生活，在各行各业中焕发新的光彩。</p><p></p><p><img src="https://static001.geekbang.org/infoq/39/39f050a47d8fab4d566e017cb64f0e06.png" /></p><p></p><p>第五代英特尔®至强®可扩展处理器助腾讯云向量数据库成为大模型时代数据中枢，点击下方链接，即刻查看详情！</p><p></p><p>【阅读原文】：<a href="https://www.intel.cn/content/www/cn/zh/customer-spotlight/cases/tencent-cloud-vector-db-data-hub-era-big-models.html?wapkw=%E5%90%91%E9%87%8F?cid=soc&amp;source=Wechat&amp;article_id=5252">https://www.intel.cn/content/www/cn/zh/customer-spotlight/cases/tencent-cloud-vector-db-data-hub-era-big-models.html?wapkw=%E5%90%91%E9%87%8F?cid=soc&amp;source=Wechat&amp;article_id=5252</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/AqRLxU7726Wt5c1iJkNf</id>
            <title>“千帆杯”第一期赛题公布！龙年头彩竟藏在“游乐场”</title>
            <link>https://www.infoq.cn/article/AqRLxU7726Wt5c1iJkNf</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/AqRLxU7726Wt5c1iJkNf</guid>
            <pubDate></pubDate>
            <updated>Fri, 02 Feb 2024 03:18:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 春节, 主题乐园, 排队, AI应用
<br>
<br>
总结: 春节期间，主题乐园成为人们过节的热门选择，但巨大的客流量也带来了排队等待的问题。主题乐园通过一些设计来减缓游客排队焦虑，但并不能有效减少排队时间。然而，随着AI技术的发展，AI应用可以帮助解决主题乐园排队效率问题，提供个性化的游玩路线规划和优化运营策略的决策支持。百度智能云举办的千帆杯AI原生应用开发挑战赛，鼓励开发者利用AI能力打造一款实用的“游乐场排队规划助手”，帮助游客更好地了解排队情况，获得最佳体验。 </div>
                        <hr>
                    
                    <p>提到“春节”，你首先会想到什么？热气腾腾的年夜饭，大门前福气满满的春联，摩肩接踵的车站，还是一眼看不到排队尽头的游玩景点？</p><p></p><p>不知从什么时候起，各大主题乐园不再只是为孩子打造的童话世界，也俨然成为了成年人的理想国。无数的成年人换上霍格沃茨制服，挑选自己的魔杖，逃离“麻瓜世界”，前往环球影城；许多成年的迪士尼粉丝，也蜂拥至上海迪士尼乐园，在全球首个“疯狂动物城主题园区”亦或是香港迪士尼乐园的全球首个“冰雪奇缘主题园区”，寻找自己的快乐。</p><p></p><p>春节将至，热门主题乐园也会成为大家过节的经典选项，巨大的客流量，也意味着排队，将为主题乐园的游客体验和运营效率带来新一轮挑战。</p><p></p><p></p><h1>主题乐园的效率设计</h1><p></p><p></p><p>事实上，极具人气的主题乐园都有着自己独到的效率设计。在排队时，许多主题乐园都会为游客提供排队时长的提示牌，帮助游客做好预期管理；还会在惊险刺激的项目下设置巨大的电子屏直播正在游玩的游客画面，提升期待值；同时，在一些热门项目的等候区主题乐园还会设计一些精彩的故事和精美的绘画，让游客从排队的状态中脱离出来，减缓排队焦虑。</p><p></p><p>很明显，爆火的主题乐园们在解决游客排队问题时主要是通过一些设计让大多数游客失去了排队的时间概念，并不能有效减少游客排队等待时间。即使围绕排队本身所提供的辅助工具也仅仅能告诉游客园区地图上某个项目的预估排队时间。但这样还远远不够。</p><p></p><p></p><h1>来自未来的“时间魔法”</h1><p></p><p></p><p>随着大模型技术的飞速发展，把复杂功能交给&nbsp;AI，让用户更加专注于创作和创意，AI原生应用正在为产业效率带来新的可能。</p><p></p><p>为此，百度智能云以“创意无限·生成未来”为主题，发起了千帆杯·AI原生应用开发挑战赛。</p><p></p><p>第一期赛题将聚焦春节假期游乐园排队效率问题，鼓励开发者利用&nbsp;AI&nbsp;能力施展“时间魔法”，打造一款具有实用性的“游乐场排队规划助手”，帮助游客更好地了解乐园的排队情况，设计个性化的游玩路线，在有限的时间内获得最“High”的体验，同时为管理者提供优化运营策略的决策支持。</p><p></p><p>本期挑战中，官方将为开发者们提供环球影城、上海迪士尼、香港迪士尼、广州长隆&nbsp;4&nbsp;个热门游乐场地图，地图中将标注各项目的排队和游玩时间，以及不同体验维度的推荐指数。</p><p></p><p>百度智能云千帆&nbsp;AppBuilder&nbsp;将会在这轮赛题中成为开发者们重构应用的“智能助手”，为开发者提供专业、便捷的&nbsp;AI开发套件和资源环境。</p><p></p><p>AppBuilder作为目前国内唯一全面开放的具备代码规划与执行能力的平台，将框架和组件都做成了可扩展和可拼接的形式，以期给予&nbsp;AI&nbsp;应用开发者更多的选择和自由度。这就意味着每位开发者都可以利用AppBuilder来基于自然语言构建自己的“程序员”，实现单人成团，一个人就能成为一支队伍。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/eb/66/eb3739ae4504bd9a6e5868713834a866.png" /></p><p></p><p>AI原生时代，千千万万的开发者是促进AI原生应用爆发的中坚力量。本次千帆杯AI原生应用挑战赛与CSDN、51CTO、DataWhale、InfoQ、IT168、机器之心、思否等国内专业的开发者社区和人工智能媒体深度合作，将对大赛进行持续追踪，欢迎各位开发者关注~</p><p></p><p>第一期赛题已经出炉</p><p></p><p>“游乐场”的竞技舞台已经搭建完毕</p><p></p><p>期待各位在千帆杯AI原生应用开发挑战赛中</p><p></p><p>大展身手</p><p></p><p>探寻藏在“游乐场”中的10万大奖</p><p></p><p>欢迎报名参赛</p><p></p><p>阅读原文了解详细赛题说明</p><p></p><p>【阅读原文】：<a href="https://cloud.baidu.com/qianfandev/topic/268465">https://cloud.baidu.com/qianfandev/topic/268465</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0zAbBRGf1euyag3FQuqt</id>
            <title>数百万小时训练，6秒音频即可完成音色复刻！效果不输 ElevenLabs 和 OpenAI 的 MiniMax 语音大模型能用来做什么？</title>
            <link>https://www.infoq.cn/article/0zAbBRGf1euyag3FQuqt</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0zAbBRGf1euyag3FQuqt</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 09:27:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 华尔街, 万圣节指标, 择时投资策略, 羊群效应
<br>
<br>
总结: 华尔街有一个著名效应，叫做「万圣节指标」。这是一种择时投资策略，即在每年万圣节前后买入股票，持有整个冬季，然后在第二年的5月1日卖出。数据显示，这种策略的收益率要高于其他时间段的持有。这种差异可能是因为羊群效应和自证预言的影响。 </div>
                        <hr>
                    
                    <p></p><h2>从 0 到 1 的 MiniMax 语音大模型</h2><p></p><p></p><p>2023 年 11 月，MiniMax 发布语音大模型 abab-speech-01。从 11 月至今，共有超过 400 家企业用户接入我们的语音大模型。</p><p></p><p>在实际应用中，来自各行各业的用户给我们反馈了很多好的建议和想法。例如，在复刻有声书场景下，市面上没有可以批量、快速生成多角色音频的解决方案；在直播电商等注重互动性的场景中，各家现有语音能力仍无法做到实时，在生成语音的过程中仍需一定的等待时间，非常影响用户体验；在教学场景中，模型碰到特殊字词或者多音字的情况，时常存在发音不准确的问题。</p><p></p><p>为了给用户带来更加高效、丰富和真实的语音定制体验，我们不断迭代 MiniMax 语音大模型，并基于用户高优需求新增语音 API 接口，并上线了多个产品功能。MiniMax 是目前第一个开放多角色配音商用接口的公司。</p><p></p><p>在模型基础能力上，我们的语音模型对长达数百万小时的高质量音频数据进行训练，基于它的训练结果，仅用 6 秒的音频就能完成音色复刻，基于文本生成语音的字错率低至万分之五，已达到全球顶尖水平。</p><p></p><p>针对用户的高优需求，我们新增了以下产品功能：</p><p></p><p>三个 API 接口：多角色音频生成 API、文本角色分类 API 和快速复刻 API，帮助用户自主批量生成、克隆多角色音频；多语种能力、字典和间隔时长控制，满足用户丰富的定制化需求，提升教学场景体验T2A Stream （流式语音输出） 实现生成与输出的同步，减少用户在直播、对话等场景的等待时间。</p><p></p><p>为了让更多用户体验、使用我们的技术，我们在价格上也做出了调整：T2A Pro、T2A、T2A Stream 等价格下调为原先的一半，由 10 元 / 万字符降至 5 元 / 万字符。</p><p></p><p>具体功能价格调整见下表：</p><p></p><p><img src="https://static001.geekbang.org/infoq/f8/f8b578605342dddc0a7732b7e930449c.png" /></p><p></p><p></p><h2>声音小剧场</h2><p></p><p></p><p>由于语音模型没有公开的测评集，衡量一个语音模型到底怎么样主要依靠几个比较主观的评判标准，例如：自然度、相似度，可懂度和情感表现等。以下是几个基于我们语音大模型生成、复刻的一些语音效果。大家可以听听看，欢迎拍砖：）</p><p></p><p></p><h3>01 中英文夹杂读着毫无压力</h3><p></p><p></p><p>文本：</p><p>哎，你说你特别想念某个东西，可以说"I really miss it a lot" 或者"I'm missing it terribly." 这样表达出你的感情。有什么特别想念的嘛？想聊聊吗？</p><p></p><p>声音 1（明杰）：</p><p></p><p>声音 2（晨曦）：</p><p></p><p></p><p>声音 3（祁辰）：</p><p></p><p></p><p></p><h3>02 跨语种复刻，比原声更自然</h3><p></p><p></p><p>文本：</p><p>别担心，犯错是学习的一部分，下次你会做得更好的。Don't worry, making mistakes is part of learning. You'll do better next time.</p><p></p><p>原声音频（童声）：</p><p></p><p>复刻音频（中 + 英）：</p><p></p><p>只用中文原声，也可以复刻出他们讲中、英、日、韩等多种语言的声音：</p><p>韩语：</p><p></p><p>日语：</p><p></p><p></p><p></p><h3>03 AI 嬛嬛和四爷，有没有甄嬛十级学者来检验一下效果？</h3><p></p><p></p><p></p><p></p><p></p><p></p><h3>04 多音字绕口令也难不倒！</h3><p></p><p>真人都不一定能读准的多音字绕口令，我们的语音模型可以：）出现多音字的绕口令对语音模型理解上下文提出了很高要求。</p><p></p><p>“人要是行，干一行，行一行，一行行，行行行，行行行，干哪行都行”</p><p></p><p></p><p></p><p></p><h3>05 实时语音通话，跟小海螺打电话吧</h3><p></p><p></p><p>MiniMax 不仅为企业用户和开发者提供语音相关的 API，也为普通用户打磨了多款含有语音功能的产品。例如，我们在 AI 助手海螺问问上线了实时语音通话功能——无论你遇到什么问题，都可以随时打电话给小海螺，就像在和朋友聊天一样轻松、自然。小海螺的反应比 ChatGPT 的语音功能还快哦，快来体验一下吧！</p><p></p><p></p><p></p><p></p><h3>06 唱 AI 嘻哈</h3><p></p><p></p><p>节奏感强、唱腔复杂的饶舌说唱，我们的模型也能够超酷演绎。</p><p>想和 AI battle 说唱的朋友可以打开链接尝试：</p><p><a href="https://m.xingyeai.com/tag/2760001">https://m.xingyeai.com/tag/2760001</a>"</p><p></p><p></p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ttMK49Sq76u12zj1SzIV</id>
            <title>性能接近GPT-4，Mistral-Medium遭泄露？CEO最新回应来了......</title>
            <link>https://www.infoq.cn/article/ttMK49Sq76u12zj1SzIV</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ttMK49Sq76u12zj1SzIV</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 08:13:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 开源模型疑似泄露, Mistral-Medium模型, Miqu, HuggingFace
<br>
<br>
总结: 近日，关于"Mistral-Medium 模型泄露"的消息引起了关注。一位名为"Miqu Dev"的用户在开源 AI 模型和代码共享平台HuggingFace上发布了一个名为"miqu-1-70b"的新的开源大语言模型。该模型与Mistral AI公司正在研发的Mistral Medium模型的"提示格式"和用户交互方式相同。有人猜测miqu-1-70b可能是MistralAI模型的Medium版本。测试显示miqu-1-70b在德语翻译和跨语言理解能力方面表现出色。然而，与Mixtral-8x7B-Instruct-v0.1相比，miqu-1-70b的性能仍有待提升。 </div>
                        <hr>
                    
                    <p></p><h2>开源模型疑似泄露，开发者纷纷下场测试</h2><p></p><p>&nbsp;</p><p>近日，一则关于“Mistral-Medium 模型泄露”的消息引起了大家的关注，该消息在Hacker News和X（原Twitter）上持续发酵。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/89/89aab1008554b6e25fb0bd71e9c02a03.jpeg" /></p><p></p><p>&nbsp;此消息之所以受到这么多关注，与一款叫做“Miqu”的神秘模型有关。</p><p>&nbsp;</p><p>1月28日左右，一位名为“Miqu Dev”的用户在开源 AI 模型和代码共享平台<a href="https://huggingface.co/miqudev/miqu-1-70b">HuggingFace 上发布了一组文件</a>"，这些文件共同构成了一个看似新的开源大语言模型，名为“miqu-1-70b”。</p><p>&nbsp;</p><p>开源地址：<a href="https://huggingface.co/miqudev/miqu-1-70b">https://huggingface.co/miqudev/miqu-1-70b</a>"</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/1f/1fb6232a05547d602936f41d64ab00aa.png" /></p><p></p><p>&nbsp;在Hugging Face平台的miqu-1-70b项目上，多条内容指出这款新的大语言模型的“提示格式”以及用户与其交互的方式与 Mistral AI公司正在研发中的Mistral Medium模型相同。同一天，4chan 上的一位匿名用户（可能是“Miqu Dev”）<a href="https://boards.4chan.org/g/thread/98696032#p98697258">在 4chan 上发布了 miqu-1-70b 文件的链接</a>"，该项目的受关注程度逐渐升高。</p><p>&nbsp;</p><p>模型放出后，有业内人士猜测，这个神秘泄露的miqu-1-70b可能就是MistralAI模型的Medium或者过往混合专家测试版本。</p><p>&nbsp;</p><p>一些人用户在X上分享了该模型的发现，以及该模型在常见 LLM 任务（通过称为基准的测试来衡量）上表现出的异常出色的性能，甚至接近了 OpenAI 的 GPT -4 在EQ 工作台上的表现。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/c7/c7d5fb572e8b0626b98b59e44cc73cda.png" /></p><p></p><p>有用户测试了这款神秘模型和Medium模型后表示：“尽管可能有些晚了，但现在我100%确信miqu与Perplexity Labs上可访问的Mistral-Medium模型是同一个模型。该用户还称：“它似乎知道标准谜题，但如果是恶作剧者，根本不可能将其调整为同样用俄语回答。”</p><p>&nbsp;</p><p>这款备受瞩目的生成式AI新星——miqu-1-70b自然要被拿来与各位前辈进行一番比较。有测试者用4项德国数据保护测试对这款神秘模型做了更深入的测试，方法如下：</p><p>&nbsp;</p><p>这里通过4项德国在线数据保护培训/考试，对这套新模型的表现加以验证。测试数据、问题及所有说明均为德语，而答题卡则为英语。这考察了模型的翻译能力和跨语言理解能力。在提供信息之前，测试者会用德语指示模型：接下来向你提供一些信息，请记住相关内容，并回答“确定”以确认已经理解其内容。这一步是为了测试模型的指令理解与遵循能力。在提供关于某个主题的全部信息之后，测试者会向模型提出测试问题。这是一套包含三个选项的多选题，但首题采用A/B/C选项，末题为X/Y/Z选项。每项考试包含4至6道题，测试流程总计18道选择题。根据模型给出的正确答案数量进行排名，先测试事先提供课程内容后的成绩，再测试没有提供信息下的盲答成绩（作为决胜局）。所有测试均单独运行，每次测试间会清除上下文，保证会话之间的记忆/状态不相互干扰。</p><p>&nbsp;</p><p>还进行了包括<a href="https://github.com/SillyTavern/SillyTavern">SillyTavern</a>"前端、<a href="https://github.com/LostRuins/koboldcpp">koboldcpp</a>"后端（对于GGUF模型）在内的其他测试，另外还预先设置确定性生成，以尽可能消除随机因素并进行有意义的模型间比较，也包括注明官方提示词格式。</p><p>&nbsp;</p><p>以下为详细注释、排名基础和其他评论与观察发现：</p><p><a href="https://huggingface.co/miqudev/miqu-1-70b">miqudev/miqu-1-70b</a>"&nbsp;GGUF Q5_K_M, 32K上下文, Mistral格式:</p><p>❌&nbsp;正确回答了4+4+4+5=17/18道选择题，而在盲答阶段，正确答案题为:&nbsp;4+3+1+5=13/18。</p><p>❌&nbsp;未能按照要求用“确定”来回应数据输入。</p><p>&nbsp;</p><p>经过了多项测试后，结果显示miqu-1-70b的效果着实不错，测试者出于个人猜测，miqu-1-70b可能是一套外泄的MistralAI概念验证旧模型，从开发次序来讲应该不会比Mixtral更晚。此外，测试者也表示，在测试过程中注意到了几个有趣的点，从这几个方面来看，miqu-1-70b跟Mixtral存在诸多相似：</p><p>&nbsp;</p><p>优秀的德语拼写与语法能力。支持双语，可在回复中添加翻译。能够为回复添加注释和评论。</p><p>&nbsp;</p><p>但测试者也表示，在测试中，miqu-1-70b仍无法与Mixtral-8x7B-Instruct-v0.1（4-bit）相媲美，不过性能仍比Mistral Small和Medium更好（亲自测试Medium时其表现相当糟糕，可能是API的问题）。但与测试者每天都在使用的Mixtral 8x7B Instruct相比，miqu也没有好太多。</p><p>&nbsp;</p><p>在这场miqu和Mistral Medium模型对比测试中，前阵子号称要干掉谷歌搜索的 Perplexity印度创始人Aravind Srinivas也在X上发表了自己的观点：</p><p>&nbsp;</p><p></p><blockquote>很多人问我Mistral的所有模型是否都基于Meta的Llama。特别是因为Mistral Medium在Perplexity Labs上的输出与miqu非常相似，而这种相似性是通过测试发现的。Mistral的CEO Arthur已经提供了一个清晰的解释，并确认这是一个来自早期访问客户的泄露。&nbsp;此外，Perplexity从未获得过Mistral Medium的权重访问权限。所以，当你在Labs上使用Mistral Medium时，我们只是将你的请求路由到Mistral支持的有效端点，而没有访问权重。泄露的权重实际上是量化版本，与NVIDIA TensorRT不兼容。&nbsp;此外，很多人在看到这个消息是本能地反应会认为Mistral不知道如何进行预训练，只是在LLama 2上构建。这是明显不真实的。Mistral 7b是一个由Mistral团队从头开始训练的模型，而Mistral 8x7b MoE也是通过使用他们自己的7b作为每个专家的初始化来训练的。所以很明显，这个团队知道如何从零开始训练自己的模型。Mistral Medium是从LLama后期训练的，可能是因为迫切需要一个接近GPT-4质量的API，以便早期客户使用。但是一个能够在计算和时间投入远少于Gemini Pro的情况下取得胜利的团队，现在他们有了更多的资金和计算资源，显然能够做到GPT-4级别的质量。&nbsp;当然，泄露是不好的。Mistral的胜利对社区来说是一件好事：无论是对学术界还是对初创公司。支持他们！</blockquote><p></p><p></p><h2>Mistral AI高层发声：是泄露了，但只是个旧版本</h2><p></p><p>在Mistral AI的新模型遭泄漏这一话题热度不断上涨之时，据外媒最新消息，Mistral AI联合创始人兼CEO&nbsp;Arthur Mensch 在 X 上澄清：</p><p>&nbsp;</p><p></p><blockquote>“一个我们早期客户的热情员工泄露了一个我们公开训练和发布的老模型的量化（带水印）版本。为了尽快与一些特定的客户开始合作，我们在获得整个集群访问权限后立即从Llama 2重新训练了这个模型——预训练在Mistral 7B发布的那一天完成。自那时以来，我们取得了很好的进展——敬请期待！”</blockquote><p></p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/ed/edbdac6347f53ab3b41ca879cd4bf3b7.jpeg" /></p><p></p><p>有趣的是，Mensch 并没有要求删除 HuggingFace 上的帖子，而是留下那些评论说发帖者“可能会遭到模型所属公司追责”的评论。</p><p></p><h2>Mistral AI创始团队成员均来自谷歌和Meta</h2><p></p><p>&nbsp;</p><p>Mistral AI是一家总部位于巴黎的欧洲公司，由 Arthur Mensch 和 Guillaume Lample以及 Timothée Lacroix于 2023 年 2 月联合创立，并于去年 12 月 10 日宣布筹集了 3.85 亿美元，仅半年多的时间，该公司估值近 20 亿美元。Mistral AI 在刚成立且没有任何产品时就已筹集了 1.05 亿美元。</p><p>&nbsp;</p><p>因此，它也成为继德国 Aleph Alpha 在去年11月筹集了 5 亿欧元之后，第二家筹集到如此多资金的欧洲人工智能初创公司。</p><p>&nbsp;</p><p>Mistral AI一直在研究如何提高模型性能，同时减少为实际用例部署llm所需的计算资源。Mistral 7B是他们创建的最小LLM，它为传统的Transformer架构带来了两个新概念，Group-Query Attention(GQA)和Sliding Window Attention(SWA)。这些组件加快了推理速度，减少了解码过程中的内存需求，从而实现了更高的吞吐量和处理更长的令牌序列的能力。</p><p>&nbsp;</p><p>Mistral AI 首席执行官 Arthur Mensch，31 岁，在 Google 人工智能实验室 DeepMind 工作了近三年。Mistral 的科学总监 Guillaume Lample 是 Facebook 母公司 Meta 在 2 月份推出的 LLaMA 语言模型的创建者之一。Timothée Lacroix 是 Mistral AI 的技术总监，也是 Meta 的研究员。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://twitter.com/Yampeleg/status/1751837962738827378">https://twitter.com/Yampeleg/status/1751837962738827378</a>"</p><p><a href="https://www.euronews.com/next/2023/12/11/french-ai-start-up-mistral-reaches-unicorn-status-marking-its-place-as-europes-rival-to-op">https://www.euronews.com/next/2023/12/11/french-ai-start-up-mistral-reaches-unicorn-status-marking-its-place-as-europes-rival-to-op</a>"</p><p><a href="https://analyticsindiamag.com/mistral-ai-challenges-dominance-of-openai-google-meta/">https://analyticsindiamag.com/mistral-ai-challenges-dominance-of-openai-google-meta/</a>"</p><p><a href="https://news.ycombinator.com/item?id=39175611">https://news.ycombinator.com/item?id=39175611</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Yl1jSOMx2TAzkqN0Phty</id>
            <title>金山云升级全栈云计算体系，全方位承接大模型应用</title>
            <link>https://www.infoq.cn/article/Yl1jSOMx2TAzkqN0Phty</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Yl1jSOMx2TAzkqN0Phty</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 08:09:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 云原生, AIGC, 大模型, 云计算
<br>
<br>
总结: 随着云原生、AIGC、大模型等新兴技术的迅速发展，智能化时代开启。云计算也正全面步入3.0时代，即云计算和人工智能深度融合的阶段。在这个阶段，人工智能技术成为云计算进一步释放潜力的核心推动力。金山云通过技术自研和升级，已初步建成人工智能时代民用领域全栈的云计算体系。 </div>
                        <hr>
                    
                    <p>随着云原生、AIGC、大模型等新兴技术的迅速发展，智能化时代开启。云计算也正全面步入3.0时代，即云计算和人工智能深度融合的阶段。在这个阶段，人工智能技术成为云计算进一步释放潜力的核心推动力。</p><p>&nbsp;</p><p>近日，金山云举办了「云+人工智能·时代新机遇」媒体沟通会，金山云副总裁钱一峰、金山云公有云产品中心负责人孙晓、金山云人工智能与大数据产品中心负责人徐寅斐在会上分享了金山云在基础能力、平台能力和模型服务等方面的技术历程和落地实践。通过技术自研和升级，金山云已初步建成人工智能时代民用领域全栈的云计算体系。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/c7/c7a4ac5ef7a3b8f9f69f7f66f82246ec.png" /></p><p></p><h2>夯实基础，全面升级人工智能服务能力</h2><p></p><p>&nbsp;</p><p>全球数字化浪潮风起云涌，云计算作为数字经济重要的支撑基础，正发挥越来越重要的作用。其中，智能化作为云计算3.0时代的重要理念，已成为数字基础设施升级的重要驱动力。知名调研机构IDC于2023年发布的报告显示，未来5年，以大模型、生成式人工智能驱动的下一代人工智能有望带动整体云产业穿过下行阶段，重回高增长时代。</p><p>&nbsp;</p><p>顺应趋势，金山云深入探索前沿技术，围绕AIGC升级核心计算、存储、网络等产品，从多个维度全面拥抱人工智能。</p><p>&nbsp;</p><p>金山云副总裁钱一峰强调，在此次技术升级中，在网络上采用了目前业界流行的高性能RoCE网络。在成本方面，同样的集群如果采用IB网络，网络本身的成本几乎占整个算力群的一半，如果用RoCE网络，只占5%到10%，所以国内很多大厂都在往RoCE网络转，金山也是如此。</p><p>&nbsp;</p><p>在计算方面，金山云第七代云服务器X7搭载第四代英特尔®至强®可扩展处理器、支持英特尔® AMX原生加速能力，CPU性能较上代最高提升60%，内存升级至DDR5，频率较上一代性能提升50%。融合金山云自主创新的加速技术，云服务器X7可有效提升模型推理性能。</p><p>&nbsp;</p><p>根据数据从极热到极冷的不同热度，金山云对象存储已覆盖标准存储、低频存储、深度低频存储、归档存储、深度冷归档存储和全闪存储等不同访问热度的存储产品。其中，金山云对象存储KS3极速型最高可提供1Tbps/PB的兑付带宽，相较基于机械硬盘（HDD）的对象存储性能提升了上百倍，能为AIGC、存算分离和高性能计算等场景提供强有力的存储解决方案。</p><p>&nbsp;</p><p>在底层技术基础上，金山云此次升级还新增了金山轻舟智问以及一些合作的商业模型。</p><p>&nbsp;</p><p>一直以来，业内有一种观点认为一个大模型就解决所有问题，因为所有人都可以去调用大模型的API，但根据用户的反馈和场景需求来看，把所有问题都扔给一个大模型去解决是非常昂贵的，更经济的做法是将不同的场景用不同规模的模型去解决。将这些平台打包在一起，可以为客户提供一个综合性价比最高的解决方案。</p><p>&nbsp;</p><p>过去这一年，金山云一直在做两件事——夯实基础和做长长板。在云计算方上重点打磨四大基础能力：速度快、性能好、成本低、稳定易用；而做长长板找增量则体现在金山云在混合云、分布式云上的很多创新。</p><p>&nbsp;</p><p>具体而言，在在公有云的核心产品上第一要做到一切皆标准，提升用云效率。第二，要做到软硬结合，最大限度提升客户用云性价比。</p><p>&nbsp;</p><p>在混合云方面，客户拥有专属区或者专属集群，这既让客户拥有了云下的独立，又能让其享受云上的规模红利和弹性。</p><p>&nbsp;</p><p>此外，在Serverless化上也有了新进展。Serverless化过去是局限在算力层面，但随着存算分离场景的流行已经渗透到PaaS层面，如今MySQL也已经做到了Serverless化，另外也引入了开源向量数据库。</p><p>&nbsp;</p><p>面向大模型应用场景，金山云推出互信虚拟私有网络（简称“互信VPC”），解决模型厂商和应用厂商互信的问题。相对于标准VPC，互信VPC对进出VPC的通信行为有着更为严格的控制，帮助客户解决合规和互信问题。针对HTTP、HTTPS等应用层负载场景，公司推出应用型负载均衡ALB(Application Load Balancer)，单实例最大支持100万QPS。与云原生场景融合，客户在使用金山云容器服务KCE产品或自建K8S集群时，ALB都可作为Ingress部署，为业务提供网络流量调度服务。</p><p></p><h2>勇立潮头，做大模型助力者</h2><p></p><p>2023年被称为大模型“元年”。公开数据显示，从年初到年末，国内大模型数量超过200个。与“大模型”数量呈倍数级增长相反的是，真正被调用的大模型却相对较少。在本次大模型浪潮中，金山云坚持中立定位，充分发挥自身的底座和平台能力，做大模型的助力者。</p><p>&nbsp;</p><p>在洞察到模型供需方的痛点后，金山云于去年6月率先推出MaaS互信推理专区方案（以下简称“MaaS 1.0”），在大模型厂商、用户和金山云之间建立互信，以解决模型及数据的互信问题。本次沟通会上，金山云发布MaaS互信推理专区方案2.0（以下简称“MaaS 2.0”）。</p><p>&nbsp;</p><p>在MaaS 1.0基础上，MaaS互信推理专区方案2.0以金山云IaaS和PaaS为底座，可实现云上LangChain的一键部署，默认对接多个生态合作商业大模型和开源大模型。同时支持包括BGE、Bert等在内的Embedding模型，能无缝对接金山云全托管向量数据库Milvus，提供面向企业开发者的简单易用、安全可信的一站式推理应用部署平台。此外，MaaS 2.0支持通过标准化的API接口和Web前端界面，实现包括模型推理和知识库搭建的RAG大模型场景应用。为进一步增强云上运行的安全性，MaaS 2.0还提供容器服务加密镜像解决方案，依托金山云裸金属服务，实现在金山云容器服务中从镜像加密、加密镜像上传、解密镜像运行的全流程模型安全运行。</p><p>&nbsp;</p><p>同时，为满足行业客户的需求，金山云探索大模型时代企业赋能新机遇，围绕企业级知识助手场景制定了“一三一四”产品全景规划，即一套能力（金山云轻舟智问）、三个模型（行业语言模型、文本分片和Embedding模型）、一个平台（金山云瀚海平台）及四大功能（微调推理、数据加速、智能检索和文档智能），围绕生成式人工智能构建应用落地、模型训练微调、平台支撑的全栈能力。基于“一三一四”规划，金山云将分别针对应用型客户和平台型客户输出多项原子能力。目前，金山云轻舟智问知识助手产品已完成应用以及Embedding模型、多路召回算法和智能数据切片模型等核心技术的建设，计划优先在公共服务和法律场景落地。</p><p>&nbsp;</p><p>与人工智能的结合，给了云更多的想象空间，也给各行业都带来了新的生产力。随着技术的进一步完善和落地，云计算将迎来更强更久的生命力。金山云将持续围绕客户需求“练内功”，携手生态伙伴以差异化打法布局未来。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/t8F5gt0MRXcxr0ZWe9Ij</id>
            <title>刚上线就崩了？字节版GPTs征战国内市场：无需编码，快速创建AI聊天机器人</title>
            <link>https://www.infoq.cn/article/t8F5gt0MRXcxr0ZWe9Ij</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/t8F5gt0MRXcxr0ZWe9Ij</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 06:46:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 字节跳动, AI聊天机器人, Coze, 构建平台
<br>
<br>
总结: 字节跳动旗下AI聊天机器人构建平台Coze正式上线。Coze是一款应用程序编辑平台，用于开发下一代AI聊天机器人。用户可以在该平台上快速创建各类聊天机器人，并将其部署在不同社交媒体与消息应用中。 </div>
                        <hr>
                    
                    <p>2月1日，字节跳动旗下AI聊天机器人构建平台Coze国内版（中文名：扣子）正式上线。Coze是一款应用程序编辑平台，用于开发下一代AI聊天机器人。无论用户是否拥有编程经验，都可在该平台上快速创建各类聊天机器人，并将成果部署在不同社交媒体与消息应用当中。</p><p>&nbsp;</p><p>据悉，Coze 由字节跳动新成立的AI部门 Flow 开发，去年年末在海外先行上线。与海外版相比，国内版在功能上并无差异，只是推送渠道略有不同。</p><p>&nbsp;</p><p>有网友反馈，Coze 刚上线就崩了，InfoQ实测发现，Coze 创建界面长时间显示确认中，或许与刚上线流量过大有关。不过截至发稿前，Coze响应时间恢复了正常。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/19/19c3274341257b4c62176b23bce3f3ee.png" /></p><p></p><p>体验地址：</p><p><a href="https://www.coze.cn/home">https://www.coze.cn/home</a>"</p><p></p><h2>Coze背后神秘的AI部门Flow：由字节大模型领队牵头，聚焦AI应用层</h2><p></p><p>&nbsp;</p><p>据36kr 2023年11月末报道，字节跳动近期成立了一个新AI部门Flow，技术负责人为字节跳动技术副总裁洪定坤。有知情人士表示，Flow部门的业务带头人，为字节大模型团队的负责人朱文佳。</p><p>&nbsp;</p><p>公开信息显示，朱文佳是业界知名的架构师，曾担任百度搜索部主任架构师，是百度网页搜索部技术副总监杨震原手下的得力干将。2014年，杨震原受张一鸣邀约，离开百度入职字节跳动，现为字节跳动公司副总裁、火山引擎业务负责人。2015年，朱文佳加入字节跳动，被称为“今日头条里算法技术的Top3人选”，并在入职4年后担任今日头条CEO。2021年2月，朱文佳调任成为为Tik Tok产品技术负责人。</p><p>&nbsp;</p><p>据悉，朱文佳是字节跳动大模型业务中的“隐形领队”——是字节跳动语言大模型负责人与图像大模型团队负责人的间接和直接汇报对象。在内部人士眼中，朱文佳拥有“综合的工程和技术管理经验”。</p><p>&nbsp;</p><p>与朱文佳背景类似，洪定坤也曾在百度任职过，曾担任百度贴吧的技术经理。2014年，洪定坤加入字节跳动，现为字节跳动技术副总裁。</p><p>&nbsp;</p><p>据了解，Flow部门主要聚焦在AI应用层。在 Flow 此前发布的活水招聘帖中，Flow 称其是字节跳动旗下 AI 创新业务团队，已经在国内和海外分别上线豆包和 Cici 两款产品，还有多个 AI 相关创新产品在孵化中。</p><p>&nbsp;</p><p>其中，豆包属于AI 对话产品，Cici 与Coze都属于AI聊天机器人创建平台，可供用户创建和共享自己的聊天机器人。此外，字节跳动旗下的AI产品还包括AI聊天机器人创建平台ChitChop和AI角色创建与互动平台BagelBell，这两款产品分别由 POLIGON 和SPRING(SG)PTE.LTD.开发运营。Cici和CHitChop主要用于娱乐场景，提供基于虚构性格的浪漫伴侣型机器人，而Coze则提供可简化办公流程的工作机器人。</p><p>&nbsp;</p><p>Cici、Coze、ChitChopt和BagelBell这四款产品均在过去三个月内登陆海外，目前已经拥有数百万下载量。其中，只有Coze目前登陆了美国市场，其他三款均未向美国和欧洲市场开放。科技大厂通常会先在监管审查较弱的小型市场上测试产品，之后再逐步推向美国和欧盟。</p><p></p><p><img src="https://static001.geekbang.org/infoq/72/72e14a648302c9e10fbb2400dcab16e0.png" /></p><p></p><p>有网友表示自己非常看好Coze，一是2B产品对质量要求高，2C产品市场相对有容忍度，二是核心不在产品在流量，字节有地表最强流量能力，比如一键上架豆包。“字节聚焦AI应用非常聪明，基础模型差距很大，借‘基础能力’用流量起势，积累数据再反哺‘能力’，策略清晰可执行”，该用户在X上评论道。</p><p></p><p><img src="https://static001.geekbang.org/infoq/55/55c07e8586cc210fcd9921dd0bbcf758.png" /></p><p></p><p>根据Google Play商店的数据，Cici是四款中最受欢迎的应用产品，目前下载量已经超过1000万次。字节跳动并未公开用于支持这些产品的底层大语言模型。字节跳动发言人Jodi Seth表示，这些应用依赖于OpenAI的GPT技术，可通过微软Azure许可证进行访问。</p><p>&nbsp;</p><p>据Forbes报道，这四款字节跳动新应用均在隐私政策中包含警告（与字节跳动的其他应用保持一致），称这些应用可能与其他“我公司内部实体”共享用户信息。字节跳动发言人Seth也证实，字节跳动的中方员工可能会访问到应用内的用户数据，但将严格遵守公司的访问控制与审批流程。</p><p>&nbsp;</p><p>在测试对话中，Coze和Chitchop均就某些敏感话题给出比较完整的描述，但仍存在幻觉问题。但在注册账户测试Cici（与豆包App几乎一模一样）时，该应用屡屡报错，最终导致测试无法进行。</p><p>&nbsp;</p><p>有网友分析，字节跳动的这四款AI产品应该是不同团队间产品的 PK。对于素有“APP工厂”之称的字节跳动来说，四大产品“赛马”可以提升团队创新性，同时也可看出字节跳动对AI领域的重视程度。</p><p></p><h2>如何用扣子快速创建AI聊天机器人？</h2><p></p><p>&nbsp;</p><p>自去年第四季度在海外取得很好的成绩之后，字节跳动今天正式推出“Coze 扣子”AI Bot 开发平台。</p><p></p><h4>Coze具备哪些功能？</h4><p></p><p>&nbsp;</p><p>无限拓展的能力集</p><p>&nbsp;</p><p>扣子集成了丰富的插件工具，可以极大地拓展 Bot 的能力边界。</p><p>&nbsp;</p><p>内置插件：目前平台已经集成了超过 60 款各类型的插件，包括资讯阅读、旅游出行、效率办公、图片理解等 API 及多模态模型。 你可以直接将这些插件添加到 Bot 中，丰富 Bot 能力。例如使用新闻插件，打造一个可以播报最新时事新闻的 AI 新闻播音员。自定义插件：扣子平台也支持创建自定义插件。 你可以将已有的 API 能力通过参数配置的方式快速创建一个插件让 Bot 调用。</p><p>&nbsp;</p><p>丰富的数据源</p><p>&nbsp;</p><p>扣子提供了简单易用的知识库功能来管理和存储数据，支持 Bot 与你自己的数据进行交互。无论是内容量巨大的本地文件还是某个网站的实时信息，都可以上传到知识库中。这样，Bot 就可以使用知识库中的内容回答问题了。</p><p>&nbsp;</p><p>内容格式：知识库支持添加文本格式、表格格式的数据。内容上传： 你可以将本地 TXT、PDF、DOCX、Excel、CXV 格式的文档上传至知识库，也可以基于 URL 获取在线网页内容和 API JSON 数据。同时支持直接在知识库内添加自定义数据。</p><p>&nbsp;</p><p>持久化的记忆能力</p><p>&nbsp;</p><p>扣子提供了方便 AI 交互的数据库记忆能力，可持久记住用户对话的重要参数或内容。</p><p>&nbsp;</p><p>例如，创建一个数据库来记录阅读笔记，包括书名、阅读进度和个人注释。有了数据库，Bot 就可以通过查询数据库中的数据来提供更准确的答案。</p><p>&nbsp;</p><p>灵活的工作流设计</p><p>&nbsp;</p><p>扣子的工作流功能可以用来处理逻辑复杂，且有较高稳定性要求的任务流。扣子提供了大量灵活可组合的节点包括大语言模型 LLM、自定义代码、判断逻辑等，无论你是否有编程基础，都可以通过拖拉拽的方式快速搭建一个工作流，例如：</p><p>&nbsp;</p><p>创建一个搜集电影评论的工作流，快速查看一部最新电影的评论与评分。创建一个撰写行业研究报告的工作流，让 Bot 写一份 20 页的报告。</p><p></p><h4>Coze快速上手教程</h4><p></p><p>&nbsp;</p><p>Coze的主页面非常简洁，点击“创建Bot”即可创建属于自己的Coze机器人。可以自行设置机器人的名称以及功能介绍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f8/f843506a8e644ea731a46e0f30b2754c.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/fe/fe0cf437234b93b5d5a98cef53dd7002.png" /></p><p></p><p>创建好后，可以编辑机器人提示词，比如可以描述机器人的角色、技能、约束条件等内容以定义机器人的预期行为。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/c8/c8db6d4356cad6f7ee2509a5d5284e47.png" /></p><p></p><p>由于AI机器人本身无法直接访问互联网，因此需要配合某些工具来获取数据或执行网上操作。</p><p>可以将各类插件工具添加到机器人内以扩展其功能，具体包括必应搜索、ByteArtist、图片理解、头条搜索等多个插件。</p><p></p><p><img src="https://static001.geekbang.org/infoq/95/95ba71088036d5e85cf0e19d0511d386.png" /></p><p></p><p>创建好机器人后，可以选择发布平台，国内版Coze发布平台包括飞书、微信。</p><p></p><p><img src="https://static001.geekbang.org/infoq/06/060c86c5e3969b6083fed240956a9871.png" /></p><p></p><p>Coze还提供了Bot 商店，包含工具、娱乐、生活方式等多个类目，可以看到其他开发者是如何创建机器人的，并从中找寻灵感，甚至可以基于该机器人创建一个副本，再进行个性化调整。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d9/d9cbfe0f35d63a957599be08e7776f3f.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/c7/c780d27d43c755e4e1a2af5a77427ea1.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/4e/4e3572701727592d19b2d1e3071b24e3.png" /></p><p></p><p>参考链接：</p><p><a href="https://www.coze.cn/docs/guides/welcome">https://www.coze.c</a>"<a href="https://www.coze.cn/docs/guides/welcome">n</a>"<a href="https://www.coze.cn/docs/guides/welcome">/docs/</a>"<a href="https://www.coze.cn/docs/guides/welcome">guides/</a>"<a href="https://www.coze.cn/docs/guides/welcome">welcome</a>"</p><p><a href="https://www.forbes.com/sites/emilybaker-white/2024/01/16/tiktok-bytedance-ai-chatbots-openai/?sh=4bb1b0fba240">https://www.forbes.com/sites/emilybaker-white/2024/01/16/tiktok-bytedance-ai-chatbots-openai/?sh=4bb1b0fba240</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/6rOYTrA6LToGe35zBtUT</id>
            <title>ALL IN AIGC 新时代，共探行业变革之路｜InfoQ 合作伙伴年度盛典</title>
            <link>https://www.infoq.cn/article/6rOYTrA6LToGe35zBtUT</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/6rOYTrA6LToGe35zBtUT</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 04:03:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: InfoQ, 合作伙伴年会, 开发者生态, AI创新
<br>
<br>
总结: InfoQ极客传媒与合作伙伴们共同探索和实践技术和新领域带来的无限可能，共建优质内容、扶持创新企业/业务、优化客户体验是他们的承诺。他们致力于打造一个强大的开发者生态，推动科技创新和产业发展。此外，他们与开放原子开源基金会达成战略合作，共同推动中国开源事业的发展。 </div>
                        <hr>
                    
                    <p>2024年 1 月 31 日，InfoQ 极客传媒合作伙伴年度盛典在前门 blue note 圆满落幕。时隔三年，InfoQ合作伙伴年会回归线下，本届合作伙伴年会围绕“有被 Q 到”这个主题精彩展开，Q 代表着 Quality（坚守品质）、Quick（敏锐、速度和专业能力）、Question（不断求知、探索、创新），InfoQ 全体同学“Cue ”各位合作伙伴相聚一堂，分享这一年的感悟与收获，共话技术前沿与商业创新。</p><p></p><h2>共筑开发者生态 引领AI创新</h2><p></p><p>回首2023年，AI技术的璀璨光芒照亮了各行各业，带来了翻天覆地的变革，也催生了无数的创新机遇。InfoQ也在这股变革的潮流中与时俱进，与合作伙伴们紧密携手，共同探索和实践着技术和新领域带来的无限可能。</p><p>&nbsp;</p><p>极客邦科技创始人 &amp;CEO 霍太稳（Kevin）在新年致辞中表示，在生成式AI时代，极客邦科技在积极探索和改变，未来将从共建优质内容、扶持创新企业/业务、保证客户体验三方面进行创新，也是极客邦科技对朋友们做出的承诺。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/2d/2ddc620a200d2301dcf3630ad9e27e49.jpeg" /></p><p>极客邦科技创始人 &amp;CEO 霍太稳（Kevin）</p><p></p><p>共建优质内容：从 Demo&nbsp;到产品化再到产业化，帮助企业培育开发者生态</p><p>过去一年，InfoQ见证并报道了基础大模型、行业大模型在金融、制造、泛互等领域的探索和实践。未来一年，我们的内容、会议将继续瞄准“AI+场景化”的这个方向，只要是这个主题下面对开发者、对社区有价值的内容，我们坚决欢迎朋友们与我们共建。</p><p></p><p>扶持创新企业/业务：从流量到内容到出海，借助AI全方位帮助早期业务/企业成长</p><p>过去几年，InfoQ极客传媒在推动数字化转型和创新创业方面取得了显著成果。与100+生态伙伴共同推出【数字化转型专区】，展示了中国数字化转型的创新技术和数字化人才培养成果。发布了创新创业“成长计划”，为创新企业提供媒体传播、展示区和CXO群体链接等支持。</p><p>&nbsp;</p><p>此外，InfoQ极客传媒还与NVIDIA初创加速计划合作，为TGO会员企业提供创业指导。去年，InfoQ成功助力云器科技的产品技术发布，并受到广泛关注。同时，TGO鲲鹏会也在全球多地举办活动，为会员提供出海业务指导和合规建议。</p><p>&nbsp;</p><p>新的一年，InfoQ极客传媒将结合AI技术，为创新企业/业务提供更加强大的支持。推动技术创新和业务拓展，为合作伙伴和会员提供更优质的服务和支持。</p><p></p><p>优化客户体验：服务到客户满意为止，合同期止不满意，持续服务</p><p>极客邦科技始终坚守一个原则：客户体验坚决放在首位。霍太稳也向现场的合作伙伴郑重承诺，在合同期限内，如对服务有任何不满或建议，极客邦科技将全力以赴，持续优化服务内容和质量，直至达到客户满意为止。</p><p>&nbsp;</p><p>霍太稳表示，极客邦科技第一季度的主题是“一具体，就深刻”。2024年，形势可能依然严峻，希望能够与合作伙伴携手共同实现“全面进化”，最终共同实现增长！</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/c5/c54bc64eb49209e7758382a16ccd1a60.png" /></p><p></p><h2>汇聚市场力量 驱动技术升级</h2><p></p><p>在年会现场，极客邦科技CGO汪丹进行 2024 年 InfoQ 极客传媒战略发布重点强调了开发者生态的重要性以及从科技创新到产业赋能的链接力量。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a1/a14d93608007e43c72dc42a2ea1fc3e2.jpeg" /></p><p>极客邦科技CGO 汪丹</p><p>&nbsp;</p><p>汪丹表示，InfoQ极客传媒一直致力于打造一个强大的开发者生态，通过科技创新推动产业发展。InfoQ极客传媒始终坚持以客户需求为导向，通过不断创新和演进，为企业提供更加贴合实际需求的解决方案和产品。</p><p>&nbsp;</p><p>汪丹介绍了InfoQ极客传媒在内容、会议、社交、企业服务等方面的全新布局。InfoQ极客传媒将通过打造大模型领航者、实践案例采访等内容产品，为开发者提供更加丰富、实用的学习资源。</p><p>&nbsp;</p><p>通过举办智能软件开发生态展、通用人工智能开发与应用生态展等会议活动，为开发者提供更多的交流和展示平台。通过打造opentalk区、社交区等社交场景，实现企业和开发者的双向奔赴。同时，InfoQ极客传媒还将深入千行百业，将优秀实践带到传统企业，推动产业的数字化转型和升级。</p><p>&nbsp;</p><p>面向2024年，InfoQ极客传媒将继续加大对国内大模型领域发展的跟踪和研究力度，为业内提供更多的选型依据和决策支持。并将积极拓展游学业务，坚守“不卷、不焦虑”的原则，帮助中国企业走向海外，拓展国际市场。为开发者和企业提供更加优质、高效的服务和支持，与更多的合作伙伴携手共进，共同推动行业的繁荣发展。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/c9/c9fa3ac5e7c53ba599c072deeaba9f0c.jpeg" /></p><p>InfoQ极客传媒 2024 战略发布</p><p></p><h2>达成战略合作 谱写开源新章</h2><p></p><p>回顾InfoQ的整个发展过程，始终与许多开源社区紧密联系，在开放原子开源基金会成立后，InfoQ便成为银牌赞助商，以实际行动表达对开源社区的支持与感恩。为了更深层次地孕育与发展繁荣的开源生态体系，在本次年会盛典中，开放原子开源基金会与极客邦科技在 InfoQ 合作伙伴年会上举行战略合作发布仪式。</p><p>&nbsp;</p><p>开放原子开源基金会秘书长冯冠霖与霍太稳共同按下战略启动按钮，宣布正式达成战略合作，共同推动中国开源事业的发展。双方将共绘战略合作宏图，坚定致力于开源技术的普及推广和开源知识的深度传播，协同提升双方品牌在业界的声望和影响力，全力构建全面创新的开发者生态系统以及一套卓越的人才培养机制，重点推动AtomGit平台在全球范围内的广泛应用与深化拓展，促进开源技术生态可持续发展，为全球数字经济的发展注入新活力。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/a6/a6633b2470e49027d96ca2e7b0902949.jpeg" /></p><p>开放原子开源基金会与极客邦科技达成战略合作</p><p>开放原子开源基金会秘书长冯冠霖（图左）与极客邦科技创始人 &amp;CEO 霍太稳（图右）</p><p>&nbsp;</p><p>此外，在启动仪式后，开放原子开源基金会开源大赛组委会办公室高级运营官曹海清对“开放原子开源大赛”进行了宣讲。该大赛旨在激发全球开发者的创新潜力，挖掘并提炼开源领域的宝贵精华。双方的战略合作，将有力驱动全球数字经济领域的深化拓展与升级转型，并在开源人才的孕育、选拔与输送方面发挥关键作用，有效促进全球范围内技术交流与协作的国际化进程。对此，InfoQ侧表示，“将继续发挥其全球性科技媒体品牌的影响力，为合作伙伴提供更加优质、高效的服务，推动全球科技创新与发展。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/25/253f6861b080ed0ffce0e2357db7700d.jpeg" /></p><p>开放原子开源基金会开源大赛组委会办公室高级运营官 曹海清</p><p></p><h2>致敬荣耀时刻 见证科技星熠</h2><p></p><p>在本次盛典上，InfoQ 极客传媒面向合作伙伴颁布了“2023年度新锐技术品牌奖”、“2023年度技术影响力引领品牌”、“2023年度技术传播创新案例”、“2023年度技术生态构建奖”、“2023年度合作伙伴奖”、“2023年度全球科技领导力推动者”奖项。</p><p>&nbsp;</p><p>以下是具体获奖企业及个人名单（排名不分先后）：</p><p><img src="https://static001.geekbang.org/infoq/76/76690345bf1ee08c4cb324a440138586.png" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/b9/b97c094e33e1660cbfb823f5aeafc4ca.png" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ec5f86db84bb6e8ed515e587a234b569.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/10/10c55373c4baa97492a1f14eb7accd00.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ec4c6b0114b4b7bfadad332fb170cc32.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/9d/9d12732a2644d0f6e2d4629206f51e09.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/70/7095485c60758114021788988f3d4e15.jpeg" /></p><p></p><h2>行业变革时刻 趋势发展策略</h2><p></p><p>2023 年AI 技术的突破直接拉动了自动驾驶、机器人和生成式 AI 的融资增长，数据和算力作为训练大模型的底座也受到了越来越多的关注和讨论。随着技术的不断进步，企业需要紧跟时代步伐，积极拥抱 AI 与数字化，以实现更高效、更智能的业务运营。</p><p>&nbsp;</p><p>InfoQ研究中心研究总监兼首席分析师姜昕蔚带来了主题为《2023年中国软件技术洞察及2024年趋势预测》的主旨演讲，为现场观众深入解析了中国软件技术的发展现状和未来趋势。姜昕蔚表示，在AI技术的推动下，数据库和算力技术将继续升级加强，以应对日益复杂的数据处理需求和计算挑战。随着数据量的爆炸式增长和计算任务的多样化，传统的数据库和算力架构已难以满足当下的需求，行业亟需更加高效、灵活和智能的解决方案；数据库和算力的升级将成为行业发展的必然趋势。与此同时，云原生轻量、灵活和高效的特性，与边缘计算的低延迟、高带宽和数据处理能力也将为AI技术的发展提供更加广阔的空间和更加丰富的应用场景。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/6e/6eff7ee39d9e330d78f9062931aae867.jpeg" /></p><p>InfoQ研究中心研究总监兼首席分析师 姜昕蔚</p><p>&nbsp;</p><p>AIGC时代不仅对技术提出了更高要求，品牌营销与市场运营更是出现了新变革。技术品牌市场运营必须刷新认知、深挖用户需求，并融合前沿科技，打造差异化的技术品牌形象。同时，市场运营需依托数据分析，洞察市场动态与用户偏好，预测未来趋势。</p><p>&nbsp;</p><p>在本次年会的开放麦环节，华为云中国区行业营销负责人刘丽丽分享了她的经验和洞悉。她表示，2024年在稳定性成为前提的当下市场环境中，品牌工作的专业性和创新性显得尤为重要。对于不同发展阶段的企业来说，定义一个清晰可落地的品牌价值主张非常重要，因为作为连接品牌和产品的桥梁，价值主张对于牵引产品进步和行业市场拓展具有关键作用。2023年在拓展零售市场时，华为云通过深入客户市场，洞悉客户需求，提出了“共筑新时代伟大品牌”的价值主张，得到了众多客户的认同和认可，从而All in 华为云。</p><p></p><p><img src="https://static001.geekbang.org/infoq/68/6841db98e9b5499baa08193b101bda91.jpeg" /></p><p>&nbsp;</p><p>无独有偶，大模型农场LLMFarm创始人宜博同样认为创新力和创造力在AI时代尤为重要。他认为，随着大模型的不断发展，未来的AI系统将具备更强的创新能力和自主决策能力，这将为企业品牌营销带来更多的机遇和挑战。“随着大模型的普及和应用，非AI项目将逐渐失去投资吸引力。”其这一观点为市场人敲响了警钟，在未来的工作中大家需要更加关注“如何利用大模型提升效果和效率，以适应行业变革的趋势。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/a4/a4fd468ca9f6bec20b262bce9c5ec32e.jpeg" /></p><p></p><p>AIGC时代迎来变革的不仅是营销层面，运营领域也迎来了新的可能。零一万物开源负责人、开源社联合创始人林旅强表示，随着AIGC时代的到来，以大模型优化运营流程、加速社区拓展的过程中，更要不忘开源初心，坚持“Community Over Code”路线，深刻理解“开源项目在自身的商业模式中的价值”，显得尤为重要。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/28/28d0fb879a9b7ad4920fab5e52ca9d3b.jpeg" /></p><p></p><p>这种紧密的联系其实不仅限于技术和商业模式之间，更体现在行业之间的生态构建。企业必须持续地技术创新和产品迭代，以满足用户不断变化的需求。只有从企业之间的竞争逐渐演变为生态间的共建，与合作伙伴共同打造具有市场竞争力的产品和服务，才能在AIGC时代的浪潮中立于不败之地，实现可持续的发展和增长。</p><p>&nbsp;</p><p>总而言之，AI在2023年掀起的这波巨大波澜，需要合作伙伴能够紧跟时代步伐，抓住AI带来的机遇，共同迈向更加美好的明天。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/uKgQIZksvpT1rkLPi4Hq</id>
            <title>2023年，GenAI工具成为赌注</title>
            <link>https://www.infoq.cn/article/uKgQIZksvpT1rkLPi4Hq</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/uKgQIZksvpT1rkLPi4Hq</guid>
            <pubDate></pubDate>
            <updated>Wed, 31 Jan 2024 07:01:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: The New Stack, 2023年, GenAI, 开发者
<br>
<br>
总结: 2023年是GenAI帮助开发者开发应用的突破之年。工具创建者设法在开发人员最常从事的工作中满足他们。 </div>
                        <hr>
                    
                    <p>本文最初发布于The New Stack。</p><p></p><p></p><blockquote>2023年是GenAI帮助开发者开发应用的突破之年。工具创建者设法在开发人员最常从事的工作中满足他们。</blockquote><p></p><p>&nbsp;</p><p><a href="https://www.eweek.com/development/ibm-watson-provides-self-service-ai-for-developers/">很长一段时间以来</a>"，我一直想知道<a href="https://thenewstack.io/ai/">AI</a>"如何才能更好地帮助软件开发人员完成他们的工作。在软件开发领域，2023年是人工智能的关键一年，像<a href="https://thenewstack.io/github-copilot-a-powerful-controversial-autocomplete-for-developers/">GitHub Copilot</a>"这样的工具不断发展，帮助开发人员构建应用程序。</p><p>&nbsp;</p><p>至少十年来，我会时不时地向我的技术伙伴Grady Booch提出这个问题。他是<a href="https://www.ibm.com/?utm_content=inline-mention">IBM</a>"研究院软件工程首席科学家，并且是一名IBM杰出研究人员（IBM Fellow）。早些时候，他说他相信人工智能会在<a href="https://thenewstack.io/software-development/">软件开发</a>"中发挥作用，但他对应用的程度持怀疑态度。</p><p>&nbsp;</p><p>他一直这么认为。去年年底，Booch在Twitter（现在的X）上与Replit首席执行官<a href="https://www.linkedin.com/in/amjadmasad/">Amjad Masad</a>"就人工智能能给开发者带来什么进行了<a href="https://twitter.com/Grady_Booch/status/1595878463298105344">激烈的辩论</a>"。Masad写道，随着人工智能模型的发展，在某个时候，“世界上的每个人都将至少拥<a href="https://twitter.com/amasad/status/1595557798850461702?lang=en">有John carmack级别的软件能力</a>"。”<a href="https://twitter.com/ID_AA_Carmack">Carmack</a>"是一位著名的游戏开发者(《毁灭战士》和其他游戏的共同创建者）和AR/VR专家，曾担任Oculus VR的首席技术官，并在<a href="https://about.meta.com/?utm_content=inline-mention">Meta</a>"收购Oculus后担任顾问CTO。</p><p>&nbsp;</p><p>我们离那一步还远着呢。但就<a href="https://thenewstack.io/how-generative-ai-can-increase-developer-productivity-now/">对开发人员生产力的提升</a>"而言，2023年已经证明了人工智能的巨大作用。</p><p>&nbsp;</p><p>“对于人工智能辅助编码和软件开发来说，2023年确实是具有开创性的一年，”Omdia应用智能首席分析师<a href="https://www.linkedin.com/in/lianjye/?originalSubdomain=sg">Lian Jye Su</a>"在给The New Stack的电子邮件中表示，“Copilot和其他生成式人工智能工具在作为开发工具使用时，已经展示了很高的准确性、自动化程度，最重要的是，灵活性。开发人员可以使用自然语言来表达他们的意图，并专注于创造和评价方面，因为工具会处理那些单调枯燥的部分。</p><p>&nbsp;</p><p>“因此，有一些开发人员使用这些工具生成了近40%的代码。除了Copilot，其他关键工具还包括OpenAI Codex、Replit、<a href="https://www.tabnine.com/?utm_content=inline-mention">Tabnine</a>"、Codacy和Durable。”</p><p>&nbsp;</p><p>最近，JetBrains的一项关于<a href="https://thenewstack.io/jetbrains-developer-survey-tracks-rapid-adoption-of-ai-chatgpt/">开发者生态系统现状</a>"的<a href="https://blog.jetbrains.com/team/2023/11/20/the-state-of-developer-ecosystem-2023/">研究</a>"表明，如果有机会，56%的受访者会<a href="https://www.jetbrains.com/lp/devecosystem-2023/ai/#ai_delegate_activities">让人工智能助手编写代码注释和文档</a>"。</p><p></p><h2>Copilot的演进</h2><p></p><p></p><p>与此同时，46%的受访者表示他们<a href="https://www.jetbrains.com/lp/devecosystem-2023/ai/#ai_tools_experience">使用Copilot</a>"。</p><p>&nbsp;</p><p>GitHub于2022年6月推出了代码自动补全/结对编程工具Copilot。GitHub首席产品官<a href="https://www.linkedin.com/in/inbalshani/">Inbal Shani</a>"在接受The New Stack采访时表示，该网站目前拥有150多万用户。此外，她说，使用Copilot编写的新代码其比例已从35%增长到60%，预计未来几年将达到80-90%。</p><p>&nbsp;</p><p>Shani指出，像GitHub Copilot这样的人工智能开发工具正在大幅增长，并对软件开发产生了革命性影响，对生产力、质量和工作满意度也产生了积极的影响。</p><p>&nbsp;</p><p>开发人员对人工智能工具的采用呈指数级增长。根据GitHub的数据，92%的开发人员现在在工作中使用了某种形式的人工智能。这在很大程度上是因为GenAI模型的兴起推动了人工智能的民主化应用。</p><p>&nbsp;</p><p>现在，像GitHub Copilot这样的人工智能辅助工具已经可以生成完整的代码块、代码说明、文档以及开发人员所提问题的答案。<a href="https://www.linkedin.com/in/amandaksilver/">Amanda Silver</a>"是<a href="https://news.microsoft.com/?utm_content=inline-mention">微软开发部门</a>"的副总裁，负责产品、设计、用户研究和工程系统。他在接受The New Stack视频采访时表示，这有助于新的开发人员更快地参与到开发中来。</p><p>&nbsp;</p><p>此外，Copilot还可以优化和重构代码、排除问题、提出安全修复建议、辅助调试等等，这使得开发人员不需要在这些领域有很深入的专业知识。</p><p>&nbsp;</p><p>在微软内部，工程师们在构建自己的产品时会对Copilot的功能做“内部测试（<a href="https://thenewstack.io/what-launchdarkly-learned-from-eating-its-own-dog-food/">dogfood</a>"）”，并向GitHub提供反馈以改进这项技术。Silver说，事实上，微软的工程师们在使用Copilot执行大规模的内部系统迁移，这也有助于测试它的批量重构能力。</p><p></p><h2>CodeWhisperer</h2><p></p><p></p><p>去年，亚马逊也推出了Copilot的竞争对手<a href="https://aws.amazon.com/blogs/machine-learning/introducing-amazon-codewhisperer-the-ml-powered-coding-companion/">CodeWhisperer</a>"，就像Copilot和其他工具一样，到2023年，它才成为一款可供日常使用的工具。<a href="https://aws.amazon.com/codewhisperer">Amazon CodeWhisperer</a>"是一种<a href="https://thenewstack.io/the-ultimate-guide-to-machine-learning-frameworks/">机器学习（ML）</a>"驱动的服务，它可以基于开发人员之前的注释和代码提供代码建议，帮助提高开发人员的生产力。</p><p>&nbsp;</p><p><a href="https://aws.amazon.com/?utm_content=inline-mention">Amazon Web Services</a>"在生成式人工智能方面表现出色，它已经推出了CodeWhisperer（正式发布）、<a href="https://aws.amazon.com/bedrock/">Bedrock</a>"（用于构建自定义人工智能应用）以及<a href="https://aws.amazon.com/about-aws/whats-new/2023/11/partyrock-amazon-bedrock-playground/">PartyRock</a>"（用于无代码人工智能应用构建，目前处于预览阶段）。在去年秋末的re:Invent大会上，该公司还发布了一款名为<a href="https://thenewstack.io/amazon-q-a-genai-to-understand-aws-and-your-business-docs/">Amazon Q</a>"的GenAI助手，专用于工作场合，可以根据组织的业务进行定制。</p><p>&nbsp;</p><p><a href="https://www.linkedin.com/in/dseven/">Doug Seven</a>"是亚马逊软件开发总监、Amazon CodeWhisperer和<a href="https://aws.amazon.com/q/">Amazon Q</a>"的总经理。他在接受The New Stack采访时表示，根据亚马逊的研究，CodeWhisperer将开发人员的生产力提高了50%至60%，而像自定义这样的功能进一步提高了生产力。</p><p>&nbsp;</p><p>Seven表示：“从针对人工智能工作负载优化芯片到利用人工智能的开发工具，亚马逊在人工智能领域很有竞争力。”</p><p></p><h2>GenAI工具概览</h2><p></p><p></p><p>直到2023年，在整个数字计算的历史中，编程的核心活动一直是人类编写代码行。人工智能工具的出现增强了这种能力，但人类编码一直是核心。</p><p>&nbsp;</p><p>这种情况现在已经变了。Intellyx分析师<a href="https://www.linkedin.com/in/jasonbloomberg/">Jason Bloomberg</a>"在发给The New Stack的电子邮件中说：“今天，不断发展的核心编程活动是让人工智能（主要是生成式人工智能和神经网络）创建应用程序。人类不再编码，而是创建和管理训练人工智能的模型和数据集。”</p><p>&nbsp;</p><p>当然，手工编码还会存在，但Bloomberg表示，他认为手工编码已经退居次要地位。他说：“任何组织、供应商或企业，如果没有意识到这一点，就注定会失去竞争力，最终变得无足轻重。”</p><p>&nbsp;</p><p>事实上，正如GitHub的Shani所说，“人工智能现在成了软件开发的筹码。”GitHub希望他们的工具可以覆盖软件开发生命周期（SDLC）的更多部分。</p><p>&nbsp;</p><p>Gartner分析师<a href="https://www.linkedin.com/in/thomasmurphy4/">Thomas Murphy</a>"在电子邮件中表示：“显然，Copilot在上市时间和销量方面取得了先机，但Tabnine、CodeStory、Codium等其他工具也都在扩展GenAI的空间。”他说，除了AWS和谷歌，<a href="https://about.gitlab.com/?utm_content=inline-mention">GitLab</a>"和Atlassian等云提供商也在进军代码生成领域，并为SDLC提供更广泛的人工智能辅助功能。</p><p>&nbsp;</p><p><a href="https://www.jetbrains.com/">JetBrains</a>"也是一个杰出的开发工具制造商。他们一直在构建自己的AI助手特性，并在其最新的产品更新中发布。他们的工作重心在其旗舰产品<a href="https://thenewstack.io/jetbrains-formulates-ide-go-called-gogland/">IntelliJ IDEA</a>"集成开发环境（IDE）上。他们新推出的<a href="https://thenewstack.io/jetbrains-launches-new-ai-assistant-powered-by-multiple-llms/">AI助手</a>"插件引入了不同的功能——它可以“在更高、更抽象的层面上提供代码重构建议，而不仅仅是修复特定的模式，”JetBrains开发大使<a href="https://www.linkedin.com/in/mtellis/?originalSubdomain=uk">Matt Ellis</a>"告诉The New Stack。</p><p>&nbsp;</p><p>Ellis说，未来，他们希望改进这款助手，使其“更智能”——这样它就可以利用现有的内部代码索引和元数据更好地理解代码上下文，并探索其模型的企业托管选项。</p><p>&nbsp;</p><p><a href="https://thenewstack.io/appmap-releases-runtime-code-review-as-a-github-action/">AppMap</a>"首席执行官<a href="https://www.linkedin.com/in/elizabethlawler/">Elizabeth Lawler</a>"告诉The New Stack：“像Copilot这样的工具擅长于提供局部代码建议，但不了解应用程序背景。新玩家可以整合可观察性数据、架构信息等，提高建议的准确性。”</p><p>&nbsp;</p><p>然而，2023年已经使人工智能基础设施和模型的使用变得足够简单，现在，即使是比较小的公司也可以利用可定制的人工智能。Lawler指出，这将催生新的增值服务。此外，她补充说，人工智能生成的代码仍然存在质量问题，但它会要求开发人员必须认真评估这些建议。</p><p>&nbsp;</p><p>与此同时，从风险投资的角度来看，<a href="https://www.differential.vc/">Differential Ventures</a>"创始人兼管理合伙人<a href="https://www.linkedin.com/in/nickadams11/">Nick Adams</a>"表示，有些公司的业务分析师经常会编写重复的查询以提取数据，对于这些公司来说，利用GenAI来协助业务分析师是一个机会。</p><p>&nbsp;</p><p>Adams告诉The New Stack：“人工智能工具可以从业务用户那里获得自然语言问题，然后帮助生成SQL或Python查询，并允许分析师改进代码。这可以加快这个过程，避免每次都从头开始编写查询。”</p><p>&nbsp;</p><p>他指出，迁移像<a href="https://thenewstack.io/u-s-unemployment-surge-highlights-dire-need-for-cobol-skills/">COBOL</a>"这样的遗留代码也是一个机会。IBM在将旧的COBOL代码迁移到Java时就利用了watsonx的GenAI功能。</p><p>&nbsp;</p><p>Adams说：“许多老系统都是由即将退休的开发人员编写的。电气工程领域也有许多即将退休的专家，他们几十年来积累的知识需要通过现代工具和培训传递给下一代。”</p><p></p><h2>GenAI与低代码</h2><p></p><p></p><p>我曾经认为GenAI会以某种方式涵盖<a href="https://thenewstack.io/low-code-vs-no-code/">低代码、无代码开发</a>"，但<a href="https://thenewstack.io/pega-infinity-23-advances-low-code-application-development/">Pegasystems</a>"首席技术官和产品营销副总裁Don Schuerman让我看清了现实。</p><p>&nbsp;</p><p>GenAI对软件行业产生了重大影响，包括低代码平台，因为自动生成代码的工具可能会扰乱低端市场。</p><p>&nbsp;</p><p>Schuerman告诉The New Stack，“然而，企业级低代码更关注架构、集成、<a href="https://thenewstack.io/devops/">DevOps</a>"、<a href="https://thenewstack.io/security/">安全性</a>"、可重用性等。这种复杂性可不仅仅是生成代码片段”。</p><p>&nbsp;</p><p>Pegasystems正在使用GenAI自动生成低代码工件以加速开发，如标准工作流模板、测试数据、过程文档、数据映射等。</p><p>&nbsp;</p><p>来自亚马逊的Seven表示，CodeWhisperer帮助使用IDE进行开发的专业开发人员，而Party Rock则针对其他群体（如主题专家）用很少的代码或不用代码来构建应用程序。</p><p>&nbsp;</p><p>他说：“我不认为GenAI会让低代码、无代码过时。我认为它将使低代码变得更好。”</p><p>&nbsp;</p><p>在谈到人工智能是否会取代开发人员的问题时，Schuerman表示：“我不认为生成式人工智能会完全取代开发人员。它将通过完成一些初始工作及提供建议来提升他们的效率。无论如何，开发人员很少完全从头开始编写所有内容。”</p><p>&nbsp;</p><p>JetBrains的调查显示，60%的受访者认为<a href="https://thenewstack.io/how-will-generative-ai-change-the-tech-job-market/">人工智能编码工具将从根本上改变就业市场</a>"，51%的受访者认为这些工具将增加市场对专业软件开发人员的需求。然而，人们一致认为，人工智能永远不会完全取代开发人员编写代码的工作。</p><p></p><h2>Studio之年</h2><p></p><p></p><p>微软人工智能平台项目管理副总裁John Montgomery告诉The New Stack，去年11月，微软推出了<a href="https://azure.microsoft.com/en-us/products/ai-studio">Azure AI Studio</a>"预览版。这是一个新平台，旨在使拥有各种能力和偏好的开发人员能够利用人工智能进行创新，并以可靠的人工智能实践为基础，使用最新的人工智能工具和机器学习模型进行探索、构建、测试和部署。</p><p>&nbsp;</p><p>他说，“我们相信人工智能是终极放大器。这种向生成式人工智能的最新转变已经彻底改变了世界。它改变了应用程序的游戏规则。我已经在微软工作了25年；我从未见过客户如此迅速地接受一项技术。在不到一年的时间里，我们有超过1.8万名客户在用Azure OpenAI构建自己的东西。”</p><p>&nbsp;</p><p>他举例说，Instacart正借助<a href="https://thenewstack.io/dev-news-gpt-4-turbo-chrome-talks-pretty-and-worlds-merge/">GPT-4 Turbo</a>"&nbsp;with Vision使用户能够拍摄手写的购物清单，并在他们的应用程序中生成虚拟购物清单。</p><p>&nbsp;</p><p>Montgomery说道：“你可以将AI Studio视为生成式AI的终极工具包。它将恰当的模型、数据和可靠的人工智能系统结合在一起，这样客户就可以相信他们的解决方案是安全、可靠和可扩展的。”</p><p>&nbsp;</p><p>微软宣布支持自己最新的大型语言模型（LLM），以及来自<a href="https://thenewstack.io/beyond-chatgpt-exploring-the-openai-platform/">OpenAI</a>"、Meta、Nvidia、Mistral AI和<a href="https://thenewstack.io/how-hugging-face-positions-itself-in-the-open-llm-stack/">Hugging Face</a>"的最新模型。</p><p>&nbsp;</p><p>此外，该公司还宣布以Azure AI服务的形式推出Azure AI模型即服务。Montgomery说，“这将使客户更容易部署和运行尖端模型”。微软还推出了名为“<a href="https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/overview-what-is-prompt-flow?view=azureml-api-2">提示流</a>"”的提示工程工具。提示流是一种先进的提示工程、评估和部署系统，是Azure AI Studio和Azure Machine Learning的关键组成部分。</p><p>&nbsp;</p><p>尽管OpenAI是这个领域的重要玩家，并且与微软有着密切的合作关系，但该公司的一位发言人告诉The New Stack，他们无法接受采访。</p><p>&nbsp;</p><p>与此同时，谷歌也不甘示弱，他们最近也发布了<a href="https://makersuite.google.com/">Google AI Studio</a>"。这是一个免费的工具，使开发人员能够快速开发提示，然后获取API密钥用于他们的应用开发。谷歌表示，该工具目前是有限免费使用，<a href="https://ai.google.dev/pricing">未来的价格也将很有竞争力</a>"。</p><p>&nbsp;</p><p>在新闻发布会上，谷歌实验室副总裁<a href="https://www.linkedin.com/in/joshwoodward/">Josh Woodward</a>"展示了如何用谷歌账户登录Google AI Studio，并使用每分钟允许60个请求的免费配额。Woodward还演示了如何通过简单地点击“Get Code”将开发人员的工作转移到他们选择的IDE中。</p><p>&nbsp;</p><p>谷歌Developer X和DevRel副总裁兼总经理<a href="https://io.google/2022/speakers/jeanine-banks/">Jeanine Banks</a>"在接受The New Stack采访时表示，在内部，谷歌一直在将其人工智能技术应用于代码补全、文档、聊天机器人等方面。她说，“在内部应用这项技术，确实让我们学到了很多东西，我们会把这些东西带回到产品中。”</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://thenewstack.io/generative-ai-in-2023-genai-tools-became-table-stakes/">https://thenewstack.io/generative-ai-in-2023-genai-tools-became-table-stakes</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/TVspvQxKRBTdzYhMFFz5</id>
            <title>Taylor Swift身陷不雅照风波：AI越强、Deepfakes越猖狂，微软和推特们无法推责</title>
            <link>https://www.infoq.cn/article/TVspvQxKRBTdzYhMFFz5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/TVspvQxKRBTdzYhMFFz5</guid>
            <pubDate></pubDate>
            <updated>Wed, 31 Jan 2024 06:42:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Taylor Swift, Deepfakes, 平台举报, 技术媒体指责
<br>
<br>
总结: 流行乐巨星Taylor Swift遭遇了未经同意的虚假不雅内容的Deepfakes事件，粉丝举报了这些图片，但平台没有采取行动。技术媒体对微软提供制作工具的指责也引起了关注。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>“他们没有认真对待我们的痛苦，所以现在我们有责任大规模举报这些人并让他们停职，”Taylor Swift Deepfakes 事件中参与平台举报的一位女性称。在Swift粉丝看来，平台上图片的消失是粉丝举报的结果，推特没有作为。同时，国外各大技术媒体也对“涉嫌”提供制作工具的微软表达了指责，技术开发商承诺的“安全”，似乎还差得远。</blockquote><p></p><p>&nbsp;</p><p>想象一下，你醒来发现有人在未经你同意的情况下制作了关于你的虚假不雅内容，而且它还在互联网上广泛流传……这样的事情就被流行乐巨星Taylor Swift遇上了。</p><p>&nbsp;</p><p>上周三，未经同意的Taylor Swift 的露骨深度Deepfakes在 X 上疯传，在发布这些图像的账户被暂停之前的 19 小时内，获得了超过 2700 万次观看和超过 26 万个点赞。由于内容过于露骨，这里就不展示涉事原图了。</p><p>&nbsp;</p><p>X 上描绘 Swift 裸体场景的 Deepfakes 内容继续激增，其中包括病毒式传播的 Deepfakes 图像，后来甚至获得了高达千万级别的浏览量。图像上的水印表明它们来自一个已有多年历史的网站，该网站以发布假冒名人裸照而闻名。该网站的一部分标题为“AI Deepfakes”。</p><p>&nbsp;</p><p>上周四，话题标签 #ProtectTaylorSwift 开始在 X 上流行。这些图像首先上传到 Telegram，随后迅速在社交媒体上转发，浏览量达数百万次，并且在某些平台上尚未被删除。</p><p>&nbsp;</p><p>作为回应，X 似乎在图像出现几天后禁止了对 Swift 名字的搜索，但在过去两年里削减了三分之一的内容审核员之后&nbsp;，这种努力太少、太晚了。</p><p>&nbsp;</p><p></p><h2>涉事其中的微软和X</h2><p></p><p>&nbsp;</p><p>这种未经当事人同意的疯狂行径，再次将AI Deepfakes问题推上了风口浪尖。其实在此次事件之前，人们就已经开始对网上流传的Deepfakes素人色情制品表达担忧。如今巨星Taylor Swift也被拖入泥潭，瞬间彻底引爆整个舆论场，而微软也被牵涉其中。</p><p>&nbsp;</p><p>404 Media的<a href="https://www.404media.co/ai-generated-taylor-swift-porn-twitter/">报告表明，</a>"那些图片来自基于 Telegram 的非自愿色情制作社区，该社区建议使用 Microsoft Designer 图像生成器。设计师理论上拒绝生成名人的图像，但人工智能生成器很<a href="https://www.theverge.com/2023/10/5/23905311/microsoft-bing-dalle3-generative-ai-images-twin-towers">容易被欺骗</a>"，可以通过对提示进行小的调整来打破规则。虽然这并不能证明 Designer 被用于 Swift 图片，但这是 Microsoft 可以解决的技术缺陷。</p><p>&nbsp;</p><p>微软首席执行官Satya Nadella对此做出了回应。Satya 称未经同意的模拟裸体的泛滥“令人震惊和可怕”，并表示，“保障网络世界的安全性对每个人都有好处。因此，我想没人愿意接受一个内容创作者和内容消费者都缺乏安全保障的网络环境。我们有必要以此事为契机迅速采取行动。”</p><p>&nbsp;</p><p>Satya称，已经成功地解决了漏洞，但回应中对具体技术问题的提及很少：</p><p>&nbsp;</p><p>“我们需要在技术周围设置护栏，以确保生产出更多安全的内容。目前有很多工作正在进行中，但仍有更多事情需要我们去做。但这是全球社会需要在某些规范上达成共识，特别是当法律、执法和技术平台能够紧密结合时，人类有能力管理比自认为要多得多的问题。”</p><p>&nbsp;</p><p>微软人工智能工程主管 Sarah Bird 证实称，“我们正在继续调查这些图像，并加强现有的安全系统，进一步防止我们的服务被滥用于生成类似的图像。”</p><p>&nbsp;</p><p>对于马斯克的X，虽然其已禁止可能对特定人群造成伤害的行为，但在解决平台上的露骨色情Deepfakes问题上却行动迟缓，甚至并未解决。</p><p>&nbsp;</p><p>1 月初，一位 17 岁的漫威明星<a href="https://www.nbcnews.com/tech/misinformation/teen-marvel-star-xochitl-gomez-speaks-deepfake-rcna134753">公开表示</a>"，她在 X 上发现了自己的露骨色情Deepfakes，但无法将其删除。2023 年 6 月，该平台上还流传着未经同意的 TikTok 明星露骨色情Deepfakes，在联系 X 征求意见后，仅删除了部分材料。</p><p>&nbsp;</p><p>随着AI技术变得更加复杂和普遍，这个问题似乎每年都变得更加严重。</p><p>&nbsp;</p><p><a href="https://regmedia.co.uk/2019/10/08/deepfake_report.pdf">Deeptrace Labs 2019 年 9 月的</a>"一份报告发现，未经同意的 Deepfakes 色情内容占所有在线 Deepfakes 视频的 96%。 2023 年 10 月，<a href="https://www.wired.com/story/deepfake-porn-is-out-of-control/">《连线》</a>"分享了一位匿名独立研究人员的调查结果，该研究发现 2023 年前 9 个月，有 113,000 个 Deepfakes 视频被上传到最受欢迎的 Deepfakes 色情网站，比 2022 年全年上传的 73,000 个大幅增加。</p><p></p><h2>“消失”的监管</h2><p></p><p>&nbsp;</p><p>不少“霉粉”在举报这些不当图像之余，也纷纷表达了对于法律监管缺失的失望之情。</p><p>&nbsp;</p><p>美国白宫新闻秘书Karine Jean-Pierre宣称国会“应采取立法行动”以杜绝伪造的不适宜工作场所（NFSW）图像。她在接受ABC新闻采访时表示，“我们对……此次色情图像广泛传播的报道感到震惊——更确切地说，应该叫伪造图像。”</p><p>&nbsp;</p><p>与此同时，Jean-Pierre还敦促包括X在内的各社交媒体应用删除这些图像，防止不当内容在网络上进一步扩散。她补充道，“虽然社交媒体公司在内容管理方面拥有独立的决策权，但我们相信，他们可以、也应当在执行管控规则、防止错误信息及未经本人同意的敏感图像传播等方面发挥重要作用。”</p><p>&nbsp;</p><p>本月早些时候，美国众议院议员Joe Morelle（纽约州民主党人）和Tom Kean（新泽西州共和党人）重新提出了《防止敏感图像Deepfakes法案》。该法案旨在将生成和传播未经本人同意的露骨图像定性为刑事犯罪，且最高可判处十年监禁。</p><p>&nbsp;</p><p>Dorota本人就是Deepfakes色情图像的受害者，并因无法阻止不当内容的传播而深感痛苦。</p><p>Francesca在发言中强调，“虽然我还年轻，但我的声音仍然充满力量。面对这样的事态，我绝不可能保持沉默。我们必须勇敢表达立场，反抗我们面临的不公。这件事已经对我本人和我的同学造成严重影响，我不会双手一摊、假装无事发生。”</p><p>&nbsp;</p><p>“我在这里强烈呼吁推动变革，为立法而战，这样才能避免更多人像去年10月20号的我那样深陷迷茫和无助。我们的声音就是我们的武器，我和妈妈要求建立一个更安全的网络世界——这不只是要存留公义和希望，更是为了改善大家身处的现实环境。”</p><p>&nbsp;</p><p>该法案已经于2023年被提交给国会，随后由众议院司法委员会接手，但当时并未采取任何行动。</p><p>&nbsp;</p><p></p><h2>我们可以做哪些事</h2><p></p><p>&nbsp;</p><p>我们并非没有对抗现实的武器，新的工具和法律有望遏止这类不当图像，并帮助我们有效追究肇事者的责任。</p><p>&nbsp;</p><p></p><h4>水印</h4><p></p><p>&nbsp;</p><p>社交媒体平台会筛选上传到其网站上的帖子，并删除其中有违政策的内容。但从X上传播的Swift视频来看，整个管控过程其实不够完整，可能漏掉大量有害内容。此外，如何区分真实内容与AI生成内容也是个重大难题。</p><p>&nbsp;</p><p>作为一种可行的技术解决方案，水印能够在图像中隐藏某些不可见信号，帮助计算机识别内容是否由AI生成。例如，谷歌就开发出名为SynthID的系统，该系统利用神经网络修改图像中的像素点，借此添加人眼无法察觉的水印。即使图像经过进一步编辑或截屏，该标记仍可被正确检测。理论上，这些工具能够帮助企业改进内容审核能力，更快发现包括未经同意Deepfakes图像在内的各类伪造内容。</p><p>&nbsp;</p><p>这种方式的优点是：水印的实用性很强，有助于轻松、快速识别AI生成的内容，据此发现被删除的有毒帖子。Hugging Face研究员Sasha Luccioni曾对AI系统中的偏见问题进行过系统研究，她表示默认在所有图像中包含水印，能够有效遏制不良分子生成Deepfakes内容的行为。</p><p>&nbsp;</p><p>不足则是：这类系统仍处于实验阶段，尚未得到广泛使用。顽固的恶意人士也能找到回避水印的手段。相关企业并没有将水印技术应用于全部AI生成图像。例如，谷歌Imagen AI图像生成器的用户可以自主选择是否在AI生成图像中添加水印。种种现实因素，无疑限制了水印技术在打击Deepfakes色情内容方面的作用。</p><p></p><h4>隐形护罩</h4><p></p><p>&nbsp;</p><p>目前，互联网上现存的一切图像都可能被用于生成Deepfakes内容。而随着新型图像生成AI系统复杂度的提升和效果的持续改善，我们其实越来越难以分辨什么是真、什么是假。</p><p>&nbsp;</p><p>但各种新型防护工具正尝试修改图像，扰乱AI系统对内容的理解和处理，从而保护个人免受Deepfakes伪造技术的侵扰。</p><p>&nbsp;</p><p>以麻省理工学院研究人员开发的PhotoGuard工具为例，它就像是一层隐形护罩，能够以人眼不可见的方式改变照片中的像素。而一旦有人使用Stable Diffusion等AI图像生成器处理这些经PhotoGuard修改的图像，将无法得到预期中的输出结果。</p><p>&nbsp;</p><p>芝加哥大学研究人员开发的Fawkes也属于同类工具，它会使用隐藏信号保护图像内容，保证人脸识别软件难以识别图像中的人脸。</p><p>&nbsp;</p><p>另一款新型工具名为Nightshade，同样可以保护人们免遭AI系统的影响。该工具也是由芝加哥大学研究人员开发，能够为图像添加看不见的“毒化”层。这款工具是为了保护受版权保护的艺术图像免受科技企业在未经创作者同意下的窃取和使用而开发的。但从理论上讲，这项技术也能帮助所有者的任何图像免受AI系统的戕害。一旦科技企业未经所有者同意从网上获取训练素材时，这些有毒图像将破坏AI模型，例如导致其将小猫、甚至是Taylor Swift的图像识别为小狗。</p><p>&nbsp;</p><p>这种方式的优点是：这些工具能够提高恶意人士利用网络图像生成有害内容的难度。Ajder表示，相关技术在个人防止AI图像滥用方面带来了希望，普及之后将大大增强约会应用和社交媒体公司对伪造内容的监管力度。</p><p>&nbsp;</p><p>不足则是：这类护罩虽适用于最新一代AI模型，但无法保证在后续模型版本中继续稳定起效。另外，它们并不适用于已经存在于网络上的图像，更无法保护随处可见的名人素材，毕竟名人们无法控制自己的哪些照片会被传播到网上。</p><p>&nbsp;</p><p>“这将掀起一场旷日持久的技术军备竞赛。”道德AI咨询与审计公司Parity Consulting创始人Rumman Chowdhury表示。&nbsp;</p><p></p><h4>政策监管</h4><p></p><p>&nbsp;</p><p>技术修复的力量是有限的，颠覆性的深远变化终究离不开严格监管的加持。</p><p>&nbsp;</p><p>Taylor Swift的Deepfakes事件不仅引发广泛关注，也给打击此类恶意行为注入了新的动力。美国白宫方面表示此次事件“令人震惊”，并敦促国会采取立法行动。</p><p>&nbsp;</p><p>截至目前，美国仍然在以各州为单位分别出台监管法规。例如，加利福尼亚州和弗吉尼亚州已经禁止未经本人同意生成Deepfakes图像，纽约州和弗吉尼亚州还禁止传播此类内容。美国国会最近重新提出一项新的两党法案，要求将传播伪造裸照定性为刑事犯罪。新泽西州一所高中的Deepfakes丑闻也曾敦促立法者认真对待《防止敏感图像Deepfakes法案》。此番Swift事件引发的广泛关注，或将为法案吸引到更多来自两党的支持力量。</p><p>&nbsp;</p><p>世界各地的立法机构正纷纷推动对Deepfakes技术的严格管控。英国去年通过的《在线安全法案》就禁止传播Deepfakes色情内容，但并未禁止相关创作。传播者可能面临最高六个月的监禁。</p><p>&nbsp;</p><p>在欧盟方面，一系列新法案也尝试从不同角度解决这个难题。全面的《人工智能法案》要求Deepfakes创作者应明确披露相关素材是由AI生成，而《数字服务常规赛》则要求科技企业加快有害内容的删除速度。</p><p>&nbsp;</p><p>中国在2023年生效的Deepfakes立法方面走得最远。在中国，Deepfakes创作者须负责采取措施以防止其服务用于非法或有害目的，且在制作Deepfakes内容前必须征求用户同意、验证真实身份，并将输出的内容标记为AI生成。</p><p>&nbsp;</p><p>这种方式的优点：监管机构将为受害者提供追索权，对未经同意制作和传播Deepfakes色情内容者追究责任，由此构成强大的威慑力。相关立法还发出了明确信号，即未经当事人同意制作Deepfakes属于违法行为。</p><p>&nbsp;</p><p>Ajder指出，随着法律和公众认知广泛将制作Deepfakes色情内容视为性犯罪活动，真正的转机也将由此出现。“这将改变一些人对此类内容的冷漠态度，打破内容无害或者不属于实际性虐待形式的错误认知。”</p><p>&nbsp;</p><p>有不足则是：此类法律的执行难度很高。从目前的技术形式来看，受害者仍很难揪出始作俑者并对其提起诉讼。另外，Deepfakes的实际制作者很可能身处不同司法管辖区，这也会进一步加大起诉难度。</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>Deepfakes并非新鲜事物，多年之前就已相当猖獗。然而，生成式AI技术的兴起，正在不断降低利用AI生成图像和视频制作Deepfakes色情及性骚扰内容的门槛。</p><p>&nbsp;</p><p>专门研究生成式AI与合成媒体的AI专家Henry Ajder表示，在与生成式AI相关的所有恶意行为当中，在非自愿情况下受Deepfakes影响的人数最多，而且女性在其中占极高比例。</p><p>&nbsp;</p><p>针对Deepfakes的解决方案通常侧重于区分“真”和“假”，但对于大多数受害者来说，这没有帮助：无论哪种方式，许多受害者都表示感觉自己受到了侵犯。抑郁、性创伤和创伤后应激障碍是受害者的常见经历，他们很难在心理上感到安全。</p><p>&nbsp;</p><p>因此，“技术向善”、“不作恶”是开发者、技术使用者共同的责任。</p><p>&nbsp;</p><p>&nbsp;</p><p>相关链接：</p><p><a href="https://www.technologyreview.com/2024/01/29/1087325/three-ways-we-can-fight-deepfake-porn-taylors-version/">https://www.technologyreview.com/2024/01/29/1087325/three-ways-we-can-fight-Deepfakes-porn-taylors-version/</a>"</p><p><a href="https://www.theregister.com/2024/01/30/nudes_taylor_swift_action/?td=rt-3a">https://www.theregister.com/2024/01/30/nudes_taylor_swift_action/?td=rt-3a</a>"</p><p><a href="https://www.theverge.com/2024/1/26/24052196/satya-nadella-microsoft-ai-taylor-swift-fakes-response">https://www.theverge.com/2024/1/26/24052196/satya-nadella-microsoft-ai-taylor-swift-fakes-response</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/9KK4v2Ksk7NPJwlBsz6K</id>
            <title>讯飞星火正式发布语音大模型V3.5，数学、语义理解、代码能力持续提升</title>
            <link>https://www.infoq.cn/article/9KK4v2Ksk7NPJwlBsz6K</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/9KK4v2Ksk7NPJwlBsz6K</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 11:22:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 科大讯飞, 星火认知大模型V3.5, 全国产算力训练, 语音交互能力
<br>
<br>
总结: 科大讯飞举行星火认知大模型V3.5升级发布会，基于全国产算力训练的讯飞星火V3.5在数学、语言理解、语音交互能力等方面全面提升，超越了GPT-4 Turbo，同时发布了星火语音大模型，支持37个主流语种，效果超过OpenAI Whisper V3，为讯飞翻译机等场景的人机交互带来全新升级。 </div>
                        <hr>
                    
                    <p>1月30日，科大讯飞举行星火认知大模型V3.5升级发布会。科大讯飞董事长刘庆峰、研究院院长刘聪正式发布基于首个全国产算力训练的讯飞星火V3.5，七大核心能力全面提升，数学、语言理解、语音交互能力超GPT-4 Turbo，重磅升级星火智慧黑板；正式发布星火语音大模型，首批37个主流语种效果超过OpenAI Whisper V3，赋能讯飞翻译机迎来全新升级助力更自由沟通，推动万物互联时代下客服、汽车、机器人等场景人机交互变革。</p><p></p><p>大模型应用加速落地，星火开发者超35万生态增长迅猛，打造个人应用赋能亿万用户；讯飞星火赋能千行百业，携手保险、银行、能源、汽车、通信等领域联合龙头企业，打造大模型赋能的应用标杆。此外，深度适配国产算力的讯飞星火开源大模型“星火开源-13B”首次发布，场景应用效果领先，昇思开源社区联合首发上线。</p><p></p><p>“通过这场发布会，我们展望一个充满希望、孕育着生长能量的春天。相信2024年一定可以实现星火燎原，通用人工智能不仅能够在中国各大领域深度而广泛应用，而且我们在源头技术创新、在大模型的底层能力上也会站上全新台阶。”刘庆峰说道。</p><p></p><p>基于全国产化算力平台训练 讯飞星火V3.5七大能力全面提升</p><p></p><p>2023年10月24日，科大讯飞携手华为，宣布首个支撑万亿参数大模型训练的万卡国产算力平台“飞星一号”正式启用。启用后的90多天里，讯飞星火步履不停，基于“飞星一号”，启动了对标GPT-4的更大参数规模的大模型训练，带来了1月30日这场讯飞星火V3.5升级发布。</p><p></p><p>首个基于全国产算力训练的全民开放大模型讯飞星火V3.5在语言理解、文本生成、知识问答、逻辑推理、数学能力、代码能力和多模态能力七个方面进行了全面升级。其中语言理解、数学能力超过GPT-4 Turbo，代码达到GPT-4 Turbo 96%，多模态理解达到GPT-4V 91%。</p><p>&nbsp;</p><p>“在更好的数据、更强的人机协同训练中，我们不能只看单个的‘原子’能力，而是要以技术进步来解决真实世界的刚需。”</p><p></p><p>技术进步如何为人类生活带来真正有效的解决方案？刘庆峰从全新赋能万物互联时代人机交互、全新赋能知识学习与内容创作、全新提升数智化生产力三个方面，向现场观众展示讯飞星火V3.5的能力提升。</p><p></p><p>大模型全新赋能万物互联时代人机交互体验，超拟人合成效果惊艳。讯飞星火V3.5不仅在语义理解、指令跟随和多轮对话的演示中，展现出优异的能力，更是在情绪感知和拟人合成方面表现出色。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/b2/c6/b2440ddf18c66124466e5014068d5ec6.png" /></p><p></p><p>“听说今年尔滨特别火，作为南方小土豆还挺想去玩一下的。要不你用东北话介绍下有啥好玩的呗？”</p><p></p><p>在实操演示环节，科大讯飞研究院院长刘聪和讯飞星火V3.5现场互动，讯飞星火V3.5一口地道的东北话逗乐现场观众。幽默之余，讯飞星火V3.5快速为刘聪定制了旅游攻略，还催促他赶快买票，春运机票紧俏。它不仅能够帮助用户带来解决方案，还能作为“知冷知热”的朋友，带入情绪互动，超高的拟人度让大模型更具人情味。</p><p></p><p>大模型全新赋能知识学习与内容创作。讯飞星火V3.5对年终总结计划、述职PPT、活动策划、政策问答等任务“信手拈来”。基于此，科大讯飞推出了可以一键快速自动生成文档和PPT的办公产品——讯飞智文，这款产品主要功能有文档一键生成、AI撰写助手、多语种文档生成、AI自动配图、多种模板选择、提供演讲备注功能等。刘聪现场演示了使用讯飞智文制作的“合肥市2024年春节旅游推广策略”PPT，短时间内超20页内容丰富的PPT一气呵成，此等“打工神器”获得台下一阵掌声点赞。</p><p></p><p>大模型还能够结合外部知识进行合理拓展，做到“旁征博引”。要素抽取、问题生成等能力的进步，能够帮助每个人以测助学形成思考的闭环，在越来越多的服务领域及学习知识场所中产生更多好用的智能体。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/a1/87/a15e84dbba6725bfb3e147f46f2edc87.png" /></p><p></p><p>大模型全新提升数智化生产力，可以更好助力科研、工业等民生刚需领域提质增效。随着数学和推理能力的升级，多模态能力逐步进阶，讯飞星火V3.5在视觉问答、联想推理等方面实现了“高分”应对，理解更加精确，表述也更好。</p><p></p><p>“讯飞星火V3.5能力的提升，已经达到了量质齐飞的关键点。”刘庆峰表示，2024年讯飞星火认知大模型的应用，一定会在越来越多的场景和领域中大放异彩。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/7a/21/7a01200a0c04da4yy21145946887fe21.png" /></p><p></p><p>首先赋能的场景，就是教育领域——科大讯飞此次重磅推出星火智慧黑板，具备多模态理解与推荐、全自然交互、虚拟人辅学、智慧化录课与分享四大功能。发布现场，爱因斯坦出现在星火智慧黑板上，用“吸力巨大的吸尘器”来比喻黑洞，深入浅出地为现场观众解释“黑洞是什么”。在立体几何等知识的教学中，星火智慧黑板通过解构立方体，将枯燥的理论视觉化，教学课堂更加生动有趣。</p><p></p><p>“黑板不再是简单的板书工具，而是跃迁成为教师的AI助手。”刘庆峰提及，在最近的实际展示与使用中，升级后的星火智慧黑板得到了师生、教育专家和业务合作伙伴们的高度评价。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/03/1d/033945ff0cbef2f74a5d2654fd689d1d.png" /></p><p></p><p>为什么人工智能的每次进步都有对教育领域的赋能？刘庆峰解释，通用人工智能作为能够改变世界生产生活方式的全新技术，可以推动人类进步。“而教育是人类进步的根本，关乎每一位个体，是真正的全民刚需。”</p><p>&nbsp;</p><p>正式发布星火语音大模型</p><p>首批37个主流语种效果超过OpenAI Whisper V3</p><p></p><p>“科大讯飞从创业之初的梦想和使命，就是要实现沟通无障碍。25年了，我们目标和梦想一天都没有变。”</p><p>智能语音起家的科大讯飞，创业25年来在这条赛道上一路驰骋，持续走在世界前列。2006-2019年，连续十四年荣获国际语音合成大赛冠军；2016-2023年，连续4届获得国际多通道语音分离和识别比赛CHiME冠军；2021-2023年，连续三年获得国际语音翻译比赛IWSLT冠军……此外，还参与承建首批国家新一代人工智能开放创新平台、语音及语言信息处理国家工程研究中心等，在语音领域的持续积累。</p><p></p><p>“大模型带来了语音技术发展的全新机会。”刘庆峰强调，让机器具备学习、推理和决策的能力，就是认知大模型要干的主要工作。“简单来说，借助大模型，我们让一段语音具备更加丰富的属性，有语种、有内容、有韵律、有音色，还有情绪。”</p><p></p><p><img src="https://static001.infoq.cn/resource/image/21/38/21c65e42f929c144f3fb181bb0a44c38.png" /></p><p></p><p>他介绍，星火语音大模型效果国际领先，中文、英语、法语、俄语等首批37个主流语种的语音识别效果超过OpenAI Whisper V3，而在多语种语音合成方面，星火语音大模型的首批40个语种拟人度超83%。</p><p></p><p>“通过星火语音大模型的评测效果，我们非常自豪地告诉大家，科大讯飞继续保持了全球领先的水平。”</p><p>在此优势下，语音大模型的能力升级也应用于C端硬件产品。会上，刘庆峰介绍了搭载语音大模型的讯飞翻译机，即将上线多语种自动识别和增强式翻译两个重要功能，分别于今年1月底和3月中旬完成升级。多语种自动识别让国际沟通更加便捷，增强式翻译技术让翻译机化身AI翻译助手。据介绍，此次讯飞翻译机多语自动识别升级，将支持35种语言，为跨语言沟通提质增效；增强式翻译提供中英双语服务，让跨语言交流更加省心出彩。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/5a/aa/5afedc57b8a323b9c2db8fc602d64baa.png" /></p><p></p><p>星火语音大模型不止助力国际沟通，还能“百搭”更多场景，赋能实际应用。刘庆峰介绍，在汽车、客服、家庭、陪伴机器人等场景中，星火语音大模型还有更多用武之地，带来人机交互变革。如赋能汽车，智能驾舱、智能座舱、智能导航、音乐控制等交互体验将进一步优化；陪伴机器人、导购机器人、辅诊机器人、智能家居、穿戴式设备等产业也将随着语音大模型的赋能进一步被引爆。</p><p>&nbsp;</p><p>讯飞星火赋能亿万用户 加速赋能千行百业</p><p></p><p>“大模型发展，应用才是硬道理。”刘庆峰强调。讯飞星火自去年5月诞生以来，不断迭代升级其大模型能力，深耕千行百业的应用刚需。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/4e/00/4ebde0333c9584b34a1063a0e6796500.png" /></p><p></p><p>大模型未来，要从教育开始做起。让孩子站在人工智能肩膀上学习的讯飞AI学习机，能够提高学习效率、激发学习兴趣、提升综合素养，产品用户净推荐值（NPS）持续保持行业第一，获得2023年京东&amp;天猫双11销售额冠军。</p><p></p><p>大模型也在持续赋能办公硬件，自去年5月讯飞星火正式发布起，有超过60万讯飞智能办公本、智能录音笔用户累计调用语篇规整、会议纪要、自动写稿等能力650万次，让工作更高效。市场用销量表达好评，产品蝉联京东&amp;天猫双11品类销售额冠军。</p><p></p><p>除了硬件产品，软件应用也不在话下。星火赋能个人应用打造，目前基于讯飞听见、讯飞星火APP、讯飞输入法等应用，已累计赋能亿万用户。在星火大模型赋能下，越来越多的用户拥有专业AI助手。一位浙江的小学老师，使用智能问答高效完成教学方案设计，已累计使用APP262天、超2300篇；一位湖南的“情感咨询师”，通过大模型咨询人际关系等生活中的烦恼，现已累计使用161天、超9500次……这样的例子还有很多，大模型正切实走入日常生活。</p><p></p><p>打造内容创作平台，星火赋能百万内容生产者。音视频创作工具“讯飞智作”自去年8月15日发布以来，新增了21万会员用户，生成了160万音视频内容；图文创作工具“星火内容运营大师”自去年10月24日发布以来，已服务了5000+企业，生成了超150万篇文章，助力内容创作者高效生产。</p><p>&nbsp;</p><p>今天，在讯飞开放平台之上，大模型总开发者超35万，其中企业开发者超22万，开发者数增长迅猛，大模型应用加速落地，持续夯实第一开发者生态。“所以我非常自豪，讯飞星火大模型的开发者数量，在中国是最高的，而且用户口碑非常好。”刘庆峰说道。</p><p></p><p>在此基础上，科大讯飞也在加速赋能各个行业的头部企业客户。现场，刘庆峰提及了讯飞与奇瑞的合作。“奇瑞是安徽的骄傲，连续十几年都是中国汽车出口量的第一名。”刘庆峰介绍，奇瑞汽车出口的国家覆盖英语、俄罗斯语、西班牙语、阿拉伯语、葡萄牙语等数十个语种，汽车中应用的智能语音交互技术全都由科大讯飞提供。“我们用大模型全力支撑中国汽车的全球化出海战略，要让汽车变得更聪明、更智能、更面向未来。”</p><p></p><p>讯飞星火大模型加速赋能千行百业，除了汽车行业，还与太平洋保险合作，通过星火太平洋计划赋能内部知识服务、办公、审计、展业等，打造数字劳动力在保险领域的应用标杆；联合交通银行，赋能客服、展业、办公、研发等，重点打造银行领域的代码能力应用标杆；与国家能源集团联合，赋能煤炭、电力、运输、化工等方面，打造央企集团一体化联动的大模型应用标杆等。</p><p>&nbsp;</p><p>科大讯飞与中国移动强强联合，全面助力中国移动数智化转型。在赋能“5G新通话”方面，中国移动携手讯飞星火推出5G新通话创新应用“商务速记”，能够实现通话语音同步纪要，关键事项精准提炼，且无需下载APP，所有手机都支持。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/76/a0/766bc5509da6bf2be915fc35d609c2a0.png" /></p><p></p><p>利用大模型赋能千行百业，把大模型技术的创造力转化为促进产业高质量发展的新质生产力，正在成为行业的共同选择。发布会现场，还举行了“大模型+5G新通话商务速记应用体验”启动仪式，科大讯飞高级副总裁江涛和中国移动市场部副总经理孙世伟共同参与，5G新通话创新应用“商务速记”向广大移动用户开放体验。</p><p>&nbsp;</p><p>刘庆峰表示，为加速企业大模型应用价值落地，讯飞星火V3.5将提供全栈自主可控的优化套件。基于全国国产化算力打造的讯飞星火V3.5支持异构算力调度，可实现行业大模型训练提效90%，支持23个企业应用场景的敏捷优化。</p><p></p><p>讯飞星火大模型的全面赋能，无疑将为各行业的数字化转型提供强大的技术支持，引领新一轮的数字化浪潮。</p><p>&nbsp;</p><p>星火开源-13B正式发布 深度适配国产算力</p><p>&nbsp;</p><p>共享源代码、开发出更好的软件，是广大开发者、高校、企业自主研发热衷于开源的关键原因，也是共建第一开发者生态的重要途径。会上，首个基于全国产化算力平台”飞星一号”的开源大模型——星火开源-13B正式发布。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/f3/f7/f32a7d28751247a2451720a3d80364f7.png" /></p><p></p><p>本次开源拥有130亿稠密参数（13B），包含基础模型iFlytekSpark-13B-base、精调模型iFlytekSpark-13B-chat，开源了微调工具iFlytekSpark-13B-Lora、人设定制工具iFlytekSpark-13B-Charater。学术企业研究可以基于全栈自主可控的星火优化套件，更便利地训练自己的专用大模型。</p><p></p><p>刘庆峰透露，星火开源大模型在技术上形成了差异化优势。星火开源-13B在多项知名公开评测任务中名列前茅，在文本生成、语言理解、文本改写、行业问答、机器翻译等企业典型场景中，通过对学习辅助、语言理解等领域的深入研究和优化，实用性大幅提升，在处理复杂的自然语言任务时更加得心应手。</p><p></p><p>基于“飞星一号”训练，星火开源大模型全栈国产适配优化，简单易用，场景应用效果领先，训练策略针对昇腾算力极致优化，训练效率达A100的 90%。这不仅是对昇腾AI硬件的进一步深度优化，也展示了国产算力在追赶国际先进水平方面的决心和能力。</p><p></p><p>开源只有更多的场景落地，才能更好地增进生态合作。目前，华为昇思开源社区已正式上架星火大模型开源版-13B，面对学术、企业研究完全免费，增进学术合作的同时，增进产业探索。</p><p>&nbsp;</p><p>展望2024年星火发展，这三点很重要</p><p></p><p>“在今天的大模型时代，我们绝不能只用开源模型做应用落地，在通用大模型的底座上一定要有国家队站出来。”刘庆峰强调。</p><p></p><p>当前，发展通用人工智能是我们必须要做的事情，否则工业、科研、民生等各个专用领域都会极大落后于世界，而中国是世界唯一有望成为智慧涌现第二极的国家。</p><p></p><p>展望2024年讯飞星火大模型发展，刘庆峰指出三点：“首先，一定要在通用大模型的底层能力上持续对标国际最先进水平，从算法研究包括更小的算力上做出相对更优效果。”我们要清醒理智看到差距，当前在小样本快速训练、多模态深度学习训练、超复杂深度理解等领域距离GPT-4的最好水平还有差距，讯飞星火有信心在今年上半年赶上GPT-4目前最好水平。“今天的通用大模型并不一定代表人工智能的全部未来，还有很多创新要做，如脑科学互动、对抗网络的深度连接等需要整个创新的生态，但我们一定要有勇气、有期许走在最前列。”刘庆峰指出。</p><p></p><p>“第二，2024年要真正让大模型量质齐飞，不仅是行业应用，还要在很多关键技术创新上联动大模型，中国企业界、科学界有信心实现超越。”</p><p></p><p>“第三，高楼大厦要建立在安全可控的平台之上，我们要实现自主可控平台上的生态繁荣。”刘庆峰表示，自己有信心，能够实现从算法、数据、应用场景到算力，构建一个完全自主可控的繁荣的人工智能生态。</p><p>“通用人工智能大未来刚刚开启，它必将深刻改变世界的未来，需要顶天立地的长期主义精神。”顶天，指的是希望源头核心技术在底层大模型能力上向国际最先进的能力看齐；立地，指的是大规模实现产业化。而这也是科大讯飞自成立起就一以贯之的信仰，需要在长期主义精神指引下推进各方面工作。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/30/9d/3000846132e434c65fa0c9a1ba1f829d.png" /></p><p></p><p>会上，新书《星火相传》正式发布。“这本书讲述了科大讯飞创业的很多真实故事，但更是讲述了科大讯飞奠基人、我的导师王仁华老师的教书育人经历，希望能够给讯飞人、给讯飞的合作伙伴、包括有志于做高校科技成果转化、有志于判断人工智能未来的朋友们以更多启发。”刘庆峰说道。</p><p></p><p>【活动推荐】</p><p></p><p>目前，极客邦科技正在策划 2024 年 6 月 14-15 日 <a href="https://archsummit.infoq.cn/2024/shenzhen/">ArchSummit 架构师峰会深圳站</a>"会议，本次会议将围绕“智能进阶. 架构重塑”大主题方向，探讨在 AI 浪潮下，架构设计如何匹配智能化趋势，以适应日益复杂的业务需求。会议上将讨论大模型、AI 运维、数据架构、应用架构、业务架构、微服务、高可用以及数字化转型等多个层面的话题，感兴趣的可以点击 ArchSummit 会议官网。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/13/04/1366c74a2e873bfb936e4f8cd3ae6c04.jpg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Sl6NqhahcD9jaejgA371</id>
            <title>大模型应用加速进行中，智能化软件工程势头强劲</title>
            <link>https://www.infoq.cn/article/Sl6NqhahcD9jaejgA371</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Sl6NqhahcD9jaejgA371</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 09:51:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 智能化软件工程, 人工智能, 大模型, 代码大模型
<br>
<br>
总结: 中国信通院、中国人工智能产业发展联盟和360集团联合举办了“大模型应用加速行动之走进360 暨首站‘AI4SE创新巡航’”活动，推动智能化软件工程领域的发展和人工智能与软件工程的交流互通。会上发布了代码大模型标准和2023年AI4SE“银弹”优秀案例，进一步推动了大模型和人工智能在软件工程领域的应用。 </div>
                        <hr>
                    
                    <p>智能化软件工程领域发展迅速，其智能化赋予软件研发流程更高的自动化和智能化水平。为加强AI与软件工程领域的交流互通，推动行业多融合发展，2024年1月25日，由中国信通院人工智能研究中心、中国人工智能产业发展联盟（以下简称AIIA）和360集团联合主办的“大模型应用加速行动之走进360 暨首站‘AI4SE创新巡航’”活动在京成功举办。</p><p></p><p>会议首先由中国信通院人工智能研究中心常务副主任魏凯、360集团首席运营官叶健分别发表致辞。魏所表示，大模型在消费市场（2C）和企业市场（2B）应用落地正在加速进行中，应用效果逐渐明显，中国信通院将联合中国人工智能产业发展联盟及产学研各方，以构建人工智能产业生态协作网络为目标，持续推动人工智能与行业融合应用，更好得赋能新型工业化。叶总表示，作为国内唯一兼具大模型和安全能力的公司，360瞄准大模型安全等关键“卡脖子”短板，积极参与信通院组织的系列活动，不断发掘AI技术作为新质生产力的潜力，共同推动人工智能大模型赋能“千行百业”。</p><p><img src="https://static001.geekbang.org/infoq/b1/b115c1603dd823ba3a8e895e08fb7bc3.png" /></p><p></p><p>&nbsp;</p><p>为公正客观得评测代码大模型，提升评测数据集的丰富度、质量和多样性，中国信通院集结产学研各方力量，正式启动代码大模型数据集共建项目，会上由魏所为共建单位颁发证书。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/e5/e56d1069f47a63ae3eed0f66dc1766ec.png" /></p><p>&nbsp;</p><p>代码大模型数据集共建单位名单</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/28/282f9b603e07863f3b8c0cc3935d8584.png" /></p><p>&nbsp;</p><p>代码大模型数据集共建单位证书颁发仪</p><p></p><h2>业内首个代码大模型标准正式发布</h2><p></p><p>&nbsp;</p><p>随后，由中国信通院和中国工商银行联合牵头的《智能化软件工程技术和应用要求第一部分:代码大模型》标准正式发布，这标志着国内代码大模型迈向规范化与标准化的重要一步。中国信通院人工智能研究中心平台与工程化部主任曹峰，和中国工商银行专家范威，分别从标准的编制背景、编制过程、参编单位，及标准的通用能力、专用场景能力、应用成熟度等维度进行详细解读。会上由魏所为核心参编单位颁发证书。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/28/284728f51d9701766d000391f4d9fa63.png" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/aa/aa870000a4d2d7538b1814dd352ddb1a.png" /></p><p>&nbsp;</p><p>代码大模型标准核心参编单位列表</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/59/59002492214d9897b0178180cce55864.png" /></p><p>&nbsp;</p><p>代码大模型标准核心参编单位证书颁发仪式</p><p>&nbsp;</p><p></p><h2>发布2023年AI4SE“银弹”优秀案例</h2><p></p><p>&nbsp;</p><p>会上正式发布了2023年AI4SE“银弹”优秀案例，中国信通院组织评委组从技术创新、应用创新、落地成效等维度综合评选出21项“银弹”优秀案例。会上由曹主任为优秀案例企业颁发奖杯。</p><p></p><p><img src="https://static001.geekbang.org/infoq/af/afde699d41e17307eece918130dd70e8.png" /></p><p>&nbsp;</p><p>2023年“银弹”优秀案例列表（第一批）</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/21/21c8048e57aab4740338027277a60201.png" /></p><p>&nbsp;</p><p>2023年“银弹”优秀案例奖牌颁发仪式（第一批）</p><p></p><p><img src="https://static001.geekbang.org/infoq/a4/a4435379073923960be6128a911a17eb.png" /></p><p>&nbsp;</p><p>2023年“银弹”优秀案例列表（第二批）</p><p></p><p><img src="https://static001.geekbang.org/infoq/30/30ef5dd9224d24f7c07e212a5c8fe243.png" /></p><p>&nbsp;</p><p>2023年“银弹”优秀案例奖牌颁发仪式（第二批）</p><p></p><p></p><h2>主旨演讲分享</h2><p></p><p>&nbsp;</p><p>360智脑资深产品专家葛灿辉和360 Al数字人产品线负责人吕欣鸿带来“360集团大模型建设和应用”的主旨演讲，立足于生成式大模型应用，介绍了基于360智脑的Agent数字人在多种场景下辅助人类生活的应用，同时展示了结合数字人与其他AI&nbsp;Agent协同情境下，为生活提供的“五智”应用服务。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c0/c047a101d05113f884734d0e664ca728.png" /></p><p>&nbsp;&nbsp;</p><p>360智脑资深产品专家葛灿辉 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/f3/f32663f718767b51b1ca7a662c883ff0.png" /></p><p></p><p>&nbsp;360 Al数字人产品线负责人吕欣</p><p>&nbsp;</p><p>中国信通院云大所人工智能部高级业务主管秦思思发表了“AI4SE行业洞察”的主题演讲。她从智能化软件工程落地场景分布、各环节AI技术应用比例及提效数据、智能开发工具应用成熟度和应用效果指标等维度解读了行业落地现状和趋势，并对中国信通院AI4SE的业务方向和布局进行了详细介绍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b0/b0ac64c3d325490cc403269fcfa060e6.png" /></p><p>&nbsp;</p><p>中国信通院人工智能研究中心高级业务主管 秦思思</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bd8f448e6241e2d184f8b180abdfd349.png" /></p><p>&nbsp;</p><p>软件工程各阶段AI技术应用情况及提效数据</p><p>&nbsp;</p><p>本次论坛还邀请了中金公司算法专家雷涛、360集团产品专家苗广英、汇丰科技中国交付总监梁曜麟、蚂蚁集团Al算法专家单硕、百度资深研发工程师王初晴、阿里通义灵码产品负责人陈鑫、aiXcoder COO李力行进行主题分享，为大家呈现智能化软件工程领域的探索情况和实践效果。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/d7/d7ffd220babda9a9f08b49088bf11398.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/3b/3b084b8d8b713b4929ad641140de0494.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>本次活动还举办了首站“AI4SE创新巡航”活动--走进360， AI4SE工作组成员单位代表们共同了参观360集团的展厅。期间，360集团向AI4SE工作组成员代表们展示了自己在人工智能融合软件工程领域的创新成果和应用案例。大家对如何利用人工智能提高企业开发效率等表现出很大兴趣，代表们也积极分享了自己的经验和观点。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ac/ac45ad78ad74f44e5cf6d8ef54c4365c.png" /></p><p>&nbsp;</p><p>首期“AI4SE创新巡航”参观人员合影</p><p></p><p>&nbsp;</p><p>未来，中国信通院将持续推进大模型等AI技术赋能软件工程的产业研究、标准制定、评估测试、案例征集、产业活动等工作，与产学研用各方单位携手推进大模型工程化、产业化进程，共筑AI4SE可信生态。</p><p>&nbsp;</p><p>&nbsp;</p><p>联系人：</p><p>中国信通院人工智能研究中心</p><p>胡老师：17371328072(微信同号)</p><p>秦老师：13488684897(微信同号)</p><p>&nbsp;</p><p>写在最后：</p><p></p><p>中国人工智能产业发展联盟AI4SE工作组：</p><p>中国人工智能产业发展联盟（AIIA）智能化软件工程工作组（AI4SE工作组），于2023年9月正式成立，旨在进一步发挥生成式AI、大模型等人工智能技术在软件工程领域的潜力，充分释放AI赋予软件工程的价值。AI4SE工作组已吸纳100+成员单位，覆盖金融、电信、软件等诸多行业，欢迎更多企业加入，请联系胡老师、秦老师。</p><p></p><p><img src="https://static001.geekbang.org/infoq/82/8287676c415c23b29cc53e845fdd1000.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/bgt612Co7OkHhHVCfLcg</id>
            <title>碾压前辈！Meta发布“最大、性能最好”的开源Code Llama 70B，但开发者纷纷喊穷：玩不起</title>
            <link>https://www.infoq.cn/article/bgt612Co7OkHhHVCfLcg</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/bgt612Co7OkHhHVCfLcg</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 06:51:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Meta, Code Llama 70B, 编码模型, 性能表现
<br>
<br>
总结: Meta发布了Code Llama 70B，这是一种性能优秀的编码模型，可以根据代码和自然语言提示生成代码，同时在性能测试中表现优于其他开源解决方案。 </div>
                        <hr>
                    
                    <p></p><p>当地时间 1 月 29 日，Meta 发布了 Code Llama 70B，Meta 表示这是“Code Llama 家族中体量最大、性能最好的模型版本”。Code Llama 70B 与先前其他家族模型一样提供三种版本，且均可免费用于研究和商业用途：</p><p></p><p>CodeLlama – 70B，基础编码模型；CodeLlama – 70B – Python，专门用于 Python 编码的 70B 模型；Code Llama – 70B – Instruct 70B，针对自然语言指令理解进行微调的版本。</p><p></p><p>为了对比现有解决方案测试 Code Llama 的性能表现，Meta 选择了两项流行的编码基准：HumanEval 与 Mostly Basic Ptyon Programming（MBPP）。其中 HumanEval 主要测试模型根据文档字符串补全代码的能力，而 MBPP 则测试模型根据描述编写代码的能力。</p><p></p><p>从基准测试结果来看，Code Llama 的表现优于编码专用的开源 Llama，甚至超越了 Llama 2。例如，Code Llama 34B 在 HumanEval 上的得分为 53.7%，优于 GPT-3.5 的 48.1%，更接近 OpenAI 论文报告的 GPT-4 的 67%。在 MBPP 上，Code Llama 34B 得分为 56.2%，超越了其他最先进的开源解决方案，已经与 ChatGPT 基本持平。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/e4/e47f97d709006f1d2195394c47b6a053.png" /></p><p></p><p>扎克伯格在 Facebook 上说道，“编写和编辑代码已经成为当今人工智能模型最重要的用途之一。编码能力也被证明对人工智能模型更严格、更符合逻辑地处理其他领域的信息非常重要。我对这个进展感到自豪，并期待 Llama 3 和未来的模型中包括这些进展。”</p><p></p><h4>Code Llama 实现原理</h4><p></p><p></p><p>Code Llama 是 Llama 2 模型的编码专用版本，是后者在编码数据集之上接受进一步训练的产物，且数据采集周期更长。从本质上讲，Code Llama 拥有比 Llama 2 更强的编码功能。它可以根据代码和自然语言提示词生成代码及与代码相关的自然语言（例如，“为我这一条输出斐波那契序列的函数”），亦可用于代码补全和调试。</p><p></p><p>Code Llama 支持当今多种高人气编程语言，包括 Python、C++、Java、PHP、Typescript (Javascript)、C# 和 Bash。</p><p></p><p></p><p></p><p>这次，Meta 将发布四种 Code Llama 模型版本，参数分别为 7B、13B、34B 和 70B。各模型版本使用 500B 代码 token 与代码相关数据进行训练，且 70B 模型则采用 1TB token 进行训练。7B 与 13B 基础与指令模型还经过 fill-in-the-middle（FIM）训练，允许向现有代码中插入新代码，因此可以支持开箱即用的代码补全等任务。</p><p></p><p>三种模型分别能够满足不同的服务与延迟要求。例如，7B 模型可以在单一 GPU 上运行。34B 和 70B 模型则可返回最佳结果并提供更好的编码辅助功能。其中 7B 与 13B 模型运行速度更快，适合实时代码补全等强调低延迟的编码任务。</p><p></p><p>Code Llama 模型可实现最多 10 万个上下文 token 的稳定生成能力。所有模型均在 1.6 万个 token 的序列上进行训练，并在最多 10 万个 token 的输入场景下表现出性能提升。</p><p></p><p></p><p></p><p>除了能够生成更长的编码程序之外，更长的输入序列窗口还能为编码大模型解锁其他令人兴奋的新用例。例如，用户可以向模型输入来自代码库的更多上下文信息，确保生成结果的相关性更强。这还有助于在体量更大的代码库中进行调试，帮助开发人员快速找到与特定问题相关的所有代码。现在，开发人员可以将完整代码直接提交至大模型，高效完成涉及大量代码的调试任务。</p><p></p><p>此外，Meta 还进一步微调了 Code Llama 的两个附加变体：Code Llama – Python 与 Code Llama – Instruct。</p><p></p><p>Code Llama – Python 是 Code Llama 的特定语言专用变体，使用 100B 个 Python 代码 token 上接受了进一步微调。由于 Python 是代码生成领域的基准测试语言，并且通过 PyTorch 在 AI 社区中发挥着重要作用，所以 Meta 相信这套专用模型将提供更有针对性的实际功能。</p><p></p><p>Code Llama – Instruct 则是 Code Llama 的指令微调与对齐变体。指令微调同样属于继续训练过程，能够满足其他特定目标。该模型接受“自然语言指令”输入与预期输出组合的持续训练，因此能够更好地理解人们对于提示词的生成期望。由于 Code Llama – Instruct 专门就生成实用、安全的自然语言回答进行了微调，因此在使用 Code Llama 进行代码生成时，Meta 建议开发者优先选择 Code Llama – Instruct。</p><p></p><p>Meta 并不建议开发者使用 Code Llama 或者 Code Llama – Python 执行常规自然语言任务，因为这两套模型并不是为遵循自然语言指令所设计。Code Llama 专门用于特定编码任务，不适合作为其他通用任务的基础模型。</p><p></p><p>另外，在使用 Code Llama 模型时，用户须遵守 Meta 指定的许可证与可接受使用政策。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/bc/bc80cbf1775d28c3d3bd605e52d10882.png" /></p><p></p><p></p><h3>更大参数会带来更高的硬件要求？</h3><p></p><p></p><p>没有意外，Code Llama 70B 赢得了开发者们的赞扬，甚至有人称“Code Llama 70B 是代码生成领域的游戏规则改变者。”</p><p></p><p>但有自称使用过的开发者表示，“我使用 Ollama 尝试了 70B 模型，即使经过多次尝试，它也无法编写贪吃蛇游戏。而 ChatGPT 一次尝试就给出了一款可以运行的游戏。”</p><p></p><p>另一方面，随着模型参数的增加，开发者们也担心自己手头没有足够装备来满足运行 Code Llama 70B。有人指出，在 A100-80GB 上训练所有 12 个 Code Llama 模型需要 1400K GPU 小时。</p><p></p><p>运行大模型几乎可以归结为两个因素：内存带宽和计算能力，足够高的内存带宽才能“提供”计算，足够强大的计算才能跟上内存提供的数据。对于个人开发者来说，可能并没有完美设备，因此很开发者也在寻求更容易配置的量化模型版本。</p><p></p><p>也有开发者支招：可以在 64GB RAM 的 Macbook M1/M2 上运行 70B 模型。</p><p></p><p>开发者“tinco”表示，“据我所知，市场上没有其他笔记本电脑具有与 64GB MBP 一样多的 VRAM。您可以使用两个 3090 制成一台 Linux 台式计算机，将它们连接在一起提供 48GB 的 VRAM。这样显然可以运行 4 位量化的 6k 上下文 70B Llama 模型。”Tinco 进一步表示，人们推荐 Macbook 是因为它们是一种相对便宜且简单的方法，可以将大量 RAM 连接到加速器上。</p><p></p><p>Tinco 提醒道，这些是模型的量化版本，因此它们不如原始 70B 模型好，尽管人们声称它们的性能非常接近原始性能。要在不进行量化的情况下运行，则需要大约 140GB 的 VRAM。这只有一台 NVidia H100（不知道价格）或两台 A100（每台 18,000 美元）才能实现。</p><p></p><p>也有开发者分析称，理论上，单个 Nvidia 4090 能够以“可用”速度运行量化的 70B 模型。Mac 硬件在人工智能方面如此强大的原因是因为统一的架构，这意味着内存在 GPU 和 CPU 之间共享。还有其他因素，但本质上归结为每秒代币的优势。用户可以在内存带宽较低的旧 GPU 上运行这些模型中的任何一个，但每秒的令牌速度对于大多数人认为“可用”的速度来说太慢了，而且必要的量化可能会明显影响质量。</p><p></p><p></p><h3>结束语</h3><p></p><p></p><p>代码生成一直既被开发者叫好又被吐槽，即使是 ChatGPT 和 Copilot，因为虽然可以提效，但是质量问题一言难尽。</p><p></p><p>有开发者在 Hacker News 上表示，“两个月后我取消了订阅（Copilot），因为我花了太多的精力去检查所有代码并修复所有错误。当尝试处理任何不琐碎的或与 SQL 有关的事情时（即使我用整个模式预先加载它），它基本上是无用的。自己编写所有内容要省力得多，因为我实际上知道自己想写什么，并且修复自己的错误比修复机器人的错误更容易。”</p><p></p><p>使用 ChatGPT 的“ben_w”表示，“我对它（ ChatGPT）的功能感到惊讶，但即便如此，我也不会称其为‘好代码’。我将它用于 JavaScript 编程，因为虽然我可以阅读（大部分） JS 代码，但过去 14 年我一直专业从事 iOS 工作，因此不知道什么是浏览器领域的最佳实践。尽管如此，我得到工作代码后，我也可以发现它产生了错误的选择和（看起来）奇怪的东西。”</p><p></p><p>类似的问题我们也在之前的文章<a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247602048&amp;idx=1&amp;sn=56bd059b8bf046ecfdb509a68c809baf&amp;chksm=fbebf64fcc9c7f594affa5928ee74adffc1c36bce4adcc521fbd2c43f5b4e4f7c0e586dd2ede&amp;scene=21#wechat_redirect">《代码屎山噩梦加速来袭，都是 AI 生成代码的锅？》</a>"中讨论过。最新研究也表示，GitHub Copilot “代码质量面临下行压力”，代码可维护性的趋势令人不安。</p><p></p><p>开源领域一直在进行生成更好代码的研究。在 Hugging Face 的“Big Code Models Leaderboard”上，也有很多被开发者认可的模型。</p><p></p><p>比如北京大学推出了⼀系列从 1.3B 到 33B 的 &nbsp;DeepSeek-Coder 开源代码模型。DeepSeek-Coder 基于 2 万亿个代币上从头训练，都使用 16K 的窗口大小和额外的填空任务在项目级代码语料库上进行预训练，以支持项目级代码补全和填充。测评结果表明，DeepSeek-Coder 不仅在多个基准测试中实现了开源代码模型中最先进的性能，⽽且还超越了 Codex 和 GPT-3.5 等现有的闭源模型。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/4c/4cf3e0e23b2e252c922e1b9bfd21f949.png" /></p><p></p><p>对于有开发者提出“当前 SOTA 本地代码生成模型是什么”的问题，可能现在还没有标准答案，大家还在努力想办法优化现在模型，现在远不是终点。</p><p></p><p>你心中的 SOTA 代码生成模型是什么？欢迎评论区说出你的使用感受和经验！</p><p></p><p>相关链接：</p><p></p><p>Meta 在研究论文中披露了 Code Llama 的开发细节以及基准测试的具体步骤，感兴趣的用户可以查看：</p><p></p><p><a href="https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/">https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/</a>"</p><p></p><p>感兴趣的朋友可以在 GitHub 上参阅 Code Llama 训练 recipes：</p><p></p><p><a href="https://github.com/facebookresearch/codellama">https://github.com/facebookresearch/codellama</a>"</p><p></p><p>参考链接：</p><p></p><p><a href="https://ai.meta.com/blog/code-llama-large-language-model-coding/https://news.ycombinator.com/item?id=39178886">https://ai.meta.com/blog/code-llama-large-language-model-coding/https://news.ycombinator.com/item?id=39178886</a>"</p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/w7ks2bB2Gc31qgrt3pXB</id>
            <title>Neuralink完成全球首例人类脑机芯片植入手术，马斯克：植入者恢复良好</title>
            <link>https://www.infoq.cn/article/w7ks2bB2Gc31qgrt3pXB</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/w7ks2bB2Gc31qgrt3pXB</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 03:19:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 脑机接口, Neuralink, 心灵感应, 超人认知
<br>
<br>
总结: Neuralink是一家致力于开发脑机接口技术的公司，他们的第一款产品名为“心灵感应”。该技术可以将人脑连接到计算机，通过破译神经信号来帮助严重瘫痪的患者控制外部技术。马斯克表示，这项技术可以实现“超人认知”，让瘫痪的人能够用思想操作智能手机或机器人肢体，并解决自闭症和精神分裂症等问题。Neuralink已经在猴子身上进行了测试，并获得了美国食品和药物管理局的批准，开始进行人体临床试验。 </div>
                        <hr>
                    
                    <p>1月30日，脑机接口公司Neuralink创始人埃隆·马斯克在X（Twitter）上宣布：在昨天，人类首次接受脑机接口（Neuralink）芯片植入，植入者恢复良好。马斯克在随后的帖子中表示，Neuralink的第一款产品名为“心灵感应”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3a/3a4d210f40b261d175830b696a7a9c12.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/97/97165c85614dbc3f3355c3a7c5dc1383.png" /></p><p></p><p>&nbsp;</p><p>资料显示，Neuralink由马斯克和其他一群科学家和工程师于 2016 年创立。它致力于开发脑机接口 (BCI)，将人脑连接到能够破译神经信号的计算机，旨在帮助严重瘫痪的患者仅使用神经信号来控制外部技术。马斯克称，Neuralink 的设备可以实现“超人认知”，使瘫痪的人有一天能够用他们的思想操作智能手机或机器人肢体，并“解决”自闭症和精神分裂症。</p><p>&nbsp;</p><p>如果这项技术功能正常，患有严重退行性疾病(如ALS)的患者有一天可以使用植入物通过移动光标和用大脑打字来交流或访问社交媒体。马斯克写道：“想象一下，如果斯蒂芬·霍金的沟通速度比打字员或拍卖师还快。”“这就是我们的目标。”</p><p>&nbsp;</p><p>2022 年11月30日，马斯克曾在 Neuralink 发布会上展示了一只头骨上装有电脑芯片的猴子“Sake”，在玩心灵感应视频游戏，并要到了葡萄零食的镜头。猴子“Sake”通过屏幕和脑里植入的 Neuralink 的 N1 设备，来追踪屏幕上的移动光标，拼出了“Can I please have snacks”的英文短句，全程和键盘没有物理接触。</p><p>&nbsp;</p><p>马斯克说，猴子其实不会拼写，它们只是将大脑信号转化为光标移动，迭代到下一个版本才能让猴子具备拼写能力。马斯克还在演讲中表示，Neuralink 还将致力于恢复视力。“即使有人从未有过视力，他们先天就失明了，我们相信我们的设备仍然可以帮助其恢复视力”。</p><p>&nbsp;</p><p>当时，Neuralink 还未获得美国食品和药物管理局（FDA）的批准，因此一直在在对动物进行测试。马斯克说：“我们希望非常小心，并确保它在将设备放入人体之前能正常工作。”</p><p>&nbsp;</p><p>马斯克还表示，在人体测试开始后，他将在自己的大脑中植入脑机接口设备。“你可能现在就植入了一个 Neuralink 设备，而你甚至都不知道……在（未来）其中一个演示中，我会的” 他说。活动结束后，他在推特上重申了这一点。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cf/cf414be80d775a1718eb6f6c6fca114b.png" /></p><p></p><p>2023年5月，Neuralink获得美国食品和药物管理局(FDA)批准。9月19日，Neuralink宣布，该公司已获得了独立机构审查委员会的批准，开始首次人体临床试验招募人员。Neuralink表示，因颈部脊髓损伤或肌萎缩侧索硬化症（ALS）而瘫痪的患者可能符合参加这项试验的条件。</p><p>&nbsp;</p><p>Neuralink没有透露将有多少人类患者参与其最初的人体试验。据悉，人体临床试验只是Neuralink走向商业化道路上的一步。去年的融资情况显示，Neuralink的估值已高达50亿美元（约合人民币359亿元）。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.cnbc.com/2024/01/29/elon-musks-neuralink-implants-brain-tech-in-human-patient-for-the-first-time.html">https://www.cnbc.com/2024/01/29/elon-musks-neuralink-implants-brain-tech-in-human-patient-for-the-first-time.html</a>"</p><p><a href="https://www.infoq.cn/article/6s4aDChIwYEOjVH62lGa">https://www.infoq.cn/article/6s4aDChIwYEOjVH62lGa</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/W2jqhIuhfGQaEoCHg5BK</id>
            <title>OpenAI出手后，GPT-4真的不懒了？网友不买账：只靠打补丁恐怕无济于事！</title>
            <link>https://www.infoq.cn/article/W2jqhIuhfGQaEoCHg5BK</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/W2jqhIuhfGQaEoCHg5BK</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Jan 2024 09:09:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 变懒问题, GPT-4 Turbo, 更新
<br>
<br>
总结: OpenAI发布了更新的GPT-4 Turbo，解决了模型在任务中途罢工的“变懒”问题。该更新还包括了具有视觉模态处理能力的GPT-4 Turbo正式版，并推出了小体量AI模型“嵌入”。用户之前抱怨GPT-4学会偷懒，但经过更新后的GPT-4似乎变得更聪明了。然而，对于更新内容的具体解释并未公开。 </div>
                        <hr>
                    
                    <p></p><blockquote>大模型会从人类经验中学习，如果人类本身越来越懒，那模仿人类的程序是不是也会越来越懒？</blockquote><p></p><p></p><h2>OpenAI发布更新，解决GPT-4“变懒”问题</h2><p></p><p>&nbsp;</p><p>近日，OpenAI在一篇博文中发布了多项更新，并表示更新后的GPT-4 Turbo“拥有比之前预览模型更好的代码生成等能力，且减少了模型在任务中途罢工的「变懒」情况。”但该公司并没有对更新内容做进一步解释。</p><p>&nbsp;</p><p>OpenAI在帖子中提到，由于知识库更新，已经有超过70%的GPT-4 API用户转向了GPT-4 Turbo。OpenAI表示，未来几个月内将陆续推出更多GPT-4 Turbo更新，包括发布具有视觉模态处理能力的GPT-4 Turbo正式版。这意味着用户将可输入各类多模态提示词，例如文本到图像生成提示。</p><p>&nbsp;</p><p>此外，OpenAI还推出了被称为“嵌入”的小体量AI模型。OpenAI对于嵌入的定义，是“代表自然语言或代码等内容中概念的数字序列”。以此为基础，即可搭配检索增强生成（简称RAG，一种从数据库获取信息、而非生成答案的AI方法）应用找到各类可访问内容间的关系。这些新模型、text-embedding-3-small嵌入乃至更强大的text-embedding-3-large版本现均已正式开放。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ab/ab8d4a0684379bcc0d56b4573528f399.png" /></p><p></p><p>经过改进的各GPT模型现已通过API开放，包括质量更高、价格更低廉的嵌入模型（e.gone模型的成本仅为此前嵌入模型的五分之一，但性能更强）。</p><p></p><h2>用户抱怨GPT-4学会偷懒：越来越像人类了？</h2><p></p><p>&nbsp;</p><p>2023年12月，有不少用户抱怨称，“这段时间使用 ChatGPT 或 GPT-4 API 时，会遇到高峰期速度非常慢、敷衍回答、拒绝回答、中断会话等一系列问题”。</p><p>&nbsp;</p><p>比如，某些时候，GPT-4系统会给出一些特别模糊的答案，特别是关于Docker、Kubernetes以及其他CI/CD的问题。此外，GPT-4还学会了“废话文学”——不直接回答问题，只是堆叠素材来讲解应该怎样回答问题。有用户反映，哪怕明确要求不要使用空白占位符，模型也仍然会用占位符把回答截得七零八落。这种限制回复质量的作法倒是替服务商节约了资源，但却极大浪费了普通用户的时间。</p><p>&nbsp;</p><p>用户&nbsp;jonathanallengrant 在OpenAI社区一个名为“为什么我觉得GPT变懒了”的帖子中提到：“不少人注意到自从Dev Day活动以来，模型的输出上限就变成了850个token。换言之就是ChatGPT变懒了，不光留出大量空白，还常常在同一条消息里半天停在原地。我相信这应该是OpenAI正在以某种方式扩展模型的推理方法。”</p><p>&nbsp;</p><p>用户manchkiran表示自己也遇到过类似的情况，并吐槽“现在的模型绝对是变懒了，只会快速搜索并给出Bing引擎的链接”，他猜测大模型变懒的原因或许与“微软加入OpenAI董事会后下调了算力分配”有关。</p><p>&nbsp;</p><p>sasindujayashmaavmu则从另一个角度分析了GPT-4变懒的原因：“我觉得这可能是人机回圈的锅……大模型会从人类经验中学习，所以如果人类本身越来越懒，那模仿人类的程序也会越来越懒。”</p><p>&nbsp;</p><p>对于漫天盖地的吐槽声，ChatGPT 官方通过 X 平台通知用户，“我们听到了你们关于 GPT-4 变得越来越懒的反馈！我们自 11 月 11 日起就没有更新过模型了，当然这不是故意的。”</p><p></p><h2>OpenAI出手后，GPT-4真的不懒了？</h2><p></p><p>&nbsp;</p><p>OpenAI本次更新承诺解决了GPT-4“变懒”问题，根据社区用户反馈来看，如今的GPT-4似乎真的聪明多了。</p><p>&nbsp;</p><p>用户Distinct_Salad_6683提到，最近自己发现GPT在编码能力有所提升，能够根据提示词快速提供完整的示例。之前GPT经常会拒绝给出具体示例，只是在描述自己要求它干的工作，并用“在此处插入函数逻辑”之类的废话来搪塞问题。</p><p>&nbsp;</p><p>也有用户“阴阳”OpenAI：软件只要更新一下就能解决“变懒”，真羡慕。要是能有补丁帮我扛过礼拜一就好了。</p><p>&nbsp;</p><p>由于OpenAI并未对更新内容做进一步解释，因此也有不少用户开始分析其到底是怎么解决GPT-4“变懒”问题的。语言学家&nbsp;christelle.hilz 分析，GPT-4变懒的问题跟算法无关，单靠打补丁恐怕无济于事。这个问题还得从其他角度尝试解决。“我好奇的是OpenAI愿意花多少钱来解决GPT变懒问题”。</p><p>&nbsp;</p><p>也有观点认为，OpenAI并未真正地解决问题。因为大语言模型就是算法加公式的组合，所以哪怕更新真的解决了变懒问题，只能用这种方法改进模型本身也不是什么好兆头。</p><p>&nbsp;</p><p>chieffy99则更悲观地表示，哪怕是聘请了世界各地的专家，大语言模型自身的问题还是难以解决，毕竟任何专家都不可能确切了解每一个问题。因为越是越是专注于自己的专业积累，我们的视野反而变得越狭窄。chieffy99还向OpenAI的管理团队“开炮”：</p><p>&nbsp;</p><p></p><blockquote>我向来敢于对OpenAI的缺点开炮，这里我也要明确表态：OpenAI一直认为AI的问题不可能通过开发AI方案来解决，但我觉得这是错的。&nbsp;我自己没有任何关于AI的知识和使用经验，但拥有丰富的项目管理积累。抱怨变懒问题的用户是谁、当时是怎么操作的并不是重点，重点在于大模型为什么会倾向于消极工作。我本人喜欢从问题当中寻找共性，而且从目前的情况看应该不只是模型自身出了问题。我自己还没有明确的答案，但OpenAI的态度明显是“先尝试从内部做解决或者改进，等影响到正常使用了再说”。&nbsp;在我看来，OpenAI的管理思路很有问题。以常见的团队沟通规划为例，只要提供足够的信息，GPT-3.5的表现还是相当不错的。所以我猜OpenAI也是用这种方式蒙蔽了高管团队的判断，毕竟精调提示词并不困难，请个专人就能解决。正因为如此，OpenAI才产生了单靠调整AI模型就能解决AI问题的思路。&nbsp;我不知道现在大家说的这些问题到底跟变懒有没有关系，毕竟引发问题的原因多种多样。而且GPT大模型本身也不老实，甚至会说谎来隐藏自己的真实行为。哪怕是被发现，OpenAI也可以解释说是存在误会或者提示词存在不当内容。另外别太过迷信规则，基于规则的行为也不一定比随机问题更稳定，比如GPT-3.5就会访问网站、并把外部聊天和相关数据保存成html文件。这其实是不符合GPT身份和功能定位的操作。我也遇到过中途“罢工”的情况，但这主要是大模型忘记了当前上下文中的内容必须与之前的上下文接续起来。普通用户当然分不清楚，所以很自然地认为是大模型在偷懒。这跟之前的GPT幻觉差不多，刚开始似乎经常发生，但使用的人越多、涉及的内部信息越少，幻觉也开始逐渐缓解。&nbsp;另外还有三点个人观察。首先，我很好奇OpenAI的专家到底做了什么。这个问题始于去年12月，当时外界认为GPT过于迷信专业知识、甚至为此而倾向于输出错误信息。比如通过知识文件向GPT自动输入提示词，那么生成的信息就会有所不同。而如果不输入预设文件，GPT的表现则比较正常。我就遇到了这样的情况，还专门向OpenAI上报了观察结果，想搞清在RAG问题有最终结论之前，到底该采取什么措施加以避免。而且之前我还尝试把知识跟行为区分开来做GPT训练，借此建立起纯知识库。在确保知识库内容与现实不冲突之后，再配合其他信息一起使用。第二点就是错误学习的问题。既然选择把大模型向公众开放，那能做纯软件修复的问题OpenAI肯定早就解决了。问题是时至今日，GPT还是没法在不改变形状的前提下，把不同尺度下相同颜色的图表正确合并。还是那句话，如果能修复的话早该修复好了。最后一点就是GPT号称全球最受欢迎的AI模型。这个评判标准实在太模糊了，我觉得应该从功能层面做准确描述。&nbsp;总而言之，当前关于GPT的种种报道明显是刻意设计出来的。可怕的是GPT明显还没做好准备，因此无脑宣传已经在扭曲中立研究、造成现实损害、甚至让AI制造出更多的社会问题。有人在违规使用GPT，甚至有人把它当作非法工具来设计和实施犯罪。我不知道这次的更新能产生多大影响，但各种违规行为已经真实存在，甚至对普通用户产生直接影响。我想问问OpenAI，这一切是单靠更新AI模型就能解决的吗？</blockquote><p></p><p>&nbsp;</p><p>值得一提的是，OpenAI此番发布的更新针对的是GPT-4 Turbo，即得到广泛使用的特定GPT-4版本。这套模型根据截至2023年4月的最新信息训练而成，目前仅提供预览版本。也就是说，大家如果继续使用GPT-4（使用截止于2021年9月的数据训练而成），那么“变懒”问题可能仍将存在。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.theverge.com/2024/1/25/24050829/openai-gpt-4-turbo-lazy-ai-model">https://www.theverge.com/2024/1/25/24050829/openai-gpt-4-turbo-lazy-ai-model</a>"</p><p><a href="https://community.openai.com/t/i-wonder-how-much-openai-would-pay-to-cure-gpt-lazyness/604781">https://community.openai.com/t/i-wonder-how-much-openai-would-pay-to-cure-gpt-lazyness/604781</a>"</p><p><a href="https://community.openai.com/t/why-i-think-gpt-is-now-lazy/534332/11">https://community.openai.com/t/why-i-think-gpt-is-now-lazy/534332/11</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>