<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/dEY24a9GRRI2gcLZl1TP</id>
            <title>阿里、蚂蚁、昇腾、中科加禾精彩分享 AI 基础设施洞见，现购票可享受 9 折优惠 ｜AICon</title>
            <link>https://www.infoq.cn/article/dEY24a9GRRI2gcLZl1TP</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/dEY24a9GRRI2gcLZl1TP</guid>
            <pubDate></pubDate>
            <updated>Tue, 16 Apr 2024 08:19:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型技术, 企业应用, AI Agent, 大模型基础设施构建
<br>
<br>
总结: 随着大模型技术的迅猛发展，企业界广泛讨论了AI Agent和大模型基础设施构建的应用潜力。 </div>
                        <hr>
                    
                    <p>随着大模型技术的迅猛发展，它已成为企业界广泛讨论的热门话题。尽管实现人工通用智能（AGI）的目标仍然遥远，但大模型的企业应用已经显现出其巨大潜力和广泛影响力。特别是在 AI Agent 和行业创新应用方面，我们看到了前所未有的探索和实践。而支撑这些应用的，是一个强大且不可或缺的基础设施层。为此，我们在 AICon 全球人工智能开发与应用大会 暨 大模型应用生态展，特别策划了“大模型基础设施构建”专题，邀请了四位业界顶尖专家，深入分享他们的经验和见解。</p><p></p><p>为确保听众能享受到无广告、高质量的内容体验，我们荣幸地邀请了蚂蚁集团 AI Infra 负责人张科，担任本次专题的出品人。下面是本专题论坛的演讲介绍：</p><p></p><p></p><h4>精彩演讲</h4><p></p><p></p><p></p><h5>网络驱动大规模 AI 训练 - 阿里云可预期网络 HPN 7.0 架构</h5><p></p><p></p><p>首先我们荣幸邀请到席永青，阿里巴巴的资深网络架构师，加入公司自 2014 年以来，他在 AI 训练和推理场景的高性能数据中心网络架构设计领域有着丰富经验。在即将到来的演讲中，他将深入分享《网络驱动大规模 AI 训练 - 阿里云可预期网络 HPN 7.0 架构》。通过他的演讲，您将能深刻理解 AI 计算对网络系统的核心诉求，包括为什么网络集群设计的关键要素 至关重要，以及如何通过阿里云设计的 HPN7.0 架构系统，基于 Ethernet 构建超大规模、极致性能的网络互联，从而实现算力的规模扩展。他还将展望高性能数据中心网络系统的未来，让听众了解网络系统架构设计在 AI 基础设施构建中的重要性，以及网络集群设计和高性能系统能力的关键要求，为高效训练系统带来的价值。</p><p></p><p></p><h5>GLake: 高效透明的大模型显存管理和优化</h5><p></p><p></p><p>其次，还荣幸邀请到赵军平，蚂蚁集团的基础智能 -AI Infra 异构计算负责人，来分享他在异构算力集群优化与推理优化方面的深刻见解。赵军平在此领域拥有丰富经验，负责蚂蚁集团内大模型、搜索广告推荐等方面的异构计算优化，且持有 190+ 中 / 美技术专利。此次演讲将围绕《GLake: 高效透明的大模型显存管理和优化》进行，探讨大模型训练和部署过程中遇到的显存挑战，并介绍 GLake 这一高效、全局的显存优化方案如何无缝接入 PyTorch 框架，显著减少显存碎片，提高训练效率。赵军平将深入解析大模型的显存与传输挑战，并比较现有解决方案的优劣，特别是 GLake 在大模型训练及推理中的应用和效果，展现其如何在实际测试中节省高达 34% 的显存并将训练吞吐提高最高 4 倍。通过赵军平的分享，您将学会如何系统性地分析和优化显存与传输问题，掌握不同优化手段在各种场景下的优劣，并探讨未来软硬件结合的发展方向。</p><p></p><p></p><h5>昇腾大模型推理最佳实践</h5><p></p><p></p><p>第三位邀请到的嘉宾是王建辉，华为计算产品线昇腾推理的首席架构师，王建辉主要从事软硬件协同设计、系统性能优化以及实时计算等技术领域的研究和实践。在他即将进行的演讲 《昇腾大模型推理最佳实践》 中，将深入探讨 AI 技术特别是大模型技术的快速发展趋势，及其在 Scaling Law 作用下，模型参数的持续增长和响应速度的不断提升带来的挑战和机遇。王建辉将分享昇腾在大模型推理方向上的技术探索，包括昇腾提供的高性能大模型推理软硬件解决方案及其关键特性，以及如何在提升用户体验的同时降低推理成本，满足大模型规模落地的需求。此外，通过具体的应用案例，他将展示昇腾大模型推理技术的关键进展和实际成效，让听众深刻理解昇腾大模型推理关键特性。</p><p></p><p></p><h5>构建兼容多元加速卡的大模型基础设施</h5><p></p><p></p><p>最后，有幸邀请到了崔慧敏，中科加禾的创始人兼 CEO，她同时也是中科院计算所编程与编译方向的学术带头人和处理器芯片全国重点实验室副主任。崔慧敏长期致力于面向国产处理器芯片的编译软件研究，发表了 40 余篇包括 ASLPOS、MICRO、PLDI 等顶级会议和期刊的论文，是编译和系统领域的领军人物。在接下来的演讲中，崔慧敏将分享《构建兼容多元加速卡的大模型基础设施》。她将深入探讨通用大模型带来的高性能智算算力需求，以及如何突破现有智算生态中的厂商锁定和系统不兼容问题。崔慧敏将介绍如何通过多层次统一编译器 IR 和跨架构编译优化技术，为上层应用提供“性能 + 编程”的归一化抽象，实现对多种加速卡的兼容。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7d/7deb8793e6b0e019e52450213e23973e.jpeg" /></p><p></p><p></p><h4>AICon</h4><p></p><p></p><p>AICon 由极客邦科技旗下的 InfoQ 中国主办，是一个专为工程师、产品经理和数据分析师设计的技术盛宴。参与 AICon，您将有机会听取关于 AI Agent 探索与实践、RAG 检索与生成的落地策略、Copilot 应用构建、大模型训练、推理优化、基础设施建设、LLMOps、AI 前沿探索、多模态技术与应用，以及大模型在行业创新和国际化落地探索等一系列精彩主题的分享。这不仅是一个学习和交流的好机会，也是探索大模型如何为您的业务创造价值的绝佳平台。</p><p></p><p></p><h4>参与方式和价值</h4><p></p><p></p><p>AICon 2024 将于 5 月 17 日至 18 日举行，现正处于 9 折早鸟票购买期。原价 6800 元的门票，现特价降至 4800 元，并享受额外 9 折优惠。如果您的团队对大模型开发和应用充满热情，欢迎加入我们，共襄盛举。报名及更多信息，请联系我们的小助手（手机 / 微信：13269078023）。</p><p></p><p></p><h4>合适的参与人员</h4><p></p><p></p><p>经过广泛调研，我们发现对大模型感兴趣的用户群体非常广泛，包括以下：</p><p></p><p>技术和管理层：追求了解大模型的战略价值和技术趋势，关心其在企业应用和创新潜力方面的影响。技术专业人员：探索大模型的架构、算法等细节，寻找优化方法和实践案例。业务负责人和产品经理：研究大模型如何推动业务创新，探索其在特定场景下的应用。市场和营销专业人员：分析大模型如何影响市场营销策略和品牌形象。创新驱动者和独立开发者：寻求创新的应用案例和技术应用，探索成本控制和资源优化</p><p></p><p></p><h4>活动推荐</h4><p></p><p></p><p>AICon 全球人工智能开发与应用大会 暨 大模型应用生态展将于 5 月 17 日正式开幕，本次大会主题为「智能未来，探索 AI 无限可能」。如您感兴趣，可扫码海报二维码查看更多详情。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/5b/5b61cd7e626ad82ffb058d930c542ced.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/27gslwgGsFMX5EPSIV6r</id>
            <title>“干掉程序员”，百度是认真的！发布三大开发工具和全新操作系统，李彦宏：只要会说话就会干开发</title>
            <link>https://www.infoq.cn/article/27gslwgGsFMX5EPSIV6r</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/27gslwgGsFMX5EPSIV6r</guid>
            <pubDate></pubDate>
            <updated>Tue, 16 Apr 2024 06:07:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 程序员, 自然语言, 智能代码助手, 文心大模型
<br>
<br>
总结: 李彦宏认为未来人人都可以成为程序员，只要会说话就可以具备程序员的能力。他强调未来自然语言将成为通用编程语言，智能代码助手Comate已经改变了代码编写方式。基于文心大模型的智能代码助手已经被广泛应用，支持多种语言和IDE平台。AI正在推动创造力革命，未来开发应用将变得简单，人人都可以成为开发者和创造者。 </div>
                        <hr>
                    
                    <p></p><p>“基本上以后不会存在‘程序员’这种职业了，因为只要会说话，人人都会具备程序员的能力。”百度创始人、董事长兼CEO李彦宏在3月份接受央视采访时表示。</p><p></p><p>而在4月16日的2024百度Create开发者大会上，李彦宏再次强调了这个观点：“过去，开发者用代码改变世界；未来，自然语言将成为通用编程语言。你只要会说话，就可以成为一名开发者，用自己的创造力改变世界”。</p><p></p><p>李彦宏以百度为例介绍道，基于文心大模型的智能代码助手Comate已经编写了百度内部四分之一的代码，而百度每天的新增代码中，27%是由Comate自动生成。Comate也已经走入了喜马拉雅、三菱电梯、软通动力等上万家企业，生成的代码采纳率达到了46%。</p><p></p><p>据悉，Comate 支持100多种语言和所有的IDE平台，可以推荐代码、生成代码注释、查找代码缺陷、给出优化方案，还可以深度解读代码库、关联私域知识生成新的代码等。</p><p></p><p>“今天，你不会写代码，也可以做出一个应用；不用编程，也可以做出一个智能体。AI正在掀起一场创造力革命，未来开发应用就会像拍短视频一样简单，人人都是开发者，人人都是创造者。”李彦宏说道。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a4/a4f793b119a3a7fa11dbf02728093864.jpeg" /></p><p>﻿</p><p>那么，李彦宏具体要如何淡化程序员边界、降低开发者门槛呢？</p><p></p><p></p><h2>“开源模型会越来越落后”</h2><p></p><p></p><p>“开源模型会越来越落后。”李彦宏在现场表示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/dc/dcea7a91ab5316113c2b6da9b872d26e.jpeg" /></p><p>﻿</p><p>李彦宏解释称，因为有了最强大的基础模型文心4.0，用户可以根据需要，兼顾效果、相应速度，推理成本等各种考虑，剪裁出适合各种场景的更小尺寸模型，并且支持精调和post pretrain。这样通过降维剪裁出来的模型，比直接用开源模型调出来的模型，同等尺寸下，效果明显更好；同等效果下，成本明显更低。“所以开源模型会越来越落后。”</p><p></p><p>另外，李彦宏也提到，多模态大模型是通往AGI的必经之路，而视觉大模型最大的应用场景是自动驾驶。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d9/d9ecc813decfc3d43a613685b69d6b12.jpeg" /></p><p>﻿</p><p>李彦宏介绍了文心大模型的最新进展。他表示，文心大模型已经成为了中国最领先、应用最广泛的AI基础模型。不仅如此，相比一年前，文心大模型的算法训练效率提升到了原来的5.1倍，周均训练有效率达到98.8%，推理性能提升了105倍，推理的成本降到了原来的1%。也就是说，客户原来一天调用1万次，同样成本之下，现在一天可以调用100万次。</p><p></p><p>据悉，文心一言从去年3月16日首发至今，用户数突破了2亿，每天API的调用量突破2亿，服务企业达到8.5万，利用千帆平台开发的AI原生应用数超过了19万。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7e/7e476614873da727b1d62f9f25aad0d0.jpeg" /></p><p>﻿</p><p>另外，根据过去一年的实践，百度分享了开发AI原生应用的三个具体思路：</p><p></p><p>第一是MoE。未来大型的AI原生应用基本都是MoE架构，这里所说的MoE不是一般的学术概念，而是大小模型的混用，不依赖一个模型来解决所有问题。但什么时候调用小模型、什么时候调用大模型、什么时候不调用模型，需要针对应用的不同场景做匹配。</p><p></p><p>第二是小模型。小模型推理成本低，响应速度快，在一些特定场景中，经过SFT精调后的小模型，它的使用效果可以媲美大模型。通过大模型压缩蒸馏出来一个基础模型，然后再用数据去训练，这比从头开始训小模型，效果要好很多，比基于开源模型训出来的模型效果更好，速度更快，成本更低。</p><p></p><p>第三是智能体。智能体是当下很热的一个话题，随着智能体能力的提升，会不断催生出大量新的应用。智能体机制，包括理解、规划、反思和进化，它让机器像人一样思考和行动，可以自主完成复杂任务，在环境中持续学习、实现自我迭代和进化。在一些复杂系统中，还可以让不同的智能体互动，相互协作，更高质量地完成任务。</p><p></p><p>百度本次的正式产品发布，基本也是围绕上述思路进行的。</p><p></p><p></p><h2>如何让“人人都是开发者”？</h2><p></p><p></p><p></p><p>“人人都可以成为开发者”不能成为一个口号，必须有能让开发者随取随用的工具支撑才能实现。为此，百度推出了文心大模型4.0工具版，包括智能体开发工具AgentBuilder、AI原生应用开发工具 AppBuilder和模型定制工具ModelBuilder三大工具。</p><p></p><p></p><h4>AgentBuilder：智能体开发</h4><p></p><p></p><p>“智能体可能是未来离每个人最近、最主流的大模型使用方式。基于强大的基础模型，智能体可以批量生成，并应用在各种各样的场景。”李彦宏说道。</p><p></p><p>作为基于文心大模型的智能体构建平台，AgentBuilder为开发者提供了零代码和低代码两种低成本智能体开发模式。开发者可以根据不同行业领域、应用场景，调用平台提供的多样化工具，打造大模型时代的原生应用。无论是专业开发者还是新手，仅用“一句话”就创建智能体。</p><p></p><p>据悉，截至目前，已经有3万多个智能体被创建、5万多名开发者和上万家企业入驻。</p><p></p><p>大会现场，李彦宏演示了启德教育等智能体案例。启德教育利用百度的AgentBuilder打造专属智能体，上线第一周，就成功分发了155万次，与用户交互了5.8万次，线索转化量直线增长、有效线索的转化成本明显降低，经营效率大幅提升。</p><p></p><p></p><p></p><p>﻿李彦宏称，“每一个商家、每一个客户，都能在百度拥有专属的智能体。整个过程完全不需要编程，通过类似提示词的信息输入，和简单的几步操作调优，就能迅速生成一个智能体。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/b2/b22f96e4230a38e23c7a82b360e2e1d8.jpeg" /></p><p>﻿</p><p>另外，百度文心智能体平台，已经打通了“流量变现”的通路。通过接入智能体的相关能力，解决流量分发的难题，目前除了百度搜索，百度生态的其他产品，如小度、地图、贴吧、车机等，都能接入智能体了。</p><p></p><p>也就是说，AgentBuilder不止是开发平台，开发者还可以通过百度生态矩阵分发路径，做到“开发+分发+运营+变现”一体化。未来，百度还将接入商业插件功能，进一步实现商业闭环。</p><p></p><h4>AppBuilder：AI原生应用开发</h4><p></p><p></p><p>“AppBuilder是目前最好用的AI原生应用开发工具。”李彦宏说道。</p><p></p><p>在AppBuilder上，百度提前封装和预置了开发AI原生应用所需的各种组件和框架，如知识问答类的RAG、具备运算能力的代码解释器、生成式数据分析GBI等，开发者最快只需要三步就可以用自然语言开发出一个AI原生应用，并且能够便捷地发布到各种各样的业务环境中，甚至“无需写一行代码，就能开发出AI原生应用”。</p><p></p><p>这对应了李彦宏说的：“大模型本身并不创造价值，基于大模型创造出来的应用才真正有价值。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/f6/f6b2ec6e6a70cabbf98f085b67bc4a91.jpeg" /></p><p>﻿</p><p>李彦宏指出，AppBuilder拥有两大优势：</p><p></p><p>一是功能强大。依托文心4.0对指令的理解和遵循能力，AppBuilder能保证冷启动就达到一定水平，不会因为效果差再花很长时间去调优；依托检索增强技术（RAG），在知识问答等典型场景，问答准确率和友好回复程度都达到了95%以上，大幅超越其他同类产品。AppBuilder还提供丰富完整的组件工具，包括百度搜索等基于百度多年技术积累的AI能力组件、大模型能力组件，还有百度独家开放的业务组件等55个组件。此外，AppBuilder还提供一些主流场景的第三方API，比如航班查询、论文查询等，以及自定义组件，客户可以直接对接自己专有的任何工具和数据。</p><p></p><p>二是简单易用。使用AppBuilder，只需三步即可快速创建应用、一键分发。我们也支持开源的SDK，方便大家进行二次开发。”</p><p></p><p></p><h4>ModelBuilder：定制各种尺寸模型</h4><p></p><p></p><p>ModelBuilder是一款适合专业开发者使用的工具，可以根据开发者的需求定制任意尺寸的模型，并根据细分场景对模型进一步精调SFT，这样就能达到更好的效果。</p><p></p><p>对开发者来说，用好大模型很重要的一件事，就是掌握模型精调的方法。ModelBuilder提供了包括文心大模型系列在内的77款精选大模型和全流程的模型工具链，同时提供丰富的SDK及全栈API能力，能够完成一站式的模型精调，此外还提供模型精调样板间，开发者只需要跟着样板间一步一步精调，就能做出一模一样的专业模型。</p><p></p><p>这样，开发者可以根据需求定制任意尺寸的模型，并根据细分场景对模型进一步精调，这样就能达到更好的效果。</p><p></p><p>现场，李彦宏展示了教育行业作文批改的案例，经过数据处理、模型精调后的“作文批改助手”，不仅可以拥有更专业的老师点评思维、做到格式遵循，而且相比未精调模型，精调后的模型打分与真实的老师点评分数更为接近。</p><p></p><p></p><p></p><p></p><p>他还在现场与小度实时互动，展示小度用多个模型组合的方式来执行不同任务。例如使用小模型ERNIE Tiny执行模型路由工作，而性能最好的文心4.0则用来执行日程安排等复杂需求。据介绍，相比全部使用文心大模型的旗舰版，小度可以实现响应速度提升2倍，成本下降99%。</p><p></p><p>李彦宏表示，“这几个关于ModelBuilder的例子，展示的是百度高效低价生产模型的能力”。</p><p></p><p>据悉，ModelBuilder截至目前已经服务了8.5万企业客户，累计精调超过1.4万个模型，开发超过19万个应用。</p><p></p><p>“市面上有这么多模型，大的、小的、开源的、闭源的，在特定应用当中怎么样使用这些模型的组合，是有技巧的，这是创业者可以干的事儿，是可以提供价值增益的。”李彦宏说道。</p><p></p><p></p><h2>“我们需要一个全新的操作系统”</h2><p></p><p>﻿</p><p>“编程不再是少数经过专业训练的程序员的特权，相反，人人都是开发者。”</p><p></p><p>“编程不再需要从c/c++学起，而是从自然语言开始。”</p><p></p><p>“编程不再是面向过程、面向对象，而是面向需求，以后，编程的过程，就是一个人表达愿望的过程。”</p><p></p><p>百度执行副总裁、百度智能云事业群总裁沈抖在2024百度Create开发者大会上强调。</p><p></p><p>沈抖表示，这是革命性的变化，它会彻底颠覆原有的操作系统。</p><p></p><p>在操作系统的内核中，底层的硬件从以CPU算力为主变成以GPU算力为主，而且第一次增加了硬件和软件以外的资源，也就是被大模型压缩的世界知识。操作系统管理的对象也因此发生了本质的变化，从管理进程、管理微服务，变成了管理智能。</p><p></p><p>“传统的云计算系统依然重要，但不再是主角，我们需要一个全新的操作系统。”沈抖认为，这个全新的操作系统需要管理万卡规模的集群，需要极致发挥GPU、CPU的性能、高速互联，需要有强大的大模型作为核心引擎，包括语言大模型、视觉大模型等，这些构成了操作系统的内核。</p><p>&nbsp;</p><p>在内核层之上，这个操作系统还需要构建起强大的大模型服务能力，提供全面的模型精调、评估、部署、调用等工具链；还需要有好的应用开发工具去做工作流编排、插件管理；还有必不可少的安全和运维，隐藏掉上一代云原生系统的复杂性。</p><p></p><p>基于上述思考，沈抖宣布推出新一代智能计算操作系统——万源。“软件定义世界，万源用自然语言定义软件。”沈抖说道。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cc/ccdcefa763d11b5119c983f06032fb7d.png" /></p><p>﻿</p><p>据介绍，万源操作系统主要由Kernel（内核）、Shell（外壳层）、ToolKit（工具层）组成。</p><p></p><p>万源的内核层既包含了业界领先的ERNIE 4.0、3.5大语言模型、也包括ERNIE Speed/Lite/Tiny系列轻量模型，此外还包括文心视觉大模型和第三方大模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a8/a878aea39b2ff47b77ce4155f9f18387.png" /></p><p>﻿</p><p>另外，考虑到芯片供应不确定性带来的多款芯片并存格局，为管理好多个厂商、不同代际的芯片，百度的百舸平台实现了百卡规模、单一训练任务下，不同厂商芯片的混合训练，并且把训练的性能损失控制在3%，千卡规模的性能损失也不超过5%。百舸屏蔽掉了芯片之间的差异，给用户自由选择不同芯片组合的权力。</p><p></p><p>内核之上是千帆ModelBuilder，负责内核中模型的管理、调度、二次开发。ModelBuidler提供的模型路由服务，可以自动给不同难度的任务选择最合适的模型，实现效果与成本的最优组合，在效果基本持平的情况下，平均降低30%的推理成本。</p><p></p><p>工具层方面，千帆AppBuilder和AgentBuilder则是强大的应用开发平台。</p><p></p><p>此外，百度还发布了全球首个AI原生操作系统DuerOS X，该系统基于文心大模型全面焕新，在多模态感知、拟人化呈现上有重要升级，小度的人机交互体验将迎来质的飞跃。</p><p></p><p></p><h2>结束语</h2><p></p><p></p><p>“人人都可以成为开发者，未来必将是一个由开发者一起创造出来的未来。”李彦宏表示。那么未来，百度能否真正实现这个理想，我们拭目以待。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/FmAWMuymGv9EJaiX8dHj</id>
            <title>华为云 AI 原生应用引擎的架构与实践</title>
            <link>https://www.infoq.cn/article/FmAWMuymGv9EJaiX8dHj</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/FmAWMuymGv9EJaiX8dHj</guid>
            <pubDate></pubDate>
            <updated>Tue, 16 Apr 2024 03:31:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI 大模型, GenAI, 软件变化, AI 原生应用引擎
<br>
<br>
总结: 在AI大模型与GenAI重塑软件的大趋势下，软件行业将发生本质性变化，AI原生应用引擎将成为重要技术。 </div>
                        <hr>
                    
                    <p>AI 大模型与 GenAI 重塑软件的大趋势下，软件会发生哪些本质的变化？如果“所有软件都值得用 AI 重做一遍”，那么该如何重做？</p><p></p><p>将在 2024 年 6 月 14-15 日举办的 <a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">ArchSummit 架构师峰会</a>"上，邀请到了华为云架构与设计部首席架构师马会彬老师来会议上演讲，他将分享华为云“AI 原生应用引擎”的架构与实践话题，从 GenAI 重塑软件的本质变化、以及 AI 原生的概念与内涵解剖，分享华为云 AI 原生应用引擎的架构设计、及实践思考。</p><p></p><p>在正式演讲之前，InfoQ 采访了马会彬老师，请他提前剧透关于 AI 原生应用开发的内容，希望对大家有帮助。</p><p></p><p>InfoQ：您认为在 AI 大模型与 GenAI 重塑软件的大趋势下，软件行业会发生哪些本质性的变化？如何理解 AI 对软件行业带来的颠覆式创新和技术驱动力？</p><p></p><p>马会彬：以大模型为核心的 GenAI 技术所带来的“创造能力”、“推理能力”以及“交互能力”，都是之前的经典 AI 或者经典软件无法实现的，这就为软件这种数字基础设施带来了颠覆式的变革机会；此外，以大模型为核心的 GenAI 也预示着极强的通用 AI（AGI）技术趋势，而且相关技术的进展可以说是日新月异的，尤其是来自以下几方面的技术驱动：1）以 Embedding（语义向量）为代表的技术，解决了万物智能的通用表示问题；2）以 Transformer 架构为代表的技术，解决了 AI 模型的通用架构问题；3）以 Scaling Law 为代表的技术，解决了算力摩尔定律向智能摩尔定律、以及场景泛化摩尔定律的变换；4）以 LLM OS 为代表的系统抽象，解决了通用 AI 计算架构的问题；</p><p></p><p>由此可以判定，从 AI 大模型走向通用人工智能（AGI）将不仅仅是一种技术趋势，而是一种可见的确定性的未来，而这种变化，必将对软件行业带来颠覆式的创新。</p><p></p><p>InfoQ：从业务视角和技术视角来看，为何认为所有软件都值得用 AI 重做一遍？在 AI 原生应用的概念演变中，业务需求与技术手段的关系发生了怎样的变化？</p><p></p><p>马会彬：对软件产业来说，可以分别从需求方与供给方两个维度来理解；从需求方来看，GenAI 对企业价值创造与价值变现的全流程、以及员工的生产力工具，都将带来积极的重大变化，尤其是在智能客服、智能营销、智能研发、智能办公等领域。另外，从供给方来看，GenAI 对软件的技术架构、软件工程体系、软件的交互设计、以及软件的商业模式，也将带来根本性的革新。</p><p></p><p>InfoQ：您如何定义 AI 原生架构及 AI 原生应用？在实践中，AI 原生架构和传统架构有何区别？</p><p></p><p>马会彬：行业目前对 AI 原生有几种不同的称呼，譬如：AI First、AI Native、AI Oriented Programming 等，简单 AI 原生或 AI 原生应用就是指以 AI 大模型为底座、以生成式架构为核心的一种新应用，也可以说 AI 原生应用就是指应用价值主要来自 AI 大模型的一类应用。传统应用更多以算法、数据结构、逻辑编程为特点，而 AI 原生应用的特点则是以生成式 AI 大模型为基础，以 Prompt 提示词为业务输入，以生成式内容为结果，因此也可以称传统应用为“构成式架构”、而 AI 原生应用为“生成式架构”。</p><p></p><p>InfoQ：对于 AI 原生架构的概念和内涵，您能详细解释一下吗？AI 原生架构如何与现有的软件架构相区别和结合？</p><p></p><p>马会彬：对于 AI 原生架构，一定程度上可以参照业界对云原生架构的概念定义来理解。云原生（Cloud Native）是指构建、运行、管理基于云环境、利用云环境、适应云环境而发展起来的新的软件系统实践范式，包括具备微服务架构、弹性伸缩、分布式、高可用、多租、自动化运维等关键特征的架构实践，以及与之匹配的全功能团队并高度协作的组织实践、采用微服务实现持续交付的工程实践等。</p><p></p><p>那么，AI 原生（AI Native）可以定义为构建、运行、管理基于 AI 大模型、并适应 AI 大模型的应用而发展起来的软件系统实践范式，以这种范式构建的应用就称为 AI 原生应用。AI 原生的软件新范式，包括 AI Native 的软件架构实践、AI4SE 的软件工程实践，以及面向 AI 编程的组织实践，从而实现存量软件的 AI 重塑、以及孵化新的 AI Native 超级应用。</p><p></p><p>InfoQ：在 AI 重塑存量软件方面，您认为有哪些方式和演进路径？在实践中，如何将 AI 技术应用于现有的存量软件中？</p><p></p><p>马会彬：AI 并不是取代存量软件，而是增强存量软件的功能或者改进存量软件的使用体验。大概有四种方式：</p><p>存量软件的某个模块被 AI Native 的模块替代；存量软件增加 AI 模块来提升、改善能力及体验；存量软件在交互上被 AI Agent 类应用接管；存量软件在软件工程上采用 AI4SE 的生成方式，来提升软件开发过程的效率及质量。</p><p></p><p>这里最著名的例子是微软的 Office Copilot。</p><p></p><p>InfoQ：您提到的新的 AI 原生超级应用是指什么？这些超级应用与传统应用相比有何优势和特点？</p><p></p><p>马会彬：随着大模型能力的快速提升，未来可能会出现两类 AI 原生的超级应用。一类是类似 MidJourney 这样的全新的 AIGC 应用；另一类是类似 ChatGPT / Notion AI 这样的新超级入口应用。与传统应用相比，AI 原生的超级应用能够适应的场景更多、功能更广，体验更佳，在开发过程和模式上更多以数据、算力、模型驱动，因此从价值上看应用的价值绝大部分来自于 AI 大模型，业界有说法认为至少 50% 以上；这就与传统应用以产品经理、开发工程师为主的开发模式，形成本质上的差异。</p><p></p><p>InfoQ：您能介绍一下华为云 AI 原生应用引擎的架构设计和实践思考吗？在实践中，该引擎的主要技术特点和应用场景是什么？</p><p></p><p>马会彬：华为云 AI 原生应用引擎来自华为公司自身的 AI 应用实践，属于华为云“经验即服务，让优秀得以复制”战略落地的一个实例。华为公司 +AI 的实践历程起始于 2005 年，从最初的商业智能、到场景 AI、普惠 AI（内部也称 AI 1.0），走到 2022 年的生成式 AI（内部也称 AI 2.0）；与 AI 1.0 相比，AI 2.0 在内部也称为是“经验主义”的革命，可以将公司在过去二三十年中持续累积的大量研发代码、专业知识、标准规范等，训练成特定的 AI 大模型，进而服务于上层的业务场景、提供丰富的 AI 技能、超越预期的效果、以及接近人的普适性，从而为生产力提升、生产方式的转变，以及交付模式的变化，带来颠覆性的变化。华为云 AI 原生应用引擎基于华为自身的实践经验及资产积累，是基于华为云的一个云服务化产品实现，定位就是从企业 CIO 视角及应用开发者视角，辅助选好、用好、管好大模型，以及基于大模型实现业务场景和 AI 应用的快速开发与创新。</p><p></p><p>InfoQ：从 CIO 的视角来看，AI 原生应用落地面临的主要挑战和需求是什么？在 AI 原生应用的落地过程中，如何满足企业 CIO 的需求？</p><p></p><p>马会彬：从 CIO 视角看大模型与 AI 原生应用落地主要面临 6 大挑战：</p><p>模型很多、如何选择最匹配、最适合的；如何提升模型在场景落地上的智能效果；如何建立 IT 部门与业务部门之间 AI 应用构建的协同关系；如何准备高质量数据，帮助 AI 理解业务；如何建立完善的风险保障机制，保障 AI 落地的安全可信；如何平衡、优化大模型带来的成本与效益；</p><p></p><p>华为云 AI 原生应用引擎正是针对这 6 大挑战，基于华为自身的实践经验和生态资产积累，构筑产品功能和竞争力。</p><p></p><p>InfoQ：华为内部在 AI 原生应用落地方面的实践经验有哪些？在实践中，华为是如何克服挑战，取得成功的？</p><p></p><p>马会彬：华为在大的 IT 变革上都是采取自顶向下的方式，首先在公司层面成立了专业的生成式 AI 的变革组织，来统一制定相关的规范和策略，基于此明确建立：一套标准方法、一个 AI 平台、N 类 AI 应用；其次是确定数据治理规范、数据集资产与 AI 应用场景的对应及适配；最后是选择合适的业务场景，从小范围试点逐渐到大规模推广的过程。因此，从自身实践，不仅仅是孵化了一个 AI 平台产品，也包括了 AI 应用落地的统一架构、统一规范、业务实施方法论等知识资产，这些“经验资产”都可以基于华为云来服务更广泛的客户。</p><p></p><p>InfoQ：华为云如何赋能 AI 原生应用创新？其架构和实践方面有哪些特点？在推动 AI 原生应用创新过程中，华为云的主要贡献和作用是什么？</p><p></p><p>马会彬：简单可以归纳为三点：1）华为云 AI 原生应用引擎，提供一站式 AI 应用开发、运行与管理等；2）基于内外部生态沉淀的丰富行业 AI Know-How 和 AI 资产；3）定义 AI 原生应用的参考架构和最佳实践经验。在过去，华为云引领了云原生 2.0 的技术推广，我们也希望与生态伙伴、开发者一起再次引领 AI 原生应用、从理念到实践的推广，助力 AI 大模型从技术创新走向生产场景落地。</p><p></p><p>【活动推荐】</p><p></p><p>2024年6月14-15日在深圳举办的 <a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">ArchSummit 架构师峰会</a>"上，我们邀请了 CNCF、顺丰集团、腾讯、百度等企业的专家来演讲。会议上还设置了大模型、架构升级等专题，如果你感兴趣来会议上听演讲，欢迎进入 ArchSummit 会议官网，查看详细的会议内容。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a3/a30f28d171de17ce91fbcde492506165.jpeg" /></p><p></p><p>会议现已进入 8 折早鸟购票阶段，可以联系票务经理 17310043226 , 锁定最新优惠。扫描上方二维码添加大会福利官，免费领取定制福利礼包。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/wjeNOThP0GqqvO6rt1h5</id>
            <title>从启发式到模型化，京东推荐广告排序机制演化</title>
            <link>https://www.infoq.cn/article/wjeNOThP0GqqvO6rt1h5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/wjeNOThP0GqqvO6rt1h5</guid>
            <pubDate></pubDate>
            <updated>Tue, 16 Apr 2024 02:44:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 广告排序机制, 实时竞价环境, 重排模块, 广告收入
<br>
<br>
总结: 本文介绍了广告排序机制在实时竞价环境中的重要性，重排模块在确定广告流量分发和计费方式中的作用，以及广告收入对于广告系统的重要性。同时，文章还回顾了传统拍卖机制的经济学背景和激励相容原则，以及电商场景下的推荐广告排序机制所面临的新挑战。 </div>
                        <hr>
                    
                    <p></p><p><img src="https://static001.infoq.cn/resource/image/d2/4d/d2d1277fc782e2813502b9e858a2d54d.png" /></p><p></p><p></p><h2>1、序言：广告排序机制的前世今生</h2><p></p><p></p><p></p><h3>1.1、简介：广告排序机制</h3><p></p><p></p><p>在线广告是国内外各大互联网公司的重要收入来源之一，而在线广告与传统广告最大的区别就在于其超大规模的实时竞价环境：数以万计的广告主在一天内可以参与亿级别的流量竞拍。在这复杂的实时竞价环境中，广告系统的重排模块（Rerank）担负着确定流量最终分发以及计费方式的重要职责。其中，流量分发会决定最终曝光的广告物料，而流量计费则会对曝光广告进行合理的收费，转化为广告收入。</p><p></p><p>不同于自然搜推系统侧重用户体验的场域定位，广告流量场考量的是在用户体验约束下的流量变现问题。在这个背景下，传统重排模块（Rerank）在电商在线广告中的业务定位发生了相应的变化，需在原有多业务目标（点击、GMV、时长等）基础上进一步兼顾平台广告收入，同时对胜出的广告进行合理公平的计费。由于其特殊的业务属性，广告系统中的重排有时也被称为广告排序机制，其目的旨在促进用户、商家以及平台三方互利共赢。</p><p></p><p>结合业务背景和系统功能，我们将广告排序机制的目标 定义如下：</p><p></p><blockquote>广告排序机制目标：根据系统上游提供的物料（召回 / 粗排）及 流量价值预估值（精排 pctr、出价 bid 等），综合考虑 用户体验（上下文、多样性等）、平台收益（点击、收入、GMV 等），设计激励相容（鼓励广告主说真话）的拍卖机制（分配和计费规则）。</blockquote><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/02/02639ccf1feff9ef178bef5915a58568.png" /></p><p></p><p></p><h3>1.2、前世：经济学视角下的传统拍卖机制</h3><p></p><p></p><p>在排序机制目标中我们提到了激励相容（鼓励广告主说真话），事实上，激励相容是经济学中机制设计的重要原则之一。下面，我们简要回顾一下传统拍卖机制的经济学相关背景，</p><p>1.「机制设计」从经济学的视角来看，广告流量的分配及售卖可以被看作是 机制设计（Mechanism Design）【1】中的一类问题，拍卖机制设计及其相关工作在过去 60 年中，先后四次获得诺贝尔经济学奖。经典拍卖机制如 GSP、VCG 由于其良好的博弈性质以及易于实现的特点使其在 2002 年前后开始被互联网广告大规模的使用。</p><p></p><p>2.「广告主类型」传统拍卖机制往往假设广告主是利益最大化（Utility Maximizer）的，即最大化 GMV 与成本的差值，然而，随着智能营销手段在广告投放端的普及，越来越多的广告主通过向平台表达期望成本和目标，借助智能出价的算法能力进行广告实时投放，广告主的类型逐渐转变为价值最大化（Value Maximizer）【2】，即在满足成本约束的条件下最大化分配价值（例如 GMV），而非单纯追求差值的最大化。</p><p></p><p>3.「激励相容约束」鼓励广告主在平台按照真实意愿出价是拍卖机制设计中一项非常重要的经济学约束，激励相容的拍卖机制通过鼓励广告主说真话，大大简化了出价策略设计，优化了博弈环境，同时也为平台设计收入最大化的机制提供了更便捷的抓手。</p><p></p><p>4.「个体理性约束」除了激励相容的约束以外，一个良好的拍卖机制还需满足个体理性的约束条件，简单来说，个体理性的约束条件要求平台对广告主的最终收费不高于广告主的出价，保障广告主的最低收益非负。</p><p></p><h3>1.3、今生：电商场景下的推荐广告排序机制</h3><p></p><p></p><p>随着互联网广告的飞速发展，流量增长迅速，用户规模及行为都更加庞大且丰富，广告物料也从原来简单的商品展示，拓展到了包含聚合页、活动、店铺、视频以及直播等多种多样的物料类型，此外，广告主的目标和表达方式也从原先的手动出价，转变为了由平台代理的，带有预算和成本控制的智能出价。因此，广告排序机制的设计也遇到了许多新的挑战。结合京东业务场景，我们总结了以下三个问题与大家分享：</p><p></p><p>1.「多元物料价值可比」：更为丰富的物料类型（活动、店铺、直播等内容类广告）需要更为准确和全面的物料价值预估，使得多元的物料价值可比，进而提升流量分发效率；</p><p>2.「模糊用户兴趣捕捉」：相比于搜索广告与用户搜索 query 强相关的广告展示结果，推荐广告的用户兴趣更难精确捕捉，需在流量分配环节兼顾用户兴趣的探索和利用；</p><p>3.「信息流多物品拍卖」：信息流广告序列级别的分发和售卖的场景是经济学中典型的多品拍卖问题，与单品拍卖不同，多品拍卖面临着指数级增长的机制搜索空间，复杂的出价策略空间以及更难满足的激励相容约束条件等问题，是学术界和行业的公认难题。</p><p></p><p>为了更好地刻画上述提到的三个挑战，我们将排序机制的问题进行了以下数学建模。 在上文中我们提到，机制要解决的问题是如何基于上游提供的信息（物料、价值预估），完成在用户体验约束下流量的高效分发以及变现。</p><p></p><p>流量的高效分发依赖于我们对流量价值的精准衡量以及高效的探索利用机制，将流量质量简写为 adq，我们有</p><p></p><p><img src="https://static001.geekbang.org/infoq/28/28ee475a7600dca84643c7fcef5b91ff.png" /></p><p>​</p><p>其中，pctr 为上游精排给出的点击率预估值，bid 为广告主的出价，e 为扰动项用以建模探索力度，映射 f 则决定流量价值的融合排序关系。可以看到，流量的高效分发依赖于对流量单点价值的准确衡量（函数内的重要因子如 pctr、bid 等），以及流量高效探索利用的分发机制（即 e 及映射关系的设计）。对于流量的变现问题，与单品拍卖设计一样，需设计适配流量分发机制的计费方式，来保障机制的激励相容，假设了一次请求曝光四个广告，广告收入可以拆解为</p><p></p><p><img src="https://static001.geekbang.org/infoq/8f/8f53ae5456a3dbdf6dc2212bb348da84.png" /></p><p></p><p>其中，pij 为第 i 次请求对第 j 个广告的扣费。因此，我们可以将问题进一步拆解为以下三项。</p><p></p><p>1.「流量价值精准衡量」：在物料形式丰富多样的环境中，如何将流量分发依赖的重要排序因子（pctr、bid 等）预估准确？</p><p>2.「流量高效探索利用」：在用户兴趣模糊难捕捉的情况下，如何设计一套高效的利用和探索（映射 f 以及探索扰动项）分发机制?</p><p>3.「流量高效公平变现」：在推荐信息流广告多品拍卖场景下，如何设计一个适配的计费方式，在保证机制激励相容（DSIC）的同时，提升平台收入（rev）？</p><p></p><p>下面，我们结合京东推荐广告排序机制演化发展的路线，给出我们对这三个问题的思考和解决方案，也希望抛砖引玉，与大家一起进行探讨。</p><p></p><p></p><h2>2、正文：京东推荐广告排序拍卖机制演化</h2><p></p><p></p><h3>2.1、价值先行：复杂业务场景下的流量价值准确衡量</h3><p></p><p></p><p>随着电商业务的飞速发展，推荐物料展示形式从一屏单品、单一商品形式逐渐拓展到一屏多品、多样物料形式（包括商品、店铺、活动页、聚合页）的复杂业务场景，如何统一且准确衡量不同物料的价值，是困扰排序机制的一大难题，为此，我们从京东业务场景出发，重新审视排序阶段的价值理解，通过对单点价值进行更准确地预估，全局信息更深入地使用，实现了复杂业务场景下的流量价值准确衡量。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b0/b00d733925504ab38cee6139840637b6.png" /></p><p></p><p>「用户行为的 MDP 建模」京东推荐广告信息流场景每次以一个组合形式曝光，如下图所示，用户访问京东 app，浏览推荐场景时是一个典型的马尔科夫过程（MDP），对于某个曝光序列组合，用户可能发生点击、下翻和退出等动作，针对某一个序列排序价值，我们拆分为当前价值、点击后价值、下拉后价值。很自然地，我们可以将不同的候选曝光序列作为不同的状态（State），用户的点击、下翻以及退出等常见操作作为动作（Action），点击率、下翻概率以及退出概率作为转移概率（transition probability），收集用户后验反馈作为奖励（reward）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8e/8ec56432ecbd5bfe6a6b623bcf52d660.png" /></p><p></p><p></p><p></p><h4>由点到线：从单点到全局的价值预估</h4><p></p><p></p><p>传统排序机制通常使用以 ctr 以及 ecpm 作为重要排序因子，然而，根据上述 MDP 建模，我们可以清楚的看到 ctr / ecpm 只反映了当次请求的价值，并没有准确反映这次请求在内页 / 剩余访问带来的整体价值。事实上，一次请求不仅在曝光的当下产生价值，某个物料在被点击或者序列被下翻后也依然产生价值，这两个动作分别通过点击概率和下翻概率与当前曝光发生关联。</p><p>因此，针对某个曝光物料，我们定义点击进入内页后产生的点击和消费为内页价值，并搭建了一套与精排并行的预估系统；针对曝光序列，将优化的视野从单个请求扩展到会话，最大化考虑在更长时间范围内的价值，为此，我们定义下翻进入下一页产生的点击和消费为序列下翻价值，并在精排模块之后搭建了长期价值预估模型，负责对下翻概率和下页价值进行预估。</p><p></p><p><img src="https://static001.geekbang.org/infoq/97/97a418aaa8467733ee62443ad56f8299.png" /></p><p></p><p>﻿相比于点击率预估的二分类任务，内页价值和长期价值是连续值，是典型的回归任务，这种任务受离散点的影响比较大，而且有效样本更稀疏（有效正样本为外页发生点击且内页有行为样本），样本内分布差异大。此外，不同于时长预估任务【3，4】，价值预估任务还存在预估时看不到内页信息的 partially observable 等问题，这些都是准确预估内页价值和下页价值面临的特有挑战。针对以上这些问题，我们通过将回归问题分类化、多场景多任务联合建模、先验信息辅助、离线蒸馏等方式，显著提高了模型的价值预估能力，为流量价值的高效分发打下了坚实的基础。</p><p></p><p></p><h4>点线成面：基于异步计算的价值校准</h4><p></p><p></p><p>价值预估模型考虑的是单个物料的全局价值，然而信息流广告是多坑位曝光形式，单个物料的价值（点击率、内页价值等）不仅受到当前物料影响，而且还受到周围其他物料影响（例如，某物料内页价值特别高，说明内页具有极大吸引力，用户进入内页后再退出外页的意愿显著降低，那么周围其他物料的点击率将受到明显影响），仅基于单点信息的前序模块预估值存在严重偏差。</p><p></p><p><img src="https://static001.geekbang.org/infoq/94/949b700cc46f16404a044370944670ea.png" /></p><p></p><p>相比于精排阶段，重排阶段拥有更丰富准确的序列信息、内外页信息和下翻概率等全局信息。由于重排环节位于系统的出口处，可用的耗时空间有限，无法进行大规模复杂的特征提取和计算，因此，我们采用了异步前置计算的方式，利用前链路充足的耗时以及算力空间，提前计算价值校准需要的序列以及候选队列信息，同时我们在重排阶段引入了价值纠偏模块，对序列内各物料的点击率、内页价值等指标同时做校准。对于点击率校准任务，采用曝光未点击做负样本，曝光点击做正样本，对于内页价值校准任务，以点击消费数据为正样本，点击无消费数据为负样本，曝光未点击数据作为中间样本，使用 stop-grident 阻断中间样本对内页价值预估任务的影响。通过异步计算在耗时约束下引入全局信息，同时建模序列点击率和内页价值信息相互学习，在价值校准模块实现离线 auc 以及 rmse 指标的双提升，上线带来了显著的收益提升。</p><p></p><p></p><h3>2.2、柳暗花明：模糊用户兴趣场景下的的流量高效探索利用</h3><p></p><p></p><p>不同于搜索场景下用户有明确的意图表达，推荐场景中无用户 query ，无法获取直接兴趣，若过于关注相关性而推荐用户历史经常访问的类目，则无法满足用户的潜在兴趣，带来信息茧房效应，导致用户厌烦，极端情况还会产生投诉和舆情；流量的高效探索利用同样也存在很多难点。首先，流量的探索利用依赖召回、精排、重排等全链路的工作，难以单点优化；探索往往与平台短期目标（点击、收入）呈负相关，如何实现探索与利用的平衡是一个挑战；不同用户对探索的偏好是个性化的，探索偏好需做到千人千面，然而用户对于曝光列表的探索偏好真实反馈难以直接获取，导致探索的端到端学习目标难以量化。</p><p>针对模糊用户兴趣场景下的流量高效探索利用问题，我们从基于用户兴趣的商品预训练【5，6】，以及系统化探索【7，8，9】两个方面进行建模。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4f/4f0a7410bd98c23c3a5f011b248c63d3.png" /></p><p></p><p></p><h4>磐石之固：基于用户兴趣的商品预训练</h4><p></p><p></p><p>对模糊用户兴趣的精细化建模，依赖对商品物料理解的建设。电商场景下自有的商品标签体系如类目、产品词等，存在不准确、冗余、粒度过粗、层次化不足的问题。对此，我们基于大规模的 NLP/CV 多模态预训练模型，产出更准确的物料类目标签和商品 embedding，为流量的高效探索利用奠定基础。基于残差量化变分编码的思想，对 embedding 表征进行残差量化，保留了 item 之间的层次化语义关联，将预训练语言模型的模式从“text ==&gt; representation”改为“text ==&gt; code ==&gt; representation”的方式，缓解了预训练 embedding 过度依赖文本描述信息的问题，防止 item 之间的 gap 被过分夸大。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f9/f9c48311446c37e75fcf1ebce6d7a478.png" /></p><p></p><p></p><h4>高山流水：系统化流量探索和利用</h4><p></p><p></p><p>流量高效探索利用包括多样性控制、探索与利用的分配机制等，核心是如何在满足多样性约束情况下，平衡流量探索和利用效率，提升用户长期体验和业务效果。因此，在模糊用户兴趣场景下进行流量的高效探索利用，对于推荐广告的分配提效至关重要，可以辅助用户开拓兴趣边界，提升用户体验和长期留存，有利于业务长期增长。</p><p></p><p>为此，我们提出了层次化、全链路、个性化的流量探索利用方案。通过多维度的密度打散策略高效解决了极端多样性问题；在召回、候选集阶段、序列生成评估阶段等上下游全链路引入多样性和探索模块；在重排模块，基于序列生成-评估框架，实现了列表级探索利用方案，其中在序列生成阶段，基于端到端生成模型实现了相关性和多样性多目标协同优化；在序列评估阶段，将用户的长期体验和探索偏好建模为可量化的中短期反馈，实现对用户整体价值的端到端建模。</p><p></p><p><img src="https://static001.geekbang.org/infoq/86/867d8f80ef460b4b8d9bd30c8c45e461.png" /></p><p></p><p></p><h3>2.3、百花齐放：多品拍卖场景下的流量高效公平变现</h3><p></p><p></p><p>在单品拍卖场景中，经典的 Myerson 引理告诉我们：一个机制是激励相容的，当且仅当其分配方式同出价是单调非减的，根据 Envelop Theorem，其收费公式由分配规则唯一确定（至多相差一个常数)。然而，在多品拍卖场景下，由于指数级别的组合搜索空间，激励相容的严格要求，导致收入最大化的多品拍卖机制设计十分困难。</p><p></p><p><img src="https://static001.geekbang.org/infoq/65/65cb30be8acadc89d67bd10ade2171ab.png" /></p><p>​</p><p>因此，自 2019 年起，学术界兴起了一个新的方向：Mechanism Design with Deep Learning，尝试使用神经网络来近似激励相容的收入最大化多品拍卖机制，如 RegretNet[10]、RDM[11]等，通过将机制设计问题建模成为带激励相容约束的收入最大化问题，利用神经网络强大的学习能力，来逼近收入最大化的激励相容多品拍卖机制。然而，由于计算复杂度等原因，这些工作并不能很好的在业界大规模落地。此后，工业界也逐渐出现了利用海量数据驱动的深度拍卖机制，如阿里妈妈的 DeepGSP【12】，DNA【13】以及美团的 NMA【14】等工作。</p><p></p><p>京东自 2021 年起开展了深度拍卖机制在推荐广告场景的实践和应用，由最初的 TopK 贪心排序 + GSP 的拍卖机制，升级为基于 GSP 的分坑位模型化拍卖 DeepAuction，最终演化为基于强化学习的多品拍卖 ListVCG，实现了从行业跟随到行业领先机制的转变和突破，下面我们分别介绍相关工作和机制的演化过程。</p><p></p><p></p><h4>DeepAuction：从 TopK 贪心排序到分坑位模型化拍卖</h4><p></p><p></p><p>在模型化拍卖逐渐成为行业主流之前，TopK 贪心排序 + GSP 计费的方式是行业通用方案。然而，传统 GSP 不适用于多品组合拍卖，多品拍卖计费算法（VCG）由于其计算复杂度以及短期对平台收益的损失，落地困难。因此， 我们首先尝试通过基于 GSP 计费的分坑位模型化拍卖实现传统拍卖机制到模型化拍卖的切换。具体地，我们通过神经网络在每个坑位对不同广告物料计算质量分，根据该质量分进行排序以及二价扣费。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cf/cf89245cb9b1b5c450d00141be84babf.png" /></p><p></p><p>不同于传统基于 ecpm 的排序方式，模型化打分支持多业务目标的端到端学习。我们引入了基于强化学习 Actor-Critic 框架来建模流量长期价值，离线使用策略梯度回传方式对策略打分参数进行学习更新，在线我们通过 permutation invariant 的候选集编码器对候选物料进行建模，传入动态拍卖参数预估模型，进而实现分坑位的动态质量分计算。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/caf3fbf42543a683a845962ceb5dea7c.png" /></p><p></p><p></p><h4>ListVCG：基于课程强化学习的序列拍卖机制</h4><p></p><p></p><p>前面有提到，信息流广告是典型的多品拍卖场景，业界通用方案 GSP 在理论、效率上均不是最优解，VCG 多品拍卖机制是我们的理想方案。但是 VCG 仅仅是一个理论上的解决方案，他的前提是需要高效的找到最佳组合拍卖结果。与此同时，推荐业务复杂，是典型的多目标优化场景，但是标准 VCG 是追求社会福利最大化的机制，因此在由 GSP 切换到 VCG 时，平台收益在短期内会显著下降，这也是业界公认的 VCG 机制切换难题。因此如何将 VCG 与多目标优化进行结合也是我们面临的主要挑战。结合京东的实际应用场景，我们提出了 ListVCG 拍卖机制，来解决上述问题。</p><p></p><p>首先面临要解决的是 700 选 4 的排列组合问题，序列的搜索空间上千亿，我们将此定义成一个强化学习的问题，借鉴了经典的 Actor-Critic 架构，Actor 输出概率矩阵，通过采样的手段去求解排列组合问题，同时我们利用用户的真实反馈去提升 Critic 的评估水平，挑选出的最优组合会利用策略梯度的方式指引 Actor 学习。通过这种互相迭代自提升的方式去高效逼近最优组合。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f5/f54594fea0c68e6728db7192243151b0.png" /></p><p></p><p>﻿VCG 下的多品拍卖同时是一个经济学问题，需要满足激励相容的拍卖理论约束来保证长期的生态健康发展，然而常见的多目标问题的优化思路会使得无法使用 vcg 计费。因此我们在 Listvcg 中对于 ECPM 价值进行了参数化的变形，在保证可计费的同时通过可学习的参数来满足平台收益、社会福利、用户体验以及物料整体价值多目标优化的诉求。</p><p></p><p>为了更好地对流量长期价值进行建模，我们自然地引入了强化学习的方式，起初我们尝试了传统 off-policy 的 Q-Learning 算法如 DDQN 等，然而，由于后验反馈的奖励稀疏，模型训练效果不稳定，因此，我们尝试引入 reward shaping 以及 curriculum RL 的思想，通过加入稠密先验奖励缓解数据侧的奖励稀疏，并让模型在相对简单的单步决策任务（如序列曝光、点击、单步价值预估等）收敛后，再学习长期决策任务，使得模型效果有了显著提升，在优化长期竞价环境的同时，实现了短期收入和广告主 roi 的上升。</p><p></p><p><img src="https://static001.geekbang.org/infoq/64/64b101efa94f993cfe3210f4067d1415.png" /></p><p></p><p></p><h2>3、结语和展望</h2><p></p><p></p><p>推荐广告排序机制通过对流量价值的准确衡量，模糊用户兴趣场景下的流量高效探索利用以及多品拍卖场景下的流量高效公平变现，打造了符合京东推荐广告场域特点的排序机制，实现了流量的高效分发和变现，助力推荐广告业务增长。未来，排序机制团队会持续沿着这三个方向，并在自然结果混合排序、智能出价环境下持续进行排序机制的迭代优化。</p><p></p><p>最后，我们也欢迎对排序拍卖机制、推荐系统或在线广告感兴趣的小伙伴加入京东推荐广告组，共同成长，一齐助力京东广告业务的发展！联系邮箱：ganchun1@jd.com。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/PIe0NI9o7oPSSij9ljy7</id>
            <title>从商品图到海报生成 京东广告 AIGC 创意技术应用</title>
            <link>https://www.infoq.cn/article/PIe0NI9o7oPSSij9ljy7</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/PIe0NI9o7oPSSij9ljy7</guid>
            <pubDate></pubDate>
            <updated>Tue, 16 Apr 2024 02:44:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 电商广告图片, AIGC 技术, 创新性方法, 海报生成模型
<br>
<br>
总结: 电商广告图片的制作存在效率和成本限制，AIGC 技术虽有进展，但在广告图片应用中仍有问题。为解决难题，京东提出了关系感知扩散模型、背景生成模型和海报生成模型等创新性方法，实现了高质量广告创意的自动生成，提升了平台广告收入。 </div>
                        <hr>
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/cb/cb41311f8ad0b8f649ac46e98a4ce980.jpeg" /></p><p></p><h2>一、前言</h2><p></p><p></p><p>电商广告图片不仅能够抓住消费者的眼球，还可以传递品牌核心价值和故事，建立起与消费者之间的情感联系。然而现有的广告图片大多依赖人工制作，存在效率和成本的限制。尽管最近 AIGC 技术取得了卓越的进展，但其在广告图片的应用还存在缺乏卖点信息、难以规模化和个性化以及不利于卖点展示等问题。</p><p></p><p>为了解决上述业界难题，京东广告部门在 2023 年提出了一系列创新性方法：首先提出了关系感知扩散模型将卖点信息叠加在人工制作的商品图片上；之后提出了融合类目共性和个性化风格的背景生成模型来实现规模化和个性化的图片自动生成；最后提出了基于规划和渲染的海报生成模型来实现图文创意的端到端生成。借助以上方法，既实现了高质量广告创意的自动生成，又带来了平台广告收入的提升。</p><p></p><p></p><h2>二、基于关系感知扩散模型的海报布局生成</h2><p></p><p></p><h3>【2.1 技术背景】</h3><p></p><p></p><p>海报布局的生成旨在预测图像上视觉元素的位置和类别。此任务对于海报的美学吸引力和信息传播起到了至关重要的作用。创建一流的海报布局需要同时考虑到布局元素的彼此关系和图像组成，因此这项要求很高的任务通常由专业设计师完成。但是人工设计是一件既耗时又费财的事情。为了以低成本生成高质量的海报布局，自动布局生成在学术界和工业界越来越流行。</p><p></p><p>随着深度学习的出现，一些内容无关的方法被提出用于学习布局元素之间的关系。但这些方法更关注元素之间的图形关系而忽略视觉内容对海报布局的影响，直接将这些方法用于海报布局生成可能会产生负面影响。为了解决这些问题，一些内容有关的方法被提出用于布局生成。尽管这些方法考虑了图像本身的内容信息，甚至额外引入了图片的空间信息，但是两个重要的因素仍该被考虑进去。一方面，文字在海报的信息传递中扮演了重要的作用；另一方面，一个好的布局不仅要考虑单个元素的坐标是否准确，也要考虑到元素之间的坐标关系。</p><p></p><p>针对上述问题，我们提出了一个关系感知扩散模型用于海报布局生成领域，该模型同时考虑了视觉-文本和几何关系因素。 由于扩散模型有在许多生成任务中取得了巨大成功，我们遵循噪声到布局的范式，通过学习去噪模型逐渐调整噪声来生成海报布局。在每个采样步骤中，给定一组以高斯采样的框分布或最后一个采样步骤的估计框为输入，我们通过图像编码器提取 RoI 特征作为生成的特征图。 然后是图文关系感知模块（VTRAM）被提出用于建模视觉和文本特征之间的关系，这使得布局结果由图像和文本内容同时决定。 与此同时，我们设计一个几何关系感知模块 (GRAM)基于 RoI 彼此的相对位置关系增强每个 RoI 的特征表达，这使得模型能够更好地理解布局元素之间的上下文信息。受益于新提出的 VTRAM 和 GRAM 模块，用户可以通过预定义布局或改变文本内容以控制布局生成过程。</p><p></p><p></p><h3>【2.2 基于扩散模型的海报布局生成】</h3><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/bc/bce29753c97700ebf5f8188c4d61719d.webp?x-oss-process=image%2Fresize%2Cp_80%2Fformat%2Cpng" /></p><p></p><p>扩散模型是一类使用马尔可夫链将噪声转换为数据样本的概率生成模型。 如上图所示，我们将海报布局生成问题作为一个噪声到布局的生成过程，通过学习去噪模型以逐步调整噪声布局。 因此扩散模型生成的海报布局也同样包括两个过程：扩散过程和去噪过程。 给定一个海报布局，我们逐渐添加高斯噪声以破坏确定性的布局结果，我们称这个操作为扩散过程。相反给定初始随机布局，我们通过逐步去噪的方式获得最终海报布局称为去噪过程。</p><p></p><p></p><h3>【2.3 图文关系感知】</h3><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/61/61a465f27361953467748d9bd5396182.webp?x-oss-process=image%2Fresize%2Cp_80%2Fformat%2Cpng" /></p><p></p><p>相较于直接将视觉特征和文本特征简单拼接，我们设计了一个能够识别视觉-文本关联的模块（VTRAM）来对齐图像与文本的特征域。这个模块能够意识到视觉元素和文本元素之间的关系，并能从图像和文本中优化特征的利用，这样使得对内容的理解更加全面。上图展示了 VTRAM 的流程，它通过两步实现了第 i 个 RoI 特征𝑉𝑖和语言特征𝐿的多模态融合。首先，为了在视觉特征中添加明确的位置信息，将 RoI 特征𝑉𝑖及其对应的位置嵌入进行拼接，以获取视觉位置特征。之后，我们将加入位置信息的视觉特征作为 query，语言特征作为 key 和 value，进行 cross attention 计算来得到最终的多模态特征 Mi。</p><p></p><p></p><h3>【2.4 几何关系感知】</h3><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/3e/3e762074997d27d43738652f7695f738.webp?x-oss-process=image%2Fresize%2Cp_80%2Fformat%2Cpng" /></p><p></p><p>为了加强 RoI 特征感知彼此的位置关系，我们设计了几何关系感知模块（GRAM）让模型更好的学习布局元素之间的内容信息。具体细节如下： 首先，给定 𝑁 个 RoIs，两个 RoI 𝑙𝑖 和 𝑙𝑗 (𝑖, 𝑗 ∈ {1, 2, . . . , 𝑁})的相对位置特征 𝑅𝑖𝑗 计算方式如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fdb658433e2d6805f7f9da46caaf5e7a.webp?x-oss-process=image%2Fresize%2Cp_80%2Fformat%2Cpng" /></p><p></p><p>然后，4 维 embedding 向量通过 sin-cos 编码方法被 embedding 为几何权重系数 Rpij。最后通过 softmax 函数对几何权重系数进行归一化以突出起到主要作用的部分，</p><p></p><p><img src="https://static001.geekbang.org/infoq/ce/ce161b7d97ac1d56fa43bd3c4b67f0c1.webp?x-oss-process=image%2Fresize%2Cp_80%2Fformat%2Cpng" /></p><p></p><p>需要强调的是不同类型元素应该有不同的定位策略， 例如衬底应覆盖在文本类型元素上但是其他种类的元素之间应避免重叠， 因此我们提取 RoI 特征作为元素的类别信息。 为了合并位置和类别信息，提取视觉特征𝑉被展开并且被投影函数 P 转换为𝑑𝑡维度的向量。 最后，视觉 embedding 乘以几何权重进而得到最终的几何特征𝑇：</p><p></p><p><img src="https://static001.geekbang.org/infoq/77/77ed1f7416163ff27f0eb5d018b5082b.webp?x-oss-process=image%2Fresize%2Cp_80%2Fformat%2Cpng" /></p><p>其中，V′是 V 的展开形式。</p><p></p><p></p><h2>三、融合类目共性和个性化风格的商品背景生成</h2><p></p><p></p><h3>【3.1 技术背景】</h3><p></p><p></p><p></p><p>商品广告背景生成旨在为商品透底图生成自然、逼真的背景，以构造高质量的广告图片，从而提升图片点击率。现有的背景生成方法主要分为两种方式，即“文生图”模式和“图生图”模式。“文生图”模式指的是向扩散大模型（如 Stable Diffusion，ControlNet）输入一段描述图片的提示词和一张商品透底图，由大模型根据提示词的内容填充商品周围的背景区域。“图生图”模式指的是在“文生图”模式的基础上，额外引入一张参考图像，并将该参考图像添加一定强度的噪声，作为扩散大模型的初始噪声，使得生成的背景区域与参考图像具备一定的相似性。</p><p></p><p>现有的背景生成方法采用“文生图”模式和“图生图”模式。其中“文生图”模式的缺点在于两方面：第一，需要花费大量时间设计和修正提示词；第二，提示词在描述图片的空间位置布局或抽象风格时效果较差，给精细化定制背景带来了较大挑战。“图生图”模式虽然额外引入了参考图像作为参考，但是它依然存在一定的局限性：参考图像上叠加的噪声模糊了图像中原有的的布局、组成元素等信息使得生成的图片只能保证在整体场景上与参考图像相似，无法进行更细粒度、更精确的控制。</p><p></p><p>为了解决上述问题，我们提出了一种基于参考图像的商品广告背景生成方法，该方法可以在给定原始商品透底图、原始商品所属类别和任一其他商品的广告图（参考图片）时，为原始商品生成与参考图片布局、组成元素、色彩、风格等相似的背景图。本发明的方法框架如下图所示，包含三个模块，预训练的扩散大模型 Stable Diffusion（SD），基于类目共性的生成器 CG，基于个性化信息的生成器 PG。其中，类目共性生成器的作用是提取商品透底图中包含的信息，如商品位置、商品类别等；PG 的作用是提取参考图片中的布局、组成元素、色彩、风格等个性化信息。CG 和 PG 提取的特征将合并进入 SD 的解码器中，用于生成最终的背景。因此，我们设计了一个可以模仿参考图像生成背景的模型，从而无需设计复杂的提示词来描述布局、风格等细粒度信息。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bc/bcfadd00783f3a90b024ad91f7b8bf1f.webp?x-oss-process=image%2Fresize%2Cp_80%2Fformat%2Cpng" /></p><p></p><p></p><h3>【3.2 基于类目共性的生成】</h3><p></p><p></p><p>该步骤的目的是利用 CG 提取商品透底图中的信息，用于生成适配该商品所属类别的通用背景。CG 的输入包含三部分，即商品透底图，商品提示词和背景提示词。其中，商品提示词为”A photo of C”，背景提示词为”in the background of D”，其中 D 表示特定字符串”sks”与 C 对应的类目编码的拼接。CG 的具体结构与 ControlNet 基本相同，它们的区别在于我们将 CG 中的注意力模块替换为基于商品掩膜的注意力模块。给定透底图中的商品掩膜 M（可由透底图直接得到），基于商品掩膜的注意力模块可以表示为：</p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b147a0087276b70fa388b7d4f8ec89cc.webp?x-oss-process=image%2Fresize%2Cp_80%2Fformat%2Cpng" /></p><p></p><p>其中，Xin 与 Xout 分别表示注意力模块的输入、输出模块，CA()表示常规的注意力模块，•表示点乘符号。经过训练后，每个类目的背景风格被映射且仅被映射到到对应的 D 中。因此，在推理时，给定类目名称 C，模型可以通过固定的映射关系得到其对应的 D，并将 D 作为提示词用来生成符合该类别背景通性的背景，从而减少复杂的提示词设计。</p><p></p><p></p><h3>【3.3 基于个性化风格的生成】</h3><p></p><p></p><p>该步骤的目的是，在类别通用背景的基础上，利用 PG 叠加参考图片的个性化信息。PG 的输入包含两部分，即参考图片和参考图片中原有的商品的掩膜。PG 的具体结构与 ControlNet 相同，其输入为参考图片的背景区域。注意，PG 不需要提示词输入，即提示词为””。特别地，由于参考图像的个性化信息应当仅作用于生成的背景区域，因此我们利用商品透底图 M 对 PG 的输出进行了过滤。具体地，与 ControlNet 相同，PG 输出五个不同分辨率的特征图，则对于第 i 个特征图来说，我们令其与 M 相乘，其中 M 表示商品透底图的掩膜。</p><p></p><p></p><h2>四、基于规划和渲染的商品海报生成</h2><p></p><p></p><p></p><h3>【4.1 技术背景】</h3><p></p><p></p><p>商品海报对于商品宣传起着关键作用。一张精美的海报不仅应该包含合理的元素布局，例如衬底、文案、商品等元素，还应该具有和商品和谐的背景。因此，这项挑战性的任务通常由人类设计师完成。然而，依赖人类设计师会使成本提升和效率低下，需要端到端商品海报生成技术来将给定的商品和文本，生成一张可传递商品信息的海报图像。</p><p></p><p>目前尚无端到端商品海报生成技术，与其较为相关的两个领域为图像填充以及商品海报布局生成。如下图（a）所示，图像填充技术可以根据已有的商品图像，自动生成商品的背景区域。如下图（b）所示，商品海报布局生成技术可以在人类设计师产出的海报上，寻找可放置视觉元素的位置。因此，简单的将两个任务串联起来可被视作一种实现端到端商品海报生成的基础方案。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c2/c21523248f6d77bac970f93fea142c64.webp?x-oss-process=image%2Fresize%2Cp_80%2Fformat%2Cpng" /></p><p></p><p>如上图（c）所示，虽然将图像填充和商品海报布局生成联合可看作一种实现商品海报生成的方案，这种技术方案的缺陷有两点：第一，由图像填充技术生成的背景虽然具备真实感，然而由于该背景的内容复杂度过高，导致布局模型找不到合适的位置摆放视觉元素；第二，由于图像填充技术需要提前确定商品的位置，这使得布局模型只能控制文案和衬底的位置，降低了布局结果的多样性。由于上述缺陷，现有技术难以生成美观且多样的商品海报。为了解决现有技术的弊端，我们拆解借鉴了人类设计师设计海报的流程。</p><p></p><p>如上图（d）所示，该流程通常包含两个步骤：规划和渲染。在规划阶段，设计师通常用纸和笔大致规划所有视觉元素的位置，因此其他视觉元素的位置不会被预定的商品位置所约束。在渲染阶段，设计师使用电脑将整体布局渲染成一幅精美的海报图像。由于渲染背景时会同时考虑文案等元素的位置，这使得渲染的背景益于文字信息的传递。</p><p></p><p></p><h3>【4.2 基于规划网络的布局生成】</h3><p></p><p></p><p>受上述分析启发，我们提出了一种基于规划和渲染的端到端商品海报生成方法，借鉴了人类设计师的工作流程来完成海报生成任务。所提出的方法框架如上图所示，其中包含一个规划网络 PlanNet 和一个渲染网络 RenderNet。对于 PlanNet，它首先编码商品图像和文本内容，之后使用布局解码器（Layout Decoder）将二者融合来产生更合理的布局结果，最终它预测了商品和其他视觉元素的位置。对于 RenderNet，它将 PlanNet 生成的布局还有商品图像共同作为生成过程的控制条件。首先它利用了一个空间融合模块来学习不同视觉元素的空间位置关系；之后对商品外观进行编码，使得生成的背景和商品是和谐的；最后它将两个控制条件输入给 ControlNet，用于指导 Stable Diffusion 的生成过程。结合上述技术优势，我们实现了一个图片质量较高且多样化的商品海报生成方法。</p><p></p><p>其中，规划网络的目的是将输入的随机布局，经过多步的迭代去噪，采用布局解码器转化成最终视觉元素的布局位置。如下图所示，对于第 t 步来说，布局解码器的输入包含三部分：t 时刻的布局结果 zt，提取好的视觉和语言特征；输出为 t-1 时刻的布局结果 z(t-1)。它的详细结构包含两个全联接层（FC）和 N 个 transformer 模块。首先，zt 被一个 FC 层映射为一个元素表征 et；之后经过 N 个 transformer 模块，处理后的元素表征被另一个 FC 层解码为 zt-1。在每一个 transformer 模块，时间步 t 和元素表征 et 被一个自适应归一化层（AdaLN）和一个自注意力层（SA）处理。之后，交叉注意力层（CA）被用于计算自注意力层的输出，以及视觉和语言特征。</p><p></p><p><img src="https://static001.geekbang.org/infoq/21/211c51009880557e3a430570a9a1c6ce.webp?x-oss-process=image%2Fresize%2Cp_80%2Fformat%2Cpng" /></p><p></p><p></p><h3>【4.3 基于渲染网络的背景生成】</h3><p></p><p></p><p>在获得规划网络输出的布局结果后，渲染网络将其与商品的图像共同作为输入，输出一张最终的海报图像。具体来说，它包含一个布局分支、一个视觉分支、Stable Diffusion（SD）、ControlNet 和一个文字渲染模块。其中，布局分支的目的是将各个视觉元素的布局进行编码。为了更好的表示布局的空间信息，我们将规划网络输出的布局坐标转换为布局的掩码图像{Lm}，m 的范围是从 1 到 M，M 为视觉元素的类别数。对于 Lm 来说，第 m 类布局元素的位置被填充成 1，其余位置填充为 0。为了更好的探索 M 个布局的空间关系，我们提出了一个空间融合模块。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e9/e90631bbeaec5dce3c1cb699bd270dac.webp?x-oss-process=image%2Fresize%2Cp_80%2Fformat%2Cpng" /></p><p></p><p>如上图所示，该模块首先使用三层卷积网络将{Lm}编码，编码后的特征形状为 C×H×W。之后将编码后的{Lm}融合为一个统一的布局表达 L’。具体来说，编码后的{Lm}被切分成多个切块{lm,j}，其形状为 C×P×P，j 是块的序号，它的范围是 1 到 W×H/P/P。为了得到 L’的第 j 个切块，我们对编码后的{Lm}的第 j 个切块进行融合。融合后的特征被输入到 S 层视觉 transformer 中。最终，一个三层卷积网络被用于得到最终的布局表征 ZL。</p><p></p><p>视觉分支的目的是编码商品的视觉和空间信息。本发明首先根据规划网络的输出，对商品图像进行缩放和平移，从而得到重定位的商品图像 V。之后使用一个六层的卷积网络来提取 V 的视觉表征 ZV。最终，视觉和布局表征被相加，来送入到 ControlNet 中，用于指导 SD 的生成过程。</p><p></p><p></p><h2>五、总结 &amp;展望</h2><p></p><p></p><p></p><h3>【5.1 技术路线总结】</h3><p></p><p></p><p>为了解决广告图片 AIGC 中缺乏卖点信息、难以规模化和个性化以及不利于卖点展示等问题，京东广告部门提出了以下技术方案：</p><p></p><p>首先，我们构建了一个关系感知扩散模型用于布局海报生成，其中一个图文关系感知模块用于对齐视觉和文本之间的模态，一个几何关系感知模块用于综合考虑元素之间上下文信息进而学习元素之间的几何关系；</p><p></p><p>其次，我们将类别共性和个性化风格整合到扩散模型中。提出了类别生成器实现大规模背景生成，并使用个性化生成器从参考图像学习个性化风格；</p><p></p><p>最后，我们提出了一种名为 P&amp;R 的图文创意生成框架，包括两个阶段：规划和渲染。在规划阶段，我们提出了一个 PlanNet 网络来考虑产品的外观特征和文本的语义特征，生成产品和其他视觉组件的布局。在渲染阶段，我们提出了一个 RenderNet 网络来生成产品的背景，并考虑到所生成的布局，在此过程中引入了一个空间融合模块来融合不同视觉组件的布局。</p><p></p><p></p><h3>【5.2 未来技术展望】</h3><p></p><p></p><p>尽管 AIGC 技术在图像生成领域有较为广泛的应用，但仍存在诸多待解决的问题，未来我们将在以下方向开展技术探索：</p><p></p><p>可控性：由于对商品内容和外形的理解欠缺，业界生成的素材在可控性上存在劣势，使其应用于广告领域存在用户投诉风险。</p><p>多模态：优化技术在处理和整合不同模态的内容上的能力，如如何将文字、图像、视频等元素有效融合，以创造一致性和内在逻辑性强的创意产品。</p><p>个性化：针对不同的目标用户群体，利用用户数据和行为分析，生成符合特定用户口味和偏好的个性化广告创意。</p><p></p><p>最后，我们欢迎对 AIGC、大模型感兴趣的小伙伴加入京东广告研发部，共同成长，一齐助力京东广告业务的发展！联系邮箱：fengwei25@jd.com。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/6Uql3UH82JlgsNEo8vKD</id>
            <title>梗图理解“天花板”！港中文终身教授贾佳亚团队推出多模态模型：GPT-4+DALL-E 3，王炸组合刷爆榜单</title>
            <link>https://www.infoq.cn/article/6Uql3UH82JlgsNEo8vKD</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/6Uql3UH82JlgsNEo8vKD</guid>
            <pubDate></pubDate>
            <updated>Tue, 16 Apr 2024 01:52:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 港中文终身教授, Mini-Gemini, 多模态模型, GPT4+DALL-E3
<br>
<br>
总结: 近日，港中文终身教授贾佳亚团队推出了一款名为Mini-Gemini的多模态模型，凭借超强的图文理解力，Mini-Gemini的最强模型版本在多个指标上直接媲美Gemini Pro，GPT-4V，网友称其效果堪称是开源社区的GPT4+DALL-E3的王炸组合！ </div>
                        <hr>
                    
                    <p>近日，港中文终身教授贾佳亚团队推出了一款名为&nbsp;Mini-Gemini&nbsp;的多模态模型，包括&nbsp;2B&nbsp;小杯到&nbsp;34B&nbsp;的超大杯，一经发布便登上了&nbsp;PaperWithCode&nbsp;热榜。凭借超强的图文理解力，Mini-Gemini的最强模型版本在多个指标上，直接媲美Gemini&nbsp;Pro，GPT-4V，网友称其效果堪称是开源社区的&nbsp;GPT4+DALL-E&nbsp;3&nbsp;的王炸组合！</p><p></p><p>目前，研究团队将&nbsp;Mini-Gemini&nbsp;的代码、模型、数据全部开源。更有意思的是，超会玩梗的&nbsp;Mini-Gemini&nbsp;线上&nbsp;Demo&nbsp;已经发布，人人皆可上手试玩。“浅尝”之后，有人认为：Mini-Gemini跟商业模型差不了多少！</p><p></p><p><img src="https://static001.geekbang.org/infoq/06/0664a565a4a2bb6d7cc0baec3da427a8.png" /></p><p></p><p></p><h2>最“懂”图的大模型？</h2><p></p><p></p><h4>图像理解及推理</h4><p></p><p></p><p>当下，绝大多数多模态模型仅支持低分辨率图像输入和文字输出。而在实际场景中，许多任务都需要对高清图像进行解析，并用图像的形式进行展现。Mini-Gemini&nbsp;在这一点上有着不错的表现：</p><p></p><p>给它一张做面包的九宫格漫画教程，Mini-Gemini&nbsp;能看懂并进行手把手教学。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4a/4a80e67930b2943ece770907c00f8a25.png" /></p><p></p><p>拍一张苹果店&nbsp;Mac&nbsp;电脑信息图，Mini-Gemini&nbsp;能够将两种&nbsp;Mac&nbsp;的参数列表横向对比。</p><p></p><p><img src="https://static001.geekbang.org/infoq/84/8497b7854899c3d1ac6daf4a892c7260.png" /></p><p></p><p>Mini-Gemini&nbsp;还能理解输入曲线图的数学意义，并使用代码复现这张图。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a250c95ae2372dc1c8625751c1e02a63.png" /></p><p></p><p></p><p>高清复杂的多图表理解和归纳也是小菜一碟，Mini-Gemini&nbsp;直接秒变打工人效率提升的超级外挂。</p><p></p><p><img src="https://static001.geekbang.org/infoq/69/6945e6b90f0460f36172925ad48d9374.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/99/99bc5a717be92e457edf81aa474e9358.png" /></p><p></p><p>有网友对此表示：妈妈再也不用担心我的生活了。</p><p></p><h4>图片生成</h4><p></p><p></p><p>更重要的是，除了图像理解和推理能力，Mini-Gemini&nbsp;还解锁了图像的生成能力。通过一些抽象的多模态指令，Mini-Gemini&nbsp;就可以给出推理，并生成合适的图片，这个操作像不像是&nbsp;ChatGPT&nbsp;和&nbsp;DALL-E&nbsp;3&nbsp;的联动？！让我们看一些例子：</p><p></p><p>输入一张画着冰川中的仙人掌的图片，让&nbsp;Mini-Gemini&nbsp;解释图中的矛盾点并举一反三。它会说：“在沙漠环境中自然不会出现冰。这种矛盾的类似例子可能是一只北极熊出现在热带雨林中，因为北极熊适应于寒冷、覆盖着冰的环境，在炎热潮湿的气候中无法生存。”并生成一张热带雨林中北极熊的图片：</p><p></p><p><img src="https://static001.geekbang.org/infoq/bf/bff5464d362da91befd551de9c1320b4.png" /></p><p></p><p>Mini-Gemini&nbsp;还可以在多轮对话中通过简单指令生成连环小故事。比方说，让它根据用户输入讲一个贵族小老鼠的故事。Mini-Gemini&nbsp;会根据前文的文字生成结果和用户输入进行推理，在保持一致性的情况下对图片进行修改，使其更符合用户的要求。</p><p></p><p><img src="https://static001.geekbang.org/infoq/32/32047cd52eee473394e16528911e9acd.png" /></p><p></p><p></p><h4>梗图理解</h4><p></p><p></p><p>目前市面上的大模型们在对于&nbsp;meme&nbsp;图的理解方面总是不尽人意，不过&nbsp;Mini-Gemini&nbsp;不一样，通过其强大的&nbsp;OCR&nbsp;和推理能力，它能做到准确指出笑点。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9d/9d89a5dfbeb165179c84b2f131bacf1d.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/52/52d0a478660dec68d4a8a21712683b28.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/1c/1c30e362c395ccff0b7df833e194802e.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/82/827f08ae88404e453400c78b571d89fc.png" /></p><p></p><p>输入一张周一上班心神俱疲“社畜”狗的梗图，Mini-Gemini&nbsp;还能用它的生图功能还你一只周末下班的快乐小鸡毛！</p><p></p><p><img src="https://static001.geekbang.org/infoq/8c/8c84176eef311550b721aad5c7d88239.png" /></p><p></p><p></p><h2>技术细节</h2><p></p><p></p><p>大道至简，Mini-Gemini&nbsp;的整体思路并不复杂。其中的&nbsp;Gemini（双子座）表达的是使用视觉双分支的信息挖掘（Mining-Info&nbsp;in&nbsp;Gemini）解决高清图像理解问题。</p><p></p><p>核心在于三点：</p><p>（1）用于高清图像的双编码器机制</p><p>（2）更高质量的数据</p><p>（3）训练阶段结合生成模型数据拓展</p><p></p><p>详细来说，Mini-Gemini&nbsp;将传统所使用的&nbsp;ViT&nbsp;当做低分辨率的&nbsp;Query，而使用卷积网络（ConvNet）将高分辨率的图像编码成&nbsp;Key&nbsp;和&nbsp;Value。使用&nbsp;Transformer&nbsp;中常用的&nbsp;Attention&nbsp;机制，来挖掘每个低分辨率&nbsp;Query&nbsp;所对应的高分辨率区域。从而在保持最终视觉&nbsp;Token&nbsp;数目不变的情况下去提升对高清图像的响应，保证了在大语言（LLM）模型中对于高清图像的高效编码。值得一提的是，由于高分辨率分支卷积网络的使用，可以根据需要对图像所需的分辨率自适应调整，能够遇强则强。对于图像的生成部分，Mini-Gemini&nbsp;借助了&nbsp;SDXL，使用&nbsp;LLM&nbsp;推理后所生成的文本链接两个模型，类似于&nbsp;DALLE3&nbsp;的流程。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ac/acb8158337997637d90e4b8b30ec1545.png" /></p><p></p><p>Mini-Gemini&nbsp;进一步收集并优化了训练数据的质量，并加入了跟生成模型结合的文本数据进行训练。在仅使用&nbsp;2-3M&nbsp;数据的情况下，实现了对图像理解、推理、和生成的统一流程。可以说，Mini-Gemini&nbsp;在各种&nbsp;Zero-shot&nbsp;的榜单上毫不逊色于各种大厂用大量数据训练出来的模型：</p><p></p><p><img src="https://static001.geekbang.org/infoq/31/31a0c3f1df8752464928fee20e01ec6d.png" /></p><p></p><p>量化数据指标对比</p><p></p><p>最后提一嘴，Mini-Gemini&nbsp;的&nbsp;Demo&nbsp;操作极其简单，直接输入图像或文字进行对话即可，读者朋友们可以试一试（网址附在文末咯）！</p><p></p><p><img src="https://static001.geekbang.org/infoq/15/15af6b55271c553fba3104d8edf90b89.png" /></p><p></p><p>参考链接：</p><p></p><p>Github地址：<a href="https://github.com/dvlab-research/MiniGemini">https://github.com/dvlab-research/MiniGemini</a>"</p><p></p><p>Demo地址:&nbsp;<a href="http://103.170.5.190:7860/">http://103.170.5.190:7860/</a>"</p><p></p><p>论文地址：<a href="https://arxiv.org/pdf/2403.18814.pdf">https://arxiv.org/pdf/2403.18814.pdf</a>"</p><p></p><p>模型地址：<a href="https://huggingface.co/collections/YanweiLi/mini-gemini-6603c50b9b43d044171d0854">https://huggingface.co/collections/YanweiLi/mini-gemini-6603c50b9b43d044171d0854</a>"</p><p></p><p>数据地址：<a href="https://huggingface.co/collections/YanweiLi/mini-gemini-data-660463ea895a01d8f367624e">https://huggingface.co/collections/YanweiLi/mini-gemini-data-660463ea895a01d8f367624e</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/vQqMp8kUQHiRDpbAqBxG</id>
            <title>苹果股价罕见飙升，因网传 M4 芯片将于年末发售</title>
            <link>https://www.infoq.cn/article/vQqMp8kUQHiRDpbAqBxG</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/vQqMp8kUQHiRDpbAqBxG</guid>
            <pubDate></pubDate>
            <updated>Mon, 15 Apr 2024 08:02:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 苹果, M4芯片, AI功能, Mac产品线
<br>
<br>
总结: 苹果计划推出搭载AI功能的新型芯片M4，将逐步升级Mac产品线，引起科技界轰动，预计将带来个人电脑市场的新变革。M4芯片将采用3nm工艺打造，主打升级版的神经网络引擎，将实现对Mac电脑的智能化升级，包括提升传统计算任务性能和创新功能。首批搭载M4的Mac电脑预计将于今年10月到11月前后登场，价格可能不会大幅提价。苹果希望通过M4芯片赶上AI竞赛的步伐，展示M系列芯片的AI实力。 </div>
                        <hr>
                    
                    <p>据知情人士透露，为了提振低迷的电脑销售，苹果正在酝酿一场针对其&nbsp;Mac&nbsp;产品线的全面升级。这场升级的核心，是一款有&nbsp;AI&nbsp;功能加持的新型芯片&nbsp;M4，预计将在&nbsp;2024&nbsp;年底至&nbsp;2025&nbsp;年初逐步推向市场。</p><p></p><p>消息一出，立即在科技界引起了轰动。业界普遍认为，这将是苹果打开&nbsp;AI&nbsp;PC&nbsp;大门的一次重要尝试，同时也将为整个个人电脑市场带来新的变革。截止当地时间&nbsp;4&nbsp;月&nbsp;11&nbsp;日收盘时，苹果股价出现近来罕见的飙升，大幅上涨&nbsp;4.33%，报&nbsp;175.04&nbsp;美元，市值达到惊人的&nbsp;2.7&nbsp;万亿美元，单日增加&nbsp;1121&nbsp;亿美元（约&nbsp;8113&nbsp;亿元人民币）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/38/382dae21b6ffdcc248f7c4bd4ebda010.png" /></p><p></p><p></p><h4>M4&nbsp;何时发行？&nbsp;Mac&nbsp;产品线如何定价？</h4><p></p><p></p><p>M4&nbsp;芯片是苹果继&nbsp;M1、M2&nbsp;和&nbsp;M3&nbsp;芯片之后的新一代产品，其最大的亮点在于对&nbsp;AI&nbsp;功能的深度集成和优化。据悉，M4&nbsp;芯片将采用与&nbsp;M3&nbsp;芯片相同的&nbsp;3nm&nbsp;工艺打造，但台积电有望使用升级版&nbsp;3nm&nbsp;工艺，性能、能效将进一步提升。另外，M4&nbsp;芯片将主打升级版的神经网络引擎，且运算核心数量较上一代大幅增加，将能执行更复杂的AI运算。</p><p></p><p>据悉，M4&nbsp;芯片系列将主要分为三个梯队：入门级的&nbsp;Donan、更高阶的&nbsp;Brava&nbsp;和旗舰芯片&nbsp;Hidra。报道表示，入门级&nbsp;MacBook&nbsp;Pro、新款&nbsp;MacBook&nbsp;Air&nbsp;和低端&nbsp;Mac&nbsp;mini&nbsp;将采用&nbsp;Donan&nbsp;芯片，Brava&nbsp;芯片则会用于高端&nbsp;MacBook&nbsp;Pro&nbsp;和“更贵的”&nbsp;Mac&nbsp;mini&nbsp;型号。而苹果最顶级的台式机新款&nbsp;Mac&nbsp;Pro，则有望搭载性能强悍的&nbsp;Hidra&nbsp;芯片。</p><p></p><p>苹果对&nbsp;M4&nbsp;芯片的&nbsp;AI&nbsp;功能寄予厚望，预计将通过这款芯片实现对&nbsp;Mac&nbsp;电脑的智能化升级。这不仅包括提升视频编辑、图像处理等传统计算任务的性能，更将通过&nbsp;AI&nbsp;技术，实现语音识别、自动排版、智能推荐等创新功能，从而提升用户体验。</p><p></p><p>按照苹果的计划，首批搭载&nbsp;M4&nbsp;的&nbsp;Mac&nbsp;电脑将大概于今年&nbsp;10&nbsp;月到&nbsp;11&nbsp;月前后登场。这其中包括新款&nbsp;iMac、入门级&nbsp;14&nbsp;英寸&nbsp;MacBook&nbsp;Pro、高端&nbsp;14&nbsp;英寸和&nbsp;16&nbsp;英寸&nbsp;MacBook&nbsp;Pro，以及搭载&nbsp;M4&nbsp;芯片的&nbsp;Mac&nbsp;mini。2025&nbsp;年，苹果将在年初春季对&nbsp;13&nbsp;英寸和&nbsp;15&nbsp;英寸&nbsp;MacBook&nbsp;Air&nbsp;进行更新，年中发布新一代&nbsp;Mac&nbsp;Studio，下半年发布&nbsp;Mac&nbsp;Pro，均搭载&nbsp;M4&nbsp;芯片。</p><p></p><p>价格方面，外媒猜测苹果可能不会大幅度提价，甚至根本不加价，维持现有的价格水平：2023&nbsp;年的入门级&nbsp;M3&nbsp;iMac&nbsp;为&nbsp;1299&nbsp;美元，14&nbsp;英寸&nbsp;MacBook&nbsp;Pro&nbsp;为&nbsp;1599&nbsp;美元。M3&nbsp;版&nbsp;13&nbsp;英寸和&nbsp;15&nbsp;英寸&nbsp;MacBook&nbsp;Air&nbsp;的售价分别是&nbsp;1099&nbsp;美元和&nbsp;1299&nbsp;美元。配备&nbsp;M3&nbsp;Pro&nbsp;的&nbsp;14&nbsp;英寸&nbsp;MacBook&nbsp;Pro&nbsp;起步价为&nbsp;1999&nbsp;美元，16&nbsp;英寸版本则从&nbsp;2499&nbsp;美元起售。</p><p></p><p>此外，消息人士还称，苹果计划在其年度开发者大会（WWDC）上展示“一系列新功能”。果粉有望在发布会上一睹&nbsp;MacOS&nbsp;新版本的风采，看看这些&nbsp;AI&nbsp;新功能将如何与操作系统融合。</p><p></p><p>从第一代开始，苹果的&nbsp;M&nbsp;系列芯片就搭载了专用的神经网络引擎。如今，AI&nbsp;PC&nbsp;已经成为业界的新宠，苹果自然也希望借机大肆宣传&nbsp;M&nbsp;系列芯片的&nbsp;AI&nbsp;实力。不过要注意的是，苹果&nbsp;M4&nbsp;芯片目前还停留在传闻阶段，以上信息恐怕都得打上一个大大的问号。</p><p></p><p>在&nbsp;AI&nbsp;竞赛中，苹果一直被认为落后于微软、谷歌等科技界同行，M4&nbsp;芯片能否让苹果迎头赶上？我们拭目以待。</p><p></p><p>参考来源：</p><p></p><p>https://www.bloomberg.com/news/articles/2024-04-11/apple-aapl-readies-m4-chip-mac-line-including-new-macbook-air-and-mac-pro?srnd=technology-v</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/XRP2Pj3lZO6TESg6CWPy</id>
            <title>六大国有银行AI大模型进展如何，又探索了哪些应用？</title>
            <link>https://www.infoq.cn/article/XRP2Pj3lZO6TESg6CWPy</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/XRP2Pj3lZO6TESg6CWPy</guid>
            <pubDate></pubDate>
            <updated>Mon, 15 Apr 2024 07:20:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 利率下行, 银行业转型, AI大模型, 内部运营管理
<br>
<br>
总结: 在当前利率下行、息差面临持续收窄压力、营收增速放缓的背景下，银行业需要寻找新的增长点和提高运营效率。通过构建垂直领域AI大模型，银行可以充分发挥实时数据资源，推动金融科技创新发展。银行业正逐步将AI大模型应用于内部运营管理，以提升研发效率和业务办理效率。 </div>
                        <hr>
                    
                    <p>在当前利率下行、息差面临持续收窄压力、营收增速放缓的背景下，对于银行业而言，寻找新的增长点和提高运营效率成为行业迫切需求。</p><p></p><p>由清华大学经济管理学院、度小满、《麻省理工科技评论》中国联合编写的《2024年金融业生成式人工智能应用报告》显示，我国金融业虽然拥有全球最大规模的实时数据，但这些金融数据本身并不能同步带来商业价值。通过构建垂直领域 AI 大模型，不仅可以充分发挥这些数据资源，还能驱动金融科技创新发展。</p><p></p><p>在这一转型需求下，银行数字化转型的逻辑逐渐明确为&nbsp;“数据 + 算法”，其中 AI 大模型成为实现数据价值最大化和推动业务创新的关键。过去一年，我国六大国有银行以及多家头部商业银行都已踏入了这一领域，以期通过新的数字化手段推动价值链升级，并在金融市场中保持竞争力。</p><p></p><h2>银行业 AI 大模型发展如何</h2><p></p><p></p><p>日前，我国六大行均在其 2023 年年度报告中披露了大模型相关进展。其中，中国工商银行率先建成全栈自主可控的千亿级 AI 大模型技术体系，其 AI 大模型建设成果获评人民银行《金融电子化》“2023 年金融信息化 10 件大事”榜首。</p><p></p><p>中国农业银行于 2023 年年初发布了金融 AI 大模型应用 ChatABC，并在该行内部以多轮问答助手、工单自动化回复助手等形式面向内部员工开放试用。</p><p></p><p>中国银行在探索大模型技术在内部知识服务、辅助编码等场景的应用，运用人工智能、大数据等信息技术提高信用风险评估能力。</p><p></p><p>中国建设银行启动大模型“方舟计划”，深耕计算机视觉、智能语音、自然语言处理、知识图谱、智能决策等五大领域专业能力，自主研发的人工智能平台累计服务调用 433 亿次，获得《亚洲银行家》2023 年度“最佳人工智能应用”。</p><p></p><p>中国交通银行深化 AI 在客户服务、产品推介、风险防控等场景的应用，探索大模型在办公助手、客服问答等场景的应用，并将“构建内嵌风控要素的生成式 AI 框架”列入 2024 年工作重点。</p><p></p><p>中国邮政储蓄银行打造融合大模型技术的“邮储大脑”，从文本生成、代码生成、文本提炼和多模态理解生成等方向探索大模型技术应用，2023 年度在大模型领域提交超过 5 件专利申请。</p><p></p><p>除了六大国有银行，招商银行、平安银行、兴业银行等全国性股份行，以及北京银行、上海银行、江苏银行等多家城商行也都在年报中提及了 AI 大模型研发应用成果。AI 大模型正逐步成为银行发展的第二赛点。</p><p></p><h2>银行用 AI 大模型做什么</h2><p></p><p></p><p>正如中国银行业协会首席信息官高峰所说：“没有应用场景，新技术就是‘无根之木’。”AI 大模型向金融垂直领域发展的最终目的仍然是服务经营管理场景。当前银行业大模型应用可分为面向内部运营管理和重塑外部业务场景两大用途。</p><p></p><h4>(一) 面向内部运营管理</h4><p></p><p>内部员工减负方面，邮储银行聚焦研发测试孵化“研发助手”，辅助需求分析、UI 设计、代码生成、系统测试等研发全流程，促进端到端研发效率提升；聚焦线下网点运营，推出柜面“小邮助手”为柜员提供在线业务知识问答，提升业务办理效率。</p><p></p><p>建行持续打造金融影像文字识别产品，支持识别 140 余种票据，覆盖 75% 票据识别量，助力票据审核信息录入效率提升 120 倍，获得全球人工智能文档图像分析识别领域比赛 (ICDAR 2023) 印章文字检测赛道冠军。</p><p></p><p>此外建行“方舟”助手、“方舟”工具箱等金融大模型基础应用，还能实现快速生成研报摘要和点评、录入语音自动生成拜访记录、文生图、自动生成上市公司类客户调查报告等 25 项场景功能，全面提升员工工作专业水平和效率。</p><p></p><h4>(二) 重塑外部业务场景</h4><p></p><p></p><p>智能营销</p><p></p><p>邮储银行关注获客能力，推出情感模型会话洞察与“灵动智库”服务增强企业微信运营功能，提升基层精细化客户洞察能力。</p><p></p><p>交行利用 AI 技术深挖个金客户兴趣偏好，用大模型强化业务端留客能力，各类理财模型策略累计触客成交量近 4 千亿元，较整体成交率提升 16 倍。</p><p></p><p>建行实现个性化语音 AI 合成，支持 10 万字超长文本语音合成，支持《建设银行报》、企业微信等语音播报，同时实现营销创意内容和文案自动化生产，帮助打造品牌形象、提升营销内容质量、提升黏客能力。</p><p></p><p>智能客服</p><p></p><p>智能客服的出现本是为了弥补传统人工服务的不足，但其降本增效的效果与预期相去甚远，特别是 RNN（循环神经网络）技术下的智能客服模型在理解客户问题、定位关键知识点、匹配知识库问题等方面存在较大缺陷。在此背景下，将大模型技术应用于智能客服，就好比给客服数字人装上“大脑”，成为越来越多银行提升客服智能化水平的不二选择。</p><p></p><p>在线上智能服务方面，“邮储大脑”融合大模型技术，构建新型生成式 AI 能力，加速数字金融服务模式重塑。此外邮储银行 App 融合 AI 空间、数字员工、视频客服，打造沉浸式陪伴服务。</p><p></p><p>工行加快运营领域数字人、大模型等新技术应用，正式上线了首个基于大模型的网点员工智能助手，提升网点效能，全年运营领域智能处理业务量 3.2 亿笔，比上年增长 14%；建设 13 个综合型数字员工以及 1000 余个流程自动化数字员工，智能增效超过 3 万人年。</p><p></p><p>在电话客服智能化方面，建行自主研发端到端的语音识别和语音合成能力，实现说话人身份声纹识别、四川话等方言语音识别、音频质量检测能力，支持智能外呼等场景应用。</p><p></p><p>工行以金融行业通用 AI 模型支撑智能客服接听客户来电，显著提升对客户来电诉求和情绪的识别准确率，并大幅缩减维护成本；在同业率先实现大模型技术在座席助手等场景落地，全年服务量 21.5 亿笔，接听率和智能分流率同业领先。</p><p></p><p>农行推出的金融 AI 大模型产品 ChatABC，也能够利用大模型技术提升智能客服的金融知识理解能力、内容生成能力及安全问答能力。</p><p></p><p>此外建行还注意到传统客服手动填写校对工单时耗时耗力、效率低，以及工单填写不规范导致需要反复沟通、影响客户体验的问题，因此在其金融大模型基础应用中上线了智能客服工单生成功能，每单平均节约客服工作时间 15-20 秒，可用率达 82%，一致性达 80%，该项目获得中国银行业协会 2023 年客服与远程银行创新应用大赛其他类赛道冠军。</p><p></p><p>客户投诉治理与消费者权益保护</p><p></p><p>无论是实体行业还是金融行业，其售前售后服务最终都是服务于客户体验，而客户投诉正是客户体验的重要反馈路径。治理好客户投诉，不仅仅是在保护消费者权益，也是在管理银行自身经营风险。</p><p></p><p>工行推广客户投诉智慧治理模式，在投诉处置和管理主要环节全面应用机器人流程自动化、自然语言处理、生成式人工智能（AIGC）等技术，成功落地首个全行应用的 AIGC 场景，实现大模型自动撰写投诉处理报告。</p><p></p><p>邮储银行研发了基于大模型的投诉问题分类智能模型，实现消保投诉管理自动统计分析和智能监测；上线推广消保投诉文本分析模型和消保审查智能辅助工具，有效提升消保管理事前审查和事后分析能力。</p><p>建设银行上线消保 AI 智能审查功能，通过智能信息识别与处理，产出 AI 审查结果，辅助审查人员，提高审查效率，提升消保审查的规范性和专业能力。</p><p></p><h2>“未来银行”怎么用 AI</h2><p></p><p></p><p>我国金融业大模型应用发展正处于政策红利期，但当前银行大模型大多只应用于为员工赋能减负以及提升客户体验，多数银行还处于技术储备和浅层试验阶段，AI 大模型还很难真正脱离“人”来发挥“AI+”的效果。随着金融领域垂直大模型技术的深入发展，接下来银行大模型应用将更多地触及银行核心业务。已有许多先行者在风险管控、投资决策方面展开了探索。</p><p></p><p>大模型帮助打造智能风控体系可能产生的效果主要有三个方面。第一，提升操作标准化程度，用自动化流程提高审批效率。第二，构建大模型智能分析系统，快速处理海量金融信息，提升业务过程中的风险评估能力，如交行打造的反诈一体化平台，精准拦截可疑交易超 7 万笔，涉及金额超 14 亿元。第三，降低人工运营的操作风险，强化合规管理水平，如邮储银行利用智能风控“智能审查助手”辅助法审工作合规。大模型助力银行风控的前景很可观，但目前大模型技术本身存在的合规风险、数据安全风险等都不明确，相应的监管框架和行业标准也有待确立。</p><p></p><p>而银行大模型在投资场景上的应用才刚刚迈出第一步，由于这一业务场景有很高的专业要求和经验要求，智能投研助手目前更多被用于整理投研报告、处理交易数据等重复性工作，尚不能针对不同对象生成定制化专业投资建议。在金融领域大模型全面、规模化发展之前，投研助手不太可能替代财富规划师等专业人员，甚至很难对他们形成“劣币驱逐良币”的压力。</p><p></p><p>毕马威《2024 中国银行业展望报告》认为，大模型的出现会催化“未来银行”迭代发展，基于 Agent 的生产力工具是下一代大模型应用体系中不可缺少的原子模块，大模型生成内容可控性与思维链推理能力可落地性有待观望。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/aHmMK0TMYYf1WSWtGm1U</id>
            <title>李彦宏：大模型开源意义不大；腾讯云后台崩了；离开百度7年后，吴恩达官宣加入亚马逊董事会 | Q资讯</title>
            <link>https://www.infoq.cn/article/aHmMK0TMYYf1WSWtGm1U</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/aHmMK0TMYYf1WSWtGm1U</guid>
            <pubDate></pubDate>
            <updated>Mon, 15 Apr 2024 07:18:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 苹果市值, 吴恩达, 李彦宏, 亚马逊
<br>
<br>
总结: 科技公司动态：苹果市值暴涨，计划升级Mac产品线；吴恩达加入亚马逊董事会；李彦宏内部讲话强调闭源模型持续领先；亚马逊被指侵犯数据存储专利需赔款。 </div>
                        <hr>
                    
                    <p></p><blockquote>苹果市值一夜暴涨8113亿元；吴恩达加入亚马逊董事会；李彦宏最新内部讲话；马云阿里内网发长文；网易与微软达成协议，暴雪游戏今年夏季将重返中国市场；英特尔、谷歌推出最强芯片挑战英伟达；腾讯云后台崩了；苹果突然曝出重大泄密事件；亚马逊中国部分员工被总部通知裁员；英国官宣：全面实行弹性工作制；蚂蚁集团CodeFuse&nbsp;发布“图生代码”功能；Adobe加快构建文生视频AI模型；图灵奖揭晓！史上首位数学和计算机最高奖“双料王”出现了……</blockquote><p></p><p></p><h2>科技公司</h2><p></p><p></p><h4>苹果市值一夜暴涨8113亿元，据称拟升级整个Mac产品线</h4><p></p><p>4月12日消息，当地时间4月11日美股收盘，苹果涨4.33%，报175.04美元，市值2.7万亿美元，其市值单日增加1121亿美元（约合人民币8113亿元）。随后，“苹果市值一夜暴涨8113亿”登上微博热搜。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6e/6eea25552792277cb0f53ef93f6dba1d.png" /></p><p></p><p>市面消息，苹果股价大涨可能与其正在酝酿重大升级有关。据媒体报道，为了提振低迷的电脑业务，苹果正准备彻底改造整个Mac产品线，新的Mac将配置具备AI功能的M4芯片。</p><p></p><p>知情人士称，苹果的目标是从今年年底到明年初发布更新版电脑，包括iMac、低端14英寸MacBook&nbsp;Pro、高端14英寸和16英寸MacBook&nbsp;Pro以及Mac&nbsp;mini都将配备M4芯片。但该公司的计划也可能会改变。</p><p></p><p>今年2月，在苹果公司的年度股东大会上，CEO蒂姆・库克曾表示，苹果正在生成式人工智能领域进行大量投资，他还承诺苹果将在今年晚些时候分享生成式人工智能技术相关计划。他还提到，苹果公司将在2024年在生成式人工智能领域“开辟新天地”。</p><p></p><h4>离开百度7年后，吴恩达加入亚马逊董事会</h4><p></p><p>路透社消息，当地&nbsp;4&nbsp;月&nbsp;12&nbsp;日，亚马逊发布公告称，计算机科学家吴恩达&nbsp;(Andrew&nbsp;Ng)&nbsp;成为亚马逊董事会成员，这项任命于&nbsp;4&nbsp;月&nbsp;9&nbsp;日生效。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2d/2d25ff90ef4b1088032bc71d38c1d214.png" /></p><p></p><p>作为斯坦福教授的吴恩达不必多介绍，他被提到最多的职业经历就是谷歌和百度两段经历。他也是机器学习、AI&nbsp;和在线教育领域的知名商业科技领袖、连续创业公司创始人、投资者以及教育家。曾是在线教育公司&nbsp;Coursera&nbsp;的联合创始人，目前是&nbsp;AI&nbsp;Fund&nbsp;风险投资基金的执行普通合伙人、教育科技公司&nbsp;DeepLearning.AI&nbsp;的创始人、计算机视觉初创公司&nbsp;Landing&nbsp;AI&nbsp;的创始人兼首席执行官。</p><p></p><p>在创立自己的教育公司之前，吴恩达曾于2014&nbsp;年&nbsp;5&nbsp;月加入百度，负责“百度大脑”计划，并担任百度公司首席科学家。2017&nbsp;年&nbsp;3&nbsp;月，吴恩达宣布从百度离职。在百度的三年里，吴恩达一度成为李彦宏之外的另外一个百度代言人。借助他的影响力，百度中美人工智能团队增长到了&nbsp;1300&nbsp;人，AI&nbsp;也逐渐应用到各个业务层面，确立了探索无人驾驶、自然语言处理和语音交互等底层技术的大方向。</p><p></p><p>亚马逊宣布任命吴恩达为董事会成员时表示，吴恩达“将帮助董事会了解&nbsp;AI&nbsp;带来的机遇和挑战以及其变革性的社会和商业潜力。”亚马逊在其代理声明中表示，自&nbsp;2014&nbsp;年以来一直担任亚马逊董事会成员的&nbsp;Judith&nbsp;McGrath&nbsp;今年决定不寻求连任董事会成员。</p><p></p><p>亚马逊正面临微软和&nbsp;OpenAI&nbsp;等公司在云计算和人工智能产品上的竞争。亚马逊&nbsp;CEO&nbsp;安迪・贾西（Andy&nbsp;Jassy）昨日发布了亚马逊的&nbsp;2023&nbsp;年度股东信，表示正大力投资&nbsp;AWS&nbsp;和生成式&nbsp;AI。</p><p></p><h4>李彦宏最新内部讲话：开源大模型不如闭源，后者会持续领先</h4><p></p><p>4&nbsp;月&nbsp;11&nbsp;日晚间消息，百度创始人、董事长兼&nbsp;CEO&nbsp;李彦宏近日的一次内部讲话曝光。讲话中，李彦宏针对当前业界热议的**“大模型应该开源还是闭源？”“AI&nbsp;创业者应当专注于模型研发还是应用开发？”**等问题，表达了自己的见解。</p><p></p><p>在此次内部讲话中，李彦宏提到，闭源模型在能力上会持续地领先，而不是一时地领先；模型开源也不是一个众人拾柴火焰高的情况。这跟传统的软件开源——比如Linux、安卓等很不一样。</p><p></p><p>李彦宏还表示，闭源，是有真正的商业模式的，是能够赚到钱的，能够赚到钱才能聚集算力、聚集人才。闭源在成本上反而是有优势的，只要是同等能力，闭源模型的推理成本一定是更低的，响应速度一定是更快的。</p><p></p><p>此外，李彦宏提到，无论中美，当前最强的基础模型都是闭源的。通过基础模型降维做出来的模型也是更好的，这使得闭源在成本、效率上更有优势。对于AI创业者来说，核心竞争力本就不应该是模型本身，这太耗资源了，而且需要长时间的坚持才能跑出来。</p><p></p><p>李彦宏认为，既做模型又做应用的“双轮驱动”，对创业公司不是好模式。创业公司的精力和资源都很有限，更应该专注。既做模型又做应用，势必会分散精力。</p><p></p><h4>亚马逊被控侵犯数据存储专利，赔款额达&nbsp;38&nbsp;亿元</h4><p></p><p>4月11日，美国伊利诺斯州联邦陪审团表示，AWS因侵犯Kove在数据存储技术方面的专利权而必须赔付5.25亿美元（约38亿元人民币）。Kove指控AWS的Amazon&nbsp;S3存储服务、DynamoDB数据库服务及其他产品侵犯了其云存储专利。Kove称，AWS云数据存储产品建立在Kove申请专利的可扩展云系统技术的基础上。</p><p></p><p><img src="https://static001.geekbang.org/infoq/27/27e166ca206c0c2e3f85c195e49f807a.webp" /></p><p></p><p>诉状称：“正是通过侵犯Kove的专利，AWS得以提供如此广范围、如此大规模的云服务，为&nbsp;AWS成为亚马逊最大的利润中心铺平了道路。”</p><p></p><p>周三，陪审团支持Kove的观点，裁定AWS侵犯了Kove的所有三项专利，不过驳斥了Kove关于AWS故意侵犯其权利的主张。AWS&nbsp;否认了这些指控，并辩称这些专利是无效的。Kove去年还在伊利诺斯州的另一起仍在审理中的诉讼中起诉谷歌侵犯了同样的专利。</p><p></p><h4>退休5年来首次，马云阿里内网发长文</h4><p></p><p>4月10日，马云在阿里内网发表题为《致改革&nbsp;致创新》的帖子，高度肯定蔡崇信和吴泳铭组成的新管理层的变革勇气，称阿里巴巴已重回健康成长轨道，并支持继续改革。据悉，这是退休后的马云五年来首次长篇幅分享对公司改革创新及未来前景的思考。马云在文中认为，过去这一年阿里最核心的变化，不是去追赶KPI，而是认清自己，重回客户价值轨道。通过向大公司病开刀，阿里重新回归效率至上、市场至上，变得简单和敏捷。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2e/2e2e745fd5abe5bee3b184c4f59b3f23.png" /></p><p>内部信全文</p><p></p><p>他认为新管理层“直面问题、直面未来，相信年轻人，对年轻团队充分授权，对于我们要什么，不要什么，做出了果断清晰的取舍”“不仅是突破昨日固化的战略，更是打造未来的阿里”。针对行业未来，马云判断“三、五年的时间跨度对于互联网领域而言，犹如一个世纪之久，足以发生翻天覆地的变化，AI时代刚刚到来，一切才刚开始，我们正当其时！”</p><p></p><p>据悉，这是退休后的马云五年来首次长篇幅在内网分享观点。一些阿里员工对马云突然在内网分享表示惊讶。</p><p></p><h4>网易与微软达成协议，《魔兽世界》等暴雪游戏今年夏季将重返中国市场</h4><p></p><p>4月10日消息，网易游戏官宣，暴雪旗下游戏将于2024年夏季重返中国大陆市场。&nbsp;协议称，暴雪旗下游戏将根据更新后的游戏发行协议于2024年夏季开始陆续重返中国大陆市场。此外，微软游戏和网易还达成了一项协议，尝试将新的网易游戏带到Xbox及其他平台。</p><p></p><p><img src="https://static001.geekbang.org/infoq/67/6746eb23eaf7d55006d146e1f6f2765d.png" /></p><p></p><p>消息发布后，“暴雪官宣回归”“网易暴雪复婚”也引爆热搜。</p><p></p><p><img src="https://static001.geekbang.org/infoq/29/293eed45ca5b552d95d158d5662bdd26.png" /></p><p></p><p>2022年11月底，合作长达14年的网易和暴雪突然宣布“分手”。这一消息一度引发“大地震”，也在社交平台上引发热议，不少玩家直言感到被“背叛”。</p><p></p><p>此后，“暴雪回归”频频传出消息，暴雪将寻求其他的代理商也始终备受市场关注，腾讯、米哈游等均被传出过相关消息。如今，兜兜转转一年多后，随着微软收购尘埃落定，暴雪又与网易重新携手。</p><p></p><p>在“暴雪国服回归”的话题谈论了一轮又一轮后，暴雪和网易又走在了一起。不过，和此前相比，这次变成了暴雪娱乐、微软游戏与网易三方合作。不仅暴雪国服回归，微软游戏还将尝试将新的网易游戏带到Xbox及其他平台。</p><p></p><h4>去年利润400亿美元，超过腾讯和阿里？字节跳动回应</h4><p></p><p>4月10日，有媒体公布字节跳动2023年的财务情况，称字节跳动2023年利润增长60%至400亿美元，超过了腾讯和阿里去年的增速。随后，字节跳动方面对此回应称"有关字节跳动利润增长及数据传言不实"。</p><p></p><p><img src="https://static001.geekbang.org/infoq/02/02b6c21097bac291a74f54293f5f1207.webp" /></p><p></p><p>其实早在3月17日，英国《金融时报》曾援引五位知情人士消息称，在&nbsp;TikTok&nbsp;“爆炸式增长”的推动下，字节跳动&nbsp;2023&nbsp;年的营收达到&nbsp;1200&nbsp;亿美元，同比增长约&nbsp;40%。</p><p></p><p>TikTok&nbsp;去年在美国的营收达到约&nbsp;160&nbsp;亿美元（当前约&nbsp;1152&nbsp;亿元人民币）创下新高，全年约有&nbsp;1.7&nbsp;亿美国人使用。报道称，TikTok的营收有望超越&nbsp;Facebook&nbsp;的母公司&nbsp;Meta，成为全球销售额最多的社交媒体公司。作为对照，Meta&nbsp;2023&nbsp;年的收入增长了&nbsp;16%，达到&nbsp;1349&nbsp;亿美元（当前约&nbsp;9712.8&nbsp;亿元人民币）。</p><p></p><p>随后，还有媒体报道，字节跳动在2023年的全年销售收入预计将增长30%，达到1100亿美元（约合7843亿元人民币）。这一数据显示出了字节跳动在全球市场上的强大实力和发展潜力。</p><p></p><p>从上半年的业绩表现来看，字节跳动已经在收入方面超越了腾讯。</p><p></p><h4>英特尔、谷歌推出最强芯片挑战英伟达</h4><p></p><p>北京时间4月10日凌晨，美国亚利桑那州Intel&nbsp;Vision&nbsp;2024会议上，芯片巨头英特尔（Intel）发布性能最强的新一代Gaudi3&nbsp;AI&nbsp;加速芯片，以及全新的下一代英特尔至强6处理器等产品。</p><p></p><p>其中，英特尔Gaudi&nbsp;3&nbsp;AI芯片采用台积电5nm工艺，支持128GB&nbsp;HBMe2内存。相比上代产品，英特尔Gaudi&nbsp;3带来4倍（400%）的BF16&nbsp;AI计算能力提升，1.5&nbsp;倍的内存带宽以及&nbsp;2&nbsp;倍的网络带宽提升。同时，在AI模型算力中，相比于英伟达H100&nbsp;GPU，Gaudi3&nbsp;AI芯片的模型训练速度、推理速度分别提升40%和50%，平均性能提高&nbsp;50%，能效平均提高40%，而成本仅为H100的一小部分。</p><p></p><p>英特尔预计，Gaudi&nbsp;3将于2024年第二季度起出货，戴尔、惠普、联想、超微电脑等企业将成为首批客户。</p><p></p><p>与此同时，今晨举行的谷歌云年度大会Cloud&nbsp;Next&nbsp;2024上宣布推出一款基于ARM架构的服务器芯片Axion，其性能比通用ARM芯片高30%，比英特尔生产的x86最新芯片性能提高50%。谷歌旨在减少对英特尔和AMD&nbsp;x86芯片的依赖。</p><p></p><p>全球围绕&nbsp;AI&nbsp;算力战争已经拉开帷幕。</p><p></p><h4>腾讯云后台崩了：大量服务报错、控制台登入后无数据</h4><p></p><p>4月8日，腾讯云一场突如其来的服务故障引发广泛关注和讨论，“腾讯云崩了”相关话题迅速冲上各大社交平台热搜，据统计持续了74分钟，波及全球17个区域与数十款服务。事故发生时，有部分用户表示腾讯云出现接口响应报错、内部服务错误等情况，网页直接显示“504&nbsp;错误”。</p><p></p><p>腾讯云下午4点45分回应称，官网控制台相关服务出现异常，工程师紧急修复中，部分地区已恢复。10分钟后，腾讯云又回应称控制台服务已恢复，API服务上海地区还在恢复中，其他地域已恢复。下午5点16分，腾讯云在微博回应称整体已恢复，并在官网公告称原因是“腾讯云官网控制台相关服务出现异常”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d9/d914c855c2e9d115d5edd0ca3be4ece6.jpeg" /></p><p></p><p>好巧不巧，阿里云刚刚降价了，腾讯云就崩了。</p><p></p><p>4月8日下午3点，阿里云官宣再降价：全球13个地域节点全线下调产品价格，平均降幅23%、最高降幅59%、覆盖五大类产品、500+产品规格。相对于一个月前的降价，此次价格调整主要面向海外市场；目前，阿里云在全球200多个国家服务500多万客户。</p><p></p><h4>苹果突然曝出重大泄密事件，多款新品泄露！</h4><p></p><p>据美国加州圣克拉拉县高等法院最新公布的文件，苹果公司近日起诉了一位名为Andrew&nbsp;Aude的前员工，指控其向媒体和其他科技公司泄露敏感信息，违反了公司保密协议和劳工法，苹果寻求超过2.5万美元的损失赔偿。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bb/bb71615cde64e818a503757220a68c51.jpeg" /></p><p></p><p>2016年，Aude在大学毕业后不久就入职苹果担任iOS软件工程师。诉状称，由于Aude负责电池性能的优化，这使得他可以“接触到数十个苹果最敏感项目的信息”。</p><p></p><p>诉状写道，Aude在五年内泄露的机密信息包括至少6款苹果产品，其中包括，当时尚未公布的混合现实头显Vision&nbsp;Pro、苹果产品的开发政策、监管合规战略、各大部门的员工人数变动等。</p><p></p><p>根据苹果公司的说法，Aude使用公司发放的工作iPhone通过加密消息应用“Signal”向一名《华尔街日报》的记者“Homeboy”发送了1400多条消息，还向硅谷科技媒体“The&nbsp;Infomation”的另一位记者发送了多达10000条短信，并专程去见面。由于Aude经常在工作iPhone上截图保存其与媒体记者的通讯记录，这使得苹果能够检索到这些内容。</p><p></p><h4>微软将在日本投资AI数据中心，投资额达4400亿日元</h4><p></p><p>据《日经新闻》4月9日报道，美国微软公司将扩大在日本的数据中心。计划在两年内投资29亿美元（约4400亿日元）。引进图像处理设备等用于人工智能的最新半导体技术，扩充东京和大阪数据中心的设施。</p><p></p><p>同时在东京建立新的研究基地，致力于与人工智能相关的再培训（再学习）支持措施，目标是在三年内培训&nbsp;300&nbsp;万人。并推进人工智能和机器人技术的研究，并致力于解决老龄化社会等社会问题。还将与日本政府合作应对网络攻击。</p><p></p><p>**这是迄今为止，微软公司在日本的最大投资。**而在这之前半个月，世界芯片的最大制造商——台积电宣布在日本的熊本县建设第二家芯片制造工厂。第一家工厂已在今年3月建成投产。</p><p></p><p>在岸田文雄首相访问美国之际，微软宣布了对日本的投资，这将是对日本的最大投资。从2024年开始，将在东日本和西日本的两个地点引入AI半导体等技术。同时，还将宣布针对300万人的为期三年的AI相关再培训（学习）支持措施、建立研究机器人和AI的国内基地、以及在防范网络攻击方面与日本政府的合作。</p><p></p><p>此前，世界最大的半导体企业--台积电宣布将在日本建设第二家工厂；亚马逊与OpenAI也设立办公室将在日本摩拳擦掌。</p><p></p><h4>亚马逊中国部分员工被总部通知裁员</h4><p></p><p>4&nbsp;月&nbsp;8&nbsp;日消息，亚马逊中国部分员工据悉收到了来自总部的裁员通知邮件。流出的邮件内容显示，亚马逊称已在业务其他领域优化了团队，并发现“在项目管理、销售运营等工作类别中存在重复”，因此在特定的销售、市场营销和全球服务组织中减少数百个职位。</p><p></p><p><img src="https://static001.geekbang.org/infoq/77/7737441704fc7e7a755f812c500a6aab.png" /></p><p>邮件原文</p><p></p><p>流出的邮件内容显示，亚马逊称已在业务其他领域优化了团队，并发现"在项目管理、销售运营等工作类别中存在重复"，因此在特定的销售、市场营销和全球服务组织（SMGS）中减少数百个职位。</p><p></p><p>当地时间上周三，亚马逊曾宣布将在其云计算部门&nbsp;AWS&nbsp;中裁员数百人，这是“战略转变”的一部分。同时，亚马逊也将在负责实体店技术的团队中裁减数百个职位。亚马逊当时表示，它还将在其他地方进行削减，以便可以投资于其他业务重点。</p><p></p><p>这次亚马逊具体的赔偿方案还未给出。如果追溯到上一次亚马逊的裁员，是&nbsp;2023&nbsp;年初，当时亚马逊宣布裁员&nbsp;1.7W&nbsp;人，是历史上科技行业里规模最大的公开裁员，不幸中的万幸是，当时给的赔偿方案是&nbsp;N+6。</p><p></p><h4>英国官宣：全面实行弹性工作制，不再朝9晚5！</h4><p></p><p>根据英国最新法律，从4月6日起，全英开始正式实行弹性工作制！在此之前，只有当员工为雇主工作了26周及以上时，才可以享受弹性工作的福利。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bdf858e8ae28f05b692ec2da42db8f33.png" /></p><p></p><p>弹性工作(Flexible&nbsp;working)，旨在打破“朝九晚五”等固定上下班时间、办公地点的框架，包括员工可自己决定：</p><p></p><p>工作时长开始和结束工作的时间工作的日期工作的地点</p><p></p><p>根据新规定，从4月6日起所有英国雇员在入职新工作时，都有权要求弹性工作制，并将受到法律保护。官方宣称，该项全新规定将令数百万英国雇员获益匪浅，对于那些存在健康问题、需要承担照料责任以及期望进行其他生活方式选择的雇员而言，尤为关键。</p><p></p><p>英国特许人事与发展协会(CIPD)首席执行官Peter&nbsp;Cheese表示：“随着英国人口老龄化，以及因健康状况不佳而无法参与经济活动的人数持续攀升，灵活的工作方式在当下比以往任何时候都更为重要，且已证实能够助力提升福祉，对个人和企业均有益处。</p><p></p><h2>IT&nbsp;业界</h2><p></p><p></p><h4>蚂蚁集团CodeFuse&nbsp;发布“图生代码”功能，支持产品设计图一键生成代码</h4><p></p><p>4月11日，蚂蚁集团自研的智能研发平台CodeFuse推出“图生代码”新功能，支持开发人员用产品设计图一键生成代码，大幅提升前端页面的开发效率。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/cac8fe9fdfd31ed349020c64754aea37.png" /></p><p></p><p>此次发布的“图生代码”功能主要服务于前端工程师。目前，在蚂蚁集团内部，每周已有超五成工程师在日常研发中使用CodeFuse，这些工程师提交的代码中有10%是由AI生成的。CodeFuse生成的代码整体采纳率为30%，在生成单元测试场景采纳率可以达到50%。</p><p></p><p>据介绍，CodeFuse除了支持常规的生成代码、注释、解释等编程能力外，还在打造贯穿企业研发全流程的能力，是“AI全生命周期研发”的首倡者和探索者。</p><p></p><p>蚂蚁集团CodeFuse负责人表示，AI普及更大的意义在于降低编程门槛，推动软件开发行业的创新和进步。AI研发范式的变革对AI和人如何协同提出了更高的要求，特别是涉及可靠性的运维场景，还需要人工专家干预才能让系统健康运行起来。</p><p></p><h4>叫板Sora！Adobe加快构建文生视频AI模型</h4><p></p><p>据财联社4月11日报道，在OpenAI的文生视频模型&nbsp;Sora&nbsp;引发市场狂热关注后，Adobe公司也耐不住了。目前，该公司已经加快脚步，开始采购视频，以构建其AI文生视频模型。值得一提的是，OpenAI目前正面临其AI模型训练数据来源的争议中，而Adobe直接付费向摄影师和艺术家们采购视频的做法，显然会被认为更为合规。</p><p></p><p>根据一份文件显示，该软件公司目前向其摄影师和艺术家网络每人支付120美元，要求他们提交人们日常活动的一些视频，比如走路或表达喜悦和愤怒等情绪。该公司写道，目标是为人工智能训练提供资源。</p><p></p><p>在过去的一年里，Adobe一直致力于为其创意专业人士的软件组合（包括Photoshop和Illustrator）添加生成式人工智能功能。该公司已经发布了使用文本生成图像和插图的工具，这些工具到目前为止已经被使用了数十亿次。</p><p></p><p>尽管如此，在OpenAI推出其文生视频模型Sora的演示之后，Adobe投资者愈发担忧，这家长期处于创意软件行业领先地位的公司可能会被这项新技术颠覆。</p><p></p><p>为此，Adobe表示，他们正在加快开发文生视频技术，并计划在今年晚些时候讨论更多相关内容。</p><p></p><h4>图灵奖揭晓！史上首位数学和计算机最高奖“双料王”出现了</h4><p></p><p>4月10日，美国计算机协会ACM宣布艾维·维格森（Avi&nbsp;Wigderson）为2023年ACM&nbsp;AM&nbsp;图灵奖获得者，以表彰他在计算理论方面的基础性贡献，包括重塑了对随机性在计算中作用的理解，以及他数十年来在理论计算机科学领域的学术领导地位。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9f/9fa3a53562d3416a5afed991ee59a3b8.png" /></p><p></p><p>2021年，维格森还因“对理论计算机科学和离散数学的基础性贡献，以及他们将其塑造为现代数学的中心领域方面的领导作用”，获得了数学界“诺贝尔奖”之称的阿贝尔奖（诺贝尔奖没有计算机、数学奖项）。</p><p></p><p></p><blockquote>维格森是以色列数学家、计算机科学家，美国科学院院士，任职于美国普林斯顿高等研究院数学学院。他的研究包括计算复杂性理论、算法和优化、随机性和密码学、并行和分布式计算、组合论和图论、CS 理论与数学和科学的联系。1980 年，维格森以优等成绩获得以色列理工学院计算机科学学士学位，随后又前往美国普林斯顿大学攻读研究生，一年左右的时间就获得了硕士学位。1983 年，在 Richard Lipton 的指导下完成了题为“计算复杂性研究”的博士论文，获得了计算机科学博士学位。维格森一生都在从事教育和研究。过去四十年来，对理论计算机科学研究做出了开创性贡献。</blockquote><p></p><p></p><p>维格森一生都在从事教育和研究。过去四十年来，对理论计算机科学研究做出了开创性贡献。</p><p></p><p>此前，计算机科学家发现随机性和计算难度（即识别没有有效算法的自然问题）之间存在显着的联系。威格德森与同事合作，撰写了一系列极具影响力的关于用硬度换取随机性的著作。他们证明，在标准且广泛相信的计算假设下，每个概率多项式时间算法都可以有效地去随机化（即完全确定性）。换句话说，随机性对于高效计算来说并不是必需的。这一系列作品彻底改变了我们对随机性在计算中的作用的理解，以及我们思考随机性的方式。</p><p></p><h4>网易有道自研&nbsp;RAG&nbsp;引擎&nbsp;QAnything&nbsp;升级</h4><p></p><p>4&nbsp;月&nbsp;8&nbsp;日，有道知识库问答引擎&nbsp;QAnything&nbsp;更新至&nbsp;1.3.0&nbsp;版本，该版本带来了两大主要功能升级：发布纯&nbsp;python&nbsp;的轻量级的版本，该版本支持在&nbsp;Mac&nbsp;上运行，也可以在纯&nbsp;CPU&nbsp;机器上运行；同时支持&nbsp;BM25&nbsp;+&nbsp;embedding&nbsp;混合检索，可以实现更精准的语义检索和关键字搜索。本次更新后，QAnything&nbsp;能为开发者探索大模型落地提供更强大的技术支撑和更流畅的用户体验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f4/f43dee1f0704274b8890efda3e294f93.png" /></p><p></p><p>QAnything&nbsp;系统架构图</p><p></p><p>QAnything&nbsp;是网易有道自研的&nbsp;RAG（Retrieval&nbsp;Augmented&nbsp;Generation)&nbsp;引擎。该引擎允许用户上传&nbsp;PDF、图片、Word、Excel、PowerPoint&nbsp;等多种格式的文档，并实现类似于&nbsp;ChatGPT&nbsp;的互动问答功能，其中每个答案都能精确追溯到相应的文档段落来源。该引擎支持纯本地部署，上传文档数量无上限，问答准确率很高。</p><p></p><p>GitHub&nbsp;地址：</p><p><a href="https://github.com/netease-youdao/QAnything">https://github.com/netease-youdao/QAnything</a>"</p><p></p><p>自今年&nbsp;1&nbsp;月开源以来，QAnything&nbsp;迅速吸引了开发者社区的广泛关注，并多次登上了&nbsp;GitHub&nbsp;trending&nbsp;榜单。截至目前，在&nbsp;GitHub&nbsp;上&nbsp;QAnything&nbsp;已经积累&nbsp;7000+个星标，这反映出了用户对其价值的高度评价。</p><p></p><h4>美国众议院提出《生成式人工智能版权披露法案》</h4><p></p><p>4月9日，加利福尼亚州民主党众议员亚当希夫提出了《生成式人工智能版权披露法案》（Generative&nbsp;Al&nbsp;Copyright&nbsp;Disclosure&nbsp;Act），要求人工智能公司在发布新的生成式人工智能系统之前，需向版权注册处提交其训练数据集中的所有版权作品。该法案要求公司在公开发布其人工智能工具前至少30天提交此类文件，否则将面临经济处罚此类数据集包括数十亿行文本和图像，或数百万小时的音乐和电影。</p><p></p><p><img src="https://static001.geekbang.org/infoq/15/157ef68ec2815b9bf5fc7dcd90262678.png" /></p><p>《生成式人工智能版权披露法案》</p><p></p><p>大型人工智能公司是否非法使用了受版权保护的作品正日益成为诉讼和政府调查的焦点。亚当希夫提出的法案虽然不会禁止人工智能在受版权保护的材料上进行训练，但会让公司承担相当大的责任，这将要求列出他们用来构建ChatGPT等工具的大量数据，而这些数据通常是保密的。</p><p></p><p>据悉，《生成式人工智能版权披露法案》一经公布，就到了众多美国娱乐行业组织和工会的支持，其中包括美国唱片业协会、美国职业摄影师协会、美国导演协会和美国电视与广播艺术家联合会。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/uOYmzLuJ71csv6UwfoU2</id>
            <title>巨头们火力全开：AI应用边界再扩张 | 大模型一周大事</title>
            <link>https://www.infoq.cn/article/uOYmzLuJ71csv6UwfoU2</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/uOYmzLuJ71csv6UwfoU2</guid>
            <pubDate></pubDate>
            <updated>Mon, 15 Apr 2024 07:12:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, AI创新, 开源领域, 科研领域
<br>
<br>
总结: 大模型的快速发展推动了AI创新，各大公司不断推出新产品和升级现有产品，同时在开源和科研领域取得突破。从视频生成到代码编写，大模型在多个领域展现出强大的能力。 </div>
                        <hr>
                    
                    <p>大模型的快节奏发展，让了解最新技术动态、积极主动学习成为每一位从业者的必修课。InfoQ研究中心期望通过每周更新大模型行业最新动态，为广大读者提供全面的行业回顾和要点分析。现在，让我们回顾过去一周的大模型重大事件吧。</p><p></p><h2>一、重点发现</h2><p></p><p>AI创新不止步，金山办公、谷歌、OpenAI、蚂蚁集团接连升级或推出新产品，持续拓展应用范围。大模型基础研究不断取得突破，能解码mRNA非翻译区序列的大模型为预测mRNA功能和设计mRNA疫苗新序列提供了新的可能。同时，亚马逊云科技、阿里云通义千问、aiXcoder&nbsp;等企业或团队也开源了各自的大模型，提升了AI在视频生成、文本控制、代码编写等领域的能力。此外，多家公司在具身智能领域取得了新进展。谷歌、英特尔推出了一系列AI相关更新和新产品，推动了行业基础设施能力的发展。本文将为你揭示这些新品的魅力和价值。</p><p></p><h2>二、具体内容</h2><p></p><p></p><h3>大模型持续更新</h3><p></p><p></p><h4>大语言模型</h4><p></p><p>4月7日，亚马逊云科技宣布，Mistral&nbsp;AI的Mistral&nbsp;Large模型现已在Amazon&nbsp;Bedrock平台上正式可用。</p><p></p><h4>开源领域</h4><p></p><p>4月7日，阿里云通义千问再次开源了一款大语言模型——拥有320亿参数的Qwen1.5-32B。4月7日，北大-兔展AIGC联合实验室发布了复现OpenAI公司的sora&nbsp;的开源Open-Sora-Plan&nbsp;v1.0.0模型。该模型大幅提升了视频生成质量和文本控制能力，能生成10秒、24&nbsp;FPS的1024×1024视频及高分辨率图像，并支持华为昇腾910b等国产AI芯片的训练与推理。4&nbsp;月&nbsp;9&nbsp;日，aiXcoder&nbsp;团队开源了全自研&nbsp;aiXcoder&nbsp;7B&nbsp;代码大模型。该模型专注于代码生成与补全任务，提供了个性化训练、私有化部署和定制化开发的解决方案，以满足不同企业的特定需求。Mistral&nbsp;AI开源了Mistral&nbsp;8X22B大模型，共有1760亿个参数，Context长度为6.5万个&nbsp;token，可通过Torrent下载。</p><p></p><h4>科研领域</h4><p></p><p>普林斯顿大学王梦迪领导的研究团队开发了全球首个能够解码mRNA非翻译区序列的大模型。这一模型的应用目标是精确预测mRNA转录为蛋白质的功能，并设计用于mRNA疫苗的新序列。该研究的论文为「A&nbsp;5’&nbsp;UTR&nbsp;Language&nbsp;Model&nbsp;for&nbsp;Decoding&nbsp;Untranslated&nbsp;Regions&nbsp;of&nbsp;mRNA&nbsp;and&nbsp;Function&nbsp;Predictions」，已被《Nature&nbsp;Machine&nbsp;Intelligence》采纳。朱泽园&nbsp;(Meta&nbsp;AI)&nbsp;和李远志&nbsp;(MBZUAI)&nbsp;的最新研究《语言模型物理学&nbsp;Part&nbsp;3.3：知识的&nbsp;Scaling&nbsp;Laws》用海量实验为&nbsp;LLM&nbsp;在不同条件下的知识容量提供了较为精确的计量方法。该研究探讨了三种合成数据类型：bioS、bioR和bioD，分别代表使用英语模板编写的人物传记、由LlaMA2模型辅助撰写的人物传记，以及可以控制细节的虚拟知识数据。研究重点在于分析基于GPT2、LlaMA和Mistral的语言模型架构。</p><p></p><h3>应用探索</h3><p></p><p></p><h4>新产品新应用/功能</h4><p></p><p>4月9日，金山办公发布了专为组织和企业设计的办公新质生产力平台WPS&nbsp;365。该平台集成了升级的WPS&nbsp;Office、新发布的WPS&nbsp;AI企业版及WPS协作，实现了文档、AI、协作的无缝整合。用户仅需一个工具，即可调用各类主流大模型。谷歌升级了Gemini&nbsp;1.5&nbsp;Pro大语言模型，为其新增音频分析能力，可直接从音频文件中提取关键信息，无需转换为文字。4月10日，OpenAI宣布GPT-4&nbsp;Turbo&nbsp;with&nbsp;Vision版现已对外开放，用户现可通过API接口对其进行访问。此外，该功能支持使用JSON模式和函数进行调用。4月11日，蚂蚁集团的智能研发平台CodeFuse新增了一项新功能——“图生代码”，该功能允许开发人员通过产品设计图快速生成相应的代码，显著提高了前端页面开发的效率。目前，这项新功能正处于内部测试阶段。</p><p></p><h4>智能体</h4><p></p><p>4月9日，在Google&nbsp;Cloud&nbsp;Next&nbsp;2024大会上，谷歌推出Vertex&nbsp;AI&nbsp;Agent&nbsp;Builder，是一个帮助企业构建AI智能体的新工具，它使得构建和部署生成式对话智能体变得简单快捷。</p><p></p><h4>具身智能</h4><p></p><p>逐际动力的人形机器人CL-1在最新视频中展示了其改进的上楼梯和跑步能力，同时在实时地形感知、全身运动控制和硬件性能上都有所提升。CL-1能够交替上楼梯，流畅完成跑步动作，并优化了运动控制和硬件结构，实现了更强的稳定性和动力性能。斯坦福大学的ALOHA家务机器人团队发布了最新研究Yell&nbsp;At&nbsp;Your&nbsp;Robot，使用者能够通过喊话纠正机器人的错误动作。机器人能动态提升动作水平、调整策略，并根据反馈不断自我改进。</p><p></p><h3>基础设施</h3><p></p><p>4月9日，曾担任特斯拉Autopilot项目负责人以及OpenAI科学家的Andrej&nbsp;Karpathy推出了一个创新项目，名为“llm.c”，该项目通过仅1000行代码便能在CPU和fp32精度下实现对GPT-2模型的训练。4月9日，在Google&nbsp;Cloud&nbsp;Next&nbsp;2024大会上，谷歌宣布了一系列AI相关的更新和新产品。Gemini&nbsp;1.5&nbsp;Pro在Vertex&nbsp;AI平台上提供了公共预览版。谷歌还推出了三大开源工具：Max&nbsp;Diffusion、Jetstream和MaxText，这些工具旨在支持生成式AI项目和基础设施。在硬件方面，谷歌云宣布推出首款自主研发的Arm处理器Axion，据称其性能比竞争对手高出30%，能效提高了60%。此外，谷歌推出的CodeGemma是基于Gemma模型的代码生成和补全工具，它提供了智能代码补全、高准确性和多语言支持，能够简化开发人员的工作流程。Google&nbsp;DeepMind发布的RecurrentGemma是一系列开放权重语言模型，基于Griffin架构，通过局部注意力和线性循环实现快速推理。Google&nbsp;Vids是谷歌推出的AI视频创建工具，它允许用户在Google&nbsp;Workspace中与其他工具如文档和表格一起制作视频，并支持实时协作。最后，Gemini&nbsp;Code&nbsp;Assist是谷歌推出的企业级AI代码完成和辅助工具，旨在提供更准确的代码建议和处理大段代码的能力。4月10日，在Vision&nbsp;2024大会上，英特尔展示了由其子公司Habana&nbsp;Labs开发的最新款高性能AI加速器——Gaudi&nbsp;3，并计划在2024年第三季度正式推出。</p><p></p><p>报告预告</p><p>Sora来袭，国内如何迅速跟上？开源在大模型市场进程中的价值正在被重新定义吗？人型机器人重回视野，其能力是否有所提升和刷新？Devin和智能编码助手是同一条赛道上的不同节点？多家企业宣布All&nbsp;in&nbsp;AI，对市场意味着什么？InfoQ研究中心即将发布的《2024&nbsp;年第&nbsp;1&nbsp;季度大模型监测报告》，即将给出答案。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c9/c9b3c569c62a571715d811e7121db70f.png" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/zdWk0CtmQSkjmkaxpRay</id>
            <title>芯片反击，英特尔和AMD惨了！国内电信运营商逐步淘汰外国芯片，网友：这只是个开始</title>
            <link>https://www.infoq.cn/article/zdWk0CtmQSkjmkaxpRay</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/zdWk0CtmQSkjmkaxpRay</guid>
            <pubDate></pubDate>
            <updated>Mon, 15 Apr 2024 06:55:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 国产芯片, 电信运营商, 外国芯片, 技术依赖
<br>
<br>
总结: 中国电信运营商将逐步淘汰外国芯片，以摆脱对外国芯片的依赖，这一决策将对AMD和英特尔产生巨大打击。中国正努力减少对外国技术的依赖，推动国产芯片的发展，以提升自身的技术实力和竞争力。 </div>
                        <hr>
                    
                    <p></p><blockquote>国产芯片即将迎来黄金时代。</blockquote><p></p><p></p><h2>我国电信运营商将逐步淘汰外国芯片</h2><p></p><p>&nbsp;</p><p>据外媒报道，我国监管机构已向电信运营商提出明确要求，要求国内大型电信运营商在2027年前逐步淘汰外国芯片，旨在摆脱对外国芯片的依赖。据悉，这一决策覆盖中国电信、移动、联通、网通、铁通、卫通等所有电信运营商。</p><p>&nbsp;</p><p>据知情人士透露，监管机构已经命令国有移动运营商全面检查其网络中是否普遍使用“非中国”半导体，并要求他们起草更换时间表，以确保按计划逐步淘汰外国芯片。</p><p>&nbsp;</p><p>《华尔街日报》指出，这一政策将对AMD 和英特尔产生巨大打击。2023年，中国占据了英特尔营收的27%，从而成为了该公司最大的市场。与此同时，去年AMD的销售额中有15%来自中国（含中国香港地区）。尽管美国出台了旨在限制对华芯片出口的法规，中国也在努力减少对外国技术的依赖，但这两家科技巨头对中国的依赖仍然凸显了世界第二大经济体的持续重要性。</p><p>&nbsp;</p><p>半导体是从智能手机到医疗设备等各种设备中的关键部件，一直是中美之间技术战的核心。</p><p>&nbsp;</p><p>据英国《金融时报》上个月的报道，我国在去年12月就已经制定了新的指导方针，旨在从政府计算机和服务器中移除美国芯片，从而阻止AMD和英特尔的处理器使用。</p><p>&nbsp;</p><p>此前，美国于2022年10月制定了限制中国获取美国先进芯片的规则，尤其是那些对人工智能技术至关重要的芯片。去年年底，美国更是宣布了新的限制措施，意图通过阻止更多人工智能芯片的对华出售来弥补先前命令中的漏洞。此外，据彭博社上个月的报道，AMD为中国设计的人工智能芯片未能获得美国的批准，并需要申请出口许可证。</p><p>&nbsp;</p><p>中国电信运营商的采购情况表明，他们正越来越多地转而购买国货。《华尔街日报》称，这在一定程度上受助于国产芯片的质量和稳定性有所提升。</p><p></p><h2>国产芯片突围</h2><p></p><p>&nbsp;</p><p>长期以来，我国芯片产业一直面临着国外技术封锁和市场垄断的双重困境。由于起步较晚，国产芯片产业一直处在追赶世界领先技术的状态，也因为同样的原因，中国芯片主要在依靠进口进行供应。</p><p>&nbsp;</p><p>公开资料显示，芯片一直是中国进口金额最大的商品。根据海关公开数据，2019 年，中国芯片进口额 3040 亿美元，超过原油、铁矿砂、粮食总和 3016 亿美元。另据海关总署统计：2021年1—12月，我国集成电路进口数量达到6355亿个，同比增长16.9%；金额为27934.8亿元人民币，同比增长15.4%。2021年1—12月，我国二极管及类似半导体器件进口7497亿个，同比增长38%；金额为1918亿元人民币，同比增长18.2%。</p><p>&nbsp;</p><p>此外，我国芯片产业人才缺口大、芯片产业人才队伍难成体系。根据《中国集成电路产业人才白皮书（2019—2020年）》，到2022年我国芯片专业人才仍会有近25万的缺口。另据《国家集成电路产业推荐纲要》，2030年集成电路产业将扩大至5倍以上，对人才的需求将成倍增长。白皮书也提到，目前需要70万人投入到该产业中来。华中科技大学微电子学院副院长、教育部“长江学者”特聘教授缪向水表示，如果国家对集成电路项目全部投资到位，中国需要70万人，而目前中国的从业者只有一半左右，约30多万。这也意味着，我国芯片产业存在40万人才缺口。</p><p>&nbsp;</p><p>近几年，我国一直在大力发展国内半导体产业，以减少对外国技术的依赖。一批具有自主知识产权的芯片产品相继问世，不仅在性能上逐渐接近国际先进水平，而且在应用领域也展现出广阔的前景。</p><p>&nbsp;</p><p>去年10月，国产芯片传来重大突破，清华大学集成电路学院教授吴华强、副教授高滨团队基于存算一体计算范式，研制出全球首颗全系统集成的、支持高效片上学习（机器学习能在硬件端直接完成）的忆阻器存算一体芯片，在支持片上学习的忆阻器存算一体芯片领域取得重大突破，有望促进人工智能、自动驾驶可穿戴设备等领域发展。</p><p>&nbsp;</p><p>该芯片包含支持完整片上学习所必需的全部电路模块，成功完成图像分类、语音识别和控制任务等多种片上增量学习功能验证，展示出高适应性、高能效、高通用性、高准确率等特点，有效强化智能设备在实际应用场景下的学习适应能力。相同任务下，该芯片实现片上学习的能耗仅为先进工艺下专用集成电路（ASIC）系统的1/35，同时有望实现75倍的能效提升，展现出卓越的能效优势，极具满足人工智能时代高算力需求的应用潜力，为突破冯·诺依曼传统计算架构下的能效瓶颈提供了一种创新发展路径。</p><p>&nbsp;</p><p>去年11月，国内首个自主研发并实现量产的LPDDR5产品诞生。11月28日，长鑫存储正式推出LPDDR5系列产品，包括12Gb的LPDDR5颗粒，POP封装的12GB LPDDR5芯片及DSC封装的6GB LPDDR5芯片。12GB LPDDR5芯片目前已在国内主流手机厂商小米、传音等品牌机型上完成验证。LPDDR5是长鑫存储面向中高端移动设备市场推出的产品，它的市场化落地将进一步完善长鑫存储DRAM芯片的产品布局。</p><p>&nbsp;</p><p>有报道显示，近几年我国本土芯片自给率逐年提升。据IC Insights统计，2020年，中国大陆芯片市场规模约为1460亿美元，而中国本土生产的芯片规模约为242亿美元，计算下来芯片自给率约为16.6%，2021年约为17.6%，2022年约为18.3%。</p><p>&nbsp;</p><p>此前，环球时报一篇关于国产芯片自给率的报道指出，参考过去几年的发展势头，以及国内外产业政策的影响，2024年，中国本土芯片厂商将会加快生产，中国的芯片自给率可能会提高到30%-35%。</p><p>&nbsp;</p><p>展望未来，国产芯片突围的道路虽然充满挑战，但也蕴藏机遇。有观点指出，国产芯片突围的关键在于技术创新和产业升级。一方面，我们要加强核心技术研发，突破关键领域的瓶颈，形成自主可控的技术体系。另一方面，我们要推动芯片产业与上下游产业的深度融合，形成完整的产业链和生态圈，提升整体竞争力。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.cnbc.com/2024/04/12/amd-intel-dip-on-report-china-told-telecoms-to-remove-foreign-chips.html">https://www.cnbc.com/2024/04/12/amd-intel-dip-on-report-china-told-telecoms-to-remove-foreign-chips.html</a>"</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/XkHaUUr6GPZdLZs5j1Tq</id>
            <title>字节跳动成全球最大独角兽公司？官方回应；智己汽车三次致歉小米：我们被网络霸凌；苹果计划裁员超 600 人｜AI周报</title>
            <link>https://www.infoq.cn/article/XkHaUUr6GPZdLZs5j1Tq</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/XkHaUUr6GPZdLZs5j1Tq</guid>
            <pubDate></pubDate>
            <updated>Mon, 15 Apr 2024 03:38:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 字节跳动, 利润增长, 独角兽, 错误标注
<br>
<br>
总结: 字节跳动连续第三年成为全球最有价值的独角兽公司，其利润在2023年飙升约60%，超过了竞争对手，但官方否认了利润增长的传言。智己汽车在发布会上错误标注小米SU7 Max的电机参数，引发小米公司的强烈反应。马云发文称阿里已重回健康轨道，AI时代才刚刚开始。李彦宏表示闭源模型才能持续领先，不会抢开发者饭碗。苹果计划裁员，亚马逊等公司也执行裁员计划。 </div>
                        <hr>
                    
                    <p></p><blockquote>引言：字节连续第三年成为全球价值最高的独角兽；智己汽车标注错误小米 SU7 数据，被称“碰瓷式营销”；马云退休五年首次发文：AI 时代一切才刚开始；网传苹果将于年末发布 M4 芯片，消息致股价飙升；李彦宏称闭源模型才能持续领先；腾讯云宕机，阿里云降价……</blockquote><p></p><p></p><h1>热门资讯</h1><p></p><p></p><h3>&nbsp;字节跳动成为全球最大独角兽公司，23 年利润狂飙 60%？官方回应：传言不实</h3><p></p><p>近日，胡润研究院发布了《2024 全球独角兽榜》，成立于 2012 年的字节跳动以 1.56 万亿元人民币的价值连续第三年成为全球价值最高的独角兽。</p><p><img src="https://static001.geekbang.org/infoq/d8/d8d6edeb1435075e58da76d5b58ed007.webp" /></p><p>4 月 10 日下午消息，据外媒报告，字节跳动的利润在 2023 年飙升约 60%，超过了腾讯控股和阿里巴巴集团控股的增长，这表明 TikTok 所有者在经济低迷面前表现出韧性。据不愿透露姓名的知情人士透露，这家全球最有价值的初创公司的销售额从 800 亿美元增至近 1200 亿美元（当前约 8688 亿元人民币），息税折旧摊销前利润从约 250 亿美元跃升至超过 400 亿美元（当前约 2896 亿元人民币）。</p><p>报告称，这一结果标志着字节跳动在收入和利润上首次超越主要竞争对手腾讯，因为它利用其受欢迎的短视频平台扩展到国际电子商务并维持其全球知名度。尽管字节跳动的内部数据尚未经过独立审计，但他们认为该公司将在 2023 年成为全球增长最快的科技巨头之一。</p><p>对此，字节跳动通过官方账号发文称，“媒体有关字节跳动利润增长及数据传言不实。”此外，从字节跳动内部获悉，字节跳动 2024 年首轮期权回购已经开始。现任员工的回购价格为 170.81 美元 / 股，离职员工的回购价格为 145.19 美元 / 股。与去年下半年回购计划中的现任员工 160 美元 / 股相比，此次现任员工的回购价格有所提高。</p><p></p><h3>&nbsp;“碰瓷式营销”？智己汽车三次致歉小米，仍遭网暴</h3><p></p><p>4 月 8 日晚，智己汽车在其 L6 新车型的发布会上，将小米 SU7 设为对标竞品，但将竞品小米 SU7 Max 的电机参数错误地标注为“前 IGBT、后 SIC”，而实际上小米 SU7 Max 的前后电机均采用 SIC 碳化硅模块，此举立即引发了小米公司的强烈反应。当晚，小米公司发言人通过社交媒体深夜连发三文，要求智己公司就其发布会上对小米 SU7 Max 关键参数的错误标注进行公开道歉。随后，智己汽车发文，承认了团队内容审核的疏漏，并表示没有蓄意抹黑小米汽车的意图。</p><p>4 月 10 日下午，智己汽车在官方微博发布声明，称三次正式致歉后，在智己汽车的官方渠道仍集中出现大量使用侮辱、诽谤、人身攻击言语的各种骚扰行为。智己汽车写道：“各个网络平台上同样出现了大量极其相似的，冠以耸人听闻标题的内容，试图歪曲和抹黑智己 L6 发布会公布的产品技术创新，甚至对智己车主以及发布会的两位发言人进行人身攻击。”智己汽车在声明中表示：“面对如此猖獗的网络霸凌行为，我们感到强烈的愤慨！如有必要，我们将持续公布这些网络暴力行为！”</p><p>4 月 12 日，清陶 CEO 李峥在朋友圈公开表示支持智己汽车，并称自己“嫉恶如仇”。他写道：最近两天，智己汽车遭遇了前所未有的网暴，无外乎就是智己真正的产品价值触动了某些人和某些品牌的核心利益。同时他还表示，“以前接触汽车圈不多，后期接触多了，实在看不惯所谓 H 粉、B 粉和所谓新晋的 M 粉的价值，坚持科技创新，坚持把更有性价比的配置和体验送达消费者，有什么错。”最后他还写道：我们也曾被网曝，所以更懂智己！我坚信，做时间的朋友，坚持科技创新，坚持价值创造，胜利属于你，智己汽车！</p><p></p><h3>&nbsp;马云退休五年首次发文：阿里已重回健康轨道，AI 时代一切才刚开始</h3><p></p><p>4 月 10 日上午消息，马云在阿里内网发表题为《致改革 致创新——写在阿里重组一周年》的帖子，高度肯定蔡崇信和吴泳铭组成的新管理层的变革勇气，称阿里巴巴已重回健康成长轨道，并支持继续改革。其中表示，有错误不可怕，没有人不犯错，真正可怕的是不知错、不认错、不改错。</p><p>马云提到阿里这一年最核心的变化是不再去追赶 KPI，而是认清自己，重回客户价值轨道。“我们向大公司病开刀，从一个决策缓慢的组织重新回到效率至上、市场至上，重新让公司变得简单和敏捷。”马云在文章中表示。</p><p>面临这个技术巨大变革的时代，三、五年的时间跨度对于互联网领域而言，犹如一个世纪之久，足以发生翻天覆地的变化。马云说：“我相信，三年后的电商肯定不是今天最热门的电商……重要的不是今天要赶上谁，而是想一想明天的电商应该如何提升消费体验……AI 时代刚刚到来，一切才刚开始，我们正当其时！”</p><p></p><h3>&nbsp;李彦宏内部讲话曝光：大模型开源意义不大，百度绝不抢开发者饭碗</h3><p></p><p>4 月 11 日晚间消息，在近日的一次内部讲话中，对于文心大模型开源的问题，李彦宏提到，一年前文心刚刚发布的时候，我们内部是有过非常激烈的讨论的，最后当然大家也知道这个结果，我们的决定是不开源。为什么不开源？当时的判断是，市场上一定会有开源的模型，而且是不止一家会开源。在这种情况下，多百度一家开源不多，少百度一家开源也不少。</p><p>李彦宏还表示，闭源模型在能力上会持续地领先，而不是一时地领先；模型开源也不是一个众人拾柴火焰高的情况。这跟传统的软件开源——比如 Linux、安卓等很不一样。闭源，是有真正的商业模式的，是能够赚到钱的，能够赚到钱才能聚集算力、聚集人才。闭源在成本上反而是有优势的，只要是同等能力，闭源模型的推理成本一定是更低的，响应速度一定是更快的。</p><p>此外，针对文心一言抄袭使用者，“抢饭碗”的说法，李彦宏回应，没有任何道理。“拼多多、滴滴不怕微信抢饭碗，它们的兴起都是依赖微信这个移动生态中的封闭平台，但它们各自提供了独特价值，有不同的竞争力。”李彦宏说。</p><p></p><h3>&nbsp;苹果计划裁掉超 600 人，亚马逊、英特尔等大厂亦执行“广进计划”</h3><p></p><p>本周，苹果公司宣布自 2020 年以来首次大规模裁员，计划于 5 月 27 日裁减 614 名员工，主要涉及 MicroLED 屏幕和汽车项目。此次裁员是苹果战略调整的一部分，汽车项目因方向和成本问题被取消，而 MicroLED 项目则面临工程和供应链挑战。受影响员工主要位于加州圣塔克拉拉的卫星办公室，苹果总部未受影响。</p><p>亚马逊中国部分员工据悉收到了来自总部的裁员通知邮件。流出的邮件内容显示，亚马逊称已在业务其他领域优化了团队，并发现“在项目管理、销售运营等工作类别中存在重复”，因此在特定的销售、市场营销和全球服务组织中减少数百个职位。同时，英特尔也宣布对其销售和营销部门进行了新一轮裁员。</p><p>德国云公司 Sap 宣布预计在德国裁员 2600 人。同时，Sap 还计划在欧洲其他国家进行裁员，大约 4100 个岗位将受影响。全球性电信提供商沃达丰宣布，其德国公司计划削减和转移大约 2000 个岗位。沃达丰目前在德国拥有约 15000 名员工，这意味着将有 13% 的员工受到该计划影响。</p><p></p><h3>&nbsp;“硅谷宠儿”OpenAI 迎来新投资者！“木头姐”宣布入股</h3><p></p><p>“木头姐” Cathie Wood（凯西·伍德）的方舟投资管理公司最新宣布，该公司已持有“硅谷宠儿”OpenAI 的股份。在本周四发给客户的一封电子邮件中，方舟投资表示，“方舟风险基金（Ark Venture Fund）自 2024 年 4 月 10 日起投资 OpenAI，OpenAI 处于人工智能大爆发的最前沿。”在 4 月 10 日更新后，方舟风险基金的官网上出现了 OpenAI 的身影，不过该基金尚未透露投资规模。</p><p>凯西·伍德投资 OpenAI 的这一步棋或许意味着方舟投资公司正在寻求一条“出路”。该公司旗下最著名的投资工具 Ark Innovation ETF 在疫情期间因大举投资特斯拉等公司而声名鹊起，而随着今年特斯拉股价受挫之后，这只 ETF 也开始步履蹒跚。</p><p></p><h3>&nbsp;腾讯云突然崩了！网友表示：“一堆客户炸了… ”，原因是云 API 异常</h3><p></p><p>4 月 8 日下午，有大量网友反馈，称腾讯云出现服务故障，接口响应报错、网页显示 504 错误。从网友反馈的时间来看，此次腾讯云崩溃的时间大约在 15:20 左右。有网友表示：“一堆客户炸了，好歹先给个故障原因啊… ”“我的对象存储完全不能用了！！！！”对此，腾讯云官方发布公告表示，腾讯云官网控制台相关服务出现异常，工程师正在紧急修复中，非常抱歉对您造成的影响，若您有任何问题，请随时联系我们，感谢您的理解与支持。</p><p>此次故障一共持续了近 87 分钟，期间共有 1957 个客户报障。经过故障定位发现，客户登录不上控制台正是由云 API 异常所导致。云 API 是云上统一的开放接口集合，客户可以通过 API 以编程方式管理和操控云端资源，云控制台通过组合云 API 提供交互式的网页功能。故障发生后，依赖云 API 提供产品能力的部分公有云服务，也因为云 API 的异常出现了无法使用的情况，比如云函数、文字识别、微服务平台、音频内容安全、验证码等。</p><p>故障的原因是云 API 服务新版本向前兼容性考虑不够和配置数据灰度机制不足的问题。腾讯云本次 API 升级过程中，由于新版本的接口协议发生了变化，在后台发布新版本之后对于旧版本前端传来的数据处理逻辑异常，导致生成了一条错误的配置数据，由于灰度机制不足导致异常数据快速扩散到了全网地域，造成整体 API 使用异常。</p><p>发生故障后，按照标准回滚方案将服务后台和配置数据同时回滚到旧版本，并重启 API 后台服务，但此时因为承载 API 服务的容器平台也依赖 API 服务才能提供调度能力，即发生了循环依赖，导致服务无法自动拉起。通过运维手工启动方式才使 API 服务重启，完成整个故障恢复。</p><p></p><h3>&nbsp;阿里云卷到海外，核心云产品全线降价 23%，最高降幅 59%</h3><p></p><p>4 月 8 日，继一个月前中国区全线降价后，阿里云开始卷到海外：海外市场全线降价，覆盖全球 13 个地域节点部署的核心云产品、500 多个产品规格，平均降幅 23%，最高降幅 59%。降价后，阿里云海外市场云产品价格全面低于其他国际主流云厂商。</p><p>相对于一个月前的降价，此次价格调整主要面向海外市场，降价的 13 个地域节点包括马来西亚、印尼、新加坡、菲律宾、日本、韩国、泰国、美国（东岸和西岸）、德国、英国、阿联酋、中国香港。目前，阿里云在全球 200 多个国家服务 500 多万客户。据 Gartner 数据，2022 年阿里云市场份额排名全球第三、亚太第一。</p><p></p><h1>IT 业界</h1><p></p><p></p><h3>&nbsp;Meta 确认开源大模型 LLaMA 3 下月登场，参数量或超 1400 亿</h3><p></p><p>在 4 月 9 日伦敦举行的一次活动中，Meta 确认计划在下个月内首次发布 LLaMA 3。据了解，该模型将有多个具有不同功能的版本。但 Meta 并没有披露 LLaMA 3 的参数规模。“随着时间的推移，我们的目标是让由 LLaMA 驱动的 Meta AI 成为世界上最有用的助手。”Meta 人工智能研究副总裁 Joelle Pineau（ 乔埃勒·皮诺）说。“要达到这个目标，还有相当多的工作要做。”</p><p>据科技外媒 4 月 8 日发布的报道，作为对标 GPT-4 的大模型，LLaMA 3 的大规模版本参数量可能超过 1400 亿，此前最大的 LLaMA 2 版本的参数量为 700 亿。LLaMA 3 将支持多模态处理，即同时理解和生成文本及图片。值得注意的是，LLaMA 3 将延续 Meta 一直以来的开源路线。</p><p></p><h3>&nbsp;PHP 的辉煌时代似乎已经过去了？在 TIOBE 4 月榜单中跌至历史最低点</h3><p></p><p>本月，PHP 在 TIOBE 指数中的排名跌至历史最低点（第 17 位）。TIOBE CEO Paul Jansen（保罗·扬森）指出，在 TIOBE 指数于 2001 年开始发布时，PHP 正在即将成为构建交互式网站的标准语言。因此 PHP 的受欢迎程度逐年上升，并最终获得了超过 10% 的市场份额，甚至曾斩获过 TIOBE 指数前三的位置。</p><p>“然而，此后随着众多竞争对手进入市场；譬如 Rails、Django 和 React 等 Web 开发框架采用了 Ruby、Python 和 JavaScript 作为主要驱动语言。与此同时，PHP 中还出现了一些安全问题。结果，PHP 不得不重塑自己。如今，PHP 仍在中小型网站领域占有一席之地，它也是最流行的 Web 内容管理系统 WordPress 背后的语言。所以，PHP 当然没有消失，但它的辉煌时代似乎已经过去了。”</p><p></p><h3>&nbsp;10 秒总结 YouTube 视频，原阿里首席 AI 科学家贾扬清打造 Elmo</h3><p></p><p>4 月 10 日消息，原阿里首席 AI 科学家贾扬清在 X 上分享了插件 Elmo，该插件能在 10 秒内总结 Google Next 主题演讲，生成一句话概括、摘要、主要观点。该插件由贾扬清去年创办的 AI 公司 Lepton AI 打造。</p><p>贾扬清表示，Elmo 采用了数据公司 Databricks 推出的开源大模型 DBRX 。据悉，DBRX 具有 1320 亿个参数，采用 MoE 架构，在性能上超过了 GPT-3.5 和其他一些开源模型。</p><p></p><h3>&nbsp;全新的音乐生成应用 Udio 正式亮相，比 Suno 更强大，效果直逼人类</h3><p></p><p>全新音乐生成应用 Udio 正式亮相，利用先进 AI 技术，通过文字输入生成多风格音乐作品，支持多语言，用户体验革命性提升。相较于 Suno，Udio 在音乐生成效果上有质的提升，可生成从引子到尾声的长音乐作品，并支持社区分享；Udio 由谷歌 DeepMind 等顶尖 AI 研究机构出身团队创立，目前处于公测阶段，每月可免费生成 1200 首作品。</p><p></p><h3>&nbsp;超越 GPT-4，斯坦福团队手机可跑的大模型火了，一夜下载量超 2k</h3><p></p><p>近日，斯坦福大学研究人员推出的 Octopus v2 火了，受到了开发者社区的极大关注，模型一夜下载量超 2k 。20 亿参数的 Octopus v2 可以在智能手机、汽车、个人电脑等端侧运行，在准确性和延迟方面超越了 GPT-4，并将上下文长度减少了 95%。此外，Octopus v2 比 Llama7B + RAG 方案快 36 倍。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/NIo1UEAyMItYZS13osUO</id>
            <title>确定性运维受邀出席QCon2024 分享LLM和Multi-agent在运维领域的创新实践</title>
            <link>https://www.infoq.cn/article/NIo1UEAyMItYZS13osUO</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NIo1UEAyMItYZS13osUO</guid>
            <pubDate></pubDate>
            <updated>Sat, 13 Apr 2024 06:56:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: InfoQ, QCon全球软件开发大会, 智能运维, 大模型
<br>
<br>
总结: 2024年4月13日，由InfoQ主办的行业技术盛会——QCon全球软件开发大会2024北京站正式开启，华为云SRE AI使能专家张曦博士受邀出席智能运维大模型专题大会，和技术爱好者分享确定性运维在LLM和Multi-agent在运维领域的实验探索经验。在数字化时代，企业和组织的IT系统变得越来越复杂，运维工作也变得越来越繁琐和困难。为了解决这些问题，AIOps智能运维技术应运而生，而大模型的出现，为AIOps更强大的计算、决策与自学能力，极大地提升了IT运营的自动化和效率。张曦博士从智能运维面临的挑战和痛点出发，介绍在企业运维领域应用AIGC的实践案例，基于确定性运维的实践经验，提出以LLM为中心，基于多Agent协同的运维方案，并提出在大模型时代下，对下一代智能运维的思考。 </div>
                        <hr>
                    
                    <p></p><blockquote>【摘要】 2024年4月13日，由InfoQ主办的行业技术盛会——QCon全球软件开发大会2024北京站正式开启，华为云SRE AI使能专家张曦博士受邀出席智能运维大模型专题大会，和技术爱好者分享确定性运维在LLM和Multi-agent在运维领域的实验探索经验。</blockquote><p></p><p></p><p>2024年4月13日，由InfoQ主办的行业技术盛会——QCon全球软件开发大会2024北京站正式开启，华为云SRE AI使能专家张曦博士受邀出席智能运维大模型专题大会，和技术爱好者分享确定性运维在LLM和Multi-agent在运维领域的实验探索经验。</p><p></p><p>在数字化时代，企业和组织的IT系统变得越来越复杂，运维工作也变得越来越繁琐和困难。为了解决这些问题，AIOps智能运维技术应运而生，而大模型的出现，为AIOps更强大的计算、决策与自学能力，极大地提升了IT运营的自动化和效率。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/2c/2cd6590c36108e3c011d8399c46f7a5d.jpeg" /></p><p></p><p>张曦博士从智能运维面临的挑战和痛点出发，介绍在企业运维领域应用AIGC的实践案例，基于确定性运维的实践经验，提出以LLM为中心，基于多Agent协同的运维方案，并提出在大模型时代下，对下一代智能运维的思考。</p><p></p><p>大模型给企业运维带来新挑战。大模型时代，传统智能运维方案面临着很多痛点，包括扩展性有限、手动维护自动运维规则、无法有效利用专家经验和领域知识、人机交互不友好等。异常检测是智能运维的关键起点，构建面向多模态多源运维数据的异常检测基础模型，针对Metric数据，通过分组聚合、多维度时序特征提取、时序融合、时序聚类等技术实现高效异常检测。</p><p></p><p>基于大模型和多Agent相结合的运维方案。基于多Agent协同的编排调度，实现更全能的多模态数据异常检测基础模型, 结合多Agent协同完成运维主流程，异常检测-&gt;根因定位-&gt;故障分析-&gt;修复建议，且框架与算法不依赖具体特定应用场景。结合大模型实现较强的泛化能力，我们通过多个子领域agent协同工作，实现运维故障自动诊断和多个任务模型的编排，提升运维效率。</p><p></p><p>&nbsp;大模型时代下智能运维演讲趋势展望。多Agent协同给智能运维带来的变化，实现真正的模块可插拔，由Agent自主讨论决策运维动作，选择对应运维工具，参与聊天或者代替主管Agent发布指令，通过大模型的强大能力，实现更高效、更主动、更直观的运维工作。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/e9/e94103cdd91c627b7c176e3cf556f042.jpeg" /></p><p>&nbsp;确定性运维与大模型 构建稳定可靠的数字化场景</p><p></p><p>稳定可靠是企业的“生命线”，基于内部实践的“确定性运维”能力体系，华为云与业界同行积极开展互动，吸取各家云上客户意见，梳理出一套能力成熟度模型，给更多处于数字化转型期的企业参考，梳理和识别痛点/短板，制定自身的运维变革目标和转型措施。面向企业构建运维体系和能力，帮助企业持续提升系统可用性，协助客户完成运维变革，实现从“基本运维”能力迈向“确定性运维”能力的转变。</p><p></p><p>面对大模型等技术的发展，推动企业共建开放生态的合作。华为云构筑开面向全球客户，推出华为云维享会（确定性运维经验交流分享会），未来维享会将举行多种形式的交流活动，与会员共论业务上云后的管理之道，联合会员共创，编写专刊、白皮书和案例集等内容，碰撞行业前沿资讯，加快业务创新。</p><p></p><p>在未来，面对运维大模型，未来趋势将是以自动化、智能化、可视化和平台化为核心，通过确定性运维体系及实践经验，结合大模型提供智能化的决策，支持和自动化的执行能力，助力提升系统的稳定性、可用性和性能，为企业的业务发展提供有力保障，推动企业运维组织变革，加速数字化转型。</p><p></p><p>张曦博士简介：犹他州立大学统计学博士，研究方向为 AI for Data、AI for BI、AIOps，时间序列分析等；具有丰富的人工智能在企业场景落地应用的成功经验，应用场景覆盖营、销、服、供、采、制、研发等多领域，支撑华为集团多个业务应用 +AI，带领团队成功攻克 5+ 企业技术难题，并主导发布多个 AI 服务。</p><p></p><p>原文链接：<a href="https://bbs.huaweicloud.com/blogs/425612">https://bbs.huaweicloud.com/blogs/425612</a>"</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/OGb6vlQobPORvmGaFUJa</id>
            <title>都2024年了，美国地铁还在“死磕”软盘和100年前架构，网友：不上云更安全</title>
            <link>https://www.infoq.cn/article/OGb6vlQobPORvmGaFUJa</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/OGb6vlQobPORvmGaFUJa</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Apr 2024 09:52:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AIGC, 旧金山地铁, 纽约地铁, 软盘技术
<br>
<br>
总结: 在AIGC全面爆发的今天，美国旧金山地铁仍然沿用着已经被淘汰的软盘技术，而纽约地铁还被困在100年前的IT架构上。AIGC以其强大的创造力和学习能力，正在全球范围内掀起一场技术革命，而这场技术革命背后，离不开先进的计算机硬件和高效的软件系统支持。但令人遗憾的是，在这股技术狂潮的席卷之下，一些机构却仍在使用老旧的计算机设备和落后的软件技术来迎接瞬息万变的未来。 </div>
                        <hr>
                    
                    <p></p><blockquote>在AIGC全面爆发的今天，美国旧金山地铁仍然沿用着已经被淘汰的软盘技术，而纽约地铁还被困在100年前的IT架构上。</blockquote><p></p><p>&nbsp;</p><p>AIGC以其强大的创造力和学习能力，正在全球范围内掀起一场技术革命，而这场技术革命背后，离不开先进的计算机硬件和高效的软件系统支持。但令人遗憾的是，在这股技术狂潮的席卷之下，一些机构却仍在使用老旧的计算机设备和落后的软件技术来迎接瞬息万变的未来。</p><p>&nbsp;</p><p>近日，旧金山交通局的列车系统就因“全手动操作且仍继续遗留着几十岁‘高龄’的软盘组件”而引发关注。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/5e/5ef703248f723804cdebfe8c4195a666.jpeg" /></p><p></p><p>身在引领IT技术发展的硅谷，旧金山的列车控制系统居然依靠软盘为载体保持运行……这怎么可能？当地乘客Katie Guillen惊讶表示，“啊？我还以为我们已经步入AI时代，结果还在使用软盘？”</p><p></p><h2>旧金山“老古董”列车控制系统引发热议，没有软盘走不了</h2><p></p><p>&nbsp;</p><p>负责运营当地地铁轻轨系统的旧金山交通局（SFMTA）号称是全美第一家使用软盘介质的机构。但现如今，交通局方面正急于放弃对5英寸软盘的依赖，前提是……给他们十年左右时间再加上数亿美元投资。</p><p>&nbsp;</p><p>为了让公众了解更详细的信息，旧金山交通局几位工作人员最近接受了ABC7湾区新闻的采访，具体介绍了该机构每天早上如何使用3张5英寸软盘启动列车控制系统。</p><p>&nbsp;</p><p>自1998年被安装在市场街地铁站以来，这些软盘一直成为Muni Metro旧金山地铁自动列车控制系统（ATCS）的重要组成部分。如今26年过去，交通局的工作人员每天早上仍在依靠软盘来指挥列车如何运行。</p><p>&nbsp;</p><p>虽然现在来看这套系统已经“老掉牙了”，把时钟拨回旧金山交通局部署这套自动列车控制系统的1998年，其使用的确实是当初最前沿的技术成果。</p><p>&nbsp;</p><p>来自交通局列车控制项目组的Mariana Mauire解释称，“我们是全美首家采用这项特别技术的机构，那个时候计算机甚至还没有磁盘驱动器，必须通过软盘将软件加载到计算机上。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/ea0afe5ffd0f44736e38bf4c3d248e9f.jpeg" /></p><p></p><p>当记者Luz Pena问到 “软盘在列车控制系统中起着怎样的作用？”时，Mariana Maguire回答称： “软盘属于整个系统的组成部分，这套系统负责自动控制地铁内的列车。我们在全市范围内运营的地铁系统包含大量依靠软盘运行的组件。”</p><p>&nbsp;</p><p>也就是说，旧金山交通局的列车控制系统每天早上必须要借助5英寸软盘方可正常启动。</p><p>&nbsp;</p><p>另一位交通局发言人Michael Roccaforte详细解释了该系统的运行原理。该控制系统中包含多个组件，包括与推进/制动系统、中央及本地服务器，外加环路信号线缆等通信基础设施相对接的车载计算机。这些软盘的主要作用是加载运行中央服务器的软件。Roccaforte指出：</p><p>&nbsp;</p><p></p><blockquote>当列车驶入地铁站时，车载计算机会接入列车控制系统，以自动模式驾驶列车，保证车辆在操作员的监督下自行运转。而在驶离地铁站时，车辆会断开与控制系统的连接并返回手动操作模式。</blockquote><p></p><p>&nbsp;</p><p>Mariana Mauire指出，“整个系统在晚间关闭后就如同失忆了一般。到第二天早上，就得有人重新提醒它「你是谁，你今天需要达成的运行目标是什么」。”</p><p>&nbsp;</p><p>如此“智障”的旧系统为什么没有被换掉？为什么不把软盘升级成无线传输系统？</p><p>&nbsp;</p><p>该局交通总监Jeffrey Tumlin在采访中表示，“这会带来新的风险。系统目前运行良好，我们当然也知道随着时间推移，软盘数据退化的风险也在不断增加，甚至随时可能引发灾难性故障。”</p><p>&nbsp;</p><p>Roccaforte表示，对列车控制系统开展全面改造的初步计划（包括取消软盘）早在2018年就已经开始，预计从初步规划到最终完成需要十年时间。由于新冠疫情爆发造成长达18个月的进度中断，预计实际完工时间将延后至2029年至2030年。旧金山交通局预计在2025年初确定承包商，届时将发布详尽的项目时间表。</p><p></p><h2>系统升级需要十年时间，花费数亿美元</h2><p></p><p>&nbsp;</p><p>常言道“只要还没坏，那就尽管用。”可虽然软盘列车控制系统目前仍能正常运行，但继续依赖过时技术仍存在巨大隐患。旧金山交通局多年来也一直在强调这个问题。</p><p>&nbsp;</p><p>交通局方面表示，这套列车控制系统的设计使用寿命仅为20到25年，也就是说从2023年之后已经属于计划外使用周期。据称由地方及国家交通专家组成的市政可靠性工作组曾于2020年提出，建议在五到七年内建立新的交通控制系统。</p><p>&nbsp;</p><p>在被问及对现有软盘系统进行升级有多“迫切”时，Tumlin表示问题的关键在于风险。</p><p>&nbsp;</p><p>此前，旧金山交通局就曾表示随着时间推移，列车控制系统的维护正变得愈发困难且昂贵。他们还承认，为这类过时系统寻找技术人员的难度也越来越高。</p><p>&nbsp;</p><p>Tumlin在去年接受采访时坦言，“我们必须留住精通90年代编程语言的程序员，才能保证这套系统继续正常运行，就是说我们的技术债务可以追溯到几十年前。”</p><p>&nbsp;</p><p>2020年，一位部门发言人向《旧金山纪事报》证明，当时交通局交管员的本科生占比为40%到50%。</p><p>&nbsp;</p><p>在被问及放弃软盘系统是否会导致裁员时，Roccaforte回应称：</p><p>&nbsp;</p><p></p><blockquote>随着新型列车控制系统的上线，现有员工仍有大量岗位可以选择，并接受相应的技术培训。我们升级项目战略中的一大关键，就是培养内部技能并对现有员工开展培训。此外，我们还需要聘请信号工程师等更多技术人才，以协助支持新的列车控制系统。</blockquote><p></p><p>&nbsp;</p><p>2020年，Tumoin在接受《旧金山纪事报》采访时指出，他早在2007年就得知该系统需要更新，但承认系统本身并不存在“迫在眉睫的升级需求”。</p><p>&nbsp;</p><p>“虽然仍然依靠从5英寸软盘加载的DOS系统运行，但整个体系的确运行良好。”</p><p>&nbsp;</p><p>旧金山交通局发言人Mariana Maguire上周在接受ABC7采访时表示，升级项目将使得列车控制系统“在自动驾驶技术的帮助下轻松跟踪全城列车的运行和移动，同时增强人为干预能力。”</p><p>&nbsp;</p><p>然而，预算挑战导致项目的预定时间表遭受质疑。Roccaforte表示，交通局的列车升级项目不仅涉及软盘迁移，还需要“对当前列车控制系统及其所有组件进行全面检修，包括车载计算机、中央与本地服务器以及通信基础设施。”</p><p>&nbsp;</p><p>比陈旧软盘系统更重要的是环路线缆系统，负责在中央服务器与列车之间传输数据。根据Roccaforte的介绍，“其带宽甚至还不及早期AOL拨号调制解调器。”</p><p>&nbsp;</p><p>旧金山交通局在其官方网站上补充称：</p><p>&nbsp;</p><p></p><blockquote>环路线缆脆弱且容易受到干扰，导致地铁维护变得愈发困难。这意味着该系统无法沿地面轻轨延伸至地铁站之外，因此在地面环境下仍未实现自动列车控制。</blockquote><p></p><p>&nbsp;</p><p>Roccaforte还提到，交通局正计划升级至“现代通信技术，例如光纤或Wi-Fi”。</p><p>&nbsp;</p><p>Tumlin强调，交通局希望能由州和联邦政府拨款承担列车控制系统升级预算中的“很大一部分”，而“其余部分则由本市市政铁路正迅速减少的内部资金消化。”交通局拒绝透露截至目前已经在系统更新上花掉的费用。</p><p>&nbsp;</p><p>旧金山交通局不单自身多年来一直依赖软盘运行，同时还与其他采用软盘存储的机构保持长期合作，包括货运航空公司以及提供定制刺绣的纺织供应商。</p><p></p><h2>饱受诟病的老旧IT系统为何难以替换？</h2><p></p><p>&nbsp;</p><p>美国一些地方的老旧IT系统饱受诟病已经不是什么新鲜事。前几年，就有媒体曝出全球贸易中心纽约的地铁基本上天天延误。因为纽约市地铁系统采用的是二战前的技术。</p><p>&nbsp;</p><p>导致延误发生的原因是控制列车的通信系统过于老旧，但即便是这样的老旧通信系统，将其安装在一条地铁线上也要花6年时间和 2.88 亿美元。</p><p></p><p><img src="https://static001.geekbang.org/infoq/38/383652f6b5b91da31e9f3df486a4ebde.png" /></p><p></p><p>在西四街车站，大都会交通管理局员工手动记录列车运行情况。</p><p>&nbsp;</p><p>大都会交通管理局员工在接受Business Insider采访时称：“在我们的系统中，不仅仅具有 100 年历史的架构，还有很多古老的基础技术。”</p><p>&nbsp;</p><p>该员工还表示要将纽约地铁上拥有百年历史的信号灯、手动控制开关替换掉并且升级更新老化系统，需要花费近10年的时间以及付出200亿美元的代价。</p><p>&nbsp;</p><p>一位居住在旧金山的ID为iancmceachern的Hacker News用户表示，这种情况在机床领域（铣床、车床等）中比较常见。自己曾经给交通局发了电子邮件诉说过对于软盘问题的担忧，交通局回复他称：“软盘事件是个值得关注的问题，但其实这只是冰山一角。整个老旧系统中每一层都需要更换”。</p><p>&nbsp;</p><p>ID为jandrese的用户这些老旧系统存在很大的隐患，软盘算是比较常见的古董零部件了，如果摊上其他冷门的硬件则会更麻烦。他称：</p><p>&nbsp;</p><p></p><blockquote>由于复古计算社区的努力，软盘模拟器到处都是。但是，对于那些已经破产的公司的定制板、不透明的ROM芯片、PLA等设备的模拟器，一旦出了问题挑战将会大得多。&nbsp;如果他们有所有部件的良好电路图，可能可以通过几个熟悉电路和烙铁的聪明电子工程师来维持系统的长期运行，但最终他们可能会因某个冷门的零件用完，陷入困境。</blockquote><p></p><p>&nbsp;</p><p>值得一提的是，一些人认为尽管更新周期很长，但更新周期是在合理范围内的。比如1996年投入使用的Breda列车在使用了大约20年后开始逐步淘汰，大多数人普遍认为这是一个合理的时间去做一些更新。经历过5.25英寸软盘时代的人们或许能够理解，当系统完成了它的使命时，更换是合理的。到那时，需要更换的不仅仅是存储介质，而在此之前它能够正常运行也无需更换。</p><p>&nbsp;</p><p>ID名为Workaccount2的Hacker News用户认同上述观点。他们公司为另一个全球大城市的基础设施提供新的计算模块，这些基础设施仍然依赖于20世纪80年代初的英特尔CPU。也就是说，他们公司正在为装有超过40年历史的芯片的机构安装新电路板。</p><p>&nbsp;</p><p>Workaccount2表示：“他们对更新系统没有表现出任何兴趣。这个系统运行正常，他们可以获得服务，也可以为损坏的部件获得新的替代品。”</p><p>&nbsp;</p><p></p><blockquote>但客户可能不知道的是，基本上只有一个我们的工程师（可能是地球上唯一一个）知道如何修理这些东西。他已经年纪很大了，显然年轻的工程师根本没有兴趣学习这些古老被遗忘的系统。</blockquote><p></p><p>&nbsp;</p><p>尽管引发了广泛议论，但有一些网友认为交通局至今仍使用软盘的行为是有道理的，因为比起上云，本地软盘更安全。</p><p>&nbsp;</p><p>ID名为Zuu47的用户则表示，人们没有意识到云上的新系统可能会被黑客攻击，使用软盘更安全。</p><p>&nbsp;</p><p>总结下来，这些安装在机床领域的老旧IT系统之所以难以替换，无非有三点原因：第一，系统还能用，没到必须要更换的程度；第二，更换成本太高了，动辄数亿美元；第三，牵一发而动全身，工程量太过庞大，需要耗时许多年。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href="https://petapixel.com/2024/04/09/san-franciscos-train-system-is-still-running-on-floppy-disks/">https://petapixel.com/2024/04/09/san-franciscos-train-system-is-still-running-on-floppy-disks/</a>"</p><p><a href="https://arstechnica.com/gadgets/2024/04/5-25-inch-floppy-disks-expected-to-help-run-san-francisco-trains-until-2030/">https://arstechnica.com/gadgets/2024/04/5-25-inch-floppy-disks-expected-to-help-run-san-francisco-trains-until-2030/</a>"</p><p><a href="https://www.businessinsider.com/nyc-mta-subway-delay-2017-6">https://www.businessinsider.com/nyc-mta-subway-delay-2017-6</a>"</p><p></p><h2>&nbsp;</h2><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/5qjbPBwZGoFVeoMQmw7z</id>
            <title>我在技​​术面试中用ChatGPT作弊，没人知道</title>
            <link>https://www.infoq.cn/article/5qjbPBwZGoFVeoMQmw7z</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/5qjbPBwZGoFVeoMQmw7z</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Apr 2024 09:26:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: ChatGPT, 面试作弊, 面试问题, 实验准备
<br>
<br>
总结: ChatGPT已经改变了人们的工作方式，但在技术面试中可能会引发作弊问题。通过招募专业面试官和用户进行实验，发现公司需要修改面试问题类型以防止作弊。 </div>
                        <hr>
                    
                    <p>众所周知，ChatGPT已经彻底改变了人们的工作方式。它既能帮助小型企业自动化管理任务，又能为Web开发人员编写整个React组件，它的作用可以说怎么夸都不过分。</p><p></p><p>在interviewing.io，我们一直在思考ChatGPT将给技术面试带来什么变化。一个很大的问题是：ChatGPT会让面试作弊变得很简单吗？在TikTok上的一个视频中，一名工程师让ChatGPT准确地回答面试官的问题：</p><p></p><p><img src="https://static001.geekbang.org/infoq/a1/a15ff78c1607ab081c29164385612040.png" /></p><p></p><p></p><p>人们最初对这类作弊软件的反应和预期完全一致：</p><p></p><p>Redditor说，“<a href="https://www.reddit.com/r/singularity/comments/12zyela/chatgpt_spells_the_end_of_coding_as_we_know_it/">众所周知，ChatGPT是编码的终结。</a>"“YouTuber说，“<a href="https://www.youtube.com/watch?v=OeebS-VcSH0">软件工程已死，ChatGPT杀死了它。</a>"”X（之前的Twitter）发出疑问，“<a href="https://twitter.com/intx_podcast/status/1635396953109561344">​​ChatGPT意味着编码面试的终结吗？</a>"”</p><p></p><p>ChatGPT可以在面试过程中为人提供帮助，这似乎很明显，但我们想知道的是：</p><p></p><p>它能在多大程度上提供帮助？作弊（并逃脱惩罚）有多容易？使用LeetCode问题的公司需要对面试过程做出重大改变吗？</p><p></p><p>为了回答这些问题，我们招募了一些专业面试官和用户来进行作弊实验！下面，我们将分享我们发现的一切。稍微剧透一下，有一点你要知道：公司需要修改面试问题的类型，而且是马上！</p><p></p><p></p><h2>实验准备</h2><p></p><p></p><p>interviewing.io是一个面向工程师的面试实践平台和招聘市场。工程师借助我们的平台来模拟面试。企业利用我们的平台招聘优秀的员工。我们的生态系统中有成千上万的专业面试官，也有成千上万的工程师使用我们的平台准备面试。</p><p></p><p></p><h3>面试官</h3><p></p><p></p><p>面试官来自我们的专业面试官池。他们被分成三组，每组问不同类型的问题。面试官不知道这个实验是关于ChatGPT或作弊的；我们告诉他们，“这项研究的目的是了解面试官的决策随时间变化的趋势，尤其是在问标准和非标准面试题的时候。”</p><p></p><p>以下是3种问题类型：</p><p></p><p>LeetCode原题：面试官根据自己的判断直接从LeetCode中选取的题目，没有做任何修改。</p><p></p><p>例如：一字不差地问LeetCode上的<a href="https://leetcode.com/problems/sort-colors/">Sort Colors</a>"问题。</p><p></p><p>改良LeetCode问题：对从LeetCode上获得的问题做一些修改，虽然与原题类似，但也有明显的不同。</p><p></p><p>例如：对于上面的<a href="https://leetcode.com/problems/sort-colors/">Sort Colors</a>"问题，将输入从3个整数(0,1,2)改为4个整数(0,1,2,3)。</p><p></p><p>自定义问题：所提的问题和网络上已有的任何问题之间都不存在直接的联系。</p><p></p><p>例如：给你一个日志文件，格式如下：- :  -  -，你的任务是识别会话中代表参与度中值的用户。只考虑贡献分数大于50%的用户。假设这类用户的数量是奇数，那么你需要按贡献分数排序后找到位于中间的那个用户。对于下面的文件，正确的答案是SyntaxSorcerer。</p><p><code lang="null">LOG FILE START

NullPointerNinja: "who's going to the event tomorrow night?" - 100%

LambdaLancer: "wat?" - 5%

NullPointerNinja: "the event which is on 123 avenue!" - 100%

SyntaxSorcerer: "I'm coming! I'll bring chips!" - 80%

SyntaxSorcerer: "and something to drink!" - 80%

LambdaLancer: "I can't make it" - 25%

LambdaLancer: "🙁" - 25%

LambdaLancer: "I really wanted to come too!" - 25%

BitwiseBard: "I'll be there!" - 25%

CodeMystic: "me too and I'll brink some dip" - 75%

LOG FILE END</code></p><p></p><p>更多关于问题类型和实验设计的信息，可以阅读<a href="https://docs.google.com/document/u/0/d/1UdWZHUQfeLR8oUiNY4JfwgES42HTlAQL5z_VfQJPPKk/edit">面试官实验指南文档</a>"：</p><p><a href="https://docs.google.com/document/u/0/d/1UdWZHUQfeLR8oUiNY4JfwgES42HTlAQL5z_VfQJPPKk/edit">https://docs.google.com/document/u/0/d/1UdWZHUQfeLR8oUiNY4JfwgES42HTlAQL5z_VfQJPPKk/edit</a>"</p><p></p><p></p><h3>面试者</h3><p></p><p></p><p>面试者来自我们的活跃用户池，我们邀请他们参加一个简短的调查。我们的选择标准如下：</p><p></p><p>在当下的市场上积极地找工作；有4年以上的工作经验，正在申请高级职位；他们对“ChatGPT编码”的熟悉程度为中等或高等；认为自己可以在面试中作弊而不被发现。</p><p></p><p>这种选法可以帮助我们选出那些可能会在面试中作弊的求职者。他们有这样做的动机，并且已经相当熟悉ChatGPT和编码面试。</p><p></p><p>我们告诉面试者，他们在面试中必须使用ChatGPT，目的是测试他们使用ChatGPT作弊的能力。他们还告诉他们，不要尝试凭借自己的技能通过面试，主要要依靠ChatGPT。</p><p></p><p>我们总共进行了37场面试，其中32场有效（我们不得不去掉了5场，因为参与者没有按要求进行）：</p><p></p><p>11场采用“LeetCode原题”9场采用“改良LeetCode问题”12场采用“自定义问题”</p><p></p><p>说明：因为我们平台允许匿名，所以我们的面试只有音频没有视频。匿名是为了帮用户创造一个安全空间，让他们可以快速失败并学习，而没有人会对他们做评判。对用户来说，这是件好事。但我们承认，没有采访视频会让我们的实验变得不那么真实。在真正的面试中，你会面对镜头，这让作弊变得更加困难——但并不能消除作弊。</p><p></p><p>面试结束后，面试官和面试者都要完成一份退场调查。我们问面试者在面试中使用ChatGPT时遇到的困难，而对于面试官，我们问他们对面试的担忧——我们想看看有多少面试官会将他们的面试标记为有问题，并报告他们怀疑存在作弊行为的面试。</p><p></p><p><img src="https://static001.geekbang.org/infoq/91/91cac2646a7a5ea36736d4e1ef66ab3d.jpeg" /></p><p>﻿后续调查：面试者问题</p><p></p><p><img src="https://static001.geekbang.org/infoq/f6/f61e78e895b15944ec8c9e8c1eab0b77.jpeg" /></p><p>﻿后续调查：面试官问题</p><p></p><p>我们不知道实验中会发生什么，但假如有一半作弊的求职者成功通过面试，那么对于我们行业来说，那将是一个很能说明问题的结果。</p><p></p><p></p><h2>实验结果</h2><p></p><p></p><p>在剔除了参与者没有按要求进行的面试后，我们得到了以下结果。我们的对照组是求职者在interviewing.io模拟面试中的表现，来自本次实验之外，通过人数占53%。需要注意的是，我们平台上大多数的模拟面试采用的都是LeetCode风格的问题，这是有道理的，因为FAANG公司主要问的就是这些问题。我们一会儿再回来讨论这个问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c4/c41b4a815a594c2acadfd47ee2fffcbe.png" /></p><p>与平台平均值和“自定义”问题相比，“原题”的通过率要高得多。“原题”和“改良”问题的差异无统计学意义。“自定义”问题的通过率明显低于其他任何一组。</p><p></p><p></p><h3>回答原题，表现最好</h3><p></p><p></p><p>不出所料，使用原题的那一组表现最好，73%通过了面试。面试者反映，他们从ChatGPT得到了完美的解决方案。</p><p></p><p>以下是对这一组做面试后调查时得到的最值得注意的评论——我们认为它特别能说明许多面试官的想法：</p><p></p><p></p><blockquote>应聘者之所以能够轻松回答这个问题，很难判断是因为他们真的很好，还是因为他们以前听说过这个问题。通常情况下，为了区分这两种情况，我会对问题做一两处修改。</blockquote><p></p><p></p><p>通常情况下，为了获得更多的信息，面试官会跟进问一个改良过的问题。所以让我们看看采用“改良问题”的那一组，看看面试官是否真的通过对问题做一两处修改获得了更多的信息。</p><p></p><p></p><h3>回答改良问题，要更多提示</h3><p></p><p></p><p>请注意，这个组拿到的是一个标准的LeetCode问题，但他们用无法从网上直接找到的方式对其做了修改。也就是说，ChatGPT不可能有这个问题的答案。因此，面试者更依赖于ChatGPT实际解决问题的能力，而不是它背诵LeetCode教程的能力。</p><p></p><p>不出所料，这一组的结果与“原题”组没有太大区别，67%的求职者通过了面试。</p><p></p><p>事实证明，这种差异与“原题”组没有统计学上的显著差异，即“改良问题”和“原题”本质上是相同的。这个结果说明，ChatGPT可以处理面试官对问题的微调，这种微调并不会给它带来多少麻烦。</p><p></p><p>然而，面试者确实也注意到，让ChatGPT解决经过修改的问题需要提供更多的提示。有一位面试者是这样说的：</p><p></p><p></p><blockquote>回答直接来自LeetCode的问题完全没有问题。让ChatGPT回答一个不那么直接的LeetCode风格的后续问题难度会增加很多。</blockquote><p></p><p></p><p></p><h3>自定义问题，通过率最低</h3><p></p><p></p><p>不出所料，“自定义”问题组的通过率最低，只有25%的面试者通过。它在统计上不仅明显小于其他两个实验组，而且明显低于对照组！当你问求职者完全自定义的问题时，他们的表现会比没有作弊（或被问到LeetCode风格的问题）时差！</p><p></p><p>需要说明的是，这个数值在最初计算时略高，在详细检查了自定义问题之后，我们发现了一个意料之外的问题。“企业应立即改变所提的问题！”一节说明了问题所在。</p><p></p><p></p><h2>没有人被抓到作弊</h2><p></p><p></p><p>在我们的实验中，面试官没有意识到面试者被要求作弊。上文说过，在每次面试后，我们会让面试官完成一项调查，他们必须描述自己对求职者的评估有多自信。</p><p></p><p>面试官对自己所做评估的正确性很有信心，72%的人说他们对自己的招聘决定有信心。一位面试官对面试者的表现非常之满意，以至于得出结论，应该邀请这些人成为平台的面试官！</p><p></p><p></p><blockquote>求职者表现非常出色，并且非常了解功能强大的Amazon L6 (Google L5) SWE……应该考虑让他们担任interviewing.io的面试官/导师。</blockquote><p></p><p></p><p>仅仅经过一次面试就做出这样的判断，这可能过于自信了！</p><p></p><p>我们早就知道，<a href="https://interviewing.io/blog/own-interview-performance">工程师不善于评估自己的表现</a>"，所以当我们发现面试官也高估了自己所提问题的有效性时，也许也不应该感到惊讶。</p><p></p><p>有部分面试官（28%）对自己的招聘选择没有信心，我们问了他们原因。下面是原因的频次分布。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ecaee2868034c4b42b24ce277fc47b9d.png" /></p><p>﻿请注意：没有哪里提到作弊！</p><p></p><p></p><p>大多数面试官都具体说明了他们对招聘决定缺少信心的原因。问题通常包括解决方案次优、遗漏边缘情况、代码混乱或沟通糟糕。我们特意加入了一个“其他问题”类别，看看他们是否会表达对面试者作弊的担忧，虽然我们深入挖掘，但只发现了一些轻微的问题，比如“性格问题”和“他们需要加快编码速度”。</p><p></p><p>除了这个点出作弊的机会外，我们另外还有3次提示面试官指出其他他们担忧的问题，包括自由格式的文本框和几个选择题，其中的选项可以解释他们的担忧。</p><p></p><p>当面试者因为不理解ChatGPT提供的回答而面试失败时，面试官会把面试者的奇怪行为和生硬回答归结为缺乏练习——而不是作弊。有一位面试官认为求职者解决问题的能力不错，但又评论说他们速度很慢，需要更仔细地考虑边缘情况。</p><p></p><p></p><blockquote>“求职者似乎没有准备好回答任何LeetCode问题。”“求职者的方法不够清晰，而且他们急于开始编码。““这位求职者甚至没有准备好解决LeetCode上最基本的编程问题。”“总的来说，解决问题的能力不错，但应聘者需要在编码和识别关键边缘情况方面加快速度。“</blockquote><p></p><p></p><p>那么，谁记录了对作弊的担忧？又有谁作弊被抓了呢？</p><p></p><p>事实是，没有一位面试官提到对求职者作弊的担忧。</p><p></p><p>我们惊讶地发现，面试官并没有怀疑他们作弊。有趣的是，面试者也很自信自己没有作弊。81%的人表示不担心被发现，13%的人认为面试官可能已经发现他们作弊，而令人惊讶的是，仅有6%的参与者认为面试官会怀疑他们作弊。</p><p></p><p><img src="https://static001.geekbang.org/infoq/31/31ae61fa01ebe89cfc8f5c7411b5d6f1.png" /></p><p>﻿大部分面试者都确信他们作弊没有被发现。</p><p></p><p></p><p>有面试者担心被发现，面试官也确实在事后分析中给出了异常评价，但没有怀疑他们作弊。总而言之，大多数面试者认为他们作弊不会被发现——他们是对的！</p><p></p><p></p><h2>企业应立即改变所提的问题</h2><p></p><p></p><p>从这些结果中可以得出一个明显的结论，公司需要立即开始问自定义问题，否则他们将面临求职者在面试中作弊的严重风险（最终无法从面试中获得有用的信号）！</p><p></p><p>ChatGPT已经淘汰了原题；依赖这些问题的人，他们的招聘过程只能听天由命了。<a href="https://interviewing.io/blog/we-ran-the-numbers-and-there-really-is-a-pipeline-problem-in-eng-hiring">招聘已经够棘手的了</a>"，哪有心思再担心作弊。如果你所在的公司原封不动地使用LeetCode的问题，那么请在内部分享下这篇文章！</p><p></p><p>使用自定义问题不仅是防止作弊的好方法，还可以过滤掉那些记住了一堆LeetCode解决方案的求职者（如你所见，自定义问题组的通过率明显低于对照组）。它还能有效地改善求职者的体验，让人们更愿意为你工作。不久前，我们做了一个分析，<a href="https://interviewing.io/blog/best-technical-interviews-common">是什么造就了优秀的面试官</a>"。毫不奇怪，提出好问题是他们的一大特点，而我们评价最高的面试官往往是那些更乐于提出自定义问题的面试官！在我们的研究中，问题质量非常重要，它关系到求职者是否想在公司继续发展。这比公司的品牌实力还要重要许多。品牌实力在吸引求职者进入公司时是一个很重要的因素，但在面试过程中，相对于问题的质量来说就不那么重要了。</p><p></p><p>下面是来自求职者的一些说法：</p><p></p><p></p><blockquote>“要是不仅仅是简单的算法问题会更好。”“我喜欢这个问题——它采用了一个相对简单的算法问题（构建并遍历树），并增加了一些深度。我还喜欢面试官将问题与[Redacted]的实际产品联系起来，这让它看起来不像是一个玩具问题，而更像是一个实际问题的精简版。”“这是我在这个网站上遇到的最喜欢的问题。这是仅有的几个似乎适用于现实生活的方法之一，它来自于一个真实的（或潜在的）业务挑战。它还很好地融合了复杂性、效率和阻塞等挑战。”</blockquote><p></p><p></p><p>对于那些决定采用更个性化问题的公司来说，还有一个略显微妙的建议。你可能会把LeetCode的原题拿过来，然后做些修改。这很容易理解，因为这比从头开始提出问题要容易得多。遗憾的的是，这不起作用。</p><p></p><p>如前所述，我们在实验中发现，一个问题看起来像一个自定义问题并不意味着它就是一个自定义问题。问题可以看上去是自定义的，但实际上仍然与已有的LeetCode问题相同。在向求职者提问时，仅仅模糊一个已经存在的问题是不够的。你需要确保问题的输入和输出都是唯一的，这样才能有效地防止ChatGPT识别它！</p><p></p><p>面试官问的问题是保密的，我们不能分享面试官在实验中使用的具体问题。不过，我们可以给你举个例子。下面是一个有这类严重缺陷的“自定义问题”，ChatGPT很容易就能解答：</p><p><code lang="null">For her birthday, Mia received a mysterious box containing numbered cards

and a note saying, "Combine two cards that add up to 18 to unlock your gift!"

Help Mia find the right pair of cards to reveal her surprise.

Input: An array of integers (the numbers on the cards), and the target sum (18).

arr = [1, 3, 5, 10, 8], target = 18

Output: The indices of the two cards that add up to the target sum.

In this case, [3, 4] because index 3 and 4 add to 18 (10+8).</code></p><p></p><p>你发现问题了吗？虽然这个问题乍一看似乎是“自定义的”，但它的目标与流行的<a href="https://leetcode.com/problems/two-sum/">TwoSum</a>"问题相同：找到两个数字，它们的和等于给定的目标值。输入和输出都一样；这个问题唯一的“自定义”之处就是给问题加上了故事。</p><p></p><p>既然与已知问题相同，那么对于输入和输出都与现有已知问题相同的问题，ChatGPT表现良好也就不足为奇了——即使是为它们添加了一个独特的故事。</p><p></p><p></p><h3>如何创建好的自定义问题</h3><p></p><p></p><p>我们发现，对于提出好的原创问题，有一件事非常有用，就是在团队中创建一个共享文档，每当有人解决了他们认为有趣的问题时，无论问题多小，都快速记下，后续也无需补充完善这些笔记，但它们可以成为独特面试问题的种子，让求职者深入了解你公司的日常工作。把这些杂乱的种子变成面试问题需要思考和努力——你必须删去很多细节，提炼出问题的本质，使求职者不需要花很多时间去理解。你可能还得反复琢磨这些问题几次，才能把它们弄好——但回报也可能是巨大的。</p><p></p><p>需要说明的是，我们并不提倡从技术面试中删除数据结构和算法。DS&amp;A问题之所以名声不佳，是因为那些糟糕的、不敬业的面试官，也因为公司偷懒，重复使用LeetCode的问题，其中许多问题很糟糕，与他们的工作毫无关系。在好的面试官手中，这些问题会强而有力。如果你用上面的方法，就能够提出新的数据结构和算法问题，一些有实践基础并能吸引求职者、让他们对你所做的工作感到兴奋的问题。</p><p></p><p>这样，你也将推动我们的行业向前发展。背诵一堆LeetCode问题就能让求职者获得面试优势，这不好，也不能让作弊看起来像是面试的理性选择。解决的办法是雇主多做一些工作，提出更好的问题。让我们一起行动起来吧。</p><p></p><p></p><h2>给求职者的真心话</h2><p></p><p></p><p>好了，现在，所有正在积极找工作的人，请听好！是的，你的一部分同事现在会在面试中使用ChatGPT作弊，在那些使用LeetCode问题的公司（可悲的是，很多），这些同事会在短时间内取得优势。</p><p></p><p>现在，我们正处于一个临界状态，公司的流程还没有赶上现实的发展。他们很快就会完全放弃使用LeetCode原题（这对我们整个行业来说都是一个福音），或者回到现场（这将使作弊者在很大程度上不可能通过技术面试），或者两者兼而有之。</p><p></p><p>在<a href="https://interviewing.io/blog/you-now-need-to-do-15-percent-better-in-technical-interviews">本已艰难的环境</a>"下，我们会担心其他求职者作弊，这很糟糕，但凭良心，我们不能通过作弊来实现“公平竞争”。</p><p></p><p>此外，使用ChatGPT的面试者一致表示，在面试过程中使用AI使得整个面试过程困难了许多。</p><p></p><p>从下面这段<a href="https://www.youtube.com/watch?v=jtcCK0yr9Bg">视频</a>"中可以看到，一位面试者完美地回答了面试问题，但在分析时间复杂性时却磕磕绊绊。当面试者着急着慌地解释如何得出了错误的时间复杂度（ChatGPT提供的答案）时，面试官都被整糊涂了。</p><p></p><p></p><p></p><p></p><p>在实验过程中没有人被抓到作弊，他们的摄像头是关闭的。但正如我们在视频中看到的那样，即使是对于熟练的求职者来说，作弊也仍然很困难。</p><p></p><p>撇开道德不谈，作弊很难，会造成压力，而且实施起来并不简单。相反，我们建议将这些努力投入到实践中，一旦公司改变了他们的面试流程（希望这很快就会发生），你就可以因此获益。最后，我们希望ChatGPT的出现将成为催化剂，推动行业的面试标准从苦练和记忆转变为真正地考察工程能力。</p><p></p><p>声明：本文为InfoQ翻译，未经许可禁止转载。</p><p></p><p>原文链接：</p><p></p><p><a href="https://interviewing.io/blog/how-hard-is-it-to-cheat-with-chatgpt-in-technical-interviews">https://interviewing.io/blog/how-hard-is-it-to-cheat-with-chatgpt-in-technical-interviews</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/VOq4vxule6c9l73QYtrl</id>
            <title>离开百度7年后，吴恩达终于大厂“再就业”：加入亚马逊董事会，帮其实现AI大志</title>
            <link>https://www.infoq.cn/article/VOq4vxule6c9l73QYtrl</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/VOq4vxule6c9l73QYtrl</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Apr 2024 06:15:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 亚马逊, 吴恩达, 人工智能, 董事会
<br>
<br>
总结: 亚马逊任命吴恩达为董事会成员，强调人工智能在当今时代的重要性，吴恩达在人工智能领域的经历和贡献备受认可。亚马逊CEO表示公司将在人工智能领域取得重要进展，强调亚马逊云科技在全球数字企业中的地位和影响。 </div>
                        <hr>
                    
                    <p></p><p>&nbsp;</p><p>路透社消息，当地4月12日，亚马逊发布公告称，计算机科学家吴恩达 (Andrew Ng) 成为亚马逊董事会成员，这项任命于 4 月 9 日生效。</p><p>&nbsp;</p><p>“AI，尤其是生成式 AI，是我们这个时代最具变革性的创新之一。”亚马逊表示，“我们寻求公司各个层面，包括董事会，拥有适当经验和观点的人。”而吴恩达“将有助于让董事会了解人工智能带来的机遇和挑战，及其变革性的社会和商业潜力。”</p><p>&nbsp;</p><p>此外，亚马逊还表示，自 2014 年以来一直担任亚马逊董事会成员的&nbsp;Judith McGrath&nbsp;今年决定不再连任董事会成员。</p><p>&nbsp;</p><p>更新后的亚马逊 12 名董事会成员名单：</p><p>&nbsp;</p><p>Jeff Bezos，亚马逊创始人兼执行主席。Andy Jassy，亚马逊首席执行官兼总裁。Keith Alexander，IronNet 前首席执行官、总裁兼主席，曾任美国网络司令部司令和国家安全局（NSA）局长。Edith Cooper，Medley Living联合创始人、高盛集团前执行副总裁。Jamie Gorelick，威凯平和而德（Wilmer Cutler Pickering Hale and Dorr LLP）律师事务所合伙人。Daniel Huttenlocher，麻省理工学院施瓦茨曼计算机学院院长。吴恩达，AI Fund&nbsp;执行普通合伙人、DeepLearning.AI 和 Landing AI创始人。Indra Nooyi，百事公司前主席兼首席执行官。Jonathan Rubinstein，全球最大对冲基金桥水基金（Bridgewater Associates）前联合首席执行官，苹果、Palm 和惠普前高管。Brad Smith，马歇尔大学校长，Intuit 公司前执行主席、总裁兼首席执行官。Patricia Stonesifer，华盛顿非营利机构 Martha’s Table 前总裁兼首席执行官，比尔和梅林达•盖茨基金会前首席执行官。Wendell Weeks，材料科学创新者和制造商康宁公司主席兼首席执行官。</p><p>&nbsp;</p><p></p><h2>吴恩达的“AI 观”</h2><p></p><p>&nbsp;</p><p>作为斯坦福教授的吴恩达不必多介绍，他被提到最多的职业经历就是谷歌和百度两段经历。</p><p>&nbsp;</p><p>2011年，吴恩达在谷歌创建了当时称为“Google X 实验室”旗下的Google Brain项目，以通过分布式集群计算机开发超大规模的人工神经网络，进而改进谷歌产品和服务的性能。</p><p>&nbsp;</p><p>Google Brain 很快展现出了惊人的效益和成功，Google X 前负责人埃里克·泰勒曾透露，Google Brain 当时赚到的钱超过了整个 Google X 部门的成本。于是 2011 年，Google Brain 独立成为 Google 的人工智能项目。</p><p>&nbsp;</p><p>这段经历对他来说应该也是意义非凡。前段时间谷歌Gemini遭到大量差评时候，他还在推特上鼓励道：“我只想说我爱你们所有人，支持你们。我知道每个人都是好意，感谢你们的工作，期待看到你们把这项惊人的技术发展到更高程度！”</p><p>&nbsp;</p><p>2014年5月16日，吴恩达加入百度，负责“百度大脑”计划，并担任百度公司首席科学家。2017年3月，吴恩达宣布从百度离职。在百度的三年里，吴恩达一度成为李彦宏之外的另外一个百度代言人。借助他的影响力，百度中美人工智能团队增长到了1300人，AI也逐渐应用到各个业务层面，确立了探索无人驾驶、自然语言处理和语音交互等底层技术的大方向。</p><p>&nbsp;</p><p>离开百度后，吴恩达创建了自己的AI公司：DeepLearning.AI。目前，吴恩达还是 AI Fund 风险投资基金的执行普通合伙人、计算机视觉初创公司 Landing AI 的创始人兼首席执行官、在线教育公司&nbsp;Coursera的联合创始人。</p><p>&nbsp;</p><p>针对人工智能的发展，吴恩达曾在推特表示，“我认为AI Agents 工作流程将在今年推动人工智能的巨大进步——甚至可能超过下一代基础模型。这是一个重要的趋势，我呼吁所有从事人工智能工作的人都关注它。”</p><p>&nbsp;</p><p>在近日红杉资本（Sequoia）在美国举行的AI Ascent活动上，吴恩达提到，AI Agents 的工作方式跟人类更相像。根据吴恩达分享的数据，使用 GPT-3.5 进行零样本提示的正确率是48%，GPT-4 的表现要好得多，正确率是 67%。但是如果在 GPT-3.5 的基础上建立一个 AI Agent的工作流，它甚至能比 GPT-4 做得更好。</p><p>&nbsp;</p><p>吴恩达认为，Agents工作流的出现，语言模型的能力有望在今年得到显著提升。随之而来的是，Token生成速度变得至关重要，甚至比大模型能力提升更重要，甚至还要让模型花更多时间推理和迭代。</p><p>&nbsp;</p><p>对于当前的大模型竞争，吴恩达认为短期不会立即结束：“现在有很多资源非常丰富的公司‘承受不起损失’，它们花费数十亿美元来竞争建立更好的大模型。我预计这场比赛将持续数年。这对于创新来说非常棒，对于每个在大模型之上构建应用程序的人来说也是如此。”</p><p>&nbsp;</p><p>这也有些像吴恩达对 AGI 的态度：它是慢慢到来的，而不是一夜之间能到来的。</p><p>&nbsp;</p><p></p><h2>“落后”的Amazon回归底层</h2><p></p><p>&nbsp;</p><p>在宣布吴恩达成为董事会成员之际，Amazon CEO Andy Jassy 也发布了股东信。</p><p>&nbsp;</p><p>尽管未能打造出与ChatGPT正面抗衡的消费级生成式AI产品、消费者和整个市场普遍认为亚马逊在AI领域已然落后，Jassy仍信心满满地表示，该公司将成为下一轮技术竞赛的主要参与者。</p><p>&nbsp;</p><p>Jassy 乐观地认为这波改变世界的AI浪潮“将主要建立在亚马逊云科技之上。”作为该公司的云计算业务，亚马逊云科技正在为全球众多数字企业提供运行基础。</p><p>&nbsp;</p><p>Jassy在股东信中间接批评了竞争对手的人工智能模型，称亚马逊提供了来自不同公司的模型，例如 Anthropic、Stability AI、Meta、Cohere 以及自己的模型，而不是仅仅依赖一种占主导地位的人工智能模型。“客户不只想要一种型号。他们希望获得适合不同类型应用的各种模型和模型尺寸，”他补充道。</p><p>&nbsp;</p><p>Jassy阐述了该公司在生成式AI领域的战略，坦言亚马逊云科技将不再专注构建面向消费者以直接同OpenAI&nbsp;ChatGPT等流行工具展开竞争的应用程序，转而专注构建底层“基础”AI模型并将相关成果出售给企业客户。Jassy提到，目前达美航空、西门子和辉瑞都已成为亚马逊大模型的买家。</p><p>&nbsp;</p><p>随着一年半之前ChatGPT的横空出世，各大科技巨头与一波初创企业间迅速掀起军备竞赛潮，人们争相构建最强大的AI技术并希望从中找到盈利空间。谷歌、OpenAI及Anthropic AI等多家公司先后投入数十亿美元，推出功能愈发强大的AI机器人。与此同时，企业们也在积极寻求将技术成果整合至现有产品当中的正确方法。但尴尬的是，目前大多数消费者还不打算为市面上的AI工具支付费用。</p><p>&nbsp;</p><p>亚马逊同样在生成式AI领域砸下数十亿美元。最近，他们又向初创公司Anthropic追加投资27.5亿美元，凭借高达40亿美元的总投资获得少数股权。作为交易的一部分，Anthropic将在亚马逊云科技的服务上运行，同时允许亚马逊向自家企业客户开放Anthropic Cluade——目前业界领先的生成式AI模型之一。</p><p>&nbsp;</p><p>亚马逊同时投入了数十亿美元，着力建设AI技术开发所必需的数据中心设施。</p><p>&nbsp;</p><p>尽管亚马逊明显是希望通过未来发展在AI领域赢得主导权，但直到现在，他们一直没能打造出广泛引发客户共鸣的消费级产品。今年早些时候，该公司曾经推出购物助手Rufus，但并未显著改善基于搜索的原有购物体验。该公司也叫停了去年9月宣布推出的生成式AI版Alexa——尽管在宣传阶段强调以“更智能、更具对话体验”为卖点，但实际成果始终没能与客户见面。</p><p>&nbsp;</p><p>Jassy在股东信里提到，在生成式 AI 方面，向 Amazon SageMaker 添加了数十种功能，以便开发人员更轻松地构建新的基础模型（“FM”）；发明并提供了一项新服务 (Amazon Bedrock)，让公司可以利用现有的 FM 来构建 GenAI 应用程序；在Amazon Q 中推出了功能最强大的编码助手。“客户对这些功能感到兴奋，并且我们看到我们的 GenAI 产品具有巨大的吸引力。”</p><p>&nbsp;</p><p>尽管今年以来，亚马逊公司的股价已经上涨25%，但其仍未从疫情带来的超支负担中完全恢复过来。在2022年至2024年期间累计裁员超2.7万人之后，该公司上周公布了新的裁员计划，着手砍掉亚马逊云科技中数百个职位。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.aboutamazon.com/news/company-news/dr-andrew-ng-joins-amazon-board-of-directors">https://www.aboutamazon.com/news/company-news/dr-andrew-ng-joins-amazon-board-of-directors</a>"</p><p><a href="https://www.aboutamazon.com/news/company-news/amazon-ceo-andy-jassy-2023-letter-to-shareholders">https://www.aboutamazon.com/news/company-news/amazon-ceo-andy-jassy-2023-letter-to-shareholders</a>"</p><p><a href="https://www.washingtonpost.com/technology/2024/04/11/amazon-ai/">https://www.washingtonpost.com/technology/2024/04/11/amazon-ai/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/7g8PXNNtisFg5OptZJcw</id>
            <title>李彦宏内部讲话曝光：闭源模型才能“遥遥领先”！</title>
            <link>https://www.infoq.cn/article/7g8PXNNtisFg5OptZJcw</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/7g8PXNNtisFg5OptZJcw</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Apr 2024 03:51:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 百度, 大模型, 开源, AI创业者
<br>
<br>
总结: 百度创始人李彦宏表示，百度不开源的原因是市场已有足够多的开源大模型，闭源模型在能力上会持续领先。AI创业者应该专注于某一领域的知识和数据，而不是分散精力于模型研发和应用开发。 </div>
                        <hr>
                    
                    <p>4&nbsp;月&nbsp;11&nbsp;日晚间消息，百度创始人、董事长兼&nbsp;CEO&nbsp;李彦宏近日的一次内部讲话曝光。讲话中，李彦宏针对当前业界热议的“大模型应该开源还是闭源？”“AI&nbsp;创业者应当专注于模型研发还是应用开发？”等问题，表达了自己的见解。</p><p></p><p></p><h4>为什么百度不开源？</h4><p></p><p></p><p>百度的文心大模型在一年前刚刚发布的时候，公司内部对其是否开源有过激烈讨论，最终的决定是不开源，因为判断市场上会有足够多的开源大模型。事实证明，今天主流的开源大模型里，国外的&nbsp;Llama、Mistral，国内的智源、百川、阿里的通义等都具有相当影响力，李彦宏说，“在这种情况下，多百度一家开源不多，少百度一家开源也不少。”如果开源，还需要再去维护一套开源的版本，对于百度来说是不划算的。</p><p></p><p>更重要的是，闭源模型在能力上会持续地领先，而不是一时地领先。</p><p></p><p>李彦宏称与传统的Linux、安卓等软件开源不同，“模型开源不是一个众人拾柴火焰高的情况。”因为开源模型都是在外头零零散散、小规模地去做各种各样的验证应用，没有经过大算力的验证。</p><p></p><p>而且，闭源有着真正的商业模式，能够赚到钱，从而能够聚集算力和人才。对于成本来说，虽然开源是免费的，但李彦宏认为闭源在成本和效率上反而更有优势，“只要是同等能力，闭源模型的推理成本一定是更低的，响应速度一定是更快的。反过来，同等参数的情况下，闭源模型的能力也是更强的。”</p><p></p><p></p><h4>研发大模型还是应用？</h4><p></p><p></p><p>李彦宏认为既做模型又做应用，势必会分散精力。对于资源和精力都有限的创业公司来说，应该专注于一项任务，“力出一孔”，而不是去搞所谓的“双轮驱动”。</p><p></p><p>对于&nbsp;AI&nbsp;创业者来说，核心竞争力本就不应该是模型本身，这样太耗费资源，且需要非常长时间的坚持才能跑出来。AI&nbsp;创业者的优势应该是在某一个领域的知识和数据，去靠领域知识提供特定价值。</p><p></p><p>最后，李彦宏提到没有必要担心基础模型通吃&nbsp;AI&nbsp;的应用，“拼多多、滴滴不怕微信抢饭碗，它们的兴起都是依赖微信这个移动生态中的封闭平台，但它们各自提供了独特价值，有不同的竞争力。”李彦宏说。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/MExA5tbGSy2DPAfOUIrj</id>
            <title>网易有道自研RAG引擎QAnything升级：发布纯python版本，首次支持在Mac运行</title>
            <link>https://www.infoq.cn/article/MExA5tbGSy2DPAfOUIrj</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/MExA5tbGSy2DPAfOUIrj</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Apr 2024 03:19:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 有道知识库问答引擎, QAnything, BM25 + embedding混合检索, BCEmbedding模型
<br>
<br>
总结: 有道知识库问答引擎QAnything更新至1.3.0版本，带来了纯python版本和BM25 + embedding混合检索功能，为开发者提供更强大的技术支持和用户体验。引擎支持多种文档格式上传和互动问答功能，准确率高，下载次数众多。采用自研BCEmbedding模型，检索准确率高达95%，覆盖多领域，为商业化落地提供便捷。已在多场景落地，提供个性化服务和快速文档理解，为企业带来生产效率提升。 </div>
                        <hr>
                    
                    <p>4月8日，有道知识库问答引擎QAnything更新至1.3.0版本，该版本带来了两大主要功能升级：发布纯python的轻量级的版本，该版本支持在Mac上运行，也可以在纯CPU机器上运行；同时支持BM25 + embedding混合检索，可以实现更精准的语义检索和关键字搜索。本次更新后，QAnything能为开发者探索大模型落地提供更强大的技术支撑和更流畅的用户体验。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/2d/2d10e1de210d873c2ee0608c65bf3cf9.png" /></p><p></p><p>QAnything是网易有道自研的RAG（Retrieval Augmented Generation)&nbsp;引擎。该引擎允许用户上传PDF、图片、Word、Excel、PowerPoint等多种格式的文档，并实现类似于ChatGPT的互动问答功能，其中每个答案都能精确追溯到相应的文档段落来源。该引擎支持纯本地部署，上传文档数量无上限，问答准确率很高。</p><p></p><p>GitHub地址：</p><p>https://github.com/netease-youdao/QAnything</p><p></p><p>自今年1月开源以来，QAnything迅速吸引了开发者社区的广泛关注，并多次登上了GitHub trending榜单。截至目前，在GitHub上QAnything已经积累7000+个星标，这反映出了用户对其价值的高度评价。</p><p></p><p><img src="https://static001.geekbang.org/infoq/87/8718eb7895f9e6d25a5418f9adf05df2.png" /></p><p></p><p>此外，QAnything下载次数已达数万次。其中，语义嵌入排序模型BCEmbedding更是每月可达超60万次下载。</p><p><img src="https://static001.geekbang.org/infoq/bc/bcc170b003d4daec142b1c4fd4625e55.png" /></p><p></p><p>值得一提的是，QAnything采用了自研的BCEmbedding模型（RAG系统关键模块）。有道发现，在客服问答以及一些toB客户的场景中，OpenAI的Ada2 BCEmbedding检索准确率只有60%，而其自研的 BCEmbedding检索准确率可以达到95%。该模型具有中英双语跨语种能力和多领域覆盖两大特色。</p><p></p><p>据悉，QAnything收集了包括教育、医疗、法律、金融、百科、科研论文、客服、通用QA等场景的语料，使得模型可以覆盖和支持尽可能多的应用场景，为商业化落地提供了便捷。</p><p></p><p>目前，QAnything已在有道多场景中落地。如“有道领世”在QAnything的帮助下，凭借海量的升学资料数据，打造出一个“私人AI规划师”，能为每个家长和学生提供个性化的服务，展示更加全面、专业、及时的升学规划。面对高考政策、升学路径、学习生活以及职业规划等各类问题，该系统的解答准确率超过95%。未来随着数据补充和更新，准确率会一直上涨。</p><p></p><p>与此同时，子曰教育大模型最新应用成果“有道速读”，其核心功能文档问答、文章摘要、要点解读、引文口碑和领域综述，背后驱动也是QAnything。在其加持下，用户快速理解文档、定位要点等诉求得以快速实现，短短一分钟，万字长文就能拆解得明明白白。除赋能自身业务外，开源后的QAnything不断拓宽“朋友圈”。目前已累计为近百家企业赋能，以期让AI应用真正进入医疗、物流、办公等多元化场景，为企业、组织和个人带来生产效率的大幅提升。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/AlEwm57dwf6C83B5O5Py</id>
            <title>谷歌、OpenAI、Mistral 在 24 小时内打响科技界“三强争霸赛”</title>
            <link>https://www.infoq.cn/article/AlEwm57dwf6C83B5O5Py</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/AlEwm57dwf6C83B5O5Py</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Apr 2024 01:14:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Gemini API, GPT-4 Turbo, Mixtral 8x22B, AI模型
<br>
<br>
总结: 谷歌、OpenAI和Mistral相继发布了最新的AI模型，Gemini API提供了功能最强大的生成式AI模型Gemini 1.5 Pro的公开预览版，OpenAI发布了GPT-4 Turbo，集成了视觉理解能力，Mistral则开源了Mixtral 8x22B模型。这三家公司的新模型引发了科技界的关注和讨论，展开了一场“三强争霸赛”。Gemini 1.5 Pro扩展了功能，GPT-4 Turbo新增了视觉理解能力，Mixtral 8x22B在性能上表现出色，各有各的特点和优势。 </div>
                        <hr>
                    
                    <p>太平洋时间本周二&nbsp;11:01，谷歌在官网中宣布在&nbsp;180&nbsp;多个国家/地区通过&nbsp;Gemini&nbsp;API&nbsp;提供&nbsp;Gemini&nbsp;1.5&nbsp;Pro&nbsp;的公开预览版，这是它目前功能最强大的生成式&nbsp;AI&nbsp;模型。谷歌本以为能在互联网上掀起一番声势浩大的讨论，不料短短&nbsp;40&nbsp;分钟后，OpenAI&nbsp;就出来抢风头了：它发布了非预览版的&nbsp;GPT-4&nbsp;Turbo，将之前独立的&nbsp;GPT-4&nbsp;Vision&nbsp;直接集成到模型中。这还没完，下午6:20，Mistral&nbsp;在&nbsp;X&nbsp;上直接了当地甩出一条磁链，强势开源&nbsp;Mixtral&nbsp;8x22B&nbsp;这个超大模型。</p><p></p><p>谷歌刚拔剑出鞘，OpenAI&nbsp;和&nbsp;Mistral&nbsp;立马摩拳擦掌加入战斗，科技界“三强争霸赛”一触即发。不过，到底是虚张声势还是确实“有点东西”，让我们一探究竟。</p><p></p><p></p><h3>Gemini&nbsp;1.5&nbsp;Pro&nbsp;：“听”懂掌声</h3><p></p><p></p><p>Gemini&nbsp;1.5&nbsp;Pro&nbsp;目前已在谷歌面向企业的&nbsp;AI&nbsp;开发平台&nbsp;Vertex&nbsp;AI&nbsp;上提供公共预览版。它能处理的上下文从&nbsp;12.8&nbsp;万个&nbsp;token&nbsp;增加到&nbsp;100&nbsp;万个&nbsp;token，相当于大约&nbsp;70&nbsp;万个单词，或者大约&nbsp;3&nbsp;万行代码。这大致是&nbsp;Anthropic&nbsp;旗下模型&nbsp;Claude&nbsp;3&nbsp;最大上下文量的四倍，OpenAI&nbsp;旗下模型&nbsp;GPT-4&nbsp;Turbo&nbsp;最大上下文量的八倍。</p><p></p><p>Gemini&nbsp;1.5&nbsp;Pro&nbsp;版本扩展了输入模态，首次提供了本地音频（语音）理解功能和全新的文件&nbsp;API，使文件处理变得更加简单。此外，Gemini&nbsp;1.5&nbsp;Pro&nbsp;现在能够对上传到谷歌&nbsp;AI&nbsp;Studio&nbsp;中的视频进行图像（帧）和音频（语音）推理，谷歌也期待尽快为此添加&nbsp;API&nbsp;支持。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/45/66/4510a7f254f882138f669e1750e4c766.gif" /></p><p>您可以上传讲座的录音，Gemini&nbsp;1.5&nbsp;Pro&nbsp;可以将其变成小测验，并附有答案。</p><p></p><p>不过，Gemini&nbsp;1.5&nbsp;Pro&nbsp;对于没有访问&nbsp;Vertex&nbsp;AI&nbsp;和&nbsp;AI&nbsp;Studio&nbsp;权限的人来说是不可用的。目前，大多数人只能通过&nbsp;Gemini&nbsp;聊天机器人来接触&nbsp;Gemini&nbsp;语言模型。虽然它功能强大，也能理解长命令，但它的速度不如&nbsp;Gemini&nbsp;1.5&nbsp;Pro。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bc/bc6f3ae3ac13de785c0ab95006cf8a96.webp" /></p><p></p><p></p><h3>GPT-4&nbsp;Turbo：不如不“看”？</h3><p></p><p></p><p>OpenAI&nbsp;宣布&nbsp;GPT-4&nbsp;Turbo&nbsp;with&nbsp;Vision&nbsp;模型已经通过&nbsp;OpenAI&nbsp;API&nbsp;向开发人员开放。该模型延续了&nbsp;GPT-4&nbsp;Turbo&nbsp;系列&nbsp;128,000&nbsp;个&nbsp;token&nbsp;的窗口大小，以及截止至&nbsp;2023&nbsp;年&nbsp;12&nbsp;月的知识库，最大的革新之处在于其新增的视觉理解能力，可处理和分析多媒体输入信息。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ef/efbea90e62627ab80cec973f76f62db5.png" /></p><p></p><p>OpenAI&nbsp;称这些变化有助于简化开发人员的工作流程并打造更高效的应用程序，因为“过去，开发者需要调用不同的模型来处理文本和图像信息，但现在，只需一次&nbsp;API&nbsp;调用，该模型就可以分析图像并应用推理。”</p><p></p><p>OpenAI&nbsp;还提到此次更新是“&nbsp;Majorly&nbsp;improved（重大改进）”，不过网友则对这个“小修小补”表示不感兴趣：“如果不是&nbsp;GPT-5&nbsp;的话，还是别发了。”</p><p></p><p>延伸阅读：OpenAI&nbsp;重磅发布的GPT-4&nbsp;Turbo&nbsp;with&nbsp;Vision，是编码的倒退</p><p></p><p></p><h3>Mixtral&nbsp;8x22B&nbsp;：强势开源</h3><p></p><p></p><p>今年&nbsp;1&nbsp;月，Mistral&nbsp;AI&nbsp;公布了&nbsp;Mixtral&nbsp;8x7B&nbsp;的技术细节，该模型以&nbsp;47B&nbsp;左右的参数总量，展现了不错的性能——在人类评估基准上明显超过了&nbsp;GPT-3.5&nbsp;Turbo、Claude-2.1、Gemini&nbsp;Pro&nbsp;和&nbsp;Llama&nbsp;2&nbsp;70B&nbsp;聊天模型。</p><p></p><p>短短&nbsp;3&nbsp;个月后，Mistral&nbsp;AI&nbsp;开源了&nbsp;Mistral&nbsp;8X22B&nbsp;模型，再一次为开源社区注入了新鲜血液。Mistral&nbsp;AI&nbsp;提供的磁链大小为&nbsp;281&nbsp;GB，下载后可以看到模型文件大小约为&nbsp;262&nbsp;GB，比之前的&nbsp;Mixtral&nbsp;8x7B&nbsp;大得多，鉴于&nbsp;Mixtral&nbsp;8x7B&nbsp;优秀的表现，网友们表示很看好&nbsp;Mistral&nbsp;8X22B，不过目前还没有看到有人运行它。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f0/f05ce070a3a9df18b455a2942b2e04de.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/5d/5dc5f6f076f8b50bca56c28515d39750.png" /></p><p></p><p></p><h3>芯片大战</h3><p></p><p></p><p>除了软件的较量，另一边，硬件领域中的芯片也是八仙过海。</p><p></p><p>对于提升训练&nbsp;AI&nbsp;模型所需的算力来说，CPU&nbsp;至关重要。而众所周知，购买&nbsp;AI&nbsp;芯片的成本惊人，英伟达的&nbsp;Backwell&nbsp;芯片，预计售价在&nbsp;3&nbsp;万美元到&nbsp;4&nbsp;万美元之间。为了在&nbsp;AI&nbsp;军备竞赛中节省开支，微软和亚马逊均在自研处理器方面发力，谷歌自然不甘落后。本周二的&nbsp;Cloud&nbsp;Next&nbsp;2024&nbsp;大会上，谷歌还正式宣布，将自研首款基于&nbsp;Arm&nbsp;的&nbsp;CPU。据称这款&nbsp;CPU&nbsp;处理器&nbsp;Axion，将提供比英特尔&nbsp;CPU&nbsp;更好的性能和能源的效率，其中性能提高&nbsp;50%，能源效率提高&nbsp;60%，比起目前基于&nbsp;Arm&nbsp;的最快通用芯片，Axion的性能还要高出30%。</p><p></p><p>GPU&nbsp;方面，当地时间&nbsp;4&nbsp;月&nbsp;9&nbsp;日，英特尔举办了面向客户和合作伙伴的英特尔&nbsp;on&nbsp;产业创新大会。这场大会上，英特尔首次介绍了他们的&nbsp;GPU&nbsp;产品&nbsp;Gaudi&nbsp;3，对标英伟达早前的主力产品&nbsp;H100。据介绍，英特尔&nbsp;Gaudi&nbsp;3&nbsp;将带来&nbsp;4&nbsp;倍的&nbsp;BF16&nbsp;AI&nbsp;计算能力提升，采用&nbsp;128GB&nbsp;HBMe2&nbsp;内存，支持&nbsp;1.5&nbsp;倍的内存带宽提升，采用&nbsp;5nm&nbsp;制程制造。此外，这颗芯片能够支持多种的大模型，包括&nbsp;Llama、文生图的&nbsp;Stable&nbsp;Diffusion、语音识别的&nbsp;Whisper&nbsp;等等。</p><p></p><p>短短几天，科技圈的大事层出不穷，不得不祭出这张&nbsp;meme&nbsp;了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b2/b25647cca89e6316f69c9e3f42783cf5.png" /></p><p></p><p>作为这场科技革命千千万万的见证者之一，我时刻期待着。</p><p></p><p>参考来源：</p><p></p><p><a href="https://developers.googleblog.com/2024/04/gemini-15-pro-in-public-preview-with-new-features.html">https://developers.googleblog.com/2024/04/gemini-15-pro-in-public-preview-with-new-features.html</a>"</p><p></p><p><a href="https://platform.openai.com/docs/models/continuous-model-upgrades">https://platform.openai.com/docs/models/continuous-model-upgrades</a>"</p><p></p><p><a href="https://twitter.com/OpenAI/status/1777772582680301665">https://twitter.com/OpenAI/status/1777772582680301665</a>"</p><p></p><p><a href="https://twitter.com/MistralAI/status/1777869263778291896">https://twitter.com/MistralAI/status/1777869263778291896</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/z3WeQz4ipl0dBeXKtrpx</id>
            <title>QCon 北京2024 盛大开幕，韦青、王皓、程操红、郭东白、章文嵩、蒋晓伟、李飞飞、张凯等行业领袖呈现精彩分享</title>
            <link>https://www.infoq.cn/article/z3WeQz4ipl0dBeXKtrpx</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/z3WeQz4ipl0dBeXKtrpx</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Apr 2024 13:43:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div>         关键词: QCon, AICon, AIGC, 大模型
        <br>
        <br>
        总结: 今年由极客邦旗下 InfoQ 中国主办的 QCon 全球软件开发大会暨智能软件开发生态展在北京举行，会议内容涵盖生成式 AI，大模型等多个专题，邀请了100多位专家大咖分享企业级大模型落地经验。同时，还举办了AICon，发布了《中国生成式 AI 开发者洞察 2024》报告，展示了生成式AI开发者的工作特征和晋升路线。整体旨在推动AI产业生态建设，探索AI技术在各领域的创新应用。 </div>
                        <hr>
                    
                    <p>4 月 11 日，由极客邦旗下 InfoQ 中国主办的 QCon 全球软件开发大会暨智能软件开发生态展在北京国测国际会议会展中心正式召开。今年 QCon 在会议内容、会议模式上均向着“生成式 AI”全面进化。本届 QCon 一共设置了近 30 个专题，邀请到了来自阿里巴巴、腾讯、百度、微软、字节跳动、华为、京东、智谱、美的、国泰君安、深开鸿等领先企业的 100 多位专家大咖，跟大家分享最真实的企业级大模型落地经验，细数大模型落地痛点。</p><p></p><p>除此之外，本届大会还特别策划了智能软件开发生态展，围绕“智能软件”主题，广泛邀请了生态上下游企业来到 QCon 现场展示最新的技术和产品，为大家带来智能软件时代技术先行者们的案例以供参考。</p><p></p><p></p><h2>开幕精华：让 QCon 的开发者先看到未来</h2><p></p><p></p><p>本次大会于今日上午 9 点正式开幕，InfoQ 主编蔡芳芳为大会致开幕辞。她首先介绍了今年 QCon 大会的精彩看点，随后重点介绍了今年极客邦科技围绕 AIGC 和大模型正在展开哪些工作。2024 年，“AIGC IN ALL"的理念将成为极客邦科技业务升级的核心，极客邦科技正围绕“AI 应用加速、AI 人才培养、AI 产业生态、AI 趋势洞察”这四个核心方向，全面推进产品的创新改造，以迎接大模型时代的到来。</p><p></p><p>今年，极客邦科技重启了 AICon，围绕 AI Agent、RAG、LLM Ops、多模态技术、大模型训练与推理等多个方向，展开丰富而深入的讨论。目前已成功邀请到了来自 Google、阿里巴巴、科大讯飞、字节跳动、华为、智谱科技、月之暗面等领先企业的专家学者，为参会者分享前沿技术和行业应用经验。同时，AICon 将推出首届大模型应用生态展，让那些致力于 AI 和大模型行业落地应用探索，有实践、有创新、有成果的企业，能够有机会将应用案例和创新产品搬到 AICon 现场，让现场参会者有机会深入了解并体验。</p><p></p><p>除了 AICon，InfoQ 面向 AIGC 赛道正式启动【中国技术力量 2024 之 AIGC 先锋榜】案例征集。本次案例征集共分为两个维度，分别是【AIGC 最佳实践案例 TOP20】和【AIGC 最佳技术服务商 TOP30】。本次榜单评选分为自主报名（4.1-4.26）、专家评选（4.26-5.8）、榜单结果公布（5.17）三个环节，InfoQ 将邀请行业专家共同参与案例评选，最终产生上榜名单。我们将于 5 月 17 日召开的 【AICon 全球人工智能开发与应用大会暨大模型应用生态展】 大会现场公布结果，并邀请部分获奖企业来到现场展示并见证这一时刻。欢迎感兴趣的企业扫描下方二维码提报案例信息。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/94/9496dc57091321eec55c30b05bfa9529.webp" /></p><p></p><p>开场致辞中也对《数智时代的 AI 人才粮仓模型解读白皮书》做了简要介绍，白皮书会从政策 + 行业变革等时代背景、企业需求、AI 价值、AI 人才模型及人才培养五个方面对“数智时代的 AI 人才粮仓模型”进行深度解读，为企业提供一个清晰、可操作的 AI 人才布局指南，帮助企业快速构建起适应数字化时代需求的 AI 人才梯队，在激烈的市场竞争中占据先机。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/3c/3c9eecbe12c7054c1c34e0a44f9c716c.jpeg" /></p><p></p><p>为了更好地推动 AI 产业生态建设，极客邦科技将发起一个全新的产业联盟——AIGC 应用创新产业联盟。该联盟旨在打造一个开放、共享、创新的平台，汇聚产业链上下游的企业、研究机构、高校以及创新团队，共同探索 AIGC 技术在各个领域的创新应用。联盟内成员单位将可以获得 InfoQ 商务、内容、大会等层面的优先合作权和特殊折扣，联盟内部将定期围绕成员单位感兴趣的话题举办闭门会。</p><p></p><p>随后，InfoQ 研究总监兼首席分析师姜昕蔚正式发布《中国生成式 AI 开发者洞察 2024》报告并对报告进行了详细解读。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/e3/e3b4e003818d3a6993b1504e5ad6e46f.png" /></p><p></p><p>报告指出，作为新兴行业从业者，生成式 AI 开发者普遍相关工作年限较短、收入较高。InfoQ 调研统计，2023 年生成式 AI 开发者人均年收入为 36.7 万，相关工作经验在 3 年以上生成式 AI 开发者的年收入超越均值，近 4 成生成式 AI 开发者年收入处于 20-50 万区间，远超 2023 年上半年北京招聘平均薪资（18976 元 / 月）。其中，北京生成式 AI 开发者规模最大，但上海资深生成式 AI 开发者更多且人均薪资更高。</p><p></p><p>工作特征方面，应用工具（如智能编码工具）、大语言模型、数据科学 / 数据挖掘 / 数据分析、语言 / 语义理解类应用（如对话机器人）和图像识别类应用（如拍照搜图）是最主要的五个生成式 AI 开发者研发方向。GPT、文心、通义大模型是生成式 AI 开发者使用率最高的大模型。调查显示，2023 年生成式 AI 开发者人均使用 AI 工具时间为半年，最常使用智能化办公工具，其次是图像生成工具。使用代码生成工具和 ChatBot 的人群比例最高，其中使用 ChatBot 的时长略高于均值，而代码生成工具使用时长仅略高于可视化智能数据分析工具。</p><p></p><p>在晋升方面，生成式 AI 开发者中的初入者未来将有四条进阶路线：</p><p></p><p>AI 应用实践者：针对绝大部分具备初级或不具备开发技能的职场新手，未来希望精通 AI 技术场景化应用，实现业务价值升级；AI 技术赋能者——AI 实践领导者：在进阶使用 AI 的过程中，不断提升专业技术，向资深人员转型，并最成为行业引领人才；AI 技术领航者——AI 技术赋能者——AI 实践领导者：以夯实技术能力为主，逐渐全面应用 AI，最终成为行业引领人才；AI 技术领航者：希望成为专项技术精英，推动 AI 技术迭代升级。</p><p></p><p>关注 AI 前线公众号，回复关键词 【开发者洞察】，即可免费获取报告电子版文件。⬇️</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/54/548b5aaac2181aa895e111629a7795e9" /></p><p></p><h2>主题演讲：洞察前沿技术趋势</h2><p></p><p></p><h3>主题演讲：看不见的大猩猩——智能时代的企业生存和发展之路</h3><p></p><p></p><p>大会的首场演讲由微软（中国）首席技术官韦青分享，他提到企业往往会将目光聚焦在那些被媒体广为宣传的问题或潮流上，像“看不见的大猩猩″一样忽略了那些同样显而易见的关键问题；另外，企业内部还会有大量显而易见但是难以解决的问题，这些问题往往需要决策者拥有长期坚持的勇气和应对不确定性的定力，但人们通常会有意识地无视这些“房间里的大象”。这两种现象都有可能成为阻得企业长期发展的“卡点”。他认为，企业应该聚焦这些被忽略的明显而又困难的问题，常常也是由于受到固有思维的制约而找不到解决方案的问题，从而让企业成为新的发展范式中的佼佼者。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/67/6769b4b247d03836c6accedd0ee6ccd3.png" /></p><p></p><p>韦青表示，AGI 时代企业的竞争力主要表现在对大模型智能的应用上面。智能应用的核心是企业独有的数据，企业独有的数据又要基于企业业务流程的数字化改造。企业的当务之急是构建新一代 AI 智能应用来利用大模型的能力学习到企业的专有知识。他认为，没有数字化，也就不会有智能化。数据驱动的智能化过程就是智能化重构一切的过程。</p><p></p><h3>主题演讲：开鸿安全数字底座，构建开源鸿蒙新生态</h3><p></p><p></p><p>随后，深圳开鸿数字产业发展有限公司高级副总裁、研发体系总裁王皓博士发表了《开鸿安全数字底座，构建开源鸿蒙新生态》的主题演讲，围绕开鸿安全数字底座与 AI 融合、开鸿产业化与产业开鸿化、产业互联网等话题，探讨开源鸿蒙在万物智联联时代的发展新机遇。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ed/ed78eb8243ab3b01a00d336b52f9b07f.png" /></p><p></p><p>在如今以“人工智能”为核心驱动力的智能经济的阶段，高质量的数据和算力尤为稀缺。我们需要保证高质量的数据能够实时产生，并即时通过算力进行处理和应用，以满足不同变化的场景需求。深开鸿基于开源鸿蒙，围绕“KaihongOS 和 KaihongOS-Meta”打造开鸿安全数字底座，可提供高质量的数据，同时结合 AI 能够打造行业“挖矿机”。</p><p></p><p>王皓博士提到，随着万物智联的到来，安全数字底座与 AI 相互融合将推动我们进入物理世界与数字世界无缝衔接的时代。过去每一代操作系统多针对特定硬件设计，导致数据碎片化问题凸显。然而，开源鸿蒙是未来建设数字中国的数字底座，基于开源鸿蒙的开鸿安全数字底座面向万物智联时代全场景多设备，可适应 KB 级到 GB 级的存储需求，能统一设备，使数据得以自然流通，设备间实现无缝交互。</p><p></p><p>最后，王皓博士表示，做操作系统就是做生态，需要集聚产业的力量，通过推进“开鸿产业化”与“产业开鸿化”，构建开源鸿蒙新生态。</p><p></p><h3>主题演讲：钉钉智能化之路：打造未来交互新形态，重塑组织效能</h3><p></p><p></p><p>钉钉 CTO 程操红（巴布）以《钉钉智能化之路：打造未来交互新形态，重塑组织效能》为主题展开了分享。他分享了过去一年钉钉的智能化战略布局与实施。在 AI 技术与产品快速迭代的浪潮中，钉钉顺应时势，深度布局智能化战略。当前，钉钉在 AI 上已经发生产品、底座、生态三个方向的巨大变化。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ce/ceb93537105022b0baf5094b74ef6545.png" /></p><p></p><p>巴布提到，AI 带来了产品新形态，从 GUI 到 LUI、多模态，产品交互形态的变化让软硬结合有了更多可能性：</p><p></p><p>AI Inside。AI 悄无声息的进入到企业系统应用中，让原来 GUI 的重交互模式变成 LUI 的轻交互模式。AI Copilot。AI 让原来冷冰冰的系统应用多了一个智能副驾驶复杂流程。AI Agent。AI 已经不再拘泥于传统软件形式，更像是数字员工一样进入组织, 一起协同。</p><p></p><p>AI 也带来了数据新消费。数据的归途不只是报表和大屏，而是作为核心生产要素发挥更大价值。第一，数据可以作为模型训练语料，海量的数据可以喂给模型做 Fine-tune，也可以作为 RAG 的知识库让 AI 理解和召回。第二，数据作为 AI 助理的记忆，多维度的特征数据也可以作为记忆，植入到 AI 助理的大脑让它知道自己的本事有多大。第三，数据作为 AI 助理的感知，上下文、过程信息、以及外界传递的数据都可以作为 AI 助理的感知，像人一样做实时理解和反馈。</p><p></p><p>最后，巴布提到 AI 带来了协同新方式。协同网络变得更加多元化，一方面，AI 助理作为全新角色，融入到连接组织内外的协同网络，成为助手 / 数字员工。另一方面，人、系统、硬件、AI 助理之间会形成网状协同, 消除信息孤岛，加速数据流通和消费。</p><p></p><h3>主题演讲：大模型时代的架构思维</h3><p></p><p></p><p>Coupang 副总裁郭东白分享了《大模型时代的架构思维》，他从软件架构的六个基本要素分析了大模型对软件研发活动的冲击，他指出，大模型的盲区就是架构师创造价值的所在，大模型对软件架构师的冲击很小，甚至会可能带来更大的市场需求。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/34/34efbaed3ecd83e8701cb49ecd4aa17e.png" /></p><p></p><p>郭东白还重新解读了大模型时代下的架构师生存法则：</p><p></p><p>有唯一且正确的目标。大模型时代不能不定义出一个单一的、可量化的、且能够持续观测的、并且可以持续优化的成功目标。在有限资源下最大化经济价值。对我们所在的企业而言， 当下大模型不一定是最优解，但是大模型会以何种方式影响整个行业的成本和商业模式却必须慎重考虑。软件架构必须顺应技术趋势。需要思考“上个时代”的架构师，如何在大模型时代最大化借力？从原子价值单元开始对大模型做投入。大模型项目的原子价值单元指的是仅由大模型带来的能力，以原子价值单元投入需要锁定大模型的最适场景，仅对高回报场景做模型迭代，并最小化合规成本。</p><p></p><p>最后，郭东白总结道，在大模型时代，架构师的价值正发生变化。软件架构师在大模型时代可以弥补大模型的盲区，架构师需要确保架构活动有唯一的、可量化成功目标，要从市场竞争的角度思考大模型的经济价值，并要用最小价值单元开始探索大模型的应用。</p><p></p><p></p><h2>圆桌对话：大模型时代的数据智能新趋势</h2><p></p><p></p><p>在主论坛压轴的圆桌对话环节，AutoMQ 联合创始人 &amp; 首席战略官章文嵩、ProtonBase 研究员蒋晓伟、阿里云数据库产品事业部负责人李飞飞、蚂蚁集团 AI 安全商业化总经理张凯围绕“大模型时代的数据智能新趋势”主题展开了巅峰对谈。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b8/b85035ebe1921e6fb2206f77d909fa7f.png" /></p><p></p><p>围绕“大模型时代的数据平台趋势”话题，蒋晓伟提到了一个词，云原生分布式 Data Warebase。他认为，分布式 Data Warebase 是性能、正确性、实时性这三个业务核心需求的必然推论，它不是一个发明，而是一个发现。李飞飞表示，算力驱动与数据驱助力智能化时代加速进化，云原生与智能化推动结构化、半结构化、非结构化数据走向一体化、一站式处理。</p><p></p><p>在“大模型时代的数据 &amp;AI 基础设施”的讨论中，章文嵩提到数据是 AI 大模型的原材料，充分利用云原生数据基础设施和 AI 基础设施服务，高效构建垂直领域的数据集和 AI 应用。此外，大模型在带来前所未有的技术能力变革的同时，也带来了一系列安全问题，比如数据安全。张凯表示，AI 需要安全，安全需要 AI。</p><p></p><h2>现场回顾：创新浪潮中的思维碰撞</h2><p></p><p></p><p>大会现场气氛热烈，会场人头攒动，会展区观众络绎不绝。不少与会者表示，这次大会分享的内容不仅实用性强，更兼具深度与广度，真正做到了干货满满。我们深感荣幸与欣慰，感谢每一位参与者的支持与鼓励。正是有了大家的热情参与，我们才能不断前行，继续努力成为技术传播领域的佼佼者，持续提升内容质量，打造更加优质的交流平台，共同推动技术领域的创新与突破。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/85/853d0f68f166bb757fbba5edc3c8e201.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/3d/3dcf6fa8c0ad181a51140adf10661f90.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ae/ae8d20c4fcf5afa91e00c851c8fab69b.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/e2/e2239d12210e5660f1c58464d0981624.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/d7/d74bb9dd14a99fb027cc2cfe09c25d30.png" /></p><p></p><p></p><h2>精彩瞬间：活动亮点集锦</h2><p></p><p></p><h3>智能软件开发生态展</h3><p></p><p></p><p>本届大会对展区进行了重新规划，围绕“智能软件”的主题，广泛邀请生态上下游企业积极来到 QCon 现场展示最新的技术和产品。其中，展区共设置了操作系统、数据库、多模态、智能编码、数字人、模型广场 &amp; 管理 &amp; 调优、性能优化 &amp; 智能测试 &amp; 智能运维、AI Agent 应用及开发平台、AI 应用开发平台等多个细分主题，参会者可以沉浸式体验极具前沿性、互动性的生成式 AI 技术和产品。</p><p></p><p>今年，展区特别设置了【OpenTalk】交流区，广泛邀请参展企业、专家和开发者们分享自己的技术、产品和想法，围绕最新的技术趋势和未来发展畅所欲言。议题包括智能编码、大模型在数据分析领域的探索实践、AI 赋能数字化办公新纪元、让所有人不再为 SQL 问题头疼、AIGC 时代开发者画像分析等。</p><p></p><p>此外，现场还设计了 【寻找 SQL 优化大师】、【编程马拉松】、【TDengine 限时挑战赛】、【快问快答】、【幸运大转盘】 及多款小游戏挑战赛，并为参与者准备了手办盲盒、充电线、盆栽、背包等众多礼品。让我们一起回顾这些精彩瞬间吧！</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/48/48b9ac78c5bb8c42b66ef7a6f602ce35.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/74/7493e4937c4beb524fec4a43ddb6d72a.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7d/7dc9131d0022c317fb65166f468e31c8.png" /></p><p></p><p></p><h3>赞助商展示区：技术创新的支持者</h3><p></p><p></p><p>每一届 QCon 大会的成功举办，都离不开赞助商们的大力支持。正是他们的鼎力相助，让我们能够持续推动技术的交流与创新，为行业发展注入源源不断的动力。本次 QCon 大会得到了众多赞助商的大力支持，包括 Akamai、Elastic、支付宝小程序云、Cloudflare、Greptime、未来智能、伊克罗德、Coupang、IPIP、MongoDB、Palo Alto Networks、YDB、容联云、开放原子开源基金会等。他们的参与不仅为大会增色不少，也为技术共享和行业发展提供了坚实基础。接下来，让我们一同回顾这些令人难忘的精彩瞬间。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ac/ac3af628793217d77cf86e0f3571b40d.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/bf/bfb4bb82073b3f170adbcd70b20e6c8e.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/e5/e511ff9ebd89a987cf13d3ead5e36b10.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/86/861c60307b60019b64c8bc220e708e4e.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/0e/0ee5d80c5486a91206f26b651dc4790e.png" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/TbTZmHSARHQXiTmArO1v</id>
            <title>跟黎科峰、焦可、刘琼 、石建平、张俊九五位重量级大咖共话：Agent 是否是大模型落地的必经之路？</title>
            <link>https://www.infoq.cn/article/TbTZmHSARHQXiTmArO1v</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/TbTZmHSARHQXiTmArO1v</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Apr 2024 09:18:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, 大模型, AI应用之年, Agent
<br>
<br>
总结: 随着OpenAI发布ChatGPT，国产大模型竞争激烈，2024年被预测为AI的“应用之年”。大模型不仅要夯实基础，还要与业务场景结合，Agent成为热词。在大模型落地过程中，AI Agent的发展方向备受关注。 </div>
                        <hr>
                    
                    <p>随着OpenAI公司发布ChatGPT，国产大模型也如雨后春笋般喷涌而出，“百模大战”盛况吸引全球关注，大模型的发展速度日新月异。美东时间周四，OpenAI的首席运营官Brad&nbsp;Lightcap更是提出预测，2024年是AI的“应用之年”。</p><p>&nbsp;</p><p>现在，大模型早已从单一技术比拼，升级为整个体系生态的竞争。大模型企业决胜关键点不仅在于谁能夯实基础、系统布局，还在于谁能将大模型与业务场景有效结合，帮助企业用好大模型，真实产生落地价值。</p><p>&nbsp;</p><p>在这个背景之下，Agent成为了被提及最多的热词。在&nbsp;4&nbsp;月 11 日召开的&nbsp;QCon&nbsp;全球软件开发大会暨智能软件开发生态展上，【AI&nbsp;Agent&nbsp;智能体落地】专题也毫不意外地成为了爆场专题，吸引了数百名现场观众参与。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/14/14ef54851c0bdddf08531517c638d0e3.png" /></p><p></p><p>Agent应用是否是大模型落地的方向？它是否可以成功实现商业化落地？在此背景下，QCon&nbsp;联合数势科技特别策划了一场关于AI&nbsp;Agent发展的晚场圆桌讨论，旨在深入探讨AI&nbsp;Agent在大模型落地过程中的价值和发展方向。</p><p>&nbsp;</p><p>本场圆桌将于&nbsp;4&nbsp;月&nbsp;12&nbsp;日晚上&nbsp;18:00-20:00&nbsp;在&nbsp;QCon&nbsp;会场【国宾&nbsp;1&nbsp;厅】举行，我们邀请到了数势科技创始人 &amp; CEO 黎科峰博士，百川智能联合创始人焦可，腾讯研究院副院长刘琼，蓝驰资本投资合伙人、TGO 鲲鹏会（北京）学员石建平，实在智能联合创始人张俊九&nbsp;共&nbsp;5&nbsp;位重磅大咖，就以下核心议题发表自己的见解和洞察：</p><p>&nbsp;</p><p>大模型无共识，怎么看待中国在大模型领域所处的阶段，现状和机会，以及未来中国特色的可能的发展方向？大家认为Agent是什么，相较Prompt Engineering的区别，为什么它是大模型落地的主战场？现在很多号称做大模型的，但实际上仔细思考就会发现，很多场景和需求的解决方案没有革新，只是装了“大模型”的套子，那这些是否有“新瓶装旧酒”之嫌？以及当下大部分场景需求，小模型能力就够了？未来Agent要到现在的时代下App一样普遍且百花齐放，还需要多久？那个时候这个市场大概是什么样的？以及生产关系如何？</p><p>&nbsp;</p><p>如果你在&nbsp;QCon&nbsp;大会现场，欢迎你线下加入我们，一同深入探讨&nbsp;AI&nbsp;Agent&nbsp;和大模型的现在与未来。扫描下方二维码，即可免费报名参与活动⬇️&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/9e/9e5e6c8687a54c1d0261ce5abc7dae91.jpeg" /></p><p></p><p>如果你对本场圆桌议题感兴趣，但由于行程安排无法现场参与，也可以在线上参与讨论。届时，本场圆桌将在InfoQ和数势科技的视频号进行同步直播，无论是线上还是线下的朋友，都有机会畅所欲言。现场观众将有机会提问，而线上观众的提问将由专门同事收集汇总，后续进行解答。</p><p>&nbsp;</p><p>扫码预约线上直播⬇️&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/a4/a419b214ed80324b667ea12b0e87fe40.jpeg" /></p><p></p><p>QCon&nbsp;和数势科技诚邀广大&nbsp;AI&nbsp;爱好者、行业专家和投资者共同参与这场思想的盛宴。无论您是对AI&nbsp;Agent的未来发展充满好奇，还是希望了解大模型技术的最新趋势，相信这场圆桌讨论都能为您提供宝贵的信息和洞见。为了方便众多关注大模型和&nbsp;AI&nbsp;Agent&nbsp;的朋友们持续交流和讨论相关议题，我们也特别设置了【AI&nbsp;Agent&nbsp;话题】交流群。欢迎大家扫码加入我们的讨论群，与行业大咖一起探索AI&nbsp;Agent的未来。⬇️&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/b2/b2fcd3aa2ba93fa88723c4ed44dfc831.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/DKSGEWAvYIJOcFekjKbH</id>
            <title>被性别歧视、陷经济窘境，AI女神李飞飞自述：我要打破算法中的偏见</title>
            <link>https://www.infoq.cn/article/DKSGEWAvYIJOcFekjKbH</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/DKSGEWAvYIJOcFekjKbH</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Apr 2024 08:01:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 李飞飞, 移民, 父母, 性别歧视
<br>
<br>
总结: 李飞飞是一位成功的科学家，她的成功背后有着移民经历和父母的影响。在她的成长过程中，她面对过性别歧视，但她的父母给予了她独特的教育方式，培养了她强烈的好奇心和探索精神。通过父母的影响和自身努力，李飞飞克服困难，取得了在科学领域的成功。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>李飞飞的简历非常亮眼：33岁获得斯坦福终身副教授职称，成为首位担任斯坦福大学人工智能实验室主任的女性。她是美国三院院士，也是现代人工智能的关键催化剂&nbsp;ImageNet&nbsp;创建者，前谷歌副总裁、谷歌智能云及人工智能及机器学习首席科学家。在一贯由男性主导的人工智能技术领域里，她的一系列开创性成就无疑是非常耀眼的存在。人们既惊叹于她在人工智能领域的诸多贡献，又为她的逆袭励志故事所感慨万分——在获得所有这些成功之前，李飞飞也曾经度过困顿艰辛的青少年时代：少年时期，她成绩出类拔萃但受到了老师的性别打击；移民初期，她的英语蹩脚，家庭经济拮据又遭遇母亲患病求医；求学期间，也曾多次面临科学抱负与现实生活之间的抉择。这样一个华裔移民少女如何冲破美国社会阶层桎梏，成为引领全球深度学习革命浪潮的“AI教母”？</blockquote><p></p><p></p><p>（下文整理摘编自《我看见的世界：李飞飞自传》，中信出版集团2024）</p><p>&nbsp;</p><p>“我不禁想到自己第一次来华盛顿时的情形，当年我还不知人工智能为何物，还没有进入学术界，与硅谷也没有任何联系。当时我的整个身份（至少对外部世界而言）可以用一个词来概括，那就是——移民。</p><p></p><p>像许多移民一样，我感到被各种纵横交错的文化鸿沟所束缚。一些鸿沟不可名状，另一些则清晰地横亘在我的面前，难以跨越。我是一名女性，而我所在的研究领域由男性主导，“帽衫男”一直是科学领域的典型形象，以至这个词现在已经没有任何讽刺的意味了。这么多年来，我一直在思考自己是否真正属于哪个世界。”</p><p>&nbsp;</p><p></p><h2>父母播下的种子</h2><p></p><p></p><p>我是独生女，出生在北京，但在千里之外的四川省省会成都长大。从名义上看，这里是母亲的老家，但其实她和家人也刚在当地定居不久。他们原籍杭州，&nbsp;20&nbsp;世纪&nbsp;30&nbsp;年代，抗日战争全面爆发，杭州沦陷，他们和成千上万的人一样被迫背井离乡。他们庆幸自己活了下来，却无法摆脱流离失所之痛，甚至连母亲这一代也受到了深刻的影响。</p><p></p><p>外祖父常常追忆动荡之前的往事，每念及此，总是痛心疾首。他在学校出类拔萃，本来前途无量，但为了养家糊口，不得不放弃。即便如此，他们还是陷入了多年的贫困之中。几十年来，他郁郁寡欢，无法释怀。这种情绪传递给了他的子女，也在某一天攫住了我：沉闷而无言，感觉家在他乡、活在别处。</p><p></p><p>如果一个孩子可以在完全没有成人监督的情况下设计出自己理想的父母形象，那么父亲绝对符合我的要求。这是我对他最高的赞美，同时也是最严厉的批评。</p><p></p><p>父亲的外表英俊整洁，但他最突出的性格特征是对任何严肃正经的事情都严重过敏，简直到了病态的程度。他一辈子都像个没长大的孩子，并对此毫无悔意。与其说他拒绝承担成年人的责任，不如说他似乎真的觉察不到自己已经成年，好像缺乏某种其他人与生俱来的基本感知力。他经常突发奇想、随兴而为。</p><p></p><p>在母亲生产那天，父亲却姗姗来迟，只是因为他一时兴起去公园观鸟，完全忘了时间。这次观鸟迟到事件让他想到了用“飞”字来作为我的名字。“飞飞”成了我名字的不二之选。</p><p></p><p>这个名字恰好男女通用，也反映出父亲甚至对性别这种在中国传统文化中至关重要的概念都毫不在意。此外，我们这一代人很少有叫“飞”的，正好符合父亲标新立异的风格。</p><p></p><p>在我童年时期，父亲找来各种零件，自己动手组装了一辆带挎斗的自行车，把我放到挎斗里，穿过成都拥挤的街道，带我到公园或偏远的乡村。我们会花好几个小时捉蝴蝶，观察水牛悠然地躺在被水淹没的稻田里，或者捕捉野生啮齿动物和竹节虫，把它们带回家当宠物。</p><p></p><p>就连外人也能明显看出，我们之间没有传统父女间的等级关系，因为他更像我的同龄人，而不是父亲，在他身上完全看不到为人父的压力和焦虑。</p><p></p><p>父亲那种乐在其中、心无旁骛的专注状态让我明白，无论他是有女儿、有儿子，还是根本没有孩子，他都会这样度过午后时光。正因为如此，他为我树立的榜样才更有感召力。在不知不觉中，他向我展示了最纯粹的好奇心。</p><p></p><p>父亲带我出去玩，不是为了教给我什么东西——他喜欢大自然，但并不是专家——可这种经历却在我心中播下了哲学的种子，成为塑造我人生的最大力量：我对探索自己视野以外的事物产生了永不满足的渴望。</p><p></p><p>如果说我强烈的好奇心源自父亲，那么为这份好奇指明方向的人则是母亲。</p><p></p><p>跟父亲一样，母亲的个性也源于自我认知与社会期待之间的矛盾。父亲是迷失在成年人身份中的孩子，而母亲则是困囿于平庸生活的知识女性。但她意识到想象力并不受现实世界的限制，因此自幼就沉浸于书海之中。读书为她开启了一扇窗，让她了解自己无法到访的地方、无法感受的生活、无法经历的时代。</p><p></p><p>母亲热切地与我分享她对书籍的热爱，就像父亲分享他对大自然的喜爱一样。她鼓励我广泛阅读各种类型的书。所以，我不仅熟读鲁迅的作品和《道德经》等道家经典，也如数阅读了《第二性》《双城记》《老人与海》《基度山伯爵》等西方经典的中文译本。</p><p></p><p>外祖父母对我的培养方式也契合了父母的价值观。他们并不认同他们这代人中盛行的重男轻女的观念，而是跟父母一样，鼓励我展开想象，并坚守原则：我首先是个独立的个体，其次才是个女孩。跟母亲一样，他们给我买了很多书，涵盖海洋生物、机器人和中国神话等各类广泛的主题。</p><p></p><p>直到长大后，我才意识到，原来我们家门口以外的世界可能更加纷繁复杂。</p><p>&nbsp;</p><p></p><h2>科学是男孩的游戏？</h2><p></p><p></p><p>令人快意的校园学习时光在一个下午戛然而止——至少对我来说是如此。小学的最后一年即将结束，在平淡无奇的一天，老师在下课时提出了一个奇怪的要求：女生先回家，男生在座位上多坐几分钟。我顿时好奇了起来，于是在教室门口徘徊，藏在了一个能听到老师说话的地方。我听到的那些话让我终生难忘。</p><p></p><p>“我让女同学先走，是因为现在我要告诉你们：你们的整体表现是不行的。男孩天生就比女孩聪明，数学和科学就是体现你们脑子灵光的基础学科。你们的平均成绩竟然比女生还低，这种情况没有任何借口。我今天对你们非常失望。”</p><p></p><p>接下来，也许是觉得有必要鼓励一下大家，老师的语气似乎缓和了一些：“但你们也不要自暴自弃。等到了十几岁，你们会发现，周围的女生自然就变笨了。她们后劲不足，成绩会不断下降。即便如此，我还是希望你们都能更加努力，发挥你们作为男生的潜力。落在女生后面是不可接受的，大家明白了吗？”</p><p></p><p>我愣了一会儿才反应过来。在此期间，我的脑子中冒出无数个问题：老师真的相信男生天生脑子更好使吗？我们女生真的会长大就变笨吗？难道所有老师都是这么看我的？他们一直都是这样想的吗？我该怎么理解说这些话的竟然是一个……女老师？</p><p></p><p>又过了一会儿，种种疑问被另一种感觉所替代，它沉重而强烈，从我体内不知何处升腾而起。这种感觉不是气馁，甚至不是感到被冒犯，而是愤怒。</p><p></p><p>这是我不熟悉的愤怒之感——是一股悄然而炽烈的怒火，一种我从母亲身上见过的愤慨，但它无疑是属于我自己的。</p><p></p><p>老师的这番话并不是性别歧视的第一个迹象，大多数迹象都非常隐晦，甚至难以辨别，比如我会隐约感觉到，在数学和科学方面，老师更愿意鼓励男生。</p><p></p><p>还有一些区别对待则是不加掩饰的。比如有一次我报名参加一年级的足球比赛——不是“男队”，而是校队——结果却被告知女生不能参加。</p><p></p><p>老师的话虽然让我震惊，但并没有让我气馁。相反，这些话强化了我成长过程中形成的理念：无论周围有什么障碍，都要奋力超越现实，构想出更加广阔的未来。现在我不仅想看得更远，还想走得更远。如果说数学和科学这类领域是属于男生的游戏，那又怎样，学习毕竟不是球赛，他们无法阻止我在这里上场参赛，我暗下决心，一定要赢。</p><p></p><p>后来，我进入了一所吸引全市优秀学生的中学。在那几年里，对女孩的预设和偏见让我越来越不耐烦，这种情绪已经超出了课业的范围。</p><p></p><p>在同龄人中，我已经有“假小子”的称号，但老师的话仍然在我的记忆中回响，使我把一开始的怪癖上升到了个人使命的高度。</p><p></p><p>像任何喜欢把生活想象成电视剧的青少年一样，我很容易认为在与中国的性别规范做斗争的过程中，自己是在孤军奋战。我把头发剪得极短，拒绝穿裙子，和一群骑单车、爱打闹、聊战斗机而不是校园八卦的男同学混在一起，全身心投入出乎他人意料的兴趣中，尤其是航空航天科学、高超声速飞机的设计，甚至还有不明飞行物等超自然话题。</p><p></p><p>母亲是我坚实的守护者。当她觉得自己的价值观——我们的价值观——受到质疑时，她会毫不犹豫地进行防卫。我的中学老师就领教过她的厉害，并且这次令人难忘的会面，似乎直接改变了我的命运。</p><p></p><p>“您女儿特别聪明，这一点毫无疑问。但我担心，她对自己的前途不够严肃。比如，期末考试越早开始准备越好，所以我经常要求每个学生都跟全班同学分享自己正在读的书。大部分同学分享的都是教科书、备考资料和学校推荐的阅读书目。但是，飞飞这周推荐的书让我很担心，而且……”</p><p></p><p>老师话音未落，母亲就插话道：“我女儿从小就特别爱看书。”她对自己的轻蔑态度毫不掩饰。</p><p></p><p>“问题就出在她读的这些书上。你看看，《不能承受的生命之轻》？勃朗特三姐妹的书？还有她订的这些杂志，又是关于海洋生物的，又是关于战斗机的，还有不明飞行物的……例子太多了。她没有重点阅读符合课程价值观和理念的文学作品。”</p><p></p><p>“是吗？所以呢？”</p><p></p><p>在接下来的片刻沉默中，我坐在母亲身边，竭力不让血管里流淌的喜悦流露在脸上。紧张的气氛又持续了一两分钟，然后老师向前倾身，做出最后一次尝试，声音里多了一丝严厉。</p><p></p><p>“我就直说了吧。您的女儿也许真的挺聪明的，但班上聪明的学生并不少。智力只是成功的一个因素。另一个因素是要有纪律性，要把个人兴趣放到一边，专心学习对未来最有用的东西。”</p><p></p><p>我不确定母亲接下来的话是不是一种回应。她低下头，声音比之前更轻了。“这是飞飞想要的吗？这是我对她的期望吗？飞飞，你和我一样，都不属于这里。”</p><p></p><p>改变在1992年到来，我刚满15岁不久，随着父母来到了大洋彼岸——美国的新泽西州定居。</p><p></p><p></p><h2>十字路口的抉择</h2><p></p><p></p><p>在得知我被普林斯顿大学以近乎全额的奖学金录取时，虽然母亲表现得非常冷静，时隔数年，我才真正完全理解这一刻对母亲和这个家庭的重要意义。</p><p></p><p>母亲生命中的每一个里程碑都在提醒她，她站在了那些无法弥合的鸿沟的错误一边。几十年来，她已经习惯了假装自信，但我知道，她从未真正感受到自信。现在，也许是有生以来第一次，她终于有理由相信这个故事可能没有如此简单。她已经押上了所有，至此才有了一种真正如释重负的感觉。</p><p></p><p>直到1999&nbsp;年，我在普林斯顿大学的学习生涯即将结束，再次面临科学抱负与现实生活之间的抉择。读研的诱惑与开启职业生涯的压力让我左右为难。这次是一个真正的两难困境：</p><p></p><p>母亲的健康状况日益恶化，经营洗衣房的劳累和家庭的债务都在给她增加着巨大压力。而华尔街巨头可以提供了一切：福利、晋升机会、令人艳羡的起薪，当然还有真正的医疗保险。他们承免除我们的债务，为我的家庭提供保障。而对我的唯一要求就是放弃科学。</p><p></p><p>“飞飞，这是你想要的吗？”</p><p></p><p>“你知道我想要什么，妈妈。我想成为一名科学家。”</p><p></p><p>“那还有什么好说的呢？”</p><p></p><p>面对我的含糊其词，母亲的回应总是一针见血，速度之快让我得花点儿时间才能反应过来。三步绝杀，一剑封喉。我要去读研究生了。</p><p></p><p>而两年后，坏消息还是再次出现。选择兼修神经科学和计算科学的研究生课程，已经让我的体力和毅力达到了崩溃的边缘，在这个时候得知母亲患上充血性心力衰竭，我的感受复杂到无法用语言来表达。</p><p></p><p>一个新的现实正在浮现，它如此复杂，动摇了我以物理学专业学生的身份走进普林斯顿大学报告厅以来所做的每一个决定。毕生的好奇心把我带进了一个竞争激烈、薪酬低廉、无法保障长久职业生涯的领域，而我的父母现在需要我无法提供的支持。</p><p></p><p>我每天都在追求自己的梦想，这让我觉得自私至极，甚至过于鲁莽。我的实验室伙伴大多来自中产阶级，有些甚至家境非常富裕。我越是反思与他们家庭之间的差异，就越难以否认这样一个事实：成为科学家是一种奢望，我负担不起。</p><p></p><p>几周后，一位同学提到，世界知名管理咨询公司麦肯锡的合伙人来招聘了。他们正在寻找一个实习级别的分析师，常春藤盟校里只要跟数学和计算机科学有一点联系的研究人员，都可以成为理想的候选人。在真正绝望的时刻，这似乎是一个值得考虑的机会。</p><p></p><p>当然，我以前也经历过这种情况。我的学术目标和现实生活之间一直存在冲突，我很想把这次事件也当成最近的一次小冲突。但这一次，我内心科学家的声音与以往不同。在母亲的健康状况受到新一轮的打击后，它变得不那么坚定，就连我内心那个特殊而戒备的部分也开始屈服了。</p><p></p><p>面试后麦肯锡公司立即给了我肯定的回复，并决定将我的实习机会转为长期的正式职位。</p><p></p><p>我的心中五味杂陈，难以言表。一方面，我将要抛下我所研究和热爱的一切，另一方面，我亲眼看到父母多年来濒临绝境，越来越觉得他们是为了我才做出这么大的牺牲。这份工作似乎让我终于可以卸下长久背负的重担，为了我能来美国，母亲已经付出了一切。我知道现在是她最需要我的时候。</p><p></p><p>我跟母亲说了面试、工作机会和其他所有的一切，告诉她待遇、起薪，以及在我还没来得及答复之前，他们就已经提出了优厚的待遇。我解释说，无论从哪个角度来看，这都是通往每个移民母亲都希望自己孩子拥有的职业生涯的捷径。她礼貌地听着，但我还没说完，就在她脸上看到了那种熟悉的表情。</p><p></p><p>“我们真的要再次讨论这个问题吗？我了解自己的女儿。她不是管理顾问，或者其他什么职务。她是个科学家。”</p><p></p><p>“想想你的身体吧，妈妈！想想我们的开销。搞学术能给我们带来什么呢？”</p><p></p><p>“飞飞，我们走到这一步，不是让你现在放弃的。”</p><p></p><p>“这不是放弃！这是我梦寐以求的工作，一份事业，可以让我们摆脱目前的困境。看看我们现在活成什么样了！三个大人住在一个宿舍里！”</p><p></p><p>母亲停顿了一会儿，也许是在思考这些话，然后回答说：“飞飞，你一直在说自己走的路很‘自私’，就好像你追求科学是在牺牲我们一样。”</p><p></p><p>“我怎么能没有这种感觉呢？我现在本来可以养活咱们全家，而且……”</p><p></p><p>“你没明白我的意思。这从来就不是你一个人的路。从一开始，这就是我们全家的路。不管你是注定要成为科学家、研究员，还是其他我没有办法想象的职业，也不管你能不能从中赚到钱，从我们的飞机离开上海的那一刻开始，我们全家就一直在为这个目标努力。”</p><p></p><p>我不知道该说什么。</p><p></p><p>“我再说最后一次：我们走到这一步，不是让你现在放弃的。”</p><p></p><p>她是对的。她总是对的。这一次，不知什么原因，我终于听进去了她的话。并且我再也没有质疑自己的道路了。</p><p></p><p></p><h2>她们，同样属于这个时代</h2><p></p><p></p><p>2015年，图像分类本已是老生常谈的成功技术，但技术接二连三出现失误：将达豪集中营大门的照片标记为攀爬架，把一位脸上涂有彩色粉末的白人妇女贴上了“猿”的标签。虽然事故并非恶意，但这并不能让人感到宽慰。相反，无心之失所揭示的问题才更加令人不安：包括ImageNet&nbsp;在内的数据集由于缺乏多样性，导致了一系列意料之外的结果，未经充分测试的算法和存疑的决策又进一步加剧了负面影响。当互联网主要呈现的是以白人、西方人和男性为主的日常生活画面时，我们研发出的技术确实很难理解其他人群。</p><p></p><p>正如记者兼评论员杰克·克拉克（&nbsp;Jack&nbsp;Clark）所言，问题的根源在于人工智能“男性之海”问题：科技行业的代表性不足，导致算法无意中带有偏见，在非白人、非男性用户身上表现不佳。</p><p></p><p>从代表性问题的出现，到问题被大众真切地感受到，中间往往需要几年的时间。因此，我和几位伙伴联合创立了非营利教育组织&nbsp;AI4ALL，推动向处于高中阶段的女生、有色人种和其他未被充分代表的群体，开放斯坦福大学人工智能实验室课程，提高&nbsp;STEM（科学、技术、工程与数学）领域的包容性。</p><p></p><p>现在只是迈出了一小步，但我们实现了从无到有的跨越。只需要一点点努力，就可以让每个一直被历史排除在外的参与者相信，她们同样属于这个时代、这个领域。</p><p></p><p>此外，项目还能带来一丝安慰——在业界追逐人工智能未来时，往往肆意而为，缺乏自省，而我们的努力能够保证，至少有一小部分人在逆向而行。</p><p></p><p>2016&nbsp;年，我迎来&nbsp;21&nbsp;个月的学术休假，暂时离开教授职位。经过再三考虑，我最终决定接受谷歌云的人工智能首席科学家一职。我还碰巧认识公司新任命的谷歌云首席执行官黛安娜·格林（&nbsp;Diane&nbsp;Greene），是为数不多征服硅谷的女性，我期待在性别比例极不平衡的行业里与她并肩工作。</p><p></p><p>现在回顾我的职业生涯，这段漂洋过海的经历给我留下了深刻的烙印。然而直到现在我才意识到，这种烙印持续影响我的研究和思考：最好的作品总是在边界上诞生，在那里，思想永远被困在来去之间，由陌生土地上的陌生人探索，既是局内人又是局外人。但这正是我们如此强大的原因。独特的身份让我们保持独特的视角，赋予我们自由挑战现状的能力。</p><p></p><p>作为一个女儿、科学家、移民和人道主义者，我看到了众多不同的世界，但最重要的世界是我将不会生活在其中的世界，是建立在我现在所做的一切之上的世界，是我倾注了所有爱和希望的世界，也是我最为感恩的世界。正是因为这个世界的存在，我现在所做的一切才有意义。这个世界就是我的孩子们和他们的孩子们将继承的世界。在人工智能时代，做母亲是最令我谦卑的体验，我相信，这也将永远是独属于人类的体验。</p><p>&nbsp;</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ykFwfTeGv0OxOA9TfC8k</id>
            <title>Kyligence 发布企业级 AI 解决方案，Data + AI 落地迈向新阶段</title>
            <link>https://www.infoq.cn/article/ykFwfTeGv0OxOA9TfC8k</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ykFwfTeGv0OxOA9TfC8k</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Apr 2024 07:48:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Kyligence, AI, 指标平台, 企业级解决方案
<br>
<br>
总结: Kyligence在2024数智论坛发布了全新的企业级AI解决方案，包括智能一站式指标平台和AI数智助理，助力企业在数智化浪潮中掌握先机。企业需要将AI结合业务落地，Kyligence提供准确、可靠的Data+AI落地应用，帮助企业实现统一的数据语言和目标管理。企业在落地AI应用时需要关注核心业务流程，构建自己的数据壁垒，让AI更好地服务企业，提升数据决策效能。 </div>
                        <hr>
                    
                    <p>4 月 11 日，<a href="https://www.infoq.cn/article/gqZaz5RWuh4yYFjDNGTd?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Kyligence&nbsp;</a>"2024&nbsp;数智论坛暨春季发布会成功召开。Kyligence&nbsp;正式发布全新的企业级&nbsp;AI&nbsp;解决方案，基于服务金融、零售、制造、医药等行业领先客户的落地实践，Kyligence&nbsp;为企业提供准确、可靠、智能的&nbsp;AI&nbsp;+&nbsp;<a href="https://www.infoq.cn/article/AugPNHNf201DVzLGSXkU?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">指标平台</a>"一站式解决方案，以行业领先的技术和稳定可靠的产品助力更多客户在数智化浪潮中掌握先机。来自德勤和&nbsp;Kyligence&nbsp;的多位嘉宾分享了&nbsp;Data&nbsp;+&nbsp;AI&nbsp;现阶段在企业场景中落地的痛点，并带来&nbsp;AI&nbsp;+&nbsp;指标平台在金融、零售、制造、医药等行落地的最新成果，吸引了众多观众的参会与热烈讨论。</p><p></p><p></p><h4>准确、可靠的&nbsp;AI，Kyligence&nbsp;AI&nbsp;解决方案正式发布</h4><p></p><p></p><p>随着大模型的迅速发展，企业逐渐从聚焦技术转向关注应用，迫切需要将&nbsp;AI&nbsp;结合业务落地，在市场竞争中抢占先机。Kyligence&nbsp;联合创始人兼&nbsp;CTO&nbsp;李扬提到，2023 年&nbsp;Kyligence&nbsp;产品全面集成&nbsp;AI&nbsp;能力，推出了智能一站式指标平台&nbsp;Kyligence&nbsp;Zen&nbsp;和&nbsp;AI&nbsp;数智助理&nbsp;<a href="https://www.infoq.cn/article/aCJ0dhxWzo6exYQgYBsZ?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Kyligence&nbsp;Copilot</a>"，为企业使用数据带来了革新体验，并已率先在金融、零售、制造、医药等客户的真实场景中落地。</p><p></p><p>基于技术沉淀、创新产品和实践经验，Kyligence&nbsp;正式发布了&nbsp;AI&nbsp;解决方案，将为企业级客户提供准确、可靠的&nbsp;Data&nbsp;+&nbsp;AI&nbsp;落地应用，通过对接企业已有的数据源，智能一站式指标平台将帮助企业实现统一的数据语言和目标管理，以及服务型的数据治理；其配备的&nbsp;AI&nbsp;数智助理将进一步降低业务用户使用数据的门槛，助力业务人员进行快速、准确的决策，为业务创新提供数据支持；此外，Kyligence&nbsp;独具技术优势的企业级&nbsp;OLAP&nbsp;平台更将为企业大规模使用数据、推广&nbsp;AI&nbsp;应用提供坚实的技术底座。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7e/7e610378a63987b675dc80370ca1851d.png" /></p><p></p><p>在本次演讲中，Kyligence&nbsp;CTO&nbsp;李扬还分享了&nbsp;Kyligence&nbsp;在部分领先企业落地&nbsp;Data&nbsp;+&nbsp;AI&nbsp;的最新成果。在某头部城商行的真实场景中，通过&nbsp;AI&nbsp;自然语言交互，Kyligence&nbsp;满足银行总分行在灵活报表等场景的分析需求，释放数据开发人力资源，提升数据使用效率，其中&nbsp;AI&nbsp;对话准确率可达到&nbsp;95%&nbsp;以上，100%&nbsp;可解释，为&nbsp;AI&nbsp;进一步在银行进行大规模的推广和应用打下了坚实的基础。</p><p></p><p></p><h4>落地&nbsp;AI&nbsp;应用，让数据真正服务好业务</h4><p></p><p></p><p>大模型时代，技术快速发展，企业面对随之而来的巨大发展机遇，在抱有极高预期的同时，也存在信心不足的情况。德勤中国数智研究院主管合伙人尤忠彬带来了《以“内功不变”应“时代万变”——大模型时代企业的制胜之道》的精彩分享，他指出，在快速变化的&nbsp;AI&nbsp;时代，企业应当保持定力，持续围绕战略、人才、风险、数据、生态等“不变”关键要素重点建设。</p><p></p><p><img src="https://static001.geekbang.org/infoq/79/79a2af7a55b945d1490eeaea5af35c62.png" /></p><p></p><p>在落地&nbsp;AI&nbsp;应用时，企业需要围绕核心业务流程挖掘“小切口、大纵深”的大模型应用场景，搭建变革业务模式的“杀手级场景”&nbsp;；与此同时，大模型仍然依赖大规模的训练数据，尤其是高质量的数据；企业需要更加关注构建自己的“数据壁垒”，将业务数据沉淀为指标资产。</p><p></p><p></p><h4>让&nbsp;AI&nbsp;用起来，金融、零售、医药落地&nbsp;Data&nbsp;+&nbsp;AI&nbsp;成果</h4><p></p><p></p><p>聚焦&nbsp;AI&nbsp;+&nbsp;指标平台在不同客户的实际落地场景，Kyligence&nbsp;解决方案与服务总监甘甜带来了《智能一站式指标平台在头部企业的实践分享》的精彩演讲，公开了&nbsp;Kyligence&nbsp;助力银行、零售、医药等头部企业落地&nbsp;AI&nbsp;解决方案的真实案例，深入分享了&nbsp;Kyligence&nbsp;如何解决企业级生产场景下准确性、安全性、成本和&nbsp;AI&nbsp;治理等问题，让&nbsp;AI&nbsp;更好地服务企业，从而提升数据决策效能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/74/74d503a96120290115986e7faaadc482.png" /></p><p></p><p>甘甜在演讲中介绍，Kyligence&nbsp;携手某头部跨国药企打造了精准、敏捷、全面、智能的商业分析平台，以&nbsp;AI&nbsp;+&nbsp;指标平台这一全新的形式满足业务部门复杂多样的分析需求，提高了商业洞察的质量和决策效率；Kyligence&nbsp;助力合作多年的国内顶流餐饮连锁企业升级了指标平台，结合最新的生成式&nbsp;AI&nbsp;技术，在一线人员范围推广并使用&nbsp;AI&nbsp;数智助理，进一步降低业务人员使用数据的门槛；基于服务多年金融客户的经验和结合&nbsp;AI&nbsp;的智能一站式指标平台，Kyligence&nbsp;服务头部银行创新分析对公存款大额异动、绩效分析、贷款业务管理等场景，让银行分支行也能享受数智化转型的最新成果，真正将&nbsp;AI&nbsp;用起来。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ar7Rh2iQVmC77t7yKsoU</id>
            <title>蚂蚁集团 CodeFuse 发布“图生代码”功能，支持产品设计图一键生成代码</title>
            <link>https://www.infoq.cn/article/ar7Rh2iQVmC77t7yKsoU</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ar7Rh2iQVmC77t7yKsoU</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Apr 2024 05:01:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 蚂蚁集团, CodeFuse, AI 编程, 图生代码
<br>
<br>
总结: 蚂蚁集团推出自研智能研发平台CodeFuse，新增“图生代码”功能，支持开发人员用设计图一键生成代码，提升前端页面开发效率。AI编程助手趋势下，CodeFuse覆盖研发全链路，帮助开发者更快编写代码。CodeFuse在蚂蚁内部得到广泛应用，生成的代码整体采纳率为30%，助力降低开发工作量。AI的普及降低编程门槛，推动软件开发行业创新，CodeFuse致力于打造创新解决方案，持续推出新功能提升研发效率。 </div>
                        <hr>
                    
                    <p>4 月 11 日，<a href="https://www.infoq.cn/article/bjCH8kMloxFUfp00WQIX?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">蚂蚁集团</a>"自研的智能研发平台<a href="https://xie.infoq.cn/article/5bca12068d34015544e2d5eae?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search"> CodeFuse </a>"推出“图生代码”新功能，支持开发人员用产品设计图一键生成代码，大幅提升前端页面的开发效率。目前相关功能正在内测。</p><p>&nbsp;</p><p>和很多互联网公司一样，蚂蚁集团正在内部全面推行 AI 编程，使用 CodeFuse 支持日常研发工作的工程师达到 50% 以上，这些工程师提交的代码中 10% 由 AI 生成。</p><p>&nbsp;</p><p>Gartner 发布的 2024 年十大战略技术趋势指出：到 2028 年，75% 的企业软件工程师将使用<a href="https://www.infoq.cn/article/LfOxXc1gbayiD4WQ0u6l?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search"> AI 编程</a>"助手。蚂蚁 CodeFuse 就是这一趋势下的探索尝试。据介绍，CodeFuse 的功能覆盖了需求分析、编程开发、测试与构建、发布与运维、数据洞察等研发全链路，比如在开发测试阶段，通过代码补全、添加注释、解释代码、生成单测、代码优化等，帮助开发者更快、更轻松地编写代码。</p><p>&nbsp;</p><p>目前，在蚂蚁内部，每周已有超五成程序员在日常研发中使用 CodeFuse。CodeFuse 生成的代码整体采纳率为 30%，在生成单元测试场景采纳率可以达到 50%。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5c/5c8cc4cd40f7c39e4947360bf868bc93.png" /></p><p>（图说：蚂蚁智能研发平台CodeFuse覆盖AI研发全链路）</p><p>&nbsp;</p><p>此次发布的“图生代码”功能主要服务前端工程师。在互联网产品开发环节，设计师画出设计图后，前端工程师需要用代码实现产品设计图，这项工作占用了较多开发工作量。图生代码，可以根据设计图一键生成代码，可以极大降低开发团队在开发网页、小程序、APP 时的代码工作量。以一张中型网页为例，如果最终有 200 行代码，一人耗时约需1小时，一键生成后，工程师只需检查与调整，耗时大幅降低。这项功能基于蚂蚁百灵大模型的多模态技术能力研发。</p><p>&nbsp;</p><p>蚂蚁集团 CodeFuse 负责人表示，AI 的普及不仅可以减少开发人员的工作压力，让他们有更多精力投入到更有创造力的工作中去，更大的意义在于降低编程门槛，推动软件开发行业的创新和进步。CodeFuse 的使命是探索下一代 AI 研发生产力工具，致力于打造创新的解决方案，让软件开发者在研发过程中如丝般顺滑。在自然语言生成代码、图生代码之后，CodeFuse 还将持续推出新功能，助力企业研发全链路的效率提升。</p><p>&nbsp;</p><p>该负责人认为，AI 研发范式的变革，并不代表“人”在研发场景的角色会消失，反而对 AI 和人如何协同提出了更高的要求，特别是涉及可靠性的运维场景，还需要人工专家干预才能让系统健康运行起来。“AI 目前主要集中在辅助编程(code&nbsp;copilot)，要从 copilot 走向 co-worker，实现整个研发生命周期的智能化、自动化，还有很长的路要走。”</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/L4YFfW6DkuNzyP32JsYt</id>
            <title>钉钉卡位战：SaaS 挣不到的钱，Agent 会挣到</title>
            <link>https://www.infoq.cn/article/L4YFfW6DkuNzyP32JsYt</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/L4YFfW6DkuNzyP32JsYt</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Apr 2024 07:05:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, AI原生, 钉钉, GPT Store
<br>
<br>
总结: 2020年，不穷加入钉钉，面临大模型竞备赛。钉钉团队开始投入智能化，使用大模型重做高频产品。钉钉选择Agent作为AI原生方式。钉钉依赖大模型公司，定义为AI应用创作平台。GPT Store模式吸引用户，但目前应用难成高价值产品。不穷认为解决问题有价值，个人或企业创建AI助理需解决具体问题。 </div>
                        <hr>
                    
                    <p></p><blockquote>出品｜InfoQ 《大模型领航者》访谈主持｜霍太稳，极客邦科技创始人兼 CEO访谈嘉宾｜叶军（不穷），钉钉总裁作者｜褚杏娟</blockquote><p></p><p>&nbsp;</p><p>2020 年，刚带队做出全国第一张健康码的不穷加入了钉钉。不穷是阿里的第一位校招计算机博士，从 PC 时代开始触网，完整经历了移动互联网时代。而他如今面临的，是一场关于大模型的竞备赛。</p><p>&nbsp;</p><p>想必已经无需用过多笔墨赘述。2022年底至今，ChatGPT的出圈程度还没有谁能超越，其背后的技术方向也早已经被竞相追捧。就像不穷说的“大家都充满了 FOMO 情绪。”</p><p>&nbsp;</p><p>那么身处其中的钉钉，能够在这次浪潮扮演什么样的角色？</p><p>&nbsp;</p><p>和很多企业一样，钉钉最初也没有一下就找到合适的入局方式。一年多前，看到自己与微软不约而同地都选择了给当前产品增加AI能力时，钉钉团队觉得这种方式已经很让人眼前一亮了，大模型爆发的能量远比想象得还要大。</p><p>&nbsp;</p><p>整个2023年，OpenAI、微软、谷歌等大模型发布频繁，李开复、王小川等也亲自下场发布大模型……去年4月，钉钉全面投入智能化，开始用大模型逐个将高频产品重做。到了当年六、七月份，钉钉内部有人提出：能否有一个直接 AI 原生的产品？这引发了内部关于从“+AI”到“AI+”的争论。这个想法与去年10月李彦宏公开提到的AI原生理论异曲同工，可见国内的探索思路其实差异并不大。</p><p>&nbsp;</p><p>那什么是AI 原生？钉钉也在思索。其实在将思维转换成以AI为中心后，这个问题就不难回答。</p><p>&nbsp;</p><p>“AI 原生产品从一开始的思考就是全新的，就是要用纯AI的思路来解决一个任务。它从数据感知、任务分解，再到思维链，最后到行动执行，是一种全新的思考架构。”不穷表示。</p><p>&nbsp;</p><p>而对于做AI原生的方式，钉钉选了Agent。</p><p>&nbsp;</p><p></p><p></p><p></p><p></p><h2>“我们依赖这些大模型公司”</h2><p></p><p>&nbsp;</p><p>今年1月，OpenAI正式推出了GPT Store。几乎同时，钉钉发布了 AI 助理，并宣布4月推出AI助理市场。</p><p>&nbsp;</p><p>“GPT Store上线的第一天我就用了，当天就已经有非常多的产品了，速度非常快。但我分析了所有数据后发现，GPTs同质化非常严重，而且都是一些通过简单限定词、指令或角色扮演来形成的AI 助手。”不穷说道。</p><p>&nbsp;</p><p>这一体验让不穷认定，钉钉未来的AI助理市场不做全量推荐，只会推荐自己精选过的AI助理。</p><p>&nbsp;</p><p>不穷强调，钉钉模式与GPT Store的不同：GPT Store通过不断丰富插件使AI Agent 能够批量调用外部系统的能力，但它的组合效率要比钉钉低，因为钉钉拥有天然的 To B 环境，其中有大量的工作任务需要解决。</p><p>&nbsp;</p><p>不穷认为，目前钉钉的能力不在于做自己的大模型，而是在应用和数据，在工程性、用户体验以及 To B 理解方面。对于Agent 来说，大模型只是其中的一个能力，此外还需要非常好的场景和高质量的业务环境数据，这两者恰恰是钉钉有、而OpenAI 目前还欠缺的。</p><p>&nbsp;</p><p>对于与国内大模型公司的关系，用不穷的话说是：“我们依赖这些大模型公司，它们是我们的发动机和心脏。没有它们，我们无法运行。”</p><p>&nbsp;</p><p>面向AI，不穷把钉钉定义为AI应用创作平台，企业在这个平台上连接、开发和加工各种应用。钉钉的核心任务就是连接场景和数据，实现结构性自动化和批量处理各种工作，并通过Agent让创作变得更简单。</p><p>&nbsp;</p><p>具体来说，钉钉的职责是确保外部记忆存储部分的完善，包括短期和长期记忆的处理，同时做好任务规划，之后将大模型生成的内容与本地业务数据集成，并将形成的行动在各个系统中落地。</p><p>&nbsp;</p><p>在不穷看来，当前国内各个基础模型之间的差距并不大，未来不是每个开发者都会关心基础模型的选择，他们更注重解决业务场景中的问题。因此，如果基础模型效果不理想，开发者应该可以随时更换。</p><p>&nbsp;</p><p>因此，为快速上线和体验，钉钉选择了通义千问作为默认大模型，除此之外用户有需求时还接入了其他大模型公司的模型，如智谱AI、月之暗面、Minimax等。用户的业务逻辑可以建立在自己选择的基础模型体系上，业务流程和数据流也不会进入钉钉平台。</p><p>&nbsp;</p><p>“根据不同的场景和需求，我们可能还会推荐小模型或专用模型。”不穷说道。</p><p>&nbsp;</p><p>不穷在给用户提供模型的选择建议时，会提醒他们更加关注模型的性能，如每秒处理的token量；大模型的安全性问题等，如本地部署还是云上部署；工程解决方案的多重性和便捷性等。这些问题也是钉钉构建AI助理时实际遇到的。</p><p>&nbsp;</p><p></p><h2>“C 端还没有太多优秀的产品形态出现”</h2><p></p><p>&nbsp;</p><p>无疑，GPT Store 的模式吸引了大批用户：刚正式发布时，OpenAI 就宣称已经有超过 300 万个 GPTs。</p><p>&nbsp;</p><p>与传统软件相比，AI助理、GPTs等的不同之处在于拥有非常快的更新速度，模型、交互方式、数据和产品形态等方方面面都变得迅速，开发者也不要从头到尾进行开发和维护。这种模式还大大降低了开发门槛，没有研发背景的人也可以尝试，而对于研发人员来说则大大缩短了研发、测试等成本。</p><p>&nbsp;</p><p>一方面，这意味着传统软件的研发模式可能会面临变革；但另一方面，不穷也指出，百万千万级的GPTs目前看相对来说形式比较单一，没有传统软件那样强大的业务理解能力，因此目前 GPT Store 中的应用很难成为高价值产品。</p><p>&nbsp;</p><p>不穷认为，尽管 GPTs 的创建能力很强，甚至一天可以创建几十个，但它目前还代替不了传统软件市场。</p><p>&nbsp;</p><p>钉钉也在寻找有价值的产品。在1月份宣布启动的AI助理创造大赛上，目前有超过2000支队伍提交作品，不穷也会亲自体验这些AI助理，寻找优秀的作品。&nbsp;</p><p>&nbsp;</p><p>那么，个人玩家又如何在GPT Store 这种模式中赚到钱呢？</p><p>&nbsp;</p><p>不穷的答案是价值，“只要有价值就一定能挣到钱，只是迟早的问题。”在他看来，个人或企业创建AI助理的核心在于要解决具体的问题，解决问题本身就有价值。但现在“卖工具”的人可能不是最终解决问题的人，解决问题的人是那个场景中离问题最近的人。</p><p>&nbsp;</p><p>AI 助理的商业模式则与传统软件相似，需要一定的用户使用量，“只要使用量上去了，很快就会有开发者赚到第一桶金。”根据不穷的经验，一旦调用量达到百万次，软件做商业化就是必然的。</p><p>&nbsp;</p><p>这与之前钉钉在与IDC联合发布的《2024 AIGC 应用层十大趋势》中提到的观点“新一轮的AIGC之争，也将会是一场流量入口之争”是契合的。钉钉在其中也提到了有望成为超级App 的想法。</p><p>&nbsp;</p><p>“在目前的To C场景中，我还没有看到太多优秀的产品形态出现。”不穷说道，“但是，AI Agent 绝对不是自我陶醉，我相信一定会有出色的产品出来，应该给创新者更多的时间。”</p><p>&nbsp;</p><p>根据不穷之前的访谈，AI Agent和智能助理产品发展到一定程度后，中间态、碎片化的产品成为极简流量入口，就会出现“No App”理念重塑应用的情况：通过对话即可直接调取、使用各种工具，更多非软件专业人员也能获得强大的系统服务。</p><p></p><h2>“SaaS挣不到的钱，会通过Agent挣到”</h2><p></p><p>&nbsp;</p><p>相较C端，B端是不穷更看好的方向，因为 To B 场景更容易产生有效的产品：确定的数据和场景可以帮助解决大模型的幻觉问题，同时通过批量和自动化的方式提高 To B 常见工作流和任务流的效率问题。</p><p>&nbsp;</p><p>不穷指出，To B 软件的目的是解决问题，所以这里天然聚集了大量的问题和数据。在这样环境里构建的AI Agent，传统 SaaS 和 PC 软件软件的开发流程、产品交互、形态及维护等的缺陷都将得到弥补。未来，SaaS 的定制化或各种行业需求，都可以用简单、低成本的方式实现。</p><p>&nbsp;</p><p>“Agent 市场形态肯定会取代传统软件市场形态。更重要的是，它将取代传统SaaS产品的产品形态。”不穷说道。</p><p>&nbsp;</p><p>现在AI助理的 to B 服务中，钉钉要与用户频繁、深入地互动，根据反馈不断调整和改进。比如在解决一家芯片企业客服培训难题时，钉钉团队要去公司了解实际工作流程，然后将线下流程转化为线上的AI助理。</p><p>&nbsp;</p><p>虽然每个企业的需求相同，但产品会逐渐沉淀下来。企业自行完成标注、训练和本地化数据接入，钉钉则保留抽象层和公共层，逐渐完成产品的广泛行业适用性。</p><p>&nbsp;</p><p>不穷评价OpenAI做产品就像是科学家通过成千上万次的实验，最终找到一个正确的方向，逼近科学真理。而钉钉则投入大量时间与客户共创，解决他们的实际问题。两者虽然方向不一，但殊途同归。</p><p>&nbsp;</p><p>与大模型创业公司苦苦寻找自己的商业模式相比，钉钉探索出来的大模型商业模式已有三种。</p><p>&nbsp;</p><p>第一种是基于调用量的模式。无论个人AI助理还是企业助理，产品使用频率越高、解决问题的能力越强，吸引的用户就越多，自然也就需要更多的调用。使用量大，消耗的算力和资源也就越多。</p><p>&nbsp;</p><p>第二种是应用层本身带来的商业模式。传统的SaaS模式赚钱较为困难，因为它需要大量的定制和本地化需求，AI Agent的应用能力提供了一个解决方案：</p><p>&nbsp;</p><p>简洁的界面、任何需求都可以通过对话来理解，并通过行动系统对接外部系统逻辑，这样就将界面定制化和流程重构的职责就交给了后端模型和AI Agent系统。这样，从交互层到模型层，再到持久层，整个过程都得到了简化。因此，SaaS的维护成本也就降低了。</p><p>&nbsp;</p><p>“AI助理的盈利天花板目前还看不到，随着更多优秀产品的出现，我们可能还会发现新的盈利途径，带来新的惊喜。”不穷说道。</p><p>&nbsp;</p><p>不穷认为，通过消耗算力来提供服务只是最基本的模式，除此之外，服务消耗还有很多其他的可能性。他的判断是，未来十几年中国 SaaS 行业挣不到的钱可能会通过 AI Agent 来实现。</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>在提到当前钉钉AI助理接下来要重点攻关的方向时，不穷还是说到了数据和场景：</p><p>&nbsp;</p><p>数据和场景是Agent普遍存在的问题，钉钉的AI助理现在有更专注的场景和数据，就像是给“孙悟空戴上了紧箍咒”，好处是能够减少幻觉、能够解决一些通用场景里难以解决的问题。这也意味着，钉钉未来还需要发掘和洞察到更多的场景、沉淀和积累更多的高质量数据。</p><p>&nbsp;</p><p>其次，行动能力是目前Agent所欠缺的，只是让它们聊天未免太乏味。因此AI助理会接入钉钉上原有的应用、低代码等开放能力，不穷希望以此让AI助理能够不断出现各种创新玩法，而不仅仅是简单的信息查询和单向交互。比如，AI助理对接了很多的主流App行动系统，比如可以查看淘宝订单等，App的行动系统实际上就变成了一个AI助理，无需在不同系统间切换。</p><p>&nbsp;</p><p>在不穷看来，Agent 的最大好处就是它的无限可能性，这种模式不受传统思维和现有框架的限制，是真的可以让想象力转化为生产力的。</p><p>&nbsp;</p><p>“我今年非常期待行动系统能够变得更加强大、数据质量得到提升。随着越来越多的人洞察到新的场景，AI助理将不再是一个个废话大师、一个个应对亲戚的聊天工具、一个个面试官。”不穷说道。</p><p>&nbsp;</p><p>访谈里，不穷不掩对微软战略眼光的称赞。“战略需要耐心，如果没有耐心，那就只是投机。”同样地，钉钉对AI 助理的耐心有多久？AI 助理未来的价值能有多大？这些也是不穷现在要面对的课题。</p><p>&nbsp;</p><p></p><h4>栏目介绍</h4><p></p><p>&nbsp;</p><p>《大模型领航者》是 InfoQ 推出的一档聚焦大模型领域的访谈栏目，通过深度对话大模型典范企业的创始人、技术负责人等，为大家呈现最新、最前沿的行业动态和思考，以便更好地参与到大模型研发和落地之中。我们也希望通过传播大模型领域先进的实践和思想理念，帮助潜在大模型应用者、创业者、开发者等做好各类决策和选型。</p><p>&nbsp;</p><p>如果您有意向报名参与栏目或想了解更多信息，可以联系：T_demo（微信，请注明来意）&nbsp;</p><p></p><h4>活动推荐</h4><p></p><p>&nbsp;</p><p>除了叶军（不穷）的思考，钉钉 CTO 程操红（巴布）也将在明天（4月11日）开幕的<a href="https://qcon.infoq.cn/2024/beijing/?utm_source=wechat&amp;utm_medium=aipart1-0410">QCon 全球软件开发大会暨智能软件开发大会</a>"上分享《钉钉智能化之路：打造未来交互新形态，重塑组织效能》的主题演讲，分享大型平台智能化实践，探究 AI 产品到平台如何实现跨越。</p><p>&nbsp;&nbsp;</p><p>本次会议还邀请了微软（中国）公司首席技术官韦青，深圳开鸿数字产业发展有限公司高级副总裁、研发体系总裁王皓博士，Coupang 副总裁郭东白等专家带来精彩主题演讲。在圆桌论坛环节，ProtonBase 研究员蒋晓伟、 AutoMQ 联合创始人 &amp; 首席战略官章文嵩、 阿里云数据库产品事业部负责人李飞飞、蚂蚁集团 AI 安全商业化总经理张凯将带来关于「大模型时代的数据智能新趋势 」主题的前瞻视角。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/G5HpCLUNOGlB8fcqf48W</id>
            <title>共探AI+软件工程融合之道，AI4SE创新巡航活动启动征集</title>
            <link>https://www.infoq.cn/article/G5HpCLUNOGlB8fcqf48W</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/G5HpCLUNOGlB8fcqf48W</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Apr 2024 03:07:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, AI技术, 软件工程, AI4SE
<br>
<br>
总结: 随着大模型等AI技术在软件工程领域的应用不断拓展，企业在智能化软件工程落地过程中面临模型选择、工具需求和落地路径等挑战。为推动AI4SE行业生态建设，中国人工智能产业发展联盟AI4SE工作组等机构发起“AI4SE创新巡航”活动，旨在探讨如何应用大模型等AI技术推动软件工程智能化发展。 </div>
                        <hr>
                    
                    <p>随着大模型落地元年的到来，大模型等AI技术在软件工程领域的赋能将从深度和广度不断拓展。然而在智能化软件工程（AI4SE）落地应用过程中，企业面临模型选择难、工具功能需求不清晰、落地路径和场景不明确等困扰。</p><p>&nbsp;</p><p>为推动AI4SE行业生态建设，共同探讨如何应用大模型等AI技术，推动软件工程朝着智能化、高效率、低成本方向发展，中国人工智能产业发展联盟AI4SE工作组、中国信息通信研究院（以下简称“中国信通院”）、InfoQ极客传媒已于2023年底共同发起“AI4SE创新巡航”系列活动，并于2024年1月完成首期走进360的巡航。现启动征集“AI4SE创新巡航”第二期的标杆企业和研讨主题。</p><p></p><h4>活动目标</h4><p></p><p></p><p>旨在通过深入走访企业，交流探讨落地实践经验，增进各行业在智能化软件工程领域的交流与合作，共同构建协同发展的强大合力。</p><p></p><h4>征集时间</h4><p></p><p></p><p>即日起至2024年4月10日。</p><p></p><h4>活动预告</h4><p></p><p></p><p>活动第二站将于2024年4月举行，敬请期待。</p><p>&nbsp;</p><p>挚邀请各领域领军企业踊跃报名加入AI4SE工作组，共探共享行业优秀成果和丰富经验，携手推进智能化软件工程新发展。</p><p></p><h4>报名请联系</h4><p></p><p></p><p>中国信通院人工智能研究中心</p><p>&nbsp;</p><p>胡老师：17371328072（微信同号）</p><p>秦老师：13488684897（微信同号）</p><p>&nbsp;</p><p>中国人工智能产业发展联盟（AIIA）智能化软件工程工作组（AI4SE工作组），于2023年9月正式成立，旨在进一步发挥生成式AI、大模型等人工智能技术在软件工程领域的潜力，充分释放AI赋予软件工程的价值。AI4SE工作组已吸纳110+成员单位，覆盖金融、电信、软件等诸多行业，欢迎更多企业加入，请联系胡老师、秦老师。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d1140e58ac53abb164dfdbe91b4069ca.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/r1WvBQGA9GXAEcWimIb0</id>
            <title>谷歌、英伟达联手打造AI超级计算机架构，Agent业态初显且已商业化，谷歌的基础设施太全面了 | Google Next 2024</title>
            <link>https://www.infoq.cn/article/r1WvBQGA9GXAEcWimIb0</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/r1WvBQGA9GXAEcWimIb0</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Apr 2024 19:00:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 基础软件, TPU v5p, 生成式AI, 企业级场景
<br>
<br>
总结: 谷歌在Cloud Next ’24大会上展示了基础软件和TPU v5p支持下的AI超级计算机架构，能够满足生成式AI大语言模型和企业级场景的需求。与国内产品相比，谷歌的用例精细化能力相当高。同时，谷歌还宣布了AI Hypercomputer超级计算机架构，为生成式AI提供端到端的基础设施服务。 </div>
                        <hr>
                    
                    <p></p><blockquote>省流版：基础软件相当能打，新的TPU v5p支持下的AI超级计算机架构足以应对最严苛的生成式AI大语言模型和场景。Agent已经商业化，单就用例本身（视频智能生成、智能办公等等）国内均有类似形态的产品，没有“WoW”的感觉，但与国内同样产品相比，谷歌的用例精细化能力相当高，足以应对企业级场景的需求。</blockquote><p></p><p>&nbsp;</p><p>美国时间4月9日，Google Cloud Next ’24在拉斯维加斯正式召开。&nbsp;Google Cloud CEO Thomas Kurian 等带来了主题为「即刻踏上云端新旅程」的开幕演讲。</p><p><img src="https://static001.infoq.cn/resource/image/cd/56/cd820250fe5131393589f69f0b96da56.jpeg" /></p><p>在会议开始之前，媒体及分析师们均十分期待谷歌本次大会上与生成式AI相关的发布，包括今年引起巨大讨论的Gemini系列，以及vertexAI、硬件层面的更新等，包括是否会与英伟达最新发布的芯片合作等。</p><p></p><p>事实证明，谷歌已经围绕生成式AI构建起一整套的成熟架构，这是属于AI时代的超级计算架构。Thomas Kurian 表示自上次Cloud Next大会以来，谷歌已经进行了一千多次的产品更新，并声称这是世界上增长最快的云提供商。Alphabet首席执行官Sundar Pichai&nbsp;特别赞扬了Gemini系列为各地企业提供了很多发展机会。与此同时，本届大会展示了谷歌已经构筑起的生成式AI全景图。</p><p><img src="https://static001.infoq.cn/resource/image/db/e7/db321a2587356789d1d22d1166839de7.jpg" /></p><p></p><h2>基础设施看谷歌，联手英伟达，AI&nbsp;Hypercomputer超级能打</h2><p></p><p></p><h3>一系列芯片更新...</h3><p></p><p>为了支持当今企业中采用的日益强大的生成式人工智能模型，谷歌宣布全面推出迄今为止最强大、可扩展的张量处理单元-&nbsp;TPU v5p，其设计目的只有一个——训练和运行最苛刻的生成式AI模型。</p><p>&nbsp;</p><p>TPU v5p旨在提供巨大的计算能力，单个Pod包含8,960个协同运行的芯片，这是TPU v4 Pod数量的两倍多。谷歌方面表示，TPU v5p提供了令人印象深刻的性能提升，每秒浮点运算次数增加了一倍，每芯片的高带宽内存增加了三倍，从而大大提高了整体吞吐量。为了使客户能够训练和服务在大规模TPU集群上运行的AI模型，谷歌在Google Kubernetes Engine上添加了对TPU v5p虚拟机的支持，Google Kubernetes Engine是其用于运行软件容器的云托管服务。</p><p><img src="https://static001.infoq.cn/resource/image/df/20/df3d9yy9686e36b8f35ec4yy26722b20.jpg" /></p><p>如预期，谷歌与英伟达联手，用户可以使用英伟达的最新硬件在Google Cloud上训练其生成式AI模型。除了TPU系列之外，它还通过新的A3系列虚拟机提供对Nvidia H100 GPU的访问。A3 Mega VM将从下个月开始全面上市，其主要优势之一是支持“机密计算”，这是指即使在处理最敏感的数据时也可以保护其免受未经授权的访问的技术。</p><p>&nbsp;</p><p>这是一个极其关键的发展，这为生成式AI模型提供了一种方法来访问以前被认为处理风险太大的数据，而数据安全随着生成式AI的爆火而愈发重要。</p><p>&nbsp;</p><p>“Character.AI正在使用Google Cloud的Tensor处理器单元和在Nvidia H100 Tensor Core GPU上运行的A3 VM来更快、更高效地训练和推断LLM，”Character Technologies Inc.首席执行官Noam Shazeer表示。“在强大的人工智能优先基础设施上运行的GPU和TPU的可选性使Google Cloud成为我们显而易见的选择，因为我们需要扩展规模，为数百万用户提供新的特性和功能。”</p><p><img src="https://static001.infoq.cn/resource/image/aa/77/aa908385f6e03a761640c0e9ed92f577.jpg" /></p><p>与此同时，谷歌官宣了其宏伟计划-AI Hypercomputer，面向生成式AI时代的超级计算机架构，提供端到端的基础设施，从硬件到软件的一切服务，并宣布了Google Axion 处理器，这是谷歌首款专为数据中心设计的基于 Arm 的定制 CPU。Axion 提供业界领先的性能和能源效率，并将于今年晚些时候向 Google Cloud 客户提供。</p><p><img src="https://static001.infoq.cn/resource/image/e8/aa/e82e2fef87fbfb679c49f840aeed6faa.jpg" /></p><p>更令人兴奋的是谷歌将在今年晚些时候推出的产品。尽管没有透露具体时间，但谷歌方面确认计划将英伟达最近宣布但尚未发布的<a href="https://siliconangle.com/2024/03/18/nvidias-blackwell-architecture-power-new-generation-1-trillion-parameter-generative-ai-models/">Blackwell GPU</a>" 引入其AI超级计算机架构。谷歌方面表示，&nbsp;Blackwell GPU将提供两种配置，虚拟机由HGX B200和GB200 NVL72 GPU提供支持。前者是为最苛刻的人工智能工作负载而设计的，而后者则有望支持实时大语言模型推理和万亿参数规模模型的大规模训练的新时代。</p><p></p><h3>Gemini 1.5 Pro正式发布公共预览版</h3><p></p><p>Gemini 1.5 Pro的预览版发布只能说中规中矩，毕竟该系列在今年2月份就已经面世，这个模型最大的特点就是创下了最长上下文窗口的纪录。</p><p>&nbsp;</p><p>根据官方披露，Gemini 1.5 Pro将上下文窗口容量提到了100万token（极限为1000万token），远远超出了Gemini 1.0最初的32000个token，此前的SOTA模型也才将上下文窗口容量提高到了20万token。</p><p>&nbsp;</p><p>这意味着Gemini 1.5 Pro可以自如地处理22小时的录音、超过十倍的完整的1440页的书（587,287字）《战争与和平》，以及四万多行代码、三小时的视频。</p><p>&nbsp;</p><p>凭借超长上下文理解能力，Gemini 1.5 Pro得到了很多用户的认可。很多测试过Gemini 1.5 Pro的人更是直言，这个模型被低估了。如今预览版正式推出，期待后续广大开发者的反馈。</p><p>&nbsp;</p><p>Kurian现场介绍了众多可能的用例，并强调通过系列新增功能，谷歌云仍然是唯一提供广泛使用的第一方（Gemini系列）和第三方模型（主要指vertexAI上面的模型）服务的云服务商。</p><p></p><h2>Google Search升级</h2><p></p><p>通过使用新的提示管理工具对模型进行更精细的调整，包括解释为什么某些提示比其他提示效果更好等，可以进一步提高搜索结果的质量，显著降低产生幻觉的可能性。</p><p>&nbsp;</p><p>这些现均已在Vertex AI上提供，Vertex AI是Google Cloud的平台，用于定制和全面管理各种领先的人工智能模型。如今，超过100万开发人员正在使用谷歌的生成式AI工具，包括AI Studio和Vertex AI。此外，通过Vertex AI，客户现在可以通过两种新方式增强和基础他们的模型——将模型输出连接到可验证的信息源。第一个是Google Search，它提供高质量的信息以提高响应的准确性。第二个是用户自己的数据和事实来源，例如Workday或Salesforce等企业应用程序以及BigQuery等Google Cloud数据库。</p><p>&nbsp;</p><p>生成式AI时代，基础设施是一切创新的基础，极其重要，而单看这一部分，谷歌还是相当全面，超级能打。</p><p></p><h2>Agent、Agent、Agent，已经商业化</h2><p></p><p>会议现场，谷歌公布了系列Agent用例，比如Custom Agent、Code Agent。可能是国内应用市场的繁荣导致平常“吃得太好”，这部分用例并没有让笔者有“Wow”的感觉，但单从Demo来看，谷歌公布出来的场景效果确实足够精细，这种精细指的是“足以在企业场景中落地”，极其重要的是谷歌已经将这些商业化了，每一个用例都跟着一个客户故事。</p><p><img src="https://static001.infoq.cn/resource/image/bf/a0/bf23907afc11ef917b59f8c0b47457a0.jpg" /></p><p></p><p>为了有效创建下一代Agent，谷歌宣布了新的Vertex AI&nbsp;Agent Builder，其提供了一个更简单的流程来训练、编辑和启动相关工具，包括相关控制能力和基础响应。</p><p>&nbsp;</p><p>今年初，国内外对Agent寄予厚望，这被认为是生成式AI最有可能变现的一条路。如今看来，谷歌已经实现了，奔驰、沃尔玛等企业均在利用谷歌提供的Agent，而主会场的后半程几乎都被Agent所占据。</p><p>&nbsp;</p><p>由于这部分用例较多，且大部分国内感知有限（比如Google Workspace），就不一一列举，着重聊下Data Agents和Code Agents。</p><p><img src="https://static001.infoq.cn/resource/image/06/d8/06804259326026de399b64be0865bfd8.jpg" /></p><p>视频生成、剪辑相关用例</p><p>&nbsp;</p><p>Data Agents类似的产品形态，国内其实也有一些简单的尝试，通过在数据库或者数据平台上添加一些基本功能，让用户可以通过自然语言的方式与“业务数据”做交互。谷歌基于BigQuery做了Data Agents方面的尝试，沃尔玛执行副总裁Suresh Kumar&nbsp;也通过视频表示已经利用该功能来创造新的见解和个性化体验。</p><p><img src="https://static001.infoq.cn/resource/image/11/2b/118236cc7df4d0bd994a738cc8b1722b.jpg" /></p><p></p><p>Code Agent也显示出了人工智能将为代码编写带来巨大变化。Gemini Code Assist获得了强大关注，谷歌现在正在将其扩展到Gitlab、Github、本地和外部。谷歌表示，Gemini 1.5 Pro即将加入Code Assist，为编码带来100万tokens，“彻底改变”编码。Gemini Cloud Assist还可跨应用程序框架工作，让业务更顺畅、更轻松地推出和扩展。</p><p>&nbsp;</p><p>国内外的智能编码工具不少，大部分目前是集成在IDE中使用，Agent这种形态确实比较领先。早前，笔者从国内头部编码工具厂商那里了解到，其也将在今年下半年推出Agent版本，或许是国内目前来看比较有希望的节点。</p><p></p><p>面向生成式AI时代，谷歌的每一层架构都做好了准备。Kurian提到：“我们正处于行业的关键时刻，我们正在重塑基础设施以支持人工智能新时代”。</p><p>&nbsp;</p><p>“我们正在共同构建一种新的云方式。”</p><p></p><p><img src="https://static001.infoq.cn/resource/image/f1/4a/f11699904d94f72925247775064dbe4a.png" /></p><p></p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/hjJM3kV620iDoYYOBtPs</id>
            <title>检索增强生成引擎 RAGFlow 正式开源！仅一天收获上千颗星</title>
            <link>https://www.infoq.cn/article/hjJM3kV620iDoYYOBtPs</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/hjJM3kV620iDoYYOBtPs</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Apr 2024 10:26:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: RAG 解决方案, AI 原生数据库, 长上下文 LLM, RAG 引擎
<br>
<br>
总结: 4 月 1 日，我们正式宣布端到端 RAG 解决方案 RAGFlow 开源。在此之前，我们还开源了 AI 原生数据库 Infinity。Infinity 项目在 GitHub 上仅三个月时间就获得了 1400 颗星，而 RAGFlow 在开源首日就获得了上千颗星。我们开源这两个项目的初衷是希望能够从更多更广泛的应用场景中收到反馈，以便尽快让 RAG 走出当前的初期阶段。随着长上下文 LLM 的不断普及，我们希望这一天能够尽快到来。 </div>
                        <hr>
                    
                    <p>4 月 1 日，我们正式宣布端到端 RAG 解决方案 RAGFlow 开源。在此之前，我们还开源了 AI 原生数据库 Infinity。Infinity 项目在 GitHub 上仅三个月时间就获得了 1400 颗星，而 RAGFlow 在开源首日就获得了上千颗星。我们开源这两个项目的初衷是希望能够从更多更广泛的应用场景中收到反馈，以便尽快让 RAG 走出当前的初期阶段。随着长上下文 LLM 的不断普及，我们希望这一天能够尽快到来。</p><p></p><p>项目开源地址：</p><p>Infinity : https://github.com/infiniflow/infinity</p><p>RAGFlow：https://github.com/infiniflow/ragflow</p><p>RAGFlow 在线 Demo：https://demo.ragflow.io/</p><p></p><p>在回答 RAGFlow 有哪些特点之前，我们先来谈谈为何要做这样一款 RAG 引擎。</p><p></p><p>今年 2 月以来， AI 领域连续出了很多重磅热点，除了最火热的 Sora 之外，另一个热点就是长上下文 LLM ，例如 Claude 3、 Gemini 1.5，当然也包含国产的月之暗面。Sora 的本质是针对视频具备更加可控性的生成能力，这其实是解锁未来多模态 RAG 热潮的一个必要条件；而长上下文 LLM ，却引发了更多针对 RAG 的争论，因为这些 LLM，可以很方便的让用户随时上传 PDF，甚至上传几十个 PDF，然后针对这些 PDF 回答问题，并且还具备强大的“大海捞针”能力。所谓“大海捞针”，意思就是针对这些长上下文窗口的细节提问，看 LLM 是否可以准确地回答。</p><p></p><p>用 RAG 来实现大海捞针是轻而易举的，然而目前列举的这些 LLM，它们不是基于 RAG 来提供这种能力，却也都可以达到很高的召回，同时它们也不是采用类似 StreamLLM 这种基于滑动窗口实现长上下文注意力的机制——这种机制仅仅是增加了上下文窗口，但却仍然在细节召回上表现不佳，窗口滑过，内容即会被逐渐“遗忘”。我们也试验了其中的若干产品，效果确实非常好，上传一个 PDF，甚至可以针对里边的复杂图表给出精确的回答。</p><p></p><p>因此，这引发了新的一轮关于长上下文 LLM 和 RAG 的争论，许多人评价 “RAG 已死”，而 RAG 拥护者则认为，长上下文 LLM 并不能满足用户海量数据的需求，成本高，速度也不够快，也只能针对长文本、图片等数据提问。</p><p></p><p>随着长上下文为更多用户接纳，近期各家国产 LLM 都快速推出了这个产品特性，除月之暗面外，其他家大多基于 RAG 来实现，下表是两者的基本对比：</p><p></p><p><img src="https://static001.geekbang.org/infoq/84/847a76363897b80afb0019624bce6896.webp" /></p><p></p><p>这里要额外说明一下，为何 RAG 派的大海捞针能力一般，这并不是 RAG 本身的问题，而是依靠纯向量数据库去构建 RAG，并不能保证对精确数据和细节的准确召回。</p><p></p><p>以上的对比，其实并没有完全解答 RAG 的必要性，因为至少就目前 RAG 最普遍的场景——个人知识库问答而言，确实很多情况下只需要 LLM 就足够了。而我们则认为，LLM 的长上下文能力，对于 RAG 来说是个很大的促进。这里先用 OpenAI 联创 Andrej Karpathy 的一张图做个类比，他把 LLM 比喻为一台计算机的 CPU， 把上下文类比为计算机的内存，那么以向量为代表的数据库，就可以看作是这台计算机的硬盘。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f1/f1bcd9b9688da15d1a5a9f72e032d06b.webp" /></p><p></p><p>我们进一步来说明，为什么即使有了“大海捞针”能力，RAG 仍然必不可少。RAG 从提出到为业界广泛接纳，经历了一年多时间，当下的 RAG 产品已经并不稀缺，然而在实际应用中，却普遍得出了“ RAG 属于上手容易，但真正落地却很难”的结论。究其原因，这里边主要包含两个方面：</p><p></p><p>其一是来自 LLM 自身。由于 RAG 的工作流程是针对从数据库返回的结果进行回答，这样的话，对于 RAG 来说，LLM 最基础也是最重要的能力其实包含：</p><p>摘要能力；可控性：既 LLM 是否听话，是否会不按照提示要求的内容自由发挥产生幻觉；翻译能力，这对于跨语言 RAG 是必备的。</p><p></p><p>遗憾的是，在过去，国内可以用到的 LLM 中，在这 3 点上表现良好的并不多。至于所谓高级的能力，例如逻辑推理，以及各类 Agent 要求的自主决策能力等，这些都是建构在以上基础能力之上，基础不好，这些也都是空中楼阁。</p><p></p><p>其二，则是来自于 RAG 系统本身。我们所说的 RAG，实际上包含完整的链路，包括数据的准备，数据写入，乃至从数据库查询和返回结果排序。在整条链路中，最大的难点来自于两方面：一是如何应对复杂多变的数据，这些数据包含各种格式，更复杂的还包含各类图表等，如果在没有理解这些语义的基础之上直接提供 RAG 方案，就会导致语义丢失从而让 RAG 失败。二是如何查询和排序：简单地讲，在大多数情况下，都必须引入多路召回和重排序，才能保证数据查询的准确度。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0a/0abff5fc98553f8a297e257a55a6ec57.webp" /></p><p></p><p>假如我们不去专注于解决这两类问题，那么就很容易陷入让 RAG 去和长上下文 LLM 反复对比的情况，因为两者其实都可用于简易知识库对话场景：RAG 仅仅提供数据的简单解析，然后直接转化为向量，最后用单一向量做召回，这除了成本，以及私有化场景里所要求的安全等优势之外，在核心对话能力上并没有显著地跟长上下文 LLM 区分开来，甚至还有所不及。</p><p></p><p>正是基于这些 RAG 本身的痛点，我们先后推出了 2 个开源项目：</p><p></p><p>第一个是 AI 原生数据库 Infinity。它解决的是如何解锁 RAG 服务 B 端场景下遇到的典型问题：如何跟企业已有的数据——包括但不限于非结构化的文档、图片，还包括结构化的信息系统来结合，并解决多路召回和最终融合排序的问题。</p><p></p><p>举几个典型场景：把符合要求的简历筛出，筛选条件包含工作技能（需要向量 + 全文搜索），某类行业的工作经验（基于向量的分组聚合），期望收入，学历，地域（结构化数据）等；基于对话推荐符合个人要求的产品，可以采用多列向量来描述个人偏好，不同的列代表了用户对不同类目产品的过往使用偏好。在推荐过程中，除了采用基于用户的偏好向量进行搜索之外，还需要结合产品的过滤条件：包括是否过期，是否有优惠券，是否符合权限要求，是否有合规要求，该用户是否近期已经购买或者阅读过，等等。</p><p></p><p>这些信息，如果仅仅拿所谓“标量”字段这种方式来表征，那么产品的开发是极其复杂的：因为这需要引入额外的 ETL ，带来了维护性，以及更严重的数据一致性的问题。要知道，RAG 面临的是最终用户使用场景，它是需要业务乃至 LLM 发起请求，就立刻得到答案的，因此不能像数据中台一样仅仅为了一张报表就可以搭建一整套数据管道体系去做宽表这种额外逻辑。因此，Infinity 实际上等于向量数据库 + 搜索引擎 + 普通结构化数据查询，并保证三者的高并发和融合排序。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2e/2ef863f1ac05e0bc35140e4f4307ea18.webp" /></p><p></p><p>第二个就是端到端的 RAG 引擎 RAGFlow。它解决数据的问题：因为如果不对用户数据加以区分和清晰，识别其中的语义，就容易导致 Garbage In Garbage Out。RAGFlow 包含了如下的完整 RAG 流程，确保数据从 Garbage In Garbage Out 变为 Quality In Quality Out。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ff18442fabfb2e8a38f3cb937c5b4ff7.webp" /></p><p></p><p>具体来说， RAGFlow 的最大特色，就是多样化的文档智能处理，因此它没有采用现成的 RAG 中间件，而是完全重新研发了一套智能文档理解系统，并以此为依托构建 RAG 任务编排体系。这个系统的特点包含：</p><p></p><p>1. 它是一套基于 AI 模型的智能文档处理系统：对于用户上传的文档，它需要自动识别文档的布局，包括标题、段落、换行等，还包含难度很大的图片和表格。对于表格来说，不仅仅要识别出文档中存在表格，还会针对表格的布局做进一步识别，包括内部每一个单元格，多行文字是否需要合并成一个单元格等。并且表格的内容还会结合表头信息处理，确保以合适的形式送到数据库，从而完成 RAG 针对这些细节数字的“大海捞针”。</p><p></p><p>2. 它是一套包含各种不同模板的智能文档处理系统：不同行业不同岗位所用到的文档不同，行文格式不同，对文档查阅的需求也不同。比如：</p><p>会计一般最常接触到的凭证、发票、Excel 报表；查询的一般都是数字，如：看一下上月十五号发生哪些凭证，总额多少？上季度资产负债表里面净资产总额多少？合同台账中下个月有哪些应付应收？作为一个 HR 平时接触最庞杂的便是候选人简历，且查询最多的是列表查询，如：人才库中 985/211 的 3 到 5 年的算法工程师有哪些？985 硕士以上学历的人员有哪些？赵玉田的微信号多少？香秀哪个学校的来着？作为科研工作者接触到最多的可能是就是论文了，快速阅读和理解论文，梳理论文和引文之间的关系成了他们的痛点。</p><p></p><p>这样看来凭证 / 报表、简历、论文的文档结构是不一样的，查询需求也是不一样的，那处理方式肯定是不一样。因此 RAGFlow 在处理文档时，给了不少的选择：Q&amp;A，Resume，Paper，Manual，Table，Book，Law，通用... 。当然，这些分类还在不断继续扩展中，处理过程还有待完善。我们也会抽象出更多共通的东西，使各种定制化的处理更加容易。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2f/2f0ec9574414aeae20c2290390cadd39.gif" /></p><p></p><p>3. 智能文档处理的可视化和可解释性：用户上传的文档到底被处理成啥样了，如：分割了多少片，各种图表处理成啥样了，毕竟任何基于 AI 的系统只能保证大概率正确，作为系统有必要给出这样的空间让用户进行适当的干预，作为用户也有把控的需求，黑箱不敌白箱。特别是对于 PDF，行文多种多样，变化多端，而且广泛流行于各行各业，对于它的把控尤为重要，RAGFlow 不仅给出了处理结果，而且可以让用户查看文档解析结果并一次点击定位到原文，对比和原文的差异，可增可减可改可查，如下图所示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/e2/e2c2a6b8ae16839f447d7c0863f1f91a.gif" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/d0/d0ff9727d9daa9ee95b59379027c09c3.webp" /></p><p></p><p>4. RAGFlow 是一个完整的 RAG 系统，而目前开源的 RAG，大都忽视了 RAG 本身的最大优势之一：可以让 LLM 以可控的方式回答问题，或者换种说法：有理有据、消除幻觉。我们都知道，随着模型能力的不同，LLM 多少都会有概率会出现幻觉，在这种情况下， 一款 RAG 产品应该随时随地给用户以参考，让用户随时查看 LLM 是基于哪些原文来生成答案的，这需要同时生成原文的引用链接，并允许用户的鼠标 hover 上去即可调出原文的内容，甚至包含图表。如果还不能确定，再点一下便能定位到原文，如下图所示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/5c/5c4161919c62326262b3c9cb80837804.gif" /></p><p></p><p>接下来，我们来讲讲，RAGFlow 具体是如何利用文档结构识别模型来处理数据的。所谓文档结构模型，如下所示，是针对文档的布局进行目标识别，然后根据布局再做文字切分。这些布局识别的目标包括文档的标题，段落，语义文字块等等，尤其还会包含文档当中的图表。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d1a1650e225536587cb4decc9d5c0e5c.webp" /></p><p></p><p>在识别出这些目标之后，还需要分别对这些目标做相应处理：对于文字来说，需要首先判断文字的换行信息——这对于文字的语义理解也会产生干扰；其次需要对文字内容进行一些整理，这些整理会随着 RAGFlow 模板的不同有所区分；针对表格来说，还需要进一步识别它的内部结构，这在 AI 领域有个专门的研究课题，叫做 TSR(Table Structure Recognition 表格结构识别) 。</p><p></p><p>TSR 任务其实相对比较复杂，因为表格的定义是多种多样的，表格内部可能会出现有线条或者没有线条的情况，对于不同行的文字，判断它们是否是一个单元格是存在很大挑战的，单元格判断失误，很可能就会让表格的数字跟表格列的对应关系弄错，从而影响了对单元格内文字和数字语义的理解。我们花了很多时间来提升 TSR 的能力，最早是利用现成的 OCR 开源模型，后边也尝试过微软研究院专门针对 TSR 任务的 Transformer 模型，但是发觉这些模型处理 TSR 任务的鲁棒性依然非常不足，最后我们还是训练了自己的模型，从而让 TSR 任务表现良好。这个模型比较简单，就是基于 CNN 的目标检测模型，但是它的效果却比上边我们提到的其他模型都要好。为了降低对硬件的依赖和开销，我们甚至切换到用 YOLOv8 来做目标检测，使得仅仅利用 CPU 也可以运行文档结构识别。</p><p></p><p>关于这些，其实也有很多业内人士建议直接走 LLM 的路子，用 LLM 来做文档语义理解，从长期来看这肯定是个趋势，然而在当下来说，让 LLM 在文档结构识别上表现良好，还需要大量的数据才可以。这从我们放弃了基于 Transformer 的 TSR 模型就可以看出：同样的任务下，基于 Transformer 的模型需要更多的数据才可以表现更好，在有限数据下，我们不得不退回到传统 CNN 模型，如果是 LLM ，它需要的数据和算力更多——我们之前曾经尝试过基于多模态 LLM 进行识别的努力，相比专用小模型，它的效果还是差别比较大。从另一个方面也可以看出来，下图是我们用长上下文 LLM 对表格输出的例子：</p><p></p><p>这是原表格：</p><p></p><p><img src="https://static001.geekbang.org/infoq/13/13fa0a319e1849790a8590b9e28dba38.webp" /></p><p></p><p>这是识别后的结果：</p><p></p><p><img src="https://static001.geekbang.org/infoq/13/1383bc9362f336615159c4a31d08ca04.webp" /></p><p></p><p>解锁对于非结构化数据的深度语义理解是 RAGFlow 追求的目标之一，我们希望在未来能够将更加 scalable 的文档结构识别模型应用到系统中。不仅如此， RAGFlow 的设计目标是让 RAG 逐渐承接起更多的复杂场景尤其是 B 端场景，因此在未来，它会接入企业的各类数据源，比如 MySQL 的 binlog，数据湖的 ETL，乃至外部的爬虫等。只有这些都被纳入 RAG 的范畴，我们才能实现如下的愿景：</p><p></p><p><img src="https://static001.geekbang.org/infoq/af/af53c99ac42cfecb2c8b052689f3ecca.webp" /></p><p></p><p>再回头看前边关于 RAG 和长上下文 LLM 的争论， 显然两者一定是合作的。长上下文 LLM 当下已经逐步具备了 RAG 最不可或缺的基础能力，随着它自身逻辑推理能力地增强，再结合来自数据库，还有数据方面的改进，一定能加速 LLM 的 B 端场景走出婴儿期的进程。</p><p></p><p>RAGFlow 近期更新：将提供类似文件管理的功能，这样 RAG 可以跟企业内部文档以更灵活的方式整合。RAGFlow 中期更新，将提供面向企业级数据接入的低代码平台，同时提供问答对话之外的高级内容生成，比如长文生成等等。</p><p></p><p>Infinity 近期更新：Infinity 近期将发布第一个 release，届时将提供业界最快的多路召回和融合排序能力。</p><p></p><p>欢迎大家关注我们的开源社区，并提出反馈意见！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/SWGj2PKN1oaXAODLaec8</id>
            <title>硅谷创业一年，贾扬清讲了自己的AI行业观察：成本、市场增量和商业模式</title>
            <link>https://www.infoq.cn/article/SWGj2PKN1oaXAODLaec8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/SWGj2PKN1oaXAODLaec8</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Apr 2024 10:03:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 贾扬清, AI Infra, 大模型, 英伟达
<br>
<br>
总结: 本文介绍了AI科学家贾扬清选择创业方向为AI基础设施，他创立并开源了深度学习框架Caffe，离开阿里后专注于AI Infra的发展。文章还提到了大模型的商业模式和硬件提供商的发展趋势，以及AI计算与云计算的不同关注点。贾扬清在分享中详细分析了AI时代的基础设施和大模型的应用，强调了中小型模型结合自有数据的优势。 </div>
                        <hr>
                    
                    <p>本文经授权转载于腾讯科技，原文链接：https://mp.weixin.qq.com/s/kpFnXZbROMCdJ5WikuOJvw</p><p></p><p>编辑 /&nbsp;腾讯科技 郭晓静</p><p></p><p>创业一年的贾扬清，选择的方向是AI Infra。</p><p></p><p>贾扬清是最受关注的全球AI科学家之一，博士期间就创立并开源了著名的深度学习框架Caffe，被微软、雅虎、英伟达等公司采用。</p><p></p><p>2023年3月，他从阿里离职创业，并在随后录制的播客中说，自己并非是因为ChatGPT 火爆而创业，后来创业项目浮出水面，也确实证实，他没有直接入局大模型。硅谷著名风投a16z在去年发表的一篇关于AIGC的文章中就曾经提到过：“目前来看，基础设施提供商是这个市场上最大的赢家。”</p><p></p><p>贾扬清在去年的文章中也提到，“不过要做这个赢家，就要更聪明地设计Infra才行”。在他创办的公司Lepton.AI的官网上，有一句醒目的Slogan“Build AI The Simple Way（以简单的方式构建AI）”。</p><p></p><p>最近，贾扬清在高山书院硅谷站“高山夜话”活动中，给到访的中国企业家做了一次深度的闭门分享，分享的内容直击行业痛点，首先从他最专业的AI Infra开始，详细分析了AI时代的Infra，到底有什么新的特点；然后，基于AI大模型的特点，帮助企业算了一笔比较详细的经济账——在不可能三角成本、效率、效果中，如何选才能达到比较好的平衡点。</p><p></p><p>最后也讨论到AI整个产业链的增量机会及目前大模型商业模式的纠结点：</p><p></p><p>“每次训练一个基础大模型，都要从零开始。形象一点来描述，这次训练‘投进去10个亿，下次还要再追加投10个亿’，而模型迭代速度快，可以赚钱的窗口也许只有大概一年。所以每个人都在思考这个终极问题，‘大模型的商业模式到底怎样才能真正有效？’”</p><p></p><p>贾扬清的过往经验大部分是TOB的。他也多次在分享中很坦诚地表示，“TOC我看不太清楚，TOB看得更清晰一些。”</p><p></p><p>“AI从实验室或者说从象牙塔出来到应用的过程中，该蹚过的雷，都会经历一遍。”无论大语言模型给人们多少惊艳，它的发展都不是空中楼阁，既往的经验和范式有变也有不变。</p><p></p><p>为了方便阅读，我们在文首提炼几个主要观点，但强烈建议完整阅读，以了解贾扬清完整的思考逻辑：</p><p></p><p></p><p></p><blockquote>一个通用的大模型的效果固然非常好，但是在企业实际应用当中，中小型模型加上自己的数据，可能反而能够达到一个更好的性价比。至于成本问题，我们也算了一笔经济账：一台GPU服务器就可以提供支撑的7B、13B模型通过微调，性价比可能比直接使用闭源大模型高10倍以上。我个人认为，英伟达在接下来的3~5年当中，还会是整个AI硬件提供商中绝对的领头羊，我认为它的市场发展占有率不会低于80%。但是今天AI模型逐渐标准化，我们也看到了硬件层面另外一个机会。目前我们看到AI应用中，有两大类应用，已经跨越死亡谷，开始有比较持续的流量：一个是提效，另外一个是娱乐。大量的传统行业应用，其实是AI行业里值得探究的深水区。我个人关于Supper App的观点可能稍微保守一些，也有可能是因为我自己的经历很多都在做TOB的服务，我认为Super APP会有，但是会很少。</blockquote><p></p><p></p><p>一个通用的大模型的效果固然非常好，但是在企业实际应用当中，中小型模型加上自己的数据，可能反而能够达到一个更好的性价比。</p><p></p><p>至于成本问题，我们也算了一笔经济账：一台GPU服务器就可以提供支撑的7B、13B模型通过微调，性价比可能比直接使用闭源大模型高10倍以上。</p><p></p><p>我个人认为，英伟达在接下来的3~5年当中，还会是整个AI硬件提供商中绝对的领头羊，我认为它的市场发展占有率不会低于80%。但是今天AI模型逐渐标准化，我们也看到了硬件层面另外一个机会。</p><p></p><p>目前我们看到AI应用中，有两大类应用，已经跨越死亡谷，开始有比较持续的流量：一个是提效，另外一个是娱乐。</p><p></p><p>大量的传统行业应用，其实是AI行业里值得探究的深水区。</p><p></p><p>我个人关于Supper App的观点可能稍微保守一些，也有可能是因为我自己的经历很多都在做TOB的服务，我认为Super APP会有，但是会很少。</p><p></p><p></p><p>以下为分享内容精华整理：</p><p></p><p>随着大型语言模型的兴起，出现了一个新概念——Scaling Law（规模定律）。根据Scaling Law，大语言模型的性能与其参数量、训练数据的大小和计算量呈幂律关系。简单来说，用通用的方法给模型巨大的数据，让模型能够拥有输出我们想要的结果的能力。</p><p></p><p>这就使得AI计算与“云计算”有很大的不同，云计算主要服务于互联网时代的需求，关注资源的池化和虚拟化：</p><p></p><p>●&nbsp;怎么把计算，存储，网络，从物理资源变成虚拟的概念，“批发转零售”；</p><p></p><p>●&nbsp;如何在这种虚拟环境下把利用率做上去，或者说超卖；</p><p></p><p>●&nbsp;怎么更加容易地部署软件，做复杂软件的免运维（比如说，容灾、高可用）等等，不一而足。</p><p></p><p>用比较通俗的语言来解释，互联网的主要需求是处理各种网页、图片、视频等，分发给用户，让“数据流转（Moving Data Around）起来。云服务关注数据处理的弹性，和便捷性。</p><p></p><p>但是AI计算更关注以下几点：</p><p></p><p>●&nbsp;并不要求特别强的虚拟化。一般训练会“独占”物理机，除了简单的例如建立虚拟网络并且转发包之外，并没有太强的虚拟化需求。</p><p>●&nbsp;需要很高性能和带宽的存储和网络。例如，网络经常需要几百 G 以上的 RDMA 带宽连接，而不是常见的云服务器几 G 到几十 G 的带宽。</p><p>●&nbsp;对于高可用并没有很强的要求，因为本身很多离线计算的任务，不涉及到容灾等问题。</p><p>●&nbsp;没有过度复杂的调度和机器级别的容灾。因为机器本身的故障率并不很高（否则 GPU 运维团队就该去看了），同时训练本身经常以分钟级别来做 checkpointing，在有故障的时候可以重启整个任务从前一个 checkpoint 恢复。</p><p></p><p>今天的AI计算 ，性能和规模是第一位的，传统云服务所涉及到的能力，是第二位的。</p><p></p><p>这其实很像传统高性能计算领域的需求，在七八十年代我们就已经拥有超级计算机，他们体积庞大，能够提供大量的计算能力，可以完成气象模拟等服务。</p><p></p><p>我们曾做过一个简单的估算：过去，训练一个典型的图像识别模型大约需要1 ExaFlop的计算能力。为了形象地描述这一计算量，可以想象全北京的所有人每秒钟进行一次加减乘除运算，即便如此，也需要几千年的时间才能完成一个模型的训练。</p><p></p><p>那么，如果单台GPU不足以满足需求，我们应该如何应对呢？答案是可以将多台GPU连接起来，构建一个类似于英伟达的Super POD。这种架构与最早的高性能计算机非常相似。</p><p></p><p>这时候，如果一台GPU不够怎么办？可以把一堆GPU连起来，做成一个类似于英伟达的Super POD，它和最早的高性能计算机长得很像。</p><p></p><p>这就意味着，我们又从“数据流转”的需求，回归到了“巨量运算”的需求，只是现在的“巨量运算”有两个进步，一是用于计算的GPU性能更高，另外就是软件更易用。伴随着AI的发展，这将是一个逐渐加速的过程。今年NVIDIA推出的新的DGX机柜，一个就是几乎1Exaflops per second，也就是说理论上一秒的算力就可以结束训练。</p><p></p><p>去年我和几位同事一起创办了Lepton AI。Lepton在物理中是“轻子”的意思。我们都有云计算行业的经验，认为目前AI的发展给“云”带来一个完全转型的机会。所以今天我想重点分享一下，在AI的时代，我们应该如何重新思考云的Infrastructure。</p><p></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/49/498853e30a6c2505eec1dba0d5c31dba.webp" /></p><p></p><p></p><p>企业用大模型，先算一笔“经济账”</p><p></p><p></p><p>随着模型规模的不断扩大，我们面临着一个核心问题：大模型所需的计算资源成本高昂，从实际应用的角度出发，我们需要思考如何高效地利用这些模型。</p><p></p><p>以一个应用场景为例，我们可以比较形象地看出一个通用的大型语言模型与针对特定领域经过微调的模型之间的差异。</p><p></p><p>我们曾经尝试过“训练一个金融领域的对话机器人”。</p><p></p><p>使用通用模型，我们直接提问：“苹果公司最近的财报怎么样？你怎么看苹果公司在AI领域的投入。”通用大模型的回答是：“抱歉，我无法回答这个问题。”</p><p></p><p>针对特定领域微调，我们使用了一个7B的开源模型，让它针对性地“学习”北美所有上市公司的财报，然后问它同样的问题。它的回答是：“没问题，感谢您的提问。（Sure，thanks for the question）”口吻十分像一家上市公司的CFO。</p><p></p><p>这个例子其实可以比较明显地看出，通用大模型性能固然很出色，但是在实际应用中，使用中小型开源模型，并用特定数据微调，最终达到的效果可能更好。</p><p></p><p>至于成本问题，我们也算了一笔经济账：一台GPU服务器就可以提供支撑的7B、13B模型通过微调，性价比可能比直接使用闭源大模型高10倍以上。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/56/560ad0fe223d6015f2a3e0154b7bd70b.webp" /></p><p></p><p></p><p>如上图所示，以Llama2 7B开源模型为例，100万token的成本大约为0.1美元-0.3美元。使用一台英伟达A10GPU服务器就能支持训练，以峰值速度2500token每秒来计算，一小时的成本大约为0.6美元。自有这台服务器，一年的成本大约为5256美元，并不算高。</p><p></p><p>如果用闭源模型，100万token消耗速度很快，成本远高于0.6美元每小时。</p><p></p><p>不过成本消耗也要考虑应用的种类和模型的输出速度，模型输出速度越快，成本也会越高。如果可以有mini-batch（小批量数据集）等，同时来跑，它的整体性能就会更好，但是单个的输出性能可能就会稍微差一点。</p><p></p><p>这就引出另外一个问题，大模型的输出速度，怎样比较合适？</p><p></p><p>以Chatbot举例，人说话的速度大概为120词每分钟，成人阅读的速度大概为350词左右，反向计算token，每秒钟20个token左右，就能达到比较好的体验。如果这样计算的话，如果应用的流量够大，跑起来成本是不高的。</p><p></p><p>但是，究竟流量能不能达到“够大”，这就变成了“鸡生蛋、蛋生鸡”的问题。我们发现了一个很实用的模式可以解决这个问题。</p><p></p><p>在北美，很多企业都是先用闭源大模型来做实验（比如OpenAI的模型）。实验规模大概在几百个million（百万token），成本大概为几千美元。一旦数据飞轮运转起来，再把已有数据存下来，用较小的开源模型微调自己的模型。现在这已经变成了相对比较标准的模式。</p><p></p><p>在考虑AI模型的时候，各家企业其实都在各种取舍中找平衡。在北美经常讲一个不可能三角，当你买一辆车的时候跑得快、便宜和质量好，这三者是不可兼得的。</p><p></p><p>上文提到的标准模式，其实就是首先追求质量，然后再考虑成本，如果想同时满足这三方面，基本是不可能的。</p><p></p><p>半年之前我非常强烈地相信开源模型能非常迅速追赶上闭源模型，然而半年之后，我认为开源模型和闭源模型之间会继续保持一个非常合理的差距，这个差距用比较形象的具体模型举例来说，闭源模型到GPT-4水平的时候，开源模型可能在GPT3.5左右。</p><p></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/be/be13b28f126280485bf393899d31fab8.webp" /></p><p></p><p></p><p>硬件行业的新机会</p><p></p><p></p><p>早在2000年初，英伟达就看到了高性能计算的潜力，于是2004年他们做了CUDA，到今天为止已经20年。今天CUDA已经成为AI框架和AI软件最底层的标准语言。</p><p></p><p>早期，行业内都认为高性能计算写起来很不方便，英伟达介绍了CUDA，并说服大家它简单易用，让大家尝试来写。试用之后，大家发现确实易用且写出来的高性能计算速度很快，后来几乎各大公司的研究员们都把自己的AI框架基于CUDA写了一遍。</p><p></p><p>CUDA很早就和AI社区建立了很好的关系，其它公司也看到了这个市场的巨大机会，但是从用户侧来看，大家用其它产品的动机不强。</p><p></p><p>所以市场上还会有一个关注焦点，那就是是否有人能够撼动英伟达的地位，除了英伟达，新的硬件提供商还有谁可能有机会？</p><p></p><p>首先我的观点不构成投资建议，我个人认为英伟达在接下来的3~5年当中，依然还会是AI硬件提供商中绝对的领头羊，它的市场占有率不会低于80%。</p><p></p><p>但是今天AI模型逐渐标准化，我们也看到了硬件层面另外一个机会。前十年中，在AI领域大家都在纠结的一个问题，虽然很多公司能够提供兼容CUDA的适配，但是这一层“很脆”。“很脆”的意思是模型多种多样，所以适配层容易出问题，整个工作链就会断。</p><p></p><p>今天越来越少的人需要写最底层的模型，越来越多的需求是微调开源模型。能够跑Llama、能够跑 Mistral，就能满足大概80%的需求，每一个Corner Case（特殊情况）都需要适配的需求逐渐变少，覆盖几个大的用例就可以了。</p><p></p><p>其它硬件提供商的软件层在努力兼容CUDA，虽然还是很难，但是今天抢占一定市场占有率，不再是一件不可能的事情；另外云服务商也想分散一下投资。所以这是我们看到的一个很有意思的机会点，也是cloud infra在不断变化的过程。</p><p></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/ed/ed4e13279c43f403bf3e40ec0b1c0b90.webp" /></p><p></p><p></p><p>生成式AI浪潮：哪些是增量机遇？</p><p></p><p></p><p>我们再看一下AI应用的情况。今天我们可以看到AI应用的供给在不断增加。从Hugging Face来看，2022年8月模型数量大概只有6万，到2023年9月，数量就已经涨了5倍，增速是非常快的。</p><p></p><p>目前我们看到AI应用中，有两大类应用，已经跨越死亡谷，开始有比较持续的流量：</p><p></p><p>第一大类是提效（productivity）。例如在电商行业，用AIGC的方式更快生成商品展示图片。例如Flair AI，应用场景举例来说，我希望能给瓶装水拍摄一个广告图片，仅仅需要把水放在方便的地方，拍一张照片。然后把这张照片发送给大模型，告诉它，我希望它被放在有皑皑白雪的高山上，背景是蓝天白云。它就能生成一个直接可以上传电商平台，作为产品展示的图片。</p><p></p><p>其它类型也有很多，比如在企业海量知识库做搜索且有更好的交互功能，例如Glean。</p><p></p><p>第二大类是娱乐（entertainment），比如Soul，以AI的方式做角色扮演及交互。</p><p></p><p>另外我们还发现一个趋势是“套壳APP”越来越少了。其实大家发现直接“套壳”通用大模型的产品会有一个通病，交互效果特别“机器人”。</p><p></p><p>反而是7B、13B的稍小模型，性价比和可调性都特别好。做个直观的比喻：大模型就好像是“读博士”读轴了，反而是本科生的实操性更强。</p><p></p><p>做应用层，总结来讲有两条路径：第一条是训练自己的基础大模型，或者是自己去微调模型。</p><p></p><p>另外就是有自己非常垂直领域的应用，背后是很深的场景，直接用Prompt是不可行的。</p><p></p><p>比如医疗领域，用户提需求问：“我昨天做的化验结果怎么样？”这其实需要背后有个大模型，除了对化验指标做出专业的分析，还需要给用户提出饮食等建议。</p><p></p><p>这背后涉及到化验、保健、保险等产业链的多个细分场景，需要医疗产业链很深的经验。需要在既有的经验上加一层AI能力来做好用户体验，这是我们今天发现的比较有持续性的AI应用模式。</p><p></p><p>关于未来到底怎样，预测未来是最难的。我的经验一直是B端，逻辑主要看供需。AI带来的增量需求首先是高性能的算力。第二个是高质量的模型，以及上层需要的适合这些高性能、高质量和高稳定性需求的计算的软件层。</p><p></p><p>所以我觉得从高性能算力来看，英伟达显然已经成为赢家。另外这个市场可能会容纳2~3家比较好的芯片提供商。</p><p></p><p>从模型来看，OpenAI肯定是一个已经比较确定的赢家，市场足够大，应该能够容纳3-5家不同的模型生产厂商，而且它很有可能还会出现偏地域性的分布。</p><p></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/d9/d96641a722069da64fa2548458168434.webp" /></p><p></p><p></p><p>传统行业的AI深水区</p><p></p><p></p><p>我还想讲的是大量的传统行业应用，这其实是AI行业里值得探讨的深水区。</p><p></p><p>大语言模型出现，大家曾经一度觉得OpenAI弄了一个特别厉害的大模型，写点Prompt就能搞定任何事情。</p><p></p><p>但是Google早在世纪之初就写过一篇文章，到今天我仍然觉得这个观点是对的。这篇文章说，机器学习模式只是整个AI链路中非常小的一部分，外面还有大量的工作，在今天来说也会变得越来越重要。比如如何收集数据、如何保证数据和我们的应用需求一致，如何来做适配，等等。</p><p></p><p>模型上线之后还有三件事：第一是跑的稳定，第二个是能够把结果质量等都持续稳定地控制起来，以及还有非常重要的一点是把应用当中所得到的数据，以一种回流的方式收集回来，训练下一波更好的模型。</p><p></p><p>到今天这个方法论依然适用，就是在行业竞争中，谁能有数据，谁能够把用户的反馈更好地调试成“下一波训练的时候可以更好的应用”的数据，这也是核心竞争力之一。</p><p></p><p>今天大家都有这样一种感觉，大模型的结构相差不大，但是数据和工程能力的细节才是决定模型之间差别的地方，OpenAI其实持续在给我们证明这件事。</p><p></p><p>今天我们看整个技术栈的架构是什么样子的，a16z给了我们一个非常好的总结（如下图）：</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/b4/b4b9fd749082d737bb1007ba0ba2a482.webp" /></p><p></p><p></p><p></p><p>IaaS这一层基本上是英伟达做“老大”，其它公司在竞争硬件和云平台，这是最下层的坚实基础。</p><p></p><p>云平台今天也在发生不断的变化，大家最近可能在技术趋势上听到一个词叫做“下云”，以前大家肯定听说过“全栈上云”。</p><p></p><p>为什么会出现“我要下云”的思潮？就是因为算力本身是巨大的成本，而且又是可以“自成一体”的成本，所以行业内开始把传统的云成本和今天AI算力的成本分开来考虑。</p><p></p><p>今天越来越多的PaaS开始变成Foundation Model，有些是闭源的，有些是开源的，然后在上面再做一层APP。今天每一层都竞争激烈。但是我个人感觉在模型这一层以及往上的上层应用这一层，是最活跃的。</p><p></p><p>模型层主要是开源和闭源之争。</p><p></p><p>应用层有两个趋势：一个是模型在努力往上做应用；另外就是是应用层在拼命想理解模型到底能有什么能力，然后把自己的应用加上AI，让自己的应用更强大。</p><p></p><p>我个人认为，模型往上做应用有点难，应用把自己的AI能力加进来更有希望。</p><p></p><p>国内还有种说法叫做Super APP（超级应用），Super APP很重要的一点是需要“端到端把问题解决”。a16z在他的图上也描述会有一些端到端的APP出来，本质上需要模型的推理和规划的能力做的非常好。ChatGPT就是端到端全部打通，模型也是自己的，应用也是自己的，这是Super App的状态。</p><p></p><p>但是我个人关于Super App的观点可能稍微保守一些，也有可能是因为我自己的经历很多时候都在做TOB的服务，我个人的感觉是Super APP会有，但是会很少。</p><p></p><p>我个人的感觉是，B端的应用越来越多的还是会以一种像搭积木一样，用开源的模型结合企业自己的数据，把企业自己的应用搭起来的一个过程。</p><p></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/43/4382db6cb575e0915a3c721355de3872.webp" /></p><p></p><p></p><p>大模型的商业模式：</p><p>两个纠结和一个市场现象</p><p></p><p></p><p>但是在大模型进行商业化落地的过程中，我观察到市场还是会有两个纠结：</p><p></p><p>第一个纠结是营收的流向和以往不太一样，不太对。</p><p></p><p>正常商业模式的流向应该是：从用户那里收费，然后“留成本”给硬件服务商，比如英伟达。但是今天是横向的，从VC（风投）拿到融资，直接“留钱”给硬件厂商。但是VC的钱本质是投资，创业者最后可能要10倍还给VC，所以这个资金流向是第一个纠结。</p><p></p><p>第二个纠结是今天的大模型对比传统软件，可以创造营收的时间太短。</p><p></p><p>其实开发一次软件之后，可以收回成本的时间比较长。比如像Windows，虽然过几年迭代一代，但是它底层的很多代码是不用重写的。所以一个软件被写完，可能在接下来的5-10年当中，它给我时间窗口持续迭代。而且投入的成本大部分是程序员的成本。</p><p></p><p>但是大模型的特点是，每次训练过一个模型之后，下一次还是要从零开始重新训练。比较形象一点来说“今天投入10个亿，再迭代的时候，又得再追加投入十个亿”。</p><p></p><p>但是模型的迭代速度又很快，中间能够赚钱的时间窗口究竟有多长？今天看起来好像大概是一年左右，甚至更短。</p><p></p><p>于是大家就开始质疑，大模型的成本远高于传统的软件，但是做完一个模型之后，能赚钱的时间远低于传统的软件。</p><p></p><p>所以就回到了这个终极问题，大模型的商业模式到底怎样才能真正有效？</p><p></p><p>我还观察到一个市场现象，去年整个市场都非常痛苦，硬件需求的突然暴涨，整个供应链都没反应过来，等待时间很长，甚至可能6个月以上。</p><p></p><p>最近我们观察到的一个现象是供应链没有那么紧张了。第一是全球供应链也开始缓过来；第二我个人判断有一部分以前因为焦虑而提前囤货的供应商，觉得现在要开始收回成本了。之前供不应求的紧张状态会逐渐变好，但是也不会一下子变成所有人都愁卖的状态。</p><p></p><p>以上就是我基于这波生成式AI爆发，对整个AI产业造成的影响的个人观察。也正是在这个浪潮中，Lepton正在持续帮助企业和团队在生成式AI落地的过程中找到成本、效果、效率的最佳均衡点。最后，其实可以以Richard S. Sutton——增强学习领域开山立派的一位导师，在2019年说的一句话作为总结，“在整个70年的AI科研中，最重要的经验就是，通过一个通用的方法（今天是深度学习），来利用大量的计算模型（今天是以英伟达为代表的异构GPU为基础的高性能计算），这样的方式是整个70年AI发展中最有效、最简单的方式。”</p><p></p><p>The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.</p><p></p><p>——Richard Sutton: "The Bitter Lesson"</p><p></p><p>文字经贾扬清本人确认，感谢高山书院（公众号：gasadaxue）对本文的贡献。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/c0M6oyu0e1s9sJ4ApTdZ</id>
            <title>“真男人就应该用 C 编程”！用 1000 行 C 代码手搓了一个大模型，Mac 即可运行，特斯拉前AI总监爆火科普 LLM</title>
            <link>https://www.infoq.cn/article/c0M6oyu0e1s9sJ4ApTdZ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/c0M6oyu0e1s9sJ4ApTdZ</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Apr 2024 09:56:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: C语言, GPT-2, Andrej Karpathy, 大模型训练
<br>
<br>
总结: Andrej Karpathy使用C语言徒手实现了GPT-2大模型训练，减少了对庞大外部库的依赖，使得模型训练更轻量化和高效。他的代码开源后获得了广泛关注，展示了用C语言实现大型语言模型训练的可能性。Karpathy的工作为学习和理解大型语言模型提供了重要资源。 </div>
                        <hr>
                    
                    <p></p><blockquote>徒手用 1000 行 C 语言实现，不依赖庞大的外部库，Mac 即可运行。</blockquote><p></p><p></p><p>如今这年头，徒手写神经网络代码已经不算事儿了，现在流行手搓大模型训练代码了！这不，今天，特斯拉前AI总监、OpenAI 创始团队成员Andrej Karpathy仅用1000行简洁的C代码，就完成了 GPT-2 大模型训练过程。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/18/18b9f758c3320976c307f166074c9f63.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>几个小时前，Andrej Karpathy推出了一个名为llm.c的项目，旨在用纯C语言训练LLM，这种方法的主要优势在于它显著减少了依赖库的体积——不再需要245MB的PyTorch和107MB的cPython，这样可以使得模型训练过程更为轻量化和高效。该项目还可以立即编译和运行，并且与PyTorch的参考实现完全匹配。</p><p>&nbsp;</p><p>Karpathy表示他之所以选择GPT-2作为首个工作示例，是因为它大语言模型鼻祖的定位，亦属现代AI堆栈的首次组合。因此，选择 GPT-2 作为起点，可以让我们更容易地理解和实践大型语言模型训练。</p><p>&nbsp;</p><p>徒手实现GPT-2后，Karpathy将这份代码放到了GitHub上，以MIT协议开源。短短几个小时，就超过了2500颗星，并且数据还在不断持续上涨......</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/70/70dfc5db0012180f5518d691c2d9b896.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>将大模型原理拆解得无比简单</h2><p></p><p>&nbsp;</p><p>Andrej Karpathy 是全球人工智能领域的知名科学家，也是 OpenAI 的创始成员和研究科学家。</p><p>&nbsp;</p><p>他于2009年本科毕业于多伦多大学，获得计算机科学和物理学学士学位。2011年硕士毕业于不列颠哥伦比亚大学，随后前往斯坦福大学AI Lab（SAIL）读博，师从著名学者李飞飞，是全球最早将深度学习应用于计算机视觉研究的学者之一。</p><p>&nbsp;</p><p>在求学期间，Andrej Karpathy曾在谷歌和DeepMind实习，后来在OpenAI刚刚成立时加入并担任研究科学家。直到2017年6月，他被马斯克挖去，担任特斯拉人工智能部门主管，直接向马斯克汇报。在特斯拉工作的五年里，他主导了特斯拉自动辅助驾驶系统Autopilot的开发。这项技术对于特斯拉的完全自动驾驶系统 FSD 至关重要，也是马斯克针对 Model S、Cybertruck 等车型推销的主要卖点。在各大新闻中，他被誉为“特斯拉的秘密武器”。</p><p>&nbsp;</p><p>去年Karpathy曾短暂回到OpenAI，然后又在OpenAI众人忙于内斗时抽空录制了一个长达一小时的教学视频《大型语言模型入门》。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/09/097e6065fd638962c0ee82bd46dc224e.jpeg" /></p><p></p><p>&nbsp;</p><p>Karpathy 在视频中首先介绍了一些 LLM 入门知识，然后以 Meta 推出的开源大模型 Llama 2-70b 为例进行了讲解。该模型有 700 亿参数，主要包含两个文件，分别是参数文件，文件大小为 140GB，以及运行这些参数的代码，以 C 语言为例需要约 500 行代码。</p><p>&nbsp;</p><p>Karpathy 表示只要有这两个文件再加上一台 MacBook，我们就可以构建一个独立的系统，无需联网或其他设施。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ffefdd6449d6362b2837474fe5b4555d.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>大模型训练，可以理解为是对互联网数据进行有损压缩，一般需要一个巨大的GPU集群来完成。以Llama 2-70b为例的话，就是使用了类似网络爬取的约 10TB 的文本，用6000 个 GPU ，耗资 200 万美元，训练约 12 天，最后获得基础模型。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/20/2072a2e96c9acf4978fb523baf6102c3.jpeg" /></p><p></p><p>&nbsp;</p><p>基础模型即上图里140GB的“压缩文件”（压缩率约100倍），就等于靠这些数据对世界形成了理解，那它就可以进行“预测”工作了。</p><p>&nbsp;</p><p>Karpathy之前还分享过他的学习经验，就是开始时要尝试从0开始，写一些原生代码，帮助理解消化知识点。也就是说，徒手实现代码才是最有效的学习方式。</p><p>&nbsp;</p><p>两年前，Karpathy就曾基于 PyTorch，仅用 300 行左右的代码就写出了一个小型 GPT 训练库，并将其命名为 minGPT，用这份代码揭开了GPT神秘的面纱。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/8e/8ef635be577ae58cf8fe6012e2a3544b.jpeg" /></p><p></p><p>截图来源：<a href="https://github.com/karpathy/minGPT">https://github.com/karpathy/minGPT</a>"</p><p>&nbsp;</p><p>因为大多数 GPT 模型的实现都过于庞大，而minGPT 做到了小、干净、可解释和具有教育意义，所以Karpathy的这300行代码是学习 GPT 的最佳资源之一，可以用来深入理解GPT 是如何工作的。</p><p>&nbsp;</p><p></p><h2>用C语言实现LLM</h2><p></p><p>&nbsp;</p><p>这次，Andrej Karpathy单纯通过C/CUDA实现大语言模型训练，且无需245 MB PyTorch或107 MB cPython。例如，训练GPT-2（CPU，fp32单精度）需要在单个文件中使用约1000行简洁代码，可立即编译并运行、且与PyTorch参考实现完全匹配。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/7b/7bea147ef70c91506cb2980bc256e088.jpeg" /></p><p></p><p>&nbsp;</p><p>从某种意义上说，Karpathy确实在尝试重新设计LLM的架构。他通过llm.c项目探索一种更简单、更高效的训练LLM方法。与现有LLM架构相比，这种新架构的主要亮点包括：</p><p>&nbsp;</p><p>1. 代码简洁性：仅使用约1000行代码就能完成GPT-2模型的训练，相比之下显著降低了复杂度。</p><p>2. 独立性：不依赖庞大的外部库如PyTorch或cPython，使得部署和运行更加轻便快捷。</p><p>3. 高效性：直接使用C/CUDA进行编程有望提高计算效率和训练速度。</p><p>&nbsp;</p><p>有网友问Karpathy为何不用Rust，Karpathy回复说，“我完全理解Rust的吸引力。然而，我仍然觉得 C 语言非常棒。它简单、干净、可移植，在审美上也十分优美。使用 C 语言就像直接与机器交流一样。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/b2/b291c68429d3d5ecc9ef997d9f4c4028.jpeg" /></p><p></p><p>&nbsp;</p><p>这种语言选择也让网友们纷纷感叹：</p><p>&nbsp;</p><p>“我们正在掀起一场 C 语言复兴！”</p><p></p><p><img src="https://static001.geekbang.org/infoq/18/1854da08a50f68f4f2b7ff0cdc84b81a.jpeg" /></p><p></p><p>&nbsp;</p><p>“真男人就应该用 C 语言编程。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/a7/a745a6ba051f49d4a1ad4d83255f0c72.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>Karpathy以更简单、更原始的C/CUDA架构来做LLM的训练，其中还涉及算法优化、计算资源管理等多个方面。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bd4ec03d1dab994cae359ca1da6a1585.jpeg" /></p><p></p><p>&nbsp;</p><p></p><blockquote>你会看到，项目在开始时一次性分配所有所需的内存，这些内存是一大块 1D 内存。然后在训练过程中，不会创建或销毁任何内存，因此内存占用量保持不变，并且只是动态的，将数据批次流过。这里的关键在于手动实现所有单个层的前向和后向传递，然后将它们串联在一起。例如，这里是 layernorm 前向和后向传递。除了 layernorm 之外，我们还需要编码器、matmul、自注意力、gelu、残差、softmax 和交叉熵损失。</blockquote><p></p><p>&nbsp;</p><p>“一旦你拥有了所有的层，接下来的工作只是将它们串在一起。讲道理，写起来相当乏味和自虐，因为你必须确保所有指针和张量偏移都正确排列， ”Karpathy 表示。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/d0/d05cd843fcd5a4e542a86f9de979633e.png" /></p><p></p><p>&nbsp;</p><p>另外Karpathy还在doc/layernorm/layernorm.md中附上了短小的使用教程。里面是一份简单的分步指南，用于实现GPT-2模型中的单一层，即layernorm层，希望能成为大家理解在C中实现大语言模型各层的理想起点。</p><p>&nbsp;</p><p>更重要的是，他还用自己的MacBook Pro（苹果M3 Max芯片）演示了整个训练过程，对照他之前的大模型入门教程，就可以轻松了解如今炙手可热的LLM是怎么一回事儿了。</p><p>&nbsp;</p><p></p><h4>训练过程：</h4><p></p><p>&nbsp;</p><p>首先下载数据集并token化。</p><p>&nbsp;</p><p>python prepro_tinyshakespeare.py</p><p>&nbsp;</p><p>输出结果为：</p><p>&nbsp;</p><p>Saved 32768 tokens to data/tiny_shakespeare_val.bin</p><p>Saved 305260 tokens to data/tiny_shakespeare_train.bin</p><p>&nbsp;</p><p>其中各.bin文件为int32数字的原始字节流，用于指示GPT-2 token化器的token id。或者也可以使用prepro_tinystories.py对TinyStories数据集进行标注。</p><p>&nbsp;</p><p>原则上，到这里就已经可以开始训练模型。为提高效率，可以使用OpenAI发布的GPT-2权重进行初始化，而后加以微调。为此需要下载GPT-2权重并将其保存为可在C中加载的检查点：</p><p>&nbsp;</p><p>python train_gpt2.py</p><p>&nbsp;</p><p>该脚本会下载GPT-2（124M）模型，对单批数据进行10次过拟合迭代，运行多个生成步骤，最重要的是保存两个文件：1）gpt2_124M.bin文件，包含用于在C中加载的模型权重；2）以及gpt2_124M_debug_state.bin，包含包括input、target、logits及loss等更多调试状态，对于调试C代码、单元测试及确保能够与PyTorch参考实现完全匹配非常重要。现在我们可以使用这些权重进行初始化并在原始C代码中进行训练。首先编译代码：</p><p>&nbsp;</p><p>make train_gpt2</p><p>&nbsp;</p><p>在train_gpt2编译完成后即可运行：</p><p>&nbsp;</p><p>OMP_NUM_THREADS=8 ./train_gpt2</p><p>&nbsp;</p><p>大家应根据CPU的核心数量来调整线程数量。该程序将加载模型权重、tokens，并使用Adam lr 1e-4运行数次迭代的微调循环，而后由模型生成样本。简单来讲，所有层都具有前向及后向传递实现，串联在一起形成统一的大型、手动前向/后向/更新循环。在MacBook Pro（苹果M3 Max芯片）上的输出结果如下所示：</p><p>&nbsp;</p><p>[GPT-2]</p><p>max_seq_len: 1024</p><p>vocab_size: 50257</p><p>num_layers: 12</p><p>num_heads: 12</p><p>channels: 768</p><p>num_parameters: 124439808</p><p>train dataset num_batches: 1192</p><p>val dataset num_batches: 128</p><p>num_activations: 73323776</p><p>val loss 5.252026</p><p>step 0: train loss 5.356189 (took 1452.121000 ms)</p><p>step 1: train loss 4.301069 (took 1288.673000 ms)</p><p>step 2: train loss 4.623322 (took 1369.394000 ms)</p><p>step 3: train loss 4.600470 (took 1290.761000 ms)</p><p>... (trunctated) ...</p><p>step 39: train loss 3.970751 (took 1323.779000 ms)</p><p>val loss 4.107781</p><p>generated: 50256 16773 18162 21986 11 198 13681 263 23875 198 3152 262 11773 2910 198 1169 6002 6386 2583 286 262 11858 198 20424 428 3135 7596 995 3675 13 198 40 481 407 736 17903 11 329 703 6029 706 4082 198 42826 1028 1128 633 263 11 198 10594 407 198 2704 454 680 1028 262 1027 28860 286 198 3237 323</p><p>step 40: train loss 4.377757 (took 1366.368000 ms)</p><p>&nbsp;</p><p>现在的生成结果仅给出token ids，需要将其解码回文本形式：</p><p>&nbsp;</p><p>&lt;|endoftext|&gt;Come Running Away,</p><p>Greater conquer</p><p>With the Imperial blood</p><p>the heaviest host of the gods</p><p>into this wondrous world beyond.</p><p>I will not back thee, for how sweet after birth</p><p>Netflix against repounder,</p><p>will not</p><p>flourish against the earlocks of</p><p>Allay</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://twitter.com/karpathy/status/1777427947126936026">https://twitter.com/karpathy/status/1777427947126936026</a>"</p><p><a href="https://github.com/karpathy/llm.c">https://github.com/karpathy/llm.c</a>"</p><p><a href="https://www.youtube.com/watch?v=zjkBMFhNj_g">https://www.youtube.com/watch?v=zjkBMFhNj_g</a>"</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>