<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/dXTlE57Er3MkRlZNWhi6</id>
            <title>端侧模型打响突围战！VC 疯抢，又一创企“杀”出</title>
            <link>https://www.infoq.cn/article/dXTlE57Er3MkRlZNWhi6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/dXTlE57Er3MkRlZNWhi6</guid>
            <pubDate></pubDate>
            <updated>Fri, 28 Jun 2024 10:17:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型融资, AI独角兽, Transformer架构, 端模型
<br>
<br>
总结: 2024年，AI领域大模型融资持续升温，各地AI独角兽崭露头角，以小参数、低成本的端模型挑战传统Transformer架构。刘凡平率领的RockAI推出非Attention机制的Yan大模型，实现原生无损部署在端侧设备上，引领AI大模型进化新方向。 </div>
                        <hr>
                    
                    <p>6 月，三笔巨额融资掀开大模型战事新篇章。</p><p></p><p>前脚，加拿大 Cohere 以 50 亿美元估值揽获 4.5 亿美元融资，英伟达、思科助力；后脚，法国 Mistral AI 喜提 6 亿欧元，General Catalyst 领投；随后，日本 SakanaAI 也传出即将斩获超 1 亿美元融资，估值飚至约 11.5 亿美元。</p><p></p><p>春江水暖鸭先知，国际 VC 押注各地 AI 独角兽强势出圈背后，一个共性趋势随即浮现：PK OpenAI，他们正以小参数、低成本落地端侧“突围”。</p><p></p><p>Cohere 开源的新一代大模型 Aya 23，以 80 亿和 350 亿两种参数，支持 23 种语言；Mistral AI 去年发布的 Mistral 7B，以 70 亿参数打败了数百亿参数的开源大语言模型霸主 Llama 2，另一款模型 Mistral Large 开发成本低于 2000 万欧元（约 2200 万美元），对比 GPT-4 的开发成本，更是打掉了超 4/5；再到 Sakana 这边，其以核心的“模型合并”技术来自动化“进化”算法，号称对算力资源的需求极小、能将数据学习周期缩短数百倍。</p><p></p><p>群雄逐鹿之下，这场 AI 盛宴行至 2024，已然不再是一场堆算力、垒数据的“烧钱”游戏。</p><p>寻找 Transformer 外的可能，</p><p></p><p></p><h2>“天选”端模来了</h2><p></p><p></p><p>身处大模型一线，近半年，刘凡平对底层技术架构的创新和突破这一趋势有着明显的直接感受。</p><p></p><p>“在全球范围内，一直以来都有不少优秀的研究者试图从根本上解决对 Transformer 架构的过度依赖，寻求更优的办法替代 Transformer。就连 Transformer 的论文作者之一 Llion Jones 也在探索‘Transformer 之后的可能’，试图用一种基于进化原理的自然启发智能方法，从不同角度创造对 AI 框架的再定义。”</p><p></p><p>他看到，技术变化永远走在最前面，需要时时刻刻保持“不被颠覆”的警惕，但一方面，这个 80 后创业者看到新技术带来新产品、新市场机遇的出现，又对行业利好倍感兴奋。</p><p></p><p>在这场对标 OpenAI 的竞赛中，刘凡平也早就做好了准备，其带队的 RockAI 亦走出了一条属于自己的进化路径。</p><p></p><p>自成立伊始，RockAI 就不曾是 Transformer 学徒，即便是在“百模大战”打得火热的去年，刘凡平就意识到 Transformer 架构底层设计逻辑对训练数据量的要求极大，虽是大模型的智能体现，却难以避免“一本正经的胡说八道”的幻觉问题，包括训练的资源消耗已成行业通病。</p><p></p><p>甚至连 Transformer 这个架构的设计者 Aidan Gomez，都对“做了很多浪费的计算”一声叹息，希望“Transformer 能被某种东西所取代，将人类带到一个新的性能高原。”</p><p></p><p>可谓，成也萧何败也萧何。</p><p></p><p>但更大的挑战在于，Transformer 在实际应用中的高算力和高成本，让不少中小型企业望而却步。其内部架构的复杂性，让决策过程难以解释；长序列处理困难和无法控制的幻觉问题也限制了大模型在某些关键领域和特殊场景的广泛应用。</p><p></p><p>在行业对于高效能、低能耗 AI 大模型的需求不断增长下，彼时，刘凡平就一直在思考“大模型动辄上万亿的 token 训练是否真的必要”，对 Transformer 模型不断的调研和改进过程中，更让他意识到了重新设计大模型的必要性。</p><p></p><p>以人类大脑几十亿的训练量来看，他判断，数据、算力并不是最终的瓶颈，架构、算法才是重要的影响因素，就此开启了 RockAI“破坏式”自研突围。</p><p></p><p>1 月，刘凡平带着国内首个非 Attention 机制的通用自然语言大模型——Yan1.0 模型公开露面。</p><p></p><p>当时，1.0 版通过对 Attention 的替换，将计算复杂度降为线性，大幅降低了对算力的需求，用百亿级参数达成千亿参数大模型的性能效果——记忆能力提升 3 倍、训练效率提升 7 倍的同时，实现推理吞吐量的 5 倍提升。</p><p></p><p><img src="https://static001.geekbang.org/infoq/95/9539861c739f0cf541257d1c1e833a0a.png" /></p><p></p><p>更令人欣喜的是现场，Yan 1.0 模型在个人电脑端的运行推理展示，证实了其可以“原生无损”在主流消费级 CPU 等端侧设备上运行的实操性。</p><p></p><p>要知道，原生无损对应的反面就是有损压缩，后者是目前大模型部署到设备端的主流方式。</p><p></p><p>大热的 AIPC 是把 Transformer 架构的模型通过量化压缩部署到了个人电脑，甚至 70 亿参数的大模型还需要定制的 PC 芯片提供算力；就连 Llama3 8B 以每秒 1.89 个 token 的速度运行树莓派 5，支持 8K 上下文窗口的战绩，也是止步于“有损压缩”。</p><p></p><p>更大的模型效果更好，但是如果不通过量化压缩是部署不到个人设备上的，恰好说明了 Scaling law 的局限。</p><p></p><p>同时，有损压缩如同把平铺的纸揉小后有褶皱般放入，让多模态下的性能损失无法恢复到原有状态去进行模型训练，更直接导致卡住不动、死机等不确定问题的出现，甚至三五分钟才能蹦完一句话。</p><p></p><p>“去”量化压缩这一步意味着 Yan 模型在设备端运行避开了多模态下的性能损失，以及具备再学习的能力，也就是说在兼容更多低算力设备上，是“天选级”端侧模型。</p><p></p><p></p><h2>同步学习，让模型边跑边进化</h2><p></p><p></p><p>“原生无损”部署到个人电脑，这只是 Yan 1.0 的表现。</p><p></p><p>刘凡平还有 2 个疑问待解，一是能不能在更低算力、更普适的设备上部署大模型；二是部署在端侧以后，模型能不能个性化的即时学习。</p><p></p><p>而这两个问题的实现，直接带着 RockAI 朝着 Yan 2.0 进发。</p><p></p><p>看到 AIPC 依然是云端大模型为主，离线状态下模型基本只勉强可用，而用户的个人隐私在云端模式下依然待解，刘凡平意识到要找到更低算力且可大部分时间离线使用的设备来做进入设备的“敲门砖”。</p><p></p><p>“PC 或者高端手机其实模型量化都能跑，但是高端设备的 GPU 算力跟低端设备差距很大，所以 PK 得往更低端设备走，才能跟设备厂商获得谈的资格。”</p><p></p><p>于是，他的目光便落到了树莓派上。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fd1483509627a1b2327775d11373dfd7.png" /></p><p></p><p>这个袖珍型小巧却又性能强大的微型电脑，可广泛应用于物联网、工业自动化、智慧农业、新能源、智能家居等场景及设备，譬如门禁、机器人等终端，同时，大部分情况没有联网。</p><p></p><p>这就意味着，跑通树莓派，等同于打开了低算力设备端的大门以及不联网的多场景应用。</p><p></p><p>为了“拿下”树莓派，刘凡平得进一步实现 Yan 模型的降本增效，于是在算法侧，基于仿生神经元驱动的选择算法便出现在了眼下的 Yan 1.2 模型上。</p><p></p><p>参考人脑的神经元大概是 800-1000 亿，功耗大概是 20-30 瓦，而一台 GPU 算力服务器功耗能到 2000 瓦，刘凡平认为主流大模型的全参数激活，本身就是不必要的大功耗浪费。</p><p></p><p>而基于仿生神经元驱动的选择算法，便是使大模型可以根据学习的类型和知识的范围分区激活，如同人开车跟写字会分别激活脑部的视觉区域和阅读区域一般，不仅可以减少数据训练量，同时也能有效发挥多模态的潜力。</p><p></p><p>据悉，在 3 月类脑分区激活的工作机制实现后，甚至 10 亿级参数的 Yan 模型通过改进在 0 压缩和 0 裁剪的情况下在一台 7 年前生产的 Mac 笔记本的 CPU 上跑通本地训练过程，5 月 Yan 1.2 模型便成功跑通树莓派。</p><p></p><p>值得注意的是，模型分区激活不仅可以降低功耗，同时还能实现部分更新，也就意味着部署后还具备持续学习能力，而这又是 Transformer 一众学徒的“软肋”。</p><p></p><p>众所周知，大模型的出现也带来一种开发范式：先通过预训练让大模型具备一定的基本能力，然后在下游任务中通过微调对齐，激发模型举一反三的能力。</p><p></p><p>这就类似先花大量的时间和资源把 1 岁孩子封闭式培养到成为大学生，然后在不同的工作场景里进行锻炼对齐。</p><p></p><p>这种范式统一了以往处理不同自然语言任务需要训练不同模型的问题，但也限制了模型在不同场景的应用。</p><p>如果换一个没有经过预训练的工作场景，一切都要从头再来，两个字概括：麻烦。</p><p></p><p>一个离自主进化遥远的 Transformer 大模型，反映到现有实践中，那就是一旦内容变化，往往要 1-2 个月去把数据清掉后，再重新训练后进行提交。</p><p></p><p>预训练完之后再大规模反向更新，无论从算力、时间还是经济成本，对企业而言“难以接受”，也让刘凡平在低消耗、显存受限的情况下，为实现端侧训推同步，在模型分区可部分激活更新下，持续寻找反向传播的更优解，试验能更低代价更新神经网络的方案。</p><p></p><p>从反向传播对参数的调节过程来看，只要模型调整足够快、代价足够小，就能更快达到预期，实现从感知到认知再到决策这一循环的加速，对现有知识体系进行快速更新。</p><p></p><p>如此一来，通过模型分区激活 + 寻找反向传播更优解“两步走”，就能实现模型的边跑边进化，“同步学习”的概念在 RockAI 逐步清晰。</p><p></p><p></p><h2>寻找设备端的智能，谁能成为具身“大脑”？</h2><p></p><p></p><p>如上，把一个训练完的 Transformer 大模型比作大学生，那么，一个可同步学习的 Yan 模型，在刘凡平看来，就是一个正在咿呀学语的孩子。</p><p></p><p>“从小在各种环境下学习，建立知识体系，又不断推翻重建，每一天都有新的体悟，会成独有的知识体系，最终个体多样性会带来群体智慧和分工协作。”</p><p></p><p>而这样个性化的端侧模型有多重要呢？可以设想：在一个智能城市中，每个家庭的智能家居系统都具备了 Yan 模型这样的能力。这些系统可以根据每个家庭成员的习惯、喜好以及环境变化进行自主学习，并做出相应的调整，个性化服务身边的每一个人。</p><p></p><p>在刘凡平的设想中，智能“大脑”，关键在于实现模型在边缘计算中的持续学习能力和适应能力。具备同步学习能力的 Yan 2.0 模型部署到手机、电脑，甚至电视、音响等各类设备后，会根据你说的话和场景进行自主学习，判断出你喜欢的事情，通过跟用户对齐，越来越具备个性化价值，最终形成可交互的多样性智能生态。</p><p></p><p>不过，刘凡平也坦言，相较于 B 端，目前设备端依然是大模型的蓝海市场，离终极的个性化 AI 还差一步。</p><p>但这，也给了具备低成本低算力基因的 RockAI，从“为设备而生”到“为设备而用”抢占先机的可能。</p><p></p><p>Yan2.0 会在年底或明年初面世， 在他看来，这些设备前期的适配工作做足至关重要，现阶段是系统适配各种硬件，端侧模型需要结合实际载体（即硬件）去做适配研究和迭代改进。</p><p></p><p>在树莓派跑通后，很多机器人厂商也找到了刘凡平，从某种意义上来说，他们也在寻找具身大脑的可能，一家教育机器人公司甚至给到了刘凡平“愿意第一时间集成 Yan 2.0”的回复。</p><p></p><p>对于具身智能这一爆火命题，刘凡平很坦率，从身到脑都需要搅局者，但他也有“野心”，去成为那个破局人：在技术创新、商业化同步发力。</p><p></p><p>四个月前，在 Yan 架构的发布会上，他曾提出了打造“全模态实时人机交互系统”的理念，期望 Yan 模型未来向全模态、实时人机交互、训推同步的方向持续升级，使 AI 技术更加易于获取和使用，推动普惠人工智能的发展。</p><p></p><p>而如今，随着 Yan 2.0 将逐步把多模态的视觉、触觉和听觉能力补齐，并结合同步学习的能力，一个在感知、认知、决策、行动四个方面得到全面提升的机器人似乎也在具象化。</p><p></p><p>可以预见：在感知方面更多模态输入后，机器人同时拥有眼睛和耳朵，可以实时看到和听到信息，然后把接受到的信息进行认知理解，随着理解加深，能做出对应的有倾向性的、个性化的判断，并支配四肢行动。</p><p></p><p>一个大模型在更加便携的设备或终端中进行无损部署的蓝图，正在徐徐展开。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/6Ez6RUXNKcpQXUhtjzEa</id>
            <title>迈进GenAI时代，亚马逊云科技的“魔法”是什么</title>
            <link>https://www.infoq.cn/article/6Ez6RUXNKcpQXUhtjzEa</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/6Ez6RUXNKcpQXUhtjzEa</guid>
            <pubDate></pubDate>
            <updated>Fri, 28 Jun 2024 10:17:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 亚马逊云科技, 云计算, 生成式AI, 技术革命
<br>
<br>
总结: 英国著名科幻作家亚瑟·克拉克曾说过：“任何非常先进的技术，初看都与魔法无异。”亚马逊云科技作为云计算的先驱，通过引领技术革命，将云计算和生成式AI等先进技术转变为可配置资源，极大地简化了企业IT基础设施的管理，改变了人们的生活和工作方式。其服务不仅帮助企业走向全球市场，还在各个领域赋能千方百业，展现着改变世界的力量。 </div>
                        <hr>
                    
                    <p>英国著名科幻作家亚瑟·克拉克曾说过：“任何非常先进的技术，初看都与魔法无异。”这句话在描述亚马逊云科技所引领的云计算革命时，显得尤为贴切。</p><p>&nbsp;</p><p>作为<a href="https://qcon.infoq.cn/2024/shanghai?utm_source=infoq&amp;utm_medium=conference">云计算</a>"的先驱，亚马逊云科技将网络、存储、数据库和计算等技术转变为可配置资源，极大地简化了企业IT基础设施的管理。如今，亚马逊云科技在全球33个地区提供超过240项全功能服务，每项服务都旨在消除创新障碍，降低创新门槛。Amazon S3作为众多用户上云的第一步，标志着从传统存储向云计算驱动的数字化转型的开始。在2023年的re:Invent全球大会上，亚马逊云科技发布了Amazon S3 Express One Zone，进一步提高了开发人员和数据科学家的工作效率，并通过不断优化自研芯片和处理器，为客户的应用程序提供了更高的性价比。</p><p></p><p>Netflix利用亚马逊云科技的计算、存储和服务网络，将流媒体播放服务拓展到全球190多个国家，彻底改变了人们的娱乐方式。Moderna在疫情期间利用亚马逊云科技的机器学习服务，在短短两天内完成了mRNA新冠疫苗的基因测序，并在25天后进行了第一批临床试验，这一过程在过去通常需要数年时间。</p><p>&nbsp;</p><p>云计算的强大能力也在帮助越来越多的中国企业“走出去”，在全球市场占据一席之地。OPPO作为新一代中国出海企业的先锋，通过与亚马逊云科技的合作，成功实现了从制造业巨头到互联网手机品牌的转型。拥有超过2亿海外用户的WPS AI办公软件，也在<a href="https://www.infoq.cn/article/0F4Ig1DlH4teqZDPqfMv?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">亚马逊云科技</a>"的帮助下，将生成式AI能力全面引入其产品线，提升了用户体验和创作效率。</p><p>&nbsp;</p><p>云计算技术不仅颠覆了科技界，也深刻地改变了我们生活和工作的方式，而如今，<a href="https://qcon.infoq.cn/2024/shanghai/track/1718">生成式AI</a>"正如曾经的云计算一样拥有着改变世界的力量。</p><p>&nbsp;</p><p>在AI技术上，亚马逊云科技提出的<a href="https://qcon.infoq.cn/2024/shanghai/track/1715">生成式AI</a>"三层技术栈，也在赋能千方百业。该技术栈包括：底层的基础设施、中间层的Amazon Bedrock服务以及顶层以Amazon Q为代表的的应用，每层都致力于消除创新障碍，降低创新门槛。</p><p>&nbsp;</p><p>底层以GPU和自研<a href="https://qcon.infoq.cn/2024/shanghai/track/1715">芯片</a>"为核心，为生成式AI提供基础设施支持。GPU是运行生成式AI的关键，亚马逊云科技为客户提供了包括NVIDIA GPU在内的多种高性能计算选择。此外，其自研芯片如Amazon Trainium和Amazon Inferentia，大幅降低了机器学习训练和推理的成本，同时提高了能效。</p><p>&nbsp;</p><p>Amazon Bedrock全面托管服务作为中间层，能提供高性能的基础模型，支持包括模型选择、模型定制、应用集成等功能。企业可以轻松导入和评估基于开源架构的定制模型，并通过Bedrock的Knowledge Base和Agents功能，利用企业数据源创建个性化应用。</p><p>&nbsp;</p><p>作为技术最顶层，Amazon Q是一系列生成式AI助手应用，包括Amazon Q Developer和Amazon Q Business。这些应用可以帮助开发人员提升效率，加速软件开发，同时让企业从数据中获得洞见，并构建应用程序。</p><p>&nbsp;</p><p>通过Amazon Bedrock和Amazon Q等服务，企业可以轻松构建和部署生成式AI应用，无论是在软件开发、内容创作还是数据分析方面，都能从中受益。</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/cH43lBKw389PDH9V4xZa</id>
            <title>开启智能体的多元宇宙！金融大模型城市环游带你走进智能金融时代</title>
            <link>https://www.infoq.cn/article/cH43lBKw389PDH9V4xZa</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/cH43lBKw389PDH9V4xZa</guid>
            <pubDate></pubDate>
            <updated>Fri, 28 Jun 2024 09:57:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 金融行业, 大模型技术, 智能体技术, 智能化转型
<br>
<br>
总结: 金融行业在技术革新中积极探索大模型和智能体技术，这些前沿技术为行业带来革新，推动智能化转型。通过举办“金融大模型城市环游”活动，展示大模型技术在金融场景中的应用，以及智能体技术的前沿应用，促进行业内部技术交流和创新。 </div>
                        <hr>
                    
                    <p></p><p>金融行业始终站在技术革新的风口浪尖，自 2023 年开始，金融企业就对大模型等前沿技术做出了积极探索，智能涌现、自主决策等名词不断撬动着人们的期待，生成式 AI、AI Agent 等技术更为整个行业带来了无限幻想。</p><p></p><p>智能体技术的发展更为金融行业带来了革新的曙光。智能体技术以其较低的上手成本和灵活的应用模式，为金融机构提供了一个“尝鲜”大模型的机会。它不仅帮助金融机构探索技术和场景的最佳匹配，更助力大模型技术从实验室走向大众应用，从而激发企业内部自下而上的智能化转型浪潮。</p><p></p><p>为了探索大模型、智能体技术在金融场景中的实践应用，7 月火山引擎将于深圳、北京两地，携手 AI 应用开发平台扣子（coze.cn）、NVIDIA、凤凰网财经频道、InfoQ 联合举办“金融大模型城市环游”智能体专场活动。</p><p></p><p></p><h2>什么是“金融大模型城市环游”？</h2><p></p><p></p><p>金融大模型城市环游由火山引擎携手 NVIDIA 共同打造，是一场面向金融和金融科技从业者的技术沙龙活动。活动聚焦大模型前沿技术，立足大模型在金融行业场景中的多元应用，力求理论和实践结合，全方位展示 AI 前沿技术对于金融机构数智化转型的推动。</p><p></p><p></p><h2>扣子专场，感受金融智能体前沿应用</h2><p></p><p></p><p>在众多智能体平台中，扣子（coze.cn）凭借其在 AI 应用开发的强大功能收获了众多关注，也受到了金融行业的青睐。极速构建、智能交互、灵活高效，这些特质为企业业务增效和用户体验革新带来了机会，通过不断的技术创新和市场培育，扣子将不仅仅是金融行业的一个可用工具，更将成为推动金融行业智能化转型的重要力量。</p><p></p><p>这一次的金融大模型城市环游，火山引擎将联合扣子（coze.cn）平台于 7 月 12 日、7 月 19 日，分别在深圳、北京举办智能体专场。活动中，你不仅能现场聆听金融、AI 业界大咖的行业洞察，还能现场动手，打造属于自己的金融智能体。</p><p></p><p>三大活动亮点，超棒体验等你现场解锁：</p><p></p><p></p><h4>&nbsp;亮点一：业界大咖在线分享，带你探索智能金融未来</h4><p></p><p></p><p>活动持续一天。上午的“智话金融”环节，将围绕“智能体在金融行业中的应用”展开精彩演讲。金融科技领域的技术专家们将为参会者带来深度分享，他们分别是：</p><p></p><p>火山引擎金融行业解决方案负责人</p><p></p><p>NVIDIA 企业服务技术专家</p><p></p><p>金融企业 CTO、数字化业务负责人</p><p></p><p>扣子 Bot Hackathon 优胜队伍</p><p></p><p>他们将从 AI 智能体协作共生、企业大模型部署、智能体搭建等角度展开深入分享。“智话金融”希望从金融行业的各个环节入手，从底层技术到解决方案，从业务落地到搭建实操，打破行业与技术的壁垒，分享智能体技术在金融领域的最佳实践与前沿应用。</p><p></p><p></p><h4>&nbsp;亮点二：手把手教学，两周晋升捏 bot 高手！</h4><p></p><p></p><p>活动下午的“动手实验营”将进行扣子的现场实操、指导和切磋。来到现场动手实操之前，扣子学习之旅也将提前开启，以线上学习 + 线下实操的形式，助力你两周内迅速晋升捏 bot 高手！活动报名成功后，你将正式开启一场金融 bot 的体验之旅：</p><p></p><p>1.添加小助手微信，加入官方社群，领取扣子搭建学习资料</p><p></p><p>2.在报名者中寻找同伴，结成队伍，共同学习，构思方向！</p><p></p><p>3.明确分工，提前脑暴 bot 方向，做好实操准备！</p><p></p><p>4.活动现场手捏金融 bot，并路演展示成果！</p><p></p><p>在活动现场开发自己的金融智能体时，扣子技术专家将会现身答疑指导；路演展示 bot 之后，评分排名靠前的小组还可获得丰富奖励！</p><p></p><p></p><h4>&nbsp;亮点三：行业技术盛会，活动收益多多！</h4><p></p><p></p><p>参与动手实验营，你可获得：</p><p></p><p>路演排名靠前者，获优胜礼！</p><p></p><p>有机会参与官方访谈，与扣子官方面对面切磋！</p><p></p><p>你制作的 bot 有机会登上官方推荐位，被更多人关注使用！</p><p></p><p>这更是一次难得的专属于金融智能体的行业盛会：</p><p></p><p>百余位金融领域从业者现场参会，一次不可多得的行业交流</p><p></p><p>大模型、智能体领域专家现场答疑，分享技术细节</p><p></p><p>在分享与交流中共进，在实践与共创中成长，这不仅是一场技术的盛宴，更是一次思想的碰撞，一个创新的起点。我们诚邀您的参与，共同开启金融智能的未来新篇章！</p><p></p><p><img src="https://static001.geekbang.org/infoq/c1/c1b7597c9bc64dfce9debad51355ebe7.webp" /></p><p></p><p>了解完以上大会亮点，如果你想线下参与金融大模型城市环游·智能体专场，那么请立即点击下方链接报名，深圳、北京活动报名同步开启！不要犹豫，与我们共同开启这场技术环游！</p><p>https://www.infoq.cn/form/?id=2238&amp;utm_source=tuiwen&amp;sign=iq_667d4b3eccfc1</p><p></p><p>报名后 3 天内告知报名结果，选择参与动手实验营的朋友，请注意活动通知短信，我们将会邀请您进入官方社群。具体活动地点将于报名审核通过后，以短信形式通知。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/F55MGfYXquNuK6s1cqA1</id>
            <title>万字干货！手把手教你如何训练超大规模集群下的大语言模型 | QCon</title>
            <link>https://www.infoq.cn/article/F55MGfYXquNuK6s1cqA1</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/F55MGfYXquNuK6s1cqA1</guid>
            <pubDate></pubDate>
            <updated>Fri, 28 Jun 2024 08:58:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大语言模型训练, 超大规模集群, 大模型训练调参, 混合并行
<br>
<br>
总结: 本文介绍了快手总结的一套超大规模集群下大语言模型训练方案，通过细致的建模解决了大模型训练调参困难的问题。演讲结合在快手超算集群上的大模型训练经验，阐述了大模型训练在超大规模集群下遇到的挑战和热点问题的演变，以及对应的解决方案。同时，探讨了大模型的发展趋势和训练领域的技术探索方向。文章还介绍了大模型的特点和为什么需要将模型扩展到如此规模，以及训练引擎的定位和训练方案好坏的指标。最后，讨论了分布式训练中的主要难点和混合并行中的经典并行方案。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>演讲嘉宾 | 刘育良 AI 平台大模型训练负责人审核｜傅宇琪 褚杏娟策划 | 蔡芳芳</blockquote><p></p><p></p><p>快手总结了一套超大规模集群下大语言模型训练方案。该方案在超长文本场景下，在不改变模型表现的情况下，训练效率相较 SOTA 开源方案，有显著的吞吐提升。通过细致的建模，可保证 Performance Model 十分接近真实性能，基于此 Performance Model，解决了大模型训练调参困难的问题。</p><p></p><p>本文整理自快手 AI 平台大模型训练负责人刘育良在 <a href="https://qcon.infoq.cn/2024/beijing?utm_source=infoq&amp;utm_medium=conference">QCon 2024 北京</a>"的分享“&nbsp;超大规模集群下大语言模型训练的最佳实践”。演讲结合在快手超算集群上的大模型训练经验，阐述大模型训练在超大规模集群下遇到的挑战和热点问题的演变，以及对应的解决方案。同时，针对最具挑战的超长文本场景，进行案例分析。最后，根据未来大模型的发展趋势，对训练领域的技术探索方向进行探讨。</p><p></p><p>本文由 InfoQ 整理，经刘育良老师授权发布。以下为演讲实录。</p><p></p><p>简单介绍一下背景，下图清晰地描述从过去到现在，即 23 年之前所有主流大模型的发展历程。从技术架构的角度来看，Transformer 架构无疑是当前大模型领域最主流的算法架构。其中包括以 Bert DIT 为代表的 Encoder-Only 结构，以 T5 为代表的 Encoder-Decoder 结构，以及现在非常火热的 GPT 系列的 Decoder-Only 结构，这也正是我今天想要讨论的重点。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f1/f19bbbf81249368735279499f2e1cb52.png" /></p><p></p><p>大模型这个名字非常直观地表达了其主要特点，那就是“大”。具体量化来说，参数数量大，比如从 LLAMA2 的 70B 到 GPT-3 的 175B，再到 GPT Moe 的 1.8T。其次，数据量大，我们训练一个大模型通常需要达到 T 级别 tokens 的数据量。再者，由于模型尺寸巨大和数据量庞大，随之带来的是巨大的计算量，基本上现在表现良好的大模型都需要 1e24 Flops 级别以上的计算量。</p><p></p><p>那我们为什么需要将模型扩展到如此规模？或者说，为什么模型越大效果越好呢？大模型持续扩大规模会变强的理论基础是 scaling law。接下来展示的这张图来自 OpenAI GPT-4 的技术报告，scaling law 简单来说就是模型的能力与计算量有强烈的正相关性。因此，我们可以通过不断增加模型规模和数据规模来提升模型的能力。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/fe/fe7fe091961ee6a95f832d1e42b5d551.png" /></p><p></p><p>接下来，我想和大家探讨一下训练引擎的定位，用一句话来概括就是“工欲善其事，必先利其器”。</p><p></p><p>首先要做的是提供一套可持续扩展的工具箱，这样就可以不断扩展模型规模、数据规模和序列长度，从而提升模型的表现。其次，我们要提高扩展效率，即提高 scaling efficiency。如果将刚才提到的 scaling law 的横轴从计算量换成计算卡时，那么我们的目标就是通过提高训练效率来减少总体的训练时间，进而增加 scaling law 的斜率。</p><p></p><p>作为大模型算法解决方案的提供方，我们要与算法进行联合优化，从训练和推理效率出发，提出模型结构的建议。同时，作为超算集群的使用方，我们需要根据大模型的典型通信模式和计算模式，提供组网策略和服务器选型的建议。</p><p></p><p>接下来，我想介绍一个衡量训练方案好坏的指标，即 MFU。MFU 的计算公式是有效计算量除以训练时间再除以理论算力。这里提到的 MFU 计算公式与之前论文发表的有所不同，原因在于当前主流的大语言模型都采用了 causal mask。对于特定的模型和特定的集群，有效计算量和理论算力都是恒定的，因此我们的目标是通过减少训练时间来提升 MFU。</p><p></p><p>为了提升 MFU，我们能做的主要有三点：</p><p></p><p>减少无效的计算，这通常来自于重计算；提高集群稳定性，减少因稳定性问题导致的集群不可用时长；减少通信的影响，这将是接下来讨论的核心内容。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/00/0023617aaf633aa2ed187d41c0c41190.png" /></p><p></p><p></p><h3>分布式训练的主要难点</h3><p></p><p></p><p>与小模型相比，大模型的挑战可以概括为“放不下和算不完”。以 GPT-3 为例，单是模型就需要 2,800 GB 的存储空间。而且，主流模型的计算量之大，以至于如果使用单张 A100 显卡，需要计算 101 年才能完成，这显然是不切实际的。</p><p></p><p>我们的解决方案是直接的，即通过混合并行的方式来实现分开放和一起算。具体来说，我们把模型状态和中间激活值分散在整个集群上，然后通过必要的通信来完成联合训练。但混合并行也带来了问题，它引入了大量的通信，这导致训练效率急剧下降。因此，在大模型训练中，我们可能需要做的工作主要集中在两个方面：第一，减少通信量；第二，降低通信对计算和训练的影响。这两项工作对于提升大模型训练的效率至关重要。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/94/9437e6f5cd4acd8436480c4d6e85bb67.png" /></p><p></p><p>简单介绍一下混合并行中经典的三种并行方案。首先是数据并行，简称 DP。正如其名，数据并行是将数据分割到不同的计算设备上，然后由这些设备完成各自的计算任务。第二种是张量并行，简称 TP。张量并行是将模型中某些层的参数分散到不同的设备上，每个设备负责完成部分的计算工作。第三种是流水并行，简称 PP。流水并行是将模型的不同层切分到不同的计算设备上，类似于流水线的工作方式，各个设备协同完成整个模型的计算过程。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/25/25e7f0594f444bc69097d2babb261359.png" /></p><p></p><p>现在我来分享一下在实际操作中，训练大模型时遇到的一些热点问题的演变。</p><p></p><p>首先，随着集群规模的扩大，即 GPU 数量的增加，而问题规模，也就是模型的大小保持不变，这导致了 PP Bubble 急剧增加。为了解决这个问题，我们引入了 interleaved pipe。然而，这种方法也带来了另一个问题，即 PP 的通信量成倍增加。集群规模的扩大同时也导致单个 iteration 的计算量成比例下降，但 DP 的通信时间与参数量成正比，所以通信时间实际上并没有减少，这导致 DP 的通信开销持续扩大。</p><p></p><p>随着我们从 66b 模型扩展到 175b，再到更大的模型规模，我们需要将 TP 的尺寸从 2 增加到 8，这导致了 TP 的通信量大幅增加。同时，由于 A800 和 H800 集群内部的 Nvlink 被阉割，这在千亿参数模型训练时，TP 的通信开销实际上超过了 30%。最后，随着 context window size 的扩大变得越来越重要，序列长度的增加，原有的方案要么需要进行 TP 跨机操作，要么会引入大量的重计算。这导致在 long context 场景下，原有的训练方案的效率极低。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/66/66478a2e4484886335741bdf1266eaea.png" /></p><p></p><p></p><h3>大模型训练在超大规模集群下的挑战与解决方案</h3><p></p><p></p><p>随着模型规模和集群规模的扩大，通信在训练过程中的占比越来越大。为了更直观地展示这一现象，我提供了两张时间线图，它们没有应用计算通信重叠技术。第一张图突出显示了在实现 DP 重叠前的数据并行通信状态，第二张图则突出显示了在实现 TP 重叠前的张量并行通信情况。</p><p></p><p>从图中我们可以看到，在端到端的训练过程中，DP 的通信占比实际上超过了 15%，而 TP 的通信时间占比也超过了 30%。因此，减少通信对训练的影响，对提升训练效率至关重要。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b8/b843d314c7240a82d665f24fe44af10b.png" /></p><p></p><p></p><h4>DP Overlap</h4><p></p><p></p><p>我们实现 DP overlap 的方法，借鉴了 ZeRO 3 的设计理念。ZeRO 的实现方式是将优化器状态分散到不同的 DP rank 上。通过 all-gather 操作来获取完整的权重，然后使用 reduce-scatter 操作将梯度累加到不同的 rank 上。由于数据依赖于第一个模型块，前向传播（forward）只依赖于第一次 all-gather。因此，在这次计算过程中，我们可以利用这段时间来完成其他 all-gather 的通信。除了第一块模型之外，其余的 all-gather 操作都可以与前向传播重叠。对于反向传播（backward），除了最后一次的 reduce 操作外，所有的 all-gather 操作都可以与反向传播重叠。</p><p></p><p>我们将这种思路应用到了混合并行中。通过分析数据依赖，我们发现情况几乎是一致的。例如，前两次的前向传播都只依赖于第一个 all-gather。在这段时间内，我们同样可以用来掩盖第二次的 all-gather 操作。类似地，reduce-scatter 操作也可以被反向传播掩盖。由于只有第一个 pipeline stage 的通信无法被重叠，所以重叠的比例是 1 减去 v 分之一，其中 v 代表虚拟 pipeline stage 的数量。当然，我们也可以通过进一步划分来完成第一个 pipeline stage 通信内容的重叠，但为了简化我们后续的讨论，我们暂时不考虑这种情况。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/75/759ff62e843d6bc03a6409615a8c49a9.png" /></p><p></p><p>DP overlap 的方案在理论上看起来非常吸引人，但实际应用中，我们真的能显著提升训练效率吗？在进行 DP overlap 优化时，我们遇到了三个主要问题。首先，是通信和计算资源之间的竞争问题。当通信和计算操作同时进行时，它们会争夺有限的硬件资源，这可能会影响整体的系统性能。其次，在混合并行场景下，DP overlap 还可能带来 PP bubble 的问题。第三，不同通信资源的争抢还可能导致网络拥塞。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/0d/0d7baef95626d40b2f334b38589271d8.png" /></p><p></p><p>我们来谈谈通信与计算之间的资源竞争问题。最突出的问题是 SM 资源的竞争。简单来说，通信会占用一部分 SM 资源，这进而会影响计算的性能。然而，我们在进行性能分析后发现，用于计算的 SM 数量与通信占用的 SM 数量并不匹配。</p><p></p><p>经过更深入的分析，我们发现在 Volta 架构之后，TPC 上的 SM 会共享其配置的共享内存。以 A800 为例，当一个 TPC 为通信内核分配了共享内存后，该 TPC 内的另一个 SM 也会共享这个共享内存配置，导致计算 kernel 无法复用这部分被分配出去的 SM。此外，在 Hooper 架构上，或者更准确地说，是 SM90 以后，我们发现系统会将一个 SM 内的一些 thread block 组织在一起形成一个 virtual cluster，然后以 cluster 为单位进行调度。这可能导致 sm 碎片问题。</p><p></p><p>我们发现通信与计算之间的相互影响主要与通信的 CHANNELS 有关。CHANNELS 越多，通信占用的 SM 数量也就越多，这导致计算速度变慢。我们的测试是使用 A800 显卡进行的，配备了四张网卡的 A800 来进行测试。从表格中可以看到，当通信的 NCHANNELS 数量小于网卡数量时，通信速度会显著下降。而当 CHANNELS 数量大于网卡数量时，通信速度几乎不再提升。如果继续增加 NCHANNELS 的数量，只会进一步导致计算速度变慢。因此，在综合考虑通信速度和计算时间的增量之后，我们选择了整体最优的通信 CHANNELS 数量。通过前面的分析，我们可以发现，通过牺牲一定的通信带宽，可以达到通信与计算的全局最优状态。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/55/55025533b2a0551a395a5d9935c12c3e.png" /></p><p></p><p>然而我们会发现一个问题，即并非所有通信都能够与计算进行 overlap。如果我们降低全局的通信 CHANNELS 数量，那么我们的策略可能在一定程度上损害到为 overlap 计算的通信效率。为了解决这个问题，我们区分对待了 overlap 计算的通信和非 overlap 计算的通信。对于 overlap 计算的通信，我们会综合考虑通信速度和计算时间增量，然后调整出一个最优的 CTA（Compute Thread Array）。而对于非 overlap 计算的通信，我们会设置带宽最优的 CTA。</p><p></p><p>除了计算与通信资源的竞争问题，我们还会遇到不同通信之间的竞争问题。我们的解决方案是采用分桶通信。分桶之后，一个 all-gather 会被拆分成多个 all-gather 操作，这样单次的 DP 通信就可以被单次的计算所掩盖，从而尽量避免与 PP 产生资源竞争。但这并没有解决所有问题。即便我们实施了分桶策略，我们发现由于网络抖动等原因，DP 的通信和 PP 的通信仍有小概率发生 overlap，导致多流打入单网卡的现象，进而引起网络拥塞。为了缓解由不同通信之间的冲突所造成的网络拥塞问题，我们从 DCQCN 拥塞控制算法和不同的流优先级上进行了优化。通过这些优化措施，我们能够减轻网络拥塞，提高整体的训练效率。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/2a/2a23dc0cfe8bd3f4c5843eccd52a783e.png" /></p><p></p><p>DP overlap 引入的 PP bubble 问题。在前面，我们讨论了通信对计算效率的影响。如果我们模仿 ZeRO 的调度策略，由于 overlap 计算的时间会长于 none overlap 计算的时间，这种负载不均衡会导致 PP bubble 的产生。即图中的 Micro batch 2 的前向传播和 Micro batch 1 的反向传播较长的现象，这展示了负载不均的情况。我们提出的解决方案是通信时机的纵向对齐，这样可以极大地缓解 PP bubble 的问题。同时需要强调的是，从计算 overlap 部分移出来的通信都被放在了 PP bubble 上，因此它不会产生任何额外的影响。这种策略有助于平衡负载，减少因通信和计算不匹配而产生的效率损失。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/fc/fc351a12126dce565add5d761a2b6bbe.png" /></p><p></p><p>下图展示了我们最终优化后的 timeline。在这个优化版本中，我们实现了 reduce-scatter 与反向传播的 overlap，同时 all-gather 操作与前向传播也实现了 overlap。此外，我们通过分桶通信、网络预测控制、通信 CHANNEL 调优以及通信时机的纵向对齐等方法，大幅优化了 DP 的通信开销。这些优化措施共同作用，提高了整体的训练效率，减少了因通信而产生的延迟和资源浪费。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c7/c7075b6bd591375193020ca2f66d8ec4.png" /></p><p></p><p></p><h4>TP Overlap</h4><p></p><p></p><p>在介绍 TP overlap 之前，我想先向大家介绍一下 Tensor Parallel 的流程。这里实际上采用的是 Megatron-LM 中提出的序列并行，但为了简便，后面我们都简称为 TP。我们以 attention 为例来介绍 TP 的流程。</p><p></p><p>在 TP 中，一个 attention 层包含两个 GEMM 操作。第一个 GEMM 是将权重沿纵轴切分，第二个 GEMM 是将权重沿横轴切分。首先，我们将输入数据沿横轴切分，然后在第一个 GEMM 计算前，使用 all-gather 操作将两个输入合并。完成第一个 GEMM 计算后，我们会得到一个沿纵轴切分的输出。接着，通过第二个 GEMM，我们可以得到一个部分求和。最后，通过 reduce-scatter 操作，我们可以得到沿横轴切分的数据结果。可以看到，这两个模块的输入和输出都是沿横轴进行切分的，因此这个过程可以持续不断地进行。</p><p></p><p>在计算过程中，实际上穿插了两个通信操作，一个是 f，一个是 g。其中，f 在前向传播时对应 all-gather 操作，在反向传播时是 all-gather 加 reduce-scatter。而 g 在前向传播时是 reduce-scatter，在反向传播时是 all-gather。我们后续的 TP overlap 策略就是围绕这些通信操作来进行的。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/36/36058c9265f6d7215c7ecb3c631dd0ea.png" /></p><p></p><p>在针对 TP 进行计算通信重叠设计时，我们将其分为两个部分：一部分是有数据依赖的通信重叠，另一部分是无数据依赖部分的重叠。下图左侧展示了无数据依赖计算重叠的方案，这是一种比较经典的计算通信重叠方案。如前所述 DP overlap 就是其中的一种情况。此外，稍后我们会讨论到的 TP 中的列线性反向传播也会采用这种方案。</p><p></p><p>右侧的图展示了有数据依赖的计算通信重叠。在这种情况下，我们会将 GEMM 操作拆分成若干份（s 份），每一份的计算可以与下一次的计算重叠。需要注意的是，我们将计算也分散到了多个 stream 中。这样做的原因是，不同 stream 之间的计算是没有依赖关系的。因此，计算在不同 stream 之间也可以实现一定的重叠。这部分重叠来自于 kernel 即将结束时，SM 资源的占用会有一定程度的下降。借助 CUDA 运行时调度，可以把另一个 stream 中的 kernel 提前调度上来，从而实现计算的重叠。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/e2/e236bc4722fc8418547c69144ee47604.png" /></p><p></p><p>下面我会介绍一些 TP overlap 的细节，关键在于合理利用分块矩阵乘法来进行矩阵乘法运算。首先，对于一个矩阵乘法操作，我们可以沿着纵轴将其切分成两部分，并将这两部分分别放到不同的 rank 上。在计算之前，需要进行 all-gather 操作，这实际上是之前介绍的 all-gather+GEMM 的方案。我们可以将这一步的计算进一步分块，在 rank 1 和 rank 2 上分别进行一部分计算，这一步可以称为 step 1。</p><p></p><p>在执行 step 1 计算的同时，我们可以进行 send 和 receive 操作，将自己持有的那一部分输入数据发送给另一个 rank。接下来执行 step 2，这样通信就与 step 1 的计算重叠起来了。同时，我们还可以通过分块的方式拆分矩阵，也就是将矩阵分为左块和右块。分块的结果可以先计算出部分结果，然后再进行 reduce-scatter 操作，这也是之前介绍的 reduce-scatter+GEMM 的计算流程。</p><p></p><p>实际上，右侧与左侧的方案类似。我们同样可以将计算分块，先执行 step 1 作为一部分计算，然后将 step 1 的计算结果发送给另一个 rank。在发送的同时，可以开始执行 step 2 的计算，这样就可以实现计算和通信的重叠。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f8/f8c065986c4babf7d23c3dff6208fbdf.png" /></p><p></p><p>然后我们可以将这种策略推广到四个 rank 的场景中。为了简化表述，我们将计算的 stream 都合并到了一起。对于 all-gather overlap GEMM，我们会特别关注第一个 rank。第一步，我们使用自己持有的那部分输入来进行计算，同时将自己持有的内容也发送给其他 rank，并接收其他 rank 中持有的那部分输入。接下来的第二步、第三步、第四步都是按照相同的原理进行。通过这种方式，我们就可以得到一个 all-gather 的 overlap 流程。这样，每个 rank 都在进行本地计算的同时，与其他 rank 进行数据交换，实现了计算与通信的重叠。这种策略可以有效地减少等待时间，提高资源利用率，从而提升整体的并行计算效率。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f5/f5b885ec307fc75096a05bd04297c61d.png" /></p><p></p><p>Reduce scatter 的操作也是类似的。我们可以首先关注 rank 4 在整个计算结果流程中的作用。在第一步中，rank 4 的计算结果被放置在 rank 1 上。rank 1 完成自己的计算后，在第二步中，它会将这个结果发送给 rank 2。rank 2 在接收到来自 rank 1 的结果后，会将其与自己的计算结果进行累加，然后继续进行下一步的计算。接着，在第三步和第四步中，流程与前两步相同。rank 3 和 rank 4 也会按照这个顺序接收之前 rank 传递的结果，并与自己的计算结果进行累加。最终，在流程的最后， rank 4 将拿到汇总后的最终结果。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/78/78101e8b2de24b75560548b8b9bb2374.png" /></p><p></p><p>通过上述步骤，我们得到了一个完整的解决方案，适用于处理通信和计算存在依赖关系时的通信计算重叠问题。</p><p></p><p>这是 TP overlap 的整体解决方案，对于计算通信没有依赖的情况，这里是指 column-wise linear 的反向传播。由于这部分操作没有数据依赖关系，我们采用了 bulk overlap 技术。对于其余的通信和计算，因为它们之间存在依赖关系，我们采用了 split pipeline overlap 的方法。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/13/13b3001e0d780e1c07e94cb6d1f5081b.png" /></p><p></p><p>下图展示了实现 TP overlap 后的 timeline，我们可以看到 TP 的通信和计算重叠在了一起。同时，我们进行了两项优化措施：第一项是使用了 peer-to-peer memory copy，以此来减轻通信对 SM 的消耗。第二项优化是将计算分散到不同的 stream 上，这样计算也可以实现部分的重叠。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/76/76b1043e61a29e1292e426cfc1e7df35.png" /></p><p></p><p></p><h4>超长文本场景解决方案</h4><p></p><p></p><p>在大语言模型项目中，长上下文问题是最具挑战性同时也非常有趣的问题之一。到目前为止，主流的大模型都已经将上下文窗口（context window）扩大到了 100K 以上，Claude 3 和 Gemini 1.5 Pro 也都支持了超过 1 兆的上下文窗口大小。最近备受关注的 Sora 也对上下文窗口大小提出了巨大的需求，Sora 单个视频输入的长度就超过了 1 兆的 token，因此，长上下文的重要性不言而喻。</p><p></p><p>在处理长上下文时，我们遇到的最大挑战来自于显存。以 175b、32K 上下文窗口、TP=8 为例进行试算，我们发现仅仅是 activation 本身就给每个设备带来了超过 180GB 的开销，这远远超过了单个设备 80GB 的显存限制。为了缓解显存压力，我们采取了以下措施。</p><p></p><p>通信换显存：通过这种方式减少显存的使用，但如果我们继续扩大 TP，会导致 TP 超出 NVlink domain，进而导致通信开销大幅增加。计算换显存：通过 recomputing 的方式减少显存需求，但朴素的 recomputing 会带来大量的无效计算。内存换显存：例如使用 ZeRO-offload 或 Torch activation offload 技术。但存在两个问题：ZeRO-offload 无法解决 activation 问题，它只能解决模型状态问题；Torch activation offload 由于调度问题会有严重的性能问题。</p><p></p><p>现有的方案都是低效且扩展性差的。</p><p></p><p>针对 TP 作为通信换显存的两大弊端——在 h 维度上切分导致的不可扩展性以及方案本身的通信量大，我们希望找到一种在 s 维度上可以切分并且通信量相比 TP 小一些的方案。为此，我们实现了上下文并行（context parallel，简称 CP）。</p><p></p><p>在 CP 场景下，整个模型的 activation 从始至终都在 s 维度上保持着切分状态。之前无法解决的问题，通过 CP=4 就可以解决。我们可以计算这个方案的通信开销，CP 引入的通信开销仅有 KV 前向时的 all-gather 和反向时的 all-gather 以及 reduce-scatter。同时，我们改变了 QKV 的计算顺序，使得 K 的通信可以与 V 的计算重叠，V 的计算可以与 Q 的计算重叠。因此，我们可以得出下述两个结论。</p><p></p><p>CP 的通信量与 KV 的 activation 大小成正比。在混合并行场景下，利用了 TP 可以减少 activation 大小的特点，使得 CP 的通信量相比于直接扩大 TP 可以减少 TP 倍。由于 CP 的通信可以与计算进行重叠，因此进一步减少了对训练的影响。同时，由于 CP 的切分维度在 s 上，理论上如果有足够的机器，CP 可以解决任意大小的上下文窗口问题。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/58/58da5eb6f728f1f06022f27a15ee71f2.png" /></p><p></p><p>CP 与其他技术结合时，会带来一些额外的好处和挑战。首先是计算负载均衡问题，这个问题的背景是大语言模型采用了 Decoder Only 架构，并且在 attention 中使用了 causal mask，这导致 CP 会引入计算负载不均的问题。从下面的左图中可以看到，rank 0 的计算负载明显低于 rank 1。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b9/b943b0a6b6b66817467c5ee37131311a.png" /></p><p></p><p>为了解决这个问题，我们采用了类似高斯求和的方法，让每个设备负责一大一小两个 attention 的计算，以此来缓解负载不均的问题。由于同一个设备上的这两个 attention 计算之间不存在依赖关系，为了进一步提升硬件利用率，我们仿照 TP overlap，使用了不同的 CUDA stream 来 launch 两个 kernel。借助 CUDA 的 runtime 调度，我们实现了更高效的并行计算。</p><p></p><p>结合 CP 还有一些额外的好处。GQA（Grouped Query Attention）是在长上下文场景下几乎必选的技术。与原来的 Multihead attention 相比，GQA 将多个 query 作为一个 group，每个 group 对应一个 K 和 V。可以发现，GQA 可以极大地减少 KV activation 的大小。正如之前提到的，CP 的通信量与 KV 的 activation 大小成正比。因此在 GQA 的场景下，我们可以进一步减少 CP 的通信量，这是结合使用 CP 和 GQA 技术的一个显著优势。</p><p></p><p>下面是关于计算换显存的方案，其中 recomputing 是一个非常经典的技术。首先，让我们对 recomputing 做一个介绍。下图展示了一个正常的模型训练过程中的数据流。由于反向传播计算对前向传播计算结果存在数据依赖，因此在前向计算完成后，计算结果并不会立即释放，而是要等到反向计算完成后才释放。</p><p></p><p>右侧的图展示了使用 recomputing 方案的情况。可以看到，在 0 到 3 层的中间结果被释放了，只有 recomputing block 的输入，也就是 layer 0 的 input 被保存了下来。在反向传播过程中，我们会使用保存下来的 input 重新计算 0 到 3 层的前向传播结果，然后再进行反向计算，从而达到节省显存的目的。这个方案从理论上看起来非常理想，但在实际应用中也会遇到一些问题。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/46/464bfd2f221c5fbbc0a81dbd94efddba.png" /></p><p></p><p>首先，主流的框架都采用了 full computing，这导致每次反向计算都会执行一次完整的 forward pass，引入了大量的无效计算。在大模型时代，这种情况是不可接受的。其次，目前的开源框架 Megatron-LM 对 attention 部分实现了 selective recomputing。然而，在 flash attention 时代，这个方案的效率已经不如以前了。</p><p></p><p>经过观察，我发现某些 kernel，例如 GEMM，其反向计算实际上并不依赖于前向传播的输出结果。例如，对于公式 𝑌=𝑋𝑊，𝑑𝑋 和 𝑑𝑊 的计算并不依赖于前向传播的结果 𝑌。如果我们将这类算子作为 recomputing block 中的最后一个算子，就无需对它们进行重计算。</p><p></p><p>大家可以看下图右侧。假设层 3 是一个 GEMM 操作，那么 layer 3 的反向计算只依赖于层 3 的输入，而不是层 3 的输出。这样，在重计算时，我们可以省去 layer 3 的前向计算。我将这种重算策略称为 GEMM last recomputing。</p><p></p><p>我们将 GEMM last recomputing 策略实施到大语言模型的训练中，发现只需要对计算量较小的算子进行重算。相比于没有采用 recomputing 的方案，我们的策略在增加了不到 1.5% 的计算量的情况下，减少了 40% 的显存开销。这是一个在保持计算效率的同时显著减少显存需求的有效方法。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/d7/d7b756e32ab2b75ab16b98229b0dced3.png" /></p><p></p><p>接下来是内存换显存的方案。我最初产生这个想法的原因是，在训练过程中，显存资源已经非常紧张，然而内存资源在训练状态下却几乎处于闲置状态，这为我们提供了一定的操作空间。其次，随着硬件的升级，PCIe 已经升级到第五代，每张卡分配到的 x16 带宽达到了 64GB/s。同时，由于 H2D（Host to Device）和 D2H（Device to Host）是 memory copy 操作，它们对计算的影响几乎可以忽略不计。在混合并行场景下，每次前向计算产生的 activation 并不会立即被使用，而是至少要间隔一个完整的虚拟 pipeline stage 计算，因此混合并行架构也为我们提供了足够的时间窗口。</p><p></p><p>我们的解决方案是，将每个虚拟 pipeline stage 前一个 micro batch 的 activation H2D 和 D2H 的通信操作与下一个 micro batch 的计算进行 overlap，这样可以极大减少 offload 对关键路径上计算的影响。通过这个 offload 方案，我们能够在几乎不影响计算性能的情况下实现内存换显存的效果，上下文窗口大小提升了 2.5 倍。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b9/b98b1668a640aa01b53698f9eee40a98.png" /></p><p></p><p>接下来展示的是这个解决方案的整体成果。我们在 H800 集群上进行的测试显示，在吞吐量上，与现有的最先进开源方案相比，我们在 任意上下文窗口下都能实现超过 30% 的性能提升。 能达到这样的性能提升主要归功于两点原因：</p><p></p><p>第一，我们采用了通信代价更小的 CP 来替代 TP，从而降低了为解决显存问题而引入的通信开销；第二，我们采用了 GEMM last recomputing 和 pipeline aware offloading 这两种更具成本效益的显存问题解决方案，减少了以通信换取显存的需求，进一步实现了训练吞吐量的提高。</p><p></p><p>在支持的序列长度上限方面，首先，我们通过内存换显存、通信换显存、计算换显存的方法，大幅提升了单个设备支持的上下文窗口。同时，由于该方案还具有极强的可扩展性，因此在设备资源充足的情况下，我们可以支持无限大的上下文窗口。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f3/f390bf8a68a2860566420426527fcac9.png" /></p><p></p><p>接下来是 cost model（成本模型）的介绍。在进行大模型训练时，参数调整是一个非常痛苦的过程，因为模型有大量的参数，并且这些参数之间相互影响，比如 TP、CP、DP 的大小，以及 offload 的比例，还有网络设置中的 CTS。如果对所有参数都进行实际运行测试，将会消耗大量的计算资源。然而，如果不进行实际运行，仅仅通过比例和一些基于 FLOPs 理论算力的简单折算来预测，会导致预测极其不准确。因此，这样的成本模型是不可行的。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/08/085f17e40b79c18287bfc36edf269692.png" /></p><p></p><p>为了解决这个问题，我们对 TP、CP、PP 等一系列可能影响性能的因素进行了细致的建模。我们将所需信息分为与模型相关的信息，比如不同组合下单层前向和反向传播的时间；以及与集群相关的信息，比如跨机器的集群通信带宽或者 H2D 的带宽等。整体的测量耗时可以在一个小时内完成，并且这些信息可以多次复用。</p><p></p><p>在 175b 的案例中，我们建模的预测值和实测值之间的误差控制在 2% 以内。在实际使用过程中，我们的成本模型的误差与实测值的对比也不超过 5%，其中大部分误差来源于网络的不稳定性。下图右边展示了我们的成本模型给出的参数配置表。通常情况下，搜索完成后，我们可以根据 MFU 的前 5 名进行实际测试，最终得到我们的训练配置。这种方法大大提高了参数调整的效率和准确性。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/78/78f21610918f69cd01623658c73a892b.png" /></p><p></p><p></p><h3>未来展望</h3><p></p><p></p><p>未来在训练引擎方面我们会专注于五个主要方向。</p><p></p><p>万亿参数规模的 MoE 模型：我们期望能够训练具有万亿参数的 MoE 模型，这将推动模型容量和性能的显著提升。继续扩大序列长度：我们希望能够支持达到百万级别的序列长度，这将极大地扩展模型处理长文本数据的能力。RLHF 框架：目前还没有看到非常高效的 RLHF 框架实现，这将是未来研究的一个重要领域。低精度训练：随着 Hopper 系列架构的推广以及 FP8、FP6 等多精度配置训练，我们将需要关注低精度训练技术的发展。异构算力的引入：我们需要考虑引入异构算力来增强训练引擎的灵活性和健壮性。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/17vyJgFHQIbcmAdj5D7S</id>
            <title>AI 老师的强大功能 + 真人老师的情感交流 = 未来教育？ | QCon</title>
            <link>https://www.infoq.cn/article/17vyJgFHQIbcmAdj5D7S</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/17vyJgFHQIbcmAdj5D7S</guid>
            <pubDate></pubDate>
            <updated>Fri, 28 Jun 2024 08:57:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能, 教育领域, 大模型, 图灵机器人
<br>
<br>
总结: 人工智能在教育领域发挥重要作用，图灵机器人公司以大模型技术为核心，为教育行业提供AI知识问答、语法纠错等服务，取得不错效果。公司历史悠久，团队成员来自交大系，投资机构为战略投资人。公司业务涵盖教育、出版、运营商业、电教和汽车领域，致力于推动教育领域的创新和发展。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>演讲嘉宾 | 郭家 图灵机器人 COO审核｜傅宇琪 褚杏娟策划 | 蔡芳芳</blockquote><p></p><p></p><p>人工智能正在深度重塑教育领域，驱动着教学模式，尤其是个性化学习的革新。作为一家以语义和对话技术为核心的人工智能公司，图灵机器人用高精度 AI 知识问答、中英文语法纠错、图文识别等技术为教育行业赋能。自 2023 年起，图灵机器人用大模型逐一替代了 CNN 模型，并创新了 AI 口语老师、阅卷 AI 助理等应用，在步步高、作业帮等产品上应用上线并取得不错效果。</p><p></p><p>在用大模型重构产品的 1 年时间里，该公司对面向成本设计产品、大模型的“能与不能”都有了深度思考。本文整理自图灵机器人 COO 郭家在 <a href="https://qcon.infoq.cn/2024/beijing?utm_source=infoq&amp;utm_medium=conference">QCon 2024 北京</a>"的演讲分享“教育大模型，说你行你才行”，拆解这段产品重构之路，并以实际案例，分享其中的辛酸苦辣。</p><p></p><p>本文由 InfoQ 整理，经郭家老师授权发布。以下为演讲实录。</p><p></p><p></p><h2>我们是谁</h2><p></p><p></p><p>图灵机器人公司专注于教育行业，已经发展了将近 15 年。在这个过程中，我们见证了许多变化，并从传统模型逐步进化到大模型。公司的 LOGO 是对图灵机器人的致敬，我们于 2017 年获得了图灵后人詹姆斯·图灵以及英国皇家社会协会的肖像授权。2019 年，我们还成为了图灵基金在中国的唯一合作伙伴。由于公司注册较早，图灵现在已成为专有名词，无法再次注册。</p><p></p><p>我们的团队成员大多来自交大系。我们的 CEO 是交大数学系毕业，一直从事人工智能和复杂决策系统的工作，CTO 老韦也是交大数学系出身，首席科学家何小坤曾是好未来 AI lab 的负责人，在双减政策实施后来到我们这家人工智能教育公司，石勇教授是中科院的合伙科学家。</p><p></p><p>我们的投资机构特色鲜明，全部是战略投资人。他们对公司的持续经营和帮助已经持续多年，也不急于退出。我们的天使投资人是赛富的创始合伙人羊东。我们还是微软在中国的第一家创投企业。此外，我们的股东还包括 HTC、奥飞动漫和洪恩教育。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a2/a2cb5f8dba985a1334a11fd92f2e8c63.png" /></p><p></p><p>公司上一次推出的 AI 产品名为虫洞语音助手，对于互联网的资深用户来说，可能对这款产品有所耳闻。我们从 2010 年开始研发并发布了这款产品，它最初是为塞班手机和黑莓手机设计的语音助手。当时，苹果公司尚未收购 Siri。随着苹果在 iPhone 4 发布期间推出 Siri，语音助手这一领域迅速变得热门，我们的用户数量也迅速增长，接近 2000 万。</p><p></p><p>在开发过程中，我们一方面专注于自己的产品，另一方面与 HTC 建立了合作关系。HTC 是安卓系统的第一款手机制造商。我们与 HTC 合作开发了小 hi 机器人，也就是小 hi 语音助手。该产品上线时拥有 100 多种虚拟人表情，400 多种技能，包括 200 多个 CP 和 SP 的接入。</p><p></p><p>我们的许多技能都是与后方的 CP 和 SP 合作实现的，例如，查询天气功能与中国天气网合作，餐饮推荐则与点评网站合作。然而，尽管用户基数庞大，语音助手的前期活跃度也不错，但将其商业化却非常困难。直到现在，手机上的语音助手仍然面临这一问题。因此，面向消费者的业务模式（to C）并不适合当时的产品。基于这一认识，我们决定将这个创业项目出售给 HTC。随后，我们开始了第二次创业。</p><p></p><p>第二次创业，我们转向了 AI To B 业务，即面向企业的人工智能服务。2014 年，我们将产品卖给 HTC 后，决定将这些技术转化为一个开放平台，主要面向开发者开放。平台吸引了超过 100 万的开发者，每天都有上百的开发者加入，他们主要利用以自然语言处理（NLP）为核心的语音助手相关产品。</p><p></p><p>2016 年，我们发现对于一家创业公司来说，儿童教育是一个需求量大、适合快速增长的领域，于是开始专注于教育领域。在 2017 年和 2018 年，我们有幸邀请到了包括我的师妹，MIT博士贾梓筠在内的人才，一起参与这个项目，那年公司业务突破1000万营收。到了 2019 年，我们开始将视觉技术纳入我们的产品和服务。在教育领域，视觉技术的需求甚至超过了语音技术，例如题目识别、图片和文字识别、绘本和图画识别等，这些都需要计算机视觉（CV）技术来完成。</p><p></p><p>公司有五条主要的业务线。首先，进校业务方面，我们正在开发中高考英语口语模考系统，这种口语模考系统特别适合利用大模型技术。我们有教案的 AIGC 助手，它帮助老师生成教案，可以插入图片或精彩案例，甚至可以适时地加入一些幽默段子，让课程更加生动有趣。我们还提供大模型实验课，让学生亲自操作，测试 prompt，并使用 RAG 工具进行训练。</p><p></p><p>在出版领域，我们主要面向教辅公司和出版社，提供 AI 英语出题、AIGC 动画课等服务。此外，我们还涉足古籍、古典和学术研究领域，同样利用 RAG 技术进行数据挖掘。</p><p></p><p>运营商业务方面，我们提供 4G 电子产品，如自动翻译扫描笔、能够识别绘本和教材的台灯，以及用于口语测评方案的学生证和学生卡。</p><p></p><p>电教领域是我们公司历史最悠久、壁垒最深厚的业务之一，市场份额高达 80%。在这个领域，我们提供的服务包括语音助手、口语老师、作文批改以及翻译相关算法，如指尖翻译、手写体翻译和印刷体翻译。</p><p></p><p>最后，在汽车领域，我们为儿童领域提供重要的平台。从去年开始，新能源汽车如理想汽车推出了“小主人模式”，后排的小主人座舱需要语音助手来承载趣味内容和知识性互动。我们配套的小助人语音助手，包括音乐版权、分级阅读版权和词典版权，为儿童提供丰富的车内互动体验。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/3e/3edc8f63a3b80521f2f4b1244b0f3b0b.png" /></p><p></p><p></p><h3>大模型产品的第一步是 Cost Down</h3><p></p><p></p><p></p><h4>相比小模型时代，成本是做大模型的新主题</h4><p></p><p></p><p>去年公司正面临大模型带来的成本压力。我们已经将许多算法商业化多年，但随着时代的发展，如果不追求大模型的发展，否则就可能被时代淘汰。要追赶大模型，我们需要考虑如何将旧算法与大模型过渡。直接将大模型引入市场，初期成本非常高。尽管图灵公司自我造血多年，但大模型的投入仍然巨大。有下述几种情况需要考虑降低成本：</p><p></p><p>自己研发或使用开源的大模型，这对算力要求很高，所有资源都需要自己提供。为企业提供大模型服务，如进校或教育部的大模型私有化部署，学校对数据安全和隐私有严格要求，不希望竞争对手获取他们的原创内容，因此要求大模型必须私有化部署并本地训练。大量使用第三方大模型，如按 tokens 结算的方式，初期试用成本可控，但一旦商业化，成本迅速上升，如我们之前使用 GPT 大模型接口，每月投入可达三四十万，对单个客户而言，一年几百万的成本难以承受。端侧芯片层的大模型运行，如高通在最新芯片上运行大模型，预示着未来手机等设备将有本地大模型支持。开源大模型的趋势，如通义、百川等公司开源大模型，目的是让更多人使用，甚至自己运行大模型，从而推动云服务的销售。未来，购买算力可能等同于购买云资源。此外，服务器情况有所变化。2023 年相比 2022 年，价格明显上涨超过 50%。2023 年 5 月的禁令前后价格也有所不同。但在 2024 年，云服务价格下降了约 20%，目前云算力和消耗量处于可控范围内，这与服务器资源逐渐变得更加充裕有关。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/32/3253ff2b1ee481ea6ab753598e38b714.png" /></p><p></p><p></p><h4>我们如何做大模型降本</h4><p></p><p></p><p>我们的产品图灵 AI 口语老师已经推出了三个版本。C 版本是我们利用大模型技术所开发的版本，它在资源消耗方面是三个版本中最低的。右侧的图表展示了我们对成本的测算，这意味着，通过采用大模型技术，我们能够在保持产品质量的同时，有效控制成本。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/32/3253ff2b1ee481ea6ab753598e38b714.png" /></p><p></p><p>C 版本口语老师用于在创作话题时，生成 AB 角的对话场景。生成对话后，系统会基于预设的预训练脚本来执行对话，重点在于发音的评测，而非表达的正确性。</p><p></p><p>B 版本的口语老师在用户每次提问时都会调用大模型进行多种识别，包括语法、地道表达、对话相关性以及句子润色等，因此大模型的调用量非常大，消耗量级也随之增加。</p><p></p><p>我们制作的大多数儿童产品的成本相对较低，可能只有几百元，甚至一百元以内。因此，在儿童电子产品上，大模型的成本是相当高的，难以承受。我们尝试了多种运营方法来进行二次转化，以降低成本。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/44/4481eab8e9d96d4ebeda06a54f02fd09.png" /></p><p></p><p>A 版本口语老师的最大特点是教案虚拟人。虚拟人如何表达得好，关键在于情感识别。我们最初展示的口语老师形象被孩子们吐槽，因为许多学生认为这位老师给人一种压迫感，不想与其对话交流。因此，我们后来采用了更多二次元、卡通的形象。这里增加了两个成本，一是虚拟人的调用成本，二是大模型中虚拟人的情感识别成本。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/df/df110b246897b676efb7505fccd8200d.png" /></p><p></p><p>目前，我们对大模型的降本措施分为三大步，共六小步。</p><p></p><p>第一步是数据标注的降本。我们采用的方法是使用优质的大模型来生成训练数据，例如让 GPT 直接生成训练数据，这样可以轻松生成高质量的数据。第二步是算力补贴。由于我们公司是专精特新的企业，我们申请了很多国家的补贴，这有助于降低成本。第三步是 GPU 端的优化算子。我们与一些服务器公司，包括华为、阿里等，合作进行服务器端的优化。GPU 本身不变，但我们基于开发者模式进行自己的服务器优化，性价比非常高。第四步是加速框架，这是算法层的框架优化。第五步是大小模型混合。例如，我们要查天气，所有的语义槽位，如城市、日期等，这些可以直接用小模型处理，其精准度远高于大模型。用大模型做意图识别，然后将确定性的意图分流到 NLU 上，还有一些用大模型来兜底，这样成本会大幅下降。第六步是混合专家模型。我认为这适合除了基座公司以外的所有公司。要提高准确率，就需要将领域限制得更窄，知识库限制得更窄，这样才会更准确。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/22/22c0e22266e22d014a748f8934832c86.png" /></p><p></p><p></p><h2>试错一年终落地</h2><p></p><p></p><p>在过去一年多的时间里，我们对图灵 AI 口语老师产品进行了试错和迭代。投入成本主要分为几个部分。</p><p></p><p>数据标注：这是成本中相对较小的一部分。由于我们长期从事语音助手的开发，已经积累了大量的数据，数据清洗和为大模型缓存数据还是非常高效的。算力成本：算力成本并不高，因为产品尚未大规模推广，用户量增长有限，因此推理成本保持在较低水平。算法重构：这是成本中较大的一块。随着大模型技术的发展，我们必须将所有的小模型算法用大模型重新开发一遍。不仅涉及到技术层面的重构，还包括算法工程师的转型和后台服务、产品测试的重构。商业化成本：这是最大的成本部分。市场营销和应用层开发人员的投入非常巨大，尤其是在产品推向市场的过程中。作为教育公司，我们还必须购买大量正版内容。这不仅是因为训练需要，还因为在儿童教育领域，版权保护非常重要。拥有知名 IP 的版权内容能够带来溢价，家长更愿意为知名品牌的教育产品付费。</p><p></p><p></p><h4>我们如何做产品迭代</h4><p></p><p></p><p>我们的口语老师的第一个版本是一个名为 Free Talk 的 AI 外教产品，大约在去年 5 月份左右，我们推出了这个版本。</p><p></p><p>这个产品受到了 OpenAI 发布的一个名为 Call Annie 的产品的启发，Call Annie 是一个大头人像，能够进行英文交互。这个产品有几个特点：首先，它呈现为一个大头形象，给人一种面对面交流的感觉；其次，它进行全英文交流，不掺杂中文，模拟一对一外教的体验，并主打一对一外教的理念。</p><p></p><p>然而，在推广一段时间后，我们发现在实际使用中，无论是孩子还是成年人，都很难主动开口说话。即使有真人外教与孩子互动，孩子们也难以开口，不知道要说什么，也不会说。这导致 AI 外教很难带动孩子们进行对话。</p><p></p><p>此外，大模型在与孩子们交流时容易“超纲”。孩子们可能只学了一些非常简单的词汇，如"What’s this? It’s a bottle."，但如果让大模型反问，可能会提出很长、很复杂的问题，这让孩子们难以接受。</p><p></p><p></p><h4>第二个版本</h4><p></p><p></p><p>在口语老师的第二个版本中，我们采取了不同的策略来解决孩子们不知道如何开口的问题。这个版本有几个关键点。</p><p></p><p>专属陪练：基于孩子们的回复虚拟老师会进行个性化回复。话题引导：我们设置了一些孩子们熟悉的学习主题，在这个范围内引导孩子进行回答，例如开学或者交朋友的场景，并基于这些场景与孩子进行互动。这种方法可以帮助孩子们更好地融入对话，并激发他们的表达欲望。推荐回复：如果孩子在对话中不知道如何回答，我们会提供一些建议性的回答。这些建议是由大模型自动生成的，可以帮助孩子学习如何表达，并引导他们更顺利地参与到对话中。</p><p></p><p>每个人的学习情况和英语掌握水平都不尽相同，即使是在有设定话题的情况下，不同学生可能会觉得内容太简单或太难。因此，我们接下来要针对每个学生的个性进行优化。</p><p></p><p>个性化学习的关键在于分析学生的开口数据，观察他们的兴趣度和意愿度。同时，还要考虑学生回答的准确率，以及他们对提示语和推荐语的使用率。这些因素都是影响个性化学习效果的重要指标。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ab/ab012bd9b0988546e2f80ec5f2368c99.png" /></p><p></p><p>在口语老师的开发中，第三点关键因素是教育教学体系的构建。我们生成的场景话题，无论是用于学校教育的打招呼场景还是开学场景，背后都有一支教研团队的支持，而最坚实的支撑来自于优质的教材。</p><p></p><p>以牛津树分级阅读为例，我们可以看到即使是像 VIP Kid 这样的真人外教一对一教学产品，其背后也不仅仅是外教的教学，还包括了一套教学方法和教案。外教会使用画板和教案，如牛津的《Let’s Go》系列，一步步引导孩子学习。我们利用 RAG 技术来学习并生成课程内容，RAG 在生成基于问答的内容方面非常擅长。我们首先生成一些问答内容，然后对这些内容进行加工，使其成为课程教学的一部分。这样的学习方式可以实现分级教学，根据学生的不同年级和水平来筛选话题的难度。</p><p></p><p>此外，尽管现在的 TTS 技术已经非常先进，但它仍然无法完全复制真人发音时的抑扬顿挫和适当的语速与停顿。因此，我们选择使用原版真人发声的内容，让孩子能够复述真人的发音，以此来提高学习效果。</p><p></p><p>我们还加入了真题练习，选用了与优质教材相配套的练习题。目前，使用 AIGC 技术生成的题目效果尚不理想，因此我们直接采用了教材中原有的配套习题。这些迭代和改进，都是口语老师产品不断进化的一部分，旨在提供更加个性化、系统化和有效的教学体验。</p><p></p><p></p><h4>第三个版本</h4><p></p><p></p><p>在口语老师的第三个版本中，我们实现了商业化的显著进展。这个版本主要针对中高考的口语模考，提供了一个全真的模拟考试环境。这个环境从孩子试音、试麦克风开始，到试听题目，再到正式进行考试，完全模拟了真实考试的各个环节和流程。</p><p></p><p>过去的口语模考打分准确率较低，常受到老师们的诟病。现在，大模型在语法打分上的准确性大幅提升。例如，在听一段短文后回答有关问题时，大模型不仅考察语法是否正确，还要看是否准确回答问题，以及答案是否与题目相关，角色、动作和时间是否匹配。这些通过传统算法难以实现的点，大模型都能很好地完成。从 2025 年开始，中国所有的中高考口语考试打分可能都会采用大模型技术，这将是一个解决痛点的质的飞跃。这也是商业化落地中一个难得的、能够快速推进的点。</p><p></p><p>最后一个特点是真题题库的应用。教育离不开版权，我们必须购买各省市的真题和模考题库。这些题库不仅涉及版权问题，而且出题人的思路独特，我们尝试过用 AIGC 技术模仿出题人的思路，但效果并不理想。如果替代率达不到一定水平，那么使用 AIGC 节省的工作量就非常有限，因此我们选择直接使用教材中的原题。</p><p></p><p></p><h4>与国外产品几种不同设计理念对比</h4><p></p><p></p><p>在国外，大模型口语老师产品有几种不同的做法，这里分享几个例子。</p><p></p><p>首先是 Yanadoo，这是一款来自韩国的产品，其母公司是韩国最大的互联网教育公司。Yanadoo 的特点包括：</p><p></p><p>十分钟教育系统：提出每堂课只需十分钟，强调短时间内高效学习。奖学金激励：通过奖金激励学生。一对一 AI 语音指导：提供一体化的 AI 指导服务。游戏化学习：利用游戏化元素和奖金刺激，让学生在 10 分钟的高强度专注训练后，通过与 AI 老师练习并获得积分，以此提高学习效果。大模型应用：主要用在口语纠错上，提升学习精准度。</p><p></p><p>第二个产品是 Ainder，这是一个社交产品，其特色在于：</p><p></p><p>AI 虚拟人社交：所有的社交对象都是 AI 虚拟人，每个虚拟人有不同的背景和人设。个性化学习：用户可以与来自不同国家、不同口音和兴趣爱好的 AI 虚拟人进行英语交流。共同兴趣：通过聊用户感兴趣的话题，比如 NBA 球星和术语，提高语言学习的兴趣和效果。多语言者学习方式：该方法与一些多语言者通过与外国人聊天学习外语的方式相似，提供了一种自然的交流环境。</p><p></p><p>第三个产品是 Speak，这是一个 OpenAI 投资的教育公司，其特点为：</p><p></p><p>真人录播课：结合真人教学和 AI 技术，真人负责上课，AI 负责作业。AI 作业：AI 用于听说读写作业的自动纠错和分析，包括发音、语法和词汇。会员收费：虽然收费较高，但提供了高质量的学习体验。产品评价：产品设计精良，无论是学英语还是其他外语，都获得了很高的评价。</p><p></p><p>第四个是多邻国，一个广为人知的平台，它在 GPT 3.5 发布时就是合作伙伴之一。多邻国采用的大模型用于：</p><p></p><p>Explain My Answer：对用户的回答进行纠错和分析。Roleplay：在有限域下进行对话交互，让用户与 AI 进行 Free Talk 练习。</p><p></p><p>第五个产品是 Call Annie，一个提供随时视频通话的美女形象的产品，App 界面就像电话一样，提供交互体验。</p><p></p><p>最后一个是 CheggMeta，可以说是美国版的作业帮，它强调：</p><p></p><p>课后作业指导：专注于孩子回家后的作业指导。自适应学习：根据孩子的学习情况调整下一步的学习计划。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a7/a7519eb081575b11a1f9d6747794a890.png" /></p><p></p><p>总结来说，国内外在 AI 口语老师产品上的思路存在一些不同点。</p><p></p><p>国内 AI 口语老师产品的 1.0 版本在功能上大体相似。尽管每家公司都在训练自己的模型，影响体验最大的因素是模型训练的强度和精度。</p><p></p><p>国外产品的 1.0 版本普遍基于 GPT，因此在智能度上几乎一致。不同产品之间的主要区别在于各自的教学理念。例如，有的产品采用 10 分钟教学法，有的通过社交方式学习，有的结合真人录播课，有的游戏化学习，有的通过虚拟形象进行互动，还有的专注于作业辅导。</p><p></p><p>国内外产品在教学理念上有明显的差异。国外产品展现了多样化的教学理念，而国内产品可能在未来会根据自己的理念逐渐分化。</p><p></p><p>在英语学习的口语老师应用中，每家公司至少都会设计一个虚拟人物头像，这是虚拟人的最基本表现形式。一些公司则更为复杂，将视频录制与虚拟人制作相结合。即使是较为简单的应用，也会加入虚拟人物头像，以增强用户体验。虚拟人的表达和人的情感连接是非常重要的一环，它与大模型技术有着天然的强关联性。</p><p></p><p>在移动互联网行业中，我们常会提到“杀手级应用”，而对于大模型技术来说，虚拟形象很可能成为杀手级应用中的核心要素。这是因为虚拟形象不仅能够展示背后的价值观、人设和情感，还能通过其形象与用户建立联系。</p><p></p><p></p><h2>大模型的“行与不行”</h2><p></p><p></p><p>大模型在教育板块的应用存在一些问题，同时也有其不擅长的领域。</p><p></p><p>课程设计不行：大模型缺乏教与学的体系支撑，无法独立进行课程设计。课程设计需要明确的目标、大纲和学生学习进度等，而大模型目前还达不到这样的要求。解题能力不行：尽管有报道显示大模型通过了某些考试，但实际上在教育领域的测试中表现并不理想。以高考为例，准确率普遍低于 60%，小学五年级的准确率低于 85%，只有一二三年级的情况还算可以。出题能力不行：大模型能出题，但题目套路明显，缺乏创意。现代中高考题目，特别是北京、上海等地的试卷，已经从传统的选择题、完形填空转变为应用题，要求考生解决实际问题，这需要综合能力。大模型目前还无法满足这样的出题要求。讲题能力不行：大模型在讲解题目时可能会出现问题，可能会“胡说八道”，即使给出正确答案，其解释过程可能会越来越偏离正确方向，最终虽然得出正确答案，但教学场景中这样的讲解是不可接受的。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/af/af02ae9289e74eb0645e016275d2c59f.png" /></p><p></p><p>大模型在教育领域的优势体现在以下几个方面。</p><p></p><p>阅读领域：大模型在阅读领域的表现是令人满意的。RAG 型的应用在这方面尤其出色，它能够增强模型对信息的检索和生成能力。大模型被成功应用于基于学习材料的自动互动场景。这种应用通过与学习材料的结合，提供了自动化的、互动式的学习体验，这在当前教育技术中是一个非常好的方向。</p><p></p><p>微调和再训练：在使用大模型时，我们发现了一个令人惊艳的现象：与小模型相比，大模型在再训练时所需的数据量显著减少。例如，在口语老师的语法纠错功能中，原本需要 10 万到 100 万级别的数据量，而大模型仅需要很少的数据量就能训练出非常好的效果。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/09/0970459e7cf68f41199586705470e874.png" /></p><p></p><p>大模型在教育领域的应用还包括过程监督式的方法。通过过程监督，可以显著提升大模型在解题方面的准确性，有望快速解决解题不准确的问题。</p><p></p><p>此外，我认为未来一两年内，教育领域将面临一个重要的改革和转型理念，即真人与 AI 老师的结合。在这个模式中，真人教师的角色是组织教学活动和建立情感联系，而 AI 老师则充当工具型的角色，提供无所不能的知识支持。</p><p></p><p>这种结合利用 AI 的强大功能，同时保留真人教师在教育中不可或缺的人文关怀和情感交流。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/32mhQ8CzR4XGbGcpTmAY</id>
            <title>京东商家智能助手：Multi-Agents 在电商垂域的探索与创新 | QCon</title>
            <link>https://www.infoq.cn/article/32mhQ8CzR4XGbGcpTmAY</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/32mhQ8CzR4XGbGcpTmAY</guid>
            <pubDate></pubDate>
            <updated>Fri, 28 Jun 2024 08:51:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 电商助手, Multi-Agents, ReAct 范式, AI 多智能体系统
<br>
<br>
总结: 电商助手是一款集合了多种电商经营决策功能的工具软件，京东零售基于 Multi-Agents 理念搭建了商家助手大模型在线推理服务架构，核心是基于 ReAct 范式定制多个 LLM AI Agents。在QCon北京2024大会上，京东集团算法总监韩艾介绍了AI多智能体系统在电商垂域的探索与创新。商家经营团队的运作模式为AI Agent提供了现实版样例，构建了一个AI版的商家经营团队，由Master Agent领导多领域Agents团队。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>演讲嘉宾 | 韩艾 京东集团算法总监、京东零售数据与算法通道委员整理 | 玉玉编辑｜褚杏娟、傅宇琪</blockquote><p></p><p></p><p>电商助手是一款集合了多种电商经营决策功能的工具软件，旨在帮助电商从业者完成从商品发布到订单管理、客服沟通、数据分析等一系列电商运营任务。</p><p></p><p>京东零售基于 Multi-Agents 理念搭建了商家助手大模型在线推理服务架构，这一系统的核心是算法层基于 ReAct 范式定制多个 LLM AI Agents，每个 Agent 都有专门业务角色和服务功能，可以调用不同的工具或多 Agent 协同工作来解决相应的问题。</p><p></p><p>在 <a href="https://qcon.infoq.cn/2024/beijing?utm_source=infoq&amp;utm_medium=conference">QCon 北京 2024 大会</a>"上，京东集团算法总监、京东零售数据与算法通道委员韩艾，根据自己和团队在京东的技术实践经历，发表了题为《京东商家智能助手：AI 多智能体系统在电商垂域的探索与创新》的演讲，她阐述了 Multi-Agents 如何模拟真实的商家经营，并介绍 ReAct 范式的 Multi-Agent 在线推理架构，以及 Agent 落地垂域的样本、训练与评估监控的方法。</p><p></p><p>本文由 InfoQ 整理，经韩艾老师授权发布。以下为演讲实录。</p><p></p><p></p><h3>现实中，商家如何进行经营决策</h3><p></p><p></p><p>Agent 需要模拟人类的决策过程，因此需要先了解现实中的经营是如何进行的。</p><p></p><p>通常，平台向商家传递各种各样的信息，包括新的玩法、新的规则条款，以及可能的惩罚通知等。面对平台的各种消息和随之而来的疑问，商家需要一个经营助手协助，他通常扮演着一个专门提供平台知识百科的咨询顾问角色。</p><p></p><p>当商家提出赔付、运费等与业务相关的复杂问题，需要先理解需求，然后从长篇的业务文本中抽取出问题解决的大方向或目标。定位问题后，形成逐步的解题思路，再灵活调用各种资源和工具来解决问题，其中包括调用知识库、进行搜索和检索，以及使用人脑进行总结和筛选重点内容。经过这一系列操作后将问题的最终答案返还给商家。</p><p></p><p>那么如何将现实空间的平台咨询顾问映射到 Agent？顾问这个角色是我抽象出来的，京东实际上并没有这样的角色。对于商家来说，每天提供专属服务的实际上是我的许多同事，包括在线客服、业务运营人员以及产品经理，他们解答各种问题。那是否需要为每个岗位角色构建一个 Agent？解决这个问题时，我们还要回到应用场景，从商家的需求出发：无论谁在回答问题，对商家来说都只有一个人帮助他们解答问题。因此，构建一个 Agent 即可，它映射到为商家提供专属咨询服务的多个业务岗位的人。 构建这样一个 AI 版的 Agent 对商家和平台都有好处。对商家而言，他们将体验到一个永远在线的百科全书，能够突破时间、体力和知识掌握的极限。对平台来说，可以降低成本。</p><p></p><p>除了上面单一的 agent 提供专属服务的情况，当我们讨论到多领域助手与商家的经营协作时，整个团队是如何协作经营的呢？比如，商家提出了一个问题：“最近我的店铺经营得怎么样？”这个问题看似简单，但实际上是商家每天在处理完各种信息后首先会思考的。</p><p></p><p>对于现代电商商家来说，了解经营状况通常从查看数据开始，然后才能评估经营状况。他不会直接去系统读取数据或编写数据库查询语言，而是直接“调度”数据分析师这一角色，因为商家清楚自己的目标是数据相关的服务。于是，他将任务分配给团队中的数据分析专家，这位专家经过一系列操作后，会返回给商家一份数据报告。接下来，商家需要阅读并理解这份数据报告，他可能会发现新用户的留存率不佳的问题。这时，商家会根据新发现的问题更新决策。</p><p></p><p>商家的上述过程是 agent ReAct 范式的一个典型例子，即基于观察（observation）来更新整个推理（reasoning）过程。 在解决问题的思路上，人类和 Agent 非常相似。</p><p></p><p>接下来，更新的决策就是商家重新选择一个角色，比如用户研究专家，来分析新用户的偏好，解决新用户的留存率不佳的问题。这样的“拿到结果更新决策 - 调度新的专业角色 - 输出结果”会不断循环往复。</p><p></p><p>一个经营诊断与优化的问题，电商商家团队的成员要懂得数据分析、平台知识、用户研究、商品选品、定价、营销投放，还需要有人掌握制作图片和音视频素材的技能，以及完成所有操作和客户售后运营。而商家自己，需要清楚地了解每个团队成员的专长（profile），以便在更新决策时知道如何调度这些资源。此外，商家还需要能够理解每个专家返回的结果，这对商家来说也不是件容易的事情。</p><p></p><p>当商家发展到一定阶段，他们通常会聘请一个“最强大脑”来代理所有这些调度工作。这个“最强大脑”可以被理解为一个“总管”。有了总管，所有的调度工作都由总管代理完成，而商家只需要与总管沟通即可。这样的协作模式可以极大地提高商家的经营效率。商家想要完成一个经营诊断，他只需向总管提出：“帮我看看最近经营得怎么样？”然后他就可以耐心等待。总管在接到任务后，会进行一系列的操作，最终给出结论：“你最近新客户的留存情况不太好，我这里有一些商品营销创意的建议，你看看是否采纳。”相关的专家们的输出材料会作为附件提供给商家。</p><p></p><p>从单一个体到各个专业领域的专家团队，再到基础的执行工具，共同帮助商家完成了一个决策过程。在当前的团队配置中，可以关注三类主要角色：</p><p></p><p>领域专家：以咨询顾问为代表，这类角色不仅具备决策能力，还能够调度工具。在 AI 空间中，他映射我们的 Agent。工具：这类角色不具备决策能力，只能执行任务。在 AI 世界中，映射为软件系统中已有的多种原子服务能力接口 API。总管：作为整个决策发起的引擎，总管不需要在某一领域深耕，但必须具备通用的电商知识，了解如何经营业务。在面对问题时，总管能知道如何发起调度，负责整体的专业服务流程编排，在 AI 空间中，他映射我们最强的 Agent。</p><p></p><p></p><h3>构建 AI 版的商家经营团队</h3><p></p><p></p><p>商家经营团队的运作模式为我们提供了 AI Agent 的现实版样例。现在来到 AI 空间，请出我们的商家智能助手，我们暂且称呼它为 Mario X。将现实空间的团队映射到 AI 空间，我们用大量 Transformers 和研发代码构建了一个 AI 版的商家经营团队：一个由 Master Agent（主代理）领导的多领域 Agents 团队，团队同时掌控着一系列原子能力工具 API。</p><p></p><p>这样的 AI 团队带来了多方面的好处：</p><p></p><p>1. 体验提升：商家可以享受到 7*24 小时的在线服务。</p><p></p><p>2. 效率提高：商家不再需要学习使用各种工具和专业知识，只需用他们最熟悉的经营语言与 Master Agent 沟通，即可直接享受系统提供的各种服务。</p><p></p><p>3. 决策质量提升：由于有大量的备选方案可供选择，商家的决策效率和质量自然会提高。</p><p></p><p>4. 成本节约：商家可以减少人力和时间的投入，平台也可以减少不必要的运营开支，让我们的业务人员从繁琐的问答中解放出来。</p><p></p><p></p><h4>ReAct Agent 构建</h4><p></p><p></p><p>构建 ReAct Agent 时，每个 Agent 会经历一个 inner loop，这个内部循环称为 reasoning（推理），它对应于我们之前讨论的思维过程，即生成解题思路和大目标的步骤。reasoning 过程包含两个主要部分：</p><p></p><p>Thought（思考）：我将其定义为用人类自然语言描述的解题决策思路。但是，为了调度系统工具，LLM 需要发出指令，因此需要将这种人类语言翻译成系统能解析的研发语言（即下面的 action code）。生成 Action Code（动作代码）：基于生成的 Thought，Agent 会继续生成 Action Code。这个 Code 不直接执行 Action，而是执行 action 的指令。Action Code 是基于 Thought 解析出来的，因为 Thought 是拆分多步骤的解题思路，所以 Action Code 是对应的一系列任务。每个任务的定义可能非常复杂，提取 JSON 中的一些简单字段来说明：调度对象：告诉系统你要调度的工具是谁，比如 Master Agent 可能会调度其他 Agents 或 API。输入信息：提供给调度对象的信息，即函数的输入参数。Job Description：如果调度的是 Agent，需要让 Agent 明白分配给它的任务是什么，类似于工作描述。Trust_Mode：这是考虑性能和 Agent 质量的一个字段，它决定了 Agent 在接收到工具返回的 observation（观察结果）后，是再次进行 reasoning 还是直接输出结果。Action Code 是服务端可解析的代码，它会与环境中广义的 Agents API 和 Tools 进行交互并执行代码。当这些工具完成工作并将 observation 返回给 Agent 时，Agent 将进行下一轮的 reasoning。这个过程会一直持续，直到 Agent 生成了一个 Trust_Mode 变为 1 的输出，这意味着 Agent 认为所有的推理和调度都已完成，可以将结果推送给用户。</p><p></p><p></p><h4>Multi-Agent 的工作流程</h4><p></p><p></p><p>打开 Mario X 首先会与商家打招呼。第一轮商家提问：“在京东开店需要交多少保证金？”时，用户和 Master Agent 之间建立了联系，它会再从 Memory 中获取与用户相关的近期和远期特征。</p><p></p><p>接下来，Master agent 开始内部推理。在这个阶段，Master agent 的 LLM 理解商家提出的问题，但意识到缺少必要的条件，因此无法直接派发任务。LLM 需要向商家追问一个条件，因为保证金与商家经营的类目密切相关。这时，它会调用一个名为 Echo 的工具，Echo 的作用仅仅是将信息传递给用户，不做任何处理。此时 Master agent 将 Trust_Mode 设置为 1，因为 Echo 的任务是单向传递信息，不需要再返回给 LLM 进行推理。Action Code 开始执行，Echo API 被唤起，将问题传回给用户，同时将上下文信息推送给 Memory。</p><p></p><p>第二轮，商家回答说他卖花，这时用户的信息再次流向 Agent，LLM 根据商家提供的信息和 Memory，生成解答思路在 Thought 中。LLM 知道现在需要调度的对象是 Consulting Advisor，即前面提到的平台咨询顾问 Agent 版。LLM 向 Advisor 传递了一个 Job Description，因为 Advisor 是一个 Agent，需要与之沟通并分配任务。Agent 之间的通信协议也是基于 Action Code，告知 Advisor 商家需要查询的类目对应的入住保证金费用。此时 Trust_Mode 设置为 1，意味着 Advisor 完成任务后不需要再返回给 LLM，因为 LLM 信任 Advisor 专门执行此类咨询任务。这是出于性能考虑，避免让用户等待过久。随后，Advisor Agent 执行任务并返回输出，同时更新 Memory。最终，Master agent 回答用户的问题。</p><p></p><p>第三轮，当客户提出为花店起名时， Master Agent 的 LLM 识别出这是一个明确的问题。为了解决这个问题，将会进行 3 轮 ReAct。第一轮：不需要调用其他 Agents，而是直接调用一个特定的 API 会更加高效。它调用的是一个名为“Shop Name Generator”的 API，这是一个基于大语言模型的起名工具，它需要接收的输入参数是店铺的类目信息。他从 Memory 中提取了之前 Advisor Agent 提供的信息，即商家经营的是“生活鲜花”，并将这个信息作为参数传递给 Shop Name Generator。在这一步，Trust_Mode 为 0，这意味着 API 生成的店铺名字将返回给 Master Agent 做其他的推理，而不是直接输出给用户。我们回到了 ReAct 流程中，API 输出了一系列的店铺名字，但用户此时还不会看到任何输出的结果。</p><p></p><p>所有这些步骤完成后，相关信息都会被推入 Memory，这就是 Multi-Agent 工作架构的一个例子。对于普通的 Agent 与 Master Agent 的区别在于，Master Agent 直接与用户交互，而普通 Agent 则接收来自 Master Agent 的 Action Code，这些 Action Code 转化为服务层协议，作为它们的输入参数。</p><p></p><p></p><h4>分层次架构</h4><p></p><p></p><p>Multi-agent 架构采用分层次的方法，将一个大模型的复杂生成任务，拆解成了多个层级化的下一步文本预测。这样，每个模型需要处理的推理难度就相对较小，因此模型的规模不需要很大，从而减少了训练和部署的计算资源消耗，并且可以快速迭代。同时，也可方便灵活地接入各种资源方，比如营销的 Agent，我们可以迅速地将其整合进我们的系统中。</p><p></p><p>这种架构也有一些潜在问题。首先，可能导致风险的累积。如果 Master Agent 出错，那么整个任务的结果可能就会受到影响。因此，我们实施了全链路监控，以确保系统的稳定性和可靠性。此外，由于可能需要经过多个 LLM 生成步骤，响应时间有时可能会较长。此外，商家面临的问题通常涉及工具操作，这些问题都需要结合具体的业务情境来解决。因此，对于我们的 Agent 来说，它们也需要“死记硬背”所有 Tools 的能力。目前，我们正在进行的工作包括在整个推理流程的多个环节中整合 Retrieval（检索）过程。例如在生成 Thought 之后，Agent 可以暂停并调用检索工具或 Agent，等待 Observation 返回后再明确调用哪个 Tools，然后生成 Action Code。这意味着 Thought 和 Action 可以分两轮生成，这是我们正在努力实现的一些改进。</p><p></p><p></p><h3>商家智能助手：关键落地技术</h3><p></p><p></p><p>今年 2 月份，我们推出了一个专门处理招商入驻问题的 Agent，并将其部署在京东的招商站点上。这个 Agent 帮助许多商家解答了他们关于入驻的相关问题和操作步骤。目前，这个全新的 muiti-Agent 架构助手产品已经在京东商家端进行灰度测试阶段。</p><p></p><p>技术上，我们目前的系统能够解决商家经营场景中的一些确定性输出问题。所谓确定性输出，是指商家面临的一些答案明确的问题，比如关于平台规则的疑问或具体的操作步骤等，这些问题相对基础，并不包括那些开放式的问题，比如“告诉我如何做好生意”。</p><p></p><p>我们在建设一个能够真正帮助商家做生意的靠谱助手，搭建完整 AI agent 经营团队。这个系统将涉及非常广泛的知识领域，处理的问题也不会有确定的答案，可能需要引入强化学习等更先进的技术来解决。</p><p></p><p></p><h4>ReAct SFT：垂域样本构建</h4><p></p><p></p><p>在解决相对确定性输出的问题时，我们的核心工作在于构建垂直领域的知识。这意味着将人类专家的知识传授给系统，特别是针对商家领域的知识。对于这类问题，通常使用标准的 SFT 加上一些预训练模型基本上就足够了。</p><p></p><p>如何构建样本？鉴于我们先解决比较确定性的问题，我们可以从在线客服、运营和产品的回复，以及商家满意度收集的接口等获得真实的数据，然后对这些数据进行清洗。接着，研发团队会根据系统的调用路径构建一个全面的路径树。最后，业务人员将构造一些剧本，描述可能的问答场景。</p><p></p><p>将这两部分结合起来，我们就得到 SFT 样本 的基础池。接下来，对基础池进行丰富度扩充。其中最主要的是对问题（Q）的扩充。有了问题和答案（A），以及调用路径，我们接下来需要生成中间的标签（label）即 thought 和 action code，这就需要依赖先验的知识库。此外，还需要研发的配合，他们需要按照标准来注册 API。因为工具的调用靠注册信息的质量，如果两个不同的工具，它们的描述写成一样的，那么我们的大模型也无能为力，因为它只能通过工具的自我介绍来选择工具来执行任务。因此，知识的准确性非常重要。</p><p></p><p></p><h4>复杂输入下的 Thought 生成</h4><p></p><p></p><p>复杂输入的问题，不像简单输入那样直接。解决这类问题，关键在于遵循 Agent 推理的流程：先生成 Thought，再解析 Action Code。因此，生成一个强大的 Thought 变得非常重要。下面看一个复杂输入下，确定性输出的例子，我们来对比单纯用 RAG 和用 LLM agent 解题的效果，比较一下有和没有好的 Thought 的区别。</p><p></p><p>（1）RAG 解题</p><p></p><p>例如，用户提出了一个问题：“在京东卖红酒要多少钱？”如果直接使用 Retrieval（检索增强）来解决问题，按照经典的方式，先进行 Query（查询）并计算 Similarity（相似度），然后召回一些文本。在召回的文本中，可能会看到白酒、黄酒等，但实际上并没有答案，因为红酒这个类目在我们的知识库中并不存在，它不是开店保证金的一个选项。基于错误的信息片段，再加上用户模糊的问题，即使是非常强大的 Summary Model（总结模型）也无法给出正确的答案。</p><p></p><p>要解决这个问题，我们需要让模型理解红酒实际上与哪些类目是有关联的。这就需要模型不仅要有检索能力，还要有推理和关联的能力，以便正确地将问题与相关的知识库内容关联起来，从而提供准确的答案。</p><p></p><p>（2） LLM Agent 解题</p><p></p><p>助手中的 Advisor 在经过训练后，会以特定的方式解题。还是开始于 Master Agent 与用户的对话。Master Agent 并不直接理解这个问题，而是将用户的询问，例如“京东红酒入住资费是多少？”通过 Action Code 传递给 Advisor。Action Code 中的 Job Description 是“请回答京东红酒入住资费”。</p><p></p><p>Advisor 在处理这个查询时，首先理解红酒实际上属于葡萄酒这一类别。因此，Advisor 的 Thought 中生成出应该查询的是葡萄酒类目的入住资费，并确定了使用哪些关键词来传给调度的检索 API 做关键入参。在生成 Action Code 时，Advisor 会传递给检索 API 这个关键入参，即 Search Query“葡萄酒保证金“。这个参数不再是用户的原始问题，而是根据 Advisor 的推理进行了调整。API 本身没有决策能力，但由于 Agent 具有推理能力，它能确保传递给工具的输入是正确的，从而用正确的参数唤起正确的工具。</p><p></p><p>在第二个任务中，Summary API 接收到一个关键的输入参数，称为 Thought for Answer，即回答思路。这个思路是 Advisor 在推理过程中在 thought 生成的关于红酒与葡萄酒类目关系的理解。Advisor 告诉用户红酒和葡萄酒之间的关系，并按照葡萄酒类目的答案来回答用户的问题。</p><p></p><p>接下来，advisor 继续遵循经典的 RAG 流程。此时，Search Query 变为“葡萄酒保证金”。虽然召回的葡萄酒与原始问题的“红酒”相似性不高，但由于顾问使用了“葡萄酒”和“保证金”作为搜索关键词，并将回答问题的思路作为 Prompt 的一部分传递给总结 API，API 就能够根据 Advisor 提供的推理思路，正确地回答关于红酒保证金的问题，即通过查看葡萄酒的保证金来得知红酒的保证金情况。</p><p></p><p></p><h4>复杂输入下的 Thought 训练</h4><p></p><p></p><p>在复杂输入的情况下，训练出能够准确生成 Thought 的模型是关键。由于这类问题的答案并不直接存储在知识库中，我们需要从算法层面进行构建。我们的方法是分析 Bad case（不良案例），从中发现问题并拓展解题思路。</p><p></p><p>当遇到一个新案例时，我们会与业务团队沟通，以获取新的知识点，并按照既定的模式进行预先处理。理解不同类目之间的关系是解决相关问题的关键。因此，我们为模型提供了大量类似的文本进行预训练（pretrain）。</p><p></p><p>在自监督学习阶段，模型学习了与业务相关的各种关键词、相似词以及它们与类目的关系。这样，当模型遇到葡萄酒相关的问题时，它已经通过预训练了解了如何处理这类问题。随后，我们对模型进行标准的 SFT，在这个阶段，模型会学习到具体的知识点，比如葡萄酒的相关信息。模型已经知道如何回答关于葡萄酒的问题，并且通过训练了解了葡萄酒与其他类目的关系。当用户询问关于红酒保证金的问题时，模型能够通过分析和推理，提供准确的答案。</p><p></p><p>通过这种方式，我们可以训练出能够处理复杂输入并生成有效 Thought 的模型，这些模型能够更好地理解和解决商家面临的实际问题。</p><p></p><p></p><h4>全链路 ReAct 监控</h4><p></p><p></p><p>为了定位 Bad Case，我们实施了全链路 ReAct 监控。具体来说，我们会收集在线推理生成的 Thought、Action Code 和 Observation，然后通过人工打标 + 大模型来进行评估。</p><p></p><p>评估函数会将人工打标的输出与 Agent 生成的输出进行比较，以确定两者之间的差异。这个评估与 Agent 的具体定义紧密相关，因为不同的 Agent 可能有不同的评估标准。评估主要基于三个结果：Thought、Action Code 和 Observation。值得注意的是，Observation 虽然是作为下一轮推理的输入，但它本身并不是由 LLM 生成的，它的质量会影响下一轮的 Thought 生成。</p><p></p><p>对于 Observation 的评估可能包括预测销量的准确性或用户对生成图像的满意度等，这些指标并不完全由 LLM 控制，因此 Observation 的评估也与服务的业务指标相关。</p><p></p><p>基于这些评估结果，我们会有一个流程来决定 Agent 的表现。如果 Agent 在第一轮的 ReAct 得分很低，我们会继续累积这个分数，但如果得分低于某个阈值，我们将停止后续的推理，并且该 Agent 将不再参与后续得分的累加，意味着它已经退出了推理过程。如果 Agent 的得分符合要求，我们会检查是否为最后一轮推理。如果不是最后一轮，Agent 将更新后进入下一轮评估。如果是最后一轮，将触发结束流程。</p><p></p><p>在多轮推理和评估后，当触发结束评估时，我们会得到一个全链路累积的 ReAct 得分。这个推理过程是链式的，涉及到递减的折扣因子γ和η，这些因子会影响 Agent 的 ReAct 得分和整体得分。我们的评价核心在于能够快速定位问题节点，这是由我们的架构决定的，必须通过这种方式来尽早发现并解决问题，防止问题在推理过程中蔓延。</p><p></p><p></p><h3>展望</h3><p></p><p></p><p>我们需要帮助商家更好地经营生意。尽管在平台上有许多类似参谋的工具，比如供应链管理、选品、定价等，但目前还没有一种方式能够让商家根据自己的业务思路，按照黄金流程组合使用这些工具。无论是问答数据、沟通数据还是交互数据，这些都需要我们去收集和整合。</p><p></p><p>我们需要将人们做生意的思维方式从人脑中提取出来，这包括训练大型模型来寻找和学习人类专家的知识。此外，我们还需要引入强化学习。 因为对于商家提出的复杂问题，如“我的生意做得怎么样？”可能存在多种解决方案，每个团队的解法可能都不同。要判断哪一个更好，可能需要每个做生意的人根据自己的打分逻辑来评估，同时还需要在市场反馈中验证。</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/pc01hLuAWfcX64Mb24Qg</id>
            <title>好消息：OpenAI 突然发了新模型！坏消息：只是纠错，没你想得逆天</title>
            <link>https://www.infoq.cn/article/pc01hLuAWfcX64Mb24Qg</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/pc01hLuAWfcX64Mb24Qg</guid>
            <pubDate></pubDate>
            <updated>Fri, 28 Jun 2024 07:39:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: ChatGPT, CriticGPT, OpenAI, RLHF
<br>
<br>
总结: 对于支持聊天机器人的大型语言模型来说，一个主要问题是如何信任它们。OpenAI开发了CriticGPT，一个帮助人类训练师发现模型错误的工具。CriticGPT能够提供更全面的评论和减少幻觉错误，同时还能在非代码任务中发现错误。通过RLHF训练，CriticGPT能够识别和标记编码错误，帮助人类更容易发现模型输出中的不准确之处。 </div>
                        <hr>
                    
                    <p></p><p>&nbsp;</p><p>整理&nbsp;|&nbsp;华卫</p><p>&nbsp;</p><p>对于&nbsp;ChatGPT&nbsp;等聊天机器人提供支持的大型语言模型来说，最大问题之一是，永远不知道何时可以信任它们。它们可以针对任何问题生成清晰而有说服力的答案，并且提供的大部分信息都是准确而有用的，但它们也会产生幻觉。用不太礼貌的话来说，它们会胡编乱造，需要人类用户自己去发现错误。它们还会阿谀奉承，试图告诉用户他们想听的内容。</p><p>&nbsp;</p><p>如今，<a href="https://www.infoq.cn/news/9PjLEHC7BKMGzGQLRzQz">OpenAI</a>"在这个问题的解决上迈出了最新的一小步：开发了一种上游工具，能够帮助训练模型的人类引导模型走向真实和准确。</p><p>&nbsp;</p><p>6月27日，OpenAI宣布，其研究人员训练了一个用于捕捉ChatGPT&nbsp;代码输出错误的模型，名为&nbsp;CriticGPT。CriticGPT&nbsp;是一个基于&nbsp;GPT-4&nbsp;的模型，它撰写了对&nbsp;ChatGPT&nbsp;响应的评论，以帮助人类训练师在&nbsp;RLHF&nbsp;期间发现错误。</p><p>&nbsp;</p><p>OpenAI发现，当人们在&nbsp;CriticGPT&nbsp;的帮助下审阅&nbsp;ChatGPT&nbsp;代码时，他们在60%&nbsp;的情况下比没有&nbsp;CriticGPT&nbsp;帮助的人表现得更好。因此，目前OpenAI正在着手将类似&nbsp;CriticGPT&nbsp;的模型集成到其人类反馈强化学习&nbsp;（RLHF）&nbsp;&nbsp;标签管道中，为自己的人类训练师提供明确的人工智能帮助。</p><p>&nbsp;</p><p>“这是朝着能够评估高级人工智能系统输出的目标，迈出的关键一步。如果没有更好的工具，人们很难对这些结果进行评分。”OpenAI这样评价CriticGPT。同时，OpenAI发布了详细介绍CriticGPT背后技术的<a href="https://www.infoq.cn/article/GdkidIFChUxVslICMamx?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">预印本论文</a>"。</p><p>&nbsp;</p><p></p><h2>CriticGPT的纠错能力</h2><p></p><p>据了解，为&nbsp;ChatGPT&nbsp;提供支持的&nbsp;GPT-4&nbsp;系列模型通过&nbsp;"从人类反馈中强化学习"（RLHF）实现了帮助和互动。RLHF&nbsp;的一个关键部分是收集比较信息，由被称为人工智能训练师的人员对不同的&nbsp;ChatGPT&nbsp;响应进行评分。</p><p>&nbsp;</p><p>随着OpenAI在推理和模型行为方面的进步，ChatGPT&nbsp;变得越来越精确，输出错误也变得更加微妙，可能会使人类训练师难以发现模型输出结果中的不准确之处，从而使为&nbsp;RLHF&nbsp;提供支持的比较任务变得更加困难。这是&nbsp;RLHF&nbsp;的一个基本局限，并且随着模型逐渐变得比任何可以提供反馈的人都更博学，可能会使模型之间的比对和调整变得越来越困难。</p><p>&nbsp;</p><p>为了帮助应对这一挑战，OpenAI对&nbsp;CriticGPT&nbsp;进行了训练，研究人员在有意插入错误的代码样本数据集上让其撰写批评意见，教它识别和标记各种编码错误。作为人类训练师的&nbsp;AI&nbsp;助手，CriticGPT&nbsp;能够负责审查&nbsp;ChatGPT&nbsp;AI&nbsp;助手生成的编程代码，其基于&nbsp;GPT-4&nbsp;系列的&nbsp;LLMS&nbsp;分析代码并指出潜在的错误，使人类更容易发现可能被忽视的错误。</p><p>&nbsp;</p><p>虽然CriticGPT&nbsp;的建议并不总是正确的，但OpenAI发现，与没有&nbsp;AI&nbsp;帮助相比，它们可以帮助人类训练师在模型编写的代码中发现更多的问题。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/9c/9c90df1dd8633271e79b5d9e61832e99.png" /></p><p></p><p>&nbsp;</p><p>此外，当人类训练师使用&nbsp;CriticGPT&nbsp;时，人工智能会增强他们的技能，从而产生比单独工作时更全面的评论以及比模型单独工作时更少的幻觉错误。在OpenAI的实验中，第二位随机训练师在&nbsp;60%&nbsp;以上的时间里，都更喜欢来自人类+CriticGPT&nbsp;团队的评论，而不是来自无辅助人员的点评。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/4b/4bac2ef034638ba47fba7d3d1d5a51ec.png" /></p><p></p><p>CriticGPT&nbsp;帮助人类训练师写出比没有帮助更全面的评论，同时产生的幻觉比仅来自模型的批评更少。</p><p>&nbsp;</p><p>有趣的是，OpenAI还发现，CriticGPT&nbsp;的功能不仅仅局限于代码审查。在研究人员的实验中，他们将该模型应用于&nbsp;ChatGPT&nbsp;训练数据的一个子集，这些数据以前被人类注释者评为完美无缺。令人惊讶的是，CriticGPT&nbsp;在这些案例中发现了&nbsp;24%&nbsp;的错误——这些错误随后得到了人工审稿人的确认。</p><p>&nbsp;</p><p>OpenAI认为，这表明该模型具有推广到非代码任务的潜力和捕捉细微错误的能力，即使是仔细的人类评估也可能遗漏这些错误。</p><p></p><h2>模型训练方法</h2><p></p><p>&nbsp;</p><p>与&nbsp;ChatGPT&nbsp;类似，CriticGPT&nbsp;也使用&nbsp;RLHF&nbsp;进行训练。但与&nbsp;ChatGPT&nbsp;不同的是，CriticGPT&nbsp;会看到大量包含错误的输入，然后对这些错误进行评论。</p><p>&nbsp;</p><p>在OpenAI的实验中，他们研究了&nbsp;CriticGPT&nbsp;能否捕捉到插入的错误，以及之前的训练师捕捉到的&nbsp;"自然发生的&nbsp;"ChatGPT&nbsp;错误。他们要求人类训练师在&nbsp;ChatGPT&nbsp;编写的代码中手动插入这些错误，然后写出反馈示例，就好像他们抓住了刚刚插入的错误一样。然后，同一个人对修改后的代码进行多次评论比较，这样他们就能很容易地分辨出评论何时抓住了他们插入的错误。</p><p>&nbsp;</p><p>其发现，在&nbsp;63%&nbsp;的自然出现的错误案例中，训练师更喜欢&nbsp;CriticGPT&nbsp;批评而不是&nbsp;ChatGPT&nbsp;批评，部分原因是新批评者提出的“吹毛求疵”（无益的小抱怨）更少，产生幻觉问题的频率也更低。</p><p>&nbsp;</p><p>此外，OpenAI还发现，通过对批评奖励模型使用额外的测试时搜索，可以生成更长、更全面的批评。通过这种搜索过程，他们可以平衡在代码中查找问题的积极程度，并在幻觉和检测到的错误数量之间进行精确召回权衡。这意味着，他们可以生成对&nbsp;RLHF&nbsp;尽可能有帮助的评论。</p><p>&nbsp;</p><p>在&nbsp;CriticGPT&nbsp;的开发中，OpenAI研究人员还创造了一种新技术，他们称之为力采样光束搜索（FSBS）。这种方法可以帮助&nbsp;CriticGPT&nbsp;编写更详细的代码评论，让研究人员可以调整&nbsp;CriticGPT&nbsp;在寻找问题方面的彻底程度，同时还可以控制它编造并不真正存在的问题的频率。他们可以根据不同&nbsp;AI&nbsp;训练任务的需求来调整这种平衡。</p><p>&nbsp;</p><p></p><h2>局限性</h2><p></p><p>&nbsp;</p><p>尽管与所有&nbsp;AI&nbsp;模型一样，CriticGPT&nbsp;取得了令人鼓舞的结果，但它也存在局限之处，包括以下几方面：</p><p>&nbsp;</p><p>目前，OpenAI用&nbsp;ChatGPT&nbsp;的简短答案来训练&nbsp;CriticGPT。为了监督未来的代理，他们需要开发能帮助训练员理解冗长复杂任务的方法。CriticGPT模型仍然会产生幻觉，有时人类训练师在看到这些幻觉后会犯下标记错误。有时真实世界中的错误会分散在输出答案的多个部分，而&nbsp;CriticGPT的工作重点是可以在一个地方指出错误，但将来也需要解决分散的错误。CriticGPT&nbsp;所能提供的帮助有限，如果一项任务或响应极其复杂，即使是有模型帮助的专家也可能无法正确评估。</p><p>&nbsp;</p><p>关于OpenAI提到的使用&nbsp;CriticGPT&nbsp;来捕捉文本错误的方面，实际上也很棘手，因为文本中的错误并不总是像代码那样明显。更重要的是，RLHF&nbsp;经常被用来确保模型在回答问题时不会出现有害偏见，并在有争议的问题上提供可接受的答案。对此，OpenAI&nbsp;研究员&nbsp;Nat&nbsp;McAleese&nbsp;也表示，在这种情况下，CriticGPT&nbsp;不太可能起到帮助作用，&nbsp;"这种方法不够有力"。</p><p>&nbsp;</p><p>可以确定的是，为了调整日益复杂的人工智能系统，未来需要更好的纠错工具。由于在对&nbsp;CriticGPT&nbsp;的研究中，OpenAI发现将&nbsp;RLHF&nbsp;应用于&nbsp;GPT-4&nbsp;有希望帮助人类为&nbsp;GPT-4&nbsp;生成更好的&nbsp;RLHF&nbsp;数据，他们正计划进一步扩大这项工作的规模，并将其付诸实践。</p><p>&nbsp;</p><p></p><h2>结语</h2><p></p><p>一位与OpenAI无关的AI研究人员表示，CriticGPT&nbsp;这项工作在概念上并不新鲜，但它在方法论上做出了有用的贡献。麻省理工学院博士生、2023&nbsp;年一篇关于&nbsp;RLHF&nbsp;局限性的预印本论文的主要作者之一&nbsp;Stephen&nbsp;Casper&nbsp;表示：“RLHF&nbsp;的一些主要挑战源于人类认知速度、注意力和对细节的关注的限制。“从这个角度来看，使用LLM辅助的人工注释器是改善反馈过程的自然方法，是朝着更有效地训练对齐模型迈出的重要一步。</p><p>&nbsp;</p><p>但Casper也指出，将人类和人工智能系统的努力结合起来“可能会产生全新的问题”。例如，“这种方法增加了人类敷衍参与的风险，并可能允许在反馈过程中注入微妙的人工智能偏见。</p><p>&nbsp;</p><p>2023&nbsp;年&nbsp;7&nbsp;月，OpenAI曾宣布将其&nbsp;20%&nbsp;的计算资源用于对齐研究。但目前OpenAI&nbsp;已经解散了其对齐团队，并将剩余的团队成员分配给其他研究小组。此次OpenAI发布的研究成果表明，至少他们仍在开展可信和开创性的对齐研究。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://openai.com/index/finding-gpt4s-mistakes-with-gpt-4/">https://openai.com/index/finding-gpt4s-mistakes-with-gpt-4/</a>"</p><p><a href="https://arstechnica.com/information-technology/2024/06/openais-criticgpt-outperforms-humans-in-catching-ai-generated-code-bugs/">https://arstechnica.com/information-technology/2024/06/openais-criticgpt-outperforms-humans-in-catching-ai-generated-code-bugs/</a>"</p><p><a href="https://spectrum.ieee.org/openai-rlhf">https://spectrum.ieee.org/openai-rlhf</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/wPzvqkTm1bVC5ArYIkb4</id>
            <title>金山办公在知识库业务中的大模型思考和实践｜QCon</title>
            <link>https://www.infoq.cn/article/wPzvqkTm1bVC5ArYIkb4</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/wPzvqkTm1bVC5ArYIkb4</guid>
            <pubDate></pubDate>
            <updated>Fri, 28 Jun 2024 07:21:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 企业发展, 知识管理, 大模型 AI 技术, 创新发展
<br>
<br>
总结: 企业构建统一知识管理体系对企业发展至关重要，结合大模型 AI 技术的知识库赋予管理体系智能化生命力，为企业创新发展提供强有力支持。金山办公在 AI 知识库业务中的实践经验包括 AI 在知识库的落地场景、技术架构设计、大模型踩坑和调优等内容。AICon 上海站将探索大模型技术在不同领域中的实际应用与成效，为感兴趣的同学提供优惠购票机会。 </div>
                        <hr>
                    
                    <p></p><p>对企业而言，构建统一知识管理体系对企业发展至关重要，它在传承内部经验、管理企业知识、减少信息重复生产等方面成效显著。结合大模型 AI 技术的知识库，则赋予了这一管理体系智能化的生命力，使其能实时整合、精准分析各类知识资源，为企业的创新发展提供强有力的支持。</p><p></p><p>本文整理自金山办公 AI 知识库技术总监陈亮在在 <a href="https://qcon.infoq.cn/2024/beijing?utm_source=infoq&amp;utm_medium=conference">QCon 2024 北京的分享“金山办公在知识库业务中的大模型思考和实践”</a>"。本次分享将介绍金山办公在 AI 知识库业务上的一些实践经验，包括 AI 在知识库的落地场景、技术架构设计、RAG 技术、大模型踩坑和调优、技术演进等方面内容。</p><p></p><p>另外，即将于 8 月 18-19 日举办的 AICon 上海站同样设置了**「大模型场景+行业应用探索」专题分享，我们将精选具有代表性和规模的典型案例，展示大模型技术在不同领域中的实际应用与成效。目前是 8 折购票最后优惠期，感兴趣的同学请访问文末「阅读原文」**链接了解详情。</p><p></p><p>本文由 InfoQ 整理，经陈亮老师授权发布。以下为演讲实录。</p><p></p><p></p><h3>金山的 AI 发展路径</h3><p></p><p></p><p>首先，我想简单介绍一下 WPS 在 AI 产品方面的一些情况。目前，大模型在应用方面还没有出现一种现象级的产品，金山办公也不例外。去年，我们选择了全面投入 AI 的战略，并在过去一年中投入了大量资源，与客户一起共创并落地了一些 AI 产品。</p><p></p><p>在 4 月 9 日，我们举办了一场金山办公生产力的发布大会。在这次大会上，我们发布了整个 AI 365 平台，其中就包括了 WPS AI 产品。WPS AI 是为企业量身定制的，旨在帮助企业提高生产力，实现更高效的工作流程。</p><p></p><p>自去年下半年以来，我们与多家企业进行了深入的合作，共同探索 AI 技术在办公场景中的应用。在这个过程中，我们收集并分析了众多客户的痛点，将这些痛点转化为标准化的产品解决方案。在 AI 领域，我们金山办公确定了三个主要的发展路径。</p><p></p><p>首先，我们推出了名为 AI Hub 的产品，它本质上是一个智能基座。AI Hub 的核心功能是解决大模型如何被有效利用的问题，帮助用户更好地理解和应用这些复杂的 AI 模型。</p><p></p><p>其次，AI Docs，即我们的智能文档库，旨在通过 AI 技术赋能文档处理，为用户提供更加丰富和有价值的应用场景。</p><p></p><p>最后是 Copilot Pro，它本质上是一个 Agent 产品。Copilot Pro 能够帮助用户调用各种工具，完成特定的任务，提高工作效率。</p><p></p><p></p><h4>AI Hub 智能基座</h4><p></p><p></p><p>AI Hub 本质上是一个基座型产品，其主要功能是让大模型能够无缝地被调用。例如，通过 AI Hub，我们可以接入商汤、MiniMax、文心一言等模型。</p><p></p><p>在企业场景中，我们考虑了员工使用大量 token 可能占用他人配额的情况，以及管理层希望了解员工使用情况的需求。因此，AI Hub 提供了一个平台，可以在企业内部提供受控的大模型接入服务和聊天场景，实现信息安全和工作效率的双重保障。目前，我们已经接入了国内主要的大模型厂商。我们的模式支持公网、私网以及混合部署专区的模式。AI Hub 的另一个特点是，使用后还可以进行计费统计。企业可以通过曲线图直观地看到每天 token 消耗的数量，以及提示词的使用情况，这些都能从企业层面得到直观的体现。</p><p></p><p></p><h4>AI Docs 智能文档库</h4><p></p><p></p><p>AI Docs，即智能文档库，是我们基于 WPS 办公文档的深厚积累所推出的产品。金山办公凭借多年的文档处理经验，积累了一套优秀的文档解析能力。今年特别强调了 AI 知识库的重要性，希望通过大模型技术结合我们自身的文档基础，让企业各个环节上的文档得以激活，真正为企业带来文档内容的价值和知识洞察能力。在当前 AI 技术的支持下，我们可以让过去的文档通过结构化的方式，结合我们的解析能力，成为 AI 输入的来源。我们的解析能力足以覆盖文本、表格、图表等复杂结构。</p><p></p><p>智能文档库还包括智能创作功能，它本质上是解决内容生产问题。在大模型的基础上，我们利用它来生产内容，尤其是在金融、公文和论文等领域，这是一个重要的落地方向。基于明确的来源，我们可以利用大模型生成符合特定风格、字数和排版要求的内容。最重要的是，这一切都是基于我们的知识库产品来实现的，知识库本质上是海量文档的聚集池。例如，如果我们需要生成一篇 QCon 大会的演讲主题稿，我们只需向知识库中添加一些 QCon 的资料，然后通过一些机制，让大模型能够输出符合今天 QCon 大会需求的演讲稿。智能创作功能为我们的客户提供了一种便捷的方式来生成内容。在后续的介绍中，我们会详细说明其关键实现技术。</p><p></p><p></p><h4>Copilot</h4><p></p><p></p><p>最后一个要介绍的 AI 产品是 Copilot，它是基于 API、Agent 和大模型的体系架构设计的产品。虽然这个概念在竞品中已经相当常见，但 Copilot 的初衷是为了帮助企业降低成本和提高效率。它旨在取代企业中日常的重复性简单劳动，通过降低人力成本来实现降本增效的目标。例如，Copilot 可以自动提取销售报表和考勤数据等任务。</p><p></p><p>我们内部已经开始使用 Copilot，它带来了极大的便利。举个例子，如果需要创建一个明天 10 点的会议，传统工作流程中，我需要寻找会议室、预定、创建日程，然后发送给相关同事。但在 Copilot 的帮助下，我只需要简单地说一句话：“明天 10 点帮我创建个会议并发给相关人员”。Copilot 会解析这个请求，调用 365 内部的 API，如果需要，还可以接入企业的组织架构 API 来找到相关人员并创建会议。接下来，我想提出一个概念，即未来企业级 AI 的形态。我们提倡构建企业专属的知识大脑。知识大脑类似于人类的行为，具有记忆、思考、行动和自我反馈调节的能力。这是未来的一个目标，我们认为每个企业都应该思考如何构建自己的企业大脑。</p><p></p><p>大模型现在已经能够调用许多能力，包括企业自己的 API 和私有数据。金山办公提供了文档处理能力，以及 365 能力，后者包括 Office、会议、日历等套件的能力。通过 AI Hub 调用大模型，大模型就拥有了强大的思考能力、洞察一切的感知能力、超大容量的记忆和自我规划的执行能力。如果要用一句话来定义 WPS AI，那就是帮助企业构建自身的企业大脑，让企业的生产经营活动获得 AI 的加持，并提升降本增效的程度。</p><p></p><p></p><h3>不同场景下的技术实践</h3><p></p><p></p><p>技术实践方面，我想通过三个环节来分享我们的经验。</p><p></p><p>首先，是智能问答。智能问答是许多智能 AI 应用的标配，它包括提问和回答的流程。这个功能与多种技术相关，本质上是基于 RAG 的检索增强架构。</p><p></p><p>我今天更想强调的是解析、数据切断和数据安全这三个部分。我认为这是企业特别需要的，很多客户找到我们，希望与我们共同创造一个产品。他们拥有业务经营数据和 AI 提效的需求，但在 RAG 前置阶段，他们缺乏对数据进行解析、切断和清洗的能力。他们有这方面的尝试，但效果并不理想，因此希望与我们合作，共同提升这方面的能力。所以，今天我想单独介绍一下我们在这方面的做法和所能达到的水平。</p><p></p><p>第二个环节是创作。创作背后的原理其实涉及到召回和 SFT。我们会对模型进行一些细微的调整，以确保它在遵循指令方面更加符合我们的要求，同时让生成的内容更加多样化。</p><p></p><p>最后一块是智能简历库。简历在许多企业场景中都是一个常见的需求。例如，如果我需要招聘一位产品经理，HR 会推送给我许多简历。在这些简历中，我需要筛选出符合要求的候选人，比如具有 AI 工作经验的硕士产品经理。</p><p></p><p>传统的方式可能需要手动搜索，但在 AI 环境下，我们可以通过问答来实现，但问答本身存在非结构化的短板。因此，我们会进行结构化提取。非结构化处理在大模型、统计类、检索类任务中的表现并不理想。例如，如果我们把所有简历都交给大模型，询问有多少产品经理，大模型可能会给出不稳定的答案。</p><p></p><h4>智能问答</h4><p></p><p></p><p>智能问答是我们 AI 知识库的一个重要应用案例。它的核心功能是在海量知识库中检索出与用户查询最相关的问题，并将其呈现给用户。我们还有一个词条功能，可以在后台配置，比如出现公司某个财务同事的名字时，可以显示出来并跳转到对应的聊天框。此外，我们还能够检索出与上下文相关的图片，并引用文档来源，即与问题召回相关的文档。</p><p></p><p>这个场景有几个要点。首先是异构文档的解析，这是 RAG 架构的第一个环节，文档进来后，需要经过处理提取出内容；其次是精准检索，这与传统的推荐或搜索技术相关；第三是企业关心的数据安全需要有管控，问答中的管控是一个挑战，具体实践方式包括：用户输入查询时，对 query 进行改写，以检索出与 query 最相关的片段，然后交给大模型生成 prompt。</p><p></p><p>知识文档入库过程中，会经过解析、切断、过滤，以及 retrieval 和召回后的权限过滤：</p><p></p><p>解析：支持海量异构数据源的精准识别和解析。文档是企业最宝贵的数字资产，格式多样。我们内部有一套机制，可以将文档解析输出成统一的规范格式，支持 Markdown、json 等。切片：根据不同的文档布局，采用不同的切片策略。我们有七大分类，包括合同、公文、财报、论文等，每种文档都有不同的布局。我们会根据文档结构进行切片，采用页码、章节、段落、block 语义等策略。这样可以提高召回率，使大模型的问答效果更好。召回：采用多路召回策略，比单路召回有更高的召回率。召回率越高，送给大模型的答案越相关，效果越好。权限：在召回文件后根据文档 ACL 权限进行校验。我们会筛选出员工能看的文档，生成答案时不会包含不能看的片段。这是基于企业安全需求的管控措施，也是我们 B 端企业客户的一个痛点。</p><p></p><p></p><h4>智能创作</h4><p></p><p></p><p>智能创作与智能问答是紧密相连的，它们之间有着相似的入口。在创作方面，用户只需输入一个主题或匹配到推荐的主题，系统就能帮助生成符合用户需求风格和内容的文本。这些生成的内容可以直接填入云文档模板中，模板支持公文、合同、财报等多种类型，并且可以附上参考文档，显示生成文件所依据的原始资料。在智能创作的应用场景中，我们分析出几个特点：</p><p></p><p>创作必须基于事实，不能随意编造。需要支持多种创作风格，以适应不同角色和行业的需求。</p><p></p><p>具体实现智能创作的方法包括：</p><p></p><p>主题匹配：根据用户输入的主题，系统会匹配或召回相关的文档片段，生成大纲。大纲生成：大纲与主题之间存在相似度关系，根据大纲进一步匹配库中的文件，生成最终文档。Prompt 调优：通过几轮确认，包括召回和重新生成，让用户逐步得到他们想要的内容。SFT：为了支持多种风格并稳定输出所需内容，采用 SFT 技术进行模型微调。我们使用开源的 Lora 模型，基于特定数据集进行训练，以适应不同的创作场景，如财报、公文和合同等。</p><p></p><p>目前，智能创作在财报和公文方面的效果是令人满意的，但还未正式推向企业和大众使用。因为在实际应用中，还需要考虑许多专业术语和行业“黑话”，比如金融领域的市盈率、P/E 等，以及医药行业的专业表述，这些都需要专门的训练和处理以确保准确率。特别是在医药行业，对创作内容的准确率要求极高。例如，药品说明书的撰写不能有任何差错，因为它直接关系到药物的使用方法和患者安全。因此，智能创作在这些领域的应用需要经过严格的多轮验证，确保其输出的可靠性和专业性。</p><p></p><h4>智能简历库</h4><p></p><p></p><p>智能简历库是我们产品的一个特色场景，它主要处理结构化数据。在招聘过程中，我们经常会遇到需要比较候选人能力或推荐合适人选的问题。简历的格式相对固定，包含头像、姓名、联系方式、工作经历、教育背景等信息。在传统的大模型处理中，对于统计类或检索类的问题，如统计应聘某职位的人数，可能无法给出稳定准确的答案。</p><p></p><p>为了解决这个问题，我们采用了结构化提取的方法。通过结合大模型技术、自然语言处理（NLP）和命名实体识别（NER）等算法，我们可以将简历中的信息如姓名、工作经历等提取出来，并以结构化的形式存储在数据库中。当用户提出问题时，我们会将问题转化为结构化或非结构化数据进行处理。例如，用户可能想要找一个产品经理，我们会将这个问题转化为 SQL 语句，通过向量搜索找到相关的简历片段。</p><p></p><p>在结构化抽取方面，我们使用了大模型的 Slot 抽取技术和 Lora 微调。Lora 微调的目的是让预训练的大模型更好地适应垂直领域场景，使其能够更准确地识别和提取简历中的关键词。我们还生成了简历的总结，这有助于进行 JD（职位描述）匹配。JD 匹配与字段匹配是两种不同的方式。我们通过语义检索，结合 ES（Elasticsearch）技术，根据职位描述中的自然语言描述，如“需要多少年以上的工作经验”等，进行精准匹配。</p><p></p><p>查询思路包括统计和检索，例如查询有多少硕士以上学历的同学，系统能够准确回答并列出具体人员。这在传统的大模型语义问答中是难以实现的，而通过结构化处理，我们可以与传统的向量检索相结合，提供更准确的结果。此外，我们还面临问题转化 SQL 技术的稳定性问题，后续我们计划通过 Lora 微调来增强其稳定性和输出的可靠性。通过这些技术的应用和优化，智能简历库能够更有效地辅助企业在招聘过程中筛选和推荐合适的候选人。</p><p></p><p>经验分享</p><p></p><p>在大模型应用过程中，我们发现这个过程非常有趣。大模型就像一个知识渊博的老人，几乎可以回答任何问题，但准确性就需要我们自己来确保了。为了确保大模型应用的准确性和有效性，我认为应该从四个维度来进行规范和约束：设计、数据、优化和踩坑。</p><p></p><p>设计，我们需要有工程化的思维，特别是在问答或创作中，必须有一个严格的 pipeline 流程。因为在大模型中，数据的任何错误都会被放大，误差会随着流程的进行而增大。数据，我们的实践经验表明，当数据量不足时，优质的数据比数据量更为重要。对大模型来说，高质量的输入是更好的选择，因为低质量的数据会导致大模型输出更加不稳定。优化，我们内部有一套质量评测平台，用于评估问答或大模型输出的质量。核心思想是通过 query、context 让模型输出答案，并结合人工审核和标注，双管齐下，来评估回答的质量。踩坑，在使用大模型时，我们经常会遇到输出不稳定的问题。由于大模型是生成式的，每次预测的结果都可能不同。因此，我们需要在前面做一些调整，比如 Lora 微调，以保证输出的稳定性。尤其是在问答场景中，即使召回的片段相同，也无法保证每次的回答都一样。这时就需要采取一些措施，比如缓存、微调或者对 prompt 进行约束等。</p><p></p><h3>展望未来</h3><p></p><p></p><p>在大模型领域，我们见证了第一波以 GPT 为代表的大模型涌现，这引起了广泛的关注和好奇，因为这些模型显示出了强大的能力。紧随其后的是第二波应用层的创新。据统计，目前国内已有上百个大模型，尽管现象级别的应用尚未普及，但各行各业已经开始了自己的尝试和探索，包括金融、医药等不同领域都在积极进行 AI 的探索和研究。</p><p></p><p>首先，第二波创新应该专注于各个行业的应用场景，进行深入的创新。大模型的发展正从初期的好奇和娱乐，转向实用性和行业特定应用，这是一个必然的发展趋势。随着 GPT 3.5 API 的发布，我们可以预见这一趋势将变得更加明显。</p><p></p><p>第二个观点是开放赋能。由于我们始终面向 B 端客户，B 端客户实际上需要的是能够加速业务成长并带来价值的能力。无论是 SaaS 还是 PaaS 的方式，企业客户关注的是实际效果。因此，深入业务、提供实际价值是未来发展的关键。</p><p></p><p>第三，纯粹的理论研究无法产生实际价值。我认为，混合模式是未来发展的一个重要方向。虽然大模型能做很多事情，但在某些方面可能表现不够完美，需要进一步的调教和优化。这包括预训练、全参数调整或部分参数调整等方法。在我们的业务中，大小模型的结合将继续是一个值得深入挖掘的方向。</p><p></p><p></p><h5>内容推荐</h5><p></p><p></p><p>大模型正在推动历史性技术革命，知识触手可及。2024年6月14日至15日，ArchSummit全球架构师峰会在深圳成功举办，我们精选了峰会中聚焦AI大模型技术应用的相关PPT，内容涵盖了华为云AI原生应用引擎的架构与实践、微众银行大模型研发实践以及B站容量管理实践等。关注「AI前线」，回复关键词「大模型落地」免费获取PPT资料。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b3/b335a9592c31df536fb9c5c2a16a9820.jpeg" /></p><p></p><p></p><h5>活动推荐</h5><p></p><p></p><p>InfoQ&nbsp;将于&nbsp;8&nbsp;月&nbsp;18&nbsp;日至&nbsp;19&nbsp;日在上海举办&nbsp;AICon&nbsp;全球人工智能开发与应用大会，汇聚顶尖企业专家，深入端侧AI、大模型训练、安全实践、RAG应用、多模态创新等前沿话题。现在大会已开始正式报名，6 月 30&nbsp;日前可以享受&nbsp;8&nbsp;折优惠，单张门票节省 960&nbsp;元（原价 4800&nbsp;元），详情可联系票务经理 13269078023 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/eb/ebb596929eddf29435e38df0379aa023.png" /></p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/8oqBK9BazIsIDQZiuJcQ</id>
            <title>章文嵩、蒋晓伟、李飞飞、张凯巅峰对谈：大模型时代的数据智能新趋势 | QCon</title>
            <link>https://www.infoq.cn/article/8oqBK9BazIsIDQZiuJcQ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/8oqBK9BazIsIDQZiuJcQ</guid>
            <pubDate></pubDate>
            <updated>Fri, 28 Jun 2024 07:01:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据智能新趋势, 大模型时代, 数据驱动, AI与数据生产关系
<br>
<br>
总结: 4月11日，由极客邦旗下InfoQ中国主办的QCon全球软件开发大会在北京召开，围绕"大模型时代的数据智能新趋势"展开了巅峰对谈。在圆桌对话中，与会者讨论了数据在大模型时代的变化，以及AI与数据之间的生产关系是否发生了变化。通过讨论，强调了数据驱动的重要性，以及大模型对数据处理的影响。 </div>
                        <hr>
                    
                    <p>4 月 11 日，由极客邦旗下 InfoQ 中国主办的 QCon 全球软件开发大会暨智能软件开发生态展在北京国测国际会议会展中心正式召开。主论坛压轴的圆桌对话环节，AutoMQ 联合创始人 &amp; 首席战略官章文嵩、ProtonBase 研究员蒋晓伟、阿里云数据库产品事业部负责人李飞飞、蚂蚁集团 AI 安全商业化总经理张凯围绕<a href="https://qcon.infoq.cn/2024/beijing/schedule">“大模型时代的数据智能新趋势”</a>"主题展开了巅峰对谈。</p><p></p><p></p><blockquote>InfoQ&nbsp;将于10月18—19日举办&nbsp;QCon&nbsp;全球软件开发大会 上海站&nbsp;，覆盖前后端/算法工程师、技术管理者、创业者、投资人等泛开发者群体，内容涵盖当下热点（AI&nbsp;Agent、AI&nbsp;Infra、RAG&nbsp;等）和传统经典（架构、稳定性、云原生等），侧重实操性和可借鉴性。会上我们也设置了【下一代 Data for AI 技术架构】专题，将从 LLM、智能 Agent、RAG 等不同的热点领域方向，来探讨新一代 Data 技术的突破方向与思路。了解更多内容，可访问大会官网：<a href="https://qcon.infoq.cn/2024/shanghai/track">https://qcon.infoq.cn/2024/shanghai/track</a>"</blockquote><p></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a9/a9d8c4201f2aeb2e2af7f1e37ac74133.jpg" /></p><p></p><p>以下是对谈实录，经过不改变原意的整理和简化（感谢 ProtonBase 对稿件整理的大力支持）：</p><p></p><h2>AI 与数据，它们的生产关系是不是发生了变化？</h2><p></p><p></p><p>InfoQ：今天我们想探讨的是数据在大模型时代发生的一些变化。当下有一个话题非常火热，大家都在讨论 Data for AI 和 AI for Data ，在接下来的圆桌环节，我们希望以这个为话题展开讨论。</p><p></p><p>接下来我们讨论的第一部分话题是 AI 与数据，它们的生产关系是不是发生了变化？这次 QCon 展区悬挂了一些条幅，有咱们四位嘉宾的金句以及 slogan。其中飞刀的条幅上写的是算力驱动与数据驱动助力智能化时代加速进化，云原生与智能化推动结构化、半结构化、非结构化数据走向一体化、一站式处理。您能否解读一下这个观点？</p><p></p><p>李飞飞（飞刀）：我觉得大模型本质上是一个数据驱动的 scaling law，从量变到质变发生作用的这么一个过程。今天这个趋势是很明显的，人工智能的经典理论体系里面是有符号主义和连接主义的，实际上这两个路线一直在螺旋式上升，有一段时间连接主义是看到一些曙光，但后来沉寂了很久，实际上我大学上本科的时候就有 Neural Network（神经网络）这个概念了，但当时根本没有看到它的潜力，但它的基本框架很早就有了。</p><p></p><p>后来我们又转到了以知识图谱为代表的三元组的这种符号主义，逻辑推理等，直到今天的大模型，我觉得有点像《指环王》里面的王者回归。好像连接主义 dominate everything，本质上是这么一个简单的总结过程。为什么我会说算力和数据驱动会让数据的处理变成一体化和一站式，核心就是数据有这么几种形态——结构化、非结构化、半结构化。在我们数据管理系统的历史发展长河中，到现在为止，我们做的比较好的是结构化数据的处理，从传统的数据库再到数据仓库，再到从数据仓库衍生出来的大数据的体系，基本上还是围绕结构化数据来处理的。</p><p></p><p>非结构化、半结构化数据的处理说实话是浅尝辄止的，但是我觉得大模型的突破，尤其是 scaling law 的进一步发展，有可能会打通符号主义和连接主义，这是我个人的一个判断。当这件事发生以后，我觉得结构化数据、半结构化数据、非结构化数据的一体化一站式处理将变成现实，我觉得这是非常激动人心的一个时刻。</p><p></p><p>在另外一个经典的模型里面叫 DIKW——Data ，Information， Knowledge， Wisdom（数据、信息、知识、最后再到智慧）。Data 是最底下一层，我觉得我断言句的核心逻辑是我认为在接下来的 3~5 年，一个非常大的机会点是如何将多模态、各种类型的数据做到统一处理。统一未必是说通过一个引擎、一个平台，这个未必，可以是多个引擎，比如说存储统一、元数据管理统一，其中还是有多个引擎的。但是数据之间的流转、语义的理解、上下文的理解、任务的转发、数据流的这种处理，我觉得是可以被自动化或者被屏蔽掉的。从最终的业务视角来看，就是数据的一体化一站式的处理。这是我对断言的一个简单的解读。</p><p></p><p>蒋晓伟（量仔）：我非常同意飞飞老师，此外再补充两句。整个数据库和大数据所做的事情就是试图去理解数据，什么是结构化数据和什么是非结构化数据，它们的定义其实是在不断变化的。在关系型数据库出现之前，可能我们认为所有的数据都是非结构化的数据，但是关系型数据库引入了表的这种抽象，我们就开始给数据库表的结构。</p><p></p><p>在过去的两年之中，大语言模型对自然语言有了越来越深的了解，通过嵌入向量这种形式，给我们传统上认为是非结构化的文本数据赋予一种新的结构。这正是大数据和数据库对数据理解的下一个阶段。</p><p></p><p>随着从 AI 开始向 AGI 迈进，下一步自然就是给数据赋予智能的结构，接下来数据系统会有一个巨大的改变，数据系统新的使命将会是让数据涌现智能。</p><p></p><p>章文嵩：我其实跟他们两个的观点是一样的，实际上未来是更多的数据，多模态的数据，包括结构化和非结构化数据。另外尤其是现在的大模型，实际上是我们用大模型生成 embedding 很多向量数据，向量数据大部分是 AI 程序在用，我们现在在关系型数据库、数仓里面实际上存的都是基于关系型的数据，未来大模型更多使用的可能是基于概率的数据，这些向量数据。所以我觉得这个市场未来会非常大，因为关系型数据库的市场是一年几千亿美金的市场，未来云原生的向量数据库市场可能也规模不小。</p><p></p><p>张凯：蚂蚁今年有一个大的背景， AI First 也就是人工智能优先是我们集团的三大战略之一，所以从整个集团层面非常重视 AI 的投入。我所在的是安全相关的领域，我们自己内部有一句口号叫“AI 需要安全，安全需要 AI”，其实是形成一个自闭环。从生产关系的角度就是 AI 跟数据，我觉得第一点是数据本身已经成为生产关系的一个制高点，因为我们原先在训练模型的时候，更多的是模型驱动，数据本身对于模型的效能的占比不会特别大。随着大模型的出现，整个数据量级，包括数据的复杂度，数据已经成为生产关系的一个制高点。</p><p></p><p>第二点就是 AI 作为一个新的生产力，包括今年政府两会的报告也经常提出新质生产力这样一个新的名词。其实本质上我是觉得 AI 本身作为生产的一个生意，它已经具备了人脑的一些能力，我们经常说 AI 助手或者 AI 助理，不是说它在体力方面能够帮助我们去做什么，而是因为它在智力层面已经具备了一定的能力。从生产力的角度来看，这是一个非常大的升级。</p><p></p><p>最后一点我觉得 AI 跟数据本身已经形成了一个自闭环，包括我们现在通过 AI 的自动化技术去做数据标注，包括像医疗、金融等垂类的一些数据标注的服务，也包括现在比较火的，像合成数据，通过 AI 去生成一些新的数据。其实本身 AI 跟数据在这层生产关系上其实已经形成了闭环。</p><p></p><h2>AI 是否已经成为数据架构新的驱动力？</h2><p></p><p></p><p>InfoQ：前几年各个公司都在提，要做数据化，以及要做智能化，这两个其实是分开提的，但是在大模型诞生之后，数据化和智能化就合二为一，变成数智化这样一个大的战略方向。AI 是不是已经成为今天数据架构新的驱动力？</p><p></p><p>章文嵩：对，关键是你说的数据架构指的是什么？是整个数据链路的工程实现吗？如果是底层的系统工程实现，AI 怎么作为一个辅助力量，类似 Github 的 Copilot。当我们在编写程序的时候，它可能会给我们一些帮助，一些提示，但是还是得我们自己来选择。因为我觉得现在深层次的人工智能，它本身是并不理解这个结果的，因为它根据历史的数据进行预训练，然后针对问题，根据过去预训练出来的这些概率统计、组合生成一个结果，我觉得模型本身对这个结果是不理解的，所以有时候我们看到它一本正经地胡说八道。当然并不否定这个模型本身的有效性，它能把人类所有的文本知识都压缩在网络里面，如果我们会问问题，能很高效地找到想要的知识的话。当然，对生成的结果我们自己也要判断。所以我们做数据链路的工程实现上，整体的架构设计我们要理解需求是什么，要知道很多架构设计背后各方面的开销是什么，最终进行取舍。我觉得目前的大模型取代不了这方面的工作，最多是一个辅助的手段。</p><p></p><p>李飞飞（飞刀）：文嵩刚才讲到的其中一部分，比如说代码生成 Github Copilot，我们在大量的实践中发现目前的这种 Copilot，它对比如说前端代码的生成已经做到几乎非常完美了，还有比如说生成 UT 我们基于通义的灵码做得已经非常完备了，但是真正的底层系统架构的这些内核的代码，说实话目前还是有挑战的。</p><p></p><p>核心的原因还是因为今天的大模型是基于连接主义的，本质上它是一个压缩总结，然后概率性地预测的一个逻辑，所以它的可解释性以及推理能力还没有那么强，当然这块是有可能会被颠覆的，因为如果它真的就是一个 scaling law 堆积的过程，可能它最终会从连接组里面自动地带出符号主义，就是所谓的智能涌现这个能力，真的就是 AGI 了。当然至少目前这件事还没发生是吧？我也不知道会不会发生，这是第一点。</p><p></p><p>第二点实际上在 AI 辅助这个事情上，我觉得这是大概率会渗透到我们的方方面面。在接下来的 2~3 年，我觉得一定会看到这件事的发生，不光是在代码生成这一个场景，可能在很多的场景下，通过 multi-agent 的这种应用，Agent 之间的，API 的，如果说我们的数字世界各个模块的 API 构建得足够地标准、完善，我觉得 AI 驱动的 multi-agent 会确定性地发生，当然前提是我们各个模块的 API 要足够标准，足够模块化。</p><p></p><p>最后一点我想讲的是，至少在目前看来，AIGC 适合没有非常严苛要求的场景，比如说生成一个文本，生成一个 transcript，生成一个图片。对有非常严苛的正确性要求的，我刚才和量仔还在底下交流，这种有极其严苛要求的任务，至少目前的大模型的能力还没有做到完全取代人的作用。这是我对这个问题的几个回应。</p><p></p><p>蒋晓伟（量仔）：我非常同意文嵩和飞飞老师所说的，智能其实分为两个部分，第一个部分是人的直觉，见到一个事情，我觉得什么是对的。第二个部分是推理能力。我给了一个证明，我是不是能够读懂这个证明，这个证明是不是严格，来做这么一个判断。现在的大语言模型，生成式 AI，在直觉上我认为已经达到了人类水平，甚至已经超过了人类水平，但是在推理能力上与人类还有很大的差距。</p><p></p><p>而推理能力的完善其实就是通向 AGI 之路，一旦它有了严格的推理能力之后，我们就已经跨越了奇点，达到了 AGI。在那步达到之前，我们需要选择对错误有容忍的场景。比如我们让它写代码，有错误的时候可能就会有问题，需要人去查看。但是如果让它写测试代码，测试一些错误，它的容忍度会相对高一些，所以我们就需要在工作之中去发现、挖掘这种场景。</p><p></p><p>InfoQ：其实我还想问一下大家，在各自的公司中有哪些地方已经开始已经利用大模型去改造你们的一些业务了？</p><p></p><p>蒋晓伟（量仔）：现在还在初期，我们尝试着用大模型写一些测试，这也还是初期的一些尝试，同时我们也试图去用大模型从文本生成一些 SQL，效果现在还是有待改进。</p><p></p><p>李飞飞（飞刀）：我具体讲两个例子。一个是代码生成，当然我们在公司内部不可能用 Github Copilot，因为安全的问题，我们自己基于通义做的灵码效果也非常好，我们现在全员用灵码做代码生成，尤其是前端代码，还有像测试 UT 等等，还有像一些任务流的生成，效果非常好，对我们 LOC 的提升是非常明显的，这是第一个。</p><p></p><p>第二个是比如说在应用侧 NL2SQL，借助大模型的能力去构建新的和数据库、大数据系统的交互方式，这块我觉得也是取得了非常好的业务进展。</p><p></p><p>张凯：大模型蚂蚁这儿其实是三类，第一类就是基座大模型或者是通用大模型，因为大模型大家现在看到它最强的能力其实是它的通用能力，也是为什么我们叫它 AGI 的原因，它能回答你各种各样的文科问题、理科问题等等，这是一类。</p><p></p><p>第二类其实我们会结合蚂蚁的禀赋去做一些垂类模型，比方说金融的大模型或者是医疗的大模型，大家在支付宝上可以看到，我们在 4 月初上线了一个医疗服务的大模型助手，因为我本人其实就头疼去医院挂号，专家问询等等。</p><p></p><p>第三类其实就是我的专业领域相关的安全大模型或者是大模型安全。因为大模型本身的一些内生的，像内容安全、数据安全等，一会儿我们可能会展开聊这块。</p><p></p><h2>湖仓一体，它的终极形态应该是怎么样的？</h2><p></p><p></p><p>InfoQ：我们可以看到目前为止，已经有各种各样的数据，它可能是非结构化的，也可能是半结构化的，包括它们可能是从不同的地方过来的，那么面对这样一些不同来源、不同形式的数据，是不是有一些新的方法能够实现更加有效的多模态数据融合？</p><p></p><p>章文嵩：前面飞飞已经提到过了，多种来源的数据肯定最好是在一个平台把它存起来，在一个平台进行加工处理。这个肯定是湖仓一体，这是大趋势。</p><p></p><p>InfoQ：我想沿着湖仓一体这个话题来问下一个问题，在您看来，湖仓一体，它的一个终极形态应该是怎么样的？尤其是在咱们大模型的推动之下。</p><p></p><p>章文嵩：湖仓一体的终极形态就是要集成多种数据源的存储处理，包括上面的使用。然后跟现有的很多系统应该可以对接起来，应该可以把更多的数据汇集到最终的一个平台上面来。</p><p></p><p>蒋晓伟（量仔）：我的观点可能稍微有点争议。湖仓一体我们首先得理解它解决的问题是什么，我觉得数据湖主要解决两个问题：第一个问题是我们在一份数据之上需要有各种各样的数据处理能力和计算能力，现在没有一个系统能够具有所有的数据计算和处理的能力，所以我们就开始有了用多个引擎在同一份数据上处理的能力，所以我们把数据放到 S3，放在对象存储之中，这就形成了一个湖。这是它需要解决的第一个问题，能够在数据之上有更丰富的处理能力、计算能力。</p><p></p><p>它解决的第二个问题是成本问题，因为对象存储相对比较便宜，把数据存在对象存储之上能够减少我们的存储成本。</p><p></p><p>随着技术的发展，慢慢地会产生更好的平台或引擎，它们具有多种计算的能力，这个时候对湖的需求就会慢慢地减少。所以随着技术的发展，我认为湖的场景会变得越来越少，甚至湖就成了仓库的一部分，变成了房间里的一个游泳池。</p><p></p><p>所以我觉得湖仓一体的最终形态可能是湖被完全吸收到了一个功能更加强大，成本更低的数仓之中。</p><p></p><p>章文嵩：我觉得没有什么冲突，因为大部分的数据无论是结构化还是非结构化数据都会汇聚到类似对象存储上面去。对象存储之后，因为存算分离上面的计算部分可以有多种多样的计算引擎，这并不矛盾，因为如果我们把所有的数据汇聚到对象存储一个统一的存储层，那上面可以支撑所有的，因为统一的数据视图对任何一家公司、任何一个组织来说是至关重要的，在上面我可以堆叠很多种引擎。</p><p></p><p>我觉得终极的形态，首先上面肯定是更多地用自然语言来使用这样一个平台，量仔也在尝试能不能通过自然语言生成 SQL，这个准确度肯定是会随着时间不断地提高的。另一方面，计算引擎之上肯定更多的 AI 的程序会来使用。我们现在数据分析师做决策，大部分都是分析师在那看，未来是更多的程序，更多的 AI 程序查看数据，所以我觉得未来肯定是这两个趋势。</p><p></p><p>李飞飞（飞刀）：为什么我那个断言里面提到了很重要的另外一个词叫云计算，我觉得算力的基础设施化，一定会让我们计算资源的解耦变成一个现实，比如说现在的存储计算分离，甚至下一代，我认为在计算这一层， CPU 和内存也会分离，内存也会池化。这样就带来一个显而易见的趋势，就是最底下的一层存储肯定是统一了，成本低，但延迟可能比较高，比如说像对象存储这样一层。然后为了计算加速，要有存储的专属格式，这是为什么以前有各种各样的数据系统的一个根因。但是存计分离以后，有三层的分离以后，专属格式可以在成本比较高的存储这一层再来实现，最低那一层的存储，就是一个通用的存储格式。所有标准层的，不管你上面是什么类型的，到那层统一掉，然后在上面这一层，比如说块存储，甚至本地盘，甚至到内存池化这一层，再转化成专属格式来做计算加速，然后计算有多个计算引擎，计算引擎计算可以是无状态的。</p><p></p><p>只要对用户做到元数据的统一管理、隔离、安全、AccessControl，并保证体验的统一，逻辑上来讲还是多个引擎，但是对用户侧来说，感知是完全统一的。我觉得未来大概率是往这个方向去演进。</p><p></p><h2>如何衡量数据系统的物理极限？</h2><p></p><p></p><p>InfoQ：量仔之前接受过我们的一个采访，当时你提到了一个新的名词 Data Warebase，这应该是一个比较新的词，能否再给我们阐释一下？</p><p></p><p>蒋晓伟（量仔）：好的。最近马斯克在他的 X 平台发布了一个分享，他说评价一个产品正确的方式，不是跟竞争对手比（太容易），而应当跟物理极限比。如果我们把追求物理极限当做一个数据系统的目标，那我们应该从哪几个维度来评价物理极限呢？技术到最后还是要服务于业务，我认为从业务的视角来看，它有三个核心的需求：性能、正确性和实时性。</p><p></p><p>第一个需求是性能，它也是最显然的一个需求，性能也是过去 20 年里大数据蓬勃发展背后最主要的推动力，特别是在 AI 时代，数据量急剧增长，AI 对性能的需求也在不断地提升，用户希望数据系统能够满足 AI 所带来的无论多么高的性能需求，这是一个方面。第二个同样在 AI 时代，用户使用数据的方式也会变得越来越多样，场景也会越来越复杂。作为一个好的追求极限的数据系统，它能够满足数据任意使用方式的性能需求。</p><p></p><p>第二点是数据的正确性，正确性就意味着任何时候存储在系统之中的数据都是正确且一致的，当我们做任何一个查询，返回的结果也都是正确的、一致的，只有做到这一点，在数据系统之上用 AI 所做的各种智能决策才能够有坚实的基础。但数据的错误往往比较隐蔽，因此这一点比较容易被忽略，但是对于一个追求极限的数据系统来说，这必须是一个业务最核心，而且最基本的需求之一。</p><p></p><p>第三点是数据的实时性，不同的系统可能对数据的实时性要求不一样，有的系统达到小时级的实时性就够了，有的系统需要分钟级甚至秒级实时性。在有了 AI 之后，就可以通过 AI 让系统自动地做出很多决定，因此数据链路的实时性往往决定决策链路整体的实时性，这也会影响数据所能产生的业务价值。作为一个追求极致的数据系统，我们自然也希望它能够满足最苛刻业务的实时性需求，也就是它的数据延迟性必须做到任意的低。</p><p></p><p>我认为从业务这三个核心需求出发，接下来会涌现出一类全新的数据产品，它就是分布式 Data Warebase。Data Warebase 是 Data Warehouse（数据仓库）和 Database （数据库）这两个词的融合，它意味着这样一个系统同时具备了数仓和数据库的所有能力。分布式 Data Warebase 在数据库的场景将会是一个更好的数据库，因为它解决了数据库水平扩展的问题。分布式 Data Warebase 在数仓场景也会是一个更好的数仓，因为它同时解决了数仓场景数据正确性和实时性的问题。</p><p></p><p>所以分布式 Data Warebase 是从业务的三个核心需求——性能、正确性和实时性出发得到的一个必然推论。它不是一个发明，而是一个发现。</p><p></p><p>章文嵩：针对量仔说的这三点，我觉得应该再增加两点。第一个点是成本，因为是不是以最低的成本满足业务的需求，实际上是我们永远追求的。我的系统有没有足够多的弹性？随着业务的需求的增长，成本是逐渐增加的。另外就是安全性对吧？我们做任何系统怎么确保数据的安全，怎么确保用户的隐私，数据的保护，任何异常的行为，都要确保安全性，这样才会有业务的安全。</p><p></p><p>InfoQ：量仔其实提出过一句话，叫“从业务本质需求出发，探索数据系统物理极限”。所以前面的回答是在阐释这句话？</p><p></p><p>蒋晓伟（量仔）：是的，如何衡量数据系统的物理极限，我们刚才说到了性能、正确性和实时性。文嵩老师又加了一个成本，在我看来成本其实是性能的一部分。</p><p></p><p>章文嵩：我觉得可能我们可以综合一下，这 5 点有可能是我们做系统永无止境追求的目标。</p><p></p><p>蒋晓伟（量仔）：是的，非常同意。</p><p></p><h2>数据和 AI 的基础设施协同目前已经达到有效的方式了吗？</h2><p></p><p></p><p>InfoQ：文嵩老师其实一直在深耕数据基础设施层面的工作，在您看来，当前这个情况下，数据的基础设施和 AI 基础设施它们的协同目前已经达到一个有效的方式了吗？还是说我们还可以有一个更好的方式让它们更好地协同起来？</p><p></p><p>章文嵩：因为数据跟 AI 本身就是一体的，AI 需要数据，在数据上我们能产生更多的智能，但是我们知道 AI 成功的三个主要要素，我觉得是人、数据还有算力。为什么说人，我觉得人在里面是最关键的，人包括领域的人才、算法的人才，还有工程的人才，实际上要聚合这么多的人才并不容易，这实际上使得 AI 的门槛相对来说是比较高的。所以怎么样复用这些人才的经验，你要有数据的基础设施，包括 AI 应用的基础设施，能不能让更多的用户来使用 AI 的基础设施，搭建应用更方便。前面郭东白老师的分享中提到他是做应用架构的，要做很多的选择，其中一个考量点是要不要做 AI 大模型，我实际上有不同的观点。因为 AI 的模型实际上规模越来越大，从几千亿的参数到几万亿、几十万亿，未来 GPT6 要到 100 万亿这样参数的规模，这些 AI 的大规模训练成本不是中小企业能承担的，也不应该是中小企业要考虑的范围。所以我们更多地要用第三方的基础大模型服务，或者基于开源已经训练好的开源大模型来做，因为上面有更多灵活性。</p><p></p><p>所以上面你刚刚提到的两者，云原生的数据基础设施跟云原生的 AI 基础设施，肯定是相互协同的，因为数据基础设施提供了统一的、共享的数据平台，然后 AI 的基础设施之上开发应用会更加方便，更加快捷。我觉得在大模型时代， AI 应用的模型各方面的开发门槛会大幅降低，越来越多的中小企业甚至个人都可以做自己的 AI Agent。</p><p></p><h2>数据安全领域的新挑战与发展方向</h2><p></p><p></p><p>张凯：在当下的应用来讲，生成式 AI 的特性已经模糊了我们传统安全的边界，所以带来了大量的不确定性。主要包括三块：</p><p></p><p>第一块是数据层面，数据层面按照大模型的生命周期来讲，最早是要做预训练。预训练的时候，喂大量 PB 级别的数据进去之后要祛毒，包括里面的一些数据安全、伦理安全等等，需要快速甄别海量数据的安全挑战，这是第一块。</p><p></p><p>第二块是预训练结束之后需要进入到微调阶段。微调阶段其实核心是考验数据标注的准确性，数据标注的准确性可以帮助我们让大模型的价值往我们想要的那个方向往前发展。</p><p></p><p>但是这两块其实也只是基础，再往前走的话，其实是应用层面。应用层面我们蚂蚁团队现在在做一个产品，叫蚁天鉴。它分为两部分，一个叫蚁鉴，蚁鉴是给大模型做体检的，包括大模型本身的数据安全、内容安全以及科技伦理等等，就看整体大模型的一些风险程度，确保这块是没问题的；另外一部分叫天鉴，相当于我们在大模型的外部设置了一个围栏，确保整体大模型在应用层面有边界保障。</p><p></p><p>InfoQ：当前在数据安全领域，老师观察到有哪些让您觉得很兴奋的，或者说让您觉得非常有潜力的应用方向吗？</p><p></p><p>张凯：确实有几块，一块是数据层面，比方说像合成数据，合成数据大家可以关注一下做合成数据的一些，像美国的一些公司，估值都非常高，不亚于大模型厂商的一些估值。</p><p></p><p>然后我们看了一些研究报告的评估，有一份研究报告，比方说像 AI Epoch research，它预估在 2026 年之后，现有的能够提供给大模型训练的真实数据基本上已经被耗尽，这个大概率是一个趋势，那么在 2026 年之后合成数据的应用可能会成为一个必然。</p><p></p><p>第二块就是我刚才提到的 AI 标注，也就是大模型的数据标注。这块我们其实刚才提到 ScaleAI 这个公司，我们其实没有看到在国内有真正对标这家企业去为整个大模型产业链条提供服务的自动化的标注厂商，所以这块其实我们也是在积极地往前做探索。</p><p></p><p>最后就是我自己的本业，大模型安全这一块。</p><p></p><p>章文嵩：说到安全领域，我觉得有两个主要的方向，因为我曾经向安全领域的技术大佬请教过，安全主要做哪些事情，他给我三个关键词：可感、可控、业务优先。可感，你能感知到整体的安全形态怎么样，然后如果有危险、有风险的话要可控，安全响应系统是怎么样？当然业务优先，当安全跟业务发生冲突的时候，那个是一个价值的判断，一定要满足业务要求，然后我们最大的安全能做到怎么样。</p><p></p><p>所以在这里面我觉得可感、可控方面，这是安全里面的两个最大的领域。可感、可控，实际上 AI 技术怎么来应用到里面去，因为全局的安全事态感知系统，包括全局的安全响应系统，实际上这里面我觉得有很多值得去探讨的。</p><p></p><p>李飞飞（飞刀）：如果把人当做一个智能的计算体的话，本质上有三个关键步骤，一个是感知，文嵩和张凯讲到的这个感知这部分，就是可感、感知。</p><p></p><p>第二就是计算，获取感知以后，把它转化成各种脑能够处理的信号做计算，那么在计算过程中，需要确保不出差池。整个最后的结果是有逻辑性的，有推导条理的，这就要有安全的保障。所以总结就三件事，就是感知、计算、安全，大模型能否够帮助我们把这三件事做得更好，是挺令人激动的一件事情。</p><p></p><h2>总结：数据智能时代的未来趋势</h2><p></p><p></p><p>蒋晓伟（量仔）：过去的这么多年，业务发展非常快，数据量变得越来越大，大家都疲于奔命去解决系统的性能问题。这些性能问题有很多是由于场景变得越来越丰富，特别是 AI 所带来的。随着技术的发展，性能问题逐渐得到解决，在大部分场景已经不再是业务的主要阻碍，而当性能问题解决之后，我们就必然会看到更深层次的一些需求。比如说刚才我们提到的几个需求（性能、正确性和实时性）。除此之外更重要的是大家必然会对体验更加重视，接下来对体验的重视会使一些新的产品涌现，体验将会成为区分下一代新产品一个很重要的标准。</p><p></p><p>此外，AI 时代会给整个数据系统带来一个新的使命，就是让数据涌现智能。 我希望和大家一起来探索下一代的数据系统。</p><p></p><p>张凯：昨天我们内部看马老师写了一封长信，鼓励大家继续上路，其中他也提到了 AI 这一块，跟大家共勉，大概意思是说 AI 时代已来，但是我们现在其实才刚刚上路。我自己其实也是这样一个心态，作为一个初学者在路上，但是仍然会觉得非常兴奋。 AI 相关的这些数据模型，包括安全等等，我自己还是蛮期待未来几年这个行业的一些变化的。</p><p></p><p>李飞飞（飞刀）：其实挺难总结的，我觉得数据与 AI，两者缺一不可。未来如果大家从事相关工作、真想把 AI 做好，不是只做上面的应用，而是希望真正在这方面有一些贡献并真正产生影响力的话，底层数据系统的构建原理，是值得花时间去思考的。</p><p></p><p>章文嵩：我觉得智能化第四次的科技革命可以持续 100 年，所以在这 100 年里面，我们其实有很多工作值得去做，云原生的数据基础设施，云原生的 AI 基础设施，可以大幅降低 AI 应用的门槛，未来一定会有大量的 AI 应用涌现出来。</p><p></p><p>活动推荐：</p><p></p><p>InfoQ&nbsp;将于 10 月 18—19 日在上海举办&nbsp;QCon&nbsp;全球软件开发大会&nbsp;，覆盖前后端/算法工程师、技术管理者、创业者、投资人等泛开发者群体，内容涵盖当下热点（AI&nbsp;Agent、AI&nbsp;Infra、RAG&nbsp;等）和传统经典（架构、稳定性、云原生等），侧重实操性和可借鉴性。现在<a href="https://qcon.infoq.cn/2024/shanghai/track">大会</a>"已开始正式报名，可以享受&nbsp;8&nbsp;折优惠，单张门票立省&nbsp;960&nbsp;元（原价&nbsp;4800&nbsp;元），详情可联系票务经理&nbsp;17310043226&nbsp;咨询。</p><p></p><p><img src="https://static001.geekbang.org/infoq/65/6535951d1ed520ba19893cb8c187ad6d.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/XPMslMBIT0HggCWFQZwg</id>
            <title>大模型个性化时代到来！讯飞星火V4.0首发“个人空间”，打造实用的AI助手</title>
            <link>https://www.infoq.cn/article/XPMslMBIT0HggCWFQZwg</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/XPMslMBIT0HggCWFQZwg</guid>
            <pubDate></pubDate>
            <updated>Thu, 27 Jun 2024 10:40:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 讯飞星火V4.0, 大模型个性化时代, 语音大模型, 个人空间
<br>
<br>
总结: 讯飞科大在北京发布了讯飞星火V4.0及相关应用，该大模型在多个领域取得突破，包括语音识别、个性化应用、医疗领域等，为用户提供更智能、个性化的服务。 </div>
                        <hr>
                    
                    <p>讯飞星火V4.0来了！6月27日，科大讯飞在北京发布讯飞星火大模型V4.0及相关落地应用。讯飞星火V4.0七大核心能力全面提升，整体超越GPT-4 Turbo，在8个国际主流测试集中排名第一，国内大模型全面领先。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cb/cba8ab630a75a5446b1a4a8fa5783b9c.jpeg" /></p><p></p><p>大模型个性化时代到来！讯飞星火APP/Desk全新升级，发布“个人空间”，打造更懂你的AI助手；面向专业领域的个性化应用，科大讯飞升级讯飞晓医APP，上线个人数字健康空间，打造每个人的健康助手；业界首发星火智能批阅机，“AI助教”助力老师减负增效、因材施教；讯飞AI学习机升级 1对1 答疑辅导功能，打造每个孩子的AI学习助手。</p><p></p><p>面向万物互联时代，星火语音大模型再突破，发布74个语种/方言免切换对话，破解强干扰场景下语音识别难题，发布国际领先的极复杂场景语音转写技术，并通过云边端及软硬一体化解决方案，赋能汽车、家电、机器人等领域人机交互变革。此外，面向企业“人工智能+”场景价值落地最后一公里，科大讯飞正式发布星火企业智能体平台，并推出星火商机助手、星火评标助手等典型智能体案例，助力企业价值创造。</p><p></p><p>8个国际主流测试集测评第一，讯飞星火V4.0 整体超越GPT-4 Turbo</p><p></p><p>今年中高考真题实测中，讯飞星火语数外各科“成绩”均排名第一，被评为“更会做题的大模型”；在科研上，讯飞星火助力中国科学技术大学刘海燕教授团队，将蛋白质设计成功率从0.1%提升到20%，设计所需时间从6个月降到1天；赋能每个人，帮助一位不懂法律知识的70岁老人顺利要回养老钱欠款、帮助一位听障人士圆了文学梦······讯飞星火正成为我们每个人的AI助手。</p><p></p><p>自去年9月全面开放以来，讯飞星火APP在安卓公开市场累计下载量达1.31亿次，在国内工具类通用大模型App中排名第一，并围绕写作、编程、工作、学习等涌现出一批用户喜爱的热门助手。今年“618大促”，星火大模型加持的智能硬件销量同比增长超70%，月均使用次数超4000万，越来越多的用户开始享受到大模型带来的红利。</p><p></p><p>现场基于全国首个国产万卡算力集群“飞星一号”，讯飞星火大模型V4.0正式发布。讯飞星火V4.0 七大核心能力全面升级，全面对标GPT-4 Turbo，并实现在文本生成、语言理解、知识问答、逻辑推理、数学能力等方面的整体超越。</p><p></p><p><img src="https://static001.geekbang.org/infoq/29/29ec0d7b6d814979f3e9599a1c468bb2.jpeg" /></p><p></p><p>讯飞星火V4.0在图文识别能力上进一步升级，在科研、金融、医疗、司法、办公等场景的应用效果已领先GPT-4o。此外，星火长文本能力也全新升级，并针对长文档知识问答的幻觉问题，业界首发溯源功能。</p><p>外部权威测试集也体现出讯飞星火V4.0的领先性。在国内外12项大模型主流测试集中，讯飞星火在8个测试集中排名第一，超越GPT-4 Turbo等国际大模型，国内大模型全面领先。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cb/cb469d9b336ca37d0e09ef7197eaca3a.jpeg" /></p><p></p><p>现场，刘庆峰展示了讯飞星火V4.0在复杂指令、复杂逻辑推理、空间推理、高中数学等方面的效果，星火“智商”再度进化。以空间推理为例，“Bob在客厅里。他拿着一个杯子走到厨房。他把球放进杯子里，然后拿着杯子走到卧室。他把杯子倒过来，然后走到花园。他把杯子放在花园里，然后走到车库。问题：球在什么地方？”讯飞星火可以基于空间和常识推断出球在卧室的地面上，这些能力的进步对于以后的具身智能、家庭机器人都具有意义。</p><p></p><p>大模型个性化时代到来！讯飞星火首发“个人空间”，数百万用户一键拥有“AI智能全家桶”</p><p></p><p>大模型在给我们的工作、生活带来便利的同时，也存在各家生成内容差不多、生成内容较泛、不够实用的情况，怎么样让大模型更好用，在工作生活中形成独特的价值？科大讯飞给出答案——打造更懂你的AI助手。</p><p>如何打造懂你的AI助手？刘庆峰提出，AI助手要能够基于用户画像进行个性化表达，基于使用历史进行记忆学习，基于个人资料进行增强学习。在构建用户个人画像时，人设风格可以自己选定，也可以根据对话和使用历史动态完善，进而形成个性化的表达风格；AI助手再结合个人资料，就可以生成个性化和针对性内容。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8b/8b453421b3e5f0dffd3aef913fbdd932.png" /></p><p></p><p>基于此，讯飞星火APP及桌面版全新升级改版，率先发布“个人空间”，用户可以上传自己的工作、学习、生活、健康等各类资料，形成每个人的专属知识库，再结合人设，让大模型生成更个性化内容。此外，讯飞星火首批上线 14 个智能体，面向特定场景打造专属助手。</p><p></p><p>科大讯飞研究院院长刘聪现场演示“个人空间”效果，当他上传了女儿写的小作文并选取符合女儿风格的AI人设标签后，星火生成了一篇活泼、可爱更个性化的文章；当他上传了讯飞翻译机的产品海报、用户短视频、相关录音，星火也可以根据这些多模态信息生成产品培训文档，还可以对生成的信息进行多模态溯源。大模型进入个性化时代，大模型工作、学习“可用性”飙升！</p><p></p><p>此外，星火大模型还打通了全系讯飞C端软硬件产品生态，数百万智能硬件用户一键拥有“星火全家桶”。比如讯飞智能办公本、智能录音笔的文件可以一键同步到星火个人空间中，通过数据互通、操作联动，把一篇办公本里会议记录同步到星火中，就可以让星火进行公文写作，还可以做PPT，以及生成待办事项等，带来更高效的办公体验。</p><p></p><p>个人数字健康空间来了！讯飞晓医APP下载量超1200万</p><p></p><p>面向专业领域的个性化应用，科大讯飞升级讯飞晓医APP，发布个人数字健康空间，打造每个人、每个家庭的AI健康助手。</p><p></p><p>在医疗领域，讯飞星火医疗大模型再次升级，医疗核心能力全面超过GPT-4 Turbo和GPT-4o。在此基础上，讯飞晓医APP各项能力持续升级，覆盖1600种常见疾病、2800种常见药品、6000种常见检查检验，满足用户在看病前、用药时、检查后的核心场景健康需求。当前，讯飞晓医APP累计下载量1200万，用户好评率98.8%，主动推荐率42%。</p><p></p><p><img src="https://static001.geekbang.org/infoq/25/258fa79753bd93f1b447fd8a3f42d1b1.jpeg" /></p><p></p><p>现场刘庆峰介绍，讯飞晓医APP上线的“个人数字健康空间”，它能够根据电子病历、检查报告、体检报告等用户个性化资料，构建个人数字健康空间，在看病前可以进一步剖析病症原因，用药时给出药物禁忌的个性判断，在检查后联合对比给出数据变化，并通过角色切换，了解其他家庭成员的健康状况。</p><p></p><p>目前讯飞晓医APP已通过数据安全与隐私保护的多类权威认证，进一步保障健康数据的安全。在当前医疗资源相对匮乏的情况下，讯飞晓医 APP 的出现有效缓解了社会对医疗服务的迫切需求，为个人及家庭健康管理提供了新的模式。</p><p></p><p>老师最强辅助！星火智能批阅机让老师作业批改负担下降90%</p><p></p><p>得益于底座大模型的升级和面向教育复杂场景的图文识别效果进一步提升，科大讯飞发布首款星火智能批阅机，它集智能批改、精准学情、个性学习于一体，它支持自由排版，不限纸张大小的作业，在支持多学科多题型智能批改的同时，还能即时生成多维学情报告，还为老师作业讲评和面批辅导提供了素材。刘聪在现场演示了星火智能批阅机批改作业的全流程，15份学生作业半分钟就能批改完成，批改模拟了真人笔迹，和老师平时批改作业几乎一样。</p><p></p><p><img src="https://static001.geekbang.org/infoq/68/681f781d2fc32092db4af99cabf0d5cb.jpeg" /></p><p></p><p>有了星火智能批阅机，老师多了一个减负增效、因材施教的AI助手，原先要90分钟才能批改完的作业，现在只要5分钟就能完成；人工分析学情要60分钟，现在星火1分钟就能完成；得益于个性化作业，学生的错题解决率也从50%提升到73%。</p><p></p><p>在今年中高考评测中，讯飞星火被外界评为“更会做题的大模型”。本次讯飞星火进一步升级了讯飞AI学习机的 AI 1对1 答疑辅导功能，既能进行多模态启发式讲解、自由问个性化解答，也可以进行互动探究式学习、超拟人引导式伴学等，让孩子多了一位“AI辅学老师”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/64/649e342561579e53035f39173322eb74.jpeg" /></p><p></p><p>数据显示，相比较传统解题视频学习，AI答疑辅导的学习方式让孩子的学习完成率提高到90%，错题解决率提升到93%，孩子更愿意主动思考，学习效率更高，自信心也增强了。</p><p></p><p>星火语音大模型发布74个语种方言“自由对话”，破解强干扰场景下语音识别难题</p><p></p><p>近期科大讯飞作为第一完成单位的《多语种智能语音关键技术及产业化》项目，获得国家科学技术进步奖一等奖。发布会现场国奖得主再出“王炸”，星火语音大模型迎来新突破。</p><p></p><p>刘庆峰认为，语音将成为万物互联时代人机交互的主要方式，人机交互最重要的场景是远场、噪声、多人说话、多语言，因此万物互联时代的AIUI（人工智能用户界面）要满足远场高噪声、多语言多方言、全双工、多模态等标准。科大讯飞也主导制定了全双工语音交互ISO/IEC国际标准，并于2023年5月发布。</p><p></p><p>面向万物互联时代，本次星火语音大模型发布国际领先的多语种多方言免切换语音识别能力，可支持37个语种、37种方言“自由对话”。其中，37个语种识别效果领先OpenAI whisper-V3，37个方言识别效果平均提升30%。现场，科大讯飞演示了讯飞输入法混合方言和外语的语音输入效果，能让输入效率大大提高。</p><p></p><p>科大讯飞还发布了软硬件一体化讯飞同传系统，可支持大会同传、会议同传、展厅同传、旅游同传等多场景使用。本次参会的嘉宾座位上同样配备讯飞同传的收听设备，佩戴后即可实时收听多语种AI同声传译。</p><p></p><p>针对强干扰场景下的语音识别难题，科大讯飞突破了多人混叠场景下的极复杂场景语音转写，即使在三人混叠说话场景也能实现86%的语音识别准确率。三位讯飞研究院的研究员现场实测了在噪音场景下，同时混叠着说话，正常人耳已难以听清，只见讯飞星火的多模态能力不但实现了三人重叠语音的角色分离，还能实时转写出每个人说的话，炸裂的效果引发现场掌声不断。未来基于多模态的声音识别技术，将应用在讯飞听见智慧办公、智慧屏等会议办公产品中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e7/e7b1dd3c5ccfad777f6392ea965c0dac.jpeg" /></p><p></p><p>大模型正在推动人机交互变革，语音领域的所有应用都值得被重构。在大模型加持下，星火汽车智能座舱全新升级，不但具备了多语种多方言的“自由交互”，还具备多情感多模态的超拟人交互，让人车交互更有温度。当前，讯飞语音交互产品国内市占率稳居第一，同时广泛出口到世界各地。星火大模型为一汽、奇瑞、广汽、江淮、长城等车企的众多车型，赋予了高度智能的交互体验。</p><p></p><p>为了让大模型更好落地，科大讯飞还打造了云边端一体化和软硬件一体化的解决方案，赋能家电、运营商、机器人等更多行业场景。面向具身智能和人形机器人企业需求，本次科大讯飞正式发布机器人超脑平台2.0，业内首个支持多模态交互。目前，400+机器人企业已经采用讯飞机器人超脑平台。</p><p></p><p>星火企业智能体平台正式发布，打造每个岗位专属AI助手</p><p></p><p>自去年5月6日发布以来，讯飞星火大模型正成为国家能源集团、中国石油、中国移动、中国人保、太平洋保险、交通银行、奇瑞汽车、中国一汽、大众汽车、江汽集团、海尔集团、美的集团等多领域头部企业的首选。</p><p></p><p>讯飞星火已经在代码、合规审查、客服、评标、智能交互等多个典型场景产生应用成效，以交通银行为例，基于星火大模型能力的产品iFlyCode覆盖6000+研发人员，代码采纳率达38%，工作效率显著提升。</p><p></p><p>如何更好地解决企业大模型应用的最后一公里问题？刘庆峰谈到，企业首先要科学地认识大模型能力的边界，根据任务难度选择合适方案，并且用更少的算力、更高的效率，打造企业专属大模型。随着星火V4.0的发布，他认为用智能体平台打造每个岗位的专属助手的时间已经到了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/30/3032b1d57a923a504694e4282fbda152.jpeg" /></p><p></p><p>现场星火企业智能体平台正式发布。围绕搭建智能体的三大关键能力，当前企业智能体平台已覆盖400+AI原子能力，集成90+外部信源，打通100+内部IT系统，可供企业结合业务场景快速构建可落地的智能体应用。平台还围绕生产域、科创域、办公域、管理域上线32个企业智能体，供企业即插即用。</p><p></p><p>基于企业智能体平台，科大讯飞打造了星火商机助手、星火评标助手等典型应用案例，为企业应用打了个样。</p><p></p><p>在代码智能体iFlyCode中，它集成了代码生成助手、架构设计助手、代码问答助手、测试助手、数据库优化助手、代码审核助手等六大场景智能体，将采纳率由30%提升至52%，大幅度提升企业智能体的实用性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b5/b584b8ca75d110a4adb6772d85f40e7f.jpeg" /></p><p></p><p>星火商机助手可以实现商机线索应知尽知、客户拜访提质增效、销售管理智能研判，助力一线销售和商机管理效能提升。星火评标助手通过标前寻源、智能评标、定标审核等功能，智能评标结果人机一致率达98%，投标异常检出率超过80%，在大幅提升企业评标效率同时降低采购成本。</p><p></p><p>星火开发者生态加速增长：5个月开发者增长超100万，总开发者数破700万</p><p></p><p><img src="https://static001.geekbang.org/infoq/60/604ebae63bbd8eae1df5d07e21bfb553.jpeg" /></p><p></p><p>讯飞星火大模型带来行业赋能的同时，也在助力开发者生态蓬勃发展。自今年1月30日讯飞星火V3.5发布以来，短短5个月，星火开发者生态加速增长，开发者数从598万增长到702万，新增超104万；海外开发者数超40万；大模型开发者达57万。越来越多开发者正加入星火生态，释放更多刚需场景的应用价值。</p><p></p><p>刘庆峰说，只有自主可控的繁荣生态，才有中国通用人工智能的大未来。面向未来的人工智能新生态，他强调要关注源头技术生态、智能体生态、应用生态和行业生态，实现自主可控和软硬一体，才能实现大模型的深度落地；既要科学理性地认识中美在大模型上的综合差距，也要有信心快速追赶，给出从源头技术、到产业生态、再到应用落地的一整套的打法，以长期主义来打造真正自主可控的AI产业生态。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/w155KHeD4mR860s7cPEz</id>
            <title>从AI高管到犀利CEO，贾扬清创业这一年：我们的目标是做AI时代的“第一朵云”</title>
            <link>https://www.infoq.cn/article/w155KHeD4mR860s7cPEz</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/w155KHeD4mR860s7cPEz</guid>
            <pubDate></pubDate>
            <updated>Thu, 27 Jun 2024 10:22:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI Cloud, 大模型, Lepton AI, AI infra
<br>
<br>
总结: 贾扬清带领团队创立Lepton AI，旨在成为AI Cloud领域的领导者，专注于提供大模型训练、部署和应用所需的基础设施，解决AI infra层的核心问题，致力于提供高性能、便宜和优质的服务。通过整合各种云资源供应链，Lepton AI帮助用户找到速度和成本之间的平衡点，致力于构建新的AI时代基础设施。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>出品｜InfoQ 《大模型领航者》访谈主持｜霍太稳，极客邦科技创始人兼 CEO访谈嘉宾｜贾扬清，Lepton AI 联合创始人兼 CEO作者｜褚杏娟</blockquote><p></p><p></p><p>“我们的目标是在未来几年里，成为 AI Cloud 领域的领导者，就像最初的 AWS、Azure、Google Cloud、阿里云，以及 Data Cloud 领域的 Databricks 和 Snowflake。”这是在离开阿里创业一年多后，<a href="https://aicon.infoq.cn/2024/shanghai/speaker/8850">贾扬清</a>"现阶段的目标。</p><p></p><p>虽然贾扬清此前否认了这次创业是因为大模型，但他承认，ChatGPT 的问世，让 AI 成为一种更新、更大，且在某种程度上更加基础的计算方式，而这也是他看中的机会。</p><p></p><p>“当前人工智能对计算的需求，推动了高性能 AI 计算、异构计算及现代云原生软件的结合。”这是贾扬清选择创业赛道的逻辑。在他看来，新的平台需要有 GPU <a href="https://aicon.infoq.cn/2024/shanghai/track/1703">高性能计算</a>"能力，也需要很强的云服务，而这些都是贾扬清团队擅长的事情。</p><p></p><p>贾扬清的经历带有明显的云和 AI 印记：自己创建过深度学习框架，又曾负责过阿里云计算平台。而和他一起创业的李响是 Kubernetes 底层核心数据库 etcd 的创始人，白俊杰是神经网络标准 ONNX 创始人，并在阿里领导过全栈 AI 工程团队。</p><p></p><p>“在这个领域，我们最大的优势是‘见过猪跑和养过猪’。”贾扬清说道。</p><p></p><h3>创业的杀手锏</h3><p></p><p></p><p></p><blockquote>“Lepton AI 是一个颇具意义的名字。”</blockquote><p></p><p></p><p>贾扬清选择创业的时间，正好处于大模型逐渐标准化、但人才相对不足的节点。</p><p></p><p>在贾扬清看来，大模型会走与数据库相似的路：标准化的底层 SQL 执行不再需要用户自己优化了，同时数据库 SQL 之上会创建各种 BI 工具。对应的，随着大模型逐渐将形态收敛到几个相对确定的设计模式上，像 Llama、Mixtral、DeepSeek，系统性优化不再像之前那样困难。同样，这个领域也会出现更偏应用的 AI 中间件。</p><p></p><p>但与成熟的数据库路径不同的是，当下 AI 领域可能过早地将编程模式标准化了。毕竟一些概念现在不确定、也没共识，讨论如何为实现这些算法进行抽象显得操之过急。</p><p></p><p>贾扬清以观察到的 Lang Chain 为例说明了这个问题。用户在实验阶段非常喜欢用 Lang Chain，但做定制化功能时就会自己去写代码，因为 Lang Chain 的设计有时过于固化，而自己写代码涉及的 prompt cash 等并不复杂。这意味着，AI 中间件的产品在早期获得关注度以后，还需要进一步随着需求进化。</p><p></p><p>回到 AI infra 赛道里，贾扬清把 Lepton AI 定义为一个既小巧又敏捷的公司，运作方式上也更偏“硅谷风”：跟硅谷的许多创业公司一样，团队规模不大。目前，Lepton AI 不到 20 人，主要由工程师和产品经理组成。</p><p></p><p>创业初期，贾扬清几个创始人见投资人时，PPT 上还没有具体的公司名字，而是写着“new company”，贾扬清笑道，“感觉我们似乎不够认真”。于是他们决定取一个与 AI 相关的名字，但那时与 AI 相关的名字早都被注册了，所以他们开始想其他可用且听起来比较专业、有科学范儿的名字。</p><p></p><p>“Lepton”本意是物理学中的轻子，一种基本粒子。电子是一种轻子，中微子是一种轻子。“这体现了我们想要做的事情：首先是为 AI 时代构建新的基础设施，比如吴恩达教授说过，AI is the new electricity；其次是以一种轻量级、用户友好且成本低廉的方式构建。”贾扬清还直接道，“另外，lepton.ai 的域名也正好没有注册，所以我们就把它买下来了。”</p><p></p><p>“我们与大模型公司在一定程度上是互补的。”Lepton AI 没有训练自己的大模型，而是提供大模型训练、部署和应用时所需的基础设施。贾扬清团队要解决 AI infra 层的三个核心问题：快速、便宜和优质。</p><p></p><p>具体来说，提供高性能的<a href="https://aicon.infoq.cn/2024/shanghai/track/1703">大语言模型推理</a>"引擎，用于图片、视频生成；其次，为了实现成本效益，建立一个多云平台，整合各种云资源供应链，让用户找到性价比最高的 GPU 资源；同时，解决平台上不同云之间迁移和抽象化的成本问题，让用户可以自由交互开发；最后，提供很好的稳定性和运维服务，保障用户体验。</p><p></p><p>从技术层面如何实现呢？贾扬清表示团队没用什么“黑科技”，而是将很多大家耳熟能详的单点技术结合起来，从而显著降低成本。例如，大模型处理服务收到大量请求时的动态批处理（Dynamic Batch）、用小模型预测数个 token 的预测解码等。如何实现这些单点技术，并有机地将它们组合起来，找到速度和成本之间的平衡，是工作中的难点。</p><p></p><p>而在 AI 加速这件事上，贾扬清说道，“我比 10 年前更加乐观。”</p><p></p><p>实际上，硬件和软件之间的开发周期很难对齐，加上模型的多样性，专有硬件的可编程性通常比通用硬件要差，因此专用硬件很难全面支持各种模型创新的需求。而随着大语言模型架构开始标准化，异构芯片迎来新的机遇，这个市场也被激活：最大玩家英伟达布局 GPU；AMD 推出了与 CUDA 兼容的加速器；Grok 发布新的 AI 处理器 LPU 来挑战 GPU……</p><p></p><p>当前企业进行大量模型训练时通常有两种选择：一是自己组建一个至少 10 人的基础设施团队；二是选择 AI 云服务提供商，如 Lepton AI。</p><p></p><p>贾扬清算了这样一笔账：招聘人员并建立内部架构需要时间，而训练任务涉及许多复杂问题，比如网络中断、GPU 故障或存储速度不足等，算法工程师可能都没有处理这些问题的经验，并且这也不是他们的主要职责。这种情况下，选择与厂商合作既可以节省人力成本，用户也能根据自己的需求直接选择不同的底层资源。</p><p></p><p>“很多用户都在速度和成本之间寻求平衡，而我们可以帮助他们找到这个平衡点。”贾扬清说道。</p><p></p><p></p><h3>怎么讲好故事</h3><p></p><p></p><p></p><blockquote>“我们没有在这些产品上施加营收压力，可以专注提供卓越的用户体验。”</blockquote><p></p><p></p><p>很多人觉得做 AI infra 没有做大模型性感，贾扬清这群人却义无反顾投入到了这件“无聊但至关重要”的事情上。只是，做 infra 是很难向用户讲好故事的。</p><p></p><p>贾扬清也深知这一点，“如果跟客户说我们是一家 AI 基础设施公司，他们可能不太了解具体是什么。”</p><p></p><p>为了更好地讲清楚自己在做的事情，贾扬清和团队亲自下场了。</p><p></p><p>去年春节，贾扬清和公司前端负责人开发了一个浏览器插件 Elmo Chat，可以为用户迅速总结浏览网页的主要内容。今年 1 月，贾扬清团队用 500 行 Python 代码实现了一个大模型加持的对话式搜索引擎 Lepton Search，甚至还引起了一场与 Perplexity 创始人的“口水战”。</p><p></p><p>这些都是 Lepton AI 了解市场和用户、做品牌建设的方式，同时也让团队更加了解了端到端构建应用时的效率问题和核心痛点。</p><p></p><p>“通过这些产品，我们可以展示自己在开源模型上能做的事情，以及 Lepton 平台帮助用户构建应用的能力。”贾扬清说道。团队希望通过这些产品或 demo 可以在用户中形成好的口碑，当有人需要部署大模型时就会想到 Lepton AI。</p><p></p><p>而对 Lepton AI 来说，构建这些产品的成本非常低，并且从品牌建设和做真正实用的产品角度看，这是非常高效的方式。</p><p></p><p>在推向市场的过程中，贾扬清主要关注产品质量，他会同工程师和产品经理团队直接面向客户，更好地打磨产品。</p><p></p><p>目前，Lepton AI 整个团队主要在海外，所以目标客户主要为海外企业和国内想要拓展海外市场的企业。得益于云架构的成熟和标准化，Lepton AI 支持在全球范围内部署，与各个云服务无缝衔接的同时，还能很好地利用全球的计算资源。</p><p></p><p>产品驱动增长的策略也让 Lepton AI 主动放弃了一些潜在客户，特别是需要大量定制化服务的客户。“B 端的部分客户会希望提供商有更多人力投入到自己的项目中，但我们根据目前的成长阶段，会优先考虑与自己产品契合的客户。这是一种主动的选择，我们没有选择那些需要大量人力投入的业务和客户。”贾扬清说道。</p><p></p><p>不过贾扬清透露，Lepton AI 目前的客户数量和整体营收都处于非常健康的增长状态，他对此也比较满意，“这验证了我们之前的想法和产品在用户中的接受度。”</p><p></p><p>最近，贾扬清团队发布了基于 Lepton 平台的云 GPU 解决方案 FastGPU，主打经济高效和可靠，“限时以每小时 0.65 美元的价格提供 RTX 4090 GPU”。发布后就有人给贾扬清留言：缺货。</p><p></p><p></p><h3>一个更好的 CEO</h3><p></p><p></p><p></p><blockquote>“还在学习做一个更优秀的 CEO。”</blockquote><p></p><p></p><p>创业期间，贾扬清越来越乐于向公众表达自己的态度，或者也可以说释放自己的个性。</p><p></p><p>“在阿里时，同事对我的评价就是直率。技术领域通常没有太多花哨的技巧。因此，我倾向于以事实驱动的方式来表达观点，但这样可能会显得比较直接。”贾扬清说道，“毕竟我们现在还是一家小公司，会更加无所顾忌一些。”</p><p></p><p>他不认为这种直率的表达有什么不好。“在大公司，我们不想给人一种过于冲动或折腾的形象。但在小公司，直白地表达有时并不是坏事，甚至有时为了吸引注意也是必要的。”</p><p></p><p>就像他会承认说“所有基准测试都是错的”这样的话比较激进和吸引注意，但实际上这是他对榜单变成市场宣传工具的不满。“我更希望榜单成为一个所谓的入门资格认证，而不是奥运会金牌。”</p><p></p><p>虽然顶着各种光环，但创业初期也难免会被质疑。</p><p></p><p>有人会怀疑小公司的服务是否可靠和稳定。贾扬清分享了个小案例，有客户使用 Lepton 提供的推理服务，推理服务流量下降时系统发出了警报，团队检查后发现是客户侧代码的问题，提醒客户后他们非常满意。对于创业团队来说，“行不行”得看实际表现。</p><p></p><p>任何人的创业都不是一帆风顺的，贾扬清逐渐学会了在各种不确定过程中，做出一些重大决策。</p><p></p><p>就像在去年下半年全球 GPU 供应非常紧张，公司面临的选择是：要么囤积 GPU 进行交易，要么利用这段时间专注提高产品的成熟度来支持现有客户，并寻找那些拥有 GPU 资源但需要更高效平台和引擎的客户。</p><p></p><p>贾扬清选择了后者，而随着 GPU 供应情况的好转，也证明了他当时的判断是正确的。但事后看来，这仍然是一个相对冒险的选择，如果市场持续是卖方市场，公司就会陷入困境。</p><p></p><p>“作为一个创业公司，我们经常面临不确定性，还要在当时做出决策。一方面，我们需要不断观察市场，另一方面则要坚决执行决策，这也是提升我们自身效率的关键点。”贾扬清说道。</p><p></p><p>在一次次决策的制定中，贾扬清正在逐渐适应并把自己 CEO 的角色扮演得很好。</p><p></p><p></p><h3>多样的硅谷、同质的国内</h3><p></p><p></p><p></p><blockquote>“人们总是能找到证明自己是第一的方法。”</blockquote><p></p><p></p><p>国外创业一年多，贾扬清也亲身经历了硅谷对大模型的狂热。“硅谷现在竞争依然激烈。”贾扬清说道。</p><p></p><p>根据贾扬清观察，硅谷的企业和研究者现在主要关注两件事情：</p><p></p><p>一是如何实现产品的实际落地。尽管许多演示案例还停留在卖概念的阶段，但现在整个供应链已经非常充足，接下来的关键就是如何让技术真正被用户采用，尤其企业服务领域还有很长的路要走。</p><p></p><p>二是对基础模型的研究。越来越多的人开始思考 Transformer 和 Scaling Law 的边界在哪里。GPT-5 尚未发布，大家不确定 Transformer 架构的天花板是否到来，但已经有人探索 Transformer 之外的路径和方法，如 RWKV（Receptance Weighted Key Value）和 Mamba。同时，也有许多人尝试通过数据工程和强化学习等手段，从 Transformer 架构中挖掘更多潜力。</p><p></p><p>“硅谷的情况我觉得比较有趣。”在贾扬清看来，硅谷的产品更加多元化。“硅谷并没有很多公司做类似 ChatGPT 接口的事情，而是在寻找不同垂直领域的方向。比如做 AI 搜索的 Perplexity，其场景和产品形态就有明显不同。</p><p></p><p>贾扬清也不掩对 OpenAI 的称赞，“我认为 OpenAI 是一个非常成功的公司，无论是 To C 还是 To B 市场，它在营业数据和用户心智方面都做得非常出色。”据他了解，OpenAI 已经实现了产品市场契合，尽管仍在大量投资研发，但其在产品化方面实际上是盈利的，例如企业服务和 ChatGPT 等。</p><p></p><p>“这为整个行业带来了信心，表明这不仅仅是一个烧钱的游戏，而是真的能够赚到钱。现在，就要看各家公司是否能够成功走出一条商业化的道路。”贾扬清说道。</p><p></p><p></p><h4>国内：好坏参半</h4><p></p><p></p><p>对于国内的大模型市场，贾扬清直接指出，“我们可能还处于一个相对追赶的水平。”</p><p></p><p>“国内的 AI 领域涌现出来很多非常优秀的公司，同时大家也在受到产品同质化的困扰。例如以聊天为中心的产品，用户可能会感到眼花缭乱，因为这些产品看起来太相似了，包括我自己在内，都不知道如何选择，因为它们看起来都差不多，而用户一般也不想一个个去尝试。”</p><p></p><p>不过，贾扬清也肯定了国内的大模型企业在数据和用户量方面的重要优势。</p><p></p><p>“在国内，我们有时开玩笑说 100 万用户不算什么，但实际上这已经是一个非常庞大的用户基数了。”贾扬清说道。另外，国内用户也非常愿意尝试新事物，这为企业提供了快速验证产品有效性和市场接受度的机会。</p><p></p><p>贾扬清对国内的期待是，能有更多的产品经理投入到 AI 领域，思考新的产品形态或研究如何将 AI 更好地嵌入到现有产品形态中。他表示，现在许多应用并非由 AI 专家开发，而是产品经理推动的，而未来产品是否好用会逐渐成为用户采纳的决定性因素。</p><p></p><p>如今许多技术团队都面临着如何有效应用 AI 技术的问题，即使 ChatGPT 也无法精准解决实际业务问题，因此找到合适的业务场景并不容易，To C 的文生图、聊天机器人等已经成熟，To B 领域却还处于不甚清晰的状态。</p><p></p><p>对此贾扬清给出的建议是，企业自己的工程师要能轻松使用 AI 技术，同时积极与不同领域的工程师讨论各种概念及 AI 的能力边界。“与其花费大量精力去训练一个更强大的模型，更重要的是培养自己的团队，使他们能够快速理解 AI 的边界，并将这些边界与自身的应用需求有效结合。”</p><p></p><p>“应用大模型的企业最关心的不是价格，而是 AI 能否真正解决问题。”贾扬清提到，从这个角度看，国内愈演愈烈的大模型价格战意义并不大。不过，大模型价格下降也是趋势。“IT 领域的价格指数级下降是一个持续性的规律，我不认为大模型会是不受限制的一个领域。”</p><p></p><p>但小公司往往无法承受价格战和补贴战的资金消耗，为此，贾扬清建议小公司去精准支持和服务企业，尤其满足垂直领域的需求，这样也会找到自己独特的发展空间。</p><p></p><p>国内大模型还面临一个问题，就是常常被质疑套壳。贾扬清认为，一定程度上，使用标准架构或所谓的“套壳”并不是问题，比如大家都使用 AlexNet 或何恺明的 ResNet。他更关注的是，国内开发者如何提高自己的工程实践能力，并与海外开发者和开源社区更紧密地合作。</p><p></p><p>结合自己之前参与开源社区的经历，贾扬清认为国内大模型社区还需要提高协作等能力。“国内开发者独立工作时表现得都很好，如果能更好地协作，就能创建更大、更健康的开源生态系统和社区。”</p><p></p><p>实际上在今年 3 月份，贾扬清认为“<a href="https://www.infoq.cn/article/eVugB4V9E9cEsqaEJ27O">开源</a>"模型能迅速追上闭源模型”。现在他也认同，谷歌和百度等公司的闭源模型某些方面是会领先开源模型的，比如在全网搜索等某些企业本就擅长的领域，还有在超大规模的通用模型领域，因为这样的模型需要强大的资金和人才支持。</p><p></p><p>尽管如此，贾扬清直言自己更支持开源模型。一方面，开源可以促进更多研究，有助于找到更新、更有用的模型。另一方面，由于具有更好的定制化能力和企业自有知识产权的优势，开源模型有望迅速赶上甚至在某些方面超过闭源模型。他也提到，“有 Meta 这样资金雄厚、专业能力强的公司支持开源社区，是非常幸运的。”</p><p></p><p>对于 Lepton AI 自身也会从开源中受益的事实，贾扬清也直言不讳：“如果一切都是闭源的，我们就只能支持少数几个闭源公司，市场就不会那么大。我们希望市场能够更加多元化，越多元化越好。从市场经济原理看，竞争能够提升质量。”</p><p></p><p></p><h3>结束语</h3><p></p><p></p><p>“这一波 AI 浪潮的持续时间超出了我们所有人的预期。”贾扬清说道，“尽管 ChatGPT 会出一些问题，但我现在遇到问题或需要写文案时，还是会先让它帮忙处理。因此，我不认为 AI 行业会出现寒冬。对于一些投入明显超出收益的行为，市场会自我修正。”</p><p></p><p>在贾扬清看来，未来大模型发展的关键还是要回到商业成功上面。今天，人们已经不再怀疑 AI 是否有用，而是纠结如何让它实际产生商业价值。</p><p></p><p>但目前，AI 产品和服务还没有真正跨出自己的圈子。“当前 AI 市场，某种程度上是 AI 圈内人在自我消化需求，非 AI 用户消费 AI 服务的量还待起步。”贾扬清表示，未来 AI 技术能够被那些对 AI 一无所知的人以某种方式使用，这是它产生更大商业价值的先决条件。</p><p></p><p>现在，留给创业公司试错的空间并不多，但留给行业的问题还很多。AI 行业如何更长远地走下去，是包括贾扬清在内的每个参与者需要回答的问题。</p><p></p><p>活动推荐：</p><p></p><p>由贾扬清担任联席主席的&nbsp;<a href="https://aicon.infoq.cn/2024/shanghai/track">AICon 全球人工智能开发与应用大会</a>"将于 8 月 18 日至 19 日在上海举办，汇聚顶尖企业专家，深入端侧AI、大模型训练、安全实践、RAG应用、多模态创新等前沿话题。现在大会已开始正式报名，6 月 30&nbsp;日前可以享受&nbsp;8&nbsp;折优惠，单张门票节省 960&nbsp;元（原价 4800&nbsp;元），详情可联系票务经理 13269078023 咨询。</p><p></p><p><img src="https://static001.geekbang.org/infoq/61/6165c4a9600dcb871bf075f7c0ed5d60.webp" /></p><p></p><p>栏目介绍：</p><p></p><p>《大模型领航者》是 InfoQ 推出的一档聚焦大模型领域的访谈栏目，通过深度对话大模型典范企业的创始人、技术负责人等，为大家呈现最新、最前沿的行业动态和思考，以便更好地参与到大模型研发和落地之中。我们也希望通过传播大模型领域先进的实践和思想理念，帮助潜在大模型应用者、创业者、开发者等做好各类决策和选型。</p><p></p><p>如果您有意向报名参与栏目或想了解更多信息，可以联系：T_demo（微信，请注明来意）</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/S8RBeCZOXA0HZFxa4Phb</id>
            <title>一群顶尖搜索人才如何2个月出货，还把GPU利用率干到60%！揭秘百川智能研发大模型这一年</title>
            <link>https://www.infoq.cn/article/S8RBeCZOXA0HZFxa4Phb</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/S8RBeCZOXA0HZFxa4Phb</guid>
            <pubDate></pubDate>
            <updated>Thu, 27 Jun 2024 10:17:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型之战, 系统性工程, 冷启动, 训练效率
<br>
<br>
总结: 去年的大模型之战强调快速入场、发布和迭代。百川智能团队在研发大模型时注重系统性工程，选择冷启动并关注训练效率。团队通过评估不同阶段的研发问题来提高整体研发效率，同时关注推理成本的提升。整个大模型研发过程是从经验科学到数据科学的转变，核心竞争力来自于对模型关键问题的定义。 </div>
                        <hr>
                    
                    <p></p><p>去年的大模型之战讲究一个“快”字：入场快、发布快、迭代快。</p><p></p><p>王小川在去年 4 月份宣布成立“百川智能”的两个月后，就迅速对外推出了 70 亿参数量的中英文预训练大模型 Baichuan 7B。一年多后的今天，百川智能已经将大模型迭代到了 Baichuan 4。</p><p></p><p>曾担任搜狗搜索研发总经理的陈炜鹏如今在百川智能负责模型研发，这对他来讲也是一次挑战。“搜索与模型研发有很大的不同，研发经验不一定能完全复刻，比如两者对数据的定义可能完全不一样。”</p><p></p><p>但陈炜鹏也表示，做搜索和大模型也有共性，就是它们都是系统性工程。“在大模型之前，被认为系统性工程的算法问题只有三个：搜索系统、推荐系统和广告系统。以前的搜索经验让我知道怎么样解决一个系统性的问题。”</p><p></p><p>实际上，百川智能的技术团队可以分为两部分：一部分是像陈炜鹏这样有很强系统性工程经验的人，他们做过复杂的项目，知道怎样把复杂的问题拆解成为子问题，然后做有效的科学管理；另一部分则是对语言模型本身有很好认知的研发人员。</p><p></p><p>“大模型的研发不是一个单点问题，而是一个系统问题。解决系统性问题，是我们团队的优势。”陈炜鹏说道。那百川智能（以下简称“百川”）具体是如何解答“大模型研发”这道题的呢？</p><p></p><p></p><h3>大模型冷启，分阶段评估</h3><p></p><p></p><p>回顾当初，OpenAI 的 GPT-3.5 在全球范围内爆火的时候， 国内对怎么做大模型还没有形成很好的共识。</p><p></p><p>基于 BloomZ、OPT(Open Pretrained Transformer)、Llama，还是自己设计模型结构，这其实是两种不同的大方向，不同公司的路径都不一样。百川要做的第一个决策也是要选择从零做起的冷启动，还是基于 Llama 训练的热启动。</p><p></p><p>这个选择其实对百川来说几乎不需要犹豫，答案肯定是要从头开始做起。</p><p></p><p>逻辑很简单：热启动可能遇到的诸如新数据与之前数据的配比、合并，中英文能力平衡等问题，虽然可以提升团队在数据、模型能力、训练技巧等方面的认知，但是并不能给团队带来价值更高的技术栈认知。对于百川这样的创业技术团队来说，只有自己跑通大模型的整个技术栈，掌握完整的 pipeline，才可以真正把技术沉淀下来。</p><p></p><p>冷启动是风险更高的选择，百川接下来就是要想办法把研发模型的风险降到最低。</p><p></p><p>对此，团队的想法是一个小模型的结果能不能映射到大模型上，先用小成本模型验证后再进行大量投入。比如对于数据的多样性、规模和质量哪个更重要的问题，团队就是在提出假设后先用小模型做了验证。</p><p></p><p>百川技术团队选定了某一模型结构后，做了小、中、大三种参数的模型，然后观察不同参数之间的表现是否存在线性关系，如果符合 scaling law，后续就可以用该结构继续做各种数据实验、训练框架调优等。现在看，这条路百川走通了。</p><p></p><p>为了提升整体研发的成功率，百川团队把不同阶段的研发问题转化成为了评估问题，具体来讲就是把整个模型的研发过程拆分成了不同的阶段，并在每个阶段完成后用对应的方式进行评估。</p><p></p><p>在陈炜鹏看来，拆分和评估做得好，意味着团队对整个问题的理解和定义会更好，可以给整体研发带来明确的方向和效率提升。</p><p></p><p>“如果不能给当下的研发任务进行有效评估，而是通过最后大模型的效果来证明，势必会导致整个研发链条非常长，难以及时将研发工作转化为有效认知，进而导致整个团队的认知迭代非常慢、效率非常低。”陈炜鹏解释道。</p><p></p><p>好的评价结果意味着团队掌握了这个认知。因此，百川花了很多精力去做模型能力评估。“只有知道怎么评估，才能知道要往哪走、要怎么做。”</p><p></p><p>在效果评价方面，行业里有各种各样的测评榜单。企业每次发布大模型时都会介绍自己的 Benchmark 结果、对齐结果。实际上，大模型评估也是一个动态发展、跟模型能力强关联的演变过程。</p><p></p><p>很多 Benchmark 只能阶段性地反映模型能力。去年大家关心 MMLU 这种更偏知识类的测评和侧重数学能力的 GSM8K；去年下半年至今，大模型评测更是深入到了指令跟随、工具调用、多步推理能力、逻辑自洽性，甚至是否具备时间理解能力等方面。</p><p></p><p>“我们研发期间是想跳出现在的公开评测，自己去定义指标和任务的。”陈炜鹏进一步说到，“参考外部评测主要是为了知道自己在业内大概什么水平，但更重要的是能定义自己的 Benchmark，能够自己定义评估标准代表了一个企业对大模型的理解和技术方向。”</p><p></p><p>早期，很多评测标准是由高校、头部企业定义的。比如 OpenAI 提出的 GSM8K，就与其对模型能力的定义和想象有关。当 OpenAI 公布自己的测评标准时，自己在内部已经跑通了一段时间，一定程度上这也意味着企业已经有了超越行业的认知。而头部企业对大模型能力的理解也会在业内达成共识。</p><p></p><p></p><h3>大模型训练，从效率到成本</h3><p></p><p></p><p>“整个大模型研发，其实是一个从经验科学到数据科学的过程。”陈炜鹏说道。</p><p></p><p>大模型训练上，业内比较依赖 Megatron-LM、DeepSpeed 等分布式训练框架，这方面大家是相似的。而不同公司大模型训练的的差异在于训练框架解决不了的训练效率、稳定性和容错等问题。</p><p></p><p>训练效率或者推理效率是一种支撑性的技术。提升训练效率主要是提升整个机器的利用率，业内已经做了很多工作，比如并行策略、调优等。</p><p></p><p>训练效率不同的公司千卡利用率是不同的，百川千卡集群的利用率目前在 60% 以上。而大模型里涉及到很多 pipeline 和不确定性，使用工具做好过程管理非常重要。当集群出现故障时，需要及时发现并恢复。诸如此类才是大模型厂商技术比拼的点。</p><p></p><p>当然一些工具很大程度上可以起到提效作用，但真正的核心竞争力来自于认知，认知的差异则来自厂商对整个模型关键问题的定义。</p><p></p><p>比如重点研究多模态的企业，就会重点研发语言能力与不同模态怎么做结合等。因为从语言模型走到多模态模型的不确定性是显著增加的，而整个行业对如何做统一建模并没有确定的答案，需要企业做大量的实验。</p><p></p><p>与此同时，这一年多以来，大模型训练的重点也在发生变化。</p><p></p><p>去年的时候，行业更关心训练效率，对于推理成本没有特别多关注。“我觉得，去年整个竞争并没有非常激烈，因为当时模型的效果是最大的障碍，这种情况下，大家并没有非常关注推理成本。”</p><p></p><p>到了今年，业内显然开始更加关注推理成本。核心的原因是当前的模型能力已经在很多场景中具备较好可用性。这种情况下，当大模型开始落地时，大家的焦点自然就会转移到成本上。</p><p></p><p>百川团队现在也在探索如何在相同的推理成本下提升模型能力上限。比如对齐阶段遇到的能力平衡问题，研发团队要做的是围绕不同的能力方向，训练好几个模型，然后再把多个模型整合成一个模型。在选择哪个模型回答问题上，百川没有使用粗暴投票的方式，因为这会显著增加推理成本。</p><p></p><p>整个大模型推理加速优化上，Infra 层很难有数量级的优化，这个可能性几乎不存在，所以很多优化都是算法层面的优化。在这些优化措施中，效果加速度最大的方式是在模型结构不变的情况下提升模型的能力上限，其次是改变模型结构，获得与之前差不多的效果，但成本比之前更低；最后则是算子层或框架层的优化。</p><p></p><p>这与之前机器学习成本优化方面的规律一样，算法提升带来的成本下降比工程层面的要更显著，但技术实现也更难。</p><p></p><p>提升模型本身的能力是降低推理成本效率最高的方式，比如以前用千亿的模型，可能未来百亿的模型就能得到千亿模型相同的效果。较小参数规模的模型能够媲美更大参数模型的原因在于对数据质量的提升，比如 1 篇文章能讲清楚 10 篇文章论述的事情，就是更高质量的数据。</p><p></p><p>大模型训练是基于现在看到的数据分布建模，而所有数据内容是我们对整个世界的投射，也可以说是对整个世界“打点”，打的这个“点”存在大量重复的内容，如果能够找到一种方式，用最少的数据把整个世界描述清楚，那效率一定是更高的。</p><p></p><p>现在采样数据还是用已有的知识描述整个世界，能用最小的篇幅把整个世界描述清楚，也是合成数据的价值之一。对于合成数据可能带来的数据噪声问题，陈炜鹏认为，数据存在噪声不一定是灾难性的，正确数据的规律性比错误数据的规律性更强，大模型能够学习到这个规律，所以存在一定的抗噪能力。</p><p></p><p>“核心的问题是现在的数据构建方式并没有产生新的智能。”陈炜鹏指出。大部分数据合成的工作，都是在让小模型更接近大模型。但是很少有人提出数据合成的方法能给大模型能力带来显著提升。</p><p></p><p>合成数据只是做到这种程度的话，只能是提效。只有构建的数据能够超越现在的质量、超越现在的分布，合成数据才有可能带来智能的进一步提升。不过，合成数据能不能创造更高的智能，如今还是一个比较开放的问题，虽然重要，但大家都没有找到通用解法。</p><p></p><p>“整个大模型的发展还挺有意思的，它既是一个 infra 问题，也是一个算法问题。”陈炜鹏说道。</p><p></p><p>行业之前取得的大的进展，本质上都是在工程上突破，而不是在算法上。很大程度上，当模型结构确定后，infra 层的价值可能比算法层的价值更大。</p><p></p><p>在 scale up 假设下，大模型越来越大，国内一些企业选择万卡互联，这对 infra 层面的挑战非常大。而像语言与多模态之间结合等没有达成高度共识的实现方式上，算法还有很大的探索空间。</p><p></p><p>对于大模型更高的算力要求，陈炜鹏是比较乐观的。“现在有三股力量在解决这个问题，一是芯片层，他们自身的动力是非常强的。另外就是在 infra 层和算法层，infra 层跟芯片层配合、算法层就是在模型结构里面做一些工作。”</p><p></p><p></p><h3>迭代的本质：智力、应用</h3><p></p><p></p><p>与百川一样，市面上其他模型也都进行了多次大版本迭代，但大家在发布的时候，还是围绕各种基本能力的提升，业内的人可能能够更好理解提升数据，但行业外的人对于代际的差异比较后知后觉。</p><p></p><p>陈炜鹏对此解释道，基座模型最关注是本身的智能水平，具体表现上没有特别多可差异化的点，真正产生代差的是模型之间的智力水平。</p><p></p><p>以 GPT-4o 为例，GPT-4o 比 GPT-4V 在应用层的想象空间打开了很多，但 GPT-4o 并没有被命名为 GPT-5，因为它们的智力水平某种程度上还在同一个水平。</p><p></p><p>对于热门的长窗口、推理优化等，陈炜鹏认为，这些只能带来短时间的差异化，在半年以上的周期里，这些差异都会抹平。“整个行业里，我觉得大家某种程度上把长文本窗口这个事情‘神话’了。”陈炜鹏提到，“在我的理解里，上下文窗口大家更多的工作是工程上的，算法层的突破非常有限。”</p><p></p><p>另外，大模型厂商在基座模型的迭代期间，其实也已经考虑到了未来自家大模型可能的应用方向。</p><p></p><p>“大家既要在智力水平上拉开差距，还要在应用上找到差异。这就是守正出奇的逻辑，‘守正’就是我能不能够在智力水平上跟别人产生代差，‘出奇’就是出于对技术成熟度和产品的判断，来决定我差异化的功能是哪些。”陈炜鹏表示。</p><p></p><p>陈炜鹏举了一个比较形象的例子。大家要制造一个 super man，首先要知道它要具备什么样的能力，然后从 AI 本质出发需要怎样的底层支持，类似有没有比现在 token predict 更超前的方式等非常本质的问题。</p><p></p><p>这之后，人们会考虑 super man 除了有一个非常强大的大脑外，还需要具备哪些能力。到了这一步，大家就会有各种各样的定义。实际上，这时大家已经转换到了另外一个视角，即应用层，从应用层获得各种对应的能力。</p><p></p><p>也就是说，相同的智力水平下能够做出什么样的产品，这与企业对应用的想象有关。比如企业重视长文本能力的应用就会在上下文窗口上投入更多。</p><p></p><p>因此，总的来看，很多大模型研发决策是 AGI 视角和应用视角交错下的产物，只是不同的公司在不同视角里的投入有所差别。</p><p></p><p>以百川为例，Baichuan 3 的定位虽然还是基座模型， 但在医疗方面做了加强。</p><p></p><p>一方面，百川团队发现模型训练过程中，语言能力、知识能力的提升是快收敛的，逻辑推理能力的提升也比较慢，且周期较长。而医疗是一个既包含知识，又包含复杂推理过程的场景，可以很好地衡量大模型能力。</p><p></p><p>另一方面，百川也很在意医疗场景里模型的表现，这个就与其对模型应用的想象有关系。模型是要面向应用的，大模型厂商认为哪些场景重要，就会希望模型这方面的能力达到业内领先，带来应用优势。</p><p></p><p>为此，百川增强了大模型在医疗这个垂直场景的能力。百川团队先是深入到这个领域里做行业理解，之后花了很多精力解决场景的数据构建和数据配比的问题。</p><p></p><p>但有一点是毋庸置疑的，就是未来信仰 scaling law 的大模型厂商，发布节奏可能不会像去年那么快了。</p><p></p><p>就像王小川说的，“如果想达到智能，从现在的路径来说我们必须 scale up ，但 scale up 不一定会带来智能。不管怎样，这个事情我们得做。”而随着模型规模的增加，整个计算的复杂性、所需的数据量、背后依赖的算力资源等都要有数量级增加，这无疑是会拉长研发周期的。</p><p></p><p>王小川在 Baichuan 4 的发布会上就表示，以后的发布不会再以月为单位，而是季度，要把时间放到长线做事情。</p><p></p><h3>结束语</h3><p></p><p></p><p>时代的浪潮终归会落到每个技术人身上，包括但不只是像陈炜鹏这样的大模型厂商里的技术负责人。</p><p></p><p>大模型时代，技术人才的画像发生了很大的变化。比如之前的产品经理对用户端的理解非常重要，但现在要做一款好的产品，就不能只关注用户端，还要对当前技术能力的边界、成熟阶段有较好的预判。</p><p></p><p>现在的大模型技术不像之前的技术那样成熟，历史的经验不一定能够非常好转化为生产力。一个人有很强的发现新问题、定义新问题、解决新问题是更重要的能力。因此，百川也会倾向招聘新人、年轻人，“因为我们本身就在做一个很新的事情、要解决新问题，所以很多过去的具体算法经验，在如今场景下并没有那么重要，研究能力才是最重要的。”</p><p></p><p>目前，百川中的技术人员占整个公司人数的 70%-80%，其中有经验丰富的前搜狗各个业务线最优秀的干将和其他知名科技公司核心 AI 人才，也有越来越多的研发新星。期待汇集了多样人才的百川未来为我们带来更多惊喜。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/pk7tgkcHeyR5GTmdXimg</id>
            <title>AI 激战进入下半场，“推理”还卷得动吗？</title>
            <link>https://www.infoq.cn/article/pk7tgkcHeyR5GTmdXimg</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/pk7tgkcHeyR5GTmdXimg</guid>
            <pubDate></pubDate>
            <updated>Thu, 27 Jun 2024 07:25:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI, 价格战, 云计算, 大模型
<br>
<br>
总结: 一场前所未有的价格战在AI领域打响，云厂商纷纷降价，推动了云计算技术的发展和大模型的普及。随着AI推理需求增加，云服务的规模经济效应凸显，GPU云服务器成为AI基础设施的关键。同时，云服务的创新也推动了IT基础架构的变革，加速了AI技术的落地和应用。 </div>
                        <hr>
                    
                    <p>不久前，一场前所未有的价格战在 <a href="https://www.infoq.cn/article/9RsNpcVNEZorwctMtfqy">AI</a>" 领域打响，其激烈程度堪比一场商业风暴。以阿里云、百度、腾讯为代表的头部厂商纷纷宣布大幅降价，引发了圈内巨大震动，其中阿里云的通义千问 GPT-4 级主力模型 Qwen-Long，其 API 输入价格从 0.02 元 / 千 tokens 直降至 0.0005 元 / 千 tokens，降幅高达 97%！</p><p></p><p>价格战愈演愈烈的原因有很多，但无论是什么原因，我们看到的都是，通用大模型崛起后的这场价格战，将云厂商的竞争推向高潮。从讲“服务故事”到血拼 tokens 价格，云厂商的价值在这场“降本”的变革中再次受到严峻审视。但聚焦技术本身，如果想要实现技术的可持续性发展，把握好技术革新与规模经济之间的关系才是真正的破局之法。</p><p></p><p>随着云计算技术的不断革新和规模效应的扩大，AI 服务成本显著降低，让更多企业和个人能够负担得起并采纳 AI 服务。同时，云计算飞轮的加速旋转也带来了极大丰富的计算资源，让 AI 模型能够更快、更准地完成训练和推理。</p><p></p><p>过去半年，美国湾区的推理已经迈入每秒生成千个 token 的大关，英伟达发布了号称“史上最强的 AI 芯片”，官方称推理性能提升了 30 倍；百度发布了文心大模型 4.0 的工具版，官方称该模型的推理性能提升了 105 倍，推理成本降到了原来的 1%；腾讯太极机器学习平台研发了 Angel-HCF 推理框架和 Angel-SNIP 压缩框架；Meta 公布了其定制 AI 芯片 MTIA 的最新版本，专门设计用于 AI 训练和推理工作，还在 AI 推理和规划方面进行了深入探索，逐渐接近通用人工智能（AGI）……显而易见，当大家“卷”完行业大模型的构建，比拼谁能拥有更多业务数据进行模型训练之后，“AI 推理”或成为新赛点。</p><p></p><p>根据 IDC 数据，随着人工智能进入大规模落地应用的关键时期，云端推理占算力的比例将逐步提升，“预计到 2026 年，推理占到 62.2%，训练占 37.8%。”这一预测进一步强调了 AI 推理在未来市场竞争中的核心地位。而高性能 AI 推理的背后是海量算力，这意味着 AI 基础设施将是未来市场竞争的基本盘。</p><p></p><p>据信通院发布的《新一代人工智能基础设施白皮书》数据显示，AI 领域的大模型参数量正在以惊人的速度增长，年均复合增长率达到 400%，算力需求的增长更是超出了摩尔定律的预测，达到了惊人的 15 万倍，对 AI 基础设施提出了前所未有的挑战。传统的 CPU、GPU 堆砌方案已经无法满足 AI 大模型的研发需求，加上企业对于 MaaS（大模型即服务）的需求日益增加，企业需要更高效、更灵活的基础设施来支撑 AI 应用的开发和部署。</p><p></p><p>可以说，新一代 AI 基础设施不仅要关注硬件设备的升级，更要注重软件、算法和数据服务的整合与优化，需要通过精细化的设计和重构，提升计算、存储、网络以及数据服务的性能，为 AI 应用提供更高效、更可靠的支持。</p><p></p><p></p><h2>一、云服务"规模经济"：AI 基础设施成本大降的终极利刃</h2><p></p><p></p><p>今年 3 月，开源平台 ClearML 发布的最新调研报告《2024 年 AI 基础设施规模现状：揭示未来前景、关键见解和商业基准》中显示，企业购买推理方案的关键因素是成本——为了解决 GPU 缺乏的问题，约 52% 的受访者在 2024 年积极寻找低本高效的 GPU 替代品用于推理，其中 20% 的受访者表示对低本高效的 GPU 替代品感兴趣，但还找不到替代品。这意味着，由于大多数企业尚未达到生成式 AI 的大规模生产，低本高效推理计算需求将呈现增长趋势。</p><p></p><p>在如此趋势下，越来越多的企业开始将 AI 推理迁移到按需付费的云端进行。</p><p></p><p>云计算服务市场是一个典型的“规模经济”。随着用户基数的扩大，云厂商可以通过大规模采购硬件、优化资源分配和提高运营效率来分摊固定成本，从而实现成本效益的最大化，这种成本优势让云厂商能够以更具竞争力的价格向市场提供服务。同时，规模经济效应还能加速技术创新和服务多样化，较大的用户基础为其带来了更多的数据和反馈，这有助于其更深入地理解客户需求，快速迭代产品，推出更符合市场需求的新服务和功能。</p><p></p><p>而在所有的云服务中，GPU 云服务器对 AI 基础设施建设的意义最为关键，它极大地提升了 AI 基础设施的处理能力。通过集成 GPU 云服务器，AI 基础设施能够更高效、更快速地完成训练和推理任务，从而加速 AI 项目的研发进展。这不仅能使企业抢占市场先机，还能在获得大量数据后进一步优化自身模型，积累更为丰富的数据库。</p><p></p><p>以阿里云 GPU 云服务器为例，其神龙架构支撑裸金属实例，实例内 GPU 实现全速 P2P 功能，集合通信能力提升 20%，在微调和多卡推理过程提升性能 6%。在支持包年包月和按量计费的两种低成本购买方式的情况下，阿里云 GPU 云服务器还提供了针对 AI 应用部署及优化的免费工具，实现面向训推场景的 GPU 性能优化，其在同等硬件条件下，LLM 大模型推理性能提升超 100%，LLM 大模型微调训练性能提升 50%。</p><p></p><p><img src="https://static001.geekbang.org/infoq/32/32377af5fdc678a4fbcc5e6f91cc2776.webp" /></p><p></p><p>去年一经上线就出圈爆火的 AI 应用“妙鸭相机”，随访问量的激增，对 GPU 服务器的算力需求激增至数千台规模。阿里云 GPU 云服务器为其提供了训推一体的解决方案，助其缩短 19% 的端到端微调时间，推理效率提升 100%。训练时间的减少，不仅意味着成本的降低，也意味着妙鸭 C 端客户更短的等待时间和更好的体验。</p><p></p><p></p><h2>二、云服务创新：AI 时代 IT 基础架构变革的雷霆引擎</h2><p></p><p></p><p>深度学习自 2012 年在 AI 领域确立其核心地位之后，尽管为应用带来了显著赋能，但很长一段时间里并未彻底改变应用研发范式。直至云服务的崛起，数字化基础设施的格局发生了根本性变化，计算、网络和存储的虚拟化使得算力成为基础服务，云原生架构的应用研发模式大幅提升了开发迭代效率。后来随着大模型技术的广泛应用，大模型以 AI 原生应用的形式深入多场景，并转化为一种通用的服务 MaaS，降低了 AI 技术的落地门槛。而作为基础设施的云服务，也在大模型发展的推动下，产生了云原生“AI 化”的转变，重塑了云计算产业格局。</p><p></p><p>这种转变不仅体现在 AI 技术作为服务（MaaS）的广泛应用上，更在基础设施层面推动了 GPU 云服务器的革命性转变。面对高速演进的 AI 技术对 GPU 资源提出的愈来愈高的要求，基于云原生“AI 化”的趋势，以确保资源能够按需分配、高效利用。当前，以容器为代表的云原生技术正在完成进一步创新，IT 系统需要更加模块化和灵活以适应 AI 应用的迭代和更新。</p><p></p><p>在 AI 应用研发场景中，当 GPU 云服务器被多个用户或应用共享时，特别是在资源需求不均或变化频繁的情况下，资源分配和调度可能不够灵活，导致 GPU 利用率低下。此时便可以使用类似于阿里云容器服务 Kubernetes 版 ACK 提供的云原生技术来解决问题。ACK 丰富的 GPU 集群弹性伸缩能力可以帮助企业灵活应对工作负载变化，根据资源使用情况，企业可以快速动态调整容器数量，数分钟内扩展至上千节点。容器所具备的环境隔离性保证了 AI 模型推理的稳定性和一致性，减少因环境差异导致的错误和冲突，可以加速模型的迭代和部署过程。</p><p></p><p>阿里云 ACK 提供“云原生 AI 套件”，企业可以充分利用云原生架构和技术，在 Kubernetes 容器平台上快速定制化构建 AI 生产系统，并为 AI/ML 应用和系统提供全栈优化。在实际 AI 推理场景下，基于标准 Kubernetes 提供的组件化能力，同时通过共享 GPU 方案，对比自建 GPU 集群算力利用率提升 100%。除此之外，通过数据加速 Fluid，AI 推理场景数据访问资源成本可以降低 10 倍左右。更值得一提的是，这套云原生 AI 套件自 6 月 6 日起全面免费，企业成本直接降为 0！</p><p></p><p><img src="https://static001.geekbang.org/infoq/f6/f6574b62e3c0e5ff0780d5bae63d592e.webp" /></p><p></p><p>除了云原生架构的迭代创新，数据作为 AI 技术的“食粮”，其存储架构也在发生变革。随着数据量的激增，传统的存储解决方案已经无法满足 AI 对于高吞吐量和低延迟访问的需求。因此，可以在单个全局命名空间中无限扩展到数十 PB 甚至更多、可以为 AI 工作负载提供理想的存储解决方案——对象存储技术被广泛应用并持续迭代。</p><p></p><p>在目前的 AI 推理场景中，大家常会遇到的问题是，模型推理需要拉取加载模型文件，在调试过程中还需要不断切换新的模型文件进行尝试，而且随着模型文件的不断增大，推理服务器拉取模型文件所需时间越来越长。</p><p></p><p>面对这个挑战，许多企业将阿里云对象存储 OSS 作为解决方案。对比传统存储，OSS 的吞吐能力超过 10Tbps，从 OSS 下载 270GB 模型文件用时降低至 21s，通过低延时高吞吐的方式快速把模型文件传输到容器节点，减少 GPU 等待时间，可大大提升推理效率。此外，阿里云 OSS 加速器在 AI 推理环节支持 SD、Transformers 等多种推理框架，性能最高可 burst 至 40GB/s。</p><p></p><p><img src="https://static001.geekbang.org/infoq/80/80a1ff2f0993dd54328b5b321ddd9d5f.webp" /></p><p></p><p>可以说，大模型的发展标志着 AI 技术进入了一个全新的阶段，它不仅仅是对以往 AI 技术迭代的延续，更是对底层 IT 基础设施和上层应用开发模式的一次深刻重构。云服务作为 IT 基础架构的核心部分，必须承担起引领创新变革的重任。</p><p></p><p></p><h2>三、生态协同：云计算与 AI 深度融合的超级加速器</h2><p></p><p></p><p>如今，大模型已经开始卷价格，对比云计算用了 16 年才开始卷价格，AI 市场厮杀的激烈程度不言而喻，甚至 AI 已经让卷到"很卷"的云计算变得“更卷”。</p><p></p><p>于此，云厂商不仅需要有强大的技术研发能力，更需要构建一个健康、活跃的生态，以实现资源的优化配置和价值的最大化，而创新就是云计算飞轮持续旋转的核心动力。AI 借助云计算的强大算力处理海量数据，实现智能化应用；云计算则为 AI 提供稳定的技术底座，促进技术再升级。两者形成的良好技术生态共同助力着全产业智能化发展，吸引着更多开发者、企业参与技术创新。</p><p></p><p>通过生态协同，云厂商能够与上下游企业共同产品和服务的持续创新；通过与合作伙伴的深度合作，实际业务场景下的需求正在驱动着云厂商技术迭代与创新。</p><p></p><p>这种繁荣的生态系统为<a href="https://xie.infoq.cn/article/73bcc133affb775ea3afa5138?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">阿里云</a>"带来了更多的创新服务和应用，从而铺建了其在行业里的领先地位。通过合作伙伴的支持，阿里云为客户提供更加丰富多样的云计算产品，其“先进、稳定、易用、高性价比”的优势也助力许多企业客户获得了业务成功。这个过程中，阿里云积累了丰富的市场经验，同时拥有了庞大的计算资源和海量数据，为 AI 大模型的研发提供了坚实的后盾，从而走在了大模型厂商前列。</p><p></p><p>阿里云在 AI 大模型研发与云计算领域的双重领先优势，让其在 AI 基础设施构建方面拥有了得天独厚的条件。不仅为 AI 基础设施的构建提供了坚实的基础，更在不断地将这一优势转化为实际的产品和服务。而且，阿里云非常清楚——除了技术具有前沿性外，如何将这些技术有效地应用到实际场景中以解决实际业务问题，同样至关重要。</p><p></p><p>于是，基于深厚的 AI 技术实力和深刻的市场洞察，阿里云正在持续为企业提供既领先又容易落地的 AI 基础设施解决方案。为了帮助企业和开发者在多达数百款云产品中，根据自身业务问题快速定位关键产品需求，阿里云还推出了明星云产品推荐计划“飞天星品”，（点击本文文末的"阅读原文"可查看飞天星品的页面详情）大家可以在“飞天星品”上解决云产品选型难、使用方式复杂、场景定位模糊等问题，轻松选到最好用、最高性价比、最适合自己的云产品。</p><p></p><p>不仅如此，今年 618 阿里云首度推出 5 亿算力补贴，并带来多项 200 余种热门云产品折上折活动，助力更多企业、创业者与开发者可以使用普惠算力，更好地上云创新。</p><p></p><p><img src="https://static001.geekbang.org/infoq/61/611f589d69158529457713d37dd9c866.webp" /></p><p>登录阿里云官网，获取算力补贴</p><p></p><p>展望未来，云计算和 AI 技术的融合将进一步加速，共同推动数字化转型的浪潮。云计算的飞轮已经加速旋转，它带来的不仅仅是成本的降低和效率的提升，更是业务模式的创新和生态的构建，AI 技术也因此将得到更加广泛的应用和普及。我们期待看到更多的企业能够利用阿里云产品和服务，实现业务的快速增长和创新发展，共同推动 AI 技术的更快发展。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/kw6Sq83j1hwhE46tFULN</id>
            <title>美的集团在“AI+”战略下的布局与最新实践进展</title>
            <link>https://www.infoq.cn/article/kw6Sq83j1hwhE46tFULN</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/kw6Sq83j1hwhE46tFULN</guid>
            <pubDate></pubDate>
            <updated>Thu, 27 Jun 2024 06:41:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 智能化感知技术, AI+, 美的中央研究院, 技术创新
<br>
<br>
总结: 美的中央研究院以“AI+”战略为核心，通过智能化感知技术与人工智能的深度融合，推动了多个行业的技术革新和应用变革。通过在“AI+ 工业机器人”、“AI+ 智能制造”、“AI+ 智能家居”、“AI+ 医疗影像”四个主要方向的布局，力图提升产品力和竞争力，服务于多元化经营的目标。通过技术创新，美的中央研究院致力于实现智能化解决方案，推动智能化感知技术在机器人、医疗及智能家居领域的应用研究和技术创新。 </div>
                        <hr>
                    
                    <p></p><p>受访嘉宾 | 奚伟 美的中央研究院智能技术与应用研究所所长</p><p></p><p>智能化感知技术与人工智能的深度融合，推动了众多行业的技术革新和应用变革。在这一背景下，<a href="https://www.infoq.cn/article/t9Of1Azbdkx18hD2rrZg?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">美的</a>"中央研究院以“AI+”战略为核心，通过在“AI+ 工业机器人”、“AI+ 智能制造”、“AI+ 智能家居”、“AI+ 医疗影像”四个主要方向的布局，力图通过技术创新，提升美的产品力和竞争力，服务于多元化经营的目标。</p><p></p><p>智能化感知技术是通过传感器和数据处理装置实现对环境、物体和人类行为的感知、识别和分析的技术手段。<a href="https://aicon.infoq.cn/2024/beijing/">AI </a>"则利用这些感知数据进行深度分析和决策，形成智能化的解决方案。</p><p></p><p>在美的中央研究院的“AI+”战略中，智能技术与应用研究所起到了至关重要的技术支撑作用。本文中，InfoQ 通过与美的中央研究院智能技术与应用研究所所长奚伟的交流，探讨了美的“AI+”战略布局及智能化感知技术应用的最新进展。</p><p></p><p>据介绍，美的通过开展新型传感器、<a href="https://aicon.infoq.cn/2024/beijing/track/1669">多模态</a>"智能感控技术，以及<a href="https://aicon.infoq.cn/2024/beijing/presentation/5791">具身智能</a>"大模型技术的研究，为下一代工业机器人，先进医疗影像设备，和未来家电等前沿方向提供关键核心感知部件支撑和差异化感知平台技术方案。这些技术不仅增强了产品的智能化水平，还为未来的应用场景奠定了坚实的基础。</p><p></p><p>同时，随着技术的快速发展，美的在推广和应用过程中也面临着诸多挑战，如技术的成本效益、市场竞争的压力以及人才的培养和团队的建设等。如何在这些挑战中找到平衡点，也是美的未来需要持续探索和解决的问题。</p><p></p><h2>爆发期下的智能化感知技术：美的如何布局“AI+”战略</h2><p></p><p></p><p>InfoQ：您能介绍一下美的中央研究院在“AI+”战略上的整体布局和目标吗？其中您所负责的“智能技术与应用”板块在其中是扮演什么样的角色？</p><p></p><p>奚伟：美的中央研究院目前主要在 “AI+ <a href="https://www.infoq.cn/article/Q468bQj8jL3HJIkHbfSJ?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">工业机器人</a>"”“AI+ <a href="https://www.infoq.cn/article/1fSLGpl1K3OYX6AgLZ34?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">智能制造</a>"”“AI+ 智能家居”“AI+ 医疗影像”四个主要方向展开布局。目标是通过 AI+ 的革命性技术创新实现美的产品力和竞争力的领先，服务美的多元化经营的目标。</p><p></p><p>智能技术与应用板块是美的“AI+”战略的重要技术支撑力量和新技术应用的先锋队。通过开展新型传感器、多模态智能感控技术，以及具身智能大模型技术的研究，为下一代工业机器人，先进医疗影像设备，和未来家电等前沿方向提供关键核心感知部件支撑和差异化感知平台技术方案。</p><p></p><p>比如，在关键传感器方面，我们研发了智能 3D 相机、一体化力觉传感器和一些精密编码器，以及毫米波雷达等一系列传感器，涵盖从空间感知到位置感知的各个领域。实现关键传感器，医用探测器核心部件自研自制。</p><p></p><p>在此基础上，我们重点研究核心算法，探索如何将这些传感器和探测器应用于<a href="https://aicon.infoq.cn/2024/beijing/presentation/5748">机器人</a>"和医疗设备中进行研发，推动前瞻性感知技术跨事业部布局及产业应用。我们的最终目标是在工业、医疗领域、以及未来家电领域智能化感知行业实现产品创新和行业领先。</p><p></p><p>InfoQ：在推动 AI+ 战略方面，您的日常工作主要关于哪些方面？</p><p></p><p>奚伟：我的工作主要包括整体“AI+”和“三个一代”（研究一代、储备一代、开发一代）技术战略的规划和布局，“AI+”在新业务领域的探索及国际化、高水平团队的组建及能力建设，带领团队在核心技术上攻关，实现技术突破，并协同事业部积极推进技术应用转化。</p><p></p><p>InfoQ：从整体进展来看，智能化感知技术在机器人、医疗及智能家居领域的应用研究及技术创新，目前是什么样的现状？</p><p></p><p>奚伟：智能化感知技术在机器人、医疗及智能家居领域的应用研究及技术创新目前处于一个新的爆发期。</p><p>整体趋势是从自动化、精准化向智能化转变，目前也是全球各大公司投入大量资源，竞争差异化的重点。</p><p></p><p>以机器人为例，结合视觉及多元感知的具身智能技术的出现，改变了传统通过编程实现自动化的过程，机器人的技能通过学习完成，高质量训练数据成为核心。医疗领域也是如此，利用深度学习技术可提高扫描速度，减少 CT 的辐射剂量，基础大模型技术可以辅助报告生成和智能化诊疗。“AI”技术极大促进了传统技术的升级。</p><p></p><h2>AI 和智能化感知技术的选择与发展策略</h2><p></p><p></p><p>InfoQ：在技术的研发和应用过程中，美的如何确定哪些 AI 技术和智能化感知技术值得投资和开发？选择技术的逻辑和标准是什么？</p><p></p><p>奚伟：我们主要从 AI 技术相对传统技术的差异化优势和价值，以及行业技术发展趋势来进行技术布局和投资。如通过引入 AI 技术可以扩展产品的应用领域，或提升产品的关键指标，这些都是明显判断技术价值的标准。再比如对于机器人，通过 AI 技术进行智能制造，可以减少工人的劳动强度并提高制造效率，也是价值的体现。</p><p></p><p>如果技术成熟度不太高，市场不明确，但未来有重大前景的 AI 技术，我们也会提前布局。</p><p></p><p>InfoQ：有没有出现在技术投入过程中，我们最初判断某项技术符合需求并能带来商业价值，但经过一段时间的投资和研发后，发现方向不对的情况？</p><p></p><p>奚伟：这种情况也是会有的，但正如我刚才提到的，美的拥有一个较好的研发体系，可以最大程度地减少这种情况的发生。我们采用“研究一代、储备一代、开发一代”的模式。</p><p></p><p>每个阶段投入的资源是不同的。在前沿研究方面我们会选择一些我们认为有潜力的技术进行尝试，当可行性得到验证后才会进一步进行结合业务场景进行更详细的论证。这些系统性的技术管理保证了技术决策的合理性及延续性。</p><p></p><p>InfoQ：能否举个例子说明这种情况？</p><p></p><p>奚伟：比如毫米波雷达，虽然这个例子可能不是特别典型，但它确实展示了我们在技术判断过程中的一些经验。</p><p></p><p>在早期阶段，毫米波雷达是我们非常看好的方向，因为它可以提升智能家居的智能化程度。我们在前期投入了一些资源进行底层技术研发和关键传感器的开发。然而，随着时间推移，我们发现市场上的竞争非常激烈，很多供应商也在开发类似的技术，导致差异化程度降低。</p><p></p><p>最终，由于市场竞争充分，毫米波雷达的价格变得非常低。在这种情况下，我们认为继续自研这项技术的商业价值不大。就会调整技术方向，选择投入更多资源到其他更具技术附加值的领域，特别是那些需要攻关的技术方向。</p><p></p><p>InfoQ：那在领导技术团队进行攻关时，您认为最关键的因素是什么？团队素质最重要的点有哪些？</p><p></p><p>奚伟：这个问题比较大，但有几个关键点。</p><p></p><p>首先是热情。尽管我们做的是前沿技术，但很多时候是在摸着石头过河，没有现成的方法可以参考。我们需要阅读大量文献，做许多实验，并不断试错，才能找到相对可靠的解决方案。这对团队的耐心是个巨大挑战。</p><p></p><p>其次是结果导向。在国内，结果导向非常重要。我们需要在研发过程中能够阶段性地输出一些成果，以保持团队的信心。让大家看到我们一步一步取得进展，这样能促使团队坚持下去。</p><p></p><p>另外在管理团队方面，我认为首先是能力要强，找到有能力的领军专家，通过“老带新”，进行有效率的技术攻关；其次有共同的目标，把自己的工作看成是对社会发展有影响的事业；日常管理上，“三个一代”战略牵引，工作目标牵引。</p><p></p><p>InfoQ：从结果导向来看，这对您来说有哪些挑战？特别是很多前沿技术需要很长时间才能转化为实际成果。</p><p></p><p>奚伟：确实如此，前沿技术从研发到成果转化往往需要很长时间。我们需要展示技术的潜力，让领导层看到未来的投资价值。如果一项技术经过一段时间的研发，发现对产品能力和用户价值的提升不大，投资的动力就会逐渐减弱。</p><p></p><p>比如说，IoT 技术曾被寄予厚望，希望实现万物互联，提升智能家居的用户体验。但由于技术标准不一致和用户价值体现不够明显，渐渐地很多企业对 IoT 的投资逐渐减少。这提醒我们在技术研发过程中，需要确保技术的潜力和价值被充分展示和认可。</p><p></p><h2>AI 驱动下的场景应用与产业创新</h2><p></p><p></p><p>InfoQ：在“AI+ 工业机器人”方面，智能技术与应用研究所取得了哪些主要成果？这些技术突破是如何支撑产业转型升级并推动制造数字化转型？</p><p></p><p>奚伟：美的承建了蓝橙全国重点实验室，其中 AI+ 工业机器人是实验室四大建设内容之一。</p><p></p><p>技术方面，首先在底层传感器方面，我们在包括 3D 相机、力传感器、编码器、激光雷达和毫米波雷达等传感器都取得了显著进展。其次，在技术平台方面，搭建了工业视觉技术平台、低代码编程平台、智能导航平台以及数据仿真平台等。这些平台不仅能够迅速实现技术落地，还支撑了产品的升级。第三，通过仿真平台，也加速了模型的迭代和开发。</p><p></p><p>结合应用场景来看，也可以举几个例子：</p><p>针对工业制造应用场景接近 100% 识别成功率的需求，美的研发了基于<a href="https://s.geekbang.org/search/c=0/k=%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/t=">视觉大模型</a>"的通用强泛化高精度识别定位的技术，应用于焊接，装配等 5 类典型制造场景，完成 100 余条产线规模化复制。针对重载移动 AMR 的高鲁棒精准定位需求，美的研发了多源异构传感器的多模态融合智能定位技术及 AI 安全感知技术，也已应用于库卡 KMP 系列潜伏式 AGV 超过 1000 台套。&nbsp;高速 3D 成像，应用于复杂场景下的动态制造，如涂胶和焊接等，通过计算机相机辅助智能制造。针对重载机器人要达到应用低代码编程目标，美的研发图形化编程软件，大大缩短了机器人部署时间。</p><p></p><p>目前工业机器人典型场景完成了验证并实现了批量化落地应用，覆盖 80% 重载应用场景。</p><p></p><p>另外，美的结合生成式 AI 大模型，大数据，物联网，云计算等数字技术创新，把数字技术与制造业深度融合，建立了 28 家国家级绿色工厂、3 家零碳工厂、5 家世界级灯塔工厂，入选国家级双跨平台。其中电子车间的无人化场景是“AI+ 制造”智能化发展的重要方向之一。</p><p></p><p>InfoQ：在技术推广过程中，主要会遇到哪些阻力？</p><p></p><p>奚伟：推广过程中主要的阻力有以下几个方面：</p><p></p><p>现有产线改造的复杂性：在新的产线上推广相对容易，因为可以整体规划。然而，在已有产线上的改造涉及到很多方面的工作，包括人员对新技术的不了解和需要学习新知识等问题。技术适应性和环境因素：传统制造依赖人的适应性，而机器在适应不同环境时要求更高。例如，环境的稳定性和光照等因素对机器的影响较大。很多技术在一个工厂研发完成后，扩展到其他产线并不像软件复制粘贴那么简单，需要进行很多微调和定制化。人员培训和接受度：培训人员接受并拥抱智能化技术是一个很大的挑战。很多员工对熟悉的流程依赖较强，对新的智能化技术不熟悉，可能会感到困难，不愿意使用。</p><p></p><p>针对人员培训，库卡也推出了新匠星计划，目标是培养更多适应数字化、AI 时代的高级技术人才 。</p><p></p><p>InfoQ：针对“AI+ 智能家居”方向，智能技术与应用研究所在家电智能化和机器人化方面有哪些创新？</p><p></p><p>奚伟：智能家居是我们研究所的新方向。我们研究所主要面向 To B 方向，但在 To C 方向，尤其是“AI+ 智能家居”方面，未来智能家居将出现两个发展方向：</p><p></p><p>&nbsp;第一，家电主动服务，识别用户意图，学习用户习惯，为用户提供服务。&nbsp;第二，家电机器人化，家电即机器人，比如说现有的扫地机器人，未来还会有更多的家电以机器人的形态出现。两个发展方向均需要突破 AI 技术。</p><p></p><p>目前，美的集团在语音语言、边端智能、AI 大模型等 AI 技术方向持续突破：</p><p></p><p>&nbsp;语音方面，打通了语音全链条上技术环节，已上线 5 个不同特色发音人并提供了稳定的 TTS 服务，低信噪比环境增强后语音识别率上升 15%，唯一唤醒成功率 90%。边端智能方面，持续对模型压缩和推理加速优化，实现语音模型压缩比&gt;7x, 推理时延降低 70%；视觉模型压缩比&gt;16x, 推理时延降低 75%；美的发布国内首个家居领域 AI 大模型“美言”，为智能家居构建了智能感知、自然交互和自主决策三个基础能力。在服务机器人方向，美的利用 AI 等技术研发服务机器人产品，美的集团获批建设“智能服务机器人国家新一代人工智能开放创新平台”。</p><p></p><p>在新的方向上，我们目前主要围绕着家电机器人化展开工作。我们现在有一些业务场景，大家相对接受度较高的是扫地机器人。除了扫地机器人，我们还在探索其他智能家居产品的可能性。</p><p></p><p>InfoQ：以扫地机器人为例，它在智能化方面，还有哪些潜在的想象空间？</p><p></p><p>奚伟：其实有几个方面可以考虑：</p><p>扫地能力提升：传统的扫地机器人只能进行基本的清扫工作。未来，我们希望它能智能识别不同的地面类型和污渍情况，进行更高效的清扫。这是一个重要的方向，我们在这方面也做了许多技术攻关。语音交互：目前，扫地机器人有时会出现无法找到的问题。通过改进语音交互功能，用户可以更方便地与机器人进行沟通，例如让它自动找到某个地方进行清扫，而不需要手动操作手机或遥控器。&nbsp;拓展清扫范围：现有的扫地机器人还无法触及某些地方。我们正在尝试让机器人清扫地脚线等区域，甚至结合拖洗一体机，扩展其功能和应用场景。</p><p></p><p>这些探索都可以进一步提升扫地机器人的智能化水平和用户体验。</p><p></p><p>InfoQ：现在大语言模型给许多行业带来了影响，无论是效率提升还是智能理解方面都有比较大的进展。能否再详细谈谈美的对美言大模型的期望和愿景？</p><p></p><p>奚伟：美言大模型的主要优势在于提升平台化的语义理解能力。传统的语音交互只能响应特定的关键词，而美言大模型可以理解用户的自然语言表达，从而提供更优质的服务。这对现有家电来说是质的飞跃。</p><p>通过结合美言大模型，我们可以将其应用于烤箱、微波炉等家电产品，提升它们在烹饪和清洁方面的智能化水平。模型的生态系统能积累大量用户数据，进而不断优化模型，增强产品功能和用户体验。通过这种持续的改进，产品的用户复购意愿也会提高。</p><p></p><p>核心在于，传统的 To C 产品在销售后基本不会再有大的升级变化，而借助大模型技术，产品将具备自学习能力。它们可以根据用户习惯调整操作方式，例如提供更节能的模式，或在烹饪过程中提供智能化提示。对于老人用户，系统可以在忘记关火或操作失误时主动提示和纠错。</p><p></p><p>总体而言，美言大模型将实现家电的主动服务和家电机器人化，不仅理解用户需求，还能进行智能操作。</p><p></p><p>InfoQ：在“AI + 医疗影像”领域，智能技术与应用研究所如何通过 AI 技术提升医疗资源的整合和优化？这些技术对医疗影像诊断和治疗流程有什么影响？</p><p></p><p>奚伟：在“AI+ 医疗影像”方面，人工智能将加快医疗领域的资源整合优化，用 AI 技术赋能诊疗一体化全流程，实现一体化精准诊疗新技术、及跨科室、多模态、数智化的诊疗新方案。</p><p></p><p>目前美的开发的昆仑 AI 智慧影像平台，通过将 AI 深度嵌入临床影像工作流程，改善扫描流程，提升成像质量，辅助医生进行诊断、治疗决策等，进而打通筛诊疗预随全临床流程。</p><p></p><p>同时覆盖多病种 AI 解决方案，精准高效触达更多临床场景。比如，AI 自动定位及自动扫描覆盖 60% 临床部位，效率提升 30%；AI 静音技术降低 86% 的噪声声压，改善医患检查舒适度；AI 去噪加速功能，实现 4 倍加速的同时图像信噪比提高 30%；妙笔 AI 质控软件，除了全天候自动检测影像报告外，更进一步提供报告质控应用，智能提醒医生报告书写错漏，正确识别准确率高达 93%，提高各级医疗机构医生报告质量，大大降低因书写错误造成的纠纷。</p><p></p><p>InfoQ：美的智能家居有一个美言大模型，那在<a href="https://aicon.infoq.cn/2024/beijing/presentation/5843">医疗领域</a>"，会不会也有一个垂直领域的大模型？</p><p></p><p>奚伟：是的，我们确实有这个计划，目前还在进行当中。我们正在组建团队，进行相关的布局。</p><p></p><p>InfoQ：医疗领域的大模型有应用于科研、药物研发等不同领域的，而美的会更侧重于智能诊断和设备方面？</p><p></p><p>奚伟：对，美的主要在智能诊断和设备方面进行布局。万东医疗有一个云平台，可以将医院产生的数据上传，帮助生成病理报告。此外，我们希望通过大模型技术在设备端自动生成术后的扫描报告，辅助医生进行后续诊断。我们在这些方面都在进行布局。</p><p></p><h2>智能化技术挑战与应对策略</h2><p></p><p></p><p>InfoQ：在推动数智化技术产业化落地的过程中，遇到过哪些主要挑战？您是如何应对这些挑战的？</p><p></p><p>奚伟：最大的挑战有两个，一个是技术的预期和现实的差距，一个是高质量数据资源不足。第一个问题最难解决，通过客户希望智能化技术能像人一样的灵活度，但实际情况是目前智能化技术仅能实现有限场景的智能化。随着 LLM 大模型技术和 AGI 技术的不断发展，我认为这一个差距会逐渐缩小。</p><p></p><p>对于数据稀缺问题我们积极采用数据仿真合成的方式进行补足。另外，高端前沿人才还较为缺乏，总体来说，AI 人才相对欠缺，传统人才更多一些。</p><p></p><p>InfoQ：根据您的经验，您认为哪些因素最能决定 AI 技术和智能化感知技术的成功落地？</p><p></p><p>奚伟：有几个主要因素。首先是技术应用的成熟度，这是最关键的因素。很多智能化技术无法落地，主要还是因为技术还不够成熟。第二个因素是找到产品价值、对于用户的价值。第三是新技术培育的土壤和成本控制，早期必须要有正向的收益来推动技术的不断迭代，但在中国的环境中，这很难实现。很多技术还没推广，就已经降到了“地板价”。</p><p></p><p>InfoQ：最后想问下在美的集团的“AI+”战略下， 智能技术与应用研究所下一步有哪些规划和目标？</p><p></p><p>奚伟：总体来说，我们会围绕美的产业布局不断深化扩展“AI+”场景应用，形成真正的新质生产力。另外还会探索未来新赛道、探索场景新应用。</p><p></p><p>也计划将更多传感器、毫米波激光雷达等技术植入机器，让工业机器人、医疗设备、智能家居拥有智能大脑和眼睛，从而提升用户体验。</p><p></p><h4>嘉宾介绍</h4><p></p><p>奚伟，高端重载机器人全国重点实验室副主任，国家级科技创新领军人才，美国马里兰大学帕克分校博士，本硕毕业于清华大学。目前担任美的中央研究院智能技术与应用研究所所长，主导集团内部多个数智化技术研发和产业化落地项目，推动集团在“AI+”战略上技术布局和创新。项目内容涉及 AI 工业视觉算法研发及低代码免编程技术在智能制造应用，多模态 SLAM 导航及视觉安全感知技术， AI 智能化医疗影像平台技术研发，未来智能家居场景下家电机器人化技术研究，下一代机器人及医疗设备关键传感技术研究等。</p><p></p><p></p><h4>活动推荐</h4><p></p><p><a href="https://aicon.infoq.cn/2024/beijing">AICon 全球人工智能开发与应用大会</a>"将于 8 月 18 日至 19 日在上海举办，汇聚顶尖企业专家，深入端侧AI、大模型训练、安全实践、RAG应用、多模态创新等前沿话题。现在大会已开始正式报名，6 月 30&nbsp;日前可以享受&nbsp;8&nbsp;折优惠，单张门票节省 960&nbsp;元（原价 4800&nbsp;元），详情可联系票务经理 13269078023 咨询。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f1/f1d06e1c7f30e0f58123c07a21cdc1de.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/L4JzNvIDLNutDqb8y656</id>
            <title>字节跳动发布“豆包MarsCode”智能开发工具，面向国内开发者免费</title>
            <link>https://www.infoq.cn/article/L4JzNvIDLNutDqb8y656</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/L4JzNvIDLNutDqb8y656</guid>
            <pubDate></pubDate>
            <updated>Thu, 27 Jun 2024 02:22:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 字节跳动, 豆包MarsCode, AI时代, 开发者工具
<br>
<br>
总结: 字节跳动发布了基于豆包大模型打造的智能开发工具豆包MarsCode，并面向国内开发者免费开放。豆包MarsCode团队更多地从如何赋能开发者的角度入手，希望打造一款软件，能够助力提升开发者工作效率，让开发者有更多精力和时间用于思考和创造。豆包MarsCode通过提供编程助手和Cloud IDE两种形态，以及各种功能如项目问答、代码补全、单测生成、Bug Fix等，帮助开发者更轻松、更专注地编程。 </div>
                        <hr>
                    
                    <p>6&nbsp;月&nbsp;26&nbsp;日，<a href="https://www.infoq.cn/profile/9D04D70F8614EE/publish?utm_term=geek_search_source&amp;utm_campaign=geek_search_source&amp;utm_source=geek_search_source&amp;utm_content=geek_search_source&amp;utm_medium=geek_search_source">字节跳动</a>"发布了基于豆包大模型打造的智能开发工具<a href="https://www.marscode.cn/home??utm_source=626&amp;utm_medium=wx">豆包MarsCode</a>"&nbsp;，并面向国内开发者免费开放。</p><p>&nbsp;</p><p>现场，字节跳动开发者服务团队、豆包MarsCode&nbsp;负责人李东江还分享了一些对&nbsp;AI&nbsp;时代开发工具演进趋势的思考。</p><p>&nbsp;</p><p>进入&nbsp;AI&nbsp;时代，大语言模型在编程语言方面具备强大的优势和潜力，相比起复杂的自然语言，编程语言是更加简洁、严谨、可预测。关于“应当如何构建一款&nbsp;AI&nbsp;时代的开发者工具”的命题，豆包MarsCode&nbsp;团队更多的从如何赋能开发者的角度入手。</p><p>&nbsp;</p><p>李东江认为，在&nbsp;AI&nbsp;技术驱动下，一定会衍生出下一代的开发工具。AI&nbsp;不是替代开发者的“竞争者”，而是开发者的“好帮手”，团队更希望打造一款软件，能够助力提升开发者工作效率，让开发者有更多精力和时间用于思考和创造。</p><p>&nbsp;</p><p></p><h2>豆包MarsCode&nbsp;首发功能揭秘</h2><p></p><p>随后，豆包MarsCode&nbsp;产品负责人王海建介绍了豆包MarsCode&nbsp;产品的两种形态：编程助手和&nbsp;Cloud&nbsp;IDE，同时通过需求开发、修复Bug、开源项目学习三个实际场景，详细演示了豆包MarsCode&nbsp;的项目问答、代码补全、单测生成、Bug&nbsp;Fix等功能。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/ef/efa0113b931686d6935da9c38e0ba5ed.png" /></p><p></p><p></p><p>需求开发场景</p><p>&nbsp;</p><p>通过一个翻译机器人构建的实际案例，王海建演示了在&nbsp;AI&nbsp;的辅助下，开发者可以如何通过唤起编程助手进行&nbsp;Chat&nbsp;提问，分析需求、熟悉代码、编写代码和调试代码。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/f6/be/f6c6905cb819015107d407ce0c162dbe.gif" /></p><p></p><p>代码补全不仅仅可以帮助开发者更快地输入代码，更是可以通过不断提供代码建议，给开发者带来灵感和启发。豆包MarsCode&nbsp;的创新功能——代码补全&nbsp;Pro，不同于传统的代码续写，它支持自动根据用户编辑意图预测下一个改动点并给出代码推荐。</p><p>&nbsp;</p><p><img src="https://static001.infoq.cn/resource/image/90/8d/90e4596ae75b6190362297bb27ee438d.gif" /></p><p></p><p>除了代码预测与补全，当编码中出现需要修复的代码&nbsp;Lint&nbsp;错误时，编程助手会直接在编辑器中主动给出修改代码，我们不需要去查看是什么报错原因，只需要判断修复结果是否正确，如果正确，一键采纳修复后的代码即可。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/17/e4/1701569e98a74f285e809589f24955e4.gif" /></p><p></p><p>最后，当我们写完代码，为了保障代码的质量与后续的可维护性，通常还需要写单元测试。这时只需要在编程助手中触发&nbsp;test，就可以得到这个函数的测试用例。可以看到，相比于传统的开发方式，豆包MarsCode&nbsp;编程助手可以帮助开发者更轻松、更专注地编程。</p><p>&nbsp;</p><p>Bug&nbsp;修复场景</p><p>在&nbsp;Debug&nbsp;场景下，豆包MarsCode&nbsp;的&nbsp;AI&nbsp;修复功能可通过理解报错信息、调用栈的代码、全局的项目代码，去分析错误原因，从而直接给出针对性的修复建议。</p><p>&nbsp;</p><p><img src="https://static001.infoq.cn/resource/image/90/4c/90fb104a71b5c2cf1bf7e31314e7184c.gif" /></p><p></p><p>除了单轮修复，豆包MarsCode&nbsp;也在尝试基于&nbsp;Agent&nbsp;方式实现多轮自动修复，AI&nbsp;会自主调用一系列代码查询工具、调试工具获取报错信息、自主规划方案、自主写出代码并应用到项目当中去，来修复&nbsp;Bug。目前，该功能正在字节内部做验证。</p><p>&nbsp;</p><p><img src="https://static001.infoq.cn/resource/image/e7/3d/e7574d6ddd357a3276ca112a4d23ef3d.gif" /></p><p></p><p></p><p>开源项目学习场景</p><p>第三个场景下，豆包&nbsp;MarsCode&nbsp;IDE&nbsp;通过提供开发模板，让开发者能够快速进入项目而无需运维本地环境。借助原生集成的&nbsp;AI&nbsp;能力，开发者不再需要自己去理解代码，从而更高效地上手项目。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/d9/d935bb85d4f1efd9c3bb0242416cb77a.gif" /></p><p></p><p></p><p>总的来说，豆包MarsCode在以下两个方面帮助开发者：</p><p>对于想的阶段，提供更好的信息，例如做代码解释，研发知识的问答，来激发开发者创造；对于做的阶段，帮助开发者更快地完成编码，例如代码的补全、下一步编码动作的预测，代码的错误修复，来提升开发者效率。</p><p>&nbsp;</p><p>后续，豆包MarsCode&nbsp;会通过成立用户组、各类系列开发者活动等方式，助力开发者探索&nbsp;AI&nbsp;编程新范式。现场，豆包MarsCode&nbsp;市场运营负责人赵旭东介绍了豆包MarsCode&nbsp;开发者与社区共创计划。</p><p>&nbsp;</p><p>据悉，豆包MarsCode&nbsp;用户组将由开发者自组织自运营，豆包MarsCode&nbsp;团队不会参与到用户组的管理，但是会为用户组提供丰富的各类资源支持，支持各地用户组发展，例如场地资源、产品资源、活动物料、专家讲师支持等。在开发者活动方面，豆包MarsCode&nbsp;将陆续在北、上、深、杭等城市举办&nbsp;Meetup&nbsp;。</p><p>&nbsp;</p><p>豆包MarsCode&nbsp;还计划协同各开发者社区，将&nbsp;AI&nbsp;能力融入到社区使用场景中，将豆包MarsCode&nbsp;的能力更便捷地提供给开发者。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/k5tx08DktK7vfdKjJm83</id>
            <title>长文本 vs RAG：谁将主导大模型未来？</title>
            <link>https://www.infoq.cn/article/k5tx08DktK7vfdKjJm83</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/k5tx08DktK7vfdKjJm83</guid>
            <pubDate></pubDate>
            <updated>Wed, 26 Jun 2024 09:55:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型技术公司, 长文本, RAG, 上下文
<br>
<br>
总结: 一些顶级大模型技术公司正在支持更大的上下文作为更新升级的重点，长文本突破引发了社区争议，对于 RAG 的未来存在疑虑。大模型企业开始降价，引发了低成本与选择长上下文和 RAG 的关系讨论。 </div>
                        <hr>
                    
                    <p>前段时间，国内外包括 OpenAI、谷歌、月之暗面等一大批顶级大模型技术公司都以支持更大的上下文作为更新升级的重点。长文本的突破也让社区中有了一些争议，认为这可能意味着 RAG可能会消亡。另外，最近国内一众大模型企业都开始宣布降价，那么低成本是否跟选择长上下文和 RAG 是否有关系？</p><p>&nbsp;</p><p>在这期极客有约节目中，我们邀请到月之暗面唐飞虎、zilliz 合伙人栾小凡、英飞流创始人张颖峰三位重磅专家给大家解读“长文本和 RAG”方面的技术。</p><p>&nbsp;</p><p>直播视频链接：<a href="https://www.infoq.cn/video/vZZIkkIt85rVRa61xsR8">https://www.infoq.cn/video/vZZIkkIt85rVRa61xsR8</a>"</p><p>&nbsp;</p><p>InfoQ：前段时间，大模型企业都在宣传自己取得了上下文上的突破，那么更大上下文能让我们做些什么？</p><p>唐飞虎：我认为可以将模型上下文长度类比为计算机内存。随着模型上下文长度的提高，应对较为复杂的逻辑推理或生成 repo level 的代码等问题的性能都有了提升。举例来说，使用谷歌 Gemini 对癌细胞进行影像学的诊断问题，是需要组合多模态和长文本才能完成，目前来说并没有较好的 RAG 解决方法。对多模态模型而言，更长的上下文不仅能帮助模型理解并读取更多文档，在模型的整体性能方面也有帮助。</p><p>栾小凡：对大模型来说，RAG 应用的构建时，更长的文本的确有利。这本质是一个输入输出窗口的问题，在具备捕捉信息和上下文能力的基础上，大文本输入的信息越多，输出也会越好。另一方面，我个人认为长文本只是大模型能力的其中之一，我是非常反对文本越长越智能的观点。</p><p>张颖峰：长文本对大模型来说是能力上的巨大提升，这种提升主要体现在其摘要能力的生成。结合我们过去的经验来说，去年在实施一些 RAG 项目时，我们遇到的最大痛点其实是来源于大模型本身，但在今年这些痛点就已经逐渐被消除或大大地改进了。长上下文也让模型的可控性得到了增强，我们在做 RAG 的过程中很多流程因此发生了变化，我们不会去追求特别复杂的检索策略，而是将结果检索到后将后续任务都交给了大模型来完成，这种实施也会相对较容易一些。</p><p>&nbsp;</p><p>InfoQ：我们是不是必须要200万甚至无限长的上下文？</p><p>张颖峰：长上下文很有意义，但无限长的上下文则是更偏向于是营销的宣传策略。上下文长度到达一定程度后，丢失的信息也会更多。毕竟上下文再大也只是内存，没有必要追求无限大的内存，再大的内存也还是需要硬盘的。</p><p></p><p>InfoQ：今年看到“长文本”我们可能就会想到“月之暗面”，之前月之暗面Kimi将上下文扩展到了200万，那么 Kimi 是如何实现的这个突破的？</p><p>唐飞虎：我们从20万字到200万字的扩展并没有采取常规的渐进式提升路线，而我们在技术攻关中遇到的难度也是呈指数级增加。为了达到更好的长窗口无损压缩的性能，我们团队从模型的预训练到对齐再到推理环节，均进行了重新的设计和开发，并且没有走滑动窗口、降采样等常规技术捷径，而是攻克了很多底层的技术难点。但是长上下文也不是无限长更好，我认为对现阶段的开发者而言这是一个 trade-off 的过程，上下文长度的增加对推理成本和时间都有一个平方几倍的增加。很多技术也是类似，都是遵循从 paper 到实验室，再从实验室到工程，最后再到产品这样一个循序渐进的过程，在现在的研究阶段，我们要将成本做到最低，长文本效果做到最好，这才是更重要的事情。</p><p></p><p>&nbsp;</p><p>InfoQ：Kimi 也直接在国内带来了比拼上下文长度的热潮，各厂商瞬间就突破了500万、1000万的处理能力，那这种效果属于算法还是算力上的突破？</p><p>唐飞虎：各个厂商目前还没有公开自己的 technical report，所以这方面开发者还是更有话语权，他们会实地使用、体验各家不同的产品。上下文的性能可以参考一些常规的 Benchmark，比如大海捞针和腾讯的数星星，最近还有更加复杂的评测，比如在长文本中添加代码、逻辑片段。如何在长文本上下文长度边长的同时还要保质保量，这是一个非常有挑战性的技术难题。</p><p>栾小凡：我人认为以目前的 Transformer 架构来讲，算法的提升空间还是相对有限，再想将上下文提升到下一个数量级，这条路已经很难走得通了。我最近做的一些测试来看，不说几十万、几百万 token 的上下文，光是上百 KB 的上下文在各家的线上测试都有几秒甚至是几十秒的延迟。所以在真正落地的生产环境中，上下文的长度还是应该控制在数十 K 的范围内。</p><p>张颖峰：从实现的角度来说，我会将其分为两派，一派是使用了 RAG，一派是没用 RAG。在目前提供公共大模型服务的厂商里，我认为这两派都有，且比例都不低。站在技术的角度来说，我在某些方面上还是更倾向于纯模型派。</p><p>&nbsp;</p><p>InfoQ：增加上下文窗口大小且不影响模型性能，会存在哪些挑战以及有什么应对方法？</p><p>唐飞虎：这个其实关系到我们 Transformer 模型的它的底层架构，就如果你去看它的那个计算的方法，然后再做一些相关的评测的话，就是有一些长文本性能上的一些常规上的瓶颈，那基本上说出目前分析下来的话是长文本的并发性能，它会随着你上下文长度的长度增加而反比下降。然后预填充的延迟会随着你上下文长度的增长而平方级别的增长，然后解码延迟和上下文切换的开销也会随着你上下文长度的增加而线性的增加啊。这几个参数里面对整个推理性能影响比较明显的是并发性能和预填充的延迟，也是现在各个厂商，包括一些学者专家，大家在研究和攻坚的这个难题。</p><p>栾小凡：飞虎老师提到的 Transformer O(n^2)&nbsp;的问题，我比较期待的是最近 LSTM 的卷土重来。我认为是需要对模型架构进行调整，才可能到达下一个阶段的。</p><p>&nbsp;</p><p>InfoQ：在处理大上下文长度时，原始 Transformer 架构的主要限制是什么？目前有哪些优化技术可以加速 Transformer 并将上下文长度增加到更大？</p><p>唐飞虎：主要的瓶颈就在于前面提到的矩阵计算导致预填充延迟和并发性能随上下文长度的增长而显著增加。目前这方面的优化角度主要还是硬件、机器学习工程和模型架构这三方面。硬件现在有 A100 memory 的 hierarchy 设计；机器学习工程有 VLLM 和较为主流的 Flash Attention 等技术；模型架构有前段时间流行的 MoE（多专家模型），也有最近对 Transformer 架构底层的颠覆，比如 Mamba 等等。但有些优化技术目前还没有较为完善的支持，导致其在工程上的实现并不可行。举例来说，键盘 QWERTY 的布局对人类而言并不是最优的，但大家都已经用了很久也非常习惯了，除非有更加优秀、能带来颠覆性结果的设计，大家才能更好地 follow。因此，如果只是从某一层面上带来效率的提升，那还要关注优化是否能再工程上有更好的结合。最近的一些论文有提到从 layer、head、hidden layer 等层面上做优化。大家可以在 ICIP 这些网站上搜索“推理优化”相关的论文结果。</p><p>栾小凡：如果大家对降低成本方面感兴趣的话，DeepSeek 的 MLA（Multi-latent Attention）和多专家模型是他们能将价格压下来的最底层的技术。从硬件层面来说，最早有去年很火的 Grok，今年也有越来越多的硬件厂商从推理方向加入这个赛道，比如美国的 SambaNova、英特尔等公司，这方面在我看来会比训练更容易得出成果，模型架构本身相对简单，硬件定制优化更容易得到很好的吞吐。</p><p>唐飞虎：旗舰店的另一个好处是不会出现前面提到的技术选型方向不兼容的问题。</p><p>张颖峰：模型成本方面最近降价很猛，我认为这和长上下文确实是有一些关系的。在目前的架构下， 主要瓶颈都在于对 KV cache 的低利用率。DeepSeek 本身的 ML 架构其实也是对 KV cache 做 low-rank 压缩。其实无论是推理加速还是长上下文，这些都是围绕着 KV cache 的利用率，要怎么将 cache 里的 attention 量化、提升和压缩。</p><p>&nbsp;</p><p>InfoQ：有论文声称能赋予LLM处理无限长度文本的能力，那工程上是否可实现，或大概多久后能达到无限上下文长度？</p><p>栾小凡：这个问题还是要回到第一原理上，如果说 AI 最终的目标是替代人做事，那么我们首先要问的一个问题是，人有什么样的能力？每个人的记忆都不是无限的，我们有短期也有长期的记忆，所以这其实是一个伪命题。再说什么才是真正的智能，我认为这是个伪命题，长文本不代表智能，长文本只是一种手段。能记住更多东西的人不一定更聪明。OpenAI 最近一年内的演进方向来看，更好地使用工具才是用大模型解决问题的一个关键点。记忆短有记忆短的处理办法，但记忆变长就一定意味着密度更小，以现在的 Transformer 机制来说，随着上下文的边长，模型一定会遗忘一些东西。举例来说，让大模型找到一百个数字中唯一的乱序数字，现有的模型都可以轻松完成，但换成一万个数字，那么基本上所有的模型都会犯错，因为它已经找不到重点了，甚至已经忘记了自己的任务是去找乱序的数字。</p><p></p><p>InfoQ：Jeff Dean的twitter提到，百万Token上下文，精度还能达到99.7%（千万99.2%），那么在这种情况下的大模型解决了RAG技术方案下所存在哪些痛点？但也还存在哪些问题待解决？</p><p>张颖峰：RAG 的痛点基本来源于这几个方面，一是在用户意图很强的情况下，召回可能不够精确。在实际的企业内部数据集或者用户的个人数据中进行检索，因为答案不再一个语义空间内，所以很可能搜索不到想要的东西。其次也是我们在公司创业到现在做各种 RAG 项目时遇到的问题，检索查询时数据本身非常杂乱。这些问题部分可以用上下文进行缓解，比如在长上下文的语义空间中包含检索需要的答案，这样大模型也能对问题进行回答。不过这种方案还不能完全解决这种问题，我认为还是需要提高 RAG 自身的能力。以今年春节为分割线，春节之前最大的瓶颈是大模型本身，春节之后在大家对大模型能力要求的情况下，我认为整套体系最大的瓶颈已经落到 RAG 上了。如果未来大家对模型期待提到，希望能引入更多的逻辑推理，那么痛点可能又会转移到大模型上去，这个问题总是在不断变化的。</p><p>唐飞虎：这个我可以举的例子是上个月我们的联合线下黑客马拉松，有一个场景是需要基于百大行业分析的研报文档知识库，生成相关数据的图表。当时在现场无论怎么组织 prompt，RAG 都没有办法得到想要的结果，于是我们直接将其换成长文本。因为 prompt 的上限导致还是无法不支持，所以我们采用了自定义插件将文档全部输入进去，结果的图表很效果很好。这样类似的例子其实有很多，需要的上下文信息分散在整个文章文档且有一定的上下文逻辑关系时，长文本目前的确比 RAG 更好。</p><p>栾小凡：我个人认为，无论是 RAG 还是长文本，和之前的数据处理没有什么本质区别。长文本是对所有数据的实时处理，RAG 则是对数据的离线处理。在没有理解用户意图的情况下，离线的数据处理可能结果出错。所以我认为现在的各种小技巧无非都是建立更加普遍适用的关系，猜测用户的意图。从本质来讲，如果大模型对一个问题没办法，那 RAG 也很难回答。所以我认为从模型最终的智能角度来说，一定是先有大模型智能再有 RAG。至于说 RAG 是否是不必要，人可以把所有书都读一遍来找到自己想要的答案，但搜索引擎却可以为我们节省时间。我认为 RAG 存在的一个核心价值是能帮助我们在大模型预处理数据之后更快速地完成重复工作。</p><p>&nbsp;</p><p>观众提问：这种场景一般要求准确性会比较高，长文本替换 RAG 的解决方案不会出现模型幻觉吗？</p><p>唐飞虎：如果只是将数据提取出来，那么出现幻觉的概率还是相对较低的。重灾区一是体现在一些较长的复杂逻辑上，数学证明题可能就在中间某个步骤就乱了，现在的模型也没有办法去很好地验证结果的正确；二是一些比较冷门的知识点，模型完全不知道结果；还有一种是数据本身 bias 导致的幻觉。这些都是我们目前需要攻克的问题，之前提到的场景，如果只是生成一个图表，那么模型出现的幻觉相对较低，即使生成了幻觉也能很容易地去验证，然后再重新调整 prompt。</p><p>&nbsp;</p><p>InfoQ：Gemini 1.5出来后，社区认为百万 Token 的上下文意味着 RAG 可能会消亡，老师们怎么看待这个观点？</p><p>张颖峰：我个人认为这种观点大部分正确，在海外媒体比如推特上这方面的讨论很多，基本是对 RAG 在成本、性能方面优势进行赞同，但在其他方面则持反对意见。我认为在 C 端个人知识库这类简单的场景下，大模型足矣。不论成本、性能和数据安全，只谈能力的情况下，B 端的 RAG 应用场景非常多，而随着上下文长度的增加，查询率也会降低。所谓的大海捞针也只是不会错过最近的针，更靠前的还是会有遗漏的可能。在这些场景下，我们不能只依赖大模型，还要将 RAG、大模型以及内部系统进行整合。这样一来又会带来更多问题，比如 RAG 检索出的结果虽然语义相似但答案却不是很相关，这种情况反而会降低模型回答的质量。</p><p>唐飞虎：上下文长度决定了模型的基础能力，RAG 则是决定了开发者对应用开发的上限。和模型微调类似，在大模型能力还不够的时候，我们很多的业务场景都是靠微调实现的，现在我们直接用长文本或 RAG 替代，不过还是会存在一些场景需要靠微调解决。RAG 也是一样，目前的一些 RAG 场景可能在未来用长文本会更快速、更方便，但 RAG 本身也会进步扩展，也会开发出新的场景。开发者还是需要同时掌握这两门技术的。</p><p>栾小凡：我不太认同 RAG 是 ToB 的观点，实际上我们目前的应用中有很多 ToC 场景，其中多租户就是一个非常有挑战性的问题。在应用有十万甚至百万的活跃用户时，怎样在所有的数据中精确搜索到某个租户的数据，这其实是 RAG 或向量数据库中非常重要的一个问题。OpenAI，包括 ChatGPT 中的许多能力也是围绕 RAG 和向量数据库构建的。RAG 解决的核心问题是准确帮助大模型找到所需的信息，它的本质是 retrieval。模型的能力需要提升，搜索的质量也要有提升。至于大家现在说到的分 chunk 等各种 trick，很可能会随着上下文和模型能力的提升而变得不再重要。</p><p>张颖峰：我也认同目前的一些 trick 可能在未来不再需要。我们目前这些杂乱无章的非结构化数据可能会随着未来多模态模型的能力提升，模型自己就能进行解读，但在目前我们还是要依赖 RAG 来完成这些脏活。</p><p>&nbsp;</p><p>观众提问：RAG 在 ToC 中的多租户难题是具体指什么？</p><p>栾小凡：以现在陪伴类 APP 为例，每个用户和大模型聊天时都会有上下文，用户在隔天登录时会需要对上下文的回顾，我们可以将这些聊天记录全部作为 context 输入到模型中去，但首先过多信息的输入会对大模型产生干扰，其次是成本的问题，随着聊天深入，上下文长度增长，整体的成本也会快速增加。我们需要在有限的长聊天记录中摘取出与当前对话相关的内容。对一个用户来说这点非常简单，但在数百万用户的情况下我们就会有百亿甚至千亿级别的数据量，要如何在这其中找到某个用户的数据，这其实是业界面临的一个较大挑战。</p><p></p><p>InfoQ：模型幻觉有什么常用的解决办法吗？要如何避免幻觉的出现？</p><p>唐飞虎：一个方法是 function calling，引入一些外部工具验证模型的中间生成结果。在这方面 OpenAI 的 code interpreter 就做得很好。比如说在生成一道数学题的答案后，它会立即生一段相关代码进行结果的验算。这是一种思路，我们可以把这种方法推广到更多的场景中，通过引入外部工具解决模型幻觉问题。</p><p>栾小凡：复杂的 RAG 其实在本质上就是各种工具的混合使用，前面提到的在数列中寻找乱序数的例子，真正智能的解法我认为应该是让模型生成并运行代码，这样一来无论多长的上下文模型都能找到一个最终的答案。</p><p>&nbsp;</p><p>InfoQ：现在大家最关注的就是“价格战”，比如有说百万token只要1元的，那么目前的一些大模型可能是通过哪些方式做到如此低的成本的？这其中，是否跟选择长上下文和RAG是否有关系？</p><p>唐飞虎：推理优化方面的技术有很多，OpenAI 的 GPT 3.5 Turbo 在这方面就已经很好了。这一次大厂商所谓的价格战其实更多是对一些相对较小的模型（8k、32k 等）进行降价，但在实际的开发场景中，很多开发者已经需要 128K 甚至是更长的模型，8K 的模型用户可以直接用 GPT 3.5 Turbo 来实现。这次的价格战真正能帮到开发者多少，还是要实际使用才能知道。当然，技术方面也确实是有进步的，比如量化和推理性能的提高。可以预期在未来 Q3、Q4 模型的价格和推理的成本还会进一步降低，但目前更重要的还是让长文本模型也能以较低的价格给到开发者。</p><p>张颖峰：在我看来，这次的降价除了营销因素外，技术方面与提高 Transformer 所依赖的 KV cache 利用率有很大关系。压缩 KV cache 后，推理的 batch size 就会更大，从而在解码阶段可以将内存的限制转换为计算的限制，从而实现每个 token 的降价。</p><p>栾小凡：模型本身我可能不是专家，但我之前在朋友圈看到一个有趣的观点，叫“引进 token 成本”，也就是说大模型的 token 虽然相对较贵，但同样的任务所需的 token 数可能更少。大家可能都深有感触，在模型上下文较短的时候，我们需要采取大量压缩方式或预处理，消耗很多的 token，但如果能有更加智能的模型，则回节省很多预处理的时间成本。从这个角度来看，我们更需要期待模型变得更智能，而不是过早地打价格战。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：长文本的不断突破对开发者意味着什么，更长的上下文是否会对开发人员应用程序的性能产生负面影响？</p><p>唐飞虎：我认为目前对开发来说受益还是很明显的，类比 LMOS 来说，上下文就像是内存、RAG 就像是外部的硬盘，其他可能还有存储架构之类。在 DOS 时代的开发者可以用 256 KB 的内存开发出 WPS、永远的毁灭公爵这类非常厉害的应用，但如今 256 KB 可能连程序都无法启动。技术是在不断迭代的，在简单的场景和低廉的成本下，我们可以直接将上下文当作 prompt，让模型自己去判断。</p><p>栾小凡：RAG 面临的最大挑战其实是数据之间的关系。尤其是较长的文章中会存在许多代指和因果关系，这些是简单的 RAG无法解决的问题。“他拿着一个苹果”，“然后他走出了教室”，那么苹果在哪里？如果 chunking 没有做好，将这两句话切分到了不同的段落中，这其实是超过了 RAG 能解决的范畴。随着上下文能力的提升，架构的相似性让未来的 embedding 模型上下文也会随着 generation 的模型提升。智源的 Landmark embedding 可以将很长的文章切割成多个 embedding 的同时又保持了一定的上下文关系，意味着有了长上下文之后，处理这类复杂的逻辑或代指关系就有了更多的工具。</p><p>&nbsp;</p><p>InfoQ：zilliz 在增强检索技术上有什么打算？</p><p>栾小凡：对我们来说有两件事是至关重要的，一是提高搜索性能，现在的 embedding 模型非常丰富，除了传统的 dense embedding，还有 sparse embedding、ColBERT embedding、Landmark embedding 等等，这些 embedding 其实都是在不同层面上提升搜索的质量，也可以将不同的 embedding 混合使用并结合 reranking 技术，大幅提高召回的效果和质量。这其实是 Milvus 或者说向量数据库在近一年内都在专注解决的问题。</p><p>另一件事则是成本，过去做 embedding，很多时候都是用一个 PDF 文件生成几百几千个 embedding。比如说曾经很火的 autoGPT，曾有次将 vectorDB 从他们的架构中移除了，人们问 Andrew G. 用什么 vectorDB 的时候，他说因为自己只有几千的向量，所以用的 numpy。但在今年，很多应用场景都发生了巨大变化，很多模型公司在用线下数据库做模型训练和微调时，ToB 或 ToC 的应用场景中出现了一亿甚至十亿级别的数据，这就让存储的成本成为了问题的核心。我在最近和美国的一家头部 AI 公司聊的时候，发现他们每年在线上数据库中花费的钱比 OpenAI 还要多，这个问题已经成为了很多头部 AI 应用公司的诉求。现在 binary embedding 的出现就是通过量化的技术降低存储的成本，再加上其他分存存储的技术和我们之前在做的 GPU index，都是降低成本的方向。</p><p>&nbsp;</p><p>InfoQ：英飞流在增强检索技术上有什么打算？</p><p>张颖峰：我们的规划有很多，对于前面提到的 RAG 的痛点，我们有些是在数据库层面解决，有些则是站在RAG全局的角度解决。在数据库层面，我们针对用户意图明确的情况下提供了丰富的多路召回能力。目前我们的多路召回只提供两路，但在一个月后的 release 中我们会提供更加丰富的在召回形式，包括用于精确召回的全文搜索、用于语义召回的向量搜索、以及稀疏向量等等。先前在四月 IBM 提出的 blended RAG 中提出同时采用这三路召回效果可能会更好。此外，我们还会提供基于张量 Tensor 的高级召回能力，将文章中的段落用张量表示，从而实现更高级的排序和重排序能力。</p><p>&nbsp;</p><p>这些能力都是围绕用户意图非常明确的情况下，通过多样化的多路召回能力保证回答的精确、语义和性能。其次，在用户意图不明显的时候，我们计划会在今年夏天启动这方面工作，将外部知识图谱存放到数据库中，方便根据用户意图的不同来改写用户提问。这些都是站在数据库的层面来解决的问题。</p><p>在 RAG 层面，我们打算做的也有很多，比如目前我们是通过 一些小模型来解决数据本身杂乱的问题，但在未来这一定是通过多模态大模型来解决。此外还有 RAG 本身必备的排序模型。RAG 本质上是过去搜索引擎的增强，搜索引擎十年前的口号“relevance is revenue”，这直到今天也同样适用。</p><p>&nbsp;</p><p>观众提问：多路召回是否会降低性能？</p><p>张颖峰：会略微降低，毕竟在同时使用多种的搜索方式，再有就是 reranking 也就是融合排序，这会比单纯的搜索略慢。但在 RAG 系统中，瓶颈永远不会是 RAG 本身，大模型的并发是远低于 RAG 中使用的各种数据库的。即使多路召回存在性能开销，也不会成为整体系统的瓶颈。</p><p></p><p>观众提问：月之暗面什么时候可以开放 200 万字 token？</p><p>唐飞虎：目前我们也在积极扩展用户的体验资格，前段时间 Kimi 也推出了打赏的功能，一些的打赏用户也已经提前拿到功能体验了，但具体什么时候能够全量放出，还是要依据具体的研发进度判断。</p><p>&nbsp;</p><p>观众提问：关于提示词有两种观点，一种认为很重要，一种认为随着 AI 的发展提示词将会越来越无关紧要。嘉宾们对此的怎么看？</p><p>唐飞虎：找个问题要要看分析的角度，从发展的角度来看，提示词二代重要性肯定会下降，但目前阶段提示词还是很重要的，在测试 benchmark 时提示词不对结果都会不准。如果我们再将目光放得长远些，如果我们将 AI 想象成智能体或人类，我们与其交流时需要学会提问的艺术，之前 HackerNews 上一篇很值得看的文章有提到 prompt 的艺术。随着时间的发展，模型越发智能，即使提示词表达不好模型也有会理解你的意图。我们的一些用户在反馈模型回答不准确时，我们发现这些问题就算是人类也没办法很好地回答，也需要对细节进行二次询问、</p><p>我们的 Kimi 大模型中有一些场景，比如在调用外部工具时如果缺失 required 的prompt 参数，模型会反问用户这个参数是什么。这只是一个例子，未来的大模型可能会更加智能。就像是乔布斯从来不做用户调研而是为产品创造用户，因为他更深刻地理解了用户背后的需求。未来大模型这方面的能力也会提升，从一个不完整或有缺陷的 prompt 中理解用户背后的真实意图。</p><p>栾小凡：我认为 prompt 还是非常重要的，大模型的本质是 next token prediction，如果上下文都比较模糊那么后续的结果也大概率会出现问题，从更长期的角度来说，prompt 的重要性会下降，但就目前所有的模型来看，提示词还是设计给专家来使用的。大家在写代码时大模型虽然可以有所帮助，但对于一个完全不懂代码的人来说他一定会遇到各种问题，也没办法解释任何一个报错。但如果是一个代码专家，我们能够看懂模型生成代码的模式，稍加调整就能得到还算不错的结果。在未来随着模型输入手段的多元化，这种门槛可能会下降，我们可以通过图片、声音或者更多的输入方式将信息直接传递给大模型， 甚至在未来大模型也可能具备获取信息的能力。</p><p>&nbsp;</p><p></p><p>观众提问：有哪些可以有效提高 RAG 召回率和准确率的方法吗？</p><p>张颖峰：在我看来，提高 RAG 召回率和准确率的方法有两种，一是前面提到的多路召回，也就是通过多种方法将结果取回后进行统一的融合排序，从而提高 RAG 的召回率。另一种方式则相对较为前沿，例如采用ColBERT 这种基于张量的召回方式会有相对来说较好的结果，但这种方式时需要有对应的模型可以产生相应的张量。这两种方式也有相互融合的可能，站在 Infrastructure 的角度来说，我们有必要同时提供这些能力。</p><p>栾小凡：我认为是可以从两方面回答的，一是数据层面，大家提到 RAG 的第一反应都是向量数据库，但其实也有很多别的方式，比如关键词检索和图数据库的使用。在意图相对较为明确的时候，数据的组织结构是和意图和应用场景对齐。二则是复杂的 RAG 或 agent 中一些流程，比如基于问题生成答案进行搜索、ranking 、chain of thought、trail of thought，甚至是基于图等复杂构造的流程。这些都可能会在不同的场景中有所帮助，我自己在做 RAG application 的时候一般会先基于 LangChain 或 LLamx 系统做 profiling，尝试应用场景中不同组合的效果。当然，构建自己的 evaluation dataset 其实是关键，只有知道要怎样评价模型或系统，我们才能进行各种的尝试。</p><p>&nbsp;</p><p>观众提问：向量数据库的未来是纯向量化还是混合型？ RAG 要如何选择合适的向量数据库呢？</p><p>栾小凡： 在我看来万物皆可向量化。但向量不代表 dense embedding。Dense embedding 很多会关注上下文，但却在关键词方面有所欠缺，因此需要用 hybrid search 进行弥补。当然也有专门针对关键词的 embedding，比如将 dense embedding 和 sparse embedding相结合，这个我们在 Milvus 的最新版本中进行了支持。但无论怎么说，我认为搜索的未来一定是基于模型上限的提高，而不是传统的统计信息。因此，知识图谱的构建等场景也一定是通过大模型自动化来构建，而非是通过业务的规则，搜索也是一样，只有在见过更多语料后搜索的质量才会提升。所以在我看来，最终很多都会变成向量，只不过不一定是 dense embedding 而已。</p><p>&nbsp;</p><p>观众提问：数据分析趋势会是 RAG 的一个场景吗？</p><p>张颖峰：数据分析我认为严格来说不能算是 RAG，当前的数据分析更多是依靠 text-to-SQL 的方式完成，是由大模型将自然语言转换为一条查询 SQL 语句，再用这条 SQL 去查询数仓返回结果。换句话说，我们从数据库中拿到的数据并没有经过大模型的加工。因此，我认知至少在当下，数据分析不算是 RAG。至于未来，这种可能性是存在的。如果我们在拿到数据图表和分析的结果后，希望结合其他数据源获得一些更深层次的洞见 insight，那么这种未来的用例是可以归纳到 RAG 中的。</p><p>&nbsp;</p><p>观众提问：多模态大模型的 RAG 未来的路要怎么走？世界大模型能带来哪些 RAG 和向量数据库的变化？</p><p>栾小凡：最近在 GPT-4o 发布后确实出现了较多的多模态应用场景。其中我认为比较重要的一个趋势是类似之前的 AI native 和 cloud native 的多模态 native。过去大家在训练多模态模型时，往往采取对齐的手段，将 image mode 和文本 model 通过对比学习或其他方式对齐到同一语义空间之中。但随着新的模型的出现，文本 token 和图像 token 天生就是在同一个语义空间中训练。因此，这类模型的性能相较其他会更好，语义理解能力也会更强，从而诞生出很多的应用场景。其中比较有意思的是一个无人驾驶的训练场景，过去大家在做动态搜索的时候往往是以图搜图或文字搜图的形式，但这种方式其中缺乏了很重要的意图理解：针对同一个图片，用户可以询问的问题有很多，生成的 embedding 或搜索结果也会不同。然而随着多模态 embedding 的发展，这种想法逐渐变得越发现实。结合多模态的生成能力，无论是视频领域还是 GPT4 演示的语音助手领域，我认为都会有更多新场景演化出来。</p><p>张颖峰：说到多模态大模型 RAG，我认为在 RAG 本身部分可能变化没有很大，但要是想让这条链路在未来成为爆款，有更大的普及，那么瓶颈还是在模型本身。就目前来说，不说视频，光看文和图之间的生成，就已经有很多的选择了，但要是想要达到商用标准，在模型的可控性上还有很长的路要走，基于多模态的 RAG 在图像生成方面其实是非常薛定谔的，我们没有办法精准控制生成的结果。这其实是阻止我们将多模态 RAG 大规模使用的主要瓶颈，但我相信这个瓶颈的突破应该不会花太长时间。</p><p>&nbsp;</p><p>&nbsp;</p><p>观众提问：哪些 embedding model 可以同时支持多语言呢？</p><p>唐飞虎：现在支持多语言的模型有很多，大家熟悉的 OpenAI embedding 目前来看效果相对来说很不错。国内包括智源和 GenAI 在内也有很多跨语言的 embedding model，如果大家对embedding model 的效果和能力感兴趣的话，可以关注 HuggingFace 上的 MTEB 榜单，里面也有一些多语言的 benchmark。</p><p></p><p>观众提问：开发基于大模型的应用生产环境是应该考虑用 Python 还是其他的语言呢？</p><p>唐飞虎：这个问题主要取决于开发应用的量级和它依赖的生态。就我们所观察到的情况来说，开发者社群内使用的语言从 Python、Java、GoLang 到 Rust 都有，在我们的 GitHub repo 里也是 Python、NodeJS、Go、Java、C# 等等，能想要的语言应有尽有。从编程语言来说并不会对开发者有很大的限制，哪怕是只会一种语言也是可以和我们的模型有很好的交互。主要的问题在于其他的生态，对 bot 来说，Node 或 Python 更加合适，但要是想和企业内已有的服务打通，那么 Java 可能会是更好的选择。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/9RsNpcVNEZorwctMtfqy</id>
            <title>没想到国内大模型厂商又一次high起来，是因为OpenAI 断供！</title>
            <link>https://www.infoq.cn/article/9RsNpcVNEZorwctMtfqy</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/9RsNpcVNEZorwctMtfqy</guid>
            <pubDate></pubDate>
            <updated>Wed, 26 Jun 2024 06:33:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 褚杏娟, 华卫, OpenAI, API
<br>
<br>
总结: 中国大陆等地区的开发者收到了OpenAI的邮件，表示将停止不支持地区的API使用。OpenAI关闭了中国、俄罗斯、朝鲜、伊朗等国家的账户，引发了国内大模型厂商的回应，提供迁移计划和激励措施。智谱AI、百度智能云、阿里云等公司相继推出了针对OpenAI用户的替代方案和优惠活动。 </div>
                        <hr>
                    
                    <p>整理&nbsp;|&nbsp;褚杏娟、华卫</p><p>&nbsp;</p><p>6月25日起，陆续有包括中国大陆在内的各国和相关地区&nbsp;API&nbsp;开发者在社交媒体上表示，他们收到了来自&nbsp;OpenAI&nbsp;的邮件，表示将采取额外措施停止其不支持的地区的&nbsp;API&nbsp;使用。</p><p>&nbsp;</p><p>根据网上流传的邮件截图，OpenAI&nbsp;表示：“根据数据显示，你的组织有来自&nbsp;OpenAl&nbsp;目前不支持的地区的&nbsp;API&nbsp;流量。从&nbsp;7&nbsp;月&nbsp;9&nbsp;日起，我们将采取额外措施，停止来自不在&nbsp;OpenAI&nbsp;支持的国家、地区名单上的&nbsp;API&nbsp;使用。”</p><p>&nbsp;</p><p>“要继续使用&nbsp;OpenAI&nbsp;的服务，您需要在受支持的地区访问该服务。”在&nbsp;OpenAI&nbsp;给出的<a href="https://platform.openai.com/docs/supported-countries">“支持访问国家和地区”名单</a>"上，世界上大部分地区都可以使用&nbsp;OpenAI，包括几乎整个西方、东欧大部分地区、南亚和大约一半的非洲，但中国大陆、中国香港、俄罗斯、朝鲜、伊朗等地均未在列。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/09/09663dc10fa89bee01622f66e59e62e3.jpeg" /></p><p></p><p>&nbsp;</p><p>而上述不受支持的中国、俄罗斯、朝鲜、伊朗四个国家，似乎“踩在”&nbsp;OpenAI&nbsp;的雷达上已有一段时间。今年&nbsp;2&nbsp;月，这家人工智能公司宣布关闭了其声称由这四个国家的&nbsp;"国家附属恶意行为者&nbsp;"使用的账户，表示他们使用&nbsp;ChatGPT&nbsp;帮助进行网络钓鱼攻击和开发恶意软件。上个月底，OpenAI&nbsp;打击了另一组来自中国、俄罗斯、伊朗和以色列的账户。</p><p>&nbsp;</p><p>实际上，OpenAI&nbsp;早先就对中国大陆地区的用户实行了注册门槛，限制了其对&nbsp;ChatGPT&nbsp;服务的访问权限。中国大陆的开发者群体在构建基于&nbsp;OpenAI&nbsp;API&nbsp;的衍生服务时，往往需要通过代理服务器或在海外部署反向代理机制。这不仅增加了运维成本，也无法保证服务的稳定性。</p><p>&nbsp;</p><p>这次，OpenAI&nbsp;的强制决策一出，便立刻引发了国内大模型厂商的回应，各厂商纷纷表示可以支持企业“无痛”迁移，并发布了不少吸引OpenAI用户使用其平台的激励措施。而根据多位行业专业人士的看法和预测，国内大模型行业内部此时也有更深层次的担忧与挑战悄然浮现。</p><p>&nbsp;</p><p></p><h2>“百模大战”</h2><p></p><p>&nbsp;</p><p>首先作出反应的是<a href="https://www.infoq.cn/article/YYBJGK4C07VM7KjcHt34?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">智谱&nbsp;AI</a>"。当天下午一点半左右，智谱&nbsp;bigmodel.cn&nbsp;推出了&nbsp;OpenAl&nbsp;API&nbsp;用户特别搬家计划，帮助用户切换至国产大模型，具体包括为开发者提供&nbsp;1.5&nbsp;亿&nbsp;Token（5000&nbsp;万&nbsp;GLM-4&nbsp;+1&nbsp;亿&nbsp;GLM-4-Air)&nbsp;以及从&nbsp;OpenAl&nbsp;到&nbsp;GLM&nbsp;的系列迁移培训。对于高用量客户，智谱提供与&nbsp;OpenAl&nbsp;使用规模对等的&nbsp;Token&nbsp;赠送计划(不设上限)，以及与&nbsp;OpenAl&nbsp;对等的并发规模等。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/50/50824c5d8401618664f73da69e3ee7fb.png" /></p><p></p><p>&nbsp;</p><p>当天下午四点半左右，百度智能云千帆推出了大模型普惠计划，即日起为新注册企业用户提供：</p><p>&nbsp;0&nbsp;元调用：文心旗舰模型首次免费，赠送&nbsp;ERNIE3.5&nbsp;旗舰模型&nbsp;5000&nbsp;万&nbsp;Tokens&nbsp;包，主力模型&nbsp;ERNIE&nbsp;Speed/ERNIE&nbsp;Lite&nbsp;和轻量模型&nbsp;ERNIE&nbsp;Tiny&nbsp;持续免费；针对&nbsp;OpenAI&nbsp;迁移用户额外赠送与&nbsp;OpenAI&nbsp;使用规模对等的&nbsp;ERNIE3.5&nbsp;旗舰模型&nbsp;Tokens&nbsp;包。0&nbsp;元训练：免费模型精调训练服务0&nbsp;元迁移：零成本&nbsp;SDK&nbsp;迁移工具0&nbsp;元服务：专家服务（迁移&nbsp;&amp;使用指导）</p><p>&nbsp;</p><p>不过，百度智能云表示，以上优惠活动均在&nbsp;2024&nbsp;年&nbsp;7&nbsp;月&nbsp;25&nbsp;日&nbsp;24&nbsp;点前适用。</p><p></p><p>不到半小时后，阿里云紧接着宣布，将为OpenAI&nbsp;API用户提供最具性价比的中国大模型替代方案，并为中国开发者提供2200万免费tokens和专属迁移服务。据悉，<a href="https://www.infoq.cn/article/GF4Jqtkgho4EhcsoYFLF?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">通义千问主力模型</a>"Qwen-plus在阿里云百炼上的调用价格为0.004元/千tokens，仅为GPT-4的50分之一。根据斯坦福最新公布的大模型测评榜单HELM&nbsp;MMLU，Qwen2-72B得分为0.824，与GPT-4并列全球第四。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/86/863810ae0d81f916c898af4e9681830e.png" /></p><p></p><p></p><p>随后在当天六点&nbsp;20&nbsp;分左右，零一万物宣布发起了“Yi&nbsp;API&nbsp;二折平替计划”，面向&nbsp;OpenAI&nbsp;用户推出了平滑迁移至&nbsp;Yi&nbsp;系列大模型的服务，并针对接入&nbsp;OpenAI&nbsp;的不同模型的用户，一一对应地提供了高模型性能且极具性价比的替换方案。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/a0/a0ab82a9c92e9569dee4fbc135cb2a14.jpeg" /></p><p></p><p>&nbsp;</p><p>据介绍，目前注册使用&nbsp;Yi&nbsp;API&nbsp;的新客户，零一万物立即赠送&nbsp;100&nbsp;元额度；平台充值还将赠送&nbsp;50%&nbsp;到账额度，上不封顶，为用户提供更长线的优惠；任意充值即可享受&nbsp;RPM/TPM&nbsp;限速直升&nbsp;Tier3，直达高级别的服务质量和超快响应速度。此外，零一万物&nbsp;API&nbsp;还将提供&nbsp;Prompt&nbsp;兼容调优服务支持，陪伴用户适配&nbsp;Yi&nbsp;系列大模型。</p><p>&nbsp;</p><p>零一万物表示，在模型性能相近的同时，Yi-Large&nbsp;的定价远低于顶配模型&nbsp;GPT-4o。以&nbsp;GPT-4o&nbsp;的定价计算（取&nbsp;Input&nbsp;和&nbsp;Output&nbsp;均值为&nbsp;Open&nbsp;API&nbsp;价格），接入&nbsp;Yi-Large&nbsp;后使用成本可下降&nbsp;72%；而对比&nbsp;GPT-4&nbsp;Turbo&nbsp;的价格，用户接入&nbsp;Yi-Large-Turbo&nbsp;后使用成本可下降九成以上；对于简单任务的处理，Yi-Medium&nbsp;的使用成本较&nbsp;GPT-3.5-Turbo-1106&nbsp;下降&nbsp;66%。</p><p></p><p>此外，零一万物还可提供支持实时搜索的&nbsp;Yi-Large-RAG，适用于需要结合实时信息进行推理的场景，以便用户基于自身需求选择更匹配的模型。</p><p>&nbsp;</p><p>当日，AI&nbsp;Infra&nbsp;厂商硅基流动则直接宣布开放7款大模型：SiliconCloud&nbsp;平台的&nbsp;Qwen2(7B)、GLM4(9B)、Yi1.5（9B）等开源大模型免费使用。SiliconCloud&nbsp;是集合主流开源大模型的一站式云服务平台，目前已上架包括&nbsp;DeepSeek-Coder-V2、Stable&nbsp;Diffusion&nbsp;3&nbsp;Medium、Qwen2、GLM-4-9B-Chat、DeepSeek&nbsp;V2、SDXL、InstantID&nbsp;在内的多种开源大语言模型、图片生成模型，支持用户自由切换符合不同应用场景的模型。同时，SiliconCloud&nbsp;提供开箱即用的大模型推理加速服务。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2b/2bd8e21aa6a48def16b5dd251491c8ee.png" /></p><p></p><p></p><p>当天晚&nbsp;8&nbsp;点左右，腾讯云宣布，即日起，新迁移企业用户可免费获得腾讯混元大模型&nbsp;1&nbsp;亿&nbsp;Tokens。目前，腾讯云提供混元&nbsp;Pro、Standard、Lite&nbsp;等多个不同版本和尺寸的模型，用户可任意选择。腾讯还将为新迁移企业用户提供免费专属迁移工具和服务，该福利截止&nbsp;7&nbsp;月&nbsp;31&nbsp;日&nbsp;24&nbsp;点前。</p><p>&nbsp;</p><p>今日凌晨，百川智能也跟进宣布了“零成本迁移”的措施：免费赠送&nbsp;1&nbsp;千万&nbsp;token、Assistants&nbsp;API&nbsp;免费使用。另外，百川开设了专家技术群，表示专家随时答疑，五分钟即可完成&nbsp;API&nbsp;迁移。</p><p></p><p>据悉，百川智能前不久刚发布最新一代基座大模型Baichuan&nbsp;4，并推出成立之后的首款AI助手“百小应”。Baichuan&nbsp;4相较Baichuan&nbsp;3&nbsp;在各项能力上均有极大提升，其中通用能力提升超过10%，数学和代码能力分别提升14%和9%。</p><p></p><p></p><h2>一时的机会，更卷的将来</h2><p></p><p></p><p>中美之间日益紧张的关系可能是促使&nbsp;OpenAI&nbsp;决定打击不受支持的用户的一个因素。自特朗普执政以来，美国已经对中国实施了制裁和关税，包括拜登总统增加对中国芯片、电池和电动汽车的关税。</p><p>&nbsp;</p><p>为此，中国也加大了实现技术自给自足的力度，规定其电信公司在&nbsp;2027&nbsp;年前停止使用英特尔和&nbsp;AMD&nbsp;的&nbsp;CPU，并要求其汽车制造商在&nbsp;2025&nbsp;年前至少在国内采购四分之一的计算机处理器。</p><p>&nbsp;</p><p>尽管OpenAI正计划阻止在中国的API访问，但这对中国公司来说，无疑是一个迅速填补即将到来的市场空白以获得更多用户的好机会。不过，之后国内其他厂商是否会跟进，目前尚不能确定。</p><p>&nbsp;</p><p>有专家预测，OpenAI&nbsp;主动“送生意”的做法，给了国内的大模型厂商喘气的机会，但后续可能就得被迫继续卷价格。这意味着，已经有些降温的“大模型价格战”或将再次“火热”。可以看出，国内大模型行业在机遇重重的同时，竞争也将进一步加剧。</p><p>&nbsp;</p><p>正如百川智能&nbsp;CEO&nbsp;王小川所说，“我们不需要一千、一万个大模型，在没有价格战的时候，中国可能真的是上百、上千个大模型在进行。”</p><p>&nbsp;</p><p>同时，有不少网友纷纷议论：部分国产大模型是否会因OpenAI&nbsp;的断服“现原形”。国产大模型中是否存在调用&nbsp;OpenAI&nbsp;&nbsp;API&nbsp;的这一问题暂且不论，目前根据各类大模型用户在公开平台发表的使用反馈来看，许多国产大模型的中文文本上处理能力并不弱于ChatGPT&nbsp;，在视频、图片等多模态方面有所不足，但整体来说影响有限。</p><p></p><p>不可否认的是，对于国内一批使用&nbsp;OpenAI&nbsp;&nbsp;API&nbsp;的开发者来说，影响是巨大的。还有人士对国内用户发出了“谨慎付费”的友善提醒，一些通过调用海外大模型&nbsp;API&nbsp;接口来提供服务的套壳网站，之后可能因高昂的站点迁移成本关停跑路。</p><p></p><p>此外，OpenAI强制执行其不支持国家列表的访问政策，在对中国开发者产生负面影响的同时，也可能带来其他方面的双向后果。</p><p>&nbsp;</p><p>根据Reddit上的一篇帖子，总部位于美国的云平台公司Vercel的用户如果通过Vercel的边缘网络访问OpenAI，也会收到同样的OpenAI邮件。目前还不清楚这封电子邮件是否发送有误，但&nbsp;Vercel&nbsp;的边缘网络确实有一个位于香港的区域，与中国大陆一样不受&nbsp;OpenAI&nbsp;支持。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/jAuKRqF6FkHsq0RtmAuR</id>
            <title>FCon x AICon联诀来袭、干货翻倍，8折倒计时最后5天</title>
            <link>https://www.infoq.cn/article/jAuKRqF6FkHsq0RtmAuR</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/jAuKRqF6FkHsq0RtmAuR</guid>
            <pubDate></pubDate>
            <updated>Wed, 26 Jun 2024 03:36:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: InfoQ, FCon, AICon, 金融科技
<br>
<br>
总结: InfoQ旗下的FCon全球金融科技大会和AICon全球人工智能开发与应用大会将在上海联合举办，涵盖金融科技和人工智能领域的前沿技术和实践经验。FCon聚焦金融AI大模型落地实践，AICon深入剖析RAG等前沿技术及其应用，两大会议将为参与者带来丰富的干货和一站式体验。 </div>
                        <hr>
                    
                    <p>8月16日-19日，InfoQ旗下FCon&nbsp;全球金融科技大会和&nbsp;AICon&nbsp;全球人工智能开发与应用大会上海站将联诀来袭，AI+金融、技术前沿+行业前沿，双会联动、一站式体验、干货翻倍！</p><p></p><h3>FCon：囊括近1年金融AI大模型落地实践</h3><p></p><p></p><p>FCon是InfoQ推出的首个垂直行业大会，2023年第一届FCon大会在上海成功举办，来自中国工商银行、中国民生银行、平安银行、兴业银行、中信银行、北京银行&nbsp;、苏州银行、汇丰科技、国泰君安证券、平安人寿、阳光保险集团、度小满、蚂蚁集团等公司的上百位业界专家，现场分享了关于金融企业数字化转型的最新研究成果和实践经验。</p><p></p><p><a href="https://fcon.infoq.cn/2024/shanghai/">2024年FCon</a>"将于8月16日-17日在上海举办，由中国信通院铸基计划作为官方合作机构，以“科技驱动，智启未来——激发数字金融内生力”为主题。在“十四五”收官之际，本届大会将致力于展示金融数字化在“十四五”期间的关键进展，帮助金融机构更具针对性地“查缺补漏”。同时，聚焦金融行业在数智化的全面革新，紧跟当下技术热点，分享近一年来金融行业AI大模型的落地实践经验和成果。</p><p></p><p>原国有大型商业银行资深业务架构师、天润聚粮执行董事总经理付晓岩，度小满金融技术委员会执行主席、数据智能应用部总经理杨青已确认担任本届大会领航团联席主席。目前11个专题内容已就绪，中国信通院泰尔终端实验室数字生态发展部主任王景尧、极客邦科技CGO/InfoQ 极客传媒&amp;极客时间企业版总经理汪丹（Yolanda）、广发银行信用卡中心商业智能负责人徐小磊、北京银行软件开发中心副总经理代铁、平安银行金融科技部数据资产管理及研发中心数据及AI团队负责人廖晓格、方正中期期货数字科技中心负责人张志明、文因互联董事长/创始人鲍捷博士、Aloudata 大应科技周卫林、宇谷金融科技研究院院长吴易璋、飞轮科技CTO王猛等将作为出品人为专题内容品控献计献策。</p><p></p><p>此外，已有近20位嘉宾已确认参与FCon大会议题分享：</p><p></p><p><img src="https://static001.geekbang.org/infoq/02/02c6d0a4695670c8c017a2c6fd667c65.png" /></p><p></p><p>点击链接可查看更多大会详情：<a href="https://fcon.infoq.cn/2024/shanghai/">https://fcon.infoq.cn/2024/shanghai/</a>"</p><p></p><h3>AICon：深入剖析RAG等前沿技术及其应用</h3><p></p><p></p><p><a href="https://aicon.infoq.cn/2024/shanghai">AICon&nbsp;全球人工智能开发与应用大会</a>"是InfoQ面向各行业对人工智能和大模型感兴趣的资深工程师、产品经理、数据分析师&nbsp;推出的人工智能和大模型技术盛会。在5月17-18日北京站，我们紧扣热点，坚持实践驱动，为超过&nbsp;1000&nbsp;位观众留下了深刻印象。</p><p></p><p>AICon&nbsp;上海站将于&nbsp;8月18日-19日举办，本次大会邀请了蚂蚁集团&nbsp;蚂蚁超级计算部负责人余锋（褚霸）、Lepton&nbsp;AI&nbsp;联合创始人&nbsp;&amp;&nbsp;CEO&nbsp;贾扬清、北京智源人工智能研究院副院长兼总工程师林咏华、百川智能技术联合创始人谢剑担任联席主席。</p><p></p><p>目前多位专题出品人已确认，包括科大讯飞&nbsp;AI&nbsp;研究院副院长/科研部部长李鑫、阿里巴巴总监郭瑞杰、Seasalt.ai&nbsp;CEO&nbsp;姚旭晨、阿里巴巴企业智能算法负责人陈祖龙、DeepWisdom（MetaGPT）&nbsp;创始人兼&nbsp;CEO&nbsp;吴承霖、阅文集团技术副总经理&nbsp;&amp;&nbsp;AIGC&nbsp;负责人陈炜于、字节跳动&nbsp;Code&nbsp;AI&nbsp;团队技术负责人杨萍、京东高级技术总监周默、智源智能评测组负责人杨熙等，他们将负责精选针对特定主题论坛的高质量议题，确保论坛的技术深度。</p><p></p><p>此外，已有近10位嘉宾已确认参与大会议题分享：</p><p><img src="https://static001.geekbang.org/infoq/1b/1b0d078a00653ce775e37658524167b5.png" /></p><p></p><p>点击链接可查看更多大会详情：<a href="https://aicon.infoq.cn/2024/shanghai">https://aicon.infoq.cn/2024/shanghai</a>"</p><p></p><h5>优惠购票</h5><p></p><p>目前是FCon、AICon两场大会&nbsp;8&nbsp;折优惠期，单场单张门票节省&nbsp;960&nbsp;元（原价&nbsp;4800&nbsp;元），特惠折扣最后5天倒计时，详情可扫码或联系13269078023&nbsp;咨询票务经理。</p><p></p><p><img src="https://static001.geekbang.org/infoq/46/46472b60308a2aa865d8d80412971e56.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/kgFyRwZx2ruihJlcZDRv</id>
            <title>大模型训练检查点写入速度相比 PyTorch 加快 116 倍！微软提出FastPersist 新方法</title>
            <link>https://www.infoq.cn/article/kgFyRwZx2ruihJlcZDRv</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/kgFyRwZx2ruihJlcZDRv</guid>
            <pubDate></pubDate>
            <updated>Wed, 26 Jun 2024 02:13:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 微软DeepSpeed, FastPersist, 检查点写入速度, NVMe SSDs
<br>
<br>
总结: 微软DeepSpeed团队提出了FastPersist技术，通过优化NVMe SSDs的使用、提高写入并行性，以及实现检查点操作与独立训练计算的重叠，显著提升了检查点的创建速度，降低了训练过程中的I/O开销。 </div>
                        <hr>
                    
                    <p></p><blockquote>近日，微软&nbsp;DeepSpeed&nbsp;研究组发布最新论文，提出一种名为&nbsp;FastPersist&nbsp;的新方法，旨在解决大模型训练时写检查点十分耗时的问题，相比&nbsp;PyTorch&nbsp;基线，写入速度提升超过&nbsp;100&nbsp;倍。</blockquote><p></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/e9/e9fa72d4738a423bd5b0711c2189dd3f.png" /></p><p></p><p></p><p>深度学习作为推动人工智能发展的关键技术，其模型检查点（checkpoint）的生成对于确保训练过程的稳定性和容错性至关重要。然而，随着模型规模的不断扩大，传统的检查点写入方法已经无法满足日益增长的I/O需求，成为制约深度学习发展的瓶颈。FastPersist技术的提出，正是为了解决这一问题。</p><p></p><p>FastPersist&nbsp;是微软DeepSpeed团队针对深度学习模型训练中检查点创建效率低下的问题提出的解决方案。据介绍，这项技术的核心在于通过三种创新的方法，即优化NVMe&nbsp;SSDs的使用、提高写入并行性，以及实现检查点操作与独立训练计算的重叠，显著提升了检查点的创建速度，降低了训练过程中的I/O开销。实验结果表明，FastPersist能够在几乎不影响训练性能的前提下，实现高达116倍的检查点写入速度提升。这项技术的提出，不仅解决了大规模深度学习训练中的一个关键问题，也为未来深度学习模型的进一步发展提供了强有力的技术支持。</p><p></p><p>AI&nbsp;前线进一步了解到，&nbsp;在微软很多重要的大模型训练中，由于工作负载高度密集，经常出现&nbsp;GPU&nbsp;error，所以需要很高频地写检查点操作，而这些大模型训练其实都在使用&nbsp;FastPersist&nbsp;这套系统。</p><p></p><p>论文链接：<a href="https://arxiv.org/pdf/2406.13768">https://arxiv.org/pdf/2406.13768</a>"</p><p></p><p></p><h2>现状和问题</h2><p></p><p></p><p>深度学习作为人工智能领域的一个重要分支，近年来在图像识别、自然语言处理、推荐系统等多个领域取得了突破性进展。随着研究的深入，深度学习模型的规模也在不断扩大，从早期的百万级参数模型发展到现在的百亿甚至千亿级参数的超大型模型。模型规模的增长带来了更强的表征能力和更高的准确率，但同时也带来了计算复杂度的提升和存储需求的增加。特别是模型参数、梯度信息以及中间特征图等数据的存储，对存储系统的I/O性能提出了更高的要求。</p><p></p><p>尽管计算性能的提升可以通过硬件加速和算法优化来实现，但I/O性能的提升却受到了传统存储设备和系统的限制。特别是在模型训练过程中，检查点的生成是一个不可或缺的步骤，用于保存模型在特定迭代步骤的状态，以便在发生故障时能够从最近的检查点恢复训练，从而避免重复计算。然而，检查点的生成和保存是一个资源密集型的操作，涉及到大量的数据写入。在大规模训练中，模型参数和中间数据的体积巨大，检查点的生成和保存需要消耗大量的I/O带宽和时间，这不仅增加了训练的总体时间，也可能导致I/O系统的饱和，影响其他训练操作的执行。因此，提高检查点创建的效率，成为提升深度学习模型训练性能的关键。</p><p></p><p>当前深度学习框架中的检查点生成机制，大多数基于传统的文件I/O操作，这些操作并没有充分利用现代存储设备，如NVMe&nbsp;SSDs的高性能特性。这导致了在大规模训练场景下，检查点写入成为制约整体性能的瓶颈。此外，由于检查点写入操作与模型训练的其他计算任务之间存在数据依赖性，传统的检查点生成方法无法实现与训练过程的完全解耦，进一步限制了检查点生成的效率。</p><p></p><p>为了解决I/O瓶颈问题，研究者和工程师们提出了多种解决方案，如使用更快的存储介质、优化文件系统、改进数据写入策略等。但是，这些解决方案往往存在一定的局限性。例如，简单地更换更快的存储介质虽然可以提高I/O性能，但成本较高，且在大规模并发写入时仍可能遇到瓶颈。优化文件系统和数据写入策略可以在一定程度上提高效率，但往往需要对现有的深度学习框架和训练流程进行较大的改动，兼容性和通用性有待提高。</p><p></p><p>针对上述问题，微软DeepSpeed团队提出了FastPersist技术。</p><p></p><p></p><h2>FastPersist&nbsp;技术方案</h2><p></p><p></p><p>FastPersist&nbsp;通过深入分析深度学习训练过程中的I/O需求和特点，结合现代存储设备的特性，提出了一种全新的检查点生成和保存方法。主要通过以下三个方面来提升检查点创建的效率：</p><p></p><h3>1. NVMe存储设备的优化利用</h3><p></p><p>FastPersist&nbsp;针对NVMe&nbsp;SSDs的高性能特性进行了优化。通过使用专为NVMe设计的I/O库，如libaio和io_uring，FastPersist能够更高效地管理数据在GPU和SSD之间的传输，从而显著提高了单节点上的检查点写入速度。</p><p></p><p>FastPersist还采用了双缓冲技术来进一步提高写入效率。在双缓冲机制中，当一个缓冲区的数据正在写入SSD时，另一个缓冲区可以同时从GPU内存中预取数据，这样就能实现数据写入和数据预取的流水线操作，减少了等待时间，提高了整体的写入性能。</p><p></p><p>另外，&nbsp;FastPersist针对NVMe&nbsp;SSDs的特性，对数据块的大小和对齐进行了优化。通过调整数据块的大小，使其匹配SSD的页面大小，可以减少写入操作的数量，提高写入效率。同时，通过对齐数据块到合适的边界，可以避免额外的拷贝操作，进一步提高性能。</p><p></p><p></p><h3>2.&nbsp;写入并行性的实现</h3><p></p><p>在深度学习模型训练中，特别是在大规模分布式训练环境中，数据并行（Data&nbsp;Parallelism）是一种常见的训练策略。在数据并行训练中，模型被复制到多个训练节点上，每个节点处理不同的数据子集。这种训练方式可以显著提高计算资源的利用率，加快模型的训练速度。然而，如果检查点的写入操作仍然集中在单个节点上执行，那么I/O操作就可能成为限制整体性能的瓶颈。</p><p></p><p>FastPersist技术通过实现检查点写入的并行性，解决了这一问题。在FastPersist中，检查点的写入操作被分布到所有参与训练的节点上，每个节点只负责写入其对应的模型部分。这样，写入操作就可以同时在多个节点上执行，从而显著提高了整体的写入速度。</p><p></p><p>为了实现高效的写入并行性，FastPersist采用了以下几个关键策略：</p><p>数据分片：FastPersist将检查点数据均匀地分割成多个片段，每个训练节点只负责写入其分配到的数据片段。这种分片策略确保了写入负载在所有节点上的均衡分配。无通信写入：在FastPersist中，每个节点独立地完成其检查点数据片段的写入，无需与其他节点进行通信或协调。这种设计减少了节点间通信的开销，提高了写入操作的效率。动态负载平衡：FastPersist能够根据节点的计算能力和存储性能动态调整数据片段的大小，确保所有节点的写入负载保持均衡。这种动态调整机制可以适应不同的硬件环境和训练配置。容错和恢复：在分布式训练环境中，节点的故障是不可避免的。FastPersist通过在写入操作中实现容错机制，确保即使部分节点发生故障，也不会影响检查点的完整性和训练的连续性。</p><p></p><p></p><h3>3.&nbsp;操作重叠的策略</h3><p></p><p></p><p>在深度学习模型训练中，检查点的生成通常需要在每个训练迭代后执行，以确保模型状态的持久化。然而，如果每次迭代后都进行完整的检查点写入操作，那么这些操作可能会占用大量的计算资源，影响模型训练的速度。为了解决这一问题，FastPersist采用了操作重叠的策略，将检查点的写入操作与模型训练的其他计算任务并行执行。</p><p></p><p>操作重叠的核心思想是利用深度学习训练中的计算特性，将检查点写入操作与模型的前向传播和后向传播操作重叠。由于前向传播和后向传播操作通常占据了模型训练的大部分时间，通过将检查点写入操作与这些操作并行化，可以有效地隐藏I/O操作的延迟，提高整体的训练效率。</p><p></p><p>FastPersist实现操作重叠的具体策略包括：</p><p>异步写入：FastPersist采用异步写入机制，使得检查点的写入操作不会阻塞计算操作的执行。在每个训练迭代的优化器步骤之后，FastPersist会启动检查点的异步写入过程，而计算线程可以继续执行下一个迭代的前向传播和后向传播。双线程模型：FastPersist引入了一个辅助线程专门负责检查点的写入操作。主线程负责执行模型的计算任务，而辅助线程在主线程的协调下执行检查点的写入。这种双线程模型确保了计算和I/O操作的并行执行，减少了相互之间的干扰。数据局部性优化：FastPersist通过优化数据的存储和访问模式，提高了数据在GPU和CPU之间的传输效率。通过利用数据的局部性原理，FastPersist减少了不必要的数据移动，降低了I/O操作的延迟。依赖性管理：在操作重叠的过程中，FastPersist通过精确管理计算任务和检查点写入操作之间的数据依赖性，确保了检查点的一致性和完整性。即使在发生故障的情况下，FastPersist也能够保证从最近的检查点正确恢复。</p><p></p><p>通过精心设计的操作调度策略，FastPersist实现了检查点写入操作与模型训练的其他计算任务的重叠执行，从而在不增加额外计算负担的情况下，规避检查点的写入延迟。</p><p></p><p></p><h2>效果评估</h2><p></p><p></p><p>研究团队对&nbsp;FastPersist的性能表现进行多场景、多维度的评估。为了验证NVMe&nbsp;优化和并行优化在减少检查点延迟方面的有效性，团队使用单GPU和多节点环境的微基准测试，对检查点写入的吞吐量做了测试；并使用真实世界的密集和稀疏深度学习模型，评估了新方法相比基线（baseline）对训练性能的加速效果。</p><p></p><p>在微基准测试中，FastPersist在单GPU和多节点环境下，相比于基线的torch.save()方法，检查点写入速度显著提升。</p><p></p><p>在真实世界的深度学习模型训练测试中，FastPersist在不同的模型规模和数据并行度下，均能够实现高速的检查点创建，且引入的开销极小。下图显示，在128个V100&nbsp;GPU上，FastPersist实现的加速比从gpt3-13B的28倍到gpt3-0.7B的116倍不等。这些改进证明了FastPersist&nbsp;技术方案在&nbsp;NVMe&nbsp;优化和并行优化方面的有效性。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/57/57289e6d3f967d9b5c4654f6702e1f07.png" /></p><p></p><p>图：FastPersist&nbsp;应用于GPT-3密集模型训练的效果</p><p></p><p>FastPersist在大规模训练场景下的性能尤为重要。实验结果表明，即使在数千个GPU上进行训练，FastPersist也能够保持检查点创建的低开销，并且随着数据并行度的增加，FastPersist的效率提升更加明显。</p><p></p><p>鉴于GPU硬件的限制，研究团队通过预测高达128的数据并行度（即6.7B模型使用1024个GPU，13B模型使用2048个GPU）来模拟像GPT-3&nbsp;6.7B和13B这样的大型密集模型的性能表现。下图显示了FastPersist相对于基线的预计训练加速比，其中蓝色/橙色条代表6.7B/13B模型。当扩展到数千个GPU时，FastPersist&nbsp;的检查点开销基本保持一致（小于2%的训练计算时间），而基线的检查点开销则与数据并行度成比例增长。对于6.7B和13B模型，FastPersist&nbsp;分别实现了高达10.2倍和3.6倍的训练加速。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/fb/fb046e0eb7930ec2b185d75baf3a4f1d.png" /></p><p></p><p>图：数据并行度≤128的训练加速效果预测</p><p></p><p>另外如上图中灰色条所示，如果放弃流水线并行（PP），并在一个数据并行组中完全采用&nbsp;16&nbsp;个&nbsp;GPU&nbsp;的张量并行（TP）设置，与标准TP和PP结合的模型分割（即图中的橙色条）相比，FastPersist&nbsp;可以做到更高的基线加速比，实现高达11.3倍的训练加速。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/cLFzIsK51LR29CQSZ6qv</id>
            <title>AI视频技术突破静默，让每一帧画面实现声色同步 | 大模型一周大事</title>
            <link>https://www.infoq.cn/article/cLFzIsK51LR29CQSZ6qv</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/cLFzIsK51LR29CQSZ6qv</guid>
            <pubDate></pubDate>
            <updated>Wed, 26 Jun 2024 01:44:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 视频生成模型, V2A技术, 多模态智适应
<br>
<br>
总结: 大模型的快速发展推动了视频生成模型技术的竞争，V2A技术为解决视频声音局限性提供了新方案，多模态智适应大模型在教育领域有重要应用。 </div>
                        <hr>
                    
                    <p>大模型的快节奏发展，让了解最新技术动态、积极主动学习成为每一位从业者的必修课。InfoQ研究中心期望通过每周更新大模型行业最新动态，为广大读者提供全面的行业回顾和要点分析。现在，让我们回顾过去一周的大模型重大事件吧。</p><p></p><h2>一、重点发现</h2><p></p><p>视频生成模型的发展速度令人瞩目，在人工智能领域的竞争已经达到了白热化阶段。各大厂商之间的激烈角逐不仅推动了技术的快速进步，也为整个行业营造了一个更加公正和开放的竞争环境。尽管如此，从年初令人瞩目的Sora到近期的可灵、Luma、Gen-3&nbsp;Alpha等模型，它们所生成的视频作品均未能突破声音的局限。然而，Google&nbsp;DeepMind推出的V2A技术，为这一问题提供了解决方案。从技术应用来看，V2A技术与Veo等视频生成模型的结合，将能够创造出既具有戏剧性配乐、逼真音效，又能与视频中的角色、风格完美融合的对话镜头。这一创新标志着AI视频即将告别无声时代，迎来一个充满活力、充满创新的有声世界。</p><p></p><h2>二、具体内容</h2><p></p><p></p><h3>大模型持续更新</h3><p></p><p>6月19日，中国气象局发布人工智能全球中短期预报系统“风清”、人工智能临近预报系统“风雷”和人工智能全球次季节—季节预测系统“风顺”。这三个大模型都是由中国气象局与清华大学联合攻关团队开发的。这三个大模型完成了基于国产全球大气再分析资料CRA-40、雷达观测资料、风云卫星遥感资料的训练和检验评估，显著降低了当前主流气象预报大模型对国际再分析资料的依赖，提升了自主数据源的应用效率和准确性。6月21日，Anthropic发布最新大模型Claude&nbsp;3.5&nbsp;Sonnet，拥有前代模型2倍的推理速度和1/5的调用成本，在多项评测中超过了GPT-4o。</p><p></p><h4>多模态领域</h4><p></p><p>6&nbsp;月&nbsp;17&nbsp;日，Runway公司发布了其最新力作——视频生成基础模型Gen-3&nbsp;Alpha，该模型能够生成包含丰富场景变换、多样电影风格以及精细艺术指导的视频作品。6&nbsp;月&nbsp;18&nbsp;日，松鼠Ai全新多模态智适应大模型发布会在上海召开，不仅宣布了教育大模型及系统的全方位升级，还推出了多款全新智适应教育硬件产品。在大模型方面，全新多模态智适应大模型在多模态智能错因分析与追根溯源、多模态智能人机互动、多模态智能测试与评估三大维度进行了全面迭代。在硬件方面，松鼠Ai推出了三款全新松鼠Ai智能老师——S211白鹭松鼠Ai智能老师、S139松鼠Ai智能老师以及Z29松果Ai智能老师，能给匹配不同用户需求，并全系搭载松鼠Ai最新多模态智适应教育大模型。</p><p></p><h4>开源领域</h4><p></p><p>6月15日，英伟达宣布推出&nbsp;Nemotron-4&nbsp;340B，其包含一系列开放模型，可用于生成合成数据，训练大语言模型，以及所有行业的商业应用。6月18日，潞晨&nbsp;Open-Sora&nbsp;团队在&nbsp;720p&nbsp;高清文生视频质量和生成时长上实现了突破性进展，支持无缝产出任意风格的高质量短片，模型权重和训练代码已经全面开源。6月18日，基于文本生成音效工具，ElevenLabs开源视频生成音效工具。无需寻找合适的音效，用户可以通过输入文本来生成配音，且大部分音效具有Shutterstock&nbsp;的商业授权。6月19日，B站开源了轻量级&nbsp;Index-1.9B&nbsp;系列模型，包含基座模型、对照组、对话模型、角色扮演模型等多个版本。6月19日，Hedra&nbsp;Labs发布视频生成模型Character-1的研究预览版，对多平台用户开放使用。Character-1是一款能够通过文本和图片生成说话和唱歌视频的模型，最长支持60秒的免费体验，还是一个全新的创作平台，为用户提供视频创作机会。</p><p></p><h4>科研领域</h4><p></p><p>6月16日，由上海科技大学、影眸科技以及宾夕法尼亚大学联合研发的DressCode，标志着3D服装生成技术的重大突破。作为首个全面支持CG操作，并无缝融入工业生产流程的框架，DressCode通过文本驱动的方式，能够自动生成具备卓越渲染品质、高度可编辑性、可驱动性以及仿真特性的3D服装。</p><p></p><h3>应用探索</h3><p></p><p></p><h4>新产品新应用/功能新动态</h4><p></p><p>6&nbsp;月&nbsp;17&nbsp;日，谷歌DeepMind发布了V2A技术进展，该技术可以结合视频像素和自然语言文本提示，为无声视频添加逼真的音效，能够实现同步的视听生成。6月19日，OpenAI宣布和Color&nbsp;Health&nbsp;合作，探索通过GPT-4o创建AI工具Cancer&nbsp;Copilot，帮助医生根据患者数据制定筛查和治疗计划，从而能够就癌症筛查和治疗做出更加合理的决策。6月19日，Meta发布AudioSeal，一款音频水印技术，能在音频片段中精准识别AI生成的音频内容。6月19日，月之暗面Kimi开放平台将启动Context&nbsp;Caching内测，将支持长文本大模型以及上下文缓存机制。6月19日，前小度CEO景鲲和前小度CTO朱凯华联合创立的AI创新产品公司MainFunc推出了旗下首款AI&nbsp;Agent搜索产品GenSpark。该产品是一款AI&nbsp;Agent引擎，旨在“利用AI提供更好的搜索体验”。6月19日，Luma&nbsp;AI对其视频生成模型Dream&nbsp;Machine进行了重大更新，推出了Extend功能。这项新功能允许用户在保持原有视频风格和人物特征一致性的前提下，将原本生成的5秒视频延长至10秒以上。6月20日，百度智能云的曦灵数字人平台即将经历一次重大升级。此次升级不仅优化了2D和3D数字人的生成过程，实现了成本效益和效率的双重提升，而且还在直播、短视频和对话等多种应用场景中实现了无缝集成。用户仅需提供一段简短的描述，系统便能迅速模仿人类的创意思维，仅需10分钟就能自动创造出栩栩如生的3D数字人形象。</p><p></p><h4>智能体</h4><p></p><p>6月20日，斯坦福大学研究人员研发了一款仿人机器人HumanPlus，这款机器人可以模仿人类的行为，并支持模仿动作来进行学习，例如自主叠衣服、搬运物品、弹钢琴等。</p><p></p><p>报告推荐</p><p>Sora来袭，国内发展文生视频模型的土壤如何？各公司用脚投票开闭源路线的当下，开源在大模型市场进程中的价值正在被重新定义吗？人型机器人重回视野，大模型是否助力其刷新能力上限？Devin和智能编码助手是同一条赛道上的不同节点？多家企业宣布All&nbsp;in&nbsp;AI，对市场意味着什么？答案尽在InfoQ研究中心发布的《2024&nbsp;年第&nbsp;1&nbsp;季度大模型监测报告》，关注「AI前线」公众号，回复「季度报告」免费下载，一睹为快吧~</p><p></p><p><img src="https://static001.geekbang.org/infoq/df/df2037200d792e5be89596273fdcf950.png" /></p><p></p><p></p><p>报告预告</p><p>金融行业是否找到了AGI应用的最佳路径？取得了哪些具体应用成果?&nbsp;又存在哪些难以逾越的挑战与桎梏？金融机构一定要做AGI建设吗？如何考量金融AGI应用产品的效果？欢迎大家持续关注InfoQ研究中心即将发布的《AGI在金融领域的应用实践洞察》。</p><p></p><p><img src="https://static001.geekbang.org/infoq/59/593f81e592f22792c23938ef704be173.jpeg" /></p><p></p><p></p><p></p><p></p><h4>活动推荐</h4><p></p><p>InfoQ&nbsp;将于&nbsp;8&nbsp;月&nbsp;18&nbsp;日至&nbsp;19&nbsp;日在上海举办&nbsp;AICon&nbsp;全球人工智能开发与应用大会，汇聚顶尖企业专家，深入端侧AI、大模型训练、安全实践、RAG应用、多模态创新等前沿话题。现在大会已开始正式报名，6&nbsp;月&nbsp;30&nbsp;日前可以享受&nbsp;8&nbsp;折优惠，单张门票节省&nbsp;960&nbsp;元（原价&nbsp;4800&nbsp;元），详情可联系票务经理&nbsp;13269078023&nbsp;咨询。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ff3179d061840fbb7821303961c91a65.jpeg" /></p><p></p><p></p><p>原文链接：https://aicon.infoq.cn/2024/shanghai/schedule?utm_source=wechat&amp;utm_medium=aiart2-0624</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/9PjLEHC7BKMGzGQLRzQz</id>
            <title>OpenAI一停服，国内大模型厂商抢生意“抢疯”了</title>
            <link>https://www.infoq.cn/article/9PjLEHC7BKMGzGQLRzQz</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/9PjLEHC7BKMGzGQLRzQz</guid>
            <pubDate></pubDate>
            <updated>Tue, 25 Jun 2024 11:56:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, API, 中国大陆, 大模型厂商
<br>
<br>
总结: 北京时间周二凌晨，OpenAI向不支持地区的API开发者发出停止使用通知，中国大陆等地未在支持名单中。国内大模型厂商纷纷推出迁移计划，包括智谱AI、百度智能云和零一万物，提供各种优惠政策和服务，以支持企业平稳迁移至国产大模型。硅基流动也宣布免费使用多个顶尖开源大模型，为开发者提供更全面的模型API体验。 </div>
                        <hr>
                    
                    <p>北京时间周二凌晨，陆续有包括中国大陆在内的各国和相关地区API开发者在社交媒体上表示，他们收到了来自OpenAI的邮件，表示将采取额外措施停止其不支持的地区的API使用。</p><p>&nbsp;</p><p>根据网上流传的邮件截图，OpenAI表示：“根据数据显示，你的组织有来自OpenAl目前不支持的地区的API流量。从7月9日起，我们将采取额外措施，停止来自不在OpenAI支持的国家、地区名单上的API使用。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/1c/1c814d651b4574777182df8ed8a1370d.jpeg" /></p><p>&nbsp;</p><p>在 OpenAI 给出的“支持访问国家和地区”名单上（<a href="https://platform.openai.com/docs/supported-countries">https://platform.openai.com/docs/supported-countries</a>"），中国大陆、中国香港、俄罗斯、朝鲜、叙利亚、伊朗等地均未在列。</p><p>&nbsp;</p><p>实际上，OpenAI 早先就对中国大陆地区的用户实行了注册门槛，限制了其对 ChatGPT 服务的访问权限。中国大陆的开发者群体在构建基于 OpenAI API 的衍生服务时，往往需要通过代理服务器或在海外部署反向代理机制。这不仅增加了运维成本，也无法保证服务的稳定性。</p><p>&nbsp;</p><p>OpenAI 的这一决策立刻引发了国内大模型厂商的回应，各厂商纷纷表示可以支持企业“无痛”迁移。</p><p>&nbsp;</p><p></p><h3>智谱AI：企业最低6折</h3><p></p><p></p><p>首先作出反映的的是智谱AI。当天下午一点半左右，智谱 bigmodel.cn 推出了OpenAl AP1 用户特别搬家计划，帮助用户切换至国产大模型，具体包括为开发者提供1.5亿 Token（5000万 GLM-4 +1亿 GLM-4-Air) 以及从 OpenAl 到GLM 的系列迁移培训。对于高用量客户，智谱提供与 OpenAl 使用规模对等的 Token 赠送计划(不设上限)，以及与OpenAl 对等的并发规模等。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/8d/8d16ff18535392932a96061c8262983e.png" /></p><p></p><p></p><h3>百度智能云：限时零成本迁移</h3><p></p><p></p><p>下午四点半左右，百度智能云千帆推出了大模型普惠计划，即日起为新注册企业用户提供：</p><p>&nbsp;</p><p>0元调用：</p><p>文心旗舰模型首次免费，赠送ERNIE3.5旗舰模型5000万Tokens包，主力模型ERNIE Speed/ERNIE Lite和轻量模型ERNIE Tiny持续免费；针对OpenAI迁移用户额外赠送与OpenAI使用规模对等的ERNIE3.5旗舰模型Tokens包。</p><p>0元训练：免费模型精调训练服务</p><p>0元迁移：零成本SDK迁移工具</p><p>0元服务：专家服务（迁移&amp;使用指导）</p><p>&nbsp;</p><p>不过，百度智能云表示，以上优惠活动均在2024年7月25日24点前适用。</p><p></p><h3>零一万物：Yi API 二折平替计划</h3><p></p><p>&nbsp;</p><p>随后在六点20分左右，零一万物宣布发起了“Yi API 二折平替计划”，面向 OpenAI 用户推出了平滑迁移至 Yi 系列大模型的服务。针对接入OpenAI 的不同模型的用户，零一万物一一对应地提供了高模型性能且极具性价比的替换方案。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/0b/0b4462363ecd3513a81dd74778a1e76e.jpeg" /></p><p>&nbsp;</p><p>零一万物介绍，目前注册使用 Yi API 的新客户，零一万物立即赠送 100 元额度，帮助用户完成平稳过渡；平台充值还将赠送 50% 到账额度，上不封顶，为用户提供更长线的优惠；任意充值即可享受RPM/TPM 限速直升 Tier3，直达高级别的服务质量和超快响应速度。此外，零一万物 API 还将提供Prompt 兼容调优服务支持，陪伴用户又好又快地适配 Yi 系列大模型。</p><p></p><p>零一万物表示，从模型评测成绩、API 价格等公开数据来看，对于原先接入 GPT-4o 的用户来说，无论是在模型性能、还是在使用成本方面，接入零一万物千亿参数旗舰模型 Yi-Large 都会是 “物美价廉” 的国产大模型平替方案。</p><p>&nbsp;</p><p>另外，在模型性能相近的同时，Yi-Large 的定价远低于顶配模型 GPT-4o。以 GPT-4o 的定价计算（取 Input 和 Output 均值为 Open API 价格），接入 Yi-Large 后使用成本可下降 72%。</p><p></p><p>对于原先使用 GPT-4 Turbo 的用户，零一万物也给出了平滑迁移至 Yi-Large-Turbo 的方案。零一万物表示，对比 GPT-4 Turbo 的价格，用户接入 Yi-Large-Turbo 后使用成本可下降九成以上。对于业务产品已经验证成立，需要降低成本的客户， Yi-Large-Turbo 会非常适用。此外，零一万物还可提供支持实时搜索的 Yi-Large-RAG，适用于需要结合实时信息进行推理的场景，以便用户基于自身需求选择更匹配的模型。</p><p></p><p>在 OpenAI API 中，GPT-3.5-Turbo-1106 聚焦于处理简单任务，主打快速、廉价。而零一万物提供了更高性价比的方案——中等尺寸模型 Yi-Medium 来完美承接用户需求，使用成本较 GPT-3.5-Turbo-1106 下降 66%。虽然仅为中等尺寸模型，但是 Yi-Medium 深度优化了指令遵循能力，适用于日常聊天、翻译等通用场景，非常匹配大规模应用大模型的需求。</p><p></p><h3>硅基流动：多个大模型免费使用</h3><p></p><p></p><p>AI Infra厂商硅基流动则宣布：SiliconCloud 平台的 Qwen2(7B)、GLM4(9B)、Yi1.5（9B）等顶尖开源大模型免费使用。换言之，开发者从此实现了“Token自由”。</p><p></p><p>SiliconCloud是集合主流开源大模型的一站式云服务平台，为开发者提供更快、更便宜、更全面、体验更丝滑的模型API。目前，SiliconCloud已上架包括DeepSeek-Coder-V2、Stable Diffusion 3 Medium、Qwen2、GLM-4-9B-Chat、DeepSeek V2、SDXL、InstantID在内的多种开源大语言模型、图片生成模型，支持用户自由切换符合不同应用场景的模型。同时，SiliconCloud提供开箱即用的大模型推理加速服务，为生成式AI应用带来更高效的用户体验。</p><p></p><p></p><h3>腾讯：1亿 Tokens免费赠，模型任选</h3><p></p><p></p><p>也是在晚8点左右，腾讯云宣布，即日起，新迁移企业用户可免费获得腾讯混元大模型1亿Tokens。目前，腾讯云提供混元Pro、Standard、Lite等多个不同版本和尺寸的模型，用户可任意选择。</p><p></p><p>腾讯还将为新迁移企业用户提供免费专属迁移工具和服务，让好用、易用、实用的大模型惠及更多人。该专属福利截止7月31日24点前。</p><p></p><p></p><h3>百川智能：送1 千万 token，设专家群答疑</h3><p></p><p></p><p>凌晨，百川智能也跟进宣布，免费赠送1 千万 token、Assistants API免费使用。另外，百川开设了专家技术群，表示专家随时答疑，五分钟即可完成 API迁移。</p><p></p><p>国内其他厂商是否会跟进，我们将持续为大家跟踪报道。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/SlHAKbcHDXtpHJWkbpAo</id>
            <title>华为云 AI Agent 实战：三步构建，七步优化，看智能体如何进入企业生产 | AICon</title>
            <link>https://www.infoq.cn/article/SlHAKbcHDXtpHJWkbpAo</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/SlHAKbcHDXtpHJWkbpAo</guid>
            <pubDate></pubDate>
            <updated>Tue, 25 Jun 2024 07:35:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI Agent, 企业生产场景, 大模型技术, 华为云
<br>
<br>
总结: 在AICon 北京站上，华为云aPaaS首席架构师陈星亮分享了《AI Agent 在企业生产中的技术实践》，探讨了AI Agent在企业生产场景中面临的挑战和解决方案。华为云通过实际场景的实践，采取多方面技术，解决企业引入AI生成技术的瓶颈，使得AI Agent在企业生产场景得以成功运用。华为云在内部实施AI Agent技术进入生产场景的策略，分为三个阶段和七个步骤，旨在使技术团队深入理解并有效运用AI Agent技术，同时让业务团队明确AI Agent的适用场景。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>在AICon 北京站上，InfoQ 邀请了华为云aPaaS首席架构师陈星亮，分享了《AI Agent 在企业生产中的技术实践》，本文为演讲整理，期待对你有所启发。在 8 月 18-19 日即将举办的 AICon 上海站，我们也设置了【AI Agent 技术突破与应用】专题，本专题将深入探讨 AI Agent 的当前技术现状与发展趋势，揭示其在各行业中的广泛应用和未来潜力。目前大会已进入 8 折购票最后优惠期，感兴趣的同学请锁定大会官网：<a href="https://aicon.infoq.cn/2024/shanghai/track">https://aicon.infoq.cn/2024/shanghai/track</a>"</blockquote><p></p><p></p><p>大模型技术发展浪潮下，AI Agent 成为新一代 AI 原生应用范式。当前，在问答、交互类应用中，大模型 +AI Agent 已经给用户带来新一代体验。但当 AI Agent 进入企业生产场景时，会面临新的挑战，如：企业生产场景面临专业复杂问题，AI 生成结果需具备严肃性，进行知识共享的同时又要保障知识安全。针对这些问题，华为云通过实际场景的实践，采取多方面技术，形成组合方案，解决企业引入 AI 生成技术的瓶颈，使得 AI Agent 在企业生产场景得以成功运用。</p><p></p><p>本次讨论将分为三个部分：首先，将详细分析 AI Agent 在进入企业生产场景时所面临的挑战；其次，介绍华为云在内部及项目实践中应对这些挑战的具体做法；最后，通过三个具体的企业场景案例，展望 AI Agent 在企业生产场景中的使用及其发展前景。</p><p></p><p></p><h4>AI Agent 进入企业生产场景时的挑战</h4><p></p><p></p><p>人工智能代理（AI Agent）无疑将成为新一轮技术革新的先锋。作为 AI 原生应用的典型形态，问答等交互式 AI 代理已经向我们展示了其在提供创新体验方面的巨大潜力。随着大模型技术和 AI Agent 的持续发展，我们正逐步探索通向人工通用智能（AGI）的路径，众多新兴技术方向正蓄势待发。</p><p></p><p>将 AI 代理成功融入企业并使其在各个生产环节发挥关键作用，是当前面临的一项紧迫任务。这不仅要求 AI Agent 具备更高的标准，还要求其能够满足企业特有的需求。在将大模型技术与 AI 代理结合应用于企业环境时，我们面临的主要挑战是如何获得业务员工的广泛认可，并确保其成为企业可信赖的工作伙伴。这一挑战主要包括以下四个方面：</p><p></p><p>专业性：企业场景通常涉及特定领域的专业知识，如化工、医疗、制造业等，这要求 AI Agent 必须具备相应的专业理解和能力。协作性：企业场景要求 AI Agent 能够与其他能力协同工作，并与现有的信息技术系统实现无缝集成。责任性：与鼓励创新和多样性的通用场景相比，企业场景更加强调 AI 代理输出的严肃性和可靠性。安全性：保护企业私有数据、明确权限划分和访问控制，防止员工无意中泄露敏感信息。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/6c/6c8defaac825afa803a9de50ab1da57d.png" /></p><p></p><p>要在企业场景中实现 AI Agent 的成功应用，我们必须将大模型和 AI Agent 与企业独有的知识体系及现有的信息技术系统紧密结合。以下是从业务部门员工的角度出发，对所面临的挑战进行的深入分析：</p><p></p><p>专业性：业务部门对服务效果有着更为严格的标准。例如，在客服领域，业务部门期望答复的准确率至少达到 90%，并且要求答复内容既专业又简明。业务部门员工需要直接可用的答复，而非需要进一步选择或确认的选项。协作性：目前，许多企业的专业能力仍然依赖于现有的模型和系统。为了满足企业对复杂生产场景智能化的高要求，AI Agent 必须与大模型及企业现有的网络系统实现深度协同。责任性：确保答复的严肃性、正确性及可解释性是至关重要的。AI Agent 和大模型需要解决知识更新滞后和幻觉问题，避免因知识更新不及时或关键信息的错误回答而影响业务部门员工对 AI Agent 的信任。安全性：在专注于提升专业性和责任性的同时，安全性问题不容忽视。需要警惕对大模型的注入攻击，如通过恶意问题制造死循环或诱导恶意动作，尤其是生成代码的 AI Agent 更需加强安全防护。同时，也需防范攻击者针对 AI Agent 框架本身的攻击。</p><p></p><p></p><h4>华为云在 AI Agent 的探索与实践</h4><p></p><p></p><p>华为云在内部实施 AI Agent 技术进入生产场景的策略，分为三个阶段和七个步骤，旨在使技术团队深入理解并有效运用 AI Agent 技术，同时让业务团队明确 AI Agent 的适用场景：</p><p></p><p>初阶：选择问答类 AI Agent 作为起点，使业务部门能够迅速体验到 AI Agent 的效果。通过选择合适的基础模型、Prompt 模板和进行微调，业务部门可以感受到问答类 AI Agent 带来的不同寻常的体验。中阶：引入相对复杂的 AI Agent，如客服助手、会议助手等，这些应用仍然处于办公领域。通过使用外挂知识库和大小模型的编排，可以满足特定场景的需求。高阶：针对高阶专业场景，例如设备智能巡查 AI Agent，需要根据 AI Agent 和大模型的特性，进一步增加防退化和防安全风险的技术，以确保 AI Agent 在专业领域的稳定和安全运行。</p><p></p><p>通过这一分阶段、分步骤的方法，华为云不仅促进了技术团队对 AI Agent 技术的深入理解，也帮助业务团队识别了 AI Agent 技术在不同业务场景中的应用潜力。</p><p></p><p>进一步地，华为云将这一方法平台化，使得各业务团队和技术团队都能够迅速掌握 AI Agent 技术的核心要点，并将其应用于构建定制化的智能业务场景。通过平台化的方法，不仅加速了 AI Agent 技术的普及，还为企业场景智能化推广奠定了坚实的基础。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/4f/4fce340225bbcc695bcec5c2432587b2.png" /></p><p></p><p>在 AI Agent 的技术实践中，针对企业面临的专业性、协作性、责任性和安全性挑战，以下是一些关键的技术实践：</p><p></p><p>企业词表的构建：整理和标准化专业术语和特定词汇，为 AI Agent 提供准确的语言和概念框架，以确保其在专业领域的有效沟通和问题解决能力。外挂知识库的整合：利用企业内部的文档、知识库资源，为 AI Agent 提供丰富的背景知识，增强其在特定领域的专业性和准确性。防退化机制的实施：定期对 AI Agent 进行性能评估和模型更新，以防止性能随时间退化，确保其长期稳定运行。模型编排的策略：通过模型编排，实现不同 AI 模型的优势互补，以适应多样化和复杂的业务需求。防安全风险的措施：采取一系列安全措施，包括数据保护、访问控制和安全协议，以防止潜在的安全威胁，保障 AI Agent 的安全和可靠。</p><p></p><p>通过这些关键实践，企业能够更有效地利用 AI Agent 技术，提升企业生产场景智能化水平，同时确保技术应用的安全性和可靠性。</p><p></p><h5>1. 企业词表，应对专业性的挑战</h5><p></p><p></p><p>为了使 AI Agent 能够深入理解企业生产场景中的问题，特别是行业术语和企业专用名词，建立企业词表是至关重要的一步。这一过程包括：</p><p></p><p>整理专业术语：收集和整理企业使用的行业标准词汇和企业特有业务名词。制定数据标准：为确保企业私域数据能够被 AI Agent 有效理解和使用，需要制定一套标准，指导这些数据如何被整合进大模型中。词表管理与共享：对企业词表有效管理，并在公司内部实现有序共享，减少信息错误和混乱。</p><p></p><p>企业词表的建立是一个动态的过程，需要随着企业业务的发展和行业知识的更新而不断进行调整和完善。定期的词表更新和维护是确保 AI Agent 长期有效性的关键。</p><p></p><p>通过这些措施，企业可以确保 AI Agent 在专业性方面的挑战得到有效应对，从而在企业生产场景中发挥更大的作用。</p><p></p><h5>2. 外挂知识库、防退化，应对责任性挑战</h5><p></p><p></p><p>为了应对责任性挑战，确保 AI Agent 的答复准确率和严肃性，以下措施是关键：RAG 技术的应用：采用检索增强生成（Retrieval-Augmented Generation, RAG）技术，通过结合检索和生成的方法，提升 AI Agent 的答复准确率和质量。知识库的分类与更新：对外挂知识库进行细致的分类，并根据业务需求设定不同的更新周期。自动化的更新流程和数据工程能力是确保知识库能够定期且快速更新的关键。数据飞轮的实施：通过数据飞轮机制，定期从业务中获取增量数据和用户反馈，为 AI Agent 提供持续学习的素材。这种能力对于 AI Agent 在各种场景下的表现至关重要，因为知识的及时更新是维持答复准确率的基础。增量数据与反馈的整合：迅速将收集到的增量数据和用户反馈整合到模型或知识库中，以保持 AI Agent 的知识最新和答复的相关性。业务流程的整合：将反馈数据的采集作为业务流程的一部分，使业务团队员工能够根据 AI Agent 的表现及时进行调整和优化。这种直接的反馈循环可以回流成为优化 AI Agent 性能的宝贵数据。</p><p></p><p>通过这些措施，AI Agent 不仅能够应对责任性挑战，还能够实现自我优化和持续进步，确保其在企业中的长期有效性和可靠性。</p><p></p><h5>3. 大小模型编排，应对协作性挑战</h5><p></p><p></p><p>为了应对协作性挑战，大小模型的协同工作模式发挥着重要作用。其基本原则：大模型通常擅长于理解、总结和提供高层次的指导，而小模型则更擅长于感知和执行具体的任务。在这个框架下，大模型扮演着团队领导者的角色，负责分配任务并协调团队成员的工作，共同完成复杂的任务。以下是模型编排的实例：</p><p></p><p>智能运维 Agent：企业已有的监控小模型负责感知环境并收集数据，然后将信息汇聚到大模型进行深入理解和分析。分析结果再通过现有的 IT API 执行具体的运维操作。会议助手 Agent：大模型首先理解与会人员的意图，然后调用不同的小模型和现有的 IT API 来执行会议管理、记录和后续的行动项。客服助手 Agent：在更复杂的客服场景中，大模型理解客户的意图后，将任务分配给自己、小模型以及现有的系统 API，进行更复杂的处理和响应。组合模式的应用：这些模式可以根据具体问题的复杂性进行组合，以分解问题并选择合适的大小模型和现有系统共同解决问题。</p><p></p><p>在垂域大模型尚未完全发展成熟时，通过大模型、小模型以及现有系统的组合，是一种实际可行的方法，可以有效地实现企业复杂场景的智能化。</p><p></p><p>通过大小模型的编排，企业可以更有效地利用现有的技术资源，提高 AI Agent 在协作性方面的性能，实现更高效的业务流程和决策支持。</p><p></p><h5>4. 防安全风险，应对安全性挑战</h5><p></p><p></p><p>在 AI Agent 技术实践中，确保安全性是至关重要的，特别是在应对隐私数据保护、模型交互安全和 Agent 应用安全的挑战：</p><p></p><p>隐私数据脱敏：统一明确隐私数据的种类，并为各类隐私数据提供相应的识别组件。这些组件能够准确识别出敏感数据，并根据隐私处理的基准要求，在训练和推理过程中对这些数据进行脱敏处理。隐私数据脱敏是企业私域数据处理中不可或缺的一步。模型交互安全：在使用外部大模型时，保障员工与 Agent 交互的安全性至关重要。可以通过建立模型网关对大模型进行统一管理，并构建一个安全隔离带来实现三层过滤机制，确保模型应答的安全性，防止企业内部敏感信息泄露到外部。这包括：内容安全评分：对大模型的输出结果进行内容检查和评分，持续评估大模型的能力。信息过滤网：在交互过程中，对检测到的企业敏感信息进行提醒和审计留存。业务领域可配置规则：允许企业各业务领域根据自身业务情况设置独特的审核规则。Agent 应用安全：Agent 应用的安全威胁可能在以下三个环节中产生：任务规划时：识别并防范注入攻击。任务执行前：识别恶意代码，避免破坏性攻击。Agent 框架自身：防护框架漏洞和服务越权问题。</p><p></p><p>为了确保 Agent 应用的安全，需要结合企业现有的安全技术，将 Agent 框架、运行与安全技术紧密结合。在任务规划、执行和 Agent 运行的各个环节中引入相应的安全技术。随着 AI Agent 应用的日益普及，相应的安全理论和技术也将逐步形成体系。</p><p></p><p>通过这些综合性的安全措施，企业能够确保 AI Agent 技术的安全应用，保护企业免受潜在的安全风险。</p><p></p><p>华为云在 AI Agent 进入企业生产场景的技术实践可以总结如下：</p><p></p><p>应对专业性和协作性挑战：</p><p></p><p>利用企业词表来增强 AI Agent 对专业术语和企业专有名词的理解，从而提升其对企业任务的理解能力。应用模型编排技术，实现大模型与小模型的协同工作，以及与现网应用的整合，共同解决复杂任务。应对责任性挑战：通过外挂知识库和防退化机制，确保 AI Agent 的答复准确率和严肃性，持续保持其效果，使其成为员工可信赖的作业系统。应对安全性挑战：实施数据安全措施，包括隐私数据脱敏，确保在训练和推理过程中敏感数据的安全。加强模型交互安全，通过模型网关和安全隔离带，实现内容安全评分、信息过滤和业务领域可配置规则，防止敏感信息泄露。强化 Agent 应用安全，识别和防范任务规划和执行过程中的安全威胁，结合企业现有的安全技术，形成体系化的安全防护方案。</p><p></p><p>通过这些综合性的技术实践，华为云确保 AI Agent 能够安全、可靠地融入企业生产环境，提升企业运营效率和智能化水平。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f3/f325a30d98fb9517d9288bed750235dd.png" /></p><p></p><p>华为云在 AI Agent 领域的实践基础上，为企业提供了一个全面的 AI 原生应用引擎产品，旨在简化企业项目交付过程</p><p></p><p>南向接入与模型整合：支持接入多个大模型和传统模型，通过统一的接口屏蔽了模型集成的复杂性，同时确保了整个系统的安全性。</p><p></p><p>北向提供 Agent 编排能力，使得 AI 场景应用开发人员能够更加便捷地开发和管理 AI Agent 应用。</p><p></p><p>通过模型中心、知识中心、Agent 编排中心和 AI 可信治理等组件，抽象并封装了 AI Agent 所需的众多技术能力，为企业提供了一个强大、灵活且安全的平台，以支持 AI 技术在企业项目中的有效应用和快速交付。</p><p></p><p>平台化服务：使得各 AI 场景开发团队能够复用技术能力，降低运维难度，提升整体的开发与交付效率。安全性保障：在提供强大功能的同时，注重安全性，确保企业数据和交互过程的安全性。降低技术门槛：通过平台化的工具和流程，降低了企业在 AI 应用开发上的技术门槛，使得项目团队快速有效的参与到 AI 项目的开发和交付中。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/6b/6b56d6d0514dfd8492d5fa7927760e6e.png" /></p><p></p><p></p><h4>AI Agent 在企业生产场景的运用效果和展望</h4><p></p><p></p><p>1.1基于华为云 AI 原生应用引擎平台的技术实践，我们可以看到 AI Agent 在企业生产场景中的多个应用案例。以下是三个具体的案例，展示了 AI Agent 如何助力企业实现数字化转型和提升效率。</p><p></p><p></p><h5>案例 1：客服助手</h5><p></p><p></p><p>背景：许多企业选择客服作为引入 AI Agent 的起点，因为客服领域具有较好的 IT 化基础、案例库和知识库。客服业务部门面临的主要挑战是在业务量增长的情况下，保持客服人数不变，同时提高客服人员的工作效率。客服助手 AI Agent 的引入旨在提高答复准确率，这是衡量客服场景成功的关键指标。</p><p></p><p>方案：</p><p></p><p>第一阶段：技术团队熟悉与验证，使用基础大模型结合外挂知识库，发现需要将企业的最新、最准确的知识整合到 AI Agent 中以提高答复质量。第二阶段：通过构建企业词表和人工标注，以及对大模型进行微调, 提升 AI Agent 的答复准确率到 70%。第三阶段：标注质量改进，建立标注规范，指导业务部门人员进行高质量标注，将标注工作纳入业务流程，积累高质量的标注数据，将 AI Agent 的答复准确率提升至 80%。第四阶段：持续自行优化，将作业与标注固化到客服业务流程中，利用持续的反馈数据和增量业务数据，形成作业与训练的双循环，逐步提升准确率至 90%。</p><p></p><p>成果：通过这四个阶段的实施，客服助手 AI Agent 不仅提高了答复的准确率，还通过持续优化，实现了与客服业务流程的深度整合，显著提升了客服效率和客户满意度。</p><p></p><p></p><h5>案例 2：会议纪要生成助手</h5><p></p><p></p><p>背景：在办公自动化场景中，自动生成会议纪要是企业业务部门关注的重点之一。然而，仅依赖于 AI Agent 的文本摘要能力，常常难以满足会议纪要的准确性和完整性，尤其是在识别会议重点和提取摘要方面。</p><p></p><p>方案：</p><p></p><p>引入语音识别和智能文档解析等先进技术，以提升会议纪要生成助手的效果。</p><p></p><p>语音识别转写：利用自动语音识别 (ASR) 技术，将会议中的发言转写成文本。智能文档处理：通过智能文档解析技术，提取会议议题和相关材料。结合公司词表：使用公司词表来增强对专业术语和内部用语的理解，确保发言稿的准确性。发言稿整理：对每个议题和每个人的发言进行整理，形成结构化的发言稿。发言稿切块与摘要：基于发言稿和会议材料，将发言稿分块，提炼出针对各个议题的意见。进行分段摘要，确保摘要内容的准确性和可用性。形成会议纪要：将整理好的摘要和意见整合，形成完整的会议纪要。</p><p></p><p>成果：通过上述流程，会议纪要生成助手能够提供高质量的会议纪要，不仅提高了信息的准确性和可用性，还大大减少了人工整理的工作量，提升了办公效率。</p><p></p><h5>案例 3：生产指挥助手</h5><p></p><p></p><p>背景：在企业生产环节，尤其是工业制造领域，设备智能巡检是保障生产效率和安全的关键环节。传统的巡检方法依赖人工检查，耗时且容易出错。AI Agent 的引入，可以自动化巡检流程，提高准确性和效率。</p><p></p><p>方案：</p><p></p><p>多轮理解澄清：利用 AI Agent 的多轮对话能力，逐步澄清和理解智能巡检的具体任务目标。任务分解：通过任务分解技术，将复杂任务拆解为更小的、可管理的子任务。使用预定义的常用子任务来降低任务分解的难度，提高效率。训练专门的 API 检索器（API Retriever）：提升 API 的检索准确率，确保 AI Agent 能够准确地执行 API 调用，与企业系统进行有效交互。自主规划与决策：AI Agent 需要具备自主规划能力，根据巡检结果进行决策。对识别出的问题进行分析，并指挥相应的处理措施。长上下文支持：确保 AI Agent 在多轮交互中能够维持长上下文的连贯性，以处理复杂任务。</p><p></p><p>成果：通过 AI Agent 在智能巡检中的应用，企业能够实现更加高效和准确的设备管理，减少停机时间，提高生产效率和安全性。</p><p></p><p>基于对 AI Agent 技术进入企业生产场景的挑战、技术实践和场景案例的探索和理解，我们对 AI Agent 在未来企业场景中的应用进行了展望。</p><p></p><p>企业运营环境的复杂性催生了 AI Agent 技术的多样化发展。预计至少将出现三类 AI Agent，以支持企业在人、事、物方面的智能化需求：</p><p></p><p>人 +AI：交互型 AI Agent 将逐步承担部分人力工作，提升工作效率。事 +AI：事务型 AI Agent 将替代或升级部分 IT 系统，实现企业事务流程智能化。物 +AI：面向物理设备的 AI Agent 将推动工业自动化，提升设备的智能化水平。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/29/298c321156e802837b8c7c1a48197649.png" /></p><p></p><p>随着 AI Agent 实例数量的增加，如何有效管理成千上万的 Agent 实例，保障它们之间的内容交互和事件监督，成为新的技术课题。Agent 实例之间的协同通信需求，推动了对更高效、更便捷通信协议和机制的探索。</p><p></p><p>构建一个兼容多家 Agent 运行时的管理和协同通信网络，实现不同来源 Agent 的互通互联，将是未来发展的关键。这要求制定统一的标准，确保不同 Agent 运行时能够在同一平台上高效协作。</p><p></p><p>AI Agent 技术在未来仍有广阔的发展空间。随着技术进步和市场接受度的提高，我们相信企业生产场景将逐步实现更高程度的智能化，为企业带来深远的变革和价值。</p><p></p><p>嘉宾介绍：</p><p></p><p>陈星亮，华为云 aPaaS 首席架构师，华为云软件领域专家，工科硕士，在应用软件和云服务开发方面有 20 年丰富经验，现任华为云 aPaaS 服务产品部首席架构师，负责开天 aPaaS 云服务产品的设计和研发工作。曾参加英国 VM、香港 HKT、南方电网等国内外大型 IT 实施项目。当前研究方向：平台工程、AI 原生应用、应用元数据模型等。</p><p></p><p>活动推荐：</p><p></p><p>InfoQ 将于 8 月 18 日至 19 日在上海举办 AICon 全球人工智能开发与应用大会，汇聚顶尖企业专家，深入端侧 AI、大模型训练、安全实践、RAG 应用、多模态创新等前沿话题。现在大会已开始正式报名，6 月 30 日前可以享受 8 折优惠，单张门票节省 960 元（原价 4800 元），详情可联系票务经理 13269078023 咨询。</p><p><img src="https://static001.geekbang.org/infoq/6a/6a282e480f9c9f28e7a53fa1f923030b.png" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/x8O7GZY28QmGTapi6sie</id>
            <title>天弘基金：AI Agent 在金融场景下的新应用｜FCon</title>
            <link>https://www.infoq.cn/article/x8O7GZY28QmGTapi6sie</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/x8O7GZY28QmGTapi6sie</guid>
            <pubDate></pubDate>
            <updated>Tue, 25 Jun 2024 06:03:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 天弘基金, 金融大数据模型, AI Agent, ArchSummit
<br>
<br>
总结: 天弘基金在金融领域持续进行自主研发和创新，其金融大数据模型取得显著进展，成功应用于投资研究和销售策略中。AI Agent作为人工智能领域的重要技术，面临着挑战，但在金融领域的实际应用案例中展现出巨大潜力。天弘基金在全球架构师峰会上分享了基于大模型的AI Agent技术，为金融行业的创新发展注入新动力。未来在金融行业应用AI Agent的重要性和必要性将逐渐凸显。 </div>
                        <hr>
                    
                    <p></p><blockquote>嘉宾｜平野 天弘基金人工智能部负责人编辑｜黄雯希</blockquote><p></p><p></p><p>近年来，随着国家对“科技金融”领域关注的不断加深，天弘基金以其多年积累的技术开发能力和丰富的行业经验，在大模型方面持续进行自主研发和创新。根据最新的行业数据分析，天弘基金的金融大数据模型在行业分析、问题解决深度以及金融数据时效性等关键指标上取得了显著进展，已成功应用于投资研究和销售策略中，展现出卓越的效果和领先的技术实力。</p><p></p><p>AI Agent 是人工智能领域的一项重要技术，它能够模拟人类的智能行为，执行各种任务。然而，在实践中，AI Agent 面临着诸多挑战。如何在复杂环境下进行决策，高效地处理数据，深入探索 AI Agent 的发展与实践，成为了当前人工智能领域的重要议题之一。</p><p></p><p>在日前举办的 ArchSummit 全球架构师峰会深圳站上，天弘基金算法团队负责人平野分享了其团队在金融行业内开发的基于大模型的 AI Agent， 以及 AI Agent 的核心技术和在金融领域的实际应用案例。AI Agent 通过深度学习和自然语言处理技术，能够理解和生成人类语言，进而实现与客户的自然对话、提供金融咨询、进行投资决策辅助等，为金融行业的创新发展注入新的动力。</p><p></p><p></p><blockquote>8 月 16-17 日，<a href="https://fcon.infoq.cn/2024/shanghai/">FCon 全球金融科技大会</a>"将在上海举办。本届大会由中国信通院铸基计划作为官方合作机构，将邀请国内外金融机构及金融科技公司专家分享其实践经验与深入洞察。AI Agent 智能体作为焦点话题，届时也将有多个议题分享，蚂蚁集团投研支小助技术负责人纪韩将带来《多智能体协同范式在金融产业中的应用实践》，文因互联董事长 / 创始人鲍捷博士将分享企业如何《精益地打造金融专家智能体》......大会更多演讲议题火热招募中，点击链接可查看目前的专题安排并提交议题：<a href="https://fcon.infoq.cn/2024/shanghai/">https://fcon.infoq.cn/2024/shanghai/</a>"</blockquote><p></p><p></p><p>以下是平野老师分享全文（经 InfoQ 进行不改变原意的编辑整理）</p><p></p><h3>大模型的发展现状</h3><p></p><p></p><p>大模型的出现经历了从兴奋，到质疑，再到理智对待的发展阶段。</p><p></p><p>提到 AI Agent ，实际上它是基于大模型的 Agent 技术。大模型从 2022 年底开始备受关注，到现在越来越流行，竞争也越来越激烈。</p><p></p><p><img src="https://static001.geekbang.org/infoq/68/68ab982188a97756d1ab045041f2d8d1.webp" /></p><p></p><p>在第一个阶段，大模型的出现是非常令人兴奋的， ChatGPT 在推出后的五天内就积累了 100 万用户，两个月内达到 2 亿用户，打破了史上所有 APP 用户增长速度。随后，国内各大厂也开始进入这个领域，全力投入 AI，很多从事大模型的公司如雨后春笋一样冒出来，全部卷入这个行业中来，那个时候经常听到“all in AI”。</p><p></p><p>接着是质疑阶段。媒体报道充斥着各种消息，有的说在 AIGC 时代需要庞大的算力，有的说斯坦福推出的 Alpaca 模型只需 100 美金就能训练出我们自己的大模型。这件事情也引发了大众的争议，质疑大模型经常会一本正经地胡说八道，也就是所谓的“幻觉”。另外，很多公司和研究机构投入大量的算力资源和人才资源，但真正落地的场景还在探索中，没有找到非常好的新技术的应用场景。这时候也有人质疑，大模型的成本是否太高？相关人才是否难找？</p><p></p><p>而且随着监管制度的不断完善，对大模型的伦理和相关的安全性、合规性要求也越来越高。现在大模型的发展已经进入了一个理智的阶段。根据天弘基金在全体员工的抽样调查中，发现约 25.7% 的用户已经基本上离不开大模型，这个数据还在不断增长。未来大模型在各行各业的应用会越来越多，越来越普及。</p><p>金融行业如何应用 AI Agent</p><p></p><p>要想在金融行业应用 AI Agent 首先要考虑三个问题：</p><p></p><p>第一个问题是资源和人才。作为一家金融公司，在开始做大模型时，没有像一些科技大厂一样拥有大量资源和人才。人才和资源的密度、总量都是有限的，需要选择性地进行投入，要决定哪些项目要坚持做，哪些项目要放弃。例如，数字人在很多销售领域的公司里可能很有用，但对金融的业务帮助不大，所以我们舍弃了这类看似高大上的技术。</p><p></p><p>第二个问题是研发方式。金融公司要不要直接购买第三方厂商的模型进行使用？但很多金融领域的使用场景中运用第三方的大模型是行不通的。因此，天弘更多地是采用自主研发。</p><p></p><p>第三个问题是算力。金融行业是否要应用 AI Agent？金融公司往往担心算力投入过大，像 ChatGPT 每个月的在算力上的开销至少是千万美金以上。金融公司是否需要上千张 GPU 卡才能将自己的 AI Agent 研发成功？但经过探索发现，可以用较小的成本做出有用且效果不差的模型。</p><p></p><p>此外，还要明确大模型并不能解决所有问题，大模型只是提升生产力的一种工具。它可以只是一把枪，单如果能在特定的场景下理解业务然后训练和优化大模型，那么它就能成为精准把握市场机会的狙击枪，这样的效果会比普通的大模型要好得多。</p><p></p><h3>金融行业应用 AI Agent 的原因和重要性</h3><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/09/09f7a4a6e73d3661ac0e79c3c3d92348.webp" /></p><p></p><p>举例来说，在金融行业，不同角色会遇到很多问题。作为基金经理，每天早上要看大量信息，看不完怎么办？作为交易员，突然发现光伏板块上涨，想知道原因或相关新闻怎么办？作为运营经理，发现有个热点，想快速抓住市场机会，领先发布相关营销物料，怎么办？这些问题看似简单，但并不是大模型就能完全解决的。</p><p></p><p>这时候，Al Agent 就能发挥作用。它可以在各种场景中提供实时性的数据，解决传统方法中训练时缺乏时效性的问题。那么，什么是 Al Agent 呢？</p><p></p><p>Agent 是一种能够自主决策、采取行动以达到某种目标的实体，Al Agent 主要依托 LLM 模型和具体的业务场景来调用相应的工具来完成任务目标，简单来说大模型 + 插件 + 执行流程 = Agent。如果延伸到智能体，那就还需要反思、环境感知等等多模块。通过应用 AI Agent，我们就能解决特定场景中的问题。</p><p>接下来简单介绍一下 AI Agent 的组成部分。AI Agent 主要有四个分支：Memory、Tools、Planning 和 Action。</p><p></p><p>Memory 分为长期记忆和短期记忆。短期记忆用来感知当前发生的状态，以即时决策。长期记忆则会把一些数据和内容存储在数据库或记忆系统中，供以后查询使用。查询后，可以通过预先调整来做相应的行动（Action）。</p><p></p><p>Tools 模块是 Agent 用来处理和分析数据、进行推理和决策的算法和方法。Tools 让模型和外部世界进行互联互通，既能让模型感知世界，也能让模型通过利用工具来改变外部状态。在金融领域使用工具，我们主要可以赋予模型感知金融市场实时变化的能力。例如，如果要查一个基金的数据，或在营销中查某个用户相关的购买数据，就需要调用相应的查询 API，我们称之为 Chat BI。Tools 决定了你需要使用什么 API。工具部分提供了 Agent 处理信息和执行任务的核心能力。</p><p></p><p>Planning 模块负责根据当前的目标和环境条件制定长期和短期的行动计划。这包括考虑到不确定性和可能性的计划制定，以及如何有效地达成设定的目标。规划使得 Agent 能够在复杂和动态的环境中进行有条理的行动。例如，如果我要写一个大纲，Planning 会告诉我第一步做什么，第二步做什么等等。或者，在写营销文案时，它会规划出逻辑顺序，确保步骤有条不紊地进行。此外，还有思维链（Chain of Thought，COT），这也是 Planning 的一部分。</p><p></p><p>Action 模块涉及 Agent 基于规划和当前环境状态选择和执行具体的行动或操作。这是 Agent 与外部世界交互的方式，通过执行行动来实现其目标和任务。则通过执行 Planning 规划的步骤，结合感知信息，调用合适的 Tools 来实现最终的行动目标。</p><p></p><p>那么，Al Agent 在金融领域能解决什么问题呢？它在金融领域最重要的应用场景是统一数据交互形式和多样化数据类型的交互。</p><p></p><p>Al Agent 的应用的核心是数据和交互。将不同模态和结构的数据进行交互，并通过简单直观的工具调用，以对话式的方式（例如 ChatGPT）呈现给用户，这是 Al Agent 的目标。</p><p></p><p>所以在金融领域的应用场景中，有几个重要的板块：</p><p></p><p>首先是搜索 API。像大家可能熟悉的 new Bing ，这些平台现在都采用实时检索结合大模型的方式。在金融领域，经常需要查询各种基金数据、交易数据或者实时市场行情数据等。</p><p></p><p>其次是多模态交互。在很多领域，多模态交互是很重要的。比如在视频创作、营销文案、财务报表等场景中，多模态交互可以更直观地呈现复杂数据，提升用户体验。</p><p></p><p>另外，还有 ChatBI 和工具交互，这取决于在每个业务场景中我们需要执行的具体操作以及调用的工具，然后将结果通过用户界面展示出来，进行一个用户界面的交互。</p><p></p><h3>天弘基金应用 Al Agent 的经验分享</h3><p></p><p></p><h4>金融分析模型框架</h4><p></p><p></p><p>这里简单介绍一下我们团队基于改良 Retrieval-Augmented-Generation 为基础的 Agent 框架的金融分析大模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/84/848d530b73580f626191e890247ac9b2.webp" /></p><p></p><p>首先是我们称之为改良 RAG 的一个框架，即实时检索与大模型结合的框架。这个框架在 2024 年越来越火，许多大模型公司都会选择在这个框架上做延展。其实我们在 2023 年初就开始尝试这个框架，因为它对计算的资源要求不高，而且具有实时性的效果。所以我们在 RAG 的基础上进行了改良，分为几个模块：</p><p></p><p>1.改写（Rewrite）：在检索之后拿到的知识板块是在召回的这一块，我们把传统的 RAG 改写，按 Agent 的思路进行改良，先进行多角度的分析思考，再在每个分析视角下进行问题拆解，问题改写，和工具调用。我们会对问题进行改写，以让大模型能更好地理解和回答问题。比如，我们会将复杂的问题拆解变成多个子任务，在这些子任务上进行规划，即 planning。在 planning 之后，如果有必要会进行二次改写，再通过规划后的内容进行检索 + 金融工具调用。</p><p></p><p>在金融领域里面会有很多复杂的问题。比如，哪些国家是因为经济下行不得不下调利率，而使得整个国家的经济健康发展？这样一个问题，在百度或者谷歌 Google 直接搜索都是搜不出来答案的。所以在这种情况下，需要把这个内容进行改写，把它变成子模块，进行每个子模块的搜索，再用大模型进行归纳。</p><p></p><p>2.检索（Retrieve）：多路召回 + 多触发条件 + 多索引打分。比如，提出一个问题，先进行搜索，而不是直接用大模型回答。搜索包括搜索互联网内容和天弘基金自己的内容库，这样不仅可以获取网上公开的实时数据、天弘基金内部的数据、专业研究员的市场观点以及所自己积累的这种内部语料等。</p><p></p><p>3.推理（Read）：即归纳和总结。我们通过改写完的问题检索，会得到很多信息，把得到的信息进行排序和推理，最后得到一个总结性答案，就是我所说的推理。天弘基金使用的是多槽位推理，在多个子任务中同时进行大模型推理，最后给出总结。</p><p></p><h4>框架创新</h4><p></p><p></p><p>在这个设计大模型的过程中我们也做了一些创新。比如大家常说的 COT，也就是思维链（Chain of Thought），我们在此基础上做了改进，称之为 COM，就是把 Thought 变成了 Mind。COM 的意思是将一个关于金融的复杂问题拆解成多个子问题再进行操作。通过很多尝试，我们发现，一些基础的大模型，对于你提出的金融问题，回答的结果虽然正确，却并不是我们所想要的。作为一个专业的金融研究员，希望得到的答案也是专业的。在这种情况下，所需要的不是一个普通的、正确的答案，而是一个可以帮助做出正确决策的答案。</p><p></p><p>所以我们创新了 COM，帮助我们在构建这个大模型中融入了研究员和基金经理的这种思维模式，让大模型也有这种研究的思维。</p><p></p><p>接下来，我要介绍的是在检索以后，我们做的一些召回策略。在检索完成后，我们会进行召回策略的制定。例如，我们使用多路召回和多条件触发。理解用户意图不仅限于关键词匹配，还涉及时效性和语义理解等。我们采用多种索引方式，包括向量索引、关键词匹配（如 BERTSpan）、实体识别（NER）等。</p><p></p><p>另外，我们在粗排阶段，我们进行了相关性的过滤模型优化。在召回模块中，除了实时检索的数据，我们还整合了内部数据。这些内部数据通过知识图谱（KB）系统进行连接，让 Al Agent 的回答更偏向于研究员的研究。我们结合了产业链系统，确保对行业上下游关系的全面理解。比如说，当我们要分析光伏行业时，需要了解其上下游的供应商、完整的供应链和产品承接方。这些数据怎么能结合在一起准确地出现在金融从业者的答案中呢？我们进行了 KB 内容建设。首先是打通了产业链的上下游数据。我们会自动化地处理了一部分不变的市场数据，如公司行业指标等，把它们客观地呈现出来。另外，我们还针对了一部分变化的数据，如一些分析师的观点和各个业的异动，把每一个子模块都纳入异动监测的模型中，确保这些动态数据也能及时反映在大模型的回答中。</p><p></p><p>通过这些改进，我们发现效果得到了显著提升。进行对比，会发现在金融领域，我们团队在金融行业内开发的基于大模型的 AI Agent 与 ChatGPT 几乎不相上下，甚至在某些场景下我们会回答得更出色，因为我们针对金融领域进行了专门的训练。</p><p></p><p>关于 reference，我们实际上也做了好几个版本。现在我们的 reference 主要有几个关键点。一个是确保输出的内容都是都是有源可溯的，也是真正所需要的。我们会在答案中加入 reference，标明每一句话的来源，无论是来自我们的知识库还是网上公开的内容，都会有明确的标注，并且可以查看最终的数据源。</p><p></p><h3>大模型产品解读</h3><p></p><p></p><p>最后介绍一下天弘基金的产品。刚才提到的可能是一些技术细节，在产品方面，天弘基金内部已经发布了大约七到八款大模型相关的产品，这些产品还没有对外发布。总结一下，大模型可以在一天之内做些什么？用一个时间线来串起整个大模型的产品，比如说，早上研究基金经理来到我们公司，可能需要浏览各种研报。这时候，我们有一个产品叫智汇，可以让研究员快速浏览市场上最新的研报，并且筛选出他们感兴趣的内容。天弘大模型的优势在于总结研报或 PDF 时，如果涉及投资领域，天弘基金训练后大模型可以识别文章中提到的投资标的，如最近的 AI 医疗、AI 办公和 AI 法律等领域。这是研究员非常关注的内容。</p><p></p><p><img src="https://static001.geekbang.org/infoq/11/118ebb49631e11ce4fb6f4bbd53ea5fe.webp" /></p><p></p><p>我们还按照不同的研报类型，例如行业分析，市场策略，宏观解读这些研报分类训练了不同的研报摘要模版。其次，作为研究员，在早间浏览黄金新闻时，我们根据研报进行进一步解读，我们发布了智读产品，专门针对特定的研报进行解读和提问。也就是当你看到一篇特别感兴趣的文章时，你想要深入研读它，这时候打开我们的系统，你可以提出问题，还可以对比多篇文章进行阅读。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ba/baa9ccb5738d3cbe7c7796222196586b.webp" /></p><p></p><p>接下来是“弘小助”板块，这是我们的核心之一，涵盖行业研究、市场分析和金融知识问答等多个方面的专项训练，在市场表现、行业分析、热点解读等几个方面做得相当出色。我们内部推出的产品，可以整合市场上公开的各种研报和公开的第三方数据源，以及包括内部基金经理的观点。比如，如果向这个大模型产品提问“光伏行业能否现在买入？”随后利用我们自己独创的 COM，将研究员的思维模式融入其中，通过意图分析后，为投资研究角色提供了答案，给出的回答会让提问者至少了解了最近光伏行业的市场表现。不仅限于这些问答，“弘小助”还会提供类似光伏指数这样的行业指标，同时将产业链中的信息整合进来，解释每一个异动点背后的原因，并深入分析这些异动点。接下来是“reference”，也就是我们提到的，你可以看到每一个内容的来源。另外，我们还将大模型整合到产业链系统中，使用弘小助来进行产业链异动解读、热点挖掘等，使得产业链更智能化，特别是在发现异动方面的效果明显优于以往小模型的应用。</p><p></p><p>除以上应用的落地，我们还进行了一些可能探索性工作，包括利用大模型挖掘金融中的量化因子。我们一直在思考大模型是否能帮助我们解决投资中的因子挖掘问题。之前国外有几个大的基金公司，专门从私募基金中挖掘出一批非常厉害的“大牛”，用于进行大模型挖因子的工作。天弘基金当时也在进行类似的尝试，这件事是否可行，是否能实现。我们进行了一系列的实验，其中有几个是我们自己创新的方法。在沪深 300 的股票池中进行了测试后，我们发现，与我们常规的 word count 101 算法或者最近流行的强化学习挖因子相比，大模型的效果非常显著，我们的信息系数（IC）达到了 0.0326，这比目前我们尝试的强化学习的效果还要好。从理论上讲，如果有很多个因子，需要进行组合，这其实是一个计算机难以完成的暴力求解过程。但是如果能够借助大模型的思维方式，通过某种逻辑形式将一些不必要的组合排除在外，就能够显著地缩小最终的搜索范围。</p><p></p><p>关于金融大模型，天弘基金会始终坚持业务导向，务实创新。始终坚持技术引领，前瞻探索。始终坚持创新合作，共创价值。始终坚持合规运营，敬畏风险。始终坚持成本效益，精准投入。</p><p></p><h5>活动推荐</h5><p></p><p></p><p>8 月 16-17 日，<a href="https://fcon.infoq.cn/2024/shanghai/">FCon 全球金融科技大会</a>"将在上海举办。本届大会由中国信通院铸基计划作为官方合作机构，来自工银科技、北京银行、平安银行、广发银行、中信银行、度小满、蚂蚁集团等金融机构及金融科技公司的资深专家将现身说法分享其在金融科技应用实践中的经验与深入洞察。</p><p></p><p><img src="https://static001.geekbang.org/infoq/20/20c66d071f402ec153289ac70f52fb35.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/qMbLYraHWTFoN1KwC2dA</id>
            <title>中国AGI市场—4543亿市场下的新机会 | 分析师研判</title>
            <link>https://www.infoq.cn/article/qMbLYraHWTFoN1KwC2dA</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/qMbLYraHWTFoN1KwC2dA</guid>
            <pubDate></pubDate>
            <updated>Tue, 25 Jun 2024 03:31:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 智能纪元, AGI, 中国AGI市场, 应用市场规模
<br>
<br>
总结: 本文介绍了围绕通用人工智能（AGI）的讨论，以及中国AGI市场发展研究报告中对2030年市场规模的预测和个人市场、企业市场的发展趋势。 </div>
                        <hr>
                    
                    <p>我们正站在一个全新智能纪元的路口，围绕通用人工智能（AGI），在学术界、科技界、产业界的讨论中，一部分 AGI 的神秘面纱已被揭开，但这面纱之后还有更多的未知等待着我们。</p><p>InfoQ研究中心在此背景下，经过数月的研究和众多专家的访谈，发布了《<a href="https://www.infoq.cn/minibook/6WyXxdu179Di1O75JPUM">中国&nbsp;AGI&nbsp;市场发展研究报告&nbsp;2024</a>"》。</p><p></p><p><img src="https://static001.geekbang.org/infoq/11/112056f4a24c61388383c53979268b50.png" /></p><p></p><p>InfoQ研究中心在《<a href="https://www.infoq.cn/minibook/6WyXxdu179Di1O75JPUM">中国AGI市场发展研究报告&nbsp;2024</a>"》中，预计 2030 年中国 AGI 应用市场规模将达到 4543.6 亿元人民币。2024-2027年 中国 AGI 应用市场将经历快速启动期；年增速持续走高。2028 年起，市场将进入平稳发展期，年市场增速保持在 50% 左右，并预计于 2027 年突破千亿人民币市场规模。</p><p>分市场来看，个人市场而言，目前各类&nbsp;AGI&nbsp;应用 APP 的用户数量大概在 5000 万左右，根据测算，2023 年个人市场整体规模在 135 亿元左右。使用频率和实际付费仍然是制约个人市场发展的主要因素。根据InfoQ&nbsp;2023 年 12 月发起的《<a href="https://www.infoq.cn/minibook/qKIPSWhV6BIdUrHF1tyZ">中国生成式AI开发者画像调研</a>"》结果，40.2% 的开发者还没有为生成式 AI 产品付费，38.6% 的开发者已花费金额在 500 元以下。开发者群体付费情况已是如此，放大到整个个人网民群体中，付费意愿和实际付费情况亦然。</p><p>另一方面，使用频率也和付费存在着某些内在联系，使用频率越高，相对而言，其实际付费意愿和水平会越高。但究其根本，仍然是产品功能能否满足个人市场的使用需求。</p><p>InfoQ研究中心认为，中国 AGI 应用市场规模发展将由企业市场引领主导，到 2030 年企业市场规模预计达到 3024.6 亿元人民币。2023 年，企业市场用户出于落地成本和应用效果的考虑，以及本身决策周期长的原因，AGI 应用企业市场刚刚起步。根据数智前线和百炼智能披露的相关数据，截至 2024 年 6 月 15 日，中国大模型市场共计发布中标公告 230 个，远超 2023 年全年的 190 个。InfoQ研究中心预计，自 2024 年到 2027 年，中国 AGI 应用企业市场处于快速启动期，年增速均在 100% 以上。同时伴随着企业市场对应用成果和落地路径的探索，预计 2027 年开始，企业市场规模将超越个人市场规模，成为中国 AGI 应用规模发展的主导力量。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ed/ed3b53e7d2af6e6043bb30c567c7cece.png" /></p><p></p><p>在经历了近 2 年的讨论后，营销、零售、金融、教育、企服等行业是如何落地AGI的，又有哪些应用案例和难点，以上问题欢迎大家点击「<a href="https://www.infoq.cn/minibook/6WyXxdu179Di1O75JPUM">文中链接</a>"」下载完整报告。各位读者朋友也可以关注「AI前线」公众号，回复「报告」免费领取更多InfoQ研究中心重磅AI报告。也欢迎大家积极留言和讨论，分享您的见解和经验。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ctTV5uvkTlmhBQTTxlx1</id>
            <title>网络架构如何支持超万卡的大规模 AI 训练？| AICon</title>
            <link>https://www.infoq.cn/article/ctTV5uvkTlmhBQTTxlx1</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ctTV5uvkTlmhBQTTxlx1</guid>
            <pubDate></pubDate>
            <updated>Mon, 24 Jun 2024 23:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI 训练场景, 算力 Scaling, HPN7.0 架构系统, Ethernet
<br>
<br>
总结: 本文介绍了在 AI 训练场景中，算力 Scaling 的核心是网络，依赖于大规模、高性能的数据中心网络集群来实现算力的规模扩展。阿里云设计了 HPN7.0 架构系统，基于 Ethernet 来构建超大规模、极致性能的网络互联。 </div>
                        <hr>
                    
                    <p>AI 训练场景的算力 Scaling 核心是网络，依赖于大规模、高性能的数据中心网络集群来实现算力的规模扩展，为此，阿里云设计了 HPN7.0 架构系统，基于 Ethernet 来构建超大规模、极致性能的网络互联。</p><p></p><p>本文整理自阿里巴巴资深网络架构师席永青在 AICon 2024 北京【大模型基础设施构建】专题的演讲<a href="https://aicon.infoq.cn/2024/beijing/presentation/5881">“网络驱动大规模 AI 训练 - 阿里云可预期网络 HPN 7.0 架构”</a>"，内容经 InfoQ 进行不改变原意的编辑。</p><p></p><p></p><blockquote>在 8 月 18-19 日即将举办的 AICon 上海站，我们也设置了【大模型训练以及推理加速】专题，本专题将全面剖析大模型训练和推理过程中的关键技术与优化策略。目前大会已进入 8 折购票最后优惠期，感兴趣的同学请锁定大会官网：<a href="https://aicon.infoq.cn/2024/shanghai/track">https://aicon.infoq.cn/2024/shanghai/track</a>"</blockquote><p></p><p></p><p>大家好，我是席永青，来自阿里云。阿里云的 PAI 灵骏想必大家都熟悉，已经是 AI 领域的标杆算力平台，服务了众多知名的 AI 大模型公司。我有幸负责灵骏智算集群网络架构设计。今天非常高兴有机会在 AICon 这个优秀的平台上与大家交流，希望能够与各位进行深入的探讨。</p><p></p><p>我在阿里云工作已经有近十年的时间，专注于数据中心网络架构和高性能系统的设计。从 2021 年开始，我专注于 AI 智算领域，负责智算集群网络的规划演进。在大模型还未如此火热之前，阿里云就开始设计 AI 计算的万卡集群。回顾整个过程，智算最初在自动驾驶领域应用较多，许多自动驾驶客户希望通过 AI GPU 集群进行视觉模型训练，在 2021 年阿里云就非常有远见地构建了第一代万卡集群，当时我们称为 HPN 6.0。</p><p></p><p>这几年来，从网络到 GPU、机器、整个 IDC，再到平台系统和上层 AI 模型框架，AI 基础设施领域的发展速度非常快。我有两点明显的感受：第一，随着 GPT 的爆发，我们几乎每天都需要更新知识库，虽然网络是底层技术，但也需要密切关注模型发展和框架变化带来的对网络使用上的变化，也包括 GPU 硬件更新迭代对网络互联和带宽的影响等。第二，集群规模的迅速变化，从一开始的千卡 GPU 到现在万卡十万卡规模，如果没有前瞻性的技术储备和规划，基础设施将面临巨大的挑战。</p><p></p><p>我今天要分享的内容主要分为四个部分，首先我会介绍高性能网络系统的发展历程以及它目前所处的阶段。接着，我会探讨在构建大规模 GPU 集群，比如万卡甚至十万卡集群时，对于网络来讲最关键的要素是什么。接下来，我将重点介绍阿里云 HPN 7.0 架构，它是阿里云 PAI 灵骏智算集群的核心网络技术。最后，我将展望以 GPU 为中心的基础设施及其高性能网络系统的未来发展趋势。</p><p></p><p>在座的可能有些是网络领域的专家，有些可能是更上层的系统、AI 平台或算法的专家，还有一些可能是 GPU 领域的专家，希望在今天的分享中，我能回答大家三个问题。第一个问题是网络对于 AI 计算意味着什么，网络在整个 AI 计算系统中扮演的角色以及它的重要性。第二个问题，如果你的公司正在做 AI 模型相关工作，无论是在构建大模型平台还是自行研发大模型，基础设施网络的方向应该如何选择。第三个问题是，一旦确定了网络方向，网络方案和一些关键技术点应该如何实施。</p><p></p><h2>高性能网络系统进入可预期时代</h2><p></p><p></p><p>让我们回顾一下网络的整个发展历程。在 2000 年左右，互联网刚刚兴起时，网络主要是由设备供应商提供的基础设施，用于支撑 IT 业务系统。那时，数据中心开始起步，电商业务如淘宝，搜索业务如百度、Google 等开始规模化使用数据，产生对数据中心大规模计算的需求。那时，数据中心内部主要使用 TCP 协议，那时的 TCP 能够满足算力连接服务的需求，随着摩尔定律的持续推进，CPU 不断升级，TCP 的能力也随之提升，网络并没有成为瓶颈。</p><p></p><p>随着云计算和大数据的兴起，网络进入了第二个发展阶段。在这个阶段，因为集群规模的扩大，网络的规模和稳定性要求以及带宽需求都在增加。这时，网络进入了软件定义网络（SDN）的时代，这是许多网络专业人士都熟悉的一个时代，诞生了许多新技术，也涌现了许多网络领域的创业公司。</p><p></p><p>随着云计算数据中心的进一步扩大，AI 智算时代逐渐到来。智算集群与传统云计算数据中心有很大的不同，它对网络的要求也截然不同。这也是我接下来要分享的重点，希望带大家了解为什么在 AI 数据中心中，网络如此重要，网络在其中扮演了多么关键的角色。我们目前正处于第三个阶段，这个阶段的网络技术架构的发展将决定 AI 计算规模化发展的趋势，这是接下来讨论的重点。</p><p></p><p>在讨论集群算力中网络所扮演的角色之前，我们首先需要明确 AI 基础设施的关键要求。对于 AI 基础设施来说，一个至关重要的要求是训练时间。训练时间对于业务创新至关重要，因为它直接关系到公司是否能高质量得到 AI 模型，是否能快速将产品推向市场，同时这个过程中训练时间所带来的创新迭代效应也将更加明显。</p><p></p><p>训练时间的关键因素包括模型的时间加上中断时间。其中模型训练的时间，与整体计算量有关，在模型、数据集确定的情况下，这是一个固定值，这个算力需求的总量，除以集群的算力，就是模型训练的时间。此外，还需要考虑中断时间，这可能包括模型调整、数据调整或因为故障导致的训练暂停从而从上一个 checkpoint 恢复。</p><p></p><p>集群算力与通信效率密切相关。组成 AI 训练集群的千卡、万卡 GPU 是一个整体，所有人在协同完成同一个计算的任务。我们往往通过增加 GPU 的规模来增加集群的总算力，比如从 1000 张 GPU 增加到 2000 张、4000 张，整个集群所表现出的算力是否还能保持“单 GPU 乘以 GPU 数量”的算力，这是我们通常所说的线性比。这个线性比怎么做到最优，核心是通过高性能的网络系统来实现的。如果网络出现问题，哪怕是影响到一块 GPU 的网络问题，都会导致整个集群的任务变慢或者停下来。因此，网络在“集群算力”中扮演着至关重要的角色，它不仅关系到算力的线性扩展，还直接影响到训练任务的稳定性和效率。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/5e/5e08f83cf42215b352a1b8603adbc56b.png" /></p><p></p><p>AI 计算中的通信模型与传统计算有着显著的不同。AI 计算过程是迭代，包括计算、通信、同步，然后再回到计算。以模型训练过程为例，首先将模型所需的数据加载到 GPU 上，然后 GPU 进行前向计算、反向计算，在反向计算完成后，关键的一步是同步模型收敛的梯度参数到每一个 GPU。这样，在下一轮的数据训练开始时，所有的 GPU 都能够从最新的模型参数开始迭代，这样将整个参数收敛到我们期望的结果。</p><p></p><p>在这个过程中，网络要做的核心工作对梯度进行全局同步。在每一轮的迭代计算中，都需要将梯度数据同步。而图中蓝色部分所表示的，正是网络所承担的工作。网络负责在各个 GPU 之间传输和同步这些梯度数据，确保每个 GPU 都能够接收到最新的模型参数，从而进行有效的并行计算。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/1d/1d91fd875585a0623a3e5e7cd1f5ffb1.png" /></p><p></p><p>网络在 AI 计算中的重要性体现在它对算力规模扩展的影响上。当算力规模扩大时，如果网络的线性比下降，实际体现出来的算力也会随之下降。如果我们将 GPU 的数量从 128 张增加到 1024 张、4096 张，再到 1 万张，理想情况下，只要扩展 GPU 规模，就能获得相应的算力提升。但实际情况往往并非如此。网络在梯度同步过程中需要时间，这个时间的长短直接影响到 GPU 在计算过程中的等待时间，尤其随着规模的扩展，梯度同步所需要的网络交换数据量也会变大，网络通信的时间也会变长，相当于损失了 GPU 算力。好的网络架构设计，高性能的网络系统，可以做到随着规模的增加仍然保持较好的线性比，充分发挥大规模 GPU 的算力，网络性能即规模化的算力。</p><p></p><h2>GPU 集群对网络的关键要求</h2><p></p><p></p><h3>传统网络集群设计不再适用 AI 计算</h3><p></p><p></p><p>在 AI 计算中，GPU 集群对网络有着更高的性能要求，希望网络在算力扩展过程中能够保持高效的通信。这引出了一个问题：GPU 集群对网络提出了哪些关键要求？</p><p></p><p>首先，我们可以得出一个结论，即传统的网络集群已不再适用于 AI 计算。过去 20 年左右，数据中心的核心算力主要来自 CPU。如果我们观察 CPU 系统和网络系统的组成，可以发现几个特点：CPU 系统通常是单张网卡的，从 CPU 通过 PCIe 到网卡出口，内部没有特殊的网络互联。CPU 系统的单机带宽最大到 200G 就已经足够，因为它们主要服务于 APP/Web 类型的应用，这些应用需要进行互联网访问和数据中心内机器的协同工作，处理各种流量。</p><p></p><p>GPU 网络的情况已经发生了很大变化。每个 GPU 都有自己的内部互联，例如 NVIDIA 的 A100 或 H800，它们内部的 NVLink 互联可以达到 600GB 甚至 900GB。这种内部互联与外部以太网网络集群设计之间存在耦合关系。GPU 是单机多网卡的，单机内的多张网卡之间有高速互联，单个服务器的带宽可以达到 3.2T，与通用 CPU 计算带宽相比至少有 6 到 8 倍的关系。GPU 需要使用 GPU Direct RDMA 来实现显存之间的数据迁移，并且需要超短的 RTT（往返时延）。</p><p></p><p>因此，在 AI 场景下，传统的数据中心集群设计很难发挥其作用。GPU 集群需要网络能够支持更高的带宽、更低的延迟和更高效的通信机制，以满足 AI 计算的需求。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/2a/2a0a12dae781270a6b9bd48298dc03dc.png" /></p><p></p><p>在传统的数据中心集群中，任务模式通常包括计算、存储以及客户端 / 服务器服务。这些服务之间需要建立大量的会话连接来交换数据，而这些连接的数量通常取决于用户量和负载等因素。因此，这些连接的数量很高，流量趋势会随着业务负载的变化而变化。例如，在淘宝上，网络流量的高低峰与交易高峰密切相关。</p><p></p><p>而在 AI 计算中，特别是在模型训练过程中，网络表现出的是周期性的行为。计算、通信和同步循环是连续不断的过程。例如，一个 400G 的网卡在每一轮计算迭代的通信部分可以在瞬间将网络带宽用满。</p><p></p><p>网络的任务是尽可能缩短计算的等待时间，这样，GPU 就可以更充分地发挥其 Tensor Core 的能力来进行计算任务，而不是浪费在等待数据同步上。所以在 AI 模型训练任务中，尤其是在大型 AI 模型的训练中，网络表现出的特点是高并发和高突发流量。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/94/945dabf0a77e6469537310c592f7a9d8.png" /></p><p></p><p>在讨论网络连接数量的特点时，我们可以看到通用计算和 AI 训练集群之间存在显著差异。在通用计算中，采用的通常是客户端 / 服务器模式，连接数量与用户的请求量和业务模型的设计紧密相关，可能会非常大。例如，一台服务器上可能有高达 10 万级别的 HTTP 连接。</p><p></p><p>在 AI 训练集群中，一个网卡上的连接数量却非常固定，通常只有百级别连接。从训练任务开始的那一刻起，每一轮对网络的操作都是相同的。在每个循环中，活跃的连接数量以及所需的连接数量都非常少。连接数量少在网络上可能会引起 HASH 问题，这是我在后续讨论 HPN 7.0 设计时会重点提到的一个关键问题。HASH 问题是目前网络领域在 AI 计算中需要解决的核心问题之一。简单来说，连接越多，熵就越大，在选路径时分散均衡的概率也更大。而当连接数量减少时，HASH 问题就会变得更加明显。</p><p></p><h3>AI 集群高性能网络系统关键要求</h3><p></p><p></p><p>当我们深入探讨 AI 网络系统时，如果从端到端的角度审视 AI 系统的网络构成，我们可以发现在 AI 训练过程中，有三个非常关键的组件。</p><p></p><p>集群架构设计：集群架构虽然看起来只是一张拓扑图，但实际上它决定了物理带宽的使用和路径的简化程度。这个架构直接影响到模型训练过程中的网络 HASH、时延和带宽。就像城市规划中的道路规划一样，只有设计得当，交通（在这里比喻为数据包）才能高效运行。端到端传输协议：它决定了数据包在网络中的传输效率。就好像交通网络的效率，需要每辆车都足够安全足够快，同时也要避免交通拥堵的发生。传输协议需要考虑传输效率、重传、流控等因素以确保高效传输。监控运维和资源管理系统：虽然在今天的分享中不会详细讨论，但这个系统非常关键。整个系统依赖于监控运维的能力进行快速的问题发现，性能分析，和问题解决。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/d4/d4869645ff48c4e58180c1e3f0c187ea.png" /></p><p></p><p>在 AI 计算网络设计中，如果我们将前述的三个部分进一步拆解，会发现在 AI 训练过程中，网络有四个关键点。</p><p></p><p>集群架构设计：合理的集群架构设计是重中之重。这个设计决定了带宽和规模能达到的程度，比如是连接千卡、万卡还是 10 万卡，带宽是 3.2T、6.4T 还是更大，网络层级是一层、两层还是三层，以及计算和存储的布局等。这些因素都会影响 AI 训练中迭代时间或每秒样本数。点到点传输协议：在集群设计的基础上，点到点之间需要使用最快的协议来实现梯度传输。这要求协议能够实现直接内存访问（DMA），减少拷贝操作，实现大带宽和低延迟。目前，无论是 RoCEv2 还是 IB，DMA 技术已经实现了这些能力，协议栈已经写入硬件，实现了零拷贝操作。incast 问题：在训练通信过程中，会出多对 1 的数据交互场景，这会导致尾跳网络出口成为瓶颈。如果没有有效的流控方法，这会在网络出口形成队列堆积，导致缓冲区溢出发生丢包，严重影响通信效率。流控的目标是保持缓冲区的能力足够不会溢出，同时确保流量带宽始终 100% 输出。网络 HASH 问题：由于 AI 计算流量波动大，带宽高，瞬间可以打满一个 400G 端口，但流的数量又非常少，这使得网络路径上的 HASH 不均匀的概率很大，这导致中间路径的不均衡，产生丢包、长尾，影响整体通信效率。</p><p></p><p>在 AI 训练中，长尾问题是非常明显的，它具有木桶效应。如果在一个迭代中有 1000 张卡，其中 999 张已经传输完毕，但有 1 张卡的梯度传输慢了，那么整个训练过程都要等待这张卡。因此，无论是 HASH 还是流控，目标都是补齐木桶的短板，充分利用带宽的同时降低长尾，确保整个网络能够实现高带宽、低时延和高利用率的统一状态。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/71/71311bc6b1584752bf6a6f704c7d2a71.png" /></p><p></p><h2>阿里云 HPN 7.0 架构：AI 计算网络集群架构演进</h2><p></p><p></p><p>在审视了 GPU 集群对网络的关键要求之后，让我们来探讨阿里云的 HPN 7.0 架构是如何解决这些问题的，以及它是如何提高模型训练的效率，达到更极致的性能。</p><p></p><p></p><p>阿里云从去年年初开始设计研发 HPN7.0，在去年 9 月份上线规模化，是专为 AI 设计的高性能计算集群架构。这个架构的特点是单层千卡、两层万卡，存算分离。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/2c/2cad6c220bf529583a285aa64405b592.png" /></p><p></p><p>千卡 Segment 设计：我们实现了一个设计，允许 1000 张 GPU 卡通过单层网络交换完成互联。在单层网络交换中，由于是点到点连接，因此不存在 HASH 问题。在这样一个千卡范围内，网络可以发挥出极致的性能，测试结果表明，这种设计下的计算效率是业界最优的。两层网络实现万卡规模：通过两层网络结构，我们能够支持多达十几个千卡 segment，从而实现万卡规模的网络交互。两层网络不仅减少了时延，还简化了网络连接的数量和拓扑。在三层网络结构中，端到端的网络路径数量是乘数关系，而两层网络只有两跳，简化了路径选择，提高了哈希效果。存算分离。计算流量具有明显的规律性，表现为周期性的波动，我们的目标是缩短每个波峰的持续时间，而存储流量是间歇性的数据写入和读取。为了避免存储流量对计算参数同步流量的干扰，我们在设计中将计算和存储流量分配在两个独立的网络中运行。在最近的 GTC 大会上，有关网络设计是采用一张网还是两张网的问题进行了深入探讨。北美几家主要公司的 AI 基础设施网络负责人都参与了讨论，并得出了一致的结论，即分开两张网是最佳选择，这与我们的设计原则相符合。</p><p></p><p>值得一提的是，HPN 7.0 架构，在两周前被选为国际网络顶会 SIGCOMM 的论文。SIGCOMM 是网络领域内最顶级的会议之一，每年仅收录大约 50 篇论文，这些论文都是由网络领域的全球顶尖专家的创新和实践成果。阿里云的 HPN 7.0 架构论文被选中，这具有重大意义。在 SIGCOMM 上发表关于网络架构设计的论文是相当罕见的。上一篇与网络架构相关的论文是 Google 的 Jupiter 网络，第一代 Jupiter 网络在 2015 年发布，第二代则是在 2022 年发表。而 HPN 7.0 的发布标志着 AI 领域内第一篇网络架构的国际顶会论文的诞生，会成为 AI 领域网架构设计的标杆。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/70/707d28866aad3b14044d889ca2c90866.png" /></p><p></p><p>在 HPN7.0 架构下，我们可以通过流量排布，来优化模型训练过程。从 GPU 的视角来看，在整个网络映射过程中，我们可以看到在 1 千卡的范围内，DP 过程可以在千卡范围内完成，无任何网络 HASH 导致的问题。PP 流量较少，可以让其跨越不同的 segment 进行传输。这样的设计使得带宽的利用率能够与模型训练过程紧密结合，从而实现更优的性能。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/07/078e83835d699f3fdea46216a48b7a0d.png" /></p><p></p><p>HPN 7.0 在端到端的模型训练性能上取得了显著提升，测试数据显示性能，模型端到端的性能提升超过 10%。除了软件架构的优化，HPN 7.0 的硬件和光互联系统也是其成功的关键因素。我们采用了基于阿里云自研的 51.2T 交换机，和 400G 光互联。</p><p></p><h2>GPU centric 高性能网络系统未来展望</h2><p></p><p></p><p>展望未来，高性能网络系统的发展将指向一些明确的方向，这些方向已经随着 AI 基础设施的变革而逐渐显现。从最近 GTC 的发布中，我们可以感知到这一变革的脉动。变革将涵盖从数据中心的电力设计、制冷设计，到网络互联的 scale out 和 scale up 设计等多个方面。</p><p></p><p>从物理层面来看，未来的数据中心将面临更高的功率密度。例如，以前一个机架（Rack）可能只有 20 千瓦的功率，但未来的机架可能达到 50 千瓦甚至 100 千瓦。这样的高功率密度将带来散热方面的挑战，因此，液冷技术将成为必须采用的解决方案，包括交换机在内的设备都将采用液冷技术。</p><p></p><p>GPU 之间的内部互联，如 NVLink 也将在机架内部甚至更大范围内进行扩展，以支持 scale up 的扩展需求。这种 scale up 的扩展需要与网络的 scale out 扩展紧密结合，以确保整个系统的高效性和可扩展性，这也是业界最热门的互联创新话题。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/29/29221f4de869d2b86f411e0cf2a729a3.png" /></p><p></p><p>面向未来，我们面临的规模挑战将更大。随着 scale up 网络的发展，我们可能会看到从当前的 8 卡配置扩展到 72 卡或更多，这样的扩展会对网络拓扑带来变化，从而影响 scale out 群网络架构的设计。包括通信框架、容灾设计，以及电力和物理布局等方面都将发生显著变化。这些变化指向了一个以 GPU-centric 的数据中心设计理念。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ba/ba790751ce378d732b8b09851beb0852.png" /></p><p></p><p>此外，网络技术的发展正朝着更高的单芯片交换能力迈进，未来一年内，我们有望看到阿里云的 HPN 8.0，它将是基于 100T 芯片的下一代架构。从 SCALE up 与 SCALE out 结合的架构设计、硬件设计，到液冷系统、IDC 设计的结合，端到端的 AI 基础设施发生变化，以网络设计为中心的 GPU-centric 基础设施时代已经到来。</p><p></p><p>高性能网络协议也将针对 AI 计算持续演进。为了推动这一进程，业界已经成立了超级以太网联盟（UEC），近期阿里巴巴入选该联盟决策委员会，是决策委员会中唯一的一家中国公司，接下来阿里云将在 AI 基础设施网络的高性能方向上重点投入，与各主要公司一起，共同致力于下一代更高性能网络系统的设计和开发。</p><p></p><p>活动推荐：</p><p></p><p>InfoQ 将于 8 月 18 日至 19 日在上海举办 AICon 全球人工智能开发与应用大会，汇聚顶尖企业专家，深入端侧 AI、大模型训练、安全实践、RAG 应用、多模态创新等前沿话题。现在大会已开始正式报名，6 月 30 日前可以享受 8 折优惠，单张门票节省 960 元（原价 4800 元），详情可联系票务经理 13269078023 咨询。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e1/e13ff2745ce7d222e772163324f836c4.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/1rDSSpvbNkcQD4xeMvBZ</id>
            <title>揭秘大模型技术在快手搜索的应用 | QCon</title>
            <link>https://www.infoq.cn/article/1rDSSpvbNkcQD4xeMvBZ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/1rDSSpvbNkcQD4xeMvBZ</guid>
            <pubDate></pubDate>
            <updated>Mon, 24 Jun 2024 10:20:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 快手搜索部门, 大模型技术, 多模态技术, 智能问答
<br>
<br>
总结: 本文介绍了快手搜索部门技术专家在 QCon 2024 北京分享的大模型技术在快手搜索中的应用。演讲重点探讨了大模型技术的具体应用，特别是多模态技术的最新科研进展，以及大模型在智能问答领域的落地实践。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>本文整理自快手搜索部门技术专家许坤在<a href="https://qcon.infoq.cn/2024/beijing">QCon&nbsp;2024&nbsp;北京</a>"的分享“<a href="https://qcon.infoq.cn/2024/beijing/presentation/5751">大模型技术在快手搜索的应用</a>"”。演讲深入探讨了大模型技术在快手搜索领域的具体应用，重点介绍了多模态技术，尤其是多模态理解和生成方面的最新科研进展。另外，在 8 月 18-19 日即将举办的 AICon 上海站，我们也设置了【多模态大语言模型的前沿应用与创新】专题，本专题将聚焦 LLM 在多模态领域的应用和创新，探讨如何将 LLM 与图像、音频、视频等多媒体数据融合，实现更智能、更自然的交互体验。目前大会已进入 8 折购票最后优惠期，感兴趣的同学请锁定大会官网：<a href="https://aicon.infoq.cn/2024/shanghai/track">https://aicon.infoq.cn/2024/shanghai/track</a>"</blockquote><p></p><p></p><p></p><blockquote>本文由 InfoQ 整理，经许坤老师授权发布。以下为演讲实录。</blockquote><p></p><p></p><p>我们在去年 3 月底至 4 月初成立了一个联合项目组，致力于大模型技术的研发。到了 8 月份，我们发布了快手的第一个大模型，命名为快意大模型。</p><p></p><p>快意大模型目前有三个不同的规模版本，分别是 13B、66B 和 175B。在去年 8 月份的评估中，我们的模型已经达到了或者说接近 GPT-3.5 的性能水平。自那以后，我们团队在内部进行了大量的迭代和优化。特别是 175B 规模的模型，目前在很多场景中，特别是在中文场景下，表现已经超过了 GPT-4。这一进步已经被实际应用到了快手的多个具体产品中，实现了技术的落地和商业价值的转化。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/2e/2e8d3da2449e955eff2213796aef67e9.png" /></p><p></p><p></p><h3>快手大模型落地场景</h3><p></p><p></p><p>快手大模型技术目前已经在多个领域进行了尝试和应用。以下是几个具体的落地实例：</p><p></p><p>AI 小快：用户在观看视频时可以通过 @AI 小快来提问有关视频理解的问题。我们的大模型会在评论区中对这些问题进行智能解答，提供用户所需的信息。智能客服：通过大模型的强大能力，智能客服能够更精准地理解用户需求，并提供更加人性化的服务。商家视频文案生成：这项服务使得我们的 ToB 用户能够更加便捷地创作文案和制作视频，提高了内容生成的效率和质量。</p><p></p><p>尽管短视频在视觉呈现上具有优势，但在某些场景下，如 how to 类查询或知识性问答，短视频内容繁多，用户需要观看完整视频才能找到答案，这实际上降低了搜索效率。此外，短视频是由人创作的，创作者与用户之间存在一定的鸿沟。在没有足够视频供给的情况下，我们希望大模型能够对用户的问题进行解答。以下是我们四个产品的具体形态：</p><p></p><p>GPT 卡片：当用户提出问题时，GPT 卡片会在搜索结果页面直接输出答案。例如，用户询问“桂花不开花是什么原因？”时，我们会利用 RAG 技术聚合视频和网页结果，直接呈现答案。AI 搜：在某些问题没有索引或视频供给的情况下，AI 搜会利用大模型在线实时生成结果，弥补 GPT 卡片的不足。这也是一种漏斗逻辑，引导用户在看完 AI 搜后，如果有后续问题，进入多轮对话场景。GPT 多轮对话：用户点击搜索框旁的 AI 图标后，会进入多轮对话场景。与 AI 搜相比，我们会重点放在多轮对话的理解上，并提供特定领域的能力，如文生图设计和朋友圈文案创作。角色聊天：在上线这些产品后，我们发现许多用户除了知识获取需求外，还有与 AI 进行交流的需求，尤其是在深夜。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b9/b9c09fd5c1b0d196a4870bafe7b36cf2.png" /></p><p></p><p></p><h3>产品实践：AI 搜 &amp; 角色聊天</h3><p></p><p></p><p></p><h4>搜索智能问答</h4><p></p><p></p><p>搜索智能问答的设计旨在提升搜索效率和补充搜索供给。</p><p></p><p>我们构建了一个框架，该框架以逻辑流程图的形式呈现。当用户提出一个查询，系统首先进行视频检索，这包括快手自有搜索流水线中的粗排、精排、个性化排序等步骤。在获取相关视频后，系统还会利用快手丰富的知识库资源对查询进行文档检索，检索到的结果将进行答案抽取，并使用生成式模型进行答案聚合。如果查询没有相关的索引资源，我们的基座模型将通过指令检索逻辑进行兜底。</p><p></p><p>在下图框架中，蓝色部分代表抽取式模型，而红色部分代表生成式模型。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b9/b9c09fd5c1b0d196a4870bafe7b36cf2.png" /></p><p></p><p>框架中还加入了一个强化学习模块，该模块与传统的大模型训练中的 RLHF 或 DPU 有所不同。我们认识到，答案的呈现形式对用户体验有显著影响。</p><p></p><p>例如，有时我们希望答案以列表形式出现，有时是图文对，有时则可能是纯文字。强化学习模块的目标是教会模型以最合适的形式回答特定类型的问题。强化学习的信号通常基于用户看到结果后的后验行为，如停留时长、后续查询搜索等。这些信号将反向传递给模型，使模型在学习过程中既能满足用户需求，也能逐步提升用户体验。</p><p></p><p>通过这种方式，我们可以形成一个闭环，使模型能够每天在线自我迭代。</p><p></p><p>在开发过程中，我们面临了三个主要挑战。</p><p></p><p>大模型的幻象：早在三年前 GPT-1 出现时，学术界就对大模型的必要性存在分歧，分为两派，一派主张走符号推理（Symbolic Reasoning）路线，瞄准大模型幻象难以解决的痛点。现在，随着 ChatGPT 等模型的效果显著，大家开始集中研究如何检测大模型幻象。在实际应用中，我们希望有一个模型或模块能够告诉系统，大模型的输出存在问题。低质索引资源影响答案准确率：在我们的系统中，落地时面临的一个严重问题是资源本身可能存在重复。例如，一个问题可能同时有正确和错误的答案，或者不同的人对同一答案的看法不同。我们如何对这些答案进行聚合，这是我们在研究中需要解决的问题。Multi-Hop 事实类问题：这类问题在检索时通常无法直接找到答案，因为它们需要进行一定的推理。</p><p></p><p>尽管大模型有一些索引资源，我们已经对这些索引的质量进行了严格控制，但仍有少数低质资源可能进入最终的排序模块。</p><p></p><p>我们观察到，绝大多数正确答案通常能够得到足够多的索引资源的支持。基于这一发现，我们构建了一个图神经网络模型。该模型的工作机制如下：它从每个文档（doc）中抽取答案，并计算每个答案被其他文档支持的程度。同时，我们还会计算答案之间的相似度，然后利用整个图的模式来判断哪个答案最有可能是正确的。这是一个常规的解决方案，它在离线测试中表现出色。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/6f/6f761bc7be56272d5fcec724f93919d4.png" /></p><p></p><p></p><h4>回答 Multi-Hop 事实类问题</h4><p></p><p></p><p>我们在线实施了一个类似“source tree”的概念。逻辑是，面对一个复杂问题时，我们需要将这个问题拆解成多个子问题。为此，我们开发了一个模块来拆解问题。拆解后，我们会针对每一个子问题进行解答。当子问题得到正确解答时，我们会进一步探索答案，直到最终解决问题。如果某个子问题没有得到解答，我们会退回到问题的根节点，并寻找另一条路径。有时如果问题确实无法解答，我们也会接受这一现实。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/00/00e08b8bc45783f62d9f447e75bc8243.png" /></p><p></p><p></p><h3>升级到角色聊天模型</h3><p></p><p></p><p>自去年以来，随着 AI 技术的火爆以及国内资本市场的变化，我们观察到市场对角色聊天这一概念非常认可。用户不仅需要获取信息，他们的情感需求也同样重要，这正是我们需要提供的价值。我们的产品框架包含三个主要部分：</p><p></p><p>角色库：用户可以与所有已存在的角色进行聊天。当前对话角色：用户与当前正在对话的角色进行互动。角色发现：用户可以在发现页寻找他们可能感兴趣的新角色。</p><p></p><p>在角色聊天领域，我们面临一个基本问题，即如何将现有的语言模型升级为角色聊天模型。虽然整体方案没有变化，包含预训练、监督训练和强化学习模块，但每个阶段使用的数据类型有所不同。在角色聊天模型中，我们主要使用了剧本数据、对话数据和人人对话数据。与机构模型使用 3T 到 6T token 的数据量相比，角色聊天模型追求的是少而精，通常 100B 到 200B 的数据量就足够了。</p><p></p><p>在指定微调阶段，基座模型预训练阶段需要几百万到上千万的指定数据。而在角色聊天中，我们关注的是三类数据：</p><p></p><p>模型是否能理解角色的含义；模型是否能理解场景的意义；模型是否具备通用能力和多轮对话能力，尤其是长上下文的处理能力。</p><p></p><p>我们特别构造了不同角色间的场景对话能力，以及长上下文对话（long SFT）的数据。虽然在搜索场景中，很多人认为 DPU 没有太大作用，但在角色聊天中情况完全不同，因为高情商的回复与低情商的回复对用户体验的影响非常大。GPT-4 在这方面也无能为力，因为它提供的是更正式的回复，与角色聊天所需的口语化回复不同，常规使用 GPT-4 进行打标的方法在角色对话中并不适用。</p><p></p><p>因此，在强化学习阶段，我们进行了很多用户模拟器的开发，并结合人工标注进行对齐，以提升模型的情商和对话质量。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/71/71f63b192609c1281a0e97ffa3485233.png" /></p><p></p><p></p><h4>挑战一：如何构建不同角色多轮对话数据</h4><p></p><p></p><p>由于我们没有大量线上数据，即使有也不一定适用。因此，我们必须从冷启动阶段开始生成数据。我们会生成数万甚至数十万的角色，然后从这些角色中两两配对，并让 GPT-4 在给定场景下生成合理的对话。接下来，我们会进行简单的人工筛选，筛选出的数据将用于训练模型。有了这个基础模型后，我们将其上线。上线后，我们会为用户提供一个功能，允许他们自己创建角色。然后，我们会从用户创建的角色中获取数据，逐步更新原始的数据集。通过这样的多次迭代，我们最终能够达到一个比较理想的效果，使模型能够更好地理解和生成符合角色特性的对话。这个过程需要不断地收集用户反馈，优化数据集，并训练模型，以实现角色聊天功能的最佳表现。</p><p></p><p></p><h4>挑战二：如何增强模型的上下文理解能力</h4><p></p><p></p><p>众所周知，GPT 或 Transformer 这类模型框架在进行 NSP（Next Sentence Prediction）任务时，通常是预测下一个 token，这种预测往往依赖于局部信息，而不太涉及全局信息。为了增强模型的长上下文理解能力，我们采取了以下措施：</p><p></p><p>● 代码预训练：我们加入了代码预训练数据，这样做可以天然地增强模型对于远距离注意力（attention）的效果，从而提升模型对长上下文的理解。</p><p></p><p>● 线上长对话数据：我们利用线上的长对话数据，让 GPT-4 帮助我们进行标注，以识别出哪些回复可能与前文历史紧密相关。如果发现有相关性，我们会采用拒绝采样（reject sampling）的方式，通过人工挑选来构建长上下文对话训练数据。</p><p></p><p>● 增强上下文效果：利用特别构建的数据，我们进一步增强了模型的上下文效果，使其能够更好地理解和回应长对话中的上下文信息。</p><p></p><p></p><h3>技术探索：多模态大模型</h3><p></p><p></p><p>与大语言模型（LLM）相比，多模态模型主要增加了两种模态：语音和视觉（包括图像和视频）。目前常规的方案基本上是以大模型作为基础，通过一个项目将多模态特征映射到 LLM 中的固定数量的 token 上，然后进行建模。最终，根据需要输出图像或语音，只需选择不同的解码器（decoder）即可。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/54/5498120af43b1029edb698d39ad592dd.png" /></p><p></p><p>这样的大型模型存在一个显著问题，它们经常使用所谓的"model adapter"结构。在这种结构中，视觉特征或语音特征被固定（fix），然后整个模型的训练主要集中在训练这个 adapter 上。这种做法引发了一系列问题。</p><p></p><p>● 多模态作为 prompt 的弱点：在建模过程中，多模态输入通常被当作 prompt 使用，它与随后文本的交互天生较弱。这是因为目前大多数模型都采用仅解码（decode-only）框架，导致多模态输入与模型的交互不够充分。</p><p></p><p>● 任务复杂性：当前的任务，尤其是多模态任务，非常复杂。如果将模型的视觉特征抽取或 LLM 固定，那么 adapter 的训练潜力将非常有限。目前，adapter 主要采用 cross attention 的方式，这可能会严重限制整个模型的能力。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/2f/2f0ccfddfea157705960e3ce6631b95a.png" /></p><p></p><p>基于现有问题，我们提出了一个新的想法，即将视觉或语音视为一种外语，即另一种语言。</p><p></p><p></p><h4>“万物皆可 token”</h4><p></p><p></p><p>以 LLama 模型为例，我们的处理方式是相同的，不论是中文数据还是图像数据。我们希望将图像离散化，转换成 token，即"万物皆可 token"的理念。Token 化后的数据输入到基础模型中，对于基础模型而言，它们仅仅是一串 token，没有任何区别。这样做的好处在于我们可以随意交叉这些 token 的位置。</p><p></p><p>为了实现这一目标，我们设计了一个名为"Image Tokenizer"的组件，作用是将图像、视频或音频转换成一系列 token，然后输入到基础模型中。</p><p></p><p>我们选择使用 LLM 的原因是，LLM 已经将人类文字知识全部压缩在内，在基础之上进行推理、理解和生成任务时，它会具有天然的优势。与从头开始训练模型相比，使用 LLM 作为基础模型可以带来更好的效果，这是我们的基本动机。通过这种方式，我们可以更有效地处理多模态数据，并提升模型的整体性能。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/5e/5e29d51d51fbb1c0d8efed9fde4cd87f.png" /></p><p></p><p>我们最近有一篇论文被 ICLR 接收，论文的基本思想是，当我们处理图像时，首先将其转换成 token，与文本 Tokenizer 处理后的文本拼接在一起，然后输入到模型中。我们的模型名为 LaVIT，其输出的 loss 与语言模型相同，都是采用 ASP loss 预测下一个 token。</p><p></p><p>与之前方案的最大区别在于，我们将图像离散化，图像的每个 patch 都有一个独特的 ID，在语言模型中它就是一个语义 token，这样我们可以在 loss 上实现同质化处理。通过这种方式，无论是视频理解还是图像理解，只需将图像转换为 token 输入模型，然后让它解码成文字就可以将图像理解任务建模。</p><p></p><p>此外，我们还可以进行生成任务，比如给模型一张图片和一段文本，然后要求它输出图片。对模型来说这没有难度，因为它只是一系列 token 的输入和输出。唯一的区别在解码阶段，我们通常会选择使用 Stable Diffusion 或 DIT 等方法来进行解码，这种方法使我们能够更灵活地处理多模态数据，并在不同的任务中实现更好的性能。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/9f/9f84b74bedbb52f582767df09b4d627f.png" /></p><p></p><p>我们的 Tokenizer 设计涉及离线预训练过程，这个过程不需要文本，只需要图像。图像输入后，我们会使用 VIT（Vision Transformer）作为特征提取器，将图像分割成若干个 patch。每个 patch 都有一个对应的 embedding。</p><p></p><p>在这个基础上，我们进行 KNN（K 最近邻）检索，将这些 patch 映射到一个 Codebook 中。这个 Codebook 可以理解为我们自然语言中的词汇表，其中包含了大约 1 万到 2 万个“词汇”。有了这些词汇后，我们可以将图像中的每个区域映射成一个词。然后，我们会对编码过程使用一个解码 loss，即要求模型能够恢复出原始图像，这是一个回归 loss，具体来说是均方误差（MSE）loss。</p><p></p><p>完成这个离线预训练过程后，我们将得到一个优秀的图像编码器和解码器。编码器的作用是将图像转换成一系列的 token，而解码器的作用是将这些 token 还原成图像。解码器的基础我们采用了 Stable Diffusion，并对其做了改进，实现了动态编码。</p><p></p><p>动态编码的动机其实很简单：在很多图像中，颜色可能非常相近，比如都是红色。我们不希望模型对这类图像使用过长的 token，因为这会使训练过程变得冗长。因此，我们引入了一个名为 token selector 的组件，它会在图像中选择它认为重要的 token 进行编码。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8a/8a1cc9046f52dd785ca0095e35b04789.png" /></p><p></p><p>下图展示了视觉 Tokenizer 的效果：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a0/a09176b9135c842cf414f608e91238d6.png" /></p><p></p><p>左侧第一张图我们仅使用了 95 个 token，可以从图中观察到，因为有许多颜色是一致的，而右侧灰白部分表示我们没有选择对这些区域进行编码，我们保留的有颜色区域即是保留的 token，未保留的则是我们去掉的部分。</p><p></p><p>观察右侧的钓鱼图片，可以看到图像中包含的语义信息相当复杂，因此我们大约使用了 108 个 token 来表达。而下面那张鸟站在树上的图片，实际上只需要 79 个 token 就能够进行有效编码。</p><p></p><p>通过这种动态长度编码的方式，我们能够对图片进行更为高效的编码处理。这种编码方法在我们的模型中能够显著提升训练速度，大约可以提高 3 到 4 倍，从而使得整个模型的训练过程更加快速和高效。</p><p></p><p>图像编码完成后，接下来的步骤是将其映射到一个词表中。我们使用的是一个包含 16,000 个词汇的词表，每个词汇都代表了一个特定的含义。通过可视化，我们可以发现特定的编码，比如 13014，它代表的是人手臂的语义，而编码 2223 则学会了代表铁轨的语义。本质上，我们的过程是将图像拆解，然后进行语义聚类，之后将其与语言进行同步建模。</p><p></p><p>图像的处理也是类似的。我们把图像分解，将其中的每一部分映射到相应的语义上，并与语言的语义进行融合，输入到 LLM 中。通过这种方式，我们能够将图像和文本统一到同一个语义空间中，使得模型能够更好地理解和处理多模态数据。这种方法不仅提高了模型的效率，也增强了其处理复杂任务的能力。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/3c/3ce7cc55811656a4f6ffc44689f3c546.png" /></p><p></p><p></p><h4>多种任务尝试</h4><p></p><p></p><p>完成图像编码和词表映射的工作后，我们进行了多种任务的尝试和应用。首先，我们实现了 Image Caption 和 Visual QA 任务。用户可以直接输入一张图片，然后大模型能够生成对图片内容的描述。例如，模型能够形容图片中的景象或物体。比如，用户可以上传一张图片并提出问题，比如询问图片中有多少只斑马，模型能够理解问题并回答出具体的数字，如“有三个斑马”。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/59/593a04a40600414701383baa2010422b.png" /></p><p></p><p>在下面的图表中，我们展示了一些基准测试上的结果。这些结果是我们在去年 12 月份提交论文时的数据。当时，在多模态模型领域，BLIP-2 的效果被认为是最好的，如果大家对多模态模型有所了解，可能对这个模型会比较熟悉。然而，在我们的实验设置中，当我们使用相同规模的大约 7B 参数的基础模型时，我们的结果实际上远远超过了这个竞品。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/51/51f51f22da70299407134c57defdc559.png" /></p><p></p><p>我们的框架设计得非常通用，既可以处理图片理解任务，也可以进行图片生成。在图片生成方面，我们展示了一些效果，看起来也相当不错。坦白来讲，与当前非常受欢迎的 Mid Journey 和 Stable Diffusion 相比，我们的生成质量并不逊色。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c3/c3c903879849eb029063117420901806.png" /></p><p></p><p>我们进行了一项实验，目的是比较我们的方法与一个强有力的竞争对手 SDXl 在文本提示理解方面的差异。我们特别想知道，在采用 LLM 之后，我们是否能够更好地理解文本提示。</p><p></p><p>实验中，我们给出了一个文本提示，内容是：“桌子上有两个苹果，这两个苹果没有一个是红的，都是绿的。” 结果显示，SDXl 对这个提示的理解相对较弱，它生成的图像中既有红色的苹果也有绿色的苹果。而使用我们的方法，基于语义建模，生成的图像则非常好，准确地反映了文本提示的要求，即生成了两个都是绿色的苹果。</p><p></p><p>另一个例子是，文本提示描述了一只猫位于长椅下方的篮子里。SDXl 生成的图像在空间理解上表现不佳，因为它通常使用 CLIP 进行文本建模，与我们使用 LLM 的方法完全不同。相比之下，我们的模型明显在空间理解上做得更好，能够准确地描绘出猫在指定位置的场景。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/1d/1d3d6b0a42936d770186b45bb3c962e6.png" /></p><p></p><p>我们展示了一些文本到图像（Text to Image）的结果，与我们的结果比较接近的是 Parti 的效果，在 FID（Fréchet Inception Distance，一种评估生成图像质量的指标）这个维度上非常接近。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f0/f05cfeeaafd20a3ac4d2074ad41c4961.png" /></p><p></p><p>我们的框架非常灵活，不仅可以支持从文本生成图像（文生图），还能处理图像生成文本（图生文）、以及图像加文本或图像加图像的组合（图加文加图）。</p><p></p><p>如果我们在左边给出一张猫的图片，然后在右边给出一个文本提示，比如说“这只猫在海滩上”，我们的模型就能够生成出一张猫在海滩上的图像。如果我们想让这只猫戴上眼镜，只需在文本提示中加入这一要求，模型同样能够生成出相应的效果。这是一个图像加文本输入的例子。</p><p></p><p>我们还可以进行图像和图像的输入组合。比如，如果我们将梵高的画作和猫的图片放在一起作为输入，模型能够生成出具有梵高风格的猫的图像。同样，如果我们将一只朋克风格的狗和猫的图片放在一起，模型就能生成出朋克风格的猫的图像。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/1b/1bef78b536433719d1b89b4997fcb3ac.png" /></p><p></p><p>我们还进行了一项更复杂的实验，即文加图加文加图加文，也就是三个文本和两个图像的组合。例如，假设我们说“这是一幅画”，然后给出一张狗的图片，并希望将这只狗以那幅画的风格呈现出来，我们的模型同样能够生成这样的图像。当然，如果你有更具体的特定需求，比如需要更多的文本描述，或者想要结合两张图片、三张图片以及文本作为输入，这也是可行的。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/07/07157d3f8694e46dd455754fdf8ed32a.png" /></p><p></p><p></p><h4>Video-LaVIT 框架</h4><p></p><p></p><p>今年第一季度，我们开发了一个名为 Video-LaVIT 的框架，介绍一下它的基本思想。</p><p></p><p>在之前框架的基础上，我们进行了视频编码和解码的工作。目前，大家普遍知道 GPT 这样的框架属于较高级的结构。但在国内，许多人处理视频的方法是将其拆解成多帧，然后分别进行建模。另一种流行的方案是 Sora。</p><p></p><p>我们的工作始于 2 月 6 日，原本计划稍后再推出更新版本，但 Sora 的进展比我们快得多，并且效果显著。Sora 的方案考虑了 3D 方案，与单帧抽取方案相比，其 token 数量非常庞大。这会带来一个问题：如果有 100 万个 token，学习它们之间的 attention 关系将需要巨大的数据量和计算资源，这是我们所不具备的。</p><p></p><p>我们并没有选择 Sora 的方案，也没有选择单帧抽取方案，因为这样会丢失帧与帧之间的动作时序变化。最终，我们选择了一个从编解码领域借鉴的思路，这是一个折中的方案，旨在保留视频帧之间的时序信息，同时避免上述两种方案的缺点。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/47/47c44ebdeec0c0df8ac28b45ac4b1c60.png" /></p><p></p><p>如果对视频编码有所了解，你就知道 H.264 方案，这是一个相对传统的标准。它的基本思想是在视频编码或压缩时，将语义信息单独压缩，特别是所谓的运动向量（Motion Vectors）。这个方案的核心思想是对视频中每一帧（patch）与下一帧之间的动作变化进行建模，而像素级别的变化则被正交解耦。我们不需要对每一帧都进行单独建模，也不需要像 Sora 方案那样创建一个非常复杂的 3D token。</p><p></p><p>我们的基本方案采用了关键帧加运动向量（key frame + motion vectors）的方法。简单来说，我们会从视频中提取关键帧，然后基于这些关键帧对后续动作进行运动向量建模。这样，我们就无需保留整个视频的所有关键帧，只需保留运动向量即可。同时，这种方法也不会丢失视频的时序信息。</p><p></p><p>基于这个概念，我们设计了一个编码 Tokenizer 和解码 Detokenizer，用于将视频编码并恢复成期望的视频效果。这种方法允许我们以更高效和节省资源的方式来处理视频数据，同时保留了视频内容的核心信息和动态变化。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/5c/5cb98bb9a94ccc0a87e4d06bf59f09e9.png" /></p><p></p><p>我们的框架中新增了一个组件，称为 motion tokenizer，它的功能是将视频中的动作编码成 token，并将这些 token 输入到 Video-LaVIT 模型中。这个 motion tokenizer 的训练过程与 LaVIT 的训练过程非常相似，都是将向量通过语义编码转换成 token。具体来说，motion tokenizer 的训练方案与 LaVIT 相同，它使用 MSE loss 来进行训练，这是一个离线过程。与 LaVIT 不同的地方在于，motion tokenizer 的训练不需要文本对齐，它仅依赖视频本身即可完成训练。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/3e/3e722d459981a02356e9e5001611c0fd.png" /></p><p></p><p>我们还开发了一个解码器，目的是在视频预测阶段将关键帧和运动向量恢复成视频效果。为此，我们训练了一个名为 3D U-Net 的框架。简单来说，操作过程是将关键帧和运动向量输入到 3D U-Net 中，然后对其进行加噪处理，接着进行去噪，最终得到视频的输出效果。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/e8/e85ba0d3d71b72e0bd0b59ee5c7e743c.png" /></p><p></p><p>在离线训练 Tokenizer 的过程中，我们首先对视频进行编码，然后再次解码，以检验视频信息是否能够被有效复原。尽管我们观察到复原视频的分辨率较低（仅为 520P），因此效果并不完美，但基本的语义信息已经通过模型学习到。</p><p></p><p>我们特别在两个任务上进行了重点评估。首先，我们对图像理解（image understanding）进行了评测，发现在现有的图像理解基准测试上，我们的效果是最佳的。其次，在视频理解方面，特别是在 ActivityNet-QA 数据集上，该数据集用于衡量视频中的动作，我们的效果显著优于现有所有工作。这是因为我们对 motion 的建模非常精准，而其他许多工作往往忽略了对运动的建模。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/54/545803b1f79832f3c78c28147958c893.png" /></p><p></p><p>我们还尝试生成了较长的视频，用户只需输入一段文本或者提供一张图片，模型就能基于这张图片生成视频。在没有进行任何控制的情况下，视频的稳定性已经达到了一个相当不错的效果。这表明我们的模型在处理长视频生成任务时，即便在没有额外控制机制的情况下，也能够保持较高的稳定性和合理性。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/47/470b860aca7dd0dd83c22be93dffa086.png" /></p><p></p><p>我们制作了一个较长的视频，大约 10 秒左右。LLM 本身对输入长度没有太多限制，不过我们训练集中的大部分视频都在 6 秒左右。因为我们的训练集未曾见过更长的视频，这可能导致对后面关键帧的预测存在一些问题。但总体来说，生成的视频结果还是符合预期的。</p><p></p><p>我们的长视频是通过拼接多个几秒的视频片段来实现的。虽然与 Sora 相比，我们的效果还有一定差距，但个人认为这个差距可能不是由模型本身造成的，而可能是因为我们目前使用的数据还不够充分。我们没有使用任何闭源数据，也没有使用快手的数据，目前的效果是基于公开数据实现的。</p><p></p><p>我们的 Video-LaVIT 框架已经引起了包括 Stable Diffusion CTO 在内的一些业界人士的关注。大家对这个框架的优势有明确的认识。</p><p></p><p>与 Sora 相比，我们只需要其 1/10 的 token 即可进行建模。虽然 1/10 token 可能会在最终生成质量上带来一些损失，但它对视频的理解能力依然非常强。我们进行了一些评测，结果表明我们的效果可以与 Sora 相媲美。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/76/76e80db81b1970f851bfc6310a34b4dc.png" /></p><p></p><p>众所周知，广告领域是视频生成的一个非常重要的应用场景，包括在快手内部，我们也进行了一些广告生成的尝试。这些广告通常时长大约在 10 到 15 秒之间，这正好是我们的文生视频模型能够充分发挥作用的场景。因此，我们的模型在广告制作和视频内容生成方面具有巨大的潜力和应用价值。</p><p></p><p>活动推荐：</p><p></p><p>InfoQ 将于 8 月 18 日至 19 日在上海举办 <a href="https://aicon.infoq.cn/2024/beijing/track">AICon 全球人工智能开发与应用大会</a>"，汇聚顶尖企业专家，深入端侧AI、大模型训练、安全实践、RAG应用、多模态创新等前沿话题。现在大会已开始正式报名，6 月 30 日前可以享受 8 折优惠，单张门票节省 960 元（原价 4800 元），详情可联系票务经理 13269078023 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/51/51770673116f76b8740cfe9f1e48c1c3.png" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/IJyPIFXaYKu8ZbgMcRFK</id>
            <title>十年磨一剑，这家云巨头正在借助AI探寻发展新机遇</title>
            <link>https://www.infoq.cn/article/IJyPIFXaYKu8ZbgMcRFK</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IJyPIFXaYKu8ZbgMcRFK</guid>
            <pubDate></pubDate>
            <updated>Mon, 24 Jun 2024 09:46:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 生成式AI, 行业应用, 亚马逊云科技, 合作伙伴计划
<br>
<br>
总结: 生成式AI时代的黎明已经到来，亚马逊云科技认为未来真正能创造最大价值的将是生成式AI的行业应用。在2023亚马逊云科技中国峰会上，亚马逊全球副总裁储瑞松表示，生成式AI将以前所未有的方式改变各行各业，为全球经济贡献7万亿美元的价值。亚马逊云科技发布了生成式AI合作伙伴计划，旨在助力企业更快地应用生成式AI，打造“人工智能+”时代的竞争优势。 </div>
                        <hr>
                    
                    <p></p><p>“生成式AI时代的黎明已经来临，未来真正能创造最大价值的将是生成式AI的行业应用。”</p><p></p><p>近日，在2023亚马逊云科技中国峰会上，亚马逊全球副总裁、亚马逊云科技大中华区总裁储瑞松如是说。</p><p></p><h2>生成式AI浪潮下的行业机遇</h2><p></p><p></p><p>储瑞松表示，生成式AI时代的黎明已经到来，它将以前所未有的方式改变各行各业。麦肯锡的研究报告预测，到2030年前，生成式AI有望为全球经济贡献7万亿美元的价值，中国将凭借战略性投资分享其中的1/3。</p><p>&nbsp;</p><p>亚马逊云科技认为，未来真正能创造最大价值的将是生成式AI的行业应用。企业需要根据自身业务场景选择合适的模型，并结合企业自身的私有数据进行模型的定制，才能打造出有差异化的创新应用，解决高价值的特定行业场景的挑战，创造新的业务模式或机会。</p><p>&nbsp;</p><p>多年来，亚马逊云科技助力企业完成生成式AI及相关应用的构建。在最底层的算力层，亚马逊云科技提供来自英伟达的高性能AI芯片，以及自研的高性价比、低能耗AI芯片Trainium和Inferentia，满足客户不同的算力需求。</p><p>&nbsp;</p><p>在中间的工具层，亚马逊云科技通过Amazon Bedrock为企业提供构建生成式AI应用最便捷的模式。Amazon Bedrock可提供一系列领先的大模型选择，包括开源模型和闭源模型，并支持客户将自己的定制模型导入，以完全托管的API方式进行访问。</p><p>&nbsp;</p><p>在顶层的应用层，亚马逊云科技发布了开箱即用的企业级生成式AI助手Amazon Q，包括Q for Business和Q for Developer，为企业提供智能客服、智能导购等应用。</p><p>&nbsp;</p><p>尽管提供了从底层至上层的全链路服务，但亚马逊云科技认为，企业在落地生成式AI应用的过程中，仍有五个要素尤其值得关注，包括业务场景的选择、模型的选择、是否能够结合企业自身的私有数据进行模型的定制、是否符合负责任的AI的原则、以及对应用进行持续提升的能力。</p><p>&nbsp;</p><p>很多企业生成式AI之旅的第一站是打造面向内部的应用，因为起步成本、门槛和风险相对较低，且可以直接提高生产力，包括面向企业内部的客户评论反馈、舆情分析、财务、运营报表的分析、会议摘要、内部QA机器人、以及代码伴侣等等；在对外的场景上，B2C行业的场景应用要比B2B的行业场景应用走得更快一些，包括聊天室和客服的实时翻译、智能导购、智能客服问答、AI伴侣以及AI助教等等。</p><p></p><p><img src="https://static001.geekbang.org/infoq/41/41fa8c757d45a18de93549f026f9a246.png" /></p><p></p><p>亚马逊云科技生成式AI合作伙伴计划发布</p><p>&nbsp;</p><p>亚马逊云科技希望利用在算力、模型和框架、以及应用层面丰富的产品和服务，成为企业构建和应用生成式AI的首选。这次峰会上，亚马逊云科技推出“亚马逊云科技生成式AI合作伙伴计划”&nbsp;&nbsp;。该计划旨在助力企业更快地应用生成式AI，打造“人工智能+”时代的竞争优势。亚马逊云科技将联合生成式AI领域顶尖的3+1类合作伙伴，为企业提供全方位的模型、工具、应用和集成服务。3是指大模型提供方、工具链提供方、以及各类开箱即用的生成式AI应用和方案提供方。1是指系统集成商合作伙伴。亚马逊云科技将为加入本计划的合作伙伴提供全面的支持，投入技术专家与合作伙伴共创，帮助合作伙伴更好地将他们的创新和亚马逊云科技的服务适配和集成，并支持合作伙伴方案上架亚马逊云科技Marketplace，服务中国客户的同时触达全球客户。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/GYCdyqMlrLvbmP793M1e</id>
            <title>AI和数据库真正的大一统时代要来了？OpenAI突然收购实时分析数据公司Rockset，剑指AI内存</title>
            <link>https://www.infoq.cn/article/GYCdyqMlrLvbmP793M1e</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/GYCdyqMlrLvbmP793M1e</guid>
            <pubDate></pubDate>
            <updated>Mon, 24 Jun 2024 09:29:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, 收购, Rockset, 数据库公司
<br>
<br>
总结: OpenAI宣布收购以数据索引及查询功能而闻名的实时分析数据库公司Rockset，以整合其技术为所有产品提供基础设施支持。Rockset团队成员将加入OpenAI，现有客户将逐步离开Rockset平台。收购细节尚未披露，Rockset创立于2016年，由前Facebook工程师共同创立，提供基于云的实时分析数据库。Venkat Venkataramani担任创始人兼CEO，Dhruba Borthakur担任联合创始人兼CTO，Tudor Bosman担任架构负责人。Rockset产品不断提取和索引数据，支持推荐引擎、物流跟踪仪表板等应用。Rockset已成功筹集超过1.175亿美元资金，拥有知名客户如Meta和JetBlue。OpenAI收购Rockset是为了强化其跨产品检索基础设施，吸纳实时分析专家团队，提升AI应用的实用性和强大性。Venkataramani表示Rockset将成为OpenAI产品套件的检索基础设施，帮助解决AI应用大规模数据库难题。 </div>
                        <hr>
                    
                    <p></p><h2>OpenAI收购数据库公司Rockset</h2><p></p><p></p><p>近日，OpenAI正式宣布收购Rockset——这是一款以数据索引及查询功能而闻名的实时分析数据库。OpenAI 在其官方博客上发表的一篇文章中表示，它将整合 Rockset 的技术来“为其所有产品的基础设施提供支持”。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/2d/2da3e1c025557638a40ad97b960f7a26.png" /></p><p></p><p>Rockset 团队的成员将加入 OpenAI，而 Rockset 的现有客户也将“逐步”离开 Rockset 平台。完整文章如下：</p><p>&nbsp;</p><p></p><blockquote>AI技术有望改变个人和组织运用自身数据的方式，也正因如此，我们（OpenAI）决定收购Rockset。Rockset是一款领先的实时分析数据库，可提供国际一流的数据索引与查询功能。&nbsp;Rockset使得用户、开发人员及企业在使用AI产品及构建智能化应用程序时，能够更好地运用自身数据并访问实时信息。&nbsp;我们将整合Rockset技术以支持OpenAI的跨产品检索基础设施，收购完成后Rockset旗下卓越的团队成员也将加入OpenAI。&nbsp;OpenAI公司首席运营官Brad Lightcap介绍称，“Rockset的基础设施能够帮助企业客户将其数据转化为可操作的情报。我们很高兴能够将Rockset的底层技术整合进OpenAI产品，从而为客户提供更多助益。”&nbsp;Rockset公司CEO Venkat Venkataramani也指出，“我们很高兴加入OpenAI，通过为AI方案引入强大检索功能的形式，帮助用户、企业及开发人员得以充分利用其数据。”&nbsp;Rockset功能的整合工作已经启动，敬请期待更多后续消息。</blockquote><p></p><p>&nbsp;</p><p>此次收购中的财务条款细节尚未披露。</p><p></p><p>Rockset 由前 Facebook 工程师 Venkat Venkataramani 和 Tudor Bosman 以及数据库架构师 Dhruba Borthakur 于 2016 年共同创立，提供基于云的实时分析数据库，允许开发人员构建数据密集型应用程序。值得注意的是，这支团队构建了RocksDB，这是 Google LevelDB 的一个分支，LevelDB 是由 Jeff Dean 亲自编写的可嵌入 NoSQL 数据库。</p><p>&nbsp;</p><p>Venkat Venkataramani 担任创始人兼CEO，曾任Facebook基础设施团队的工程总监，所带领的团队为15亿用户管理在线数据服务；更早之前，Venkat在甲骨文公司担任主要技术人员，同样从事数据库工作。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/ac/ac1b0e64d3414c52f5f370b4e0316d61.png" /></p><p></p><p>Dhruba Borthakur是公司联合创始人兼CTO，他也同样在Facebook从事过数据库工作，还是Hadoop分布式文件系统的创始工程师之一，以及开源Apache HBase项目的贡献者。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/12/1269e75f385892f11e137f8dfd1c8ae6.png" /></p><p></p><p>Tudor Bosman担任公司架构负责人，他硕士毕业于斯坦福计算机系，也曾在Facebook工作过多年，是Facebook搜索引擎Unicorn的领导者，还曾在甲骨文、谷歌等公司担任软件工程师。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/7c/7c438971d6249b19d36d6d16fe644d67.png" /></p><p></p><p>多年来，Rockset 产品不断从 Kafka、MongoDB、DynamoDB 和 S3 等产品中提取和索引数据，从而实现无需预定义架构的实时查询。Rockset 使用开源 RocksDB 持久键值存储作为基础，充当 OLTP 数据库、数据湖和流媒体平台的外部二级索引。这可以加速实时分析查询并为主要事务系统提供性能隔离。</p><p>&nbsp;</p><p>Rockset 的数据库平台支持推荐引擎、物流跟踪仪表板等，以及与 OpenAI 特别相关的金融科技和电子商务等领域的聊天机器人。</p><p>&nbsp;</p><p>据Crunchbase 数据显示，在被收购之前， Rockset已成功从 Icon Ventures、Sequoia 和 Greylock 等投资者手中筹集了超过 1.175 亿美元的资金。该公司还拥有 Meta 和 JetBlue 等知名客户，这些客户将 Rockset 用作其航班延误预测聊天机器人的组件。</p><p></p><h2>OpenAI为何决定收购Rockset？</h2><p></p><p>&nbsp;</p><p>此次收购Rockset 是 OpenAI 继Global Illumination之后进行的第二笔公开收购，Global Illumination 是一家总部位于纽约的初创公司，利用人工智能构建创意工具和基础设施。</p><p>&nbsp;</p><p>OpenAI为何会收购Rockset技术？收购完成后，OpenAI 会用 Rockset 的技术构建什么？</p><p>&nbsp;</p><p>OpenAI在文章中表示收购Rockset是为其自家跨产品检索基础设施提供支持。由此可以明确看出，对实时数据的访问和处理技术已经成为当前AI军备竞赛中的重要一环。此外，OpenAI也将通过收购Rockset吸纳一支经验丰富的实时分析专家团队，为OpenAI的能力增强贡献力量。</p><p>&nbsp;</p><p>简而言之，OpenAI 是想将其内部的各个大模型“扎根”在公司的数据上，这也许可以帮助减少其大模型的幻觉或更容易对针对任意数量的业务用例对模型进行微调。</p><p>&nbsp;</p><p>Venkataramani 也在随公告发布的博客文章中给出了Rockset融入OpenAI后的发展规划预览：“像 Rockset 这样的先进检索基础设施将使 AI 应用更加强大和实用，”他写道。“Rockset 将成为 OpenAI 的一部分，并为 OpenAI 产品套件的检索基础设施提供支持。我们将帮助 OpenAI 解决 AI 应用大规模面临的数据库难题。”</p><p>&nbsp;</p><p>对于OpenAI此次的大手笔收购，有分析人士认为，这笔收购其实是从本质上说明了向量数据库无法真正地解决“人工智能内存”问题。</p><p>&nbsp;</p><p>从去年开始，与向量数据库相关的话题一直很火热，几乎每个向量数据库厂商都试图以“LLM 记忆”进行营销。但事实可能并非如此。有声音认为，向量数据库只是 LLM 的便签，可帮助用户查找一些信息。目前市面上还没有真正出现一个可重复的堆栈来将所有数据（结构化或非结构化）传输到企业需要的运营和分析存储中。</p><p>&nbsp;</p><p>人工智能需要的内存形态是一种类似于人类的记忆的东西，人类的记忆不只是记住事情，还会把这些记忆总结并将它们相互联系——在使用之前进行分析。通用实时数据库是最接近这一点的东西。</p><p>&nbsp;</p><p>OpenAI 知道这一点，并希望开发这个适合企业的堆栈。利用数据库的廉价和高效的计算来卸载一些昂贵且缓慢的人工智能模型计算是件令人兴奋的事，而OpenAI似乎正在朝着这个方向努力。</p><p>&nbsp;</p><p>此次收购也在Hacker News引发了广泛讨论。有用户认为：“RAG 更像是一个概念，而不是一个规范。RAG不会阻止在传统数据库中添加向量索引和相似性搜索技术的潮流。这证实了传统数据库（OLAP 或 OLTP）不会消失。在所有 LLM 模型背后，仍然需要数据库中真实、权威的数据，以避免（或至少最小化）幻觉问题。无论如何，人工智能需要更多程序化的方法来获取这些数据。”</p><p>&nbsp;</p><p>曾就职于甲骨文数据库公司、现任国内某开源分布式数据库公司副总裁的Pine表示：</p><p>&nbsp;</p><p></p><blockquote>“此次收购说明OpenAI这样的大模型供应商已经认识到，当大模型要在企业中落地时，要解决好两个问题：第一个是数据的实时分析问题，这就要求数据库有很高的实时性，第二个是要解决多模态向量检索问题。&nbsp;也就是说，大模型要服务企业级应用时需要一个有云原生扩展能力、能提供实时性服务和向量搜索能力的混合型实时分析数据库。而这种情况下，纯粹的向量数据库在面对海量的、时效性要求高的、非结构化数据时优势就没有那么明显了。</blockquote><p></p><p></p><h2>收购大局已定，Rockset用户需要做何准备？</h2><p></p><p>对于当前使用Rockset产品的用户来说，时间已经相当紧迫。根据该公司发布的FAQ内容来看，所有未签订合同的按月付费用户必须在2024年9月30日之前退出。虽然签约客户将有权与自己的Rockset客服团队具体协调合适的退出计划，但全体客户必须尽快为Rockset物色替代方案已经成为不争的事实。面对板上钉钉的收购，各位Rockset用户必须提前想好下一步规划。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c4/c4f5d7df67b22564af7e8d7ff3402d56.png" /></p><p></p><p>Rockset用户可以采取以下措施进行应对：</p><p>评估自己的当前使用情况及要求：最好先做到心中有数，确保在评估替代方案前了解自己需要什么，这能为我们节省大量时间。搜集功能相当或者更好的替代平台：您的业务需求可能很简单、可能极复杂，具体取决于您此前使用Rockset的方式。每种平台都有其优势和短板，请整理出平台在稳定支持您业务时至少应当具备的功能和特性，避免浪费宝贵时间评估那些根本无法满足您性能及功能需要的解决方案。着手规划迁移流程，以避免对正常运营造成干扰：无论您选择了开源方案还是商业产品，对其背后支持能力或社区建设情况的评估都至关重要。请寻找一家能手把手指导您完成概念验证的合作伙伴，或者确定您打算选择的开源产品拥有全天候活跃、足以帮助您完成故障排查的技术社区，这一切将成为顺利迁移乃至未来长久应用的必要前提。</p><p>&nbsp;</p><p>Rockset用户有哪些方案可选？</p><p></p><p>在制定下一步计划时，Rockset用户应当探索每一种替代方案的合理性，根据企业自身的特定用例与性能需求，不同平台提供的功能配伍也各有适用范围。下面几个重要选项可以作为参考：</p><p>面向实时分析SQL工作负载的开源选项：</p><p>&nbsp;</p><p><a href="https://druid.apache.org/">Apache Druid</a>": Druid是一款高性能实时分析数据库，可在大规模、高强度负载下对流式及批量数据执行亚秒级查询。<a href="https://clickhouse.com/">ClickHouse</a>": ClickHouse是一款速度出色的开源列式数据库管理系统，允许使用SQL查询实时生成数据分析报告。<a href="https://www.starrocks.io/">StarRocks</a>": 非常适合运行可扩展的JOIN查询，并可在无需非规范化管线的情况下实现实时分析。凭借开箱即用的实时数据更新支持，StarRocks能够直接在其列式存储上为可变数据提供秒级更新支持。<a href="https://doris.apache.org/">Apache Doris</a>"：Apache Doris 是一款高性能的开源实时数据仓库，支持大规模实时数据上的极速查询分析。相较于 Rockset，Apache Doris 同样支持实时数据更新、行列混存、半结构化 JSON 数据分析以及倒排索引和全文检索的能力，能满足高并发数据服务、实时报表分析、即席查询、湖仓一体以及日志存储分析等多个场景的需求。&nbsp;</p><p></p><p>面向实时分析SQL工作负载的专有（商业）托管解决方案：</p><p></p><p><a href="https://imply.io/">Imply</a>": 具有企业级服务支持的云端托管版Apache Druid。<a href="https://celerdata.com/">CelerData</a>": 云托管版StarRocks，由StarRocks项目的发起者和维护者提供支持。<a href="https://www.selectdb.com/">SelectDB</a>"：SelectDB 是基于 Apache Doris 构建的现代化数据仓库，提供了全托管的云原生实时数仓服务 SelectDB Cloud 和私有化部署模式的 SelectDB Enterprise 两种产品形态。</p><p></p><p>开源向量搜索 (VectorDB):</p><p><a href="https://weaviate.io/">Weaviate</a>": Weaviate是一款开源向量数据库，可存储对象及向量，允许将向量搜索与结构化过滤相结合，具备云原生数据库的容错性及可扩展性。<a href="https://milvus.io/">Milvus</a>": 面向下一代AI应用的云原生向量数据库及存储方案。<a href="https://qdrant.tech/">Qdrant</a>": 面向下一代AI的高性能、大规模向量数据库。</p><p>托管向量搜索 (VectorDB):</p><p><a href="https://www.singlestore.com/">SingleStore</a>": 除SQL功能之外，SingleStore还提供托管向量搜索功能，这也使其成为适合两类工作负载的综合性解决方案。<a href="https://zilliz.com/">Zilliz</a>": 作为Milvus的同门师兄弟，Zilliz提供向量搜索托管服务，在继承Milvus优势的同时提供额外的支持和维护保障。<a href="https://www.pinecone.io/">Pinecone</a>": 一套完全托管的向量搜索平台，可简化向量搜索应用程序的部署和扩展，确保高可用性及性能水平。</p><p>&nbsp;</p><p>迁移工作已经迫在眉睫，各位用户需要确保自己的关键基础设施始终保持完整及稳定运行。不同平台各有优势，需要实际开展评估以确保成功迁移。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://web.swipeinsight.app/posts/openai-acquires-rockset-to-enhance-real-time-analytics-and-retrieval-capabilities-7788">https://web.swipeinsight.app/posts/openai-acquires-rockset-to-enhance-real-time-analytics-and-retrieval-capabilities-7788</a>"</p><p><a href="https://starrocks.medium.com/rockset-is-acquired-by-openai-what-does-it-mean-for-its-users-3fa9561979d2">https://starrocks.medium.com/rockset-is-acquired-by-openai-what-does-it-mean-for-its-users-3fa9561979d2</a>"</p><p><a href="https://techcrunch.com/2024/06/21/openai-buys-rockset-to-bolster-its-enterprise-ai/">https://techcrunch.com/2024/06/21/openai-buys-rockset-to-bolster-its-enterprise-ai/</a>"</p><p><a href="https://www.singlestore.com/blog/openai-acquires-rockset/">https://www.singlestore.com/blog/openai-acquires-rockset/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/L5ZbIZbc7qrrjTlZLENo</id>
            <title>1个芯片顶英伟达3个？这个偏爱印度的创始人爆肝8年，终于等来抢英伟达泼天富贵的一天！</title>
            <link>https://www.infoq.cn/article/L5ZbIZbc7qrrjTlZLENo</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/L5ZbIZbc7qrrjTlZLENo</guid>
            <pubDate></pubDate>
            <updated>Mon, 24 Jun 2024 08:32:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI领域, Cerebras Systems, 英伟达, 高性能计算机芯片
<br>
<br>
总结: Cerebras Systems是一家专注于AI和高性能计算领域的初创公司，准备在纳斯达克证交所进行首次公开募股。该公司推出的WSE-3芯片被认为是英伟达GPU的替代品，具有强大的性能和计算能力，引起了市场的关注。与英伟达等公司竞争，展示了Cerebras Systems在AI领域的雄心和实力。 </div>
                        <hr>
                    
                    <p>据报道，在AI领域与英伟达正面竞争的高性能计算机芯片初创公司Cerebras Systems已经向美国证券监管机构提交了保密文件，准备在纳斯达克证交所开启自己的首轮公开募股（IPO）。</p><p>&nbsp;</p><p>消息最先由The Information网站传出，其中援引一位参与决策的匿名人士的发言，称IPO预计将在今年晚些时候进行。</p><p>&nbsp;</p><p>Cerebras Systems是一家专业且颇具能力的计算机芯片生产商，成立于2016年，主要面向AI及高性能计算（HPC）类工作负载。过去一年以来，该公司曾多次登上头条新闻，声称其芯片不仅比英伟达的图形处理单元更强大，而且成本效益也更加出色。今年4月，Cerebras Systems 以285 亿人民币的企业估值入选《2024·胡润全球独角兽榜》。</p><p></p><h2>凭什么跟英伟达掰手腕？</h2><p></p><p>&nbsp;</p><p>英伟达已经成长为当今世界市值最高的公司，甚至一度没有“之一”，而其背后的驱动力主要是生成式AI热潮，而这股浪潮丝毫没有放缓的迹象。随着世界各地企业争相将强大的AI工具整合进自己的系统和应用程序当中，他们开始疯狂采购GPU，并在过去一年间将英伟达的数据中心业务收入推高超400%。</p><p>&nbsp;</p><p>尽管有能力站在英伟达对面与其竞争的对手不多，但 Cerebras 正是其中之一。他们的旗舰产品、全新WSE-3处理器发布于今年3月，底子则是2021年首次亮相的前代WSE-2芯片组。</p><p>&nbsp;</p><p>Cerebras 的 WSE-3芯片被认为是英伟达强大GPU产品的替代。</p><p>&nbsp;</p><p>WSE-3 采用5纳米制程工艺，在晶体管数量上达到了惊人的4万亿，比其前代芯片多出1.4万亿个晶体管，拥有超过90万个计算核心和44 GB的片载静态随机存取存储器。外部用户可以灵活选择1.5TB、12TB、甚至高达1200TB的内存容量。</p><p>&nbsp;</p><p>根据这家初创公司的介绍，WSE-3的核心数量达到单张英伟达H100 GPU的52倍。这款芯片将作为数据中心设备CS-3的核心器件，而CS-3的尺寸与小型冰箱差不多。WSE-3芯片则跟批萨饼大小相当，还配有集成的冷却与电源传输模块。</p><p>&nbsp;</p><p>尽管在核心数量和缓存容量的增幅上并不突出，但WSE-3的性能表现却实现了质的飞跃。Cerebras WSE-3 据称峰值浮点运算速率可达125 PFLOPS（PetaFLOPS，千万亿次每秒），即一天内就能够完成Llama 700亿参数的训练任务。Cerebras表示，这样的规格足以让WSE-3与英伟达旗下最顶尖的GPU相匹敌。该公司解释称，其芯片性能卓越，能够以更快的速度、更低的功耗高效处理AI工作负载。</p><p>&nbsp;</p><p>该款芯片预计将于今年晚些时候上市。</p><p></p><h4>大模型训练：CS-3 VS B200</h4><p></p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/a5/a5d04f009a01bde31dd91cbd812ce838.png" /></p><p></p><p>Cerebras CS-3 和 B200&nbsp;对比</p><p>&nbsp;</p><p>&nbsp;</p><p>训练大型AI模型时，性能的首要决定因素是浮点性能。凭借90万个专用AI核心，Cerebras CS-3采用行业标准FP16精度，实现了125 PFLOPS 。而单个Nvidia B200 GPU是 4.4 PFLOPS，8个GPU的 DGX B200 是 36 PFLOPS。</p><p>&nbsp;</p><p>”在原始性能方面，单个CS-3相当于3.5个DGX B200服务器，但是占用的空间更小，功耗只有原来的一半，编程模型也非常简单。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/45/45fca1e1e59e55be221b6c08ffa90007.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>人工智能开发经常遇到内存限制的问题，OOM（内存不足）经常导致训练失败。万亿参数规模的模型只会加剧这个问题——需要TB级内存、数百个GPU和复杂的模型代码来管理内存和编排训练。</p><p>&nbsp;</p><p>为此，Cerebras 硬件没有采用GPU最强“辅助”HBM（High Bandwidth Memory）方式，而是采用了独特的分解内存架构，并设计了名为MemoryX的专用外部存储设备来存储权重。MemoryX 使用闪存和DRAM以及自定义软件堆栈，以最小的延迟管道加载/存储请求。</p><p>&nbsp;</p><p>“我们1200TB 超大规模 SKU 专为 GPT-5 及更高版本而设计，可训练 24 万亿参数的大模型。它的内存容量比 B200 GPU 多 6,000 倍，比 DGX B200 多 700 倍，比全机架 NVL72 多 80 倍。”该公司提到。</p><p>&nbsp;</p><p>另外，CS-3 的分解式内存架构可以将数 PB 的内存连接到单个加速器，使其在处理大型模型时具有极高的硬件效率。</p><p><img src="https://static001.geekbang.org/infoq/83/83aaaaeb78adc618be521b47ea8dcb1f.png" /></p><p></p><p>&nbsp;</p><p>高互连性能对于多芯片的高利用率至关重要。DGX B200 等 GPU 服务器是通过 NVLink 实现。NVLink 是一种专有互连，可在服务器内部的 8 个 GPU 之间提供专用链接。CS-3 互连系统则采用完全不同的技术构建：在晶圆上布线将数十万个内核连接在一起，以最低的功耗提供最高性能。</p><p>&nbsp;</p><p>“CS-3 为90万个核心提供每秒 27 PB 的总带宽，这比 1800 台 DGX B200 服务器的带宽还要高。”该公司表示。</p><p>&nbsp;</p><p>另外在上个月，Cerebras 还与桑迪亚国家实验室、劳伦斯利弗莫尔国家实验室以及洛斯阿拉莫斯国家实验室的研究人员合作，在毫秒级速度下展示了上代WSE-2硬件进行原子级材料模拟时的性能表现。在相关研究论文中，该公司提到WSE-2的性能水平惊人，模拟速度可达到配备3.9万张英伟达GPU的便于最强超级计算机Frontier的179倍。</p><p>&nbsp;</p><p>该公司产品与战略高级副总裁 And Hock 在上个月接受采访时指出，“简单堆叠任何数量的GPU都不可能获得这样的结果。我们正在根本上为分子动力学研究解锁新的时间尺度。”</p><p>&nbsp;</p><p></p><h2>创始人：公司被AMD收购后再创业</h2><p></p><p>&nbsp;</p><p>Cerebras 是一支由先驱计算机架构师、计算机科学家、深度学习研究人员以及热爱无畏工程的各类工程师组成的团队，目前已在加拿大和日本分别设立了办事处。</p><p>&nbsp;</p><p>提到这家公司的创始团队，不得不提2012年被 AMD 以 3.34 亿美元收购的微型服务器公司 SeaMicro。</p><p>&nbsp;</p><p>这次收购在当年也引发了很大关注，被评“对低功耗服务器领域来说具有颠覆性意义”，因为 SeaMicro 一直在其下一代服务器中使用英特尔芯片，SeaMicro 的网络结构允许数百个低功耗处理器协同工作。SeaMicro 架构与处理器无关，这意味着它可以快速适应 AMD 的技术。</p><p>&nbsp;</p><p>而 SeaMicro 创始人Andrew Feldman也是如今Cerebras 的联合创始人兼CEO。</p><p>&nbsp;</p><p>Andrew拥有斯坦福大学的学士学位和工商管理硕士学位。在2007年创立SeaMicro之前，Andrew是Force10 Networks的产品管理、营销和业务拓展副总裁，该公司后来以8亿美元的价格出售给戴尔。在加入Force10 Networks之前，Andrew 曾担任 RiverStone Networks 的营销和企业发展副总裁(从公司成立到2001年IPO)。</p><p>&nbsp;</p><p>值得注意的是，Andrew 认为印度是Cerebras的优先事项，理由是该国拥有巨大的工程人才、顶尖大学和不断发展的人工智能生态系统。</p><p>&nbsp;</p><p>该公司的CTO Gary Lauterbach 也是SeaMicro的联合创始人，后来也同样加入了AMD。 Gary 是计算机架构大牛，曾担任Sun SPARC Ⅲ和UltraSPARC Ⅳ微处理器的首席架构师。在Sun 实验室，他是DARPA HPCS Petascale计算项目的首席架构师，他本人拥有50多项专利。SeaMicro 微服务器领域的领先技术也离不开Gary。在SeaMicro工作期间，Gary还是美国能源部930万美元节能计算拨款的首席研究员。</p><p>&nbsp;</p><p>Andrew 和Gary 两人共事已超过12年。</p><p>&nbsp;</p><p>另一位技术负责人Sean Lie 也曾在 SeaMicro 公司担任 IO 虚拟化结构 ASIC 的首席硬件架构师。</p><p>&nbsp;</p><p>Sean 拥有麻省理工学院电子工程和计算机科学学士学位和硕士学位，并在计算机体系结构方面拥有16项专利。在SeaMicro被AMD收购后，Sean成为AMD研究员和首席数据中心架构师。早期职业生涯中，他在AMD的高级架构团队工作了五年。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/51/51f0311e10403658b09a279f136ed628.png" /></p><p></p><p>Cerebras 还聘请了有超过 24 年执行领导经验的Vinay Srinivas担任软件工程高级副总裁。</p><p>&nbsp;</p><p>Vinay 拥有印度理工学院孟买分校的学士学位以及佛罗里达大学的硕士学位和博士学位。他曾在 Synopsys（一家美国电子设计自动化公司） 工作了 12 年，离职前担任仿真产品线的工程副总裁。早前，Vinay 还曾分别在 Archpro Design Automation 、Sequence Design担任研发副总裁。</p><p>&nbsp;</p><p>首席运营官 Dhiraj Mallick 之前也曾担任SeaMicro的工程副总裁，公司被收购后他继续在AMD担任公司副总裁和服务器解决方案部门总经理。他拥有超过20年的领导经验，在加入Cerebras前是英特尔价值200亿美元的数据中心业务的首席技术官和架构副总裁。同时，Dhiraj 还担任了几家风险投资公司顾问，并拥有斯坦福大学的电气工程硕士学位。</p><p>&nbsp;</p><p>Cerebras Systems 的产品管理副总裁Andy Hock 此前是高分辨率卫星制造商Skybox Imaging的高级技术总监，该公司后来被谷歌以5亿美元收购。收购后，他继续在谷歌担任产品经理。Andy 拥有加州大学洛杉矶分校地球物理和空间物理学博士学位，在加入Skybox之前是Arete Associates的高级项目经理、业务开发主管和高级科学家。</p><p>&nbsp;</p><p></p><h2>被资本看好</h2><p></p><p>&nbsp;</p><p>考虑到英伟达这位竞争对手在过去一年间取得的令人瞩目的收益，Cerebras作为少数能够与之竞争的芯片制造商之一，自然有理由受到投资者们的热烈追捧。</p><p>&nbsp;</p><p>Constellation Rsearch公司的Holger Mueller表示，如果Cerebras真像其宣称的那样具有竞争力，完全有可能在华尔街金融市场上引发轰动。</p><p>&nbsp;</p><p>Mueller解释道，“英伟达前阵子刚刚成为全球市值最高的上市公司。面对这泼天的富贵，竞争态势也开始快速加剧，包括不少来自传统芯片行业以外的竞争对手。Cerebras确实有可能成为英伟达的潜在竞争对手，他们在芯片的制造和销售方面采取了差异化的发展路线，而且似乎有望吸引到足量资金以投入到这场耗资甚巨的AI军备竞赛当中。”</p><p>&nbsp;</p><p>截至目前，该公司已累计融资7.2亿美元，估值约为42亿-50亿美元。</p><p>&nbsp;</p><p>在其官网的投资者一栏中，还可以看到OpenAI的身影，比如Sam Altman、Greg Brockman、Ilya Sutskever等，其中 Altman曾参与Cerebras的8000万美元D轮融资，Cerebras在官网将其列在投资人的第一位。</p><p><img src="https://static001.geekbang.org/infoq/d8/d8117993919e6257df26d3d5ae309c73.png" /></p><p></p><p>在The Information的报道中，消息人士透露称为了进一步吸引投资者，Cerebras已经通知公司注册地特拉华州的监管机构，他们计划为即将到来的F1轮融资提供优先股。与上一轮融资相比，其股票发行价将有“大幅折扣”，希望借此增强上市发行的吸引力。</p><p>&nbsp;</p><p>尽管Cerebras本身对其IPO计划讳莫如深，但彭博社此前报道称，该公司已经选择花旗集团作为其上市领投银行。在与多家IPO咨询机构进行多次讨论后，Cerebras最终选择了这家银行。报道还提到，该公司的目标是最早在2024年下半年上市，且预期市值至少应高于其2021年最新一轮2.5亿美元F轮融资时对应的40亿美元估值。</p><p>&nbsp;</p><p>消息人士还在The Information报道中指出，Cerebras IPO的具体细节尚未确定，可能会根据投资者们的实际反应做出调整。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://siliconangle.com/2024/06/20/ai-chipmaker-cerebras-systems-competitor-nvidia-reportedly-files-ipo/">https://siliconangle.com/2024/06/20/ai-chipmaker-cerebras-systems-competitor-nvidia-reportedly-files-ipo/</a>"</p><p><a href="https://www.cerebras.net/blog/cerebras-cs-3-vs-nvidia-b200-2024-ai-accelerators-compared">https://www.cerebras.net/blog/cerebras-cs-3-vs-nvidia-b200-2024-ai-accelerators-compared</a>"</p><p><a href="https://www.theinformation.com/articles/cerebras-an-nvidia-challenger-files-for-ipo-confidentially?offer=rtsu-engagement-24&amp;utm_campaign=RTSU+-+Cerebras+IPO&amp;utm_content=4480&amp;utm_medium=email&amp;utm_source=cio&amp;utm_term=3006">https://www.theinformation.com/articles/cerebras-an-nvidia-challenger-files-for-ipo-confidentially?offer=rtsu-engagement-24&amp;utm_campaign=RTSU+-+Cerebras+IPO&amp;utm_content=4480&amp;utm_medium=email&amp;utm_source=cio&amp;utm_term=3006</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/I2PS1f3dC9BS5Sy2QE19</id>
            <title>字节跳动代码生成 Copilot 产品的应用和演进 | AICon</title>
            <link>https://www.infoq.cn/article/I2PS1f3dC9BS5Sy2QE19</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/I2PS1f3dC9BS5Sy2QE19</guid>
            <pubDate></pubDate>
            <updated>Mon, 24 Jun 2024 06:54:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大语言模型, 代码生成, GitHub Copilot, 交互方式
<br>
<br>
总结: 本文介绍了大语言模型在代码生成领域的应用和发展，重点讨论了GitHub Copilot这一产品形式的成功因素，包括团队构建、GPT-3的出现、产品形态选择、交互方式设计等。另外还介绍了字节跳动在内部探索代码生成的历程，包括模型优化、工程链路优化、交互体验改进等方面的探索和挑战。 </div>
                        <hr>
                    
                    <p>大语言模型在代码生成领域取得了令人瞩目的进展。本文整理自字节跳动产品研发和工程架构部的代码智能助手架构师刘夏在 AICon 2024 北京的演讲<a href="https://aicon.infoq.cn/2024/beijing/presentation/5901">《代码生成 Copilot 产品的应用和演进》</a>"，聚焦基于大语言模型的代码生成技术，深入探讨了代码补全和代码编辑这两种典型的应用形态。同时，还分析了当前代码补全面临的挑战和局限性，阐述了代码编辑是如何在交互和构建方法上实现创新。内容经 InfoQ 进行不改变原意的编辑。</p><p></p><p></p><blockquote>在 8 月 18-19 日即将举办的 AICon 上海站，我们设置了【大模型与企业工具集成的提效实践】专题，本专题将分享大模型与企业工具的集成实践和从业者的心路历程，并探讨 AI 在哪些场景更能为企业带来助力。目前大会已进入 8 折购票最后优惠期，感兴趣的同学请锁定大会官网：<a href="https://aicon.infoq.cn/2024/shanghai/track">https://aicon.infoq.cn/2024/shanghai/track</a>"</blockquote><p></p><p></p><p></p><h2>代码生成 Copilot 产品回顾</h2><p></p><p></p><h3>GitHub Copilot 的成功因素</h3><p></p><p></p><p>首先，回顾一下代码生成 Copilot 这种产品形式。当我们谈论代码生成 Copilot 或者 Copilot 这个词时，不得不提到 GitHub 在 2021 年 6 月推出的 GitHub Copilot。这个产品不仅拥有一个响亮的名字，而且定义了一种新的 AI 产品的范式。GitHub Copilot 在 2021 年 6 月推出了技术预览版，随着不断的迭代，其效果令人印象深刻，使人们意识到将大语言模型应用于代码生成领域具有巨大的潜力。业界也开始迅速构建类似的产品，无论是在模型还是产品上都取得了快速的迭代。</p><p></p><p>这里有一个关键问题：为什么是 GitHub Copilot 引爆了这个热点？实际上，将自然语言处理（NLP）技术应用于代码生成并不是一个新概念，例如 TabNine 这样的产品在 GPT-2 时代就已经将其应用于代码补全。那么，GitHub Copilot 究竟有何特别之处呢？我们想要从几个方面和维度来探讨这个问题。</p><p></p><p>首先，我想提到团队，GitHub Next 是这个产品的孵化团队。GitHub Next 是一个具有研究属性的团队，他们的任务是探索未来软件开发的新方式。如果访问他们的官网，你会发现许多有趣的项目，其中就包括 Copilot。团队主要由程序分析师、软件工程师以及研究员组成，他们持续关注的一个重要话题是如何实现通用的代码生成。</p><p></p><p>接下来，我想谈谈一个重要的契机，那就是 2020 年 6 月 GPT-3 的问世。由于 GitHub 现在是微软的子公司，而微软与 OpenAI 有着深入的合作，GitHub 团队很早就获得了 GPT-3 的预览版，并对其能力感到非常兴奋。他们认为必须利用 GPT-3 在代码生成领域做出一些创新，因此与 OpenAI 紧密合作，基于 GPT-3 迭代开发出了专门用于代码的大型语言模型 Codex。随后，他们对 Codex 进行了持续的微调训练，打造了专属的模型。一个强大且优秀的基础模型实际上决定了产品的上限，因此 GPT-3 的出现对这款产品的贡献是巨大的。</p><p></p><p>有了模型之后，团队开始思考应该开发什么样的产品形态。根据 GitHub 的分享，他们最初的想法是开发一款 Chatbot，即一款能够解答编码过程中遇到的任何问题并提供代码的对话聊天产品。但他们很快发现，尽管知识库中大部分问题都能得到回答，但只有大约 20% 的回答是正确且被接受的。尤其是在 GPT-3 时期，ChatGPT 还要两年后才出现，他们意识到这种 Chatbot 产品的效果并不理想。如果大部分时候给出的答案都不是用户想要的，用户对产品的信任度会很低。于是他们决定先采用代码补全这种容错率更高的产品形态，一方面代码补全是个开发者使用频率非常高的功能，也有很强的依赖性，更重要的是开发者对于这个功能的预期是给出建议而不是 100% 准确的答案。</p><p></p><p>选择好产品形态后的一个要素是交互方式。GitHub Copilot 放弃了传统 IDE 中从下拉列表选择补全建议的交互，而是选择了用 Ghost Text 进行展示，用 Tab 键进行采纳，继续输入则取消推荐。这种交互方式发挥了模型在多行补全上的优势，推荐代码和已有代码融为一体，方便开发者快速基于上下文判断是否采纳。</p><p></p><p>代码补全产品的一个技术挑战是实现低延迟，Jetbrains 在开发传统的补全功能时甚至要求在 150ms 内出现推荐列表以达到最佳的开发者体验。因为专业开发者的输入速度通常较快，过高的延迟会失去很多推荐的机会或者迫使用户停顿等待。GitHub Copilot 在大语言模型的推理速度和工程链路上进行了优化，让一个基于云端推理的 LLM 应用做到 500ms 左右的平均延迟。</p><p></p><p>如果说基座模型决定了产品能力的上限，那么提示工程所做的努力就是去逼近这个上限。通过研究开发者日常开发中会关注的上下文，在 prompt 中加入文件路径、相似代码、浏览记录等信息，让模型在代码补全方面的表现大幅提升，如今这些提示工程上的实践也被大家广泛应用。</p><p></p><h2>字节跳动内部代码生成的探索历程</h2><p></p><p></p><p>字节跳动在内部探索代码生成的过程中，面临多种优化选择：可以在模型层面进行优化，也可以选择在工程链路上优化，或在交互体验上进行改进。团队需要灵活地做出决策。</p><p></p><p>随着大语言模型的发展，特别是从 2023 年开始，这个领域开始受到广泛关注，新的模型和产品层出不穷。为了迭代和优化模型，字节跳动首先建立了自己的评测方法和自动化评测系统。这涉及到模型选型的决策，快速评估训练过程中的 checkpoint 效果，以及产品上线后如何收集线上反馈，包括用户编辑过程中的正反馈和负反馈。字节跳动还建立了一个完整的数据链路，以决定哪些数据被采纳，哪些被丢弃，并实施 A/B 测试系统来验证不同的 prompt 策略、参数配置，甚至是新模型的上线效果。字节跳动的自研大语言模型也已经发布，团队逐渐切换到这个自研模型上。基于此，字节跳动引入了对话方式，使代理模型能够理解整个工程结构，并根据实际情况生成代码。此外，还引入了多点代码编辑推荐功能，这是一个较新的功能。今天的分享将围绕三个重点进行详细分析：</p><p></p><p>构建自研评测体系的重要性；如何科学定义产品指标；A/B 测试的重要性。</p><p></p><h3>构建自研评测体系的重要性</h3><p></p><p></p><p>构建自研评测体系的重要性在于，它可以帮助我们避免使用不恰当的评测指标，如 HumanEval，它可能无法准确反映模型在实际应用中的表现。HumanEval 通过完成人工编写的算法题并运行单元测试来评估模型，虽然模型在测试的分数可能很高，但这并不意味着模型在代码补全产品中的表现就一定好。例如，GitHub Copilot 在 HumanEval 上的得分可能不高，但其用户体验仍然出色。</p><p></p><p>自建评测集可以避免数据泄露问题，确保题目和答案不会被模型提前接触到。同时，自建评测集可以引入真实项目中的跨文件上下文，这对于评估模型能否合理利用上下文信息至关重要。此外，自建评测集还可以引入大量公司内部代码，因为开源代码与内部代码的使用场景和分布可能存在显著差异。评测体系还需要包括基于单元测试的验证方式，因为同一功能可能有多种不同的代码实现方式，而单元测试可以更准确地验证生成代码的正确性。</p><p></p><p>最后，安全的自动化评测系统对于模型迭代至关重要。它不仅可以通过执行结果来验证代码的正确性，还可以防止模型生成有害代码，如删除根目录或造成大量内存分配等问题。高效的沙箱测试环境和高并发支持对于大规模的评测也是必不可少的。通过这样的评测系统，我们可以在训练过程中对不同 checkpoint 的模型效果进行评估，从而为模型选型和迭代提供有力支持。</p><p></p><h3>如何科学地定义指标</h3><p></p><p></p><p>在科学地定义指标时，我们需要考虑代码补全流程中的各个环节，并确保所选指标能够准确反映产品优化的需要。一个有效的指标应该能够指导整个链路的优化，帮助我们识别瓶颈并进行相应的调整。采纳率是一个常被提到的指标，它通常定义为采纳次数除以推荐次数。虽然这个定义简单，但它并不是一个好的指标。首先，采纳率容易被操纵。例如，如果减少推荐次数，只在非常确定的时候去帮你补一个分号，采纳率就会提高，但这并不意味着产品的实际效果有所提升。其次，采纳率没有很好地拆解推荐和采纳过程中的具体因素，无法明确指出是推荐更快了，还是其他因素导致采纳次数增多。</p><p></p><p>体验指标是另一个需要考虑的方面。当用户在使用代码补全产品时，如果一个 Tab 操作就能接受推荐的代码并完成工作，这自然会带来良好的用户体验。体验指标可以反映用户对产品的满意度，但它并不直接指导产品优化的方向。在定义指标时，我们需要更细致地考虑如何反映产品的实际性能和用户体验，同时避免指标被操纵，并确保指标能够指导我们进行有效的产品迭代和优化。</p><p></p><p>在探讨如何科学地定义指标时，引入了 CPO（Character per opportunity）这一指标，它是由一家专门从事代码补全产品的公司提出的。CPO 的计算公式由五个因子相乘得到：尝试率、反馈率、采纳率、每次采纳平均的 token 数以及 token 的平均字符长度。</p><p></p><p>尝试率指的是用户在编辑器中进行操作时，AI 提供建议的频率。例如，如果用户敲击键盘 10 次，但只有 6 次触发了对模型的请求，尝试率就是 6/10。这个指标反映了 AI 实际为用户提供建议的次数。</p><p></p><p>反馈率考虑了 AI 给出补全建议时存在的延迟问题。如果因为延迟太高，开发者已经进行了其他操作，那么即使推荐返回了也没有意义。如果发起 6 次请求，最终只有 3 次被展示，反馈率就是 3/6。</p><p></p><p>采纳率是大家熟悉的指标，即用户接受推荐的次数与推荐次数的比值。例如，三次推荐中只有一次被采纳，采纳率就是 1/3。</p><p></p><p>引入每次采纳平均的 token 数和 token 的平均字符长度这两个参数，是为了衡量不同长度代码带来的价值。不同的语言模型有不同的分词器，因此需要计算每个 token 平均的字符长度。例如，ChatGPT 的词表较大，平均一个 token 可以生成的字符数可能大于其他模型。</p><p></p><p>CPO 指标的计算公式是这几个因子的乘积，它衡量的是在每次有机会向用户推荐时，推荐了多少字符给用户。这个指标不仅可以衡量产品给开发者带来的价值，还可以拆解到整个链路的各个部分进行优化。例如，可以通过优化模型推理性能，提高反馈率，或者在代码注释中提供推荐来优化尝试率。此外，当线上出现问题时，CPO 指标也可以用来分析可能存在的问题所在。</p><p></p><h3>A/B 测试的重要性</h3><p></p><p></p><p>A/B 测试在产品开发过程中扮演着至关重要的角色。尽管离线评测可以帮助我们进行模型选型，但一个模型是否真正有效，还需要通过线上测试来验证。有时候，一个模型在评测中得分很高，但这并不代表它在线上的实际表现同样出色。例如，一个非常强大的模型如 GPT-4，可能会因为高延迟而影响用户体验。</p><p></p><p>A/B 测试还可以帮助我们确定各种参数配置的合适值。比如，如果一个模型支持 16K 的上下文长度，是否就应该使用完整的 16K 呢？实际上，如果上下文过长，可能会导致整体延迟增加，影响用户体验。因此，需要通过 A/B 测试来找到最合适的上下文长度。</p><p></p><p>此外，A/B 测试还可以验证新的提示工程策略的效果。例如，如果我们在模型中加入了函数签名或其他包结构信息，是否真的能提升效果？模型是否能够有效利用这些上下文？以及为了采集这些上下文信息而引入的额外延迟，是否值得？这些问题都需要通过 A/B 测试来验证。</p><p></p><p>最后，A/B 测试还可以帮助我们发现并改进产品指标。假设我们最初使用的是采纳率作为指标，但在进行 A/B 测试后，我们发现延迟提高后，采纳率反而增加了。这种情况可能表明我们的指标存在问题，需要重新考虑和调整。</p><p></p><h2>代码编辑推荐：代码补全的进化</h2><p></p><p></p><p>代码补全的进化形式可以被视为代码编辑推荐。大语言模型擅长生成下一个 token，这与代码补全或续写任务非常契合。然而，传统的代码补全主要针对编写全新代码的场景，而软件工程师在日常工作中不仅需要编写新代码，还需要编辑现有代码，包括重构和删除代码。在这些场景下，传统的补全功能可能无法高效地满足需求。在编辑现有代码时，简单地删除一行然后重新编写是低效的。理想情况下，我们希望模型能够自动完成新增、删除、替换等操作，从而提高代码编辑的效率。因此，代码编辑推荐作为代码补全的进化，能够更好地适应软件工程师在实际工作中的各种代码操作需求，提供更加全面和智能的代码辅助功能。</p><p></p><h3>代码编辑推荐的概念</h3><p></p><p></p><p>代码编辑推荐的概念涉及到一种更高级的代码辅助功能，它不仅包括传统的代码补全，还涵盖了对代码进行更深层次的理解和编辑。例如，假设你写了一个 log 函数，该函数用于打印一个 message，并且有两个函数作为调用方来使用这个 log 函数。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f4/f41c7b69002cbe966e397682471d38a8.png" /></p><p></p><p>如果你决定给 log 函数添加两个新的参数，比如 sourceMethod 和 level，用以打印出对应的方法名称和日志等级，这时你实际上需要执行两个后续操作：首先，在 print 语句中添加新参数，以便能够打印出这些新信息；其次，在所有的调用方中也添加这些新参数，确保它们能够传递正确的值给 log 函数。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ac/ace864e1e1bdcb274ee014ed4fbf4cbf.png" /></p><p></p><p>在这种情况下，代码编辑推荐的目标是让模型在你添加完新参数后，能够自动帮你完成剩余的内容。理想状态下，当你完成添加参数的操作时，模型已经预测出你需要在 print 语句中加入这些参数，并且在你移动到调用方时，模型已经知道你接下来需要在这些调用点添加新参数。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/31/315378d8c59296a8868c7c17ac3ed21d.gif" /></p><p></p><p>在 Go 语言中，如果你有一个结构体并且希望它在多线程环境下保持线程安全，通常会引入互斥锁（mutex）来实现。在这种情况下，你需要在结构体的初始化（new）、设置（set）和获取（get）方法中添加锁操作。智能的代码编辑推荐系统应该能够预测到你接下来需要进行的操作。例如，当你在 new 函数中添加锁时，推荐系统可以自动提示你在 set 和 get 方法中也添加相应的加锁代码。当你的光标移动到相应的方法上时，推荐系统就可以给出这些建议。</p><p></p><h3>数据构建和模型训练方法</h3><p></p><p></p><p>数据构建和模型训练是提升代码生成能力的关键环节。模型的能力来源于数据，尤其是 Git 仓库中海量的 commit 数据，这些数据包含了丰富的用户编辑信息。</p><p></p><p>现有的模型训练并没有充分利用这些数据，因为它们往往包含噪音，例如在 commit 信息中夹带无关内容。因此，需要通过启发式规则或模型来过滤掉这些噪音，提取出有相关性和逻辑关系的编辑操作。</p><p></p><p>在编辑过程中，修正 Lint 错误是一个常见任务，这些错误信息及其修复方式也是非常宝贵的数据资源。在训练模型时，通常会选择一个基于大型代码表示模型作为基础，并通过持续训练和 SFT（Supervised Fine-Tuning）等方法让模型理解代码变更的差异。</p><p></p><p>模型在修正代码时可能会出现过度编辑的情况，即模型可能会过于激进地进行不必要的修改。因此，需要采取措施抑制这种行为，确保模型的编辑是恰当和准确的。</p><p></p><h3>进行中的优化</h3><p></p><p></p><p>在进行中的优化方面，我们认识到目前的交互体验和展示方式可能并非最理想的状态。我们认为，集成在集成开发环境（IDE）中并进行一些 UI 上的定制，可能会带来更好的用户体验。</p><p></p><p>此外，我们已经在内部支持了对链接错误（Link Error）和警告（Warning）的修复功能。这是一个重要的进步，因为它能够帮助开发者更快速地解决编译时遇到的问题。</p><p></p><p>我们还在探索光标移动的自动识别和推荐功能。目前，模型通常需要等到开发者的光标移动到特定位置后才能进行预测和推荐。我们希望优化这一点，让模型在开发者完成编码时就能预测下一步可能的编辑位置，并直接提供相应的推荐。这样的优化将进一步提升代码编辑的流畅性和效率。</p><p></p><h2>代码生成 Copilot 的未来展望</h2><p></p><p></p><p>对于代码生成模型来说，一个明显的趋势是能够处理更长的上下文。理想情况下，模型能够理解整个代码仓库的内容。目前，K 级别和 M 级别的上下文可能还不够，模型需要能够无限地处理上下文信息。谷歌等公司已经提出了相关计划。但随着上下文的增长，保持推理速度不降低也是一个挑战，需要维持在几百毫秒的水平。一些公司如 Magic.dev 和 Supermaven 正在探索使用非 Transformer 架构来实现这一点。</p><p></p><p>对于产品形式，完全自主的 Agent 可能不太适合复杂的任务开发。程序员有时可能想用自然语言或注释来描述编码意图，但由于自然语言的局限性和文档编写的困难，最好的做法可能是 AI 与开发者通过交互的方式反复构思确认，并迭代完成复杂功能的开发。</p><p></p><p>AI 应该更智能地识别人类的意图，例如通过编辑位置的预测来主动参与编码过程，提前帮助预判并提供推荐。虽然这个概念比较抽象，但最近出现了一些体现这一思路的例子。Replit 公司开发的代码修复 Agent 展示了 AI 作为一个虚拟协作者参与交互过程的能力。在多人协同的 IDE 中，AI 能够发现错误并以协作者的身份帮助修正，这是一种有效的主动式 AI 交互方式。</p><p></p><p>明尼苏达大学的研究 “Sketch Then Generate” 展示了一种人与 AI 交互持续迭代的方法。通过编写有结构化的注释来指导模型，这些注释可以与代码的实体、符号、方法关联起来，先构建代码架构，然后逐步指导模型生成更多细节和代码。</p><p></p><p>代码生成 Copilot 的未来将更加注重上下文理解、交互式产品开发、智能意图识别和人机协同工作，以实现更高效和智能的代码生成和编辑体验。</p><p></p><p></p><p>活动推荐：</p><p></p><p>InfoQ 将于 8 月 18 日至 19 日在上海举办 AICon 全球人工智能开发与应用大会，汇聚顶尖企业专家，深入端侧 AI、大模型训练、安全实践、RAG 应用、多模态创新等前沿话题。现在大会已开始正式报名，6 月 30 日前可以享受 8 折优惠，单张门票节省 960 元（原价 4800 元），详情可联系票务经理 13269078023 咨询。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e1/e13ff2745ce7d222e772163324f836c4.webp" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>