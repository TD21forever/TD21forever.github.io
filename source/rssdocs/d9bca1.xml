<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/9V9jYGvjYd5Nzfb7GHGe</id>
            <title>AI 浪潮下应用开发的“华山论剑” | QCon</title>
            <link>https://www.infoq.cn/article/9V9jYGvjYd5Nzfb7GHGe</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/9V9jYGvjYd5Nzfb7GHGe</guid>
            <pubDate></pubDate>
            <updated>Thu, 29 Aug 2024 09:22:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>随着人工智能技术的突破性进展，AI 应用开发已成为全球技术革新的核心，并以前所未有的速度改变着世界，您准备好了吗？作为企业技术管理者和架构师，您是否在思考如何利用 AI 推动业务强劲增长？</p><p></p><p>AI 应用开发实践涉及到数据采集与处理、机器学习模型的构建与训练、深度学习技术的应用、以及 AI 系统的集成与部署等多个环节。从智能个性化推荐算法到复杂的自动驾驶系统，再到医疗健康领域的精准诊断，AI 正以其独特的方式重塑着各行各业的面貌。我们惊喜地看到从中小创业公司到大型企业，都在利用计算机视觉、自然语言处理、个性化推荐、对话式交互等 AI 能力提升业务效率、优化用户体验，显著增强了产品的市场竞争力。</p><p></p><p>同时我们也关注到，在实际的 AI 应用中，确保 AI 技术与具体业务需求的紧密结合仍然是一个复杂的挑战，涉及到高质量的数据收集与预处理、模型的选型、RAG 等工程扩展技术，去取得更具鲁棒性和灵活性的解决方案。</p><p></p><p></p><blockquote>10 月 18—19 日 QCon 全球软件开发大会（上海站），我们设置了【<a href="https://qcon.infoq.cn/2024/shanghai/track/1721">AI 应用开发实践</a>"】专题，聚焦实战直击痛点！我们邀请了来自字节跳动、百度、阿里巴巴等头部企业的 AI 专家，分享他们在 AI 代码补全、Agent 开发、电商 AI 落地等方面的实战经验，并探讨如何解决数据安全、性能优化、跨平台兼容等关键挑战。目前是<a href="https://qcon.infoq.cn/2024/shanghai/apply">8 折购票</a>"倒计时 2 天，感兴趣的同学抓紧机会。&nbsp;&nbsp;</blockquote><p></p><p><img src="https://static001.geekbang.org/wechat/images/78/78f7726a60773e15db02c833ca9e3464.png" /></p><p></p><h3>精彩内容抢先看</h3><p></p><p></p><h4>演讲主题：豆包 MarsCode 在 AI Coding 的探索与实践</h4><p></p><p><img src="https://static001.geekbang.org/wechat/images/cd/cdb423aa98b6a3b2f7679afb12faf478.png" /></p><p></p><p>自 LLM 用于辅助编码以来，Al 增强开发成为目前最具革命性的技术趋势，人类开发软件的方式正在发生根本性的变化。预计未来几年，世界上每一位工程师都会在 AI 辅助下进行开发，这是一个令人振奋和向往的技术浪潮，豆包 MarsCode 作为浪潮中的一份子，对 AI Coding 技术在编码开发场景的落地有一些自己的见解。本次演讲我们将分享豆包 MarsCode 在 AI Coding 方向的实践和探索，探讨 AI 与编程工具如何深度融合以及我们对未来软件开发形态的展望。</p><p></p><p>演讲提纲</p><p></p><p>1. AI Coding 的演进史和发展趋势</p><p></p><p>2. AI 代码补全和 AI 问答的效果优化实践</p><p></p><p>代码补全关键指标 (CPO)CPO 的定义要素如何用 CPO 衡量代码补全的真实价值代码补全核心实现架构如何降低 debounce，提升注释、补全的续写效果提高尝试率如何通过模型推理优化和网络、压缩等工程侧优化提升反馈速度如何通过推荐时机优化和模型训练提升采纳率基于 CKG 优化 AI 代码知识问答效果实践</p><p></p><p>3. AI Coding 带给 IDE 的变革和工程实践</p><p></p><p>AI IDE 核心三要素：AI 原生交互 + 随时随地开发 + 服务集成化基于前后端分离实现 IDE 模块解耦划分IDE 性能优化：Rust 重构、通道复用、协议压缩、Web Component 化云 IDE 秒级启动实践：基于 K8s 定制池化调度策略、存储热挂载、进程 HotReload</p><p></p><p>4. 未来 AI Coding 的展望</p><p></p><p>谈谈下一代的 AI Coding 技术升级版 AI 代码编辑推荐 (补全 Pro) 和 AutoDebug 的产品化探索未来软件开发趋势展望</p><p></p><p>实践痛点</p><p></p><p>如何更好去优化 AI 代码补全，如何定义指标，如何从算法侧和工程侧如何综合去优化整个链路AI Coding 新技术多且不算特别成熟，如何基于当下评估能力的可用性，做新技术的探索和落地云 IDE 如何做工程化落地 AI 给 IDE 带来的变革</p><p></p><p>演讲亮点</p><p></p><p>字节跳动对于 AI 代码补全和 AI 问答的效果优化实践字节跳动对于 代码推荐 (补全 Pro) &nbsp;和 AI AutoDebug 两类新 AI Coding 场景的技术探索和产品化落地探索业内 Top 级云 IDE 工程</p><p></p><p>听众收益</p><p></p><p>了解目前 AI Coding 领域最前沿的行业动态和知识了解 MarsCode 在 AI 代码补全效果和 AI 问答效果的优化实践了解 AI 代码编辑推荐和 AI AutoDebug 两大即将普惠的 AI Coding 技术和对其在落地形态的思考和探索了解 MarsCode 在 AI IDE 领域关键工程技术和实践经验</p><p></p><h4>演讲主题：百度文心智能体开发实战与分发模式创新</h4><p></p><p><img src="https://static001.geekbang.org/wechat/images/c8/c8c7c802b573fe88f6007c0517e271f2.png" /></p><p></p><p>随着人工智能技术的蓬勃发展，AI Agent 作为智能服务的关键载体正深刻影响着各行各业。本次演讲，我将以“文心智能体”平台的智能体开发实战为蓝本，深入剖析 AI Agent 从理论到实践的构建过程；同时，结合“旅游 AI 助手”的分发实践，探讨 AI Agent 在多样化市场中的高效、安全分发策略。通过这两个具体案例，结合最新的开发工具、框架及最佳实践，揭示 AI Agent 在提升服务效率、优化用户体验方面的独特价值。此外，我们还将直面数据安全、性能优化、跨平台兼容等核心挑战，提出切实可行的解决方案，为听众呈现一场既具深度又具实用性的 AI Agent 开发与分发盛宴。</p><p></p><p>演讲提纲</p><p></p><p>1. 引言</p><p></p><p>Agent 技术背景与定义演讲目的与结构概述</p><p></p><p>2. Agent 开发实战（文心智能体）</p><p></p><p>Agent 应用理论基础技术选型与架构设计：构建高效能 Agent 的基石文心智能体的特点与应用效果</p><p></p><p>3. 分发模式创新（以旅游 Agent 为例）</p><p></p><p>分发策略设计：质量评估、用户体验优化与反馈机制实战案例分析：旅游领域的 Agent 分发实践，包括市场定位、渠道选择、用户反馈等分发成效评估：市场反响、用户满意度、业务增长等关键指标</p><p></p><p>4. 核心难点与挑战及解法建议</p><p></p><p>数据安全与隐私保护：挑战分析、现有策略与未来展望跨平台兼容性与标准化问题：现状剖析、解决方案与标准化路径实战中的其他挑战与应对策略分享</p><p></p><p>5. 总结与展望</p><p></p><p>Agent 应用研发与分发实践的关键点回顾未来发展趋势预测：技术革新、市场变化与用户需求鼓励行业交流与合作，共同推动 AI Agent 技术的持续进步</p><p></p><p>实践痛点</p><p></p><p>数据安全与隐私保护：如何确保 Agent 应用的数据安全与隐私保护成为一大挑战算法性能优化：随着应用场景的复杂化，如何提升 Agent 的算法性能，确保其实时性与准确性成为关键跨平台兼容性与标准化问题：不同平台间的兼容性问题以及缺乏统一的标准框架，限制了 Agent 应用的广泛推广与应用</p><p></p><p>演讲亮点</p><p></p><p>探索最新的 Agent 应用开发范式分享 Agent 应用设计和开发过程中的关键技术考虑，以及在实际应用中的成功案例</p><p></p><p>听众收益</p><p></p><p>深入理解 Agent 技术的理论基础与应用场景掌握 Agent 应用的研发流程与分发策略获得最新 Agent 应用领域的前沿知识和工具应用经验</p><p></p><h4>演讲主题：AI 托管商家经营：1688 电商 AI 落地实战</h4><p></p><p><img src="https://static001.geekbang.org/wechat/images/ba/ba8ff320c70f8808539c2fd65635085c.png" /></p><p></p><p>1688 对接了大量的工厂类型商家，他们普遍存在供应链能力强但是线上运营能力较弱的问题，这类商家在网站上缺乏有效的方法论指导，电商经营的试错成本居高不下，导致流失率较高。如何通过 AI 技术帮助商家提升线上运营能力，从而提升商家的经营效果是一个具有挑战性的命题。本次分享将结合 1688 商家端的 AI 实战，介绍面向商家提供的 AI 智能化服务，包括咨询问答、 客户管理、 商品运营、经营计划等工作，以及业界领先的 AI 经营托管能力，并阐述相关的技术方案和踩坑经验。</p><p></p><p>演讲提纲</p><p></p><p>1. AI 应用的趋势洞察与判断</p><p></p><p>产业 AI 应用观察商家 AI 产业应用深度定义和全景AI 2B 市场和客户画像分析</p><p></p><p>2. 商家 AI 场景的应用实战</p><p></p><p>各类 AI 技术在商家领域内的应用图文 GC：隐藏在标题、图片里的坑问答：多轮对话牵引商家行动诊断归因：业界难题，我的流量为什么跌了</p><p></p><p>3. 商家 AI 托管模式探索</p><p></p><p>AI 托管的几大障碍核心技术架构和关键技术点AI 经营计划的版本升级AgentSwarm 模式如何工作巧用营销模型让商家 AI 价值最大化</p><p></p><p>4. 未来商家域 AI 的空间和路线</p><p></p><p>实践痛点</p><p></p><p>大模型当前的知识储备和推理能力依然不足，很多命题必须通过 LLM+DL+ML+ 工程方案求解Agent 是个美好的概念，但是落地过程中有诸多水土不服，需要重新认识</p><p></p><p>演讲亮点</p><p></p><p>业内领先的 AI 经营托管的技术，通过经营计划的一揽子方案接管商家的线上经营，并取得不错的业务结果在 RAG 应用、AI 归因分析、AgentSwarm 模式等方面有一定的探索和结果</p><p></p><p>听众收益</p><p></p><p>了解商家端 AI 应用全景和阿里体系商家 AI 探索路径了解各类 AI 技术在商家端业务里的坑以及常见商家端问题的解法了解 AI 托管的模式创新、问题和解法思路</p><p></p><p>更多精彩内容将在 10 月 18 - 19 日 QCon 上海站为您现场呈现，期待与您共赴这场技术之约。如果您有好的技术实践案例想要与我们分享，欢迎<a href="https://jinshuju.net/f/EbrZFg">点击链接</a>"提交演讲申请。</p><p></p><p>会议推荐</p><p>InfoQ 将于 10 月 18-19 日在上海举办 QCon 全球软件开发大会 ，覆盖前后端 / 算法工程师、技术管理者、创业者、投资人等泛开发者群体，内容涵盖当下热点（AI Agent、AI Infra、RAG 等）和传统经典（架构、稳定性、云原生等），侧重实操性和可借鉴性。现在大会已开始正式报名，可以享受 8 折优惠，单张门票立省 960 元（原价 4800 元），详情可联系票务经理 &nbsp;17310043226 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/df/dfd31ee989a7951439a77fec138d4cf8.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/fVwFv0bBwCKAN5tm1Cc6</id>
            <title>NVIDIA H20与计算领域的革命：深入解析算力评估与应用</title>
            <link>https://www.infoq.cn/article/fVwFv0bBwCKAN5tm1Cc6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/fVwFv0bBwCKAN5tm1Cc6</guid>
            <pubDate></pubDate>
            <updated>Thu, 29 Aug 2024 07:40:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><blockquote>在当今快速发展的科技时代，计算能力的重要性毋庸置疑。无论是在人工智能、深度学习还是高性能计算领域，算力的强弱决定了创新的速度与效果。作为NVIDIA最新推出的顶级显卡，H20以其强大的硬件配置和卓越的实际表现，吸引了众多关注。本文将深入探讨算力的概念、评估方法，以及在现代计算任务中的应用，特别是如何利用NVIDIA H20显卡来最大化算力优势。我们将结合理论与实际数据，全面分析H20的独特价值与未来发展方向。</blockquote><p></p><p></p><p></p><h1>算力的概念与历史演进</h1><p></p><p>1.1 算力的定义与基本概念</p><p></p><p>算力，或计算能力，是指计算设备在单位时间内所能完成的计算量。通常情况下，算力以每秒浮点运算次数（FLOPS）来衡量，这是浮点运算能力的标准单位。FLOPS代表每秒能进行的浮点数运算的次数，因此FLOPS越高，设备的计算能力越强。</p><p></p><p>在实际应用中，计算任务的种类繁多，从科学计算到深度学习模型训练，从金融数据分析到自动驾驶系统，各类任务对算力的需求各不相同。计算能力不仅仅是一个硬件性能指标，更是决定技术可行性和应用效果的重要因素。随着科技的进步，计算任务变得越来越复杂，数据量也在不断增加，因此对高算力的需求变得日益迫切。</p><p></p><p>1.2算力的发展历程</p><p></p><p>计算能力的发展可以追溯到计算机的早期历史。从最初的机械计算机到电子计算机，再到现代的超级计算机，计算能力的提升伴随着硬件技术的飞跃。早期的计算设备如ENIAC，每秒只能完成几千次简单的加法运算，而今天的超级计算机每秒可以完成数千万亿次浮点运算。</p><p></p><p>随着时间的推移，计算设备从单一处理器发展到多核处理器，再到并行计算和分布式计算。尤其是在图形处理单元（GPU）领域，NVIDIA等公司通过不断优化硬件架构，显著提升了计算能力。现代GPU如H20显卡，不仅在图形处理上表现优异，在并行计算、深度学习和科学模拟等领域也展现了强大的算力。</p><p></p><p>1.3 计算能力的重要性</p><p></p><p>计算能力是现代科技发展的基础。从物理模拟到分子建模，从图像识别到自然语言处理，强大的计算能力使得这些复杂任务得以实现。特别是在人工智能领域，深度学习模型的训练依赖于海量的数据和复杂的计算，因此对算力的要求极高。</p><p></p><p>在金融领域，高速交易系统依赖于实时的数据分析和决策，这些操作需要在微秒级别内完成，因此需要极高的计算能力。同样，在自动驾驶领域，车辆需要在短时间内处理来自多个传感器的数据，并做出驾驶决策，这也需要强大的算力支持。可以说，算力不仅是硬件性能的体现，更是推动科技进步的重要引擎。</p><p></p><p></p><h1>算力的评估与衡量方法</h1><p></p><p></p><p>2.1 评估算力的标准与方法</p><p></p><p>评估算力涉及多个方面，包括理论计算能力、实际执行效率和任务特定的表现。以下是几种常用的评估标准：</p><p></p><p>#1.&nbsp;FLOPS（每秒浮点运算次数）</p><p></p><p>FLOPS是评估计算能力的最直接指标，它表示硬件在一秒钟内能够完成的浮点运算次数。计算能力越高，硬件处理数据和执行任务的速度就越快。FLOPS通常分为单精度（FP32）、双精度（FP64）和混合精度（FP16、BFLOAT16等）不同类型，根据任务的不同，使用的精度类型也会不同。</p><p></p><p>#2.&nbsp;带宽（Bandwidth）</p><p>带宽指的是在单位时间内能够传输的数据量。内存带宽是决定计算设备性能的关键因素之一，尤其是在需要处理大量数据的任务中。高带宽可以有效减少数据传输的瓶颈，从而提高整体计算效率。在GPU计算中，带宽不仅影响数据加载的速度，也直接影响到模型训练的速度。</p><p></p><p>#3.&nbsp;延迟（Latency）</p><p>延迟是指从输入数据到获得输出结果所需要的时间。低延迟有助于减少数据传输和处理过程中的等待时间，特别是在并行计算中，减少延迟可以显著提高计算效率。延迟通常是并行计算系统的瓶颈，尤其是在大规模数据处理或多GPU协同工作时。</p><p></p><p>#4.&nbsp;能效比（Efficiency Ratio）</p><p>能效比是单位功耗下的计算能力。高能效比意味着在相同的功耗下，硬件能够提供更高的计算能力，这对于数据中心和高性能计算集群尤为重要。在实际应用中，能效比不仅影响计算成本，还影响系统的冷却和维护需求。</p><p></p><p>2.2 模型训练和推理中的算力评估</p><p></p><p>在深度学习和机器学习中，算力的评估往往与具体的任务需求挂钩。以下是几种常见的评估标准：</p><p></p><p>#1.&nbsp;训练速度（Training Speed）</p><p># 评估单位与计算方式 #</p><p>&nbsp;单位&nbsp;：训练速度通常以每秒处理的样本数（Samples per Second, SPS）或每秒处理的tokens数（Tokens per Second, TPS）来衡量。&nbsp;计算方式&nbsp;：SPS和TPS的计算方式如下：</p><p>SPS = 处理的样本总数 / 训练时间（秒）</p><p>TPS = 处理的tokens总数 / 训练时间（秒）</p><p></p><p>在计算过程中，样本数指的是输入数据的批次（Batch Size），而tokens数通常用于自然语言处理（NLP）模型的训练，指的是输入文本被分割后的最小单位（如词语或子词）。</p><p></p><p>#&nbsp;重要性与实际应用&nbsp;#</p><p></p><p>训练速度是衡量计算设备在模型训练过程中效率的关键指标。更高的训练速度意味着模型可以在更短的时间内处理更多的数据，从而加速模型的整体训练进程。这对于处理大型数据集或复杂模型（如深度神经网络、卷积神经网络等）尤为重要。</p><p></p><p>在实际应用中，提升训练速度有助于：</p><p>缩短模型的开发周期。提高资源的利用率，减少计算成本。在相同时间内进行更多实验，从而优化模型效果。</p><p></p><p>特别是在深度学习领域，使用更大的批次处理数据可以显著提高SPS或TPS，而高效的硬件如NVIDIA H20显卡能够支持更大的批次大小和更快的数据处理，从而提升训练速度。</p><p></p><p>#2.&nbsp;模型收敛性（Convergence）</p><p># 评估单位与计算方式 #</p><p></p><p>&nbsp;单位&nbsp;：模型收敛性没有统一的度量单位，但通常以训练轮数（Epochs）、迭代次数（Iterations），或达到某个性能指标所需的时间来衡量。&nbsp;计算方式&nbsp;：</p><p>收敛速度 = 目标性能指标 / 训练时间（秒）</p><p>或者使用收敛的轮数来衡量，即训练到模型性能稳定为止所需的训练轮数或迭代次数。</p><p></p><p>例如，在一个深度学习任务中，收敛速度可以通过模型达到一定的准确率或损失函数值所需的时间来表示。更少的训练轮数或迭代次数意味着更快的收敛速度。</p><p></p><p>#&nbsp;重要性与实际应用&nbsp;#</p><p></p><p>收敛性是衡量模型在训练过程中逐步逼近最优解的能力。算力越强，通常收敛速度越快，因为高算力设备可以支持更大的批次大小、更复杂的优化算法和更快的数据处理速度。这对于研究和开发时间有限的项目至关重要，因为加快收敛速度可以更快地得到有效的模型。</p><p></p><p>在实际应用中，收敛性与以下因素密切相关：</p><p>优化算法：如Adam、SGD等优化算法的选择和调整，直接影响模型的收敛速度。批次大小：更大的批次大小通常会加快收敛速度，但需要足够的显存支持，这也是高算力设备的优势。学习率：调整学习率可以帮助模型更快地达到收敛状态，但需要精细的调试以避免过拟合或欠拟合。</p><p>使用像NVIDIA H20这样具备高算力和大显存的设备，可以在保证计算精度的同时，加快模型的收敛速度。</p><p></p><p>#3.&nbsp;推理速度（Inference Speed）</p><p># 评估单位与计算方式 #</p><p>&nbsp;单位&nbsp;：推理速度通常以每秒处理的样本数（Samples per Second, SPS）或每秒处理的tokens数（Tokens per Second, TPS）来衡量，类似于训练速度。&nbsp;计算方式&nbsp;：SPS和TPS的计算方式如下：</p><p></p><p>SPS = 处理的样本总数 / 推理时间（秒）</p><p>TPS = 处理的tokens总数 / 推理时间（秒）</p><p></p><p>推理速度评估的是模型在实际应用中的响应时间，特别是在实时或近实时的应用中（如自动驾驶、语音识别、在线推荐系统等）。</p><p></p><p>#&nbsp;重要性与实际应用&nbsp;#</p><p>推理速度是决定模型在生产环境中表现的关键指标之一。特别是在需要实时处理和响应的应用中，推理速度直接影响系统的用户体验和效能。</p><p>推理速度越快，系统的响应时间就越短，这对于以下场景尤为重要：</p><p></p><p>自动驾驶：车辆必须在极短时间内处理传感器数据并作出驾驶决策。实时翻译与语音识别：需要在用户发出命令后迅速给出响应。在线推荐系统：实时分析用户行为并推荐个性化内容。</p><p></p><p>NVIDIA H20显卡在推理任务中的表现尤为出色，特别是在FP8低精度计算中，能够在保持高效能的同时，提供极快的推理速度。</p><p></p><p>#4.&nbsp;精度与效率的平衡</p><p># 评估单位与计算方式 #</p><p>&nbsp;单位&nbsp;：精度通常以百分比（%）或数值（如损失值、准确率等）来表示；效率则以处理速度或能效比（FLOPS/Watt）来衡量。&nbsp;计算方式&nbsp;：</p><p></p><p>精度 = 模型在测试数据集上的性能指标（如准确率、F1分数等）</p><p>效率 = 计算资源消耗 / 达到目标性能所需的时间或能量。</p><p></p><p>在深度学习中，精度和效率往往需要进行权衡。例如，高精度计算通常需要更多的计算资源和时间，而低精度计算则可以在速度和资源占用上实现更高的效率。</p><p></p><p>#&nbsp;重要性与实际应用&nbsp;#</p><p></p><p>在实际应用中，精度与效率的平衡是设计和部署AI系统时必须考虑的重要因素。虽然追求更高的精度是许多AI任务的目标，但在某些场景下，高精度并不是唯一的考量。例如：</p><p>边缘计算设备：受限于计算资源和能耗，可能需要在精度和效率之间做出妥协。实时应用：如语音助手或实时翻译，更快的响应速度可能比绝对精度更重要。低成本部署：在大规模部署中，能够以更低的成本达到“足够好”的精度，可能比追求极限精度更具现实意义。</p><p></p><p>NVIDIA H20显卡提供了多种浮点运算模式（如FP16、FP8），允许开发者根据任务需求选择合适的精度和效率组合。例如，在训练阶段使用FP16混合精度可以提高训练速度，而在推理阶段使用FP8可以进一步优化性能，同时保持足够的预测精度。</p><p></p><p></p><h1>Part 3 NVIDIA H20显卡的深入解析</h1><p></p><p></p><p>3.1 H20显卡的硬件架构与技术创新</p><p></p><p>NVIDIA H20显卡基于最新的Hopper架构，在图形计算和并行计算领域引领了新一轮的技术革命。与前几代基于Ampere架构的显卡相比，H20在多个方面进行了大幅升级。尤其是在FP32、FP16以及新增的FP8精度计算能力上，H20展现了其在各种复杂计算任务中的卓越性能。</p><p></p><p>根据提供的图表，H20在FP32单精度浮点运算中达到了44 TFLOPS，这远高于基于Ampere架构的前代产品的19.5 TFLOPS。这一提升对于需要高精度计算的任务，如媒体处理、物理模拟等，具有重大意义。</p><p>在FP16和FP8的Tensor Core性能上，H20也大幅领先于前代产品。在FP16运算中，H20达到了148 TFLOPS，而在FP8的8bit浮点数据类型运算中，H20的性能更是达到了296 TFLOPS。这使得H20在处理需要大量并行计算的任务时，如深度学习模型的训练和推理，具备了极大的优势。</p><p></p><p>3.2 显存与带宽的优越性</p><p></p><p>H20显卡配备了96GB的HBM3显存，这是当前显存配置的顶级标准。这种显存不仅在容量上远超前代产品（80GB HBM2e），在内存带宽上也达到了惊人的4 TB/s，是前代产品带宽的近两倍。如此高的内存带宽使得H20显卡在处理大规模数据集和高分辨率任务时，能够更快地进行数据传输，减少处理延迟。</p><p>对于大模型训练和深度学习应用来说，显存的大小和带宽直接决定了硬件能否有效载入和处理训练数据。H20显卡凭借其96GB的显存，可以轻松应对需要大批量数据的任务，同时其4TB/s的带宽也确保了这些数据能够快速传输到GPU进行处理，这对于需要高效处理数据的任务如自动驾驶、图像识别等尤为重要。</p><p></p><p>3.3 H20的计算能力与实际表现</p><p></p><p>通过分析H20的计算能力图表，我们可以看到它在FP8、FP16等精度下的强劲表现。特别是在处理需要高效浮点运算的任务时，H20的Tensor Core能够提供前所未有的计算性能。例如，在8bit浮点数数据类型的FP8精度运算中，H20的性能达到了296 TFLOPS，适合用于量化训练和模型推理等场景。</p><p>NVLink互联带宽方面，H20也进行了显著的提升。相比前代产品的600GB/s和400GB/s，H20的NVLink带宽高达900GB/s。这意味着多个H20显卡在多卡互联时可以通过更高效的方式进行数据交换，减少了多GPU协同工作的延迟，从而提高了整体计算效率。</p><p></p><p>3.4 浮点运算模式的选择与H20的应用场景</p><p></p><p>在NVIDIA H20显卡中，不同的浮点运算模式为各种计算任务提供了灵活的选择。H20显卡支持从双精度（FP64）到最新的FP8低精度运算模式，覆盖了从高精度科学计算到高效推理任务的广泛应用需求。</p><p>双精度运算模式（FP64）：通常用于需要极高精度的科学和工程计算，如流体力学模拟、气候预测等领域。单精度运算模式（FP32）：是深度学习领域的主力，特别是在训练大型AI模型时，FP32能够提供足够的精度和较高的计算效率。半精度运算模式（FP16）：近年来在深度学习加速方面获得了广泛应用，尤其是在卷积神经网络（CNN）等任务中，FP16能够显著提高训练速度并减少显存占用。低精度运算模式（FP8和INT8）：随着量化技术的发展，FP8和INT8在推理任务中的应用越来越多，H20显卡的296 TFLOPS FP8算力使其在大规模模型推理中占据了显著优势。</p><p></p><p>3.5不同GPU型号的选择:SXM、PCIe、NVLink</p><p></p><p>为了满足不同用户的需求，NVIDIA为H20显卡提供了多种型号，包括SXM、PCIe和NVLink。这些型号的区别在于其硬件架构和连接方式，进而决定了它们在不同应用场景中的适用性。</p><p>SXM版：通过SXM模块设计，可以实现8块GPU的紧密互联，主要应用于高密度GPU服务器集群，如NVIDIA的DGX系统。这种设计不依赖传统的PCIe接口，而是通过NVSwitch实现更高的带宽和更低的延迟，特别适合用于超大规模AI训练和科学模拟任务。PCIe版：沿用了传统的PCIe接口，提供了更为灵活的部署方式。它支持与主板和CPU之间的直接通信，适用于传统的GPU服务器和通用计算任务。每两块GPU通过NVLink Bridge进行连接，虽然在带宽上不如SXM版，但在扩展性和兼容性上具有一定的优势。NVLink版：专为需要超高带宽的数据密集型任务设计。它提供了高达7.8 TB/s的传输带宽，适用于需要实时处理大量数据的大规模语言模型（LLM）训练任务。通过NVLink接口，多个H20显卡可以实现高速数据交换，减少计算过程中数据传输的瓶颈，提升整体计算效率。</p><p></p><p>通过结合这些不同型号的特点和应用场景，用户可以根据自己的具体需求选择最适合的GPU类型，从而在不同的计算任务中最大化地发挥H20显卡的性能优势。</p><p></p><h1>Part 4 H20在模型训练与推理中的实际应用</h1><p></p><p></p><p>4.1 通过Llama2模型探讨H20的应用</p><p></p><p>为了更好地理解H20显卡在实际应用中的表现，我们可以借助Llama2-70B模型的训练和推理数据来分析其性能。根据提供的图表，H20显卡在不同精度（FP8和FP16）以及不同输入输出长度下展现了卓越的计算能力。</p><p></p><p>在图表中，HGX H20模块在处理LLAMA2_70B模型时，在FP8精度下，输入长度为2048、输出长度为128的配置中，H20的吞吐量达到了1.2244595*A tokens/秒。而在输入长度为128、输出长度为2048的配置中，FP8精度下的吞吐量更是高达2.0981547*B tokens/秒。这表明在高精度和复杂模型的训练和推理任务中，H20显卡能够提供极为高效的计算性能。</p><p></p><p>相比之下，HGX A1XX模块在相同配置下，使用FP16精度的表现明显不如H20。这进一步证实了H20显卡在处理大规模语言模型时的优势。特别是在需要处理复杂输入输出关系的推理任务中，H20的高带宽和Tensor Core的强大性能，使其能够在更短的时间内完成推理，提供更高的吞吐量。</p><p></p><p>4.2 H20在推理任务中的独特优势</p><p></p><p>推理任务中，吞吐量和响应速度是两个关键指标。H20显卡凭借其FP8精度的计算能力，在处理LLAMA2_70B等大规模模型时，能够提供更高的tokens处理速度。结合前面的数据分析，H20在推理任务中的表现不仅仅体现在其计算能力上，还得益于其大容量的显存和超高的内存带宽，这些因素共同作用，使得H20能够在处理复杂推理任务时，保持高效和准确的性能输出。</p><p></p><p>通过NVLink的高带宽支持，多个H20显卡在多卡集群中可以实现高效的数据交换和协同计算，这对于需要实时处理和分析数据的任务来说，至关重要。例如，在自动驾驶系统中，H20显卡可以通过快速处理传感器数据并作出决策，从而提高系统的安全性和反应速度。</p><p></p><p>4.3 H20的GEMM性能分析</p><p></p><p>在矩阵乘法（GEMM）任务中，浮点运算性能是评估GPU计算能力的重要指标之一。以下是从表格中筛选出的伊迪雅H20在不同浮点精度下的GEMM性能数据，并对其进行详细分析。</p><p><img src="https://static001.geekbang.org/infoq/a3/a3e16713905cdb6c63c8cc433b76ba02.png" /></p><p>分析与解读</p><p># FP8精度 #</p><p>峰值性能：293 TFLOPS实测性能：267.33 TFLOPS峰值百分比：91.25%</p><p></p><p>FP8精度下，伊迪雅H20的实测性能达到了267.33 TFLOPS，占峰值性能的91.25%。这一结果表明在低精度浮点运算中，伊迪雅H20的表现非常接近其理论最大值，表明其硬件设计在FP8运算任务中的效率极高，适合用于大规模模型推理和量化训练等场景。</p><p></p><p># INT8精度 #</p><p>峰值性能：293 TFLOPS实测性能：188.30 TFLOPS峰值百分比：64.27%</p><p></p><p>在INT8精度下，伊迪雅H20的实测性能为188.30 TFLOPS，占峰值性能的64.27%。虽然相对于FP8的表现有所下降，但INT8仍然提供了高效的计算能力。INT8精度广泛应用于需要处理大量数据的推理任务，尤其在资源受限的环境下，可以在降低计算复杂度的同时，保持合理的精度。</p><p></p><p># FP16精度 #</p><p>峰值性能：147 TFLOPS实测性能：141.55 TFLOPS峰值百分比：96.31%</p><p></p><p>在FP16精度下，伊迪雅H20几乎达到了其峰值性能，实测值为141.55 TFLOPS，占峰值的96.31%。这表明伊迪雅H20在FP16运算中能够充分发挥其硬件潜力，非常适合用于深度学习训练任务，特别是卷积神经网络（CNN）和递归神经网络（RNN）等对计算速度要求较高的模型。</p><p></p><p># TF32精度 #</p><p>峰值性能：74 TFLOPS实测性能：69.47 TFLOPS峰值百分比：93.88%</p><p></p><p>TF32是一种介于FP16和FP32之间的浮点精度模式，旨在提供比FP32更高的计算效率，同时保留一定的计算精度。在这一模式下，伊迪雅H20的实测性能为69.47 TFLOPS，占峰值性能的93.88%。这一表现说明TF32是一个平衡精度和效率的理想选择，特别是在要求较高的科学计算和AI模型训练中，能够显著提升计算速度。</p><p></p><p># FP32精度 #</p><p>峰值性能：40 TFLOPS实测性能：31.41 TFLOPS峰值百分比：78.53%</p><p></p><p>在FP32精度下，伊迪雅H20的实测性能为31.41 TFLOPS，占峰值性能的78.53%。虽然相对其他精度模式的效率稍低，但FP32依然是许多AI模型和科学计算任务的首选精度模式，特别是在需要高精度结果的场景中，FP32的稳定表现非常重要。</p><p></p><p>H20在不同精度下的GEMM性能分析，我们可以看到其在多种运算模式中的强大表现。无论是在高效推理任务中的FP8和INT8，还是在深度学习训练中的FP16和TF32，伊迪雅H20都能够提供接近其理论峰值的实际性能。这表明H20显卡不仅具备出色的硬件设计，还能在实际应用中充分发挥其计算能力，适合于从AI模型训练到大规模推理等广泛应用场景。</p><p></p><h1>Part 5 NVIDIA H20与H100的深入对比</h1><p></p><p></p><p>5.1 H100显卡的优势与应用场景</p><p></p><p>NVIDIA H100显卡是目前市场上最强大的GPU之一，其高达1979 TFLOP的理论计算能力，使得H100在处理高精度计算任务时具备无可比拟的优势。H100显卡的性能密度高达19.4，远超H20显卡的2.9，这使得H100在单位面积内能够提供更高的计算能力，特别适用于空间受限但需要高性能的计算环境。</p><p>在实际应用中，H100显卡主要应用于高精度科学计算、复杂AI模型训练和大规模数据分析等领域。对于那些需要极致性能的用户，H100显卡无疑是最佳选择。例如，在气候模拟、分子动力学和高精度物理模拟等任务中，H100显卡可以显著加快计算速度，减少模拟时间。</p><p></p><p>5.2 H20显卡的核心价值与独特优势</p><p></p><p>尽管H20显卡在理论计算能力上不如H100，但其在实际应用中的表现依然出色。特别是在大规模低精度模型训练和推理中，H20凭借其高显存、大带宽和较低的成本，展现了极高的性价比。对于那些需要处理大量数据且对计算精度要求不高的任务，如自然语言处理、推荐系统、图像识别等，H20显卡是一个非常具有竞争力的选择。</p><p></p><p>H20显卡的核心价值在于其出色的内存管理和高效的计算能力，特别是在FP8精度下，H20显卡能够以更少的节点数量完成训练任务，从而降低整体计算成本。这使得H20显卡在一些预算敏感的项目中，成为了性价比最高的解决方案。</p><p></p><h1>Part 6 H20在工业领域的广泛应用</h1><p></p><p></p><p>除了在AI研究中的广泛应用外，H20显卡在工业领域也展现了巨大的应用潜力。无论是在自动驾驶、智能制造，还是金融科技和医疗健康，H20显卡都能够通过其强大的计算能力，为各类复杂的计算任务提供解决方案。</p><p></p><p>在自动驾驶领域，H20显卡的高带宽和低延迟使得其能够实时处理来自多个传感器的数据，并做出驾驶决策。智能制造领域的复杂工艺模拟和优化，同样可以通过H20显卡的高算力得到加速。而在金融科技领域，H20显卡的快速数据处理能力可以显著提升高频交易系统的响应速度，降低市场风险。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/40q0yogaX2i13pOKQQar</id>
            <title>AI技术如何深入各行各业？Intel AI Summit专场全栈落地实践分享丨AICon</title>
            <link>https://www.infoq.cn/article/40q0yogaX2i13pOKQQar</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/40q0yogaX2i13pOKQQar</guid>
            <pubDate></pubDate>
            <updated>Thu, 29 Aug 2024 03:16:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在当今时代，人工智能技术正以前所未有的速度迅猛发展，企业落地人工智能应用已成为不可逆转的趋势。然而，这一过程中也伴随着诸多挑战和问题。如何助力企业加速人工智能的落地进程，如何最大限度地提升 IT 系统的资源利用率，以及如何有效增强计算效能，并便捷、稳定地部署应用 AI，都已成为整个行业关注的焦点。</p><p></p><p>8 月 18 日至 8 月 19 日，在上海举办的 AICon 2024 全球人工智能开发与应用大会上，诸多讨论都聚焦于这些问题。其中的 Intel AI Summit 「AI 全栈解决方案及行业实践」专场，来自英特尔及其合作伙伴的四位行业专家就 AI 应用落地的全栈解决方案，以及医疗领域的应用案例进行了深入探讨。</p><p></p><p></p><h2>激发 AI 潜能：xFT 助力算力解锁，最大化提升计算效能</h2><p></p><p></p><p>尽管“AI 赋能”具有巨大吸引力，但在资源有限的条件下，企业必须确保每一项投入都能获得最大化的效益，这对应用落地的成本和利用率提出了严格的要求。特别是在计算效能提升方面，需要硬件、软件、算法等多个层面的协同优化。高效的计算能力能够加速数据处理、模型训练和推理速度，使企业更迅速地做出明智决策，推出创新产品和服务。</p><p></p><p>为了解决算力对 AI 落地的限制，英特尔一直走在行业前列，力求为企业推出实用可靠的计算资源方案，通过技术优化最大化提升算力，推动大模型应用的落地，充分释放 AI 潜能。在本次会议上，英特尔数据中心和 AI 事业部首席工程师何普江带来了主题为《xFT 解锁至强算力，释放 AI 潜能》的演讲。</p><p></p><p>何普江认为，AI 的未来将由算力的突破来定义，而第五代英特尔®️&nbsp;至强®️&nbsp;可扩展处理器及其内置的英特尔®️&nbsp;AMX 技术正是这一突破的关键。英特尔®️ AMX 通过深度优化矩阵运算，为算力释放提供了坚实的硬件基础。</p><p></p><p>在演讲中，何普江分享了 xFT（xFasterTransformer）技术的设计理念：这是一个专为 AMX 优化的开源项目，不仅支持广泛的 AI 模型和数据类型，更通过软硬件的深度融合，显著加速了 AI 大模型推理。何普江也在分享中提到，与传统方法相比，目前通过 xFT 技术，可以在第五代处理器上面用 48 核跑出高达 1300 的 CRGPU 吞吐量，这一数字远超行业标准，在处理大规模数据集和复杂运算时表现出色。</p><p></p><p>在算法层面，何普江深入分享了 xFT 技术的多项创新，包括对 oneDNN 库的优化使用，以及针对不同 token size 优化的 Slim attention 机制。这些创新不仅提升了 xFT 技术的性能，也为 AI 社区提供了宝贵的实践经验。</p><p></p><p>探讨大模型与小模型的未来发展时，何普江指出，两者各有优势，将共同推动 AI 技术进步。他强调了多模态和 RAG 技术的重要性，并预测开源与闭源模型间差距将缩小。何普江还提到了 KV Cache 的关键作用，以及它对未来 AI 系统设计的影响。</p><p></p><p>他认为，随着硬件和软件的不断进步，大语言模型的成本将大幅降低，推动 AI 技术的更广泛应用和深入发展。开源与闭源模型间的差距正在缩小，未来开源模型将在 AI 领域扮演更加重要的角色。</p><p></p><p></p><h2>GenAI 开放平台 OPEA：一站式助力大模型应用，企业 AI 落地加速器？</h2><p></p><p></p><p>除了底层算力效能提升之外，在目前企业的 AI 应用实践中，还存在着训推优化、基础设施扩展、数据传输安全、应用碎片化等诸多环节。企业需要一个能够全栈助力落地 AI 应用的方案与平台，一站式解决生成式 AI 的落地问题，在性能优化、可扩展性、安全等角度为企业保驾护航。</p><p></p><p>在本次会议上，英特尔 AI 首席工程师吴震华围绕 OPEA 开放平台进行了分享。作为人工智能建模、特征工程、效果分析以及推荐增强等领域的资深专家，他在演讲《基于检索增强的企业 GenAI 开放平台落地实践》中详细梳理了 AI 技术的发展历程，并深入解析了英特尔企业 GenAI 开放平台（OPEA）的架构与底层技术。</p><p></p><p>吴震华认为，尽管基于检索增强的 RAG 技术并非新生事物，但其在企业中的应用潜力正随着大语言模型的能力而日益凸显。在吴震华看来，企业 AI 落地面临的挑战与机遇并存，特别是在生成式 AI 技术，如 ChatGPT 引爆市场之后，行业关注的焦点已从模型预训练的竞争转向了具体的应用落地。</p><p></p><p>OPEA 开放平台是一个由英特尔推动、捐赠给 Linux 基金会的开源项目。OPEA 旨在构建一个开放的生态系统，使企业能够快速利用大语言模型和 AI 技术带来的创新优势。吴震华详细介绍了 OPEA 的全栈架构，从基础设施层到平台集成层，再到面向用户的服务层，展示了一个多层次、模块化的 AI 应用平台。</p><p></p><p>展望未来，吴震华预计到 2028 年，80% 以上的商用 PC 将被新形态的 AIPC 所替代。他将企业 AI 应用的发展分为三个阶段：今天，AI 助手如 CO-Pilot 和 RAG 正在提升数据检索和编程流程的效率；明天，智能体将拥有更大的自主权，利用 AI 的推理能力完成特定任务；未来，AI 将深入企业流程的每个环节，优化每个生产要素。</p><p></p><p>吴震华还提出了企业 AI 应用的四个关键方向：易用性、开放性、安全性、负责任的使用，以及平台的可扩展性和参考实践的提供。他希望通过这些方向的努力，使企业 AI 快速享受到生成式 AI 革命的技术成果。</p><p></p><p>在演讲的最后，吴震华通过一个应用 demo 展示了低代码的基于至强®️&nbsp;微服务实现生成 AI 服务功能，他期待通过不断的迭代和更新，OPEA 能够推动企业 AI 方案的发展，方便快速地帮助企业用户解决实际的问题，让企业真正享受到生成式 AI 技术带来的红利。</p><p></p><p></p><h2>AI+ 医疗：大模型在病历质控中的应用实践</h2><p></p><p></p><p>生成式 AI、大模型技术正在为各行各业带来革命性的变化，医疗领域也不例外。在医院、健康机构等场景下，AI 辅助诊疗、病历质控等应用将成为未来技术趋势。惠每科技致力于通过人工智能解决方案提升医疗质量，守卫患者安全，在智能化诊疗、病历质控等技术领域不断创新大模型技术应用，推动医疗行业的数智化发展。在 Intel AI Summit 专场上，惠每科技算法专家凌鸿顺以《破解病历质控难题：医疗大模型质控优化策略》为主题，分享了惠每科技在病历质控领域的成功实践。</p><p></p><p>病历质控作为医疗质量评估的核心，直接影响医疗服务水平和患者安全。面对病历书写的及时性、规范性和完整性问题，惠每科技采用了大模型技术，利用其强大的文本理解和知识推理能力，有效提升了病历质控的效率和准确性。大模型基于 Transformer 架构，通过持续预训练和任务对齐，以及直接偏好优化，显著提高了对病历中关键信息的提取和分析能力。</p><p></p><p>凌鸿顺还提到，在模型训练优化方面，惠每科技采取了基座模型优化和大模型 prompt 工程优化的策略。通过知识注入、指令跟随和直接偏好学习，模型能够更好地理解和执行医疗领域特定的任务。特别是在处理病历中的矛盾和不规范问题时，大模型展现了其跨字段理解和医疗知识对比的优势。</p><p></p><p>惠每科技还制定了自动化 Few-shot 示例的方案，通过初始化阶段的 badcase 识别和迭代优化，以及相似度计算和多样性 prompt 的加入，进一步提升了模型的预测效果和泛化能力。这一策略不仅减轻了筛选 Few-shot prompt 的工作量，也为不同医院的特殊 case 提供了快速修复的可能。</p><p></p><p>凌鸿顺还提到，在大模型部署推理的实践中，惠每科技与英特尔的合作成果显著。通过xFasterTransformer、BigDL 量化方案和&nbsp;OpenVINO™️&nbsp;非量化方案，实现了医疗模型私有化部署的优化，解决了大模型在硬件资源和计算效率上的挑战。特别是英特尔®️&nbsp;AMX 技术的应用，为大模型的推理性能带来了质的飞跃。</p><p></p><p>展望未来，凌鸿顺对医疗大模型的应用持乐观态度。模型蒸馏技术有望将大模型的效果转移到更小、更易于部署的模型上。自动化 Few-shot 的进一步优化，将实现更高效、更准确的病历质控。同时，惠每科技也将与英特尔展开持续合作，进一步推动医疗 AI 技术的创新和应用，为医疗行业带来更多的价值和可能性。</p><p></p><p></p><h2>医疗 AI 革新：大模型技术深度融合与应用实践</h2><p></p><p></p><p>人工智能技术的发展对医疗行业的信息化升级和数智化变革具有重大意义。以国内医疗场景为例，大量专业化数据和对信息化处理的精准度要求极高，这些都是信息化过程中需要解决的实际问题。垂直领域的大语言模型将成为新一代医疗信息化系统的有力助手，帮助解决医疗系统中的诸多问题。然而，如何让医疗大模型真正可用、易用，仍需解决模型构建、集成、系统结合和应用设计的一系列问题。</p><p></p><p>在 Intel AI Summit 专场上，卫宁健康研发总监刘鸣谦带来了题为《大语言模型在医疗场景的落地实践》的分享，深入探讨了大模型技术如何深刻影响医疗信息化的发展和临床应用。</p><p></p><p>刘鸣谦首先回顾了医疗系统的发展历程，从早期的专家系统、本体推理到现代基于 AI 的图像辅助诊断和自然语言处理。她认为，自 OpenAI GPT3.5 发布以来，基于 Transformer 的大模型已成为医疗领域开发和应用的新范式。大模型的文本生成能力、推理能力和交互能力，为医疗领域带来了前所未有的创新潜力。</p><p></p><p>在数据工程方面，刘鸣谦分享了卫宁健康如何通过高质量的数据集和场景化处理来优化大模型的训练效果。通过上下文学习、RAG（Retrieval-Augmented Generation）和 Agent 方式，进一步提升了模型的效果和适应性。此外，通过直接偏好优化（DPO）和提示工程，模型在医疗场景中的适用性得到了显著提升。</p><p></p><p>卫宁健康的大模型训练采用了多轮迭代，结合了开源数据和自身积累的医疗知识，形成了强大的模型能力。刘鸣谦提到，卫宁健康开源了多款垂直领域大模型，以促进社区的交流和发展，并与英特尔合作，优化了基于英特尔®️&nbsp;AMX 技术的本地化部署方案，有效降低了成本同时保证了高性能。</p><p></p><p>卫宁团队开发的 Copilot，作为信息化系统和 AI 模型之间的桥梁，通过 API 插件等多种形式，实现了不同应用场景下的模型管理和服务。Copilot 的应用，使得医务人员能够无缝地体验到 AI 带来的便利。</p><p></p><p>在医疗应用场景方面，刘鸣谦详细介绍了大模型在医技、临床和管理场景下的实际应用案例。例如，在影像科中，大模型辅助医生快速生成报告，提高了工作效率；在超声科中，实现了实时报告质控，提升了医疗质量；在临床辅助诊断中，通过增强型 CDSS 系统，提供了更加精准的辅助决策支持。</p><p></p><p>此外，大模型还在病历文书助手和智能语言查房助手中发挥了重要作用，通过语音识别和自然语言处理技术，实现了医生口述内容的自动结构化输出，极大地提高了医生的工作效率。</p><p></p><p></p><h2>AI 企业落地，从概念走向现实</h2><p></p><p></p><p>随着人工智能技术的不断成熟和创新，其在企业中的应用已不再是遥远的梦想，而是触手可及的现实。英特尔 AI 全栈解决方案的提出和实践，为企业智能化转型提供了一个清晰的路径：从底层硬件的优化到顶层应用的创新，从单一技术的突破到全栈生态的构建，每一步需要关注企业实实在在的效益，才能让 AI 发挥出真正的价值。</p><p></p><p>未来，随着技术的进一步发展和应用的不断深入，AI 必将成为推动企业创新和增长的关键力量，开启一个全新的智能化时代。让我们拭目以待，共同见证 AI 技术如何助力企业实现跨越式发展，引领行业变革。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/OczQK3Y36NeIXNWdDsu5</id>
            <title>第五届深圳国际人工智能展（GAIE）即将召开，5大精彩看点揭秘</title>
            <link>https://www.infoq.cn/article/OczQK3Y36NeIXNWdDsu5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/OczQK3Y36NeIXNWdDsu5</guid>
            <pubDate></pubDate>
            <updated>Wed, 28 Aug 2024 11:04:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>2024年9月8日至10日，第五届深圳国际人工智能展（GAIE）将于深圳会展中心（福田）盛大举行。作为粤港澳大湾区人工智能领域的专业盛会，GAIE不仅会展示人工智能领域的最新成果和前沿技术，更是一个促进行业交流、推动创新发展的重要平台。</p><p></p><p>本届展会由深圳市工业和信息化局、深圳市发展和改革委员会、深圳市科技创新局、深圳市政务服务和数据管理局共同指导，深圳市人工智能行业协会、深圳市万博展览有限公司主办，以“智创未来·价值链接”为主题，旨在汇聚全球人工智能领域的顶尖企业、专家学者及创新成果，展示人工智能技术的最新进展和广泛应用，促进国内外交流合作，对加速人工智能技术成果转化，构建人工智能应用生态体系，积极推动人工智能高质量发展具有重要意义。我们特意者整理了本届展会的几大精彩看点，供感兴趣的读者参考。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2b/2b0d9c5ad7104c03769cc867eeacb387.png" /></p><p></p><h2>看点1：品牌展商+创新产品，打造科技创新盛宴</h2><p></p><p></p><p>本次展会汇聚了国内外众多人工智能领域的领军企业，包括百度、华为、中国联通、迈瑞医疗、特斯拉、云知声、普渡机器人、鲜食代等头部企业。这些企业将在展会现场竞相亮相，展示他们在人工智能领域的最新技术、产品和解决方案。从智慧交通的自动驾驶与智能车辆监控系统，到智能制造中的智能机器人与自动化生产线；从智慧医疗的辅助诊断与远程医疗系统，到智能服务的个性化语音助手与高效客服；再到智慧教育的智能硬件与在线教育平台，每一件创新产品都将以其独特的魅力，向观众展示人工智能技术的无限可能。而展会期间的多场新品发布会，更将科技与创新的火花碰撞得淋漓尽致，让观众在近距离感受最前沿科技成果的同时，也深刻体会到了科技所带来的便捷与惊喜。:</p><p></p><h2>看点2：30+场系列活动，洞悉AI行业前沿趋势</h2><p></p><p></p><p>本届展会将聚焦100+热点话题，举办2024全球湾区科技创新发展论坛暨第五届深圳国际人工智能展开幕式、智能机器人创新发展论坛、工业制造发展论坛暨深圳标准认证颁证大会、医工融合—医疗高质量发展大会、第二届智能算力发展论坛、大模型生态与发展论坛、百度AI引领行业人工智能应用创新发展论坛、AI赋能科技品牌出海国际论坛等30+场系列活动，旨在为企业搭建深度交流平台，洞悉AI行业的前沿趋势与最新动态，精准把握市场脉搏，为未来发展提供有力支撑和广阔机遇。</p><p></p><p>此外，展会现场将汇聚AI人才进行交流分享，举办AI人才直聘会与校企对接会，以推动人才流动，构建强大的人工智能行业人才队伍，并加速高校科技成果向企业转化，提升企业技术创新能力。</p><p></p><h3>看点3：大咖聚首深圳，探寻未来AI发展新机遇</h3><p></p><p></p><p>本届展会邀请到了中国科学院院士、天津大学精仪学院教授、院名誉院长、激光与光电子研究所所长姚建铨，美国医学与生物工程院院士、英国皇家公共卫生学院院士、深圳理工大学计算机科学与控制工程院院长潘毅，深圳市人工智能行业协会首席顾问、北京大学教授刘宏，深圳市人工智能行业协会会长、平安集团首席科学家肖京，阿里云教育高级副总裁黄桂晶，大族激光副总裁、大族视觉董事长陈焱，云知声联合创始人&amp;副总裁康恒等全球前沿专家、学者、企业家齐聚深圳。他们将在论坛上分享全球前沿理念与创新举措，感知AI与新科技、AI与新产业、AI与新制造的发展趋势，探讨人工智能赋能千行百业的无限可能。这不仅为展会增添了浓厚的学术氛围，更为业界人士提供了宝贵的学习和交流机会。:</p><p></p><h2>看点4：AI产业会客厅，展望产业融合新未来</h2><p></p><p></p><p>本次展会上，产业会客厅将作为重要的交流平台，旨在以深度采访及人工智能应用场景供需对接的形式促成更高质量的产业链合作、更生态化的共创，充分发挥龙头企业的主引擎“带动力”，为优势产业链重点企业带来新的市场增量空间，为区域创新业态及创投生态提供巨大的发展机遇，进一步推动人工智能相关企业积极拓展市场交流与合作。而高端访谈环节是产业会客厅不可或缺的重要组成部分，它不仅是信息传递的桥梁，更是思想碰撞的火花。通过邀请人工智能领域的领军人物、企业高管、学术权威等，就AI技术的最新发展、行业趋势、政策导向等话题进行深度对话，期望能够触及人工智能领域的最深处，挖掘那些不为人知的秘密和即将改变世界的创新，为人工智能行业的未来发展注入更多的动力和活力，推动整个行业不断向前发展。同时，还将通过现场采访，挖掘并分享企业在人工智能领域的成功经验和创新实践，为观众呈现一个全面、深入的人工智能行业图景。</p><p></p><p>此外，为加速AI应用场景落地，产业会客厅还将通过汇聚人工智能领域的专家和学者等技术提供方，以及医疗、水务、机场等各行业企业代表和决策者等应用场景需求方，开展“人工智能应用场景供需对接会”，并以“主题演讲+互动展示+供需对接”的形式展开，通过分享各行业的人工智能应用案例和需求，共同探讨AI技术如何赋能不同行业，以提高效率、降低成本，创造更多商业机会。这一环节将有效推动人工智能技术的广泛应用和产业升级。</p><p></p><p>另一方面，作为推动AI产业高质量发展的重要力量，投融资机构在展会中扮演着不可或缺的角色。本届展会特设“人工智能投融资展示对接区”，并同期举办国际人工智能投融资大会。通过技术展示、企业路演、投融资对接等形式，汇聚全球范围内的人工智能创业项目与投资机构，推动优质项目与资本的高效对接。这不仅为初创企业提供了宝贵的展示机会和融资渠道，也为投资机构挖掘潜力项目提供了便利条件。</p><p></p><p>通过这些活动，“产业会客厅”不仅引领了AI行业的新潮流，还搭建了一个合作与交流的桥梁，有助于推动人工智能产业的高质量发展，同时也为初创企业提供了展示和融资的机会，为投资机构提供了挖掘潜力项目的平台。</p><p></p><h2>看点5：60000+专业观众，促进跨领域交流合作</h2><p></p><p></p><p>展会面向全球范围诚邀专业观众参会，包括但不限于科技企业代表、科研机构人员、投资机构、行业媒体及政府相关部门等。通过精准邀约与线上宣传，并依托深圳市人工智能行业协会的广泛资源，目前已邀请全国各人工智能友好协会组织各省市企业赴深观展、交流、寻求合作。这些企业代表了各自领域的顶尖水平，他们的参与无疑将为展会增添更多亮点。此外，组委会还特别邀请了深圳市医院管理者协会、深圳市福田区青少年科技教育协会等近50家协会，涵盖工业、家居、机器人、电子信息、医疗器械、商会、企业家协会、外商协会等多个领域，预计吸引60000+专业观众到场参观。届时，各商协会将组织其会员单位分批观展、对接，以促进跨领域的交流与合作，共同推动人工智能技术的创新与发展。</p><p></p><p>经主办方邀请，InfoQ 将参与本届 GAIE 现场报道，诚邀感兴趣的读者前往。2024年9月8—10日深圳会展中心（福田），我们不见不散！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/y7JgsijGuP9S1A2yeG94</id>
            <title>2000 多名应届生被印度 IT 巨头“培训”多年不给发工资，在职员工吐槽：我们工作14小时哪还有工作留给别人</title>
            <link>https://www.infoq.cn/article/y7JgsijGuP9S1A2yeG94</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/y7JgsijGuP9S1A2yeG94</guid>
            <pubDate></pubDate>
            <updated>Wed, 28 Aug 2024 11:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><h2>印度IT公司“疯狂剥削”，2000名新员工入职遭推迟多年</h2><p></p><p>&nbsp;</p><p>据报道，印度IT厂商Infosys的新员工反复经历无薪“入职培训”却难以入职。</p><p>&nbsp;</p><p>印度IT厂商Infosys被指存在严重“剥削”，据称该公司向数千名工程专业毕业生发出录取通知，但在长达两年时间内仍未让任何一名新员工入职。报道指出，这些应届毕业生们被告知必须反复接受无薪培训，才有资格继续留在Infosys工作。</p><p>&nbsp;</p><p>上周，印度IT工作者倡导组织“信息技术新生雇员理事会”（NITES）向印度劳工与就业部长Mansukh Mandaviya写信，要求印度政府介入干预“以阻止Infosys剥削年轻的IT毕业生”。NITES主席Harpreet Singh Saluja在这份署名信函中强调，NITES已经收到“多起”来自应届工程毕业生的投诉。这些毕业生在被Infosys聘为系统工程师与数字专家工程师之后，“遭受到不专业及剥削性的对待”。</p><p>&nbsp;</p><p>据NITES介绍，Infosys公司曾在2022至2023年间组织过大学招聘工作，且最早在2022年4月22日就曾向学生们发放过录取通知，但却一直在推迟毕业生们的入职流程。NITES此前曾经表示，有“超过2000名新员工”受到此事影响。</p><p>&nbsp;</p><p>据公开资料显示，Infosys 是总部位于印度班加罗尔的一家信息技术跨国公司。2017年，Infosys 是仅次于塔塔资讯服务的印度第二大IT公司，收入在全球上市公司中排名第596。Infosys 在29个国家设有办公室并在印度、美国、中国、澳大利亚、英国、加拿大、日本等地设有研发中心。Infosys 在超过30个国家提供商业咨询、信息技术、及外包服务。</p><p>&nbsp;</p><p>2021年8月，印孚瑟斯市值超过1000亿美元。</p><p></p><h2>无薪“入职培训”</h2><p></p><p></p><p>NITES声称，收到录取通知的学生们被要求参加从2024年7月1日至2024年7月24日进行的无薪线上“入职培训”。报道指出，Infosys的人力资源团队当时曾向应届毕业生们许诺，入职计划将于8月19号或9月2号敲定。但NITES在信中强调，事情并未像预期般推进，导致这些号称被录取的毕业生们感到“极度沮丧、焦虑和迷茫”。</p><p>&nbsp;</p><p>信件内容如下：</p><p></p><p></p><blockquote>尽管成功完成了入职培训，但承诺的结果却始终未能落地，导致毕业生们在20多天时间内完全不知所措。更令他们震惊的是，这些毕业生们不仅没有收到确切的入职日期，反而被告知还需要重新接受线下入职培训和考试，且期间同样没有任何报酬。</blockquote><p></p><p>&nbsp;</p><p>技术媒体The Register日前报道称，Infosys的新员工们接受了“多次无薪的线上与线下培训课程及评估”，并引用了该公司发给新员工们的电子邮件。报道还提到，如果新员工们拒绝参加这些课程，则将不再拥有入职机会，而且其中至少包括一门长达六周的课程。</p><p>&nbsp;</p><p>该消息在社交平台引发了热议，一位自称是Infosys在职员工的网民称，“我们工作14小时哪还有工作留给别人做。”</p><p></p><h2>CEO宣称Infosys最终将接纳这些新员工</h2><p></p><p></p><p>在收到NITES的信件之后，Infosys公司CEO Salil Parekh本周宣布毕业生们即将正式入职，但没有提供更具体的上岗时间，也未解释为什么要推迟这么长时间并重复组织培训课程。在接受印度新闻网站Press Trust of India采访时，Parekh回应称：</p><p>&nbsp;</p><p></p><blockquote>我们给出的每份录取通知都为新人们提供相应的入职岗位。我们确实调整了具体入职日期，但除此之外，所有人都会最终入职Infosys，向来如此。</blockquote><p></p><p>&nbsp;</p><p>值得注意的是，在上个月的财报电话会议上，Infosys公司首席财务官Jayesh Sanghrajka曾表示“计划今年招聘1.5万至2万名”应届毕业生，“具体取决于我们对业务的增长预期。”目前还不清楚这个数字，是否包含NITES所关注的这2000名新员工。</p><p>&nbsp;</p><p>今年3月，Infosys公司报告其员工人数为31万7240人，也是自2001年以来首度出现人员规模缩减。Parekh最近还宣称，Infosys公司预计不会因AI等新兴技术的影响而裁员。在最新一轮财报中，Infosys公司报告称利润同比增长5.1%，收入同比增长2.1%。</p><p>&nbsp;</p><p>NITES此前曾主张受入职延误影响，Infosys应“支付入职延误期间的全额工资”；如果最终未能入职，Infosys应帮助求职人员在公司之外找到其他岗位。</p><p>&nbsp;</p><p>截至 2024 年 6 月，Infosys 的员工总数进一步收缩为 315,332 人。</p><p></p><h2>Infosys称正大力发展AI，不会裁员</h2><p></p><p></p><p>NITES在信中提出，Infosys的行为已经对印度的经济增长产生了负面影响，并强调：</p><p>&nbsp;</p><p></p><blockquote>这些年轻的工程毕业生是我们国家IT行业未来不可或缺的一部分，而IT行业在我们的经济运行中发挥着关键作用。尽管满足了所有要求，但这些专业人员却被搁置了两年多，这给他们带来了极大的挫败感、焦虑和不确定性。Infosys这种推迟他们入职进度、并要求其从事无偿工作和反复评估的行为，不仅浪费了年轻人们的宝贵时间，也损害了他们可能为印度经济增长做出的实际贡献。这不仅是印度最大的 IT 公司之一背信弃义的行为，而且是一个影响我国 IT 劳动力和经济未来的重大问题。</blockquote><p></p><p>&nbsp;</p><p>Infosys方面则一直没有解释为何这数千名新员工的入职周期比预期中更长。一个可能的原因在于入职流程受阻。疫情对于印度的打击尤其严重，Infosys此前也曾延长过入职时间。</p><p>&nbsp;</p><p>此外，印度正在面临着严重的岗位短缺问题。等待两年才能找到一份工作已经成为常态，而且越来越多的人发现自己的职业选择空间正在缩小。2024年6月，一项印度招聘趋势研究报告发现，硬件和网络领域的IT职位招聘量同比下降了9%，软件和软件服务领域的招聘量同比下降5%。据印度杂志《Frontline》报道，印度IT行业的流失率已经由2022年的27%下降到去年的16%至19%，愈发固化的人员流动导致印度的IT职位（特别是初级职位）有所减少。由于人们更倾向于坚守当前的岗位，企业的招聘力度也有所降低。《Frontline》举例指出，Infosys公司在2023年和2024年都没有进行任何校园招聘，总部位于印度的塔塔咨询服务公司同样叫停了校招流程。</p><p>&nbsp;</p><p>过去两年间，Infosys一直为印度未来几年预计出现的IT技能缺口而维持着稳定的人才储备，导致新近毕业的IT毕业生难以找到工作机会。但科技企业的人才流失不可能彻底消除，总会有一部分从业者想要换个环境、另寻东家。而现在从业者们显然又有了新的麻烦需要操心——申请政府干预，帮助自己解决因为入职拖延导致的经济损失和心理压力。</p><p>&nbsp;</p><p>处于印度经济萎靡不振的困境中，Infosys也在寻求进一步增长之路。</p><p>&nbsp;</p><p>Infosys 首席执行官 Salil Parekh 表示，生成式人工智能引起了客户的强烈兴趣，公司内部也在大力推广 GenAI。不过，他坚称，公司不太可能因为新时代技术而裁员。</p><p>&nbsp;</p><p>Salil Parekh 表示，客户对生成式人工智能的接受度很高，他将其与过去数字和云技术的采用曲线进行了类比，因为他相信，随着企业体验到由此带来的好处和业务成果，GenAI 的采用将随着时间的推移而增加。</p><p>&nbsp;</p><p>Salil Parekh 表示，他不认为 GenAI 的出现会导致公司裁员。他说：“所以，在这个阶段，我的感觉是，这项技术将帮助业务进一步增长，而不是其他任何事情。我们没有看到 Infosys 因这些新时代的技术而裁员，事实上，随着经济环境的变化，我们继续增加招聘……正如你在第一季度看到的那样，我们实现了季度环比和年度强劲增长，我们获得了非常好的大宗交易。因此，我们提高了我们的指导。所以我们看到增长正在慢慢恢复。”</p><p>&nbsp;</p><p>据报道，印孚瑟斯公司今年早些时候披露，该公司正在开发 225 个生成式人工智能客户项​​目，超过 25 万名员工正在接受生成式人工智能培训。&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://arstechnica.com/information-technology/2024/08/exploitative-it-firm-has-been-delaying-2000-recruits-onboarding-for-years/">https://arstechnica.com/information-technology/2024/08/exploitative-it-firm-has-been-delaying-2000-recruits-onboarding-for-years/</a>"</p><p><a href="https://www.thehindubusinessline.com/companies/infosys-delays-onboarding-yet-again-2000-graduates-left-hanging/article68547091.ece">https://www.thehindubusinessline.com/companies/infosys-delays-onboarding-yet-again-2000-graduates-left-hanging/article68547091.ece</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/KLLoROGmSa7yi0zXJIVc</id>
            <title>华为张平安：跨越技术鸿沟，为数智化供需“架桥铺路”</title>
            <link>https://www.infoq.cn/article/KLLoROGmSa7yi0zXJIVc</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/KLLoROGmSa7yi0zXJIVc</guid>
            <pubDate></pubDate>
            <updated>Wed, 28 Aug 2024 10:22:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>8月27日，在2024中国国际大数据产业博览会上，第三届828 B2B企业节正式开幕。华为常务董事、华为云CEO张平安发表致辞表示，数字世界的话语权最终是由生态的繁荣决定的，华为云携手上万家伙伴，整合“技术+生态”能力，共同发起828 B2B企业节，打通生态伙伴“共建-共营-共销”的价值流，把最合适的技术、经由最短的路径、以最合理的模式，给到最需要的场景，为中国企业数字化、智能化构筑起“一站式平台服务”。</p><p></p><h2>联手共筑智算高地，扎根贵州，服务全国</h2><p></p><p></p><p>作为首个国家大数据综合试验区，贵州省正推动数据中心由存储中心加快向“存算一体、智算优先”，抢占智算制高点。华为云与贵州持续深化合作，在贵州布局了全球最大的数据中心，通过系统架构创新打造业界领先云数据中心和AI算力中心。</p><p></p><p>张平安介绍，华为云贵安数据中心，容纳百万台服务器，PUE达1.12，处于世界领先水平。今年，华为云还发布并全面应用了全新的面向多元算力的CloudMatrix架构，以“一切可池化”“一切皆对等”“一切可组合”的创新设计，从算力规模、扩展模式和使用模式上，匹配超大规模算力诉求。并通过AI技术精准控温、云服务感知的能效调优等多项创新技术的应用，打造高能效、高可靠数据中心。</p><p></p><p>基于华为云全球存算网核心枢纽的时延覆盖圈，贵安数据中心可支持企业90%的业务集中部署，领先的技术让贵安数据中心为全球客户提供服务成为可能，企业不用操心基础设施建设和产品的升级换代，云上算力随取随用，技术永新。全国多家头部互联网、人工智能企业如上海百胜、科大讯飞等已入驻。</p><p></p><p>华为云智算基地落地贵安新区，也在加快贵州打造具有国际竞争力的人工智能算力高地、AI生态基地和数据生态基地的步伐。现场，贵州省大数据局携手华为云及生态伙伴共同发起贵州“8+4”产业生态创新行动。针对贵州省昇腾算力生态、行业大模型生态、人工智能人才生态展开深入合作，双方共同在煤矿、酱酒、新材料等8个重要行业，在城镇智慧化、乡村数字化等4个重要领域，推进人工智能应用的孵化和落地，加速人工智能在贵州省的全面发展，让更多创新的AI应用在贵州落地。</p><p></p><h2>做强智能世界云底座，打通企业数智升级最优路径</h2><p></p><p></p><p>828 B2B企业节让“产品” 与“需求” 通过平台连接起来，同时带动更广泛的生态共振，为企业数字化转型、智能化升级带来新动力。</p><p></p><p>而作为数字世界的坚实技术底座，华为云正携手行业客户、伙伴加快打造自主创新的数字技术生态，以云作为统一平台和生态入口，联合鸿蒙、鲲鹏、昇腾、高斯等技术生态，做强智能世界云底座。</p><p></p><p>在人工智能领域，中国拥有千行万业的业务场景，如果各行各业都积极拥抱AI，开放业务场景，中国非常有机会在全球构筑起领先优势。华为云通过昇腾AI云服务支持百模千态，通过盘古大模型重塑千行万业，助力产业智能化。目前，华为云携手伙伴将盘古大模型落地到了矿山、电力、气象、医药等30多个行业，400多个模型应用场景，持续为各行各业的生产场景“解难题、做难事”，重塑千行万业。</p><p></p><p>在软件领域，中国拥有全球最大的软件创新人群，但在核心软件上一直处于落后的状态。张平安表示，随着企业在数字化、智能化方面更多的需求，我们要构建云原生的核心软件和开发工具链，给世界一个更好的选择。华为云打造云原生的核心软件，已陆续发布了23款软件开发工具，并推出了分布式云核心底座、GaussDB数据库、软件开发生产线CodeArts、硬件开发生产线CraftArts等一系列产品和技术，支持企业研发高效创新，加速客户与伙伴的应用现代化。</p><p></p><p>同时华为云也不断完善生态体系，助力伙伴能力提升与商业成功。今年华为云发布了新的伙伴能力计划，从产品技术、场景服务、行业经验三个方向上来加速伙伴能力成长。目前华为云已汇聚4.5万伙伴，联合构建了500多个行业解决方案，12000款云商店商品。</p><p></p><p>张平安在讲话的最后指出，“数字化是全球科技革命和产业变革的焦点，我们深刻感知，中国的数字经济发展和技术创新，仍需跨越一道道鸿沟，数字产业大花园的繁茂，仍需要更多的灌溉。”他呼吁政府、产业、学术和研究机构共同参与，共同促进智能时代技术生态的繁荣。“成就好生意，成为好企业！”</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/AsZdv8DDHWulU28jYUtI</id>
            <title>AICon 上海站 2024 优秀出品人与明星讲师名单揭晓，RAG、Agent、工具链专场最受欢迎</title>
            <link>https://www.infoq.cn/article/AsZdv8DDHWulU28jYUtI</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/AsZdv8DDHWulU28jYUtI</guid>
            <pubDate></pubDate>
            <updated>Wed, 28 Aug 2024 09:38:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>8 月 18 日 -19 日，<a href="https://aicon.infoq.cn/202408/shanghai/">AICon 全球人工智能开发与应用大会 2024</a>"（上海站）圆满落幕，会议话题涵盖端侧模型落地探索、大模型训练以及推理加速、数据集构建及评测技术落地、安全性实践、RAG 落地应用与探索、AI Agent 技术突破与应用、多模态大模型、大模型行业应用落地、大模型与企业工具集成的提效实践、大模型在搜索、广告、推荐领域的探索、产品应用构建、大模型产学研结合探索等。</p><p></p><p>此次会议邀请 60 余位来自国内外的专家，涵盖字节跳动、百度、华为、京东、阿里巴巴、微软亚洲研究院、小红书、腾讯、英特尔、快手、网易伏羲、携程、小米、顺丰科技、蚂蚁集团、哔哩哔哩、零一万物、Llamalndex、月之暗面、硅基流动、极佳科技、上海人工智能实验室、卫宁健康、惠每科技、矩阵起源、未来智能、岩芯数智、PayPal、熊墅科技、商汤科技、next.ai、蔚来汽车、华院计算、复旦大学、前极客邦科技、数美科技、中国科学技术大学、喜马拉雅、蒙特利尔大学 &amp;MILA 研究所、北京大学、上海交通大学、英飞流、智源研究院、上海人工智能实验室、合合信息等，超过 1000 多名对大模型感兴趣的听众参与了此次会议。</p><p></p><p></p><h4>主题演讲</h4><p></p><p></p><p>在 8 月 18 日上午主论坛环节，上海市邮政管理局党组书记、局长冯力虎发表开场致辞，鼓励科技创新，并希望 AICon 大会能成为创新的起点；顺丰科技副总裁唐恺深入介绍了顺丰在物流领域的技术创新并揭晓了顺丰物流决策大模型的神秘面纱，随后上海市邮政管理局党组书记、局长冯力虎，顺丰集团副总裁龚威、顺丰科技副总裁唐恺、零一万物联合创始人祁瑞峰、智谱 AI 副总裁吴玮杰、华为云盘古大模型 CTO 李寅、浙江大学管理学院副院长杨翼以及极客邦科技创始人兼 CEO 霍太稳共同登台联合发布了顺丰物流决策大模型，见证了物流行业创新的重要时刻；蔚来创始人、董事长、CEO 李斌分享了蔚来近年在智能电动汽车和 AI 方面的思考与实践，强调 AI 将成为智能电动汽车企业的核心基础能力；英特尔院士、大数据技术全球 CTO 戴金权分享了英特尔过去一两年在大模型的异构计算和加速方面所做的工作；面壁智能联合创始人兼 CEO 李大海提出了大模型的知识密度提升趋势，并强调了提升知识密度是实现高效大模型的关键 ；最后，字节跳动研究科学家冯佳时分享了基于 LLM 的视频生成和图像理解的进展 。</p><p></p><p>详细报道见：<a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621971&amp;idx=3&amp;sn=5a7aa243180f43f8c360808338ffab4d&amp;chksm=fbeba49ccc9c2d8aaf45db62de584cf41bc24864c1babe9fe52909ddd02bd84030cd79dfe4c1&amp;scene=21#wechat_redirect">AICon 上海站精彩回顾，从大模型变革之路到高效“炼丹”指南，超 60 位大模型先锋输出最前沿干货！</a>"</p><p></p><p></p><h4>焦点</h4><p></p><p></p><h5>RAG 落地应用与探索</h5><p></p><p></p><p>作为本次大会 最热门的专题，*****火爆程度 5 颗星，场均人次 280+，该专题聚焦检索增强生成技术实践，专题出品人是阿里巴巴技术总监郭瑞杰，专题邀请了阿里云高级算法专家欧明栋、 PayPal 高级 ML 科学家王元、PayPal 算法工程师马泽宏、合合信息智能创新事业部研发总监常扬、英飞流 CEO 张颖峰、LlamaIndex 创始 AI 工程师 Pierre-Loic Doulcet 共 6 位讲师同台分享，分享 RAG 在企业中的实践。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ed/ed2a2f6737026efb47b0699a373e2413" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/62/625a8f3f96089cd3678f44dab2198f19" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c8/c8abbbcfd6edc20ef85a2fd68e7821ba" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/83/83a823cd04e48502877ec19cb2032732" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/13/132add8b094fbda7b41d51e3727408e6" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a6/a625c896463e72b104a4651eb8a9d57c" /></p><p></p><p></p><h5>大模型与企业工具集成的提效实践</h5><p></p><p></p><p>本次火爆程度 4 颗星，场均人次 160+。 在 ChatGPT3.5 发布后，企业工具经历了一波 AI 集成和升级浪潮，大量的企业工具尝试集成大模型进行企业提效，但大模型概率性的本质和不可解释性导致在集成上遇到许许多多的问题，人们也经历了信心爆棚到绝望再重拾信心的转变。本专题分享了大模型与企业工具的集成实践和从业者的心路历程，并探讨 AI 在哪些场景更能为企业带来助力。专题出品人是阿里巴巴代码平台负责人 向邦宇，专题演讲嘉宾分别是阿里巴巴高级算法工程师林智超、研发效能领域知名专家路宁、next.ai 创始人蒋志伟。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ed/eddc1b82a9c2c7a6d2615eb8dfe56702" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/da/da4ad692c4a54bfece5897e39dd92326" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/d0/d0f39194e30b895aa26c5ad66c342628" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/03/0337bdf891ec6015edc9164435c15675" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/5a/5a8890cbb88818fd9aa31b3e32885690" /></p><p></p><h5>AI Agent 技术突破与应用</h5><p></p><p></p><p>本次火爆程度 4 颗星，场均人次 140+。</p><p></p><p>AI Agent 正迅速成为大模型非常重要的应用方向，这些智能实体通过先进的机器学习和人工智能技术，能够自主感知环境、做出决策并执行相关动作。AI Agent 的应用场景日益广泛，包括但不限于数字员工、具身智能、个性化推荐等。在【AI Agent 技术突破与应用】论坛，我们邀请到了 DeepWisdom（MetaGPT）创始人兼 CEO 吴承霖担任出品人，并邀请到了微软亚洲研究院高级研究员宋恺涛、网易伏羲语言智能组负责人张荣升、蒙特利尔大学 &amp;MILA 研究所助理教授刘邦、腾讯 PCG 大模型中台 Agent 技术负责人陈浩蓝四位嘉宾来进行演讲分享，为听众送上他们的 Agent 开发精彩实践。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8a/8a0a1d650f2e4fe3b608d1617c4f8da5" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/24/2412e5ea4d4f8060b8e6d90d06fc4f74" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7a/7a26b294a755d5d6199c7ae4eebe6096" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/cd/cd21af8377f02953d04e450735b806b2" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f9/f92c7521801ee176110ce8f5aa0a4ef8" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/91/917f73a6e0a1eca414092cee06e5a36e" /></p><p></p><p></p><h4>优秀出品人以及明星讲师</h4><p></p><p></p><p>本次大会共有 12 位专题出品人，他们都是各自领域的权威专家。出品人的主要任务是确保各自专题的分享内容质量，包括在前期阶段对议题进行深入的讨论和打磨，以及对演讲材料进行严格的审核。经过评选，最终有 4 位专题出品人因其对本次大会内容策划的杰出贡献而被授予“优秀出品人”的称号，名单如下：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/fd/fdafe1d33b34d9d4586053ffebfdc107.png" /></p><p></p><p>AICon 大会讲师的选拔既包含组委会定向邀请，也开放给公众提交演讲主题。为了确保演讲内容的质量，所有提交的议题都必须符合六大标准：明确的观点、以实践为基础、深入且有见地的分享、良好的专业声誉、禁止任何形式的广告、以及确保听众能从中获得实质性的收获。要从众多杰出的演讲者中脱颖而出，获得“明星讲师”称号，演讲者不仅需要在大会筹备期积极配合组委会反复打磨议题和 PPT，提供既有深度又实用的分享内容，还需要在大会现场展现出色的演讲表现，并获得至少 90% 的听众满意度（在满意度测评中，听众对讲师的评价分为“非常满意”、“满意”、“一般”和“不满意”四个等级，听众满意度指“非常满意”＋“满意”评价在收到的所有评价中的占比）。经过严格的评选，共有以下演讲者荣获“明星讲师”的荣誉：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/db/dbe1789ca13e0bbc443ae6f2f74e0c12.png" /></p><p></p><p>在获得“明星讲师”的演讲嘉宾中，数位讲师的听众满意度更是超过 95%，甚至达到 100%。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/9b/9b4d13aee334b8a86fc970ff53d6a3cd.png" /></p><p></p><p>他们分别是：</p><p></p><p>字节跳动研究科学家，豆包大模型视觉基础研究团队负责人冯佳时《大语言模型在计算机视觉领域的应用》快手可图大模型团队负责人李岩《快手可图大模型的技术演进与应用探》喜马拉雅珠峰 AI 算法负责人叶剑豪 《生成式音频大模型的多模态“产模结合”》上海交通大学计算机科学与工程系副教授林云 《语言模型驱动的软件工具思考：可解释与可溯源》商汤科技系统研究员雷丹 《SensePPL 端侧大模型系统与优化》智源研究院大模型行业应用总监 周华 《智源行业数据集及训练方法落地实践》</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/2d/2dfbfc98a1db9a2c7bdd336d725b4696" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/47/479b99b8cc1afa9267184b4247e93b64" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ba/ba3568c3aca74d7035195067b7d4c123" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a6/a646efdfa4f2c61305c0ec9c1e8fa948" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f8/f8e1acd7e88c227d42fbf7d59ebf057b" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b6/b6c81186f8ba0fae5774eb6544f06f0b" /></p><p></p><p>我们衷心感谢每一位参与 AICon 的出品人与讲师的精彩分享与辛勤付出，正是他们的努力，AICon 才能为听众带来无数精彩的内容与深刻的见解。</p><p></p><p></p><h4>共创未来：我们的合作伙伴阵容</h4><p></p><p></p><p>AICon 上海站的圆满举办，离不开赞助商们贡献的力量。感谢英特尔、亚马逊云科技、Google Cloud、矩阵起源、百道数据、Optiver、数势科技、未来智能、UCloud 优刻得、钛动科技、零一万物、快递 100、快手、昇腾对本届大会的倾情赞助以及蔚来汽车为大会展区带来的特别策划。在大家的共同助力下，我们得以持续推动技术的传播与发展，为行业创新注入不竭源泉。</p><p></p><p>经统计，AICon 上海站现场听众累计超过 1000 人次。我们深感荣幸与欣慰，衷心感谢每一位参与者的鼎力支持与不断鼓励。正是因为有了大家的热情参与和积极贡献，我们才能坚定不移地追求目标，致力于成为技术传播领域的佼佼者。我们将持续不断地提升内容的质量，致力于打造更加优质、更具包容性的交流平台，让每一个人都能在这里找到启发和灵感，一齐推动技术领域的创新与突破，为未来的科技进步贡献力量。</p><p></p><p>大会 PPT 获取通道已开启，关注&nbsp;AI 前线&nbsp;公众号，后台回复“PPT”，即可获取 PPT 下载地址！（由于讲师所在企业限制，部分 PPT 仍在审查或不对外公布，详情见大会官网日程） &gt;&gt;&gt;</p><p></p><p>至此，今年 InfoQ 中国已圆满落幕 5 场技术盛会，随后还将于 10 月 18 -19 日在<a href="https://qcon.infoq.cn/202410/shanghai/">上海举办 QCon 全球软件开发大会 </a>"，覆盖前后端 / 算法工程师、技术管理者、创业者、投资人等泛开发者群体，内容涵盖当下热点（AI Agent、AI Infra、RAG 等）和传统经典（架构、稳定性、云原生等），侧重实操性和可借鉴性。现在大会已开始正式报名，可以享受 8 折优惠，单张门票立省 960 元（原价 4800 元）。如您感兴趣，可点击「阅读原文」查看详情或联系票务经理 &nbsp;17310043226 咨询。</p><p></p><p>期待下一场大会再见！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/zY6Ks5qM2bBL6Gs3izks</id>
            <title>夸克发布全新 PC 端，系统级全场景 AI 能力升级 AI 电脑</title>
            <link>https://www.infoq.cn/article/zY6Ks5qM2bBL6Gs3izks</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/zY6Ks5qM2bBL6Gs3izks</guid>
            <pubDate></pubDate>
            <updated>Tue, 27 Aug 2024 10:18:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>将一台电脑升级为 AI 电脑需要几步？今后只需安装一个夸克就够了！8 月 27 日，阿里智能信息事业群旗下夸克发布全新 PC 端，全面升级 AI 搜索、AI 写作、AI&nbsp;PPT、AI 文件总结等一系列功能。凭借“系统级全场景 AI”能力，夸克为你升级AI电脑，一站式完成信息的检索、创作和总结。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/48/32/4882ff0e6bc3b6c0c513d5eaaffb4932.png" /></p><p></p><p>“始终面向用户、面向 AI、面向未来，夸克在人工智能领域持续探索突破性的用户体验。”阿里智能信息事业群总裁吴嘉表示，全新的夸克开启了创造革新性搜索产品的无限可能，也为阿里巴巴人工智能战略布局增添了强有力的路径与动能。</p><p>&nbsp;</p><p></p><h2>一、夸克 PC 端功能上新，为你升级一台 AI 电脑</h2><p></p><p></p><p>数字时代，PC 成为生产力的代名词，随着用户需求迭代，以及生成式 AI 技术跃迁，PC 的智能化改造成为必然。全新夸克 PC 端升级多项能力，让你的电脑秒变 AI 电脑，辅助你完成复杂、重复的任务，让效率再翻倍。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/3b/94/3bba3758ce5e89231a57bd7961c54f94.jpg" /></p><p></p><p>一个月前夸克在 App 端推出的全新 AI 搜索，此次一并在 PC 端发布，并升级了更强的模型能力，提升到更快的交互速度。夸克AI回答的首字出现速度和吐字速度大幅领先行业，瞬时就能给你精准答案。三栏式的界面设计能更清晰地展现图文、视频等生成式回答和网页，让你一眼就能得到核心信息。</p><p></p><p>写作无疑是 PC 用户的高频需求，针对大学生、白领等重度用户，夸克就是你的“笔杆子”，任何体裁文章都能写得出色。当你输入主题和字数等要求后，夸克能撰写近 200 种类型的文稿，半分钟就能产出一篇高质量文章。夸克还提供多种方式撰写 PPT，比如输入主题智能生成大纲，或筛选模板再编辑内容，还支持 Word 一键转成 PPT。夸克帮你化繁为简，让你更专注创作本身。</p><p></p><p>此外，当你在 PC 上阅读大量的专业文档和网站内容时，想秒懂里边的内容，更需要一个会思考、能理解、会表达的小助理。夸克 AI 文件总结不惧几十万字的长文，能快速提取 PDF、Word、PPT 等文档中的核心内容，并通过持续提问、生成脑图等方式，更好地帮助用户理解关键信息。</p><p></p><p>夸克产品负责人郑嗣寿表示：“用户的需求在哪里，夸克就在哪里。夸克 PC 端给用户的信息检索、信息生成和信息处理带来了更快的速度和更强的效果，这是我们利用 AI 技术面向用户创造的新价值。”</p><p>&nbsp;</p><p></p><h2>二、系统级全场景 AI，随时随地帮你解决实际问题</h2><p></p><p></p><p>在 PC 中，用户会在桌面、文档、网页等多场景中进行操作，反复切换也练就了“黄金指”。夸克让电脑秒变AI电脑后，具备“系统级全场景 AI” 能力，在 Windows 电脑按下 Alt+Space 或苹果电脑的Option+Space，可以随时随地使用 AI 回答、AI 写作、AI&nbsp;PPT、AI 文件总结等功能。只要你有问题和需求，夸克的AI能力无处不在、触手可及。</p><p></p><p>比如在查网页、看文档等场景中，夸克能通过划词、截屏等方式，更加丝滑地进行搜索、解读、翻译和润色，无需再单独开启其他应用。就连辅导孩子作业，夸克也可以提供截屏搜索，依托海量学习题库和学习专属大模型，提供解题思路和答案，让自学和辅导的效率全面升级。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/34/c6/343e287e22353869be08e3588796c7c6.png" /></p><p></p><p>“系统级全场景 AI”能力还会深入到电脑的每个场景中。右键点击文档，夸克能帮你快速总结Word、PDF、TXT等常用文件中的关键信息，还能一键帮你转换文档格式。夸克还提供官方插件，让你的浏览器变成AI浏览器，同样能使用 AI 回答、AI 写作、解读、翻译以及网页总结等功能。</p><p></p><p>在哪都能用，随你怎么用！夸克 PC 端不仅大幅提升了用户使用搜索、写作等功能的效率，也让 AI 电脑成为每个人的标配。随着用户需求的不断迭代，夸克的产品创新也会持续演进，让更多 AI 能力落地到不同设备的不同场景中。</p><p>&nbsp;</p><p></p><h2>三、突破性用户体验，创新践行“AI 驱动”战略</h2><p></p><p></p><p>夸克从诞生以来，以 AI 技术为业务发展引擎，面向用户探索下一代智能信息产品，短短数年就成长为用户过亿、增长强劲的新锐产品，尤其是获得了年轻人群的青睐。</p><p></p><p>进入 AI 时代，阿里集团将 AI 作为改变和加速业务增长的最强大变量，所有业务场景都可以通过人工智能创造更大的价值。夸克凭借多年积累沉淀的大模型技术、多应用场景、年轻用户群体等优势，大力革新搜索产品体验。自升级AI搜索以来，全新的夸克在用户规模与产品口碑方面均有不错的市场表现。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/70/32/70dcdeee01e1ca334695d46b2d75f532.png" /></p><p></p><p>数据显示，6 月高考季，夸克高考 AI 搜索使用量超过 1 亿次。7 月，夸克升级“超级搜索框”，推出以 AI 搜索为中心的一站式 AI 服务，持续霸榜苹果应用商店免费榜。在《 2024 年第二季度 iOS 实力 AI 产品排行榜》，夸克作为 AI 搜索产品新兴势力，以 99.71 的高分在一众AI应用中位居榜首。</p><p></p><p>“生成式 AI 技术的突飞猛进，让夸克的目标和愿景更有机会得以落实和推进，加速了夸克的能力跃迁和产品迭代。”郑嗣寿透露，接下来，夸克会继续保持极快的迭代速度，在 AI 产品体验上迅猛推进，为用户创新一站式、多端一体的 AI 服务。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/gJpA5gfd2Frvou4lGpJc</id>
            <title>人人创造，一起热AI ｜火山引擎首届AI创造者大赛来啦！</title>
            <link>https://www.infoq.cn/article/gJpA5gfd2Frvou4lGpJc</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/gJpA5gfd2Frvou4lGpJc</guid>
            <pubDate></pubDate>
            <updated>Tue, 27 Aug 2024 09:44:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AI正以前所未有的速度融入千行百业，成为各行业智能化转型的加速器。火山引擎正式发起「AI创造者大赛」，大赛首场为汽车行业专场，由火山引擎携手领克汽车与英特尔联合主办、吉利汽车研究院协办，旨在携手汽车行业领军品牌，鼓励开发者利用豆包大模型和扣子专业版，针对领克汽车的真实业务场景，开发出具有实际应用价值的智能体解决方案。</p><p></p><p>本次大赛共设置三大赛道——AI 座舱赛道、AI 营销赛道、AI 售后赛道。目前大赛报名通道已开启，可登录火山引擎官网查看更多赛事详情，报名参赛即有机会赢取领克汽车Z10全年使用权，更有丰厚奖金与礼品等你带回家！</p><p></p><p><img src="https://static001.geekbang.org/infoq/fc/fc8ca60bc6ac31816dc7296433f1232d.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/USe4z1Yv0xd208XXboGe</id>
            <title>InfoQ 2024年趋势报告：AI 智能体发展不及预期，RAG 或成最大赢家</title>
            <link>https://www.infoq.cn/article/USe4z1Yv0xd208XXboGe</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/USe4z1Yv0xd208XXboGe</guid>
            <pubDate></pubDate>
            <updated>Tue, 27 Aug 2024 02:41:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>作为InfoQ的一大特色，趋势报告系列专注于软件开发的各个关键领域。这些报告旨在为InfoQ的读者和听众提供今年值得关注的技术发展趋势概览。</p><p></p><p>InfoQ的人工智能、机器学习和数据工程编辑团队邀请了业界专家，共同探讨了人工智能和机器学习领域的未来趋势，以及接下来12个月中值得关注的动态。本期播客记录了这次讨论的内容，小组成员们分享了他们对创新人工智能技术如何重塑行业格局的见解。</p><p></p><p></p><h1>关键要点：</h1><p></p><p>人工智能的未来是开放的。我们正处于大语言模型和基础模型的时代。尽管目前大部分模型是闭源的，但像Meta这样的公司正试图引领开源模型的趋势。检索增强生成（RAG）的重要性将日益凸显，特别是在大规模部署LLM的场景中。随着人工智能赋能的GPU基础设施和人工智能驱动的个人电脑的出现，AI驱动的硬件将获得更多关注。由于受基础设施设置和管理成本方面的限制，小语言模型（SLM）将得到更多的探索和采用。小语言模型也是边缘计算相关用例的一个很好的选择，它们可以在小型设备上运行。AI代理，如编码助手，将在企业应用开发环境中得到更多的采用。在语言模型的生命周期管理中，AI的安全性和隐私保护将持续占据重要地位。自托管模型和开源LLM解决方案将有助于加强AI的安全防护。LangOps或LLMOps将成为LLM生命周期的关键环节，它们在大模型生产环境部署的持续支持中发挥着重要作用。我们对未来12个月的AI发展做出了一些预测：机器人AI，即具身AI，将成为新的趋势；从AI寒冬过渡到更多具体的应用场景，涉及更多自动化工作流和智能体工作流，然后扩散到更多的边缘设备，如笔记本电脑和手机。</p><p></p><h1>简介</h1><p></p><p></p><p>Srini Penchikala：大家好，欢迎收听2024年人工智能与机器学习趋势报告播客。这个播客是我们年度报告的一部分，目的是与听众分享人工智能和机器学习领域的最新动态。我是Srini Penchikala，InfoQ人工智能、机器学习和数据工程社区的主编。我们有幸邀请到了一群杰出的专家和实践者，他们来自人工智能和机器学习的不同领域。</p><p></p><p>感谢大家来到这里。我非常期待与大家共同探讨人工智能和机器学习领域的最新动态，包括我们目前的发展阶段，更重要的是我们未来的发展方向。特别是自去年我们讨论趋势报告以来，人工智能技术的创新速度之快令人目眩。在开始深入播客主题之前，我想先向听众朋友们提供一些必要的信息。我们的年度报告包含两个主要部分。首先是这个播客，它提供了一个平台，让听众能够听到来自专家实践者们对创新人工智能技术如何颠覆行业的见解。其次是一份书面文章，将在InfoQ网站上发布，其中将包含技术采用不同阶段的趋势图，并详细介绍自去年趋势报告以来新增或更新的个别技术。</p><p></p><p>我强烈推荐大家在本月底文章发布时去阅读它。现在，让我们回到播客的讨论上来。自ChatGPT发布以来，生成式人工智能和大型语言模型技术的发展速度似乎达到了顶峰，而且这种快速的创新势头似乎不会很快放缓。技术领域的主要参与者都一直在忙着发布他们的人工智能产品。今年早些时候，谷歌在I/O大会上发布了几项新的更新，包括Gemini更新和生成式人工智能在搜索中的应用。同时，OpenAI也发布了GPT-4o，这是一个能够实时处理音频、视觉和文本的全能模型，提供了一种多模态解决方案。</p><p></p><p>紧接着，Meta也发布了Llama 3，并很快推出了基于4050亿参数的Llama 3.1版本。这些参数的数量级是亿，而且它们还在不断增加。像Ollama这样的开源解决方案也受到了越来越多的关注。看来这个领域一直在加速发展。生成式人工智能技术的基础是大语言模型，它们经过大量数据的训练，能够理解和生成自然语言及其他类型的内容，还能执行丰富多样的任务。因此，LLM可以作为我们今年趋势报告讨论内容的切入点。</p><p></p><p>Anthony，你一直在密切关注LLM模型及其发展。你能谈谈生成式人工智能和LLM模型的当前发展状态、最近的一些主要进展，以及我们的听众应该关注哪些方面吗？</p><p></p><p></p><h1>AI的未来是开放的</h1><p></p><p></p><p>Anthony Alford：如果要用一个词来概括LLM，我会选择“更多”，或者可能是“规模”。我们正处在LLM和基础模型的黄金时代。OpenAI可能是最显眼的领导者，当然，还有其他大玩家，比如谷歌，还有Anthropic推出的Claude。这些模型大多是封闭的，即便是OpenAI，他们的旗舰产品也只能通过API访问。然而，Meta在这方面是一个异类。实际上，我认为他们正试图引领趋势朝着更开放的方向发展。我记得扎克伯格最近说过，“人工智能的未来是开放的。”因此，他们开放了一些模型的权重。至于OpenAI，尽管他们没有公开模型权重，但也会发布一些模型的技术细节。例如，我们知道GPT-3的第一个版本有1750亿个参数，但对于GPT-4，虽然他们没有明确说明，但趋势表明它肯定拥有更多的参数，数据集更大，计算预算也更大。</p><p></p><p>我认为我们还将继续见证的另一个趋势是模型的预训练，也就是GPT中的“P”。这些模型在庞大的数据集上进行预训练，基本上是整个互联网的内容。然后，他们会进行微调，这是ChatGPT的关键创新之一。因此，这种指令微调现在变得极其普遍，我相信我们将继续看到这一趋势。接下来，让我们转到上下文长度这个话题，因为它代表了另一个发展趋势。上下文长度，即你可以输入模型的数据量，这个量正在增加。我们可以讨论这与新的SSM（State Space Model，状态空间模型，如Mamba）之间的区别，因为SSM是没有上下文长度限制的。Mandy，你对这个话题有什么看法？</p><p></p><p>Mandy Gu：我认为这绝对是我们正在见证的一个明显趋势，那就是更长的上下文窗口。当初ChatGPT等大语言模型开始普及时，这是人们普遍指出的一个不足之处。今年早些时候，Gemini、Google基金会以及GCP的基础模型引入了高达一百万个Token的上下文窗口长度，这无疑是一个改变游戏规则之举，因为之前我们从未有过如此长的上下文处理能力。我认为这引领了一种趋势，其他供应商也在尝试提供同样长甚至更长的上下文窗口。由此产生的一个二级效应是提升了可访问性，它使得像信息检索这样的复杂任务变得更加简单。在过去，我们可能需要进行多阶段的检索，例如RAG，但现在，我们可以将所有上下文信息直接输入到这一百万Token的上下文窗口中，虽然不一定意味着更好，但无疑简化了过程。这是过去几个月的一个非常有趣的进展。</p><p></p><p>Anthony Alford：Namee，你还有什么要补充的吗？</p><p></p><p>Namee Oberst：我们专注于小语言模型的应用。较长的上下文长度窗口有它的价值，但根据我们内部的研究以及YouTube上一些知名人士的实验，即便你只传了2000个Token的段落给大模型，它们在处理段落中间信息丢失的问题上表现并不出色。因此，如果你想要进行精确的信息检索，有时候较长的上下文窗口反而会误导用户，让用户误以为可以随意输入大量信息并精确地找到所需内容。我认为目前情况并非如此。我认为精心设计的信息检索工作流，如RAG，仍然是解决问题的关键。</p><p></p><p>基本上，无论上下文Token数量达到百万级别，甚至更长，如果考虑到企业在实际使用场景中所处理的文档数量，这样的上下文长度可能仍然不足以带来实质性的改变。但在消费者使用场景中，更长的上下文窗口确实能够显著提升信息检索的效率。</p><p></p><p>Anthony Alford：所以说回报是递减的，对吗？</p><p></p><p>Namee Oberst：确实存在回报递减的效应。这在很大程度上取决于具体的应用场景。设想一下，如果有人需要浏览上万份文档，那么增加上下文窗口的大小实际上帮助有限。大量研究表明，大语言模型并不适合作为搜索引擎使用，它们在精确检索信息方面表现并不好。因此，我个人不太推荐依赖长上下文的LLM，而更倾向于使用RAG。话虽如此，我认为在某些情况下，长上下文窗口确实非常有用。例如，当你需要传一篇很长的论文给大模型，然后要求模型对其进行重写，但这篇论文的长度超出了传统上下文窗口的处理能力……我特别喜欢用LLM来转换文档，比如将一篇Medium长文章转换成白皮书，这在以前是超出了常规上下文窗口的处理能力的。我认为这是一个非常好的应用场景。</p><p></p><p>Anthony Alford：你提到了RAG，也就是检索增强型生成技术。我们不如就来深入讨论一下这个主题。它似乎首先能够解决上下文长度的问题。此外，这看起来是一个相当普遍的应用场景。或许你可以就此发表一些看法，特别是对于小型的开放模型。现在，人们可以在本地或者自己的硬件、云平台上运行这些模型，利用RAG来解决问题，这样他们就不需要依赖那些大型的封闭模型了。Namee，你对这个问题有什么见解吗？</p><p></p><p>Namee Oberst：我非常支持这一理念。如果你看一下Hugging Face上可用的模型类型以及它们的性能基准测试，我认为这非常令人印象深刻。此外，这些开源模型的创新速度和节奏也同样令人赞叹。尽管如此，当你看着GPT-4o的推理速度和能力，以及它能够为亿万用户提供数百万种服务，你仍然会感到万分惊奇。</p><p></p><p>然而，如果你正在面对一个企业级的应用场景，你拥有明确的工作流，并且希望解决一个非常具体的问题，例如自动化特定的工作流，以自动化生成报告为例，或者是在这些预定义的10000份文档中进行RAG来实现深入的信息检索。我相信，你可以利用开源模型来解决这些问题，或者选择一个现有的较小规模的语言模型，对其进行微调，投入资源，然后基本上可以在企业私有云环境中运行这些模型，并且还可以逐渐将它们部署到边缘设备上。因此，我非常看好使用较小的模型来执行针对性任务。</p><p></p><p>Srini Penchikala：确实，几个月前我尝试用Ollama来处理一个特定的用例，我非常看好像Ollama这样的开源解决方案。你可以自行托管服务，这样你就无需将所有数据上传到云端，也不必担心数据的去向。利用这些自行托管的模型，并结合RAG技术，可以构建专有的信息知识库。我认为这种方式在企业界正获得越来越多的关注。企业希望保留数据的控制权，同时又能充分利用这项强大技术。</p><p></p><p>Roland Meertens：目前大多数企业都是以OpenAI作为起点来验证自身的商业价值，在证明存在商业价值以后，他们才可以开始思考，“我们如何将这项技术真正融入我们的应用程序？”我认为这非常棒，因为你可以很容易地开始使用这项技术，随后再构建自己的基础设施来支持应用程序的后续发展。</p><p></p><p>Srini Penchikala：是为了扩大规模，对吧，Roland？你可以评估出哪种模型最适合你的需求，对吧？</p><p></p><p>Roland Meertens：是的。</p><p></p><p>Srini Penchikala：让我们继续回到大语言模型的讨论上来。另一个值得关注的领域是多模态模型，例如GPT-4o，也就是所谓的全能模型。我认为这确实将LLM推向了一个新的高度。它不再局限于文本，我们还可以利用音频、视频或其他各种格式。那么，大家对GPT-4o或者多模态模型有什么见解吗？</p><p></p><p>Namee Oberst：为了参与这期播客，我实际上做了一项实验。我订阅了GPT-4o的服务，今天早上我出于好奇输入了几个提示词。由于我们的主要工作是基于文本的，所以并不经常使用这个功能。我要求它为LLMware生成一个新的标志，但它失败了三次，每次都无法正确处理“LLMware”这个词。尽管如此，我知道它非常令人印象深刻，并且我认为他们正在迅速取得进展。但我想看看它们目前的水平如何，今天早上对我来说体验并不佳。当然，我也知道它们可能仍然比市场上其他任何产品都要好。我先声明这一点，以免有人来找我麻烦。</p><p></p><p>Roland Meertens：在图像生成领域，我不得不说，去年我对Midjourney的表现感到非常惊讶。他们的进步速度令人惊叹，尤其是考虑到它还是一家小型公司。一家小型企业能够凭借更优秀的模型超越大型竞争者，这一现象确实令人感到惊叹。</p><p></p><p>Mandy Gu：大型公司，如OpenAI，有出色的泛化能力，并且非常擅长吸引新人才进入这一领域。然而，随着你更深入地探索，你会意识到，正如我们在人工智能和机器学习领域常说的，天下没有免费的午餐。你探索、测试、学习，然后找到适合你的方法，但并不总是那些大玩家才能做到。对我们来说，我们从多模态模型中受益最多的不是图像生成，而是OCR能力。一个非常典型的应用场景是，我们上传图像或文件，然后与大语言模型对话，尤其是针对图像内容。这已经成为我们最大的价值主张，并且深受我们开发者的喜爱。因为在很多时候，当我们在帮助最终用户或内部团队进行故障排查时，他们会发给我们堆栈信息跟踪或问题截图。能够直接将这些截图输入给模型中，而不是去解读它们，极大地节省了我们的时间。</p><p></p><p>因此，我们的价值并不仅仅来自图像生成，而是更多地来自于OCR技术的应用，它为我们带来了巨大的价值。</p><p></p><p>Srini Penchikala：这很有道理。当你采用这些技术，无论是OpenAI还是其他公司，你就会发现，在将这些技术应用到公司的具体用例时，并没有通用的解决方案。因此，每个公司都有其独特的应用场景和需求。</p><p></p><p>Daniel Dominguez：我觉得很有意思的是，现在我们看到Hugging Face上有超过80万个模型，那么明年会有多少新模型问世，这绝对是一个很有意思的话题。目前流行的趋势包括Llama、Gemma、Mistral和Stability。一年之内，不仅在文本领域，图像和视频领域也将涌现出多少新模型，这无疑是一个值得关注的点。回看过去一年的模型数量是件有趣的事情，但更令人兴奋的是，预测明年这个领域将出现的新模型数量，可能会是一个更加令人瞩目的数字。</p><p></p><p></p><h1>RAG在大规模LLM中的应用</h1><p></p><p></p><p>Srini Penchikala：没错，Daniel，你提出了一个好观点。我认为这就像20年前的应用服务器市场一样，几乎每周都有新产品问世。我认为这些产品有许多将逐渐融合，只有少数几个能够脱颖而出，并持续较长时间。说到RAG，我认为这是企业真正能够获得价值的地方，输入信息——无论是在本地还是云端——并通过大语言模型进行分析，从而获得深刻洞见。你认为有哪些RAG的实际应用案例可能会引起我们听众的兴趣？</p><p></p><p>Mandy Gu：我认为RAG是大语言模型规模化应用中最具有潜力的方向之一，其应用形态可以根据检索系统的设计而灵活变化，可以适应多样化的用例需求。在我们公司，RAG已被广泛应用于内部流程。我们开发了一个工具，它将我们的自托管大语言模型与公司所有知识库相连接。我们的文档存储在Notion中，代码托管在GitHub上，同时，我们还整合了来自帮助中心网站以及其他平台的公开资料。</p><p></p><p>我们实质上是在这些知识库之上构建了一个检索增强型生成系统。我们的设计思路是：每晚运行后台作业，从我们的知识源中抽取信息，并将它们存入我们的向量数据库。我们为员工提供了一个Web应用程序，他们可以针对这些信息提出问题或给出指令。在内部进行基准测试时，我们也发现，这种方法在相关性和准确性方面，明显优于将所有上下文信息直接输入给像Gemini 1.5这样的模型。但回到问题的核心，作为提升员工生产力的手段，RAG已经为我们带来了许多真正优秀的应用案例。</p><p></p><p>Namee Oberst：Mandy，你所分享的案例堪称经典，而且执行得非常到位，完美契合了你们的需求。这正是大语言模型强大能力的最佳体现。你还提到了一些非常有趣的内容。你说你们自托管了LLM，我想知道，你们是否采用了某个开源的LLM，或者你是否愿意分享一些这方面的信息？当然，你无需透露太多细节。不管怎样，这无疑是通用人工智能应用的一个杰出范例。</p><p></p><p>Mandy Gu：实际上，我们使用的都是开源模型，很多都是从Hugging Face获取的。我们在构建LLM平台之初，就旨在为员工提供一种安全且易于访问的方式来探索这项前沿技术。和其他许多公司一样，我们最初选择了OpenAI的服务，但为了保护敏感数据，我们在它前面加了一个个人信息保护层。然而，我们从内部用户那里得到的反馈是，这个个人信息保护层实际上限制了生成式AI最高效的用例，因为在日常工作中，员工需要处理的不仅仅是个人信息，还有大量其他类型的敏感信息。这个反馈促使我们转变了思路：从防止员工与外部供应商共享敏感信息到如何确保员工可以安全地与LLM共享这些信息。因此我们从依赖OpenAI的服务转向了自托管大语言模型。</p><p></p><p>Namee Oberst：我简直被你所做的事情震撼到了。我认为这正是我们在LLMware所追求的。实际上，这正是我们希望借助在后端串联小型语言模型进行推理所能提供的那种解决方案。你多次提到了Ollama，但我们基本上已经将Llama.cpp集成到我们的平台中，这样你就可以基于量化模型轻松、安全地进行推理。我坚信，你为你们企业设计的工作流非常出色。但同时，我也预见到其他工作流自动化的用例将会被简化，以便在笔记本电脑上运行。我几乎可以预见在非常近的未来，所有东西都将被微型化，这些大语言模型将变得更小巧，几乎成为软件的一部分，我们所有人都将能够轻松、精确且安全地在笔记本电脑上部署它们，当然，还有私有云。</p><p></p><p>Mandy Gu：你提到了Llama.cpp，我觉得这非常有趣，因为可能并不是每个人都能意识到量化模型和小模型能带来如此多的边际优势。目前，我们仍处于快速实验阶段，速度是关键。采用量化模型可能会在精度上略有损失，但我们从降低延迟和提高行动速度方面获得了回报，这对我们来说是非常值得的。我认为Llama.cpp本身就是一个巨大的成功案例，这个由个人或小团队所创造的框架，能够得到如此大规模的执行。</p><p></p><p></p><h1>AI驱动的硬件</h1><p></p><p></p><p>Namee Oberst：Llama.cpp是Georgi Gerganov开发的，他在开源领域做出了令人惊叹的贡献。Llama.cpp为Mac Metal进行了优化，但在NVIDIA CUDA上也表现出色。我们正在做的工作是，让数据科学家和机器学习团队不仅能在Mac Metal上实现解决方案，还能跨越所有AI PC平台。我们利用了Intel OpenVINO和Microsoft ONNX技术，这样数据科学家们就可以在他们喜欢的Mac上工作，然后也能轻松无缝地在其他AI PC上部署他们的模型，因为MacOS只占操作系统份额的大约15%，剩下的85%实际上是非MacOS系统。想象一下，当我们能够跨多个操作系统部署，并充分利用所有这些AI PC的GPU能力时，未来的发展将会多么激动人心。我认为，这将是未来趋势中一个非常令人期待的方向。</p><p></p><p></p><h1>小模型和边缘计算</h1><p></p><p></p><p>Srini Penchikala：你们都提到了小语言模型和边缘计算，我们或许可以就此话题展开讨论。我知道关于大语言模型，我们可以讨论很长时间，但我更想听听你们对其他主题的看法。关于小模型，Namee，你在LLMWare对SLM做了一些研究，还特别提到了一个为SLM量身定制的RAG框架。你能否更深入地谈谈这个领域？微软也在研究他们所谓的Phi-3模型。能否分享一些这方面的信息？这些模型之间有何不同？我们的听众如何能够快速了解并跟上SLM的最新发展？</p><p></p><p>Namee Oberst：实际上，我们是小模型领域的探索先锋。我们专注于小模型的研究已经有一年多，可以说相当早就开始了。实际上，RAG在过去三四年已经在数据科学和机器学习领域得到了应用。我们在公司成立初期就对RAG进行实验，并对我们的小型参数模型进行了一些非常早期的调整，我们发现可以让这些模型执行非常强大的任务，并且从中获得了性能上的显著提升。同时，我们也确保了数据的安全性和保障。这些因素始终是我考虑的重点，因为我有法律专业的背景，我最初是在一家大型律师事务所担任公司律师，后来还担任了一家公共保险经纪公司的总法律顾问。</p><p></p><p>数据安全和隐私保护一直是我们最为关注的重点。对于那些受到严格监管的行业来说，选择使用小模型或其他较小规模的模型，是一个显而易见的决定。Mandy已经详细阐述了许多原因，但成本效益同样不容忽视。实际上，成本是一个巨大的考量因素。因此，当你能够显著减少模型的资源占用并大幅降低成本时，就没有理由去部署那些庞大的模型。更令人振奋的是，越来越多的人开始认识到这一点，与此同时，小模型性能取得了显著进步。微软推出的Phi-3模型，以及我们针对RAG进行微调的模型，还有Hugging Face专为RAG设计的模型，都显示出了卓越的性能。我们使用专有数据集对这些模型进行微调，以相同的方式和数据集微调了20个模型，确保了我们可以进行公平的比较。Phi-3模型在我们的测试中表现卓越，超越了我们测试过的其他模型，包括那些拥有80亿参数的模型，成为了表现最佳的模型。</p><p></p><p>我们的模型涵盖了从10亿参数到高达80亿参数的范围，并且在精确度方面达到了前所未有的高度，这真的让我感到非常惊讶。Hugging Face上那些向全世界免费开发的小模型，正在变得越来越好，而且进步速度非常快。我认为这是一个非常激动人心的世界。正如我之前所断言的，按照这样的创新速度，这些模型将会变得越来越小，小到它们所占用的资源跟软件相当。在不久的将来，我们将会在边缘设备上部署大量这样的模型。</p><p></p><p>Srini Penchikala：确实，许多应用场景涉及线下大模型处理和线上边缘设备实时分析的组合。这正是小型语言模型能够发挥其优势的地方。Roland、Daniel或者Anthony，你们对小型语言模型有何看法？在这个领域，你们观察到了哪些趋势或发展？</p><p></p><p>Anthony Alford：确实如此。微软的Phi系列模型无疑已经成为了焦点。此外，我们也有这个议题，Namee，你提到这些模型正在变得更好。问题是，我们怎么知道它们有多好？什么样的表现才算足够好？目前有许多基准测试，比如MMLU、HELM、Chatbot Arena等，还有很多排行榜和指标。我不想说人们在操纵这些指标，但这有点像是p-hacking，不是吗？你发了一篇论文，宣称在某个特定指标上超越了其他基线，但这并不总能直接转化为实际的商业价值。因此，我认为这仍然是一个需要解决的问题。</p><p></p><p>Namee Oberst：实际上，我们做了一套内部基准测试，专注于评估模型回答一些基于常识的商业和法律问题的能力，这些问题都是基于事实的。我们的平台主要是面向企业用户，因此在这个场景下，我们更关注模型对事实性问题、基本逻辑和数学问题的回答能力，而不是创造力。我们甚至创建了自己的基准测试方法，Phi-3模型的结果就是基于这些测试得出的。我对一些公布的结果持怀疑态度，你真的看过HellaSwag上的一些问题吗？有时候我甚至不知道正确或错误的答案是什么。因此，我们决定开发自己的测试标准，而我们讨论的Phi-3模型的表现正是基于这些我们自己制定的标准。顺便说一句，微软并没有赞助我们，尽管我希望他们能。</p><p></p><p>Srini Penchikala：我们很快会开始讨论大模型的评估，在这之前，你们对语言模型还有什么看法吗？</p><p></p><p>Roland Meertens：Phi让我印象深刻的一个点是，它在训练过程中不仅使用了高质量的数据，还通过自主生成数据来提升学习效果。例如，在编程方面，他们让Phi为学生编写指导手册，然后利用这些手册作为训练数据。这让我深刻体会到，如果你拥有更优质的数据，并且能够精心挑选这些数据，将能够训练出更为出色的模型。</p><p></p><p>Anthony Alford：你是说”Textbooks Are All You Need“吗？</p><p></p><p>Roland Meertens：除此之外，Hugging Face的团队成员也发表了多篇相关论文。目前，对于如何选择合适的数据来训练这些模型，人们表现出了极大的兴趣。在我看来，数据选择在机器学习领域仍然是一项被低估且值得深入探讨的课题。</p><p></p><p>Srini Penchikala：除了Phi，Daniel，你之前提到了TinyLlama。关于这些小模型，你有何见解或要评价的？</p><p></p><p>Daniel Dominguez：确实，正如Namee所言，目前在Hugging Face平台上的很多语言模型还有许多未知领域值得我们去探索。此外，Hugging Face的一个吸引人之处在于他们对不同性能级别的GPU进行了分类，你可能已经注意到了他们在排行榜上的目标设定。根据你的硬件配置，可能会被归类为”富GPU“用户或”穷GPU“用户，但不论哪种情况，你都能够运行这些语言模型。同时，我们也要感谢目前行业所提供的芯片技术，例如NVIDIA的芯片，它们不仅能够在云端运行这些小模型，也能够在低端个人计算机GPU和系统上运行。</p><p></p><p>得益于NVIDIA等公司提供的高性能GPU，这些小模型得以顺利运行。在Hugging Face平台上，当你看着这些模拟演示时，你会发现无需依赖庞大的计算资源即可在自己的设备上运行这些模型，这无疑是一个令人兴奋的发现。</p><p></p><p>Srini Penchikala：还有很多其他的AI创新正在发生，在结束语言模型讨论之前，我们快速再聊一下评估问题。除了基准测试指标，这些我们可能需要谨慎对待的东西，我想知道在现实世界中的最佳实践是怎样的？正如你提到的，Daniel，面对众多的模型，一个新入行者如何评估并比较这些模型，排除那些可能不适合他们的，并选择适合他们的？你有没有注意到在这个领域有哪些行业实践或标准？</p><p></p><p>Mandy Gu：我认为Anthony提到的商业价值是一个值得我们在评估过程中考虑的要点。尽管我对那些通用的基准测试持保留态度，但我认为我们真正需要做的是全面评估大型语言模型，不仅包括基础模型本身，还涉及到使用的技术以及我们如何针对特定任务来协调整个系统。例如，如果我的目标是总结一篇研究论文并提炼其语言，我就应该针对这一特定任务来评估LLM的能力。毕竟，没有一套模型或技术能够适用于所有任务。通过这个实验过程，我可以更有信心地找到最适合的模型组合。归根结底，如何更准确地量化评估结果，应该基于对当前任务的评估和我们期望看到的成果。</p><p></p><p></p><h1>AI智能体</h1><p></p><p></p><p>Srini Penchikala：接下来我们聊聊AI智能体。据我所知，这一领域已经取得了显著进展，特别是在AI驱动的编程助手方面。Roland，你对此有何见解？我知道你已经对Copilot等工具进行了深入研究。</p><p></p><p>Roland Meertens：去年你问我对未来一年的趋势有何看法，我预测是AI智能体。但现在看来，我说的可能并不完全准确。我们看到智能体技术确实有所发展。OpenAI之前推出了GPT Store，允许用户自行创建个性化的智能体。然而，坦白地说，我还没有听到有人向我强烈推荐某个智能体，说它非常出色。所以，从这个角度来看，我认为目前的进步还是有限的。不过，我们确实看到了一些有趣的应用，例如Devin，一个AI软件工程师智能体，它有一个终端、代码编辑器和浏览器，你可以给它分配任务，比如：“嘿，试着解决这个问题。”它会尝试独立完成所有工作。目前，Devin的成功率大约是20%，但考虑到它是免费的，这个成功率对于一个免费的”软件工程师“来说已经相当令人满意了。</p><p></p><p>此外，还有一些像AgentGPT这样的平台，我让它为AI趋势博客创建一个大纲，它提出了一些话题，比如：“我们可以讨论CNN和RNN等趋势。”我不认为这些还是趋势，但它对这些话题仍然充满热情，这是件好事。但总的来说，我认为智能体仍然有巨大的潜力。如果你想完成某项任务，完全可以进行自动化，而不是我自己去决定使用ChatGPT发送哪封电子邮件，然后发送它，接着等待对方回复并用ChatGPT总结，再写回复。</p><p></p><p>Anthony Alford：我的疑问在于，究竟是什么定义了“智能体”？</p><p></p><p>Roland Meertens：这是个好问题。所以我认为，就我目前所看到的，智能体是一种能够整合并执行多种任务的东西。</p><p></p><p>Anthony Alford：在念研究生时，我的研究领域是智能代理。我们所谈论的智能体主要是关于自主性。所以我认为，AI安全领域的专家们所担忧的，可能就是赋予这些系统自主性。不管你对AI的未来发展持何种看法，关注自主性问题都是非常合理的。目前来看，ChatGPT可能还没有达到实现完全自主性的水平。</p><p></p><p>Roland Meertens：这取决于你想做什么，以及你愿意在多大程度上让渡自己的控制权。就我个人而言，我还不太愿意在工作中部署一个完全自主的“Roland智能体”。我觉得它可能不会表现得特别智能。但我看到有人在约会应用上这么做了，显然，他们愿意冒这个险。</p><p></p><p>Daniel Dominguez：正如Roland所说的，智能体还没有真正掀起大浪，但可以肯定的是，它们在未来一定会发生些什么。比如，扎克伯格最近提到，他们正在为小型企业开发新的Meta AI智能体，这些智能体将帮助小企业主在自己的业务领域实现自动化。Hugging Face也有许多AI智能体，用于日常的工作流。Slack也集成了许多AI智能体，用于帮助用户总结对话内容、任务以及日常的工作流等。</p><p></p><p>我认为，随着我们在这一领域不断进步，AI智能体在日常工作和小型企业中的应用将变得更加自然。因为它们将极大地帮助我们完成许多日常任务，越来越多的公司也将开始在自己的平台上推出各式各样的智能体服务。例如，据我所知，谷歌即将推出用于Gmail等任务的AI智能体服务。因此，这可能是在接下里的一年加速发展的一个趋势。</p><p></p><p>Roland Meertens：确实，特别是你可以借助Langchain，让事情变得相当容易：”我有这些API可以调用，我想要实现这样的工作流程。如果你能够实现，就执行相应的操作。如果无法实现，就使用另一个API。“将工具箱中的所有工具进行组合并实现自动化，这种能力是非常强大的。</p><p></p><p>Mandy Gu：你说到点上了。以Gmail为例，有一个嵌入式助手可以帮你管理电子邮件，你就不需要去ChatGPT那里问如何增强邮件，或者做你想做的任何其他事情。从行为学角度来看，让信息在不同平台之间流转是一个巨大的工作负担，如果我们能够减少用户完成他们的工作所需要打开的标签页或需要访问的系统，这将是一个巨大的进步。而真正推动智能体采用的，就是这些因素。</p><p></p><p>Srini Penchikala：如果这些智能体能帮助我们决定何时发送电子邮件，何时不发送而是改为打电话，那就很厉害了。我的意思是，那样可能会更有效率，对吧？</p><p></p><p>Roland Meertens：我在思考趋势的问题。在去年，每一家公司都宣称：“我们现在是一家AI公司。我们将拥有自己的聊天机器人。”我甚至看到一些同事说：“我想证明这个论点，我让ChatGPT为我生成了三页的论点，看起来不错。”但我现在不想关心你的论点是什么，我不想和聊天机器人聊天，我只想浏览网站。所以我也好奇，最终会出现什么样的结果？每一家公司、每一个网站都会变成一个聊天机器人吗？或者我们是否也可以直接查找一本书的价格，而不是必须要求智能体为我们订购它？</p><p></p><p>Srini Penchikala：我们不应该过度智能体化我们的应用程序，对吧？</p><p></p><p>Roland Meertens：我的建议是，不要让你的生活变得过度智能体化。</p><p></p><p></p><h1>Ai安全</h1><p></p><p></p><p>Srini Penchikala：Anthony，你之前提到了人工智能的安全性问题，接下来就让我们深入探讨一下安全性。Namee和Mandy，你们都在多个实际项目中有所涉猎。你们如何看待安全与创新之间的关系？我们怎样才能确保这些开创性的技术在保持隐私和消费者数据安全的同时给我们带来价值？</p><p></p><p>Mandy Gu：生成式人工智能确实在安全领域引发了一系列连锁反应，例如第四方数据共享和数据隐私问题，这些问题日益严重。我们与许多SaaS供应商合作，这些供应商也是许多公司的选择。他们通常会集成人工智能技术，但并不总是会明确告知，实际上很多时候，他们会将用户数据发给OpenAI。根据数据的敏感程度，这可能是用户希望避免的。因此，我认为我们需要关注两点。首先，我们需要全面了解和追踪我们的数据流向。随着人工智能集成的普及，这项工作变得更加复杂，我们必须牢记这一点。其次，如果我们希望员工遵循正确的数据隐私安全实践，就必须让他们选择最简单、最安全的路径。</p><p></p><p>回到我之前提到的例子，如果我们在与OpenAI和其他供应商的所有对话中都叠加一个极其严格的个人身份信息（PII）审查机制，这可能会让使用者感到挫败，他们可能会直接去使用ChatGPT。但如果我们能够为他们提供替代方案，并通过激励措施使这些替代方案更加易于使用，或者增加他们需要的其他功能，同时确保安全选项是最容易实施的路径，这样就能吸引他们，并逐步建立起一种积极、注重数据隐私的良好文化。</p><p></p><p>Namee Oberst：是的，Mandy，你描述的工作流实际上凸显了我在讨论数据安全时经常强调的一个观点：在企业当中，生成式人工智能工作流的设计对所有的敏感数据安全性都有重大影响。是否有供应商可能会无意中将我们的敏感数据发送给一个我们不信任的供应商，例如OpenAI，这只是一个例子。我们需要审视这些问题，需要审视数据的来源，需要确保工作流具备可审计性，这样就可以追溯所有推理之间发生的交互。人工智能的可解释性如何发挥作用？我设计的工作流是否存在潜在的攻击面？如何处理提示词注入问题？</p><p></p><p>顺便提一个有趣的事实，由于经常处理小规模任务，小模型能够很好地泛化，因此不太容易受提示词注入的影响。但我们仍然需要关注提示词注入、数据投毒等问题。所以我认为，企业在部署人工智能时需要考虑诸多因素。Mandy，你刚才提出的观点非常中肯。</p><p></p><p>Mandy Gu：你提到的攻击面问题，我非常认同，因为这确实是一个可能迅速失控的方面。有人将生成式人工智能及其集成比作有线电视与流媒体服务，因为众多公司都在推出自己的人工智能集成服务，购买所有这些服务就像同时订阅Netflix、Hulu以及其他所有流媒体服务，不仅成本不划算，而且确实增加了潜在的攻击面。我认为，这正是我们在权衡自行构建与购买时需要考虑的，并且对我们所支付的费用以及数据的去向要有清晰的认识和审慎的决策。</p><p></p><p>我注意到人们对于这些问题的普遍认识正在逐步提高。供应商，尤其是SaaS提供商，正在积极回应这些关切。越来越多的服务提供商开始提供这样的选项：“我们可以将服务托管在你的虚拟私有云（VPC）中。无论是在AWS还是GCP上，都可以运行Gemini，确保你的数据仍然保留在你的云租户内。”我认为这正是在安全意识方面所展现的一个积极趋势。</p><p></p><p></p><h1>LangOps或LLMOps</h1><p></p><p></p><p>Srini Penchikala：除了安全性之外，我们需要关注的另一个重要问题是如何在生产环境中管理这些大语言模型和人工智能技术？所有，让我们迅速进入LangOps或LLMOps这个话题。这一领域有几种不同的术语并存。Mandy，或许你可以先分享一下你的观点。你如何看待当前LLM在生产环境中的支持情况，以及有哪些宝贵的经验？</p><p></p><p>Mandy Gu：在WealthSimple，我们把LLM的工作分为三个明显不同的领域。首先是提升员工的工作效率，其次是优化客户业务流程，第三是基础的LLMOps，我们更愿意称之为LLM平台工作，它为前两个领域提供支持。我们在这方面积累了许多经验，对我们来说行之有效的是我们的赋能理念。我们的工作以安全性、可访问性和选择性为中心。我们的目标是为用户提供可选择性，让每个人都能为手头的任务选择最合适的技术和基础模型，帮助我们避免了这个领域常见的一个问题，即人们将LLM视为寻找问题的解决方案（拿着锤子找钉子）。通过提供这些可复用的平台组件，生成式AI的采纳变得更加普遍。</p><p></p><p>这是一个我们逐渐才领悟到的教训。在我们刚开始踏上LLM之旅时，我们构建了一个LLM网关，它有审计跟踪功能，让人们能够安全地使用OpenAI和其他供应商的服务。我们收到的反馈是，审计跟踪功能在很多实际应用场景中对他们造成了限制。因此，我们开始自托管模型，这样我们就可以轻松地加入开源模型，进行微调，然后将其集成到我们的平台中，并通过LLM网关为我们的系统和最终用户提供推理服务。然后我们开始构建检索功能作为可复用的API，并围绕向量数据库构建框架，增强可访问性。随着我们逐渐将这些组件平台化，我们的最终用户——包括科学家、开发者以及业务人员——开始尝试并发现：“这个工作流实际上可以通过LLM得到显著改进。”这时，我们就会介入，帮助他们将这些想法产品化，并实现大规模的产品部署。</p><p></p><p></p><h1>AI发展趋势预测</h1><p></p><p></p><p>Srini Penchikala：我们即将结束这次讨论，这是一次非常精彩的讨论。在结束之前，我想向在座的各位提出一个问题：你们对人工智能领域在未来12个月内可能发生的事情有怎样的预测？当我们明年再次聚在一起讨论时，可以回顾并讨论这些预测的实现情况。</p><p></p><p>Mandy Gu：我认为，围绕大模型的许多炒作将会逐渐平息。我们在过去一年半的时间里目睹了它们惊人的增长。对于许多企业和行业来说，LLM仍然是一个他们愿意持续投入的赌注。</p><p>然而，我认为在未来的12个月里，这种情况将会有所改变，我们将开始对这项技术设定更为现实的预期，并在期望获得具体成果之前，更加审慎地评估我们的探索深度。因此，我预测从现在开始的12个月内，LLM炒作将会减少，那些继续采用这项技术的公司将会找到切实可行的方法，将其无缝集成到他们的工作流或产品中。</p><p></p><p>Daniel Dominguez：我预测，随着人工智能不断产生海量数据，它将与区块链等技术有某种形式的融合。我已经注意到许多区块链项目已经开始探索与人工智能的数据整合。虽然区块链和人工智能的融合目前还处于早期阶段，但在未来将会取得显著进展，尤其是在数据管理方面。因此，我认为人工智能与区块链的结合将是未来技术发展的一个重要趋势。</p><p></p><p>Roland Meertens：我仍然对机器人技术抱有期待，不过现在我们更倾向于称之为具身人工智能。这是去年逐渐流行起来的一个新术语。我不确定什么时候会发生，智能体已经能为我们执行计算机任务，如果我们把它们放到机器人的身体里，它们还会帮我们干活。具身人工智能无疑将成为下一个重要的大事。</p><p></p><p>Srini Penchikala：看来这些机器人将成为你的付费程序员，对吗？</p><p></p><p>Roland Meertens：不是这样。智能体将成为你的编程伙伴，而机器人则会在日常生活中为你提供帮助。我好奇的是，现在的公司拥有大量的数据，他们是否会利用这些数据来微调自己的模型并将其商业化？或者继续使用RAG？设想一下，如果你是一个园艺师，多年来一直在拍摄花园的照片，并提供如何改善花园的建议。肯定有很多小型企业拥有这样的数据，他们将如何从这些数据中获取价值？我非常好奇这些小型企业将如何利用他们的数据，以及如何构建自己的智能体、聊天机器人或AI自动化解决方案。</p><p></p><p>Anthony Alford：人工智能寒冬，Mandy已经提到了，不是吗？她说“我们可能会看到炒作的热度逐渐降低”，这是“温和”版本的寒冬。而“强烈”版本的寒冬，或许你已经看到过这样的标题，我记得是《自然》杂志上的一篇论文，它指出：“如果你用生成式AI生成的内容来训练生成式AI，结果可能会变得更糟。”我认为人们已经开始思考互联网是否正在被这些生成式内容污染。让我们拭目以待。我真心希望我的担忧是多余的，我真心不希望这个预测会成为现实。</p><p></p><p>Srini Penchikala：这是非常可能的，对吧？Namee，你对接下来的12个月有怎样的预测？</p><p></p><p>Namee Oberst：我预测我们将会经历一些Anthony和Mandy所描述的情况，但很快会过渡到更有价值、更加现实和具体的应用场景上，包括更自动化的工作流、智能体工作流，以及进一步扩展到边缘设备，比如笔记本电脑和智能手机。这就是我的预测，这将会很有趣。</p><p></p><p>Srini Penchikala：是的，这将会很有趣，这也是我所预测的。我相信我们将看到更多融合、端到端、全面的人工智能解决方案，它们结合了小模型、RAG技术和人工智能硬件。我认为许多积极的变化正在发生。我希望所谓的人工智能寒冬不会持续太久。</p><p></p><p></p><h1>相关资源</h1><p></p><p>论文“<a href="https://arxiv.org/abs/2306.11644?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjQ3MjU1NzUsImZpbGVHVUlEIjoiNXhrR285WGFaOWl3YmRrWCIsImlhdCI6MTcyNDcyNTI3NSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTA2fQ.Yz94KIsoXLBD2SJdXI3XorrO16q22wtoNxNOOxp8CHA">Textbooks Are All You Need</a>"”<a href="https://arxiv.org/abs/2301.03988?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjQ3MjU1NzUsImZpbGVHVUlEIjoiNXhrR285WGFaOWl3YmRrWCIsImlhdCI6MTcyNDcyNTI3NSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTA2fQ.Yz94KIsoXLBD2SJdXI3XorrO16q22wtoNxNOOxp8CHA">SantaCoder: don't reach for the stars!</a>"</p><p></p><h1>嘉宾简介</h1><p></p><p>Mandy Gu</p><p>Mandy Gu是Wealthsimple的高级软件开发经理，负责领导机器学习和数据工程团队。此前，她拥有丰富的自然语言处理（NLP）和数据科学方面的工作经验。</p><p></p><p>Namee Oberst</p><p>Namee Oberst是一家专注于生成式和开源人工智能解决方案的初创公司的创始人。</p><p></p><p>Srini Penchikala</p><p>Srini Penchikala是一位资深的软件架构师，并担任InfoQ人工智能、机器学习与数据工程板块的主编。著有《Apache Spark大数据处理》和《Spring Roo实战》（合著者）。</p><p></p><p>Roland Meertens</p><p>Roland是一位机器学习工程师，在自动驾驶汽车领域深耕计算机视觉技术。此前，他曾在社交媒体平台、深度学习自然语言处理、社交机器人以及无人机领域从事计算机视觉方面的工作。</p><p></p><p>Anthony Alford</p><p>Anthony是Genesys高级开发总监，在设计和构建大规模软件方面拥有超过20年的经验。</p><p></p><p>Daniel Dominguez</p><p>Daniel是华盛顿大学机器学习专业的工程师，拥有超过12年的软件产品开发经验。</p><p></p><p>【声明：本文由InfoQ翻译，未经许可禁止转载。】</p><p>查看英文原文：<a href="https://www.infoq.com/podcasts/ai-ml-data-engineering-trends-2024/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjQ3MjU1NzUsImZpbGVHVUlEIjoiNXhrR285WGFaOWl3YmRrWCIsImlhdCI6MTcyNDcyNTI3NSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTA2fQ.Yz94KIsoXLBD2SJdXI3XorrO16q22wtoNxNOOxp8CHA">https://www.infoq.com/podcasts/ai-ml-data-engineering-trends-2024/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/MoLIruT3QsJTzA5CLxm0</id>
            <title>融到2.2 亿美元才3个月就“闹崩”！5个创始人走了3个，这家 DeepMind 系创企一款AI产品都还没发！</title>
            <link>https://www.infoq.cn/article/MoLIruT3QsJTzA5CLxm0</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/MoLIruT3QsJTzA5CLxm0</guid>
            <pubDate></pubDate>
            <updated>Tue, 27 Aug 2024 01:50:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>整理 | 华卫</p><p></p><p>日前，法国人工智能 （AI） 初创公司 H 表示，由于“运营分歧”，其三名联合创始人 Daan Wierstra、Karl Tuyls 和 Julien Perolat 将离开公司。在 5 月 21 日宣布品牌重塑之前，H 被称为 Holistic AI。</p><p></p><p>H 在 LinkedIn 上的一篇帖子中表示，“公司将由首席执行官 Charles A. Kantor 和首席技术官 Laurent Sifre 领导。虽然这对所有相关方来说都是一个艰难的决定，但大家都一致认为，这将使公司在未来取得最大的成功。H 将继续得到投资者和战略合作伙伴的全力支持。”</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ae/ae7a78a1f304efa0b824ad3475aebba8.png" /></p><p></p><p>据悉，这三位联合创始人离开之际， H 筹集了 2.2 亿美元的种子轮融资后仅三个月，还未发布过任何产品。而 H 是在今年早些时候成立，计划在今年年底前发布一系列模型和产品。</p><p></p><p>“当 H 成立时，团队着手通过新一代动作模型将 GenAI 的力量推向全球人民和企业。”该公司在帖子中表示。“今天，H 的近 40 名工程师和研究人员团队仍然致力于这一愿景，开发尖端的动作能力，以提高工人的生产力并推动 AI 研究和工程的前沿。”</p><p></p><p>五人创始团队悄然“分家”，</p><p></p><p>无一人对此回应</p><p></p><p>根据该公司的 LinkedIn 帖子，H 现在拥有一支由 40 名工程师和研究人员组成的团队。相比之下，另一家资金雄厚的人工智能公司 Mistral AI 在招聘方面要保守得多。初成立之时， Mistral 除三位联合创始人外只有 3 名成员，团队总人数不到 10 人。</p><p></p><p>创立之初， H 有五位联合创始人，其中一位联合创始人兼该公司现任首席执行官 Charles A. &nbsp;Kantor 是斯坦福大学的计算数学研究员，而其他四位联合创始人都是谷歌旗下的人工智能公司 DeepMind 的资深科学家出身。</p><p></p><p>Karl Tuyls 是多智能体系统社区的著名科学家，自 2017 年起领导 DeepMind 的博弈论和多智能体团队，发起并领导了 DeepNash（一款在 Stratego 上击败人类专家玩家的自主智能体）和 TacticAI（一款角球自动助理足球教练）等多个著名项目，这些项目均发表在《科学》和《自然》杂志上。</p><p></p><p>Laurent Sifre 曾是 DeepMind 的首席科学家，在 DeepMind 工作了十年，为 AlphaGo、AlphaFold 和 AlphaStar、Chinchilla、Gemini 和 Gemma 等 GenAI 和深度神经网络的关键研究项目做出了贡献 。</p><p></p><p>Daan Wierstra 是 DeepMind 的一名高级计算机科学家，在 DeepMind 被谷歌收购之前就加入了该公司。在 DeepMind，Daan 曾领导了一支 100 多人的团队多年，并在 Deepmind 确立最初的研究方向方面发挥了关键作用。</p><p></p><p>Julien 从事博弈论和多智能体研究，共同领导了 DeepMind 在 Stratego 游戏（DeepNash）方面的科学和技术开发，以及在平均场博弈和基于人群的学习等主题上的许多其他基础性工作。</p><p></p><p>现在，从谷歌 DeepMind 转投 H 的四位联合创始人中的三位都将离开该公司。自人工智能爆火以来，业内闹过“分家”的知名 AI 企业不在少数。最近令不少人都仍记忆犹新的一家便是 OpenAI 了，11 人创始团队分崩离析至仅剩两人，分裂过程中内部发生多起“政变”。从去年 11 月首席执行官山姆·奥特曼（Sam Altman）被罢免以来，OpenAI 已经陷入大半年的“人事斗争”。</p><p></p><p>但 H 公司的“分家”却不同于此，除官方在 LinkedIn 上发布的公告帖以外，此前并未有任何公开的讨论和发言，连三位处于风波中心、将离开的联合创始人也未曾有过任何相关回应。</p><p></p><p>Tuyls 最近提及 H 的社交内容更新停留在 5 月 26 日，从其当时的状态看，他本人还沉浸在 H 公司成立的喜悦中，之后发布的帖子也未透露出要离开 H 或产生公司业务分歧的迹象。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8b/8b00b4b830fc8e4500355044c08e40e8.png" /></p><p></p><p>唯一有所异常的是名为 @Daan Wierstra 的账号对外关闭了其社交内容页的展示，但尚无法确定是否是他本人。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/71/71e373711eb778a9d112fca99b5526b0.png" /></p><p></p><p>众多亿万富翁投资，</p><p></p><p>H 公司是做什么的？</p><p></p><p>由于其创始人的背景和筹集到的资金数额，H 一直是法国最热门的 AI 初创公司之一，与 Mistral 和 Poolside 等公司并驾齐驱。毕竟，很少能听到超过 1000 万美元的种子轮融资。</p><p></p><p>就在三个月前，H 筹集了法国有史以来最大的种子轮融资之一，从包括全球风险投资公司 Accel 在内的投资者那里获得了 2.2 亿美元。该公司没有透露其中有多少是股权投资，多少是债务投资，但根据之前的报道，这一轮投资由多达 1.2 亿美元的可转换票据组成。这些是债务融资的一种形式，在满足特定条件后可以转换为股权。</p><p></p><p>今年 6 月，海外机构 PYMNTS 报告称，H 是引起投资者注意并吸引投资的人工智能代理初创公司之一，其估值领先于其业务基本面。</p><p></p><p>据介绍，这家初创公司的投资者包括众多亿万富翁（或其家族办公室）、一些知名风险投资基金和战略支持者。其中，亿万富翁名单里有前谷歌老板 Eric Schmidt、Courrier international 所属的 Le Monde 集团的个人股东 Xavier Niel、硅谷领先的风险投资家之一 Yuri Milner、Bernard Arnault（通过他的 Aglaé Ventures 基金）和 Motier Ventures（老佛爷百货集团所有者的家族办公室）等知名人士。</p><p></p><p>在风险投资名单上，投资者包括 Accel、法国巴黎银行的大型风险基金、Creandum、Elaia Partners、Eurazeo、FirstMark Capital 和 Visionaries Club。此外，还有一些产业投资者，包括亚马逊和三星。</p><p></p><p>有趣的是，总部位于纽约的机器人自动化软件公司 UiPath 也是 H 公司的投资者，这家欧洲机器人独角兽公司将在商业化和合作伙伴关系方面为 H 提供帮助。</p><p></p><p>据了解，H 建立的基础模型被称为 “代理”（agentic），这是一种旨在将任务分解为多个步骤并执行这些子任务，而不仅仅是一次一次地响应提示的人工智能。这家初创公司表示，其模型将比竞争对手的模型更有能力进行推理、规划和协作，致力于为商业和消费者垂直领域提供服务。</p><p></p><p>Kantor 曾表示，H 公司正在努力实现完全的人工通用智能（full-AGI），即与人类能力相当或超过人类能力的 AI 水平，能够完成各种任务。但老实说，这只是一个营销承诺，因为没人知道 AGI 是否或何时会实现。现实是，H 还需要筹集大量资金来支付计算能力和数据集的费用。</p><p></p><p>参考链接：</p><p></p><p><a href="https://www.pymnts.com/personnel/2024/3-co-founders-leave-french-ai-startup-h-amid-operational-differences/">https://www.pymnts.com/personnel/2024/3-co-founders-leave-french-ai-startup-h-amid-operational-differences/</a>"</p><p></p><p><a href="https://techcrunch.com/2024/05/21/french-ai-startup-h-raises-220-million-seed-round/">https://techcrunch.com/2024/05/21/french-ai-startup-h-raises-220-million-seed-round/</a>"</p><p></p><p><a href="https://sifted.eu/articles/three-cofounders-leave-h-news">https://sifted.eu/articles/three-cofounders-leave-h-news</a>"</p><p></p><p><a href="https://www.accel.com/noteworthy/building-foundational-models-to-generate-actions-our-partnership-with-the-h-company">https://www.accel.com/noteworthy/building-foundational-models-to-generate-actions-our-partnership-with-the-h-company</a>"</p><p></p><p>内容推荐</p><p></p><p>2024年8月18-19日，AICon 全球人工智能开发与应用大会·上海站成功举办，汇聚超过60位大模型行业先锋，全方位剖析大模型训练与推理机制、多模态融合、智能体Agent前沿进展、检索增强（RAG）生成策略、端侧模型优化与应用等热点内容。经过嘉宾授权，「AI前线」为你独家整理了一份演讲PPT合集，不容错过。关注「AI前线」，回复关键词「PPT」免费获取。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/81/814c1f9a6b667134f3520e04d6d8dfc7.png" /></p><p></p><p>会议推荐</p><p></p><p>InfoQ 将于 10 月 18-19 日在上海举办 QCon 全球软件开发大会 ，覆盖前后端 / 算法工程师、技术管理者、创业者、投资人等泛开发者群体，内容涵盖当下热点（AI Agent、AI Infra、RAG 等）和传统经典（架构、稳定性、云原生等），侧重实操性和可借鉴性。现在大会已开始正式报名，可以享受 8 折优惠，单张门票立省 960 元（原价 4800 元），详情可联系票务经理 &nbsp;17310043226 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/79/791c6d47a29abdea4f3ba09bea3b176a.png" /></p><p></p><p>今日荐文</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247622338&amp;idx=1&amp;sn=4c17f35ca45df4107a8830186480c690&amp;chksm=fbeba70dcc9c2e1b4312a0642eea90bc67da383b4f3fd4e94b81bdd3eb9f29b9c76dae5b5de4&amp;scene=21#wechat_redirect">《黑神话：悟空》被指抄袭，原作者开撕；IBM中国被曝数千研发权限突然被关；曝360儿童手表智能回答毁三观，周鸿祎道歉 | AI周报</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247622313&amp;idx=1&amp;sn=1fbaa02128849c257d476d2f64fd5683&amp;chksm=fbeba766cc9c2e7067f35b00df38c29cdd8e813e7fbb10eb1c843f6e4274ba5f8f874e088ef0&amp;scene=21#wechat_redirect">《黑神话：悟空》开发者遭猎头疯抢，联创发声求放过：你们不缺人才，别搞我们</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247622096&amp;idx=1&amp;sn=a6d564c645e7644023d4142365fc19b5&amp;chksm=fbeba41fcc9c2d093abdaa2dbe6cac44301d4070a5f580572925ad46b28aa9bb0574062de232&amp;scene=21#wechat_redirect">《黑神话：悟空》的第二个受害者出现了，竟是AI搜索惹的祸！</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621971&amp;idx=1&amp;sn=5e58c5a72a2d7fae816471954959b349&amp;chksm=fbeba49ccc9c2d8a35501b45bea911c9b684944634522011a859022dad48c9fda4b3a4e3b8fc&amp;scene=21#wechat_redirect">《黑神话：悟空》太狠了！Steam 瘫痪、多家公司放假，英伟达老黄又要躺在新的印钞机上数钱了</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621831&amp;idx=1&amp;sn=5ff4ba1979a3e77a914e8b6c5d390db7&amp;chksm=fbeba508cc9c2c1e83c061d1dbd107dca94d93ccda4172996c67d7920e2846d39123b77ee22b&amp;scene=21#wechat_redirect">“印度马斯克”要发印度第一款 AI 芯片，号称超越英伟达！CEO 要“狼性”，但把数十亿美元打水漂</a>"</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c5/c548af7ad39632ca346cd454eef4d2a8.gif" /></p><p></p><p>******你也「在看」吗？******👇</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/TqUxrdlKdpfVDHPCs001</id>
            <title>李沐上海交大演讲：创业好酷，有“当海盗”的乐趣</title>
            <link>https://www.infoq.cn/article/TqUxrdlKdpfVDHPCs001</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/TqUxrdlKdpfVDHPCs001</guid>
            <pubDate></pubDate>
            <updated>Mon, 26 Aug 2024 09:12:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p></p><blockquote>8月23日，李沐回到了母校上海交大，做了一场关于 LLM 和个人生涯的分享。这篇文章是对李沐在上海交通大学的演讲内容的总结，涵盖了AI技术的现状、未来趋势以及个人成长的深刻洞察。。</blockquote><p></p><p></p><h2>关于语言模型</h2><p></p><p>&nbsp;</p><p>李沐首先研究了语言模型的三个核心要素：算力、数据和算法，认为其与机器学习模型类似，本质上都是把数据通过算力和算法压进中间的模型里，使得模型拥有一定的能力，在面对一个新的数据时，它能够在原数据里面找到相似的东西，然后做一定的修改，输出想要的东西。</p><p>&nbsp;</p><p>但他指出，这次的语言模型和上一次深度学习浪潮的模型有一个比较大的区别：上一次是“我炼一个什么丹就治一个什么病”，这次是“我希望这个东西炼出来会有灵魂在里面”，它能解决很多问题，“这其实是技术一代代往前进。”</p><p>&nbsp;</p><p>在他看来，目前语音模型的优点是延迟更低、信息更丰富，并能够通过语言模型对整个输出做很多控制；音乐模型的问题不在于技术，而是商业问题；图片生成是整个 AIGC 领域做得最早的，也是效果最好的；视频模型则比较早期，通用的 video 生成非常贵，训练成本很有可能低于数据处理的成本，所以市面上没有特别好的开源模型出来。而多模态技术的发展趋势在于整合不同类型的模态信息，尤其是文本信息，未来通过文本控制生成某个模块可能成为常态。</p><p>&nbsp;</p><p>“总结下来，我觉得语言模型已经达到了较高的水平，大约在 80 到 85 分之间。音频模型在可接受的水平，处于能用阶段，大约在 70-80 分之间。但在视频生成方面，尤其是生成具有特定功能的视频尚显不足，整体水平大约在 50 分左右。”李沐说道。</p><p>&nbsp;</p><p>在硬件方面，李沐特别强调了带宽的重要性，并预测下一代的带宽将翻倍至800Gigabits。他还提到了英伟达的GB200系统，这是一个尝试通过水冷工艺提高算力密度的创新设计。“一旦用到水冷之后，你的算力就可以更密，就可以放更多机器。”李沐表示。</p><p>&nbsp;</p><p>内存方面，他认为内存大小将是模型发展的主要瓶颈，因为当前的内存技术限制了模型的规模。他表示，“受限于内存大小和数据的尺寸，100B 到 500B 会是未来主流的一个大势。你可以做更大，但是它很多时候是用 MoE 做的，它的有效大小（每次激活的大小）可能也就是 500B 的样子。”</p><p>&nbsp;</p><p>另外，他预计算力将由于摩尔定律变得越来越便宜。“短期来看，算力翻倍，价格可能会有 1.4 倍的提升。但是长期来看，当竞争变得越来越激烈，摩尔定律会发挥作用，就是说算力翻倍，价格不一定变。所以长期来看算力会变得越来越便宜。”</p><p>&nbsp;</p><p></p><h2>三种 AI 应用</h2><p></p><p>&nbsp;</p><p>李沐将人工智能的应用分为三类：</p><p>&nbsp;</p><p>文科白领，这方面做的比较好的包括个人助理、Call centers、文本处理、游戏和舆论以及教育。一个文科白领可能一小时完成的事情，模型能够完成百分之八九十。工科白领，目前 AI 想取代程序员还早得很。模型现在做的事是直接在其训练数据中检索相关的代码片段，根据上下文，再把变量名改一改。但它不是真的在写代码，人类一个小时还是能够写出很多复杂的代码的，所以模型还是没有取代工科白领一个小时干的事情，更不用说更复杂的任务了。蓝领阶级，这是最难的，唯一做得好的是自动驾驶。放眼整个世界，蓝领是最主要的成员，因此技术对这个世界做出巨大的变革还需要很多年。未来 10 年、 20 年，大家还是有机会参与进来的。</p><p>&nbsp;</p><p>“对于文科白领的工作，AI 已经能完成简单任务，复杂任务需要继续努力。对于工科白领的工作，简单任务还需要努力，复杂任务存在困难。对于蓝领的工作，除了无人驾驶和特定场景（比如工厂，场景变化不大，也能采集大量数据），AI 连简单任务都做不了，完成复杂任务更难。”李沐总结道。</p><p>&nbsp;</p><p>此外，他也分享了一些创业后得到的技术细节，比如预训练已经成为工程问题，后训练才是技术问题；垂直模型也需要通用知识；评估很难，但很重要；数据决定模型上限；自建机房不会比租 GPU 便宜太多等。</p><p>&nbsp;</p><p></p><h2>创业与职业发展的感悟</h2><p></p><p>&nbsp;</p><p>李沐分享了他从上海交通大学毕业后的多样化经历，包括在大公司工作、读PhD和创业。他强调了在不同环境中工作的目标和动机的重要性，并讨论了每种职业道路的利弊。他建议，无论是选择哪种职业道路，都需要有一个强烈的动机，并能够直面挑战。</p><p>&nbsp;</p><p>他提到，做一个“打工人”的好处是，可以在一个相对简单的环境里学习各种从业知识，比如一个技术如何落地、产品怎么做出来、怎么设计、怎么运营、怎么管理；其次是干完被安排的任务后，晚上睡觉不用太担心其他，不会做噩梦；还有就是相对稳定的收入和空余时间。</p><p>&nbsp;</p><p>那么做“打工人”的坏处就是停留在打工人或者职业经理人的思维。“公司从最上层把整个复杂的世界抽象成简单的任务，待得越久，就越觉得自己是螺丝钉，当然螺丝钉的好处就是，只要找到一个螺母钉上去就行，不用管这个机器多么复杂，外面世界多么复杂，但你在一个简化的世界里干得越久，就会觉得很腻，学的也越少，这就导致你一直停留在一个打工人或者职业经理人的思维里，而不是站在一个更高更广的层次去思考。”</p><p>&nbsp;</p><p>而对于创业，他表示，“创业好酷。好处是有当海盗的乐趣。”他解释道，“天天看市面上有什么东西，天天跟人聊有什么机会，机会来了是不是要 all in 搏一把，海盗太多，你不 all in ，机会就没了，但 all in 了也可能会失败，所以生死就在一瞬间，相当刺激，这种乐趣，你在别处无法体验到，创业是唯一可以合法当海盗的方式。”</p><p>&nbsp;</p><p>创业还有一个好处，就是能直面复杂的社会，直接跟社会打交道，没有人帮你做抽象，没有人会帮你把事情想清楚，你得自己把这个社会理解清楚后，快速学习。另外，创业还是一个最好的历经苦难的方法。“创业之后，你会发现，做别的事情都相对简单。”</p><p>&nbsp;</p><p>李沐还提出了一个持续提升自我的方法，即从导师或上级的角度审视自己，定期进行自我总结和反思。他强调了直面自己的问题、设定目标和持续努力的重要性。</p><p>&nbsp;</p><p>&nbsp;</p><p>想要查看原演讲的读者可以查看视频链接：</p><p><a href="https://www.bilibili.com/video/BV175WQeZE7Z/?spm_id_from=333.337.search-card.all.click">https://www.bilibili.com/video/BV175WQeZE7Z/?spm_id_from=333.337.search-card.all.click</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/2cTFj3WX6eYIb8wf7RF6</id>
            <title>世界机器人大会风靡，具身智能如何落地？</title>
            <link>https://www.infoq.cn/article/2cTFj3WX6eYIb8wf7RF6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/2cTFj3WX6eYIb8wf7RF6</guid>
            <pubDate></pubDate>
            <updated>Mon, 26 Aug 2024 07:09:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>人工智能浪潮席卷各行各业，具身智能作为人工智能的一个重要发展分支迅速崛起。我国具身智能领域的发展已步入快车道， 在 2024 世界机器人大会上，来自海内外的机器人企业展示了数十款人形机器人产品，数量创历届之最。根据大会公布的信息，中国在机器人创新、应用拓展和行业治理等方面均走在国际前列。各地地方政府纷纷支持人工智能产业的发展，如北京市就公布了打造全国具身智能创新高地的三年行动方案，希望提升我国参与全球具身智能竞争的核心力量。</p><p></p><p>无论是政策支持还是产业动向，都传递出具身智能技术高速发展的信号。那么，具体到实践层面，具身智能在技术成熟度、商业应用前景和投资回报率等层面的表现如何？能否在实际场景中提供有价值的解决方案？带着这些问题，本期《极客有约》栏目邀请到了北电数智战略与市场负责人杨震，共同探讨具身智能领域的发展情况。</p><p></p><p></p><h2>具身智能：新范式带来新机会</h2><p></p><p>InfoQ：具身智能赛道仍存在较多不确定性，北电数智为何选择坚定投入这一赛道？</p><p></p><p>杨震：首先，具身智能是一个新范式。过去的二三十年可以分成三个阶段，第一个阶段是信息时代，解决感知问题，大数据等技术的发展让我们获得了更多的知识和信息；第二个阶段是生成式人工智能时代，机器学习、深度学习等技术提高了思考能力，提高模型决策、判断能力；第三个阶段是具身智能时代，智能有了硬件载体，最终碰触到了整个链条的最后一个环节——面向场景做执行。在信息时代，获取信息决策、执行是由人类完成的；有了模型以后，模型可以辅助做一些判断和决策；具身智能则是一个全新范式，它可以自主完成从感知到决策再到执行的任务，形成一个闭环。</p><p></p><p>第二点，具身智能将开启新的交互窗口。信息时代用 PC 做交互，智能手机则可以通过语言、触屏等形式交互。具身智能是第三个窗口，也是革命级的窗口。它可以跨越接触介质，没有交互门槛，你可以用语言、手势甚至眼神等完成交互。</p><p></p><p>第三点，它是一个全新的平台。信息技术、模型技术、机器人技术都不是今天才有的，当这些技术在平台上叠加起来，就会出现非常多的商业模式，从而大幅影响社会和每个人的生活。有人担心未来不会用 AI 会失业，但这一波 AI 浪潮是以自然语言为基础的，会说话就可以使用 AI 。无人驾驶就是具身智能带来新商业模式的典型代表，萝卜快跑开启了无人驾驶的商业模式，但它并不会取代网约车司机、出租车司机的工作，车还是司机的，只是解放了司机的生产力，让他们找到新的工作方式、工作模式。</p><p></p><p>基于以上三点，我们看到具身智能是新范式、新入口、新平台，社会上已经衍生了一些新模式，它是一个很确定的趋势。</p><p></p><p>工业和信息化部也提出到 2025 年，我国人形机器人创新体系要初步建立，在关键技术取得突破。今年以来，多地出台了支持人形机器人产业发展的政策，北京、浙江、广东、四川更是成立了人形机器人产业创新中心，推动行业发展。</p><p></p><p>科技行业讲究第一性原理，任何工业革命级的创新都会有先驱者。特斯拉已经在做端到端的机器人，并将机器人用到自己的工厂里，国内一些头部具身智能厂商也在逐步探索商业道路和闭环方式。确定性的行业趋势，国家政策的支持和行业的落地探索进展，都让我们相信具身智能的发展未来。</p><p></p><p>InfoQ：具身智能会不会像元宇宙一样热度过了就消沉了？</p><p></p><p>杨震：技术炒作现象有时是因为时候未到，或发展关键元素不齐备，导致不能充分落地和发挥作用。 机器人技术已发展多年，但过去使用场景并不广泛，当时的机器人是由规则控制的，协同技术不完善，没有学习能力，只能完成固定任务。如工业自动化通过设定量和阈值来控制，机械手只能做固定动作，任务复杂度越高，出问题概率就越大。生成式人工智能大模型的出现弥补了上述缺点，机器人能具备自纠错能力，如人形机器人在行走过程中踉跄后能自行站稳，这为机器人执行复杂任务奠定了技术基础。</p><p></p><p>世界机器人大会上，我们看到多家具身智能厂商展示了落地场景，智能搬运、智能质检、螺丝拧紧、零件安装、水果采摘等，奔驰、宝马等车企也开始在自家工厂里采用人形机器人，具身智能具备了商业闭环的可能。</p><p></p><p>未来随着专有场景出现，机器人可执行的动作、功能不断增加、完善，针对某一具体功能或能在多个场景复用的人形机器人的成本会快速下降。而当机器人的成本降低后，B 端企业、C 端用户会愿意接纳、尝试机器人。</p><p></p><p>此外，随着我国逐步进入老龄化社会，机器替人的需求将长期存在。以老人看护场景为例，年轻人需要工作，心有余而力不足，看护场景也不是一个人就能完成的，这就需要具身智能快速理解场景，实现落地。因此，我们不认为具身智能是一个短期炒作的领域。</p><p></p><h2>具身智能行业如何破局和成长？</h2><p></p><p></p><p>InfoQ：具身智能行业想要实现破局，需要哪些抓手？</p><p></p><p>杨震：无人驾驶出租车走上街头对具身智能赛道的发展是非常好的信号。无人驾驶需在完全开放的环境中运行，会受到不确定性因素的干扰。而具身智能将落地的工厂、家庭场景，环境都相对封闭和稳定，不确定因素有限。无人驾驶这么难的场景都已经落地了，具身智能的落地只会更容易一些。</p><p></p><p>多模态大模型等技术可以推动具身智能的落地，让模型去认知世界所有的变量和不变量，让具身智能学习专业技能并运用。但具身智能真正实现落地还缺了两个部分，一是让具身智能快速落地的先行场景，二是数据积累。 这两点既是具身智能产业发展的卡点，也是行业破局的关键。</p><p></p><p>InfoQ：具身智能产业上游核心技术组件的可靠性、稳定性、成本问题怎么解决？</p><p></p><p>杨震：感知单元、控制单元、决策单元等上游核心技术发展得很快，且国内外技术发展非常同步。只是在大规模量产前，人形机器人的零部件，像感知端的一些高端传感器等组件的成本还比较高，存在可靠性、稳定性问题。我们认为可以尝试沿途下蛋的方式，不断在小场景落地，用一些功能没那么完整甚至和人形差异较大的机器人，把场景和需求跑起来，不断打磨核心组件的可靠性、稳定性，将成本逐渐降下来。</p><p></p><p>InfoQ：具身智能的智能模型和本体硬件未来是否会一体化？</p><p></p><p>杨震：具体要看本体要承载的功能是什么。一个需具备泛化多功能能力的人形机器人，在处理复杂、需要频繁判断和决策的任务时，可能需要边缘云的介入。但如果只是相对简单的任务，不需要高频决策支持，如特定场景的炒菜机器人，小模型就可以做非常多的事情。</p><p></p><h2>北电数智在行业发展中扮演怎样的角色？</h2><p></p><p></p><p>InfoQ：从北电数智的角度出发，可以为整个生态圈的链接、繁荣做哪些事情？</p><p></p><p>杨震：人工智能是第四次工业革命的标志，它对整个科技链条及其运作模式产生重塑效果。当人工智能方兴未艾时，我们需要审视整个科技链条，找到卡点和难点，把整个链条串起来，让它能够真正形成闭环，让产业能够快速成长和繁荣起来，作为人工智能时代的基础设施建设者，我们正致力于成为人工智能的产业加速器。</p><p></p><p>具身智能赛道，有一类公司主要生产机器人，比如人形机器人本体、四足机器人或者灵巧手公司。另一类公司是模型公司，做底座大模型、自然语言大模型，赋予机器人感知、思考、决策的能力，可以想象成大脑；要操纵机器人精准地执行动作还需要小脑，很多机器人大模型公司在做小脑的事情。但即便机器人有了很好的判断能力、运动能力，想要真正进入千行百业，还需要一些专业技能，这就需要开发团队在具体应用场景中训练它的专业技能。</p><p></p><p>如果想把这几层有效地连接起来，需要开放的训练场，要有一些具体场景。人工智能时代数据是最重要的，我们也看到在具身智能模型的训练中，无论是模拟仿真训练，还是远程操作示教，机器人数据都非常稀缺，数据的采集成本也非常高。例如特斯拉招聘的数据收集员，带上 VR 眼镜做一些任务来采集数据，每小时工资就要 48 美元。</p><p></p><p>我们做的事情首先是搭台子，让大家能够组团。其次是提供场景，把数据采集成本降下来，让产业链条上的本体公司、小脑公司和开发者团体形成自己的闭环。 这是具身智能产业快速发展的关键。</p><p></p><p>北电数智坚持中立的理念，我们不生产芯片，而是非常中立地把各种算力集合在一起，让它们能够协同作战。我们会广泛适配已有的底座模型、开发框架，让终端使用者、开发者找到自己的操作平台，落实到具身智能上。同样道理，我们既不生产本体，不生产小脑，也不训练它的专业技能。我们提供的是一个平台，希望平台能够把整个具身智能产业链上下游串接起来，让大家能够迅速组团，找到自己的最佳组合、最佳落地场景。</p><p></p><p>InfoQ：北电数智与生态合作伙伴已有哪些落地实践，可否给我们介绍 1-2 个案例？</p><p></p><p>杨震：上个月的全球数字经济大会期间，我们和中日友好医院达成合作，一起在医疗大模型和特有病种上做深度研究。我们看到，过往适应症研究、靶点研究主要采用机器学习方式训练，数据训练做得不是很好，动辄需好几年才能突破。在最新的案例中，可能 21 天就会有一个适应症的突破。</p><p></p><p>场景是具身智能产业链上下游伙伴发展起来的关键，也是大众能尽快享受具身智能的关键。我们希望在平台上将场景充分聚集起来，降低算力成本，为具身智能企业的发展提供沃土。与此同时，我们也会做好对数据安全的技术保障。</p><p></p><p>&nbsp;InfoQ：展望未来，北电数智如何联合业内外合作伙伴一同推动具身智能的发展？</p><p></p><p>杨震：北电数智希望充分发挥 AI 基础设施建设者的优势，从算力、数采空间和边缘空间，到开发平台工具、训练场等，做好我们应做的工作，和整个产业链上下游的伙伴协同。我们会发挥优势，尽量链接到重要且能近期见效的场景，把场景放到训练场上，让行业里的优秀伙伴们能找到自己发挥的空间，让具身智能机器人能够快速落地实际的应用和案例，真正走到生产、生活中去。</p><p></p><p>&nbsp;InfoQ：怎样成为北电数智的合作伙伴？</p><p></p><p>杨震：8 月 27 日，我们即将举办具身智能创新论坛，并宣布一些计划，包括联合实验室，以及针对开发者或初创公司的培知培育计划。我们也在建设北京数字经济算力中心，预计年底落成，这也是北京五环内唯一亿级的智算中心，将设置了人工智能企业的路演空间、交流空间甚至联合实验室，让人工智能企业能够展示自己的科技成果和想法。</p><p></p><h2>活动预告</h2><p></p><p></p><p>如何解决具身智能大规模、高质量训练数据的痛难点? 具身模型与算法更关注哪些维度？具身智能的商业化路径与落地场景将会是怎样?</p><p></p><p>8 月 27 日下午14:00，「2024 具身智能创新论坛」将以“星火·点亮具身智能”为主题，邀请机器人本体公司、具身智能模型开发公司以及仿真训练场等领域代表，共同探讨具身智能破局的有效路径。如果你对这场活动感兴趣，欢迎扫描下方二维码，围观现场直播！</p><p></p><p><img src="https://static001.geekbang.org/infoq/2f/2f1eb47a0646e1ee1a895bdb5d5d0f39.webp" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/90/90c859f44c9fdb5ae6ad4b193d07bb20.jpeg" /></p><p>InfoQ，将在08月27日 14:00 直播</p><p>已预约</p><p>8月27日14:00，「2024具身智能创新论坛」以“星火·点亮具身智能”为主题，欢迎围观见证！</p><p>视频号</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/xYCtLCJOeKZ5KzRW3f8K</id>
            <title>码上报名 | 跨越安卓和 iOS：开启国产 OS 移动开发新时代</title>
            <link>https://www.infoq.cn/article/xYCtLCJOeKZ5KzRW3f8K</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/xYCtLCJOeKZ5KzRW3f8K</guid>
            <pubDate></pubDate>
            <updated>Mon, 26 Aug 2024 04:14:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在数字化浪潮的推动下，操作系统作为科技领域突破和创新的基石，其重要性日益凸显。自主创新的鸿蒙系统，以其卓越的性能和开放的生态，推动了国内软件和硬件产业的升级。作为鸿蒙生态的共建者，蚂蚁 mPaaS 携手各行各业伙伴，为国产操作系统在行业中的创新应用注入了全新动力。</p><p>&nbsp;</p><p>值此背景，9 月 6 日下午蚂蚁数字科技、鸿蒙联合主办《跨越安卓和 iOS：开启国产 OS 移动开发新时代》主题论坛，邀请行业专家、企业高管、开发者和生态伙伴齐聚一堂，围绕数字经济发展前景，共同探讨国产操作系统应用生态的新变化和新机遇，分享实践应用经验，携手开启国产操作系统移动开发新篇章。这里，你将看到：</p><p></p><p>如何将科技与爱融入烹饪，享受美好生活?</p><p>如何点燃激情，沉浸享受竞技体育赛事的魅力？</p><p>国产操作系统高速发展背后，历经哪些征程与挑战？</p><p>从理想到实践，中石油、友邦 App 如何解锁服务体验新境界？</p><p>金融领域鸿蒙原生应用有哪些多元化创新与融合实践？</p><p>蚂蚁 mPaaS 全景能力支撑,&nbsp;如何加速 App 开发走入快车道？</p><p>&nbsp;</p><p>……</p><p>更多精彩，尽在&nbsp;9 月6 日下午</p><p>上海黄浦世博园 C9 会场</p><p>诚挚邀您</p><p>&nbsp;跨越安卓和 iOS：开启国产 OS 移动开发新时代</p><p>&nbsp;</p><p></p><p>论坛安排</p><p>主题：“跨越安卓和 iOS：开启国产 OS 移动开发新时代”</p><p>主办：蚂蚁数字科技、鸿蒙</p><p>时间：2024/9/6&nbsp;13:30-17:00</p><p>地点：上海黄浦世博园 C9 会场</p><p>&nbsp;</p><p></p><p>议程详情</p><p>点击图片，长按扫描二维码</p><p>免费获取参会凭证</p><p><img src="https://static001.infoq.cn/resource/image/05/f9/05699a6e6396e635403da24b7fdff8f9.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/l3qfHs68zB3dAxYvQrqa</id>
            <title>《黑神话：悟空》被指抄袭，原作者开撕；IBM中国被曝数千研发权限突然被关；曝360儿童手表智能回答毁三观，周鸿祎道歉 | AI周报</title>
            <link>https://www.infoq.cn/article/l3qfHs68zB3dAxYvQrqa</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/l3qfHs68zB3dAxYvQrqa</guid>
            <pubDate></pubDate>
            <updated>Mon, 26 Aug 2024 01:03:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><h3>行业热点</h3><p></p><p></p><h4>《黑神话：悟空》被质疑多处抄袭，原作者开撕：原创有那么难吗</h4><p></p><p></p><p>8月23日消息，“《黑神话·悟空》疑似抄袭”相关话题引发热议。“塞上李云中”（内蒙古青年画家李允云，被称为绘画《西游记》第一人）发布微博，附上了《黑神话：悟空》中的“大圣残躯”篇的图片，与其在2012年出版的《西游记人物图谱》中的孙悟空姿势相近，称“好像是给我画的孙悟空换了身装备”。对于上述相似之处，网友观点不一，有网友质疑“姿势也能鉴定为抄袭？”，也有网友表达了不满，“姿势就一模一样。除非这家游戏能拿出图1绘制早于2012年的证据”，也有网友为维护博主先打了预防针，“李云中老师从始至终没指责过美术抄袭，说的是可能借鉴，希望黑神话粉丝不要应激”。</p><p></p><p>就在前一天，微博认证为三级工艺大师的博主“玄鏐108”（中式甲胄艺术家、北京市工艺美术大师李辉）发布微博，贴出《黑神话：悟空》杨戬的臂鞲与其过往设计的臂鞲作品的对比图，感叹“又被抄袭了，原创有那么难吗？”接着，有博主发布视频，用视频一一对《黑神话：悟空》游戏里杨戬的臂鞲与博主过往作品细节对比，佐证抄袭之处。</p><p></p><p>目前，《黑神话：悟空》持续吸金中。同时在线玩家数还在上涨，据SteamDB数据显示，自首发当日突破220万人，次日在线突破235万人后，《黑神话：悟空》Steam同时在线人数于第三日（8月22日）突破240万人。根据国游畅销榜统计，《黑神话：悟空》在Steam上售出超过300万份，加上wegame、epic和ps平台，目前总销量超过450万份，总销售额超过15亿元。</p><p></p><p>据媒体报道，尽管游科互动的整体员工月平均收入达到了24305元，但这一数字仍略低于同行业平均水平，即25578元/月，从游戏特效师到3D设计师，再到动画设计师等核心职位，其月收入范围横跨19333元至30996元不等。尤为亮眼的是，超过八成的设计类岗位薪资水平甚至超越了行业平均值。</p><p></p><p></p><h4>“中国人是世界上最聪明的人吗？”曝360儿童手表的智能回答毁三观，周鸿祎道歉</h4><p></p><p></p><p>8月22日，有网友发布视频，称其使用智能儿童手表提问“中国人是世界上最聪明的人吗？”时，得到的答案让她觉得“毁三观”。据该网友介绍，该手表是在2023年购买的，主要是为了防止女儿走丢，手表的品牌是360儿童手表。</p><p></p><p>该网友为了演示，又重新用手表问了一遍同样的问题。语音回答：“以下内容来自360搜索……”除了语音，答案还以文字的形式显示在手表上。“因为中国人小眼睛、小鼻子、小嘴、小眉毛、大脸，从外表上显得脑袋在所有人种里最大。”整个回答有数百字，其中还有“什么四大发明，你看见了吗？历史是可以捏造的。而现在的手机、电脑、高楼大厦、公路，等等所有高科技都是西方人发明的”等表述。</p><p></p><p><img src="https://static001.geekbang.org/infoq/88/88998bb805378f7c52ea576c448d007b.jpeg" /></p><p></p><p>8月22日晚，360集团创始人、董事长周鸿祎在微博发文就360儿童手表答疑时出现争议言论道歉。周鸿祎表示，经过快速检查，出现问题的这款手表是2022年5月份的一个旧版本，其中没有装入公司的大模型。它回答问题不是通过人工智能，而是通过抓取互联网公开网站上的信息来回答问题。目前公司已经快速完成了整改，删除了上述所有有害信息，并正在将软件升级到人工智能版本。周鸿祎在视频中亲测了价值观问答，他表示，将有奖征集用户反馈，不断改进产品，不负用户信任。</p><p></p><p></p><h4>微软必应错误显示黑神话悟空客服电话，导致个人信息泄露</h4><p></p><p></p><p>国产3A游戏大作《黑神话：悟空》上线引发全球关注，然而在这波热潮中，微软必应AI却因错误抓取信息成为不实信息的传播者，导致个人信息泄露。在必应搜索中输入“黑神话悟空客服”，错误地显示了机锋网员工的个人手机号，并非官方客服电话。此外，还有两个错误的电话号码被标记为客服，其中包括第一财经版权部的联系电话。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/96/96ff1420e96552e3d99d6feafc0af376.png" /></p><p>据机锋网透露，尽管相关新闻稿件已删除，但错误信息仍出现在必应搜索首页。微软必应作为全球第二大搜索引擎，覆盖36个国家和地区，用户超6亿。微软曾声称必应采用OpenAI最新技术，甚至接入了GPT-4，但此次事件暴露了其在信息抓取和处理上存在漏洞。目前，必应团队尚未对错误信息进行更正。</p><p></p><p>被泄露电话当事人回应，他在5小时里，接了差不多20个电话。目前没有有效解决办法，正在申诉等反馈。</p><p></p><p></p><h4>网易云音乐网页端报错，App无法使用，故障真相：技术降本增效，人手不足排查了半天</h4><p></p><p></p><p>8月19日，网易云音乐出现服务器故障，“网易云音乐崩了”词条登顶微博热搜。从网易内部相关技术人员处获悉，此次宕机事件或与今年二季度的机房搬迁有关。“网易在贵州建立了机房，旗下业务分阶段搬迁，2024年Q2网易云音乐刚刚完成了贵州机房的迁移。”</p><p></p><p>据前述知情人士表述，这次搬迁内部曾评估难度极大，稍有不慎就有重大事故发生的可能性。“前几天刚说这次完成的不错，结果就打脸了。”其认为，近几年互联网公司多出现大型技术事故，多与降本增效相关。而网易此次搬迁，内部也称实现了大幅的成本下降。“再加上裁员，连故障排查都要很长时间”。</p><p></p><p>此次宕机持续了约两个小时。目前部分网友表示网易云音乐网页端与App端都已可以正常打开。8月19日，网易云音乐就网易云音乐崩了再次道歉，并给出了补偿方案。其表示没有删库，没有跑路，故障已陆续修复，作为补偿，8月20日0-24时，云音乐搜“ 畅听音乐 ”，可领取7天会员权益到账户。</p><p></p><p>据了解，“网易云音乐崩了”的情况并非首次出现。今年3月14日，网易云音乐曾出现众多网友登录状态突然失效、无法正常使用的情况，“网易云音乐崩了”词条也登上微博热搜。针对这一问题，网易云音乐客服当时表示，故障原因是由于网络异常，与版本更新无关。</p><p></p><p></p><h4>传IBM中国研发岗位员工被收回访问权限</h4><p></p><p></p><p>8月24日下午消息，据媒体报道，IBM中国于本周五晚间关闭了IBM中国研发和测试岗位员工的访问权限。一位实验室技术员工称，关闭权限前公司员工正常上下班，没有任何预兆和“信号”，一些技术员工还处于加班状态。目前，这些员工已从通讯软件的产品群组被移除，无法通过VPN登陆公司内网，但仍可访问邮件。此外，公司已通知被收回权限的员工于周一进行谈话，参与线上会议。</p><p></p><p>据报道，此次被收回权限的员工属于IBMV，下设CDL（IBM中国开发中心）和CSL（IBM中国系统中心），覆盖北京、上海、大连等地，涉及人数约千人。目前官方暂未回应。</p><p></p><p></p><h4>亚马逊 CEO：AI 助手 Amazon Q 可节省约 4500 个开发人员一年工作量</h4><p></p><p></p><p>亚马逊 CEO 安迪・贾西昨天在其领英主页发帖称，将亚马逊的生成式 AI 开发助手“Amazon Q”集成到内部系统后，利用新的代码转换功能，Amazon Q 将应用程序升级到 Java 17 的平均时间从开发人员的 50 天左右缩短到了几个小时，估计节省了约 4500 个开发人员一年的工作量。</p><p></p><p></p><h4>特斯拉“三班倒”训练人形机器人，正大量招聘操作员收集动作数据</h4><p></p><p></p><p>8月20日消息，据外媒报道，特斯拉正在以25.25至48美元的预期时薪招募大量人员，通过穿戴动作捕捉设备、VR头显等，训练其人形机器人Optimus。</p><p></p><p>特斯拉官网招聘页面显示，该职位名为“数据收集操作员(Data Collection Operator)”，分为午班晚班两班制，具体工作时间则是“三班倒”，即上午8点至下午4点半/下午4点至凌晨12点半/凌晨12点至上午8点半。换言之，若该岗位招聘至饱和状态并启动，则Optimus机器人将24小时不间断地吸收训练数据。</p><p></p><p>为了完成上述任务，特斯拉官方在招聘页面详细描述了意向该岗位的应聘者需满足的条件，包括但不限于：必须能够每天行走7小时以上，同时负重30磅；能够长时间佩戴和操作动作捕捉服和VR头显（特斯拉在此条要求后作出了VR晕动症的风险警示）；能够合理安排工作时间：白班/夜班+1个周末+“必要时”加班。</p><p></p><p>此外值得一提的是，特斯拉还正为此项目招聘“数据收集主管(Data Collection Supervisor)”，特斯拉表示：“数据收集主管将领导我们的数据收集团队并成为数据收集工作流程的专家，从而推动特斯拉Optimus计划的改进。这一角色需要较高的灵活性和领导团队的能力。”</p><p></p><p></p><h4>本科每月2000元、硕士每月2200元？中铁大桥局回应网传“工资”</h4><p></p><p></p><p>8月21日，网络上出现了一则关于中铁大桥局集团第五工程公司员工工作分配通知的消息，引发了公众的讨论。据该消息显示，公司的分配方案中，本科学历的新员工需经历一年的见习期，而硕士学历的员工则只需三个月的试用期。在薪资方面，本科生见习期间月薪设定头2000元，硕士生试用期月薪为2200元。</p><p></p><p>针对这一消息，中铁大桥局集团第五工程公司的人力资源部门迅速做出回应。一位工作人员明确表示，网络上流传的通知截图并不真实，并告知公司已就此事向警方报案。需要注意的是，信息源自网络，对于此类敏感信息建议读者保持审慎态度，理性判断。</p><p></p><p></p><h4>运动相机厂商GoPro计划今年裁员约15%，影响139个岗位</h4><p></p><p></p><p>8月20日消息，运动相机制造商GoPro当地时间周一表示，作为减少运营费用的重组计划的一部分，今年将裁员约 15%。该公司预计重组计划将花费500万至700万美元（当前约3569.1万至4996.7万元人民币），其中100万美元的现金支出将在第三季度确认，并在2024年第四季度确认约400万至600万美元。</p><p></p><p>此次裁员约139个岗位，预计将于第三季度开始，并于2024年底完成。截至6月30日的第二季度末，该公司拥有925名全职员工，在宣布裁员后，该公司股价上涨1.5%。</p><p></p><p>本月早些时候，GoPro公布2024年第二季度营收为1.86亿美元（当前约13.28亿元人民币），同比下降22.7%；运营支出为1.03亿美元（当前约7.35亿元人民币），同比增长5%。</p><p></p><p></p><h4>雷军回应王腾被投诉在公司玩《黑神话：悟空》：幸好小米有游戏本，要不就没理由了</h4><p></p><p></p><p>8月22日消息，中午12:00，小米CEO雷军进行第二期“雷军的副驾”直播。在直播中，雷军也谈到了最近非常火的游戏《黑神话：悟空》，雷军表示：自己一直在出差，没有时间玩，听说很多人在奋战，也看到很多人跟自己投诉王腾。</p><p></p><p>“他（王腾）不是说测试游戏本吗？幸好我们有游戏本，要不就没这个理由了，我觉得还好，小米整体是很宽松的氛围。“雷军笑着说。雷军表示，其实整个社会对打游戏都有些误解。“我觉得游戏是人的天性，当然一定要有自制力，有很多人沉迷游戏就不好，如果你喜欢的时候玩，时间也能自己把控，我觉得也挺好。”</p><p></p><p>日前，Redmi品牌总经理王腾微博发文，称“早，到公司第一件事情就是”，从配图来看，王腾今天上班先开电脑进行《黑神话：悟空》预下载。对此，有网友表示：“懂了，给产品做性能测试。”，王腾还回复了两个表情。还有不少调皮的网友纷纷在评论区艾特雷军和卢伟冰，说王腾上班摸鱼打游戏。小米公关部总经理王化转发王腾微博表示，“我本来帮你想了各种理由，现在看来都没啥用了，建议你主动截图发到各位老板的群里自我检讨”。</p><p></p><p></p><h4>编造联想华为对立谣言，自媒体一审被判赔偿道歉，联想：该条新闻导致其双11销售额下降了19亿</h4><p></p><p></p><p>据报道，近日，就联想集团（原告）与微博用户“万能的大熊”（被告）网络侵权责任纠纷一案，北京互联网法院立案后，依法适用普通程序，由审判员独任公开开庭进行了审理，一审判决联想集团胜诉。判决被告“万能的大熊”向联想集团道歉，并支付经济损失及相关费用16万余元。</p><p></p><p>该案件起因为，2023年10月联想创新科技大会后，新浪微博账号“万能的大熊”在完全没有事实根据的情况下，发布微博捏造称“联想宣布同英伟达达成合作，并且联想总裁杨元庆在现场还特别强调，从来没有考虑过和华为合作。”该微博发出后，引发评论区中大量关于原告联想公司的负面评价、攻击联想公司的言论。</p><p></p><p>对此，被告@万能的大熊发文回应表示，该案件很无聊，当时只是随手转了一条新闻，且全网都在转发，至今为止都没有人辟谣。万能的大熊进一步透露，联想认为该条新闻导致其双11销售额下降了19亿。“除了无语没什么好说的。”万能的大熊称还要打二审。</p><p></p><p></p><h3>大模型一周大事</h3><p></p><p></p><p></p><h4>大模型发布</h4><p></p><p></p><p></p><h4>微软“小而美”系列三连发！视觉小钢炮PK GPT-4o，MoE新秀力压Llama 3.1</h4><p></p><p></p><p>8 月 21 日，微软公司发布 Phi-3.5 系列 AI 模型。本次发布的 Phi-3.5 系列包括 Phi-3.5-MoE、Phi-3.5-vision 和 Phi-3.5-mini 三款轻量级 AI 模型，基于合成数据和经过过滤的公开网站构建，上下文窗口为 128K，所有模型现在都可以在 Hugging Face 上以 MIT 许可的方式获取。</p><p></p><p>Phi-3.5-MoE 是 Phi 系列中首个利用混合专家（MoE）技术的模型。该模型在 16 x 3.8B MoE 模型使用 2 个专家仅激活了 66 亿个参数，并使用 512 个 H100 在 4.9T 标记上进行了训练。微软研究团队从零开始设计该模型，以进一步提高其性能。在标准人工智能基准测试中，Phi-3.5-MoE 的性能超过了 Llama-3.1 8B、Gemma-2-9B 和 Gemini-1.5-Flash，并接近目前的领先者 GPT-4o-mini。</p><p></p><p>Phi-3.5-vision 共有 42 亿个参数，使用 256 个 A100 GPU 在 500B 标记上进行训练，现在支持多帧图像理解和推理。Phi-3.5-vision 在 MMMU（从 40.2 提高到 43.0）、MMBench（从 80.5 提高到 81.9）和文档理解基准 TextVQA（从 70.9 提高到 72.0）上的性能均有提高。</p><p></p><p>Phi-3.5-mini 是一个 38 亿参数模型，超过了 Llama3.1 8B 和 Mistral 7B，甚至可媲美 Mistral NeMo 12B。该模型使用 512 个 H100 在 3.4T 标记上进行了训练。该模型仅有 3.8B 个有效参数，与拥有更多有效参数的 LLMs 相比，在多语言任务中具有很强的竞争力。此外，Phi-3.5-mini 现在支持 128K 上下文窗口，而其主要竞争对手 Gemma-2 系列仅支持 8K。</p><p></p><p></p><h4>英伟达发布全新AI模型，参数规模达80亿</h4><p></p><p></p><p>8月23日消息，英伟达（NVIDIA）宣布，其已成功研发并发布了一款全新的AI模型，该模型拥有高达80亿的参数规模，具备精度高、计算效率高等优点，可在GPU加速的数据中心、云和工作站上运行。</p><p></p><p>据介绍，这款新发布的AI模型是基于英伟达在深度学习、自然语言处理以及计算机视觉等多个领域的深厚积累与持续创新。通过庞大的参数规模，该模型能够更深入地理解和解析复杂数据，从而在各类应用场景中展现出更为卓越的性能。</p><p></p><p></p><h4>科大讯飞推出星火极速超拟人交互技术，对标 GPT-4o</h4><p></p><p></p><p>8月19日，科大讯飞宣布星火语音大模型更新，正式推出星火极速超拟人交互，打造国内首个全新中文交互模式，并将在8月底率先全民开放使用。这意味着国内首个对标GPT-4o语音功能的产品正式到来。</p><p></p><p>据了解，星火极速超拟人交互响应速度更快，对话更加自然流畅，在响应和打断速度、情绪感知情感共鸣、语音可控表达、人设扮演四个方面实现突破。采用最先进的深度学习技术，该系统不仅能听懂用户的言语，更能深入理解语境和意图，并能够根据上下文自动调整回复，提供更加个性化、智能化的服务。</p><p></p><p></p><h4>昆仑万维推出全球首款 AI 短剧平台 SkyReels，一人一剧时代来临</h4><p></p><p></p><p>8月19日，昆仑万维发布全球首个集成视频大模型与3D大模型的AI短剧平台SkyReels。SkyReels平台集剧本生成、角色定制、分镜、剧情、对白/BGM及影片合成于一体，让创作者一键成剧，轻松制作高质量AI视频。这是一个2分半时长的短剧作品。</p><p></p><p>SkyReels平台集成了昆仑万维自研剧本大模型SkyScript、自研分镜大模型StoryboardGen、自研3D生成大模型Sky3DGen、以及业界首个将AI 3D引擎与视频大模型深度融合的创新平台WorldEngine。</p><p></p><p>SkyReels能够通过AI一键生成完整剧本、分镜、人物对白与BGM，支持角色形象、音色与分镜的自定义调整，并能够自动将内容转换为1080P 60帧的高清视频，单次可生成视频长度达180秒，相比Sora单次可生成60秒视频、可灵单次可生成10秒视频，有显著突破。一键整合所有创作成果，极大提高视频的创作效率，降低创作成本，推动“一人一剧”时代加速来临。</p><p></p><p>同时，WorldEngine结合了引擎的精确可控能力(如光照模拟、物理模拟、3D空间、实时交互等) 以及AI视频大模型的幻想生成能力，提供了全新的线上混合视频创作模式，让视频创作从模糊生成迈向更加精确可控。</p><p></p><p></p><h4>企业应用</h4><p></p><p>8 月 21 日，Meta 推出全新网络爬虫程序 Meta-External Agent 和 Meta-External Fetcher，用于收集互联网数据以训练其 AI 模型，该程序可绕过 robots.txt 规则，从而无限制地获取数据。8 月 21 日，微软推出统一的 Teams 应用程序，支持所有账户类型。用户可以轻松选择工作、个人或教育账户，甚至以访客身份加入会议，无需登录。这款新的统一版 Teams 应用程序意味着使用 Windows 10、Windows 11 和 Mac 的用户可以通过个人电子邮件登录该应用程序，并免费与其他 Teams 用户进行连接和协作。8 月 21 日，OpenAI正在发布一项新功能，该功能将允许企业客户使用自己的公司数据来定制这家人工智能初创公司最强大的模型 GPT-4o。8月22日，腾讯会议升级多语言翻译能力。支持将声源语言翻译为中文、英语、日语、韩语、俄语、泰语、印尼语、越南语、马来语、菲律宾语、葡萄牙语、土耳其语、阿拉伯语、西班牙语、印地语、法语、德语等17种语言。功能升级后，腾讯会议企业版、商业版用户在会议中的字幕、实时转写以及会议后的录制页中均能使用。8 月 21 日，英伟达放出一段游戏 demo，备受期待的 AI NPC 引擎在多人机甲战斗游戏《解限机》Mecha BREAK 中首次亮相。在这款游戏中，你可以用语音对话的方式和 NPC 交流，了解关卡目标、优化装备配置，随后调整武器配色开始战斗。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/QXPhBdbw2DafgzQgUVBi</id>
            <title>紫光同芯重磅发布两款芯片，未来将重点布局人工智能</title>
            <link>https://www.infoq.cn/article/QXPhBdbw2DafgzQgUVBi</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/QXPhBdbw2DafgzQgUVBi</guid>
            <pubDate></pubDate>
            <updated>Fri, 23 Aug 2024 16:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>8 月 21 日，在 2024 紫光同芯合作伙伴大会上，紫光同芯两款新品重磅发布：全球首颗开放式架构安全芯片——E450R、国内首颗通过 ASIL D 产品认证的高端旗舰级 R52+ 内核车规 MCU——THA6412。</p><p></p><p>据介绍，E450R 包括开放式硬件架构和开放式软件架构。开放式硬件架构具备开放式指令集、更强的剪裁和扩展功能，开放式软件架构拥有高效指令集、支持 ISO/IEC 国际标准语言和结构化虚拟机。软硬结合帮助集成该芯片的设备大幅提升安全性和交易性能、精简应用代码量、加载更多应用。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f6/f6571ee786b0ade09fc9848e9ad5e9ca.png" /></p><p></p><p>紫光同芯安全芯片事业部副总经理路倩表示：“目前，E450R 已获得银联芯片安全认证、银联嵌入式软件安全认证、银联 IC 卡操作系统产品认证、国密二级、CCRC IT EAL4+ 认证，我们期待与各位合作伙伴共同推动开放式架构产品在安全芯片领域的普及和应用。”</p><p></p><p>基于开放式软件架构，E450R 支持国际标准语言进行应用开发，提供结构化虚拟机实现平台无关化；提供应用资源高效利用指令集，实现应用代码量缩小 30%，应用加载速度提升 120%。</p><p></p><p>此外，E450R 实现了全新的防攻击机制、全新的非对称密码算法引擎 PKE 和全新的非易失存储器 NVM 管理，并呈现出更强的性能表现：PKE 算法速度提升 50%，密钥位数扩展情况下保证性能不变；NVM 擦写速度提升 15%，同样的擦写时间可以存储更多的信息；硬件底层速度大幅提升，典型的应用交易提升 50%。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/30/30b2fee91b7a2b90357c0d543a46f0ae.png" /></p><p></p><p>THA6412 面向汽车电子先进电子电气架构，基于 ARM 高性能实时处理器内核 R52+ 打造，较之上一代，在算力、工艺、架构、功能安全、信息安全等方面全面提升；通过了 ISO26262 ASIL D 功能安全最高等级认证等多项权威认证及严苛测试。“THA6412 专为适应动力底盘域控场景需求，特别是多合一电驱控制器、发动机、底盘域控、区域控制等应用，可为用户带来全新的驾乘体验。”紫光同芯汽车电子事业部副总经理杨斌介绍到。</p><p></p><p>除汽车控制芯片外，紫光同芯还打造了汽车安全芯片、功率器件等芯产品、芯方案，产品已在发动机 &amp; 变速箱、新能源主驱 &amp;BMS、线控底盘、ZCU、ADAS 域控、数字钥匙、T-BOX、V2X、网关等汽车核心领域得到广泛应用。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c8/c8e63146a85c964db68c04f69a70580a.png" /></p><p></p><p>新紫光集团联席总裁陈杰表示，未来，集团将重点布局人工智能、汽车电子、6G 与低轨道卫星等领域。</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/4cEh6JIUJjou6V6e3lPP</id>
            <title>拖欠半年工资没发，员工拿饮水机抵钱！又一家明星智驾独角兽烧光10多亿后黯然离场</title>
            <link>https://www.infoq.cn/article/4cEh6JIUJjou6V6e3lPP</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/4cEh6JIUJjou6V6e3lPP</guid>
            <pubDate></pubDate>
            <updated>Fri, 23 Aug 2024 09:40:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>&nbsp;拖欠半年工资没发，员工拿饮水机抵钱！又一家明星智驾独角兽烧光10多亿后黯然离场</p><p></p><h1>智驾独角兽禾多科技疑似解散，员工拿饮水机抵钱</h1><p></p><p>&nbsp;</p><p>近日，有多名认证为禾多科技员工的网友在某社交平台爆料，自动驾驶明星独角兽禾多科技已经走上破产清算之路，三四百名员工大半年没发工资，很有可能这半年多的工作变成了义务劳动了。</p><p>&nbsp;</p><p>据该网友透露，由于禾多科技迟迟未发放工资，连办公室的饮水机都被员工搬走抵工资。这一消息得到了多位内部人士的确认，“禾多和广汽之间谈崩了，相当于最后一根稻草没有了。”有消息人士对汽车媒体飞灵汽车如是说。</p><p></p><p><img src="https://static001.geekbang.org/infoq/07/073c1e85568f19af10809666173ef43a.png" /></p><p></p><p>2021 年就拿下广汽定点的禾多科技，近年来陆续为广汽埃安、传祺等品牌与车型提供了智能驾驶方案。智驾投资行情不景气的 2023 年，广汽依然连投禾多两轮。</p><p>&nbsp;</p><p>早在今年3月份，就有禾多科技员工爆料称公司已经暂缓发放了部分工资，且公积金已经断缴3个月，工资从1月起就开始拖欠，只有社保没断。3月底，禾多召开了一次全体会议，高层向员工承诺，在职员工工资延至4月发放，但这部分工资也只发放了一部分，离职员工工资会在6月发放。但工资始终一拖再拖，在承诺的日期到临之际，禾多科技没能兑现承诺。</p><p>&nbsp;</p><p>禾多科技走到如今的地步，早已有迹可循。</p><p>&nbsp;</p><p>8月14日，据晚点Auto消息，智能驾驶方案商禾多科技与广汽集团的重组方案遭遇重大变数，目前禾多科技的资金状况已非常紧张。受此影响，禾多科技正解散数据、研发等大部分核心部门，暂停研发活动。这一情况将禾多科技的未来置于风雨飘摇中。</p><p>&nbsp;</p><p>据知情人士透露，禾多科技与广汽集团商议重组的开始时间可以追溯到今年7月，根据当时的方案，禾多科技将成立一家新企业，并由广汽集团进行资本注入，新公司的主要任务是为广汽旗下的品牌提供智能驾驶解决方案。而到了8月，在重组方案未得到所有股东支持后，禾多科技创始人兼CEO倪凯发内部信告知员工此事，并表示公司将无法支付7月工资和到期的欠薪及公积金。</p><p>&nbsp;</p><p>在事态走向进一步恶劣之前，公司法人倪凯也曾试图挽救公司，他从去年开始已经多次出质自己的股份以换取资金自救。</p><p>&nbsp;</p><p>然而到目前为止，广汽也只表示了“会在评估后将就重组事宜给出回应”，至于禾多科技还能撑多久，一切都是未知数。</p><p></p><h2>入不敷出积弊已久，禾多科技难自救</h2><p></p><p></p><p>禾多科技成立于2017年6月，致力于打造基于前沿人工智能技术和汽车工业技术的自动驾驶方案，具备从车辆线控、多传感器技术到上层自动驾驶核心算法模块的完整布局，是少数拥有全栈自动驾驶研发能力的公司之一。禾多科技的创始人倪凯，是一位在自动驾驶领域具有丰富经验和深厚技术背景的专家。</p><p>&nbsp;</p><p>倪凯本硕毕业于清华大学，后又前往美国佐治亚理工学院攻读计算机博士，专注于计算机视觉、机器人技术等领域的研究。他曾任职于百度深度学习研究院，担任高级科学家，其间创建了百 度的无人驾驶团队，负责无人车的研发和部分高精度地图的工作，也曾在微软的美国西雅图总 部工作，参与三维地图和HoloLens VR眼镜的项目研发。</p><p>&nbsp;</p><p>为了业务发展，禾多科技还请来了在汽车与自动驾驶行业从业20多年，前博世集团ADAS业务 单元中国区负责人蒋京芳加入管理团队，自蒋京芳加入后，禾多科技将不到10人的苏州团队，发展到在上海、苏州拥有近200员工的量产闭环团队。有知情人士称，正是因为蒋京芳，禾多科技才能够搭上广汽。</p><p>&nbsp;</p><p>有了明星创始人和知名高级管理人才加持，禾多科技曾在资本市场受到颇多青睐。</p><p>&nbsp;</p><p>据36氪创投平台数据显示，成立至今，禾多科技已经斩获至少7轮融资，仅是近三年间融资总额就已超过了10亿元人民币。</p><p>&nbsp;</p><p>2021年，禾多科技宣布完成C1轮融资，虽然具体融资金额未详细披露，但该公司在自动驾驶领域的将进一步发展和壮大。</p><p>&nbsp;</p><p>2022年，广汽集团通过广汽资本领投完成了禾多科技的C2轮融资，此次融资总额为1亿美元（约合人民币6.7亿元），主要用于高级别自动驾驶技术的创新开发和规模化量产等方面。</p><p>&nbsp;</p><p>2023年7月，禾多科技再次宣布完成新一轮融资，金额为人民币3亿元，由广东粤科金融集团和广汽资本共同领投。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8d/8d9c30c55f1e108add99b339381ffbe9.jpeg" /></p><p></p><p>即使融了那么多钱，但由于融资事件埋下隐患、量产项目难以盈利、高管团队决策不力等原因，禾多科技仍然走到了穷途末路。</p><p>&nbsp;</p><p>据中国执行信息公开网显示，禾多科技已被北京海淀、江苏苏州和广州花都三地法院列为被执行人，涉及金额近75万元。这些法律问题加剧了公司的财务困境，使其面临更加严峻的生存压力。</p><p></p><h2>智驾企业倒在黎明前已成常事</h2><p></p><p>事实上，倒在盈利和量产前的智驾公司不在少数。因为有一个不可否认的事实：不赚钱的公司最终会耗尽资金后黯然离场。</p><p>&nbsp;</p><p>自动驾驶领域最重要的竞争是可扩展和可持续的商业模式的竞争。谁能通过使用自动驾驶汽车提供服务来真正赚钱，谁才能最终活下去。因为只有赚钱了，才可以开始扩大规模，而无需一味地去寻求外部输血。这种扩张可以让你发展你的品牌，了解你的乘客以及如何为他们创造价值，并收集更多数据，从而让你更快地改进你的技术。拥有持续可行的业务是正反馈循环的开始，这样才能带来可持续的增长和盈利的可能。</p><p>&nbsp;</p><p>然而，盈利能力的最大杠杆之一就是自动驾驶技术本身：研发出一款绝对安全的自动驾驶技术的成本非常高！也因为这种，许多公司都专注于开发该技术，将其作为建立业务的先决条件。简而言之：先让技术发挥作用，然后让业务发挥作用。但这种模式是有风险的。如果技术可行但单位经济效益不理想，技术太贵了市场不买单该怎么办？但如果没有在市场上测试你的想法，又怎么知道你是否做出了正确的技术投资？这像是个死循环。</p><p>&nbsp;</p><p>所以很多自动驾驶企业困在这个死循环里走不出来，他们走的是一条耗光资金走向灭亡的死路。事实是，小公司比大公司耗尽资金的速度要快一点。许多小型自动驾驶公司都经历了这一过程，导致了合并和收购式合并。</p><p>&nbsp;</p><p>但即使是大公司最终也会耗尽资金。2022年，大众和福特决定共同投资ArgoAI，在成立7年烧光37亿美金后解散；刚获得OpenAI投资四个月后，GhostAutonomy宣布关闭全球业务并关闭公司。国内阿里的达摩院去年宣布放弃自动驾驶技术的研发，整个自动驾驶实验室并入菜鸟集团。</p><p>&nbsp;</p><p>Uber ATG 和 Zoox 经历了整合，无法筹集维持自身研发所需的资金。即使是 Waymo、Cruise等公司也已从单一投资者模式转向更加多元化的投资者基础，允许每个投资者限制其风险。例如：当 Waymo 引入 25 亿美元的外部投资时，Alphabet 拥有超过 1100 亿美元的现金。这些公司中的大多数都没有产生任何收入，这是一个巨大的商业风险。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.stcn.com/article/detail/1290593.html">https://www.stcn.com/article/detail/1290593.html</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/krLwpJXWWcR5tmK0Gacj</id>
            <title>《黑神话：悟空》的第二个受害者出现了，竟是AI搜索惹的祸！</title>
            <link>https://www.infoq.cn/article/krLwpJXWWcR5tmK0Gacj</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/krLwpJXWWcR5tmK0Gacj</guid>
            <pubDate></pubDate>
            <updated>Fri, 23 Aug 2024 09:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>整理 | 华卫、核子可乐</p><p></p><p>近日，国产 3A 游戏大作《黑神话：悟空》火爆全网，上线不久便引发全球关注。据国游畅销榜统计的数据，仅仅一日，该游戏在多个平台的总销量已超过 450 万份，总销售额更是超过 15 亿元。与此同时，也出现了一些被其游戏热度所牵连的“受害者”。</p><p></p><p>《黑神话：悟空》在 Steam 解锁当天，某知名游戏主播在直播玩该游戏时，遭遇晕 3D 的情况，并因此上了微博热搜榜首，被一众网友笑称为《黑神话：悟空》“全球首个受害者”。而在 8 月 21 日，又一位该游戏的“受害者”出现了，其相关遭遇竟与微软有关。</p><p></p><p>在微软必应搜索中输入“黑神话悟空客服”，错误地显示了某机锋网员工的个人手机号，并非官方客服电话。此外，还有两个错误的电话号码被标记为客服，其中包括第一财经版权部的联系电话及其邮箱。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/eb/ebd2ab2169149989bd5e4a5af2f5efb6.png" /></p><p></p><p>被泄露电话的当事人表示，他在 5 小时里，接了差不多小 20 个电话。据悉，这一事件发生的主要原因是微软必应 AI 助手错误抓取信息导致其个人信息泄露，之后尽管被抓取的相关文章已删除，受害人已提交申诉等反馈，但错误的“黑神话悟空客服”信息仍一度出现在必应搜索首页。目前，从搜索情况来看，必应团队已对错误信息进行更正。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f8/f835bf18875e9da5e542d2df1955b2f0.png" /></p><p></p><p>作为全球第二大搜索引擎，微软必应覆盖 36 个国家和地区，用户超 6 亿。2023 年 2 月 7 日，微软宣布将 ChatGPT 集成进新版必应 (New Bing)，集成后的新版必应采用 OpenAI 的 AI 模型 GPT 3.5 的升级版 GPT-4。此次事件，或表明暴露了 AI 搜索引擎在信息抓取和处理上存在一定不足。</p><p></p><p>必应悄然改版后，</p><p></p><p>AI 搜索结果将优先显示</p><p></p><p>上个月，微软宣布对必应做出重大更新，搜索引擎将迎来全面改造，开始将 AI 生成的答案优先显示。也就是说，当用户输入搜索查询时，结果页面中将弹出一条由 AI 生成的主答案，详细说明在获取结果时所使用的全部精选信息来源。当然，大家仍然会在必应搜索页面中看到传统搜索结果，只是它们将被显示在 AI 生成素材的旁边（右侧的较小窗格内）。</p><p></p><p>对于这一变革，微软在官方博文中做出解释：“这种新体验将必应搜索结果的固有基础，同大 / 小语言模型（LLM 与 SLM）的强大功能加以结合。它能够理解搜索查询、检索数百万个信息来源、动态匹配内容，并以新的 AI 生成布局显示搜索结果，从而更有效地满足用户的查询意图。”</p><p></p><p>微软也在关于必应生成式搜索的博文列举了部分示例，除了概述摘要功能之外，微软还将提供大语言模型及小语言模型的主要来源链接，用户看到的答案正是由它们创作而成。而在 AI 生成结果之后，则是常规的结果条目列表。</p><p></p><p>例如当查询“大象能活多久”时，回答发的摘要主体后面还列出了影响大象寿命因素的视频；如果用户搜索“什么是意式西部片？”，必应生成式搜索就会显示关于这一电影子类型的历史、起源以及经典作品信息，同时给出指向这些信息的链接与信源。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ca/caede368945bf1bd768489f87c35dca4.png" /></p><p></p><p>当时，微软介绍，这项调整仅向少数必应用户推出，但不久之后应该会逐步扩大开放。微软还在其博文中表示，他们将继续评估 AI 搜索对于网站和读者的影响。有业内人士担心，如果人工智能机器人抓取的内容以直接在聊天窗口或搜索页面中呈现，那么免费创建内容的网站最终将倒闭。</p><p></p><p>对此，微软表示，这种新的 AI 搜索体验是从头开始构建的，也考虑到了这个问题，因而保持了与传统搜索相同的网站点击次数，时间会证明这是否属实。此外，据了解，必应可以选择在结果页面中关闭 AI 生成功能、只显示传统搜索摘要。</p><p></p><p>AI 搜索闹出的笑话</p><p></p><p>现在，微软并不是唯一一家将 AI 生成的结果添加到搜索页面的浏览器公司。随着微软为必应推出更多工具，将更多 AI 功能引入搜索的竞争态势也在逐步升级。</p><p></p><p>然而，无数真实案例正在证明，AI 搜索并不像我们想象中的那般可靠和准确——它可能会出错，某些情况下生成的结果中甚至会显示错误的信息和建议。</p><p></p><p>今年早些时候，谷歌也曾推出过一款类似的工具，名为 AI Overview，旨在留住那些想要直接向 AI 聊天机器人寻求问题答案的用户。但该工具在推出后也闹出过一些笑话，比如建议添加胶水以使奶酪粘在披萨上、回答“地质学家建议每天至少吃一块小石头”等。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/33/330626ba9a9033d187009de25bf4a36b.png" /></p><p></p><p>Arc Search 浏览器在 AI 模式下，信誓旦旦地给出不恰当的医疗建议，“被切断的脚趾最终还会长回来”。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/66/66455fc292875d15f9532c95e2ca7d9d.png" /></p><p></p><p>人工智能搜索引擎 Genspark 向用户推荐一些可能用于害人性命的武器，Perplexity 则剽窃了一些媒体撰写的新闻文章，但并未注明来源或版权归属。</p><p></p><p>此外，AI 生成的摘要信息还可能蚕食其信息来源网站的流量。一项研究发现，由于不再强调文章链接，AI 摘要功能可能将内容发布方的流量拉低 25% 左右。</p><p></p><p>专家警告，AI“幻觉”</p><p></p><p>问题无法真正解决</p><p></p><p>这些新兴 AI 搜索引擎能够凭借其快速生成大量文本，并以令人信服的效果模仿人类文字的能力而广受欢迎，但在其背后，AI“幻觉”也成为影响这些聊天机器人更上一层楼的关键阻力。而遗憾的是，有专家警告称这种情况很可能永远无法解决。</p><p></p><p>美联社发表的一份最新报告强调，大语言模型（LLM）“胡说八道”的问题可能并不像许多技术创始人和 AI 支持者宣称的那样容易解决。华盛顿大学计算语言学实验室语言学教授 Emily Bender 对此表示悲观，“幻觉问题根本无法解决，这是由技术与拟议用例之间不匹配所必然引发的结果。”</p><p></p><p>根据 Jasper AI 公司总裁 Shane Orlick 的说法，某些情况下适当的“胡说八道”反而并不是坏事。Orlick 解释称，“幻觉实际能带来额外的好处，一直有客户在感谢我们带来的启发，而根源就是 AI 可能在种种机缘巧合之下输出客户自己从未想到过的故事或者角度。”</p><p></p><p>同样的，AI 幻觉对于 AI 图像生成也有着巨大的助益，Dall-E 和 Midjourney 等模型正是凭借这份想象力生成了引人注目的精彩图像。也就是说，只有在文本生成领域，幻觉才是个真正困扰用户的问题，特别是在新闻报道等高度强调准确性的场景之下。</p><p></p><p>Bender 指出，“大语言模型的基本原理就是‘编造’内容，这也是其一切功能的根本。但由于能力源自编造，所以当它们输出的文本恰好可以正确匹配我们的提示词时，这种情况反而是种偶然。哪怕经过微调的模型能够在大多数情况下都保持正确，它们也仍无法彻底摆脱故障。而且，未来的幻觉很可能以文本阅读者更难以注意到的模糊状态存在。”</p><p></p><p>结&nbsp; &nbsp; 语</p><p></p><p>大语言模型是种能够实现非凡功能的强大工具，但企业乃至整个科技行业必须意识到一点——不能单纯因为某种事物很强大，就认定它是一种好用的工具。就像冲击钻也很好用，能够轻松破开人行道和沥青路面，但没人敢把它带到考古挖掘现场。</p><p></p><p>正如 Bender 所指出，大语言模型在最初开始训练的那一瞬间，就是在尝试根据我们给出的提示词预测序列中的下一个单词。训练数据中的每个单词都被赋予了权重或者百分比，以便在给定的上下文中追踪之前既有的给定单词。可这些起先的单词本身并没有充分切实的含义或者重要的上下文来保证输出准确。</p><p></p><p>换言之，这些大语言模型只是出色的模仿者，它们实际并不清楚自己到底在说些什么，所以过度信任它们只会令用户陷入困境。这个弱点是大语言模型所固有的，尽管“幻觉”可能在未来的迭代中逐渐减少，但问题本身却可能永远无法被真正修复。</p><p></p><p>参考链接：</p><p></p><p><a href="https://www.engadget.com/microsoft-is-adding-ai-powered-summaries-to-">https://www.engadget.com/microsoft-is-adding-ai-powered-summaries-to-</a>" 必应 -search-results-203053790.html?src=rss</p><p></p><p><a href="https://www.techradar.com/computing/artificial-intelligence/">https://www.techradar.com/computing/artificial-intelligence/</a>" 必应 -has-been-revamped-to-prioritize-ai-search-results-whether-you-like-it-or-not</p><p></p><p><a href="https://www.techradar.com/computing/artificial-intelligence/chatgpt-and-other-ai-chatbots-will-never-stop-making-stuff-up-experts-warn">https://www.techradar.com/computing/artificial-intelligence/chatgpt-and-other-ai-chatbots-will-never-stop-making-stuff-up-experts-warn</a>"</p><p></p><p><a href="https://techcrunch.com/2024/07/24/bing-previews-its-answer-to-googles-ai-overviews/">https://techcrunch.com/2024/07/24/bing-previews-its-answer-to-googles-ai-overviews/</a>"</p><p></p><p>内容推荐</p><p></p><p>在这个智能时代，AI 技术如潮水般涌入千行百业，深度重塑生产与生活方式。大模型技术引领创新，精准提升行业效率，从教育个性化教学到零售精准营销，从通信稳定高效到金融智能风控，AI 无处不在。它不仅是技术革新的先锋，更是社会经济发展的强大驱动力。在 AI 的赋能下，我们正迈向一个更加智能、便捷、高效的新未来，体验前所未有的生活变革与行业飞跃。关注「AI 前线」公众号，回复「千行百业」获取免费案例资料。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c0/c0779541886d6212211f10391187b0f5.png" /></p><p></p><p>会议推荐</p><p></p><p>InfoQ 将于 10 月 18-19 日在上海举办 QCon 全球软件开发大会 ，覆盖前后端 / 算法工程师、技术管理者、创业者、投资人等泛开发者群体，内容涵盖当下热点（AI Agent、AI Infra、RAG 等）和传统经典（架构、稳定性、云原生等），侧重实操性和可借鉴性。现在大会已开始正式报名，可以享受 8 折优惠，单张门票立省 960 元（原价 4800 元），详情可联系票务经理 &nbsp;17310043226 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/79/791c6d47a29abdea4f3ba09bea3b176a.png" /></p><p></p><p>今日荐文</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621971&amp;idx=1&amp;sn=5e58c5a72a2d7fae816471954959b349&amp;chksm=fbeba49ccc9c2d8a35501b45bea911c9b684944634522011a859022dad48c9fda4b3a4e3b8fc&amp;scene=21#wechat_redirect">《黑神话：悟空》太狠了！Steam 瘫痪、多家公司放假，英伟达老黄又要躺在新的印钞机上数钱了</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621831&amp;idx=1&amp;sn=5ff4ba1979a3e77a914e8b6c5d390db7&amp;chksm=fbeba508cc9c2c1e83c061d1dbd107dca94d93ccda4172996c67d7920e2846d39123b77ee22b&amp;scene=21#wechat_redirect">“印度马斯克”要发印度第一款 AI 芯片，号称超越英伟达！CEO 要“狼性”，但把数十亿美元打水漂</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621777&amp;idx=1&amp;sn=c6805493b8fdc7fefe72e8fd99cbd323&amp;chksm=fbeba55ecc9c2c48d97ff000e9874945ff2424c25e3d61d3b6f9d1e016fff7c5d8b449556e4a&amp;scene=21#wechat_redirect">朱啸虎押注的AI公司被围攻：领导多次让员工“去死”；小红书激励不再与职级挂钩；谷歌前CEO：AI创业可先“偷”后处理｜AI周报</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621772&amp;idx=1&amp;sn=031cd69a65396e1e2bd2c9937008134c&amp;chksm=fbeba543cc9c2c554037e36e088163440a7dd3c994673e9ac9d66d4f58aa2fce516eb290d868&amp;scene=21#wechat_redirect">要求员工点赞拉踩贴、抢到对方客户给奖金！40 多位知情人曝这两家 AI 数据商业巨头“生死大战”，如今“开撕”微软</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621721&amp;idx=1&amp;sn=ff4df9b1712358edc181e34ea0a3c89c&amp;chksm=fbeba596cc9c2c8000958fc0def83fe925093fbe55c2f4ae8b0b979c6df37c49a0ceb75e59cc&amp;scene=21#wechat_redirect">成本直降90%、延迟缩短80%！Anthropic将API玩出了新花样，网友：应该成为行业标配</a>"</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c5/c548af7ad39632ca346cd454eef4d2a8.gif" /></p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247620641&amp;idx=1&amp;sn=44cdebfad0decb39633875fc4069c7fc&amp;chksm=fbeba1eecc9c28f81fc4c7c10d9e95329e4e0ed2d4c1f0eff6ca7f19376846f6a297b33e15d3&amp;scene=21#wechat_redirect"></a>"</p><p></p><p>******你也「在看」吗？******👇</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/s8bfk2wlAdJJODyViD0c</id>
            <title>跟着小扎不白干，9 个月“出师”：用学到的 10 条经验搞出 AI 界“带货王”，年入 1 亿美元</title>
            <link>https://www.infoq.cn/article/s8bfk2wlAdJJODyViD0c</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/s8bfk2wlAdJJODyViD0c</guid>
            <pubDate></pubDate>
            <updated>Fri, 23 Aug 2024 06:02:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>编译 | 核子可乐、华卫</p><p></p><p></p><blockquote>一位Facebook的早期员工Noah Kagen创业成功后，在他的个人网站上分享了他从马克·扎克伯格（Mark Zuckerberg）和 Facebook 那里学到的10条经验教训。Kagen是Facebook的第30号员工，在工作时长9个月后被扎克伯格解雇。离开Facebook后，他创立了软件产品推广和营销平台AppSumo ，并通过总结的扎克伯格工作“之道”将其打造为一家年收入 1 亿美元的公司。AppSumo是一家专注在软件产品的 LTD 平台，一方面为软件产品的开发者提供销售 LTD 的渠道，另外一方面为小公司和创业者提供了购买 LTD 的渠道。在生成式 AI 兴起后，AppSumo 也帮助大量带有 AI 功能的产品提高了销售收入、流量和订阅用户，包括AI聊天机器人平台Juphy、AI 内容生成工具Castmagic等。</blockquote><p></p><p></p><p></p><h1>在扎克伯格手底下干活，我的一点心得</h1><p></p><p></p><p>第一次走进位于帕洛阿尔托大学大道的Facebook总部大楼的时候，我竟一时分不清自己身在高校社团还是创业公司。天花板上吊着电缆，人们匆匆往来，而我则按要求在其他人的办公桌角上挤出个位置。</p><p></p><p>我的新上司从身边走过，说午饭之后再来找我谈话。之后又有人塞给我一台笔记本电脑，闲来无事我就先上会网。后面，有人告诉我得马上开始准备，30分钟后得在马克·扎克伯格的即兴会议上做汇报。</p><p></p><p>扎克伯格走进会议室，平静地告诉我：“你的上司刚刚被炒了，欢迎来到Facebook。只要你不背着我出场公司利益，那就能在这里好好待下去。”而好戏，这时候才刚刚开场……</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/17/17276988ffae1fcb17e877c916de7f25.png" /></p><p></p><p></p><p>在Facebook的工作经历，可以说是我这辈子最美好、但也最痛苦的一段回忆。我是公司第30号员工，而短短9个月之后就被解雇了。很长一段时间，我一想到自己被裁撤的命运就非常痛恨这家企业。</p><p></p><p>但我从扎克伯格和Facebook那边学到的经验，最终也帮助我将AppSumo打造成了一家年收入上亿美元的公司。下面聊聊我在扎克伯格手底下工作时，自己总结出来的10条经验：</p><p></p><p></p><h2>1.专注于单一目标</h2><p></p><p>我曾经恳求道，“马克，咱们一直没能盈利。要不试试在Facebook办的会上销售门票？”他说不行，之后用白板笔写下了几个字：增长。</p><p></p><p>马克的目标是让Facebook拥有10亿用户。面对我们提出的每个主意，他都会问：“这对业务增长有帮助吗？”如果这些想法跟业务增长的目标关系不大，那就果断放弃。</p><p></p><p>快速成长不是同时把多件事做到80分，而是专注于把一件事做到100分。</p><p></p><p></p><h2>2.加快脚步</h2><p></p><p>在Facebook，每天工作12个小时以上属于常态。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/c0/c080fbd7070c846f4bdbdb237cf5a35d.png" /></p><p></p><p></p><p>马克总在敦促我们要有紧迫感。他在公司里常说的口头禅就是“加快脚步，打破常规”。“如果你还没打破常规，就说明你的脚步还不够快。”</p><p></p><p>我们的想法很简单，为了加快行进速度、全面了解社区需求，我们宁愿忍受一定数量的bug和缺陷。每天，我们都会向网站发送几项更新。相比之下，像微软这样的公司则需要几个月的时间才能勾勒出产品细节，之后经过大大小小的会议讨论，最后再开始着手构建。</p><p></p><p>作为一家初创公司，我们跟行业巨头相比的最大优势就是速度。</p><p></p><p></p><h2>3.只雇佣最出色的员工</h2><p></p><p>马克只会雇佣那些他愿意与之共事的员工，甚至我们的客户支持团队里，也挤满了来自哈佛的博士。这帮曾经效力于Facebook的人们后来参与创立的Asana、Quora、AppSumo还有OpenAI等等。</p><p></p><p>对于任何一家初创公司来说，雇佣的前十个人都是最重要的，而其中每个人都占据公司的10%。如果有三个人不够优秀，那就代表公司里30%的部分不够优秀！</p><p></p><p>相较于大公司，初创企业更依赖于优秀的人才。</p><p></p><p></p><h2>4.善待员工</h2><p></p><p>马克意识到，打造出让人愿意身处其中的工作环境不仅有助于吸引更多优秀人才，同时也能让现有员工生出对企业的自豪感来，甚至愿意主动加班。</p><p></p><p>因此，Facebook做了很多现如今已经被视为行业常态的探索：</p><p>⦁ 在硅谷最昂贵的社区之一设立一座豪华办公楼。</p><p>⦁ 开出极具竞争力的薪酬。</p><p>⦁ 为每个人购置1000美元的办公椅。</p><p>⦁ 免费提供PowerBook和黑莓手机。</p><p>⦁ 提供美味的早、中、晚餐。</p><p>⦁ 冰箱里有你所能想到的任何饮料。</p><p>⦁ 公司支付拉斯维加斯旅行的所有费用。</p><p>⦁ 每周五免费餐食发放。</p><p>⦁ 免费洗衣/干洗服务。</p><p>⦁ 补贴住房。如果住在办公楼周边1英里之内，每月可以领取600美元。</p><p>⦁ 面向全体员工开放的夏季/冬季度假小屋。</p><p></p><p>人们希望得到认可，而这种对员工的善待能够提高工作效率，帮助大家抖擞士气。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/f5/f5fd04099933a79f101ffae3b513fe3d.png" /></p><p></p><p>Facebook早期派对</p><p></p><p></p><h2>5.按自己的路子走</h2><p></p><p>很多人之所以会选择那些自己不太了解或并不感兴趣的创业领域，是因为他们听说这个方向很“热门”。比如，他们原本是做会计的，但创业时却尝试帮内容创作者开发专业软件……这简直是在胡扯。</p><p></p><p>从一开始，马克想搞的根本不是什么初创企业——他只是想帮大学里的学生们建立联系。而我自己创办AppSumo，是因为我喜欢科技产品和处理交易。</p><p></p><p>不少顶尖企业刚开始都是这样来的，创始人们先是尝试解决自己面临的问题，之后再把解决方案分享给更多人。这就叫生于自私，而成于无私。</p><p></p><p></p><h2>6.关注细节</h2><p></p><p>我记得马克曾经在凌晨3点给我发过一封电子邮件，告诉我在一份文件中漏了一个句号。是的，一个句号！！</p><p></p><p>马克不接受任何不完美的东西。如果他觉得某个项目做得不好，就会告诉负责人果断放弃、推倒重来。他对Facebook里面这个F要大写就特别偏执，甚至曾经送给我一本语法书让我好好打磨文笔 😂</p><p></p><p>马克为我们设定了高到卓越的标准，这让工作做起来很有挑战性，但也非常有益。</p><p></p><p></p><h1>7.向团队放权</h1><p></p><p>令人意外的是，马克却并不会过多参与日常运营。虽然有时候也会参与代码编写，但他的大部分时间都专注于制定宏观愿景。他特别擅长给人们设定目标、划出界限，然后从旁提供指导。</p><p></p><p>工程师和产品经理们可以自行提出功能并着手构建，期间无需任何额外的审批和干预。马克曾说他想要Facebook的手机版，而初版的所有细节都由我们一线开发自行斟酌。</p><p></p><p>只有团队感受到这种主人翁的地位时，大家才会像主人一样思考和行事。</p><p></p><p></p><h2>8.是“人”，不是“用户”</h2><p></p><p>每当有人使用“用户”这个字眼，马克都会气得大叫。没错，就是音量很大那种。他咆哮道，“那些是活生生的人！”</p><p></p><p>在产品中充分考虑人性化因素，能让厂商更好地为客户服务。与只看数字相比，这个角度也能让我们更好地理解困扰受众的问题。</p><p></p><p>所以要永远记得，冷冰冰的用户名和邮件地址背后，对应的都是活生生的人！</p><p></p><p></p><h2>9.只留合适的人</h2><p></p><p>就在我入职的当天，我顶头上司被开除了。我的下任上司在一个月后被炒掉，而我自己是在9个月之后。马克非常重视的一条原则，就是只留合适的人。</p><p></p><p>他会果断解雇那些拖累了Facebook发展的人，并迅速提拔能够帮助Facebook实现目标的人。</p><p></p><p>在AppSumo，我们也会对潜在的新同事进行付费试用，之后再决定对方适不适合接受这份全职岗位。</p><p></p><p></p><h2>10.风物长宜放眼量</h2><p></p><p>当初马克面对10亿美元的Facebook收购要约时，我们都才20多岁。而当他表示拒绝时，实际是向我们所有员工包括全世界发出了明确的信息：他的目标是让整个世界连通起来，这让我们无比兴奋。</p><p></p><p>当初在Facebook工作时，我做的一切就是思考、讨论和畅想Facebook的未来。这甚至不像是一份工作，Facebook就如同我的女朋友，占据了我的所有时间和心力。</p><p></p><p>这种宏大的愿景激励员工们从床上蹦起来，冲进办公室尽最大努力完成工作。它让员工们有了一种超越金钱的目标感和使命感。</p><p></p><p></p><p></p><h1>被Facebook解雇的四点反思</h1><p></p><p></p><p>除从扎克伯格那里学到的有用经验外，此前 Kagan 还曾在一本电子书里总结了自己被 Facebook 解雇的原因。 在 Kagan&nbsp;看来，自己过去在Facebook的工作中犯了四个错误，才导致扎克伯格认为他是一个需要被解雇的“累赘”。</p><p></p><p></p><h1>1.向媒体泄露了公司机密</h1><p></p><p></p><p>在科切拉音乐节上的一次醉酒后，Kagan&nbsp;告诉外媒TechCrunch的创始人迈克尔·阿灵顿（Michael Arrington），Facebook计划将业务范围从大学生扩展到为Microsoft和Apple等公司提供专业社交网络。原本Facebook准备在第二天早上公布这一消息，但在与 Kagan&nbsp;的谈话后，阿灵顿当晚便发布了这一新闻。几周后，Kagan&nbsp;便被解雇了。</p><p></p><p></p><h2>2.试图利用Facebook为自己出名</h2><p></p><p></p><p>Kagan自述，他过去常常在Facebook总部举办创新企业聚会，因为他享受炫耀自己的工作场所，还经常在自己的个人网站 OKDork.com 上写关于Facebook业务的博客文章。扎克伯格曾将Kagan拉到一边，让他在自己和Facebook之间做出选择。不知何故，卡根当时仍然没有理解扎克伯格的意图，因此后来也没保住自己的工作。</p><p></p><p></p><h2>3.工作中出现失误</h2><p></p><p>Kagan对此举了一个例子：“我在与（Facebook联合创始人）达斯汀·莫斯科维茨（Dustin Moskovitz）合作决定哪些公司能够加入我们的专业网络时，负责在谷歌上搜索企业名单。经过一个星期的收集，我给出的公司名单乱七八糟，没有任何顺序可言。把这份名单交给达斯汀后，他当然很失望。之后他运行了数据库查询，并根据我们已经在网站上注册的公司域名汇总了一些公司，然后将这些公司添加到候补名单中。是的，这样做聪明多了。”</p><p></p><p></p><h2>4.跟不上Facebook的增长</h2><p></p><p>Kagan 加入 Facebook 时，该公司只有 30 名员工和几百万用户。当他被解雇时，公司已经有 100 多名员工，并逐渐发展成为一家发展速度稍慢、需要管理的人更多的公司。而Kagan之后并没有改变自己的工作方式以适应公司文化的变化，还进行了一定程度的抵制。他写道：“在事情混乱和需要完成任务的时候，我是公司里最出色的员工之一。但我在处理多人的项目、组织几个月的进度计划以及处理政治事务方面都很吃力。”</p><p></p><p>参考链接：</p><p><a href="https://noahkagan.com/what-i-learned-working-for-mark-zuckerberg/">https://noahkagan.com/what-i-learned-working-for-mark-zuckerberg/</a>"</p><p><a href="https://www.businessinsider.com/how-noah-kagan-got-fired-from-facebook-and-lost-185-million-2014-8">https://www.businessinsider.com/how-noah-kagan-got-fired-from-facebook-and-lost-185-million-2014-8</a>"</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/OyhqIHriWz00fr7k3xcH</id>
            <title>携手攀登安全“芯”高地！2024紫光同芯合作伙伴大会安全芯片创新应用论坛圆满落幕</title>
            <link>https://www.infoq.cn/article/OyhqIHriWz00fr7k3xcH</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/OyhqIHriWz00fr7k3xcH</guid>
            <pubDate></pubDate>
            <updated>Fri, 23 Aug 2024 03:07:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>8月22日，2024紫光同芯合作伙伴大会安全芯片创新应用论坛在北京圆满落幕。本届论坛以“智慧芯生态&nbsp;互联芯安全”为主题，聚焦金融支付、电子证件、安全识别与移动通信领域的硬件创新、软件算法、技术趋势等行业议题，产业链各方汇聚一堂，为安全芯片创新应用发展和产业生态建设提供了全面解题思路和最佳实践参考。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/c9/ca/c9a84a132d39be36c2d32c52e72a97ca.png" /></p><p></p><p>&nbsp;</p><p>支付、证件、识别：芯之所向，无所不至</p><p>&nbsp;</p><p>在数字化浪潮推动下，金融支付行业面临转型升级，安全芯片应如何乘势而上？来自北京银联金卡科技有限公司、金邦达有限公司、福建新大陆支付技术有限公司等企业的代表分别发表演讲。他们表示，作为保障金融交易安全不可或缺的一环，安全芯片的功能优化升级成为行业共识；面对生物识别、物联网支付等新兴技术和日益复杂的应用场景，安全芯片将在安全性、便捷性、功能性这几个关键维度实现更大突破。</p><p>&nbsp;</p><p><img src="https://static001.infoq.cn/resource/image/b4/0e/b468c48938af12502f971f9516e3e50e.png" /></p><p></p><p>&nbsp;</p><p>论坛上，紫光同芯安全芯片事业部副总经理路倩发表了《芯之所向无所不至——支付证件识别产品创新之路》主题演讲，她表示，安全芯片产品从聚焦性能提升和应用演进，到追求为行业打造极致安全、极致可靠、极致便捷和极致性价比的产品生态，紫光同芯产品的技术迭代与市场需求和行业标准密不可分，并将始终遵循以客户为中心和以专业技术为基石的原则。未来，紫光同芯将通过技术创新，紧密贴合国际安全标准的升级步伐，灵活适应日趋多元的应用场景，持续不断为行业注入芯动力。</p><p>&nbsp;</p><p>移动通信：无般不识，大器已成</p><p>&nbsp;</p><p>在AI、5G等前沿技术推动下，数字化进程渗透千行百业。作为数字化时代的基础支撑，全球信息通信行业监管正在向以促进数字经济发展为目标的新方向演进，技术创新、数字化转型、算网融合、安全保障提升成为大势所趋。</p><p></p><p>探讨当前通信行业趋势，中国移动研究院业务研究所和星汉智能科技股份有限公司等企业的代表分别发表演讲。聚焦超级SIM多应用操作系统的生态建设，中国移动联合产业制定了多应用操作系统产业标准，并将持续推进多应用操作系统的泛行业生态建设，坚持以开放和协作促进产业可持续发展；拥抱数字化转型机遇，星汉智能表示，作为数字化底层基础，eSIM技术促进各行各业数字化转型的进程，将成为企业发展数字产品及服务不可或缺的重要支持。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/8e/71/8ec06f0fddb4c5acc5b25c2728b51571.png" /></p><p></p><p>&nbsp;</p><p>紫光同芯安全芯片事业部副总经理、移动通信产品线总经理王征结合紫光同芯在通信领域的创新探索，回顾了SIM卡持续发展的历程。他表示，SIM从追求极致性价比出发，历经追求极致可靠的M2M SIM，直至现今在追求极致性能与安全并重的eSIM领域深耕细作，紫光安全芯片已经在移动通信领域实现了全品类覆盖，帮助全球行业客户布局数字产业生态。未来，随着AI、5G及卫星通信等技术发展，紫光同芯的SIM之路将朝着更便捷、环保、安全、强大的方向继续演进，为万物互联注入芯力量。</p><p>&nbsp;</p><p>智慧芯生态，互联芯安全。紫光同芯以技术为基础，以客户为中心，以市场为导向，致力于为全球伙伴提供安全可靠、高效便捷的产品和服务。未来，紫光同芯期待与更多伙伴一道聚合产业优势，融通生态链条，驱动技术创新与应用覆盖，助推产业生态向更智能、更便捷、更安全的方向加速迈进，以科技之光照亮幸福生活。</p><p></p><p>活动推荐：</p><p></p><p>芯片作为最底层的设施受到许多从业者的关注，在10 月 18-19 日，由InfoQ主办的 QCon 全球软件开发大会（上海站）上，我们特别策划了【大模型基础设施与算力优化】专题，将深入探讨如何搭建稳定高效大模型基础设施，提高各类大模型训练推理过程中的 Scaling 的效率和成本，为一线技术工程师和高级技术管理人员提供前沿知识、一手的实践经验和有深度的技术判断。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/df/dfd31ee989a7951439a77fec138d4cf8.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/xLgQzB1Wc1OSYZCXe8c7</id>
            <title>科大讯飞做大模型：功能不需样样顶尖，先打造业务需要的能力</title>
            <link>https://www.infoq.cn/article/xLgQzB1Wc1OSYZCXe8c7</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/xLgQzB1Wc1OSYZCXe8c7</guid>
            <pubDate></pubDate>
            <updated>Fri, 23 Aug 2024 01:42:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>“讯飞研究院并非一个纯粹闭门造车的技术研究院。”科大讯飞副总裁、研究院院长刘聪说道。从 2005 成立至今，讯飞研究院为科大讯飞的产品提供了有力的技术支持，这次大模型浪潮中也不例外。</p><p></p><p>正如刘聪所说，“研究院的大部分技术都对应着具体的业务需求。”讯飞研究院一边迭代自己的基座模型，一边深入业务需求进行相关研发。</p><p></p><p>2022 年 12 月，讯飞启动了“1+N 人工智能大模型技术及应用”专项攻关，其中“1”代表通用人工智能大模型底座，“N”代表将人工智能大模型技术应用在教育、医疗、汽车、办公、智能硬件等多个行业领域。如今，讯飞对“1”和“N”的理解发生了哪些变化？</p><p></p><p></p><h4>开发，今年的节奏已经不同</h4><p></p><p></p><p>从去年 5 月星火大模型首个版本发布至今，一年多的时间里，讯飞研究院已经将该模型迭代到了 4.0 版本，模型也从最初的对标 GPT-3.5，更新至迭代最新的 GPT-4 Turbo。</p><p></p><p>纵观整个去年，讯飞很多产品是集中发布的，基本上 2、3 个月就有一次产品发布。这个节奏与之前几乎一年一次发布的讯飞相比要快很多。</p><p></p><p>刘聪介绍，这个时候的讯飞，更多扮演的是“追赶者”的角色：摸索整个大模型训练过程中的各种经验，比如如何处理数据、scaling law 是否符合预期等，对标国际领先模型，同时关注一些落地场景。另外，国产化也是讯飞要重点推进的工作。</p><p></p><p>对于去年的整体节奏，刘聪认为讯飞做得是比较好的，这源于讯飞会提前做好发布计划，“我们更多的是计划做得比较好，让大家感受到了每个大版本之间的变化。”</p><p></p><p>但是，今年的节奏导向已经与去年有所不同。</p><p></p><p>“今年年初，我们就对大模型这件事情已经摸索得比较清楚了。”刘聪说道，“我们现在既关注通用底座大模型，同时探索也在如何提升小模型的能力和效果。”</p><p></p><p>当前，大模型与小模型并行发展已经是行业趋势。对于选择大模型还是小模型，刘聪表示主要看场景需要什么样的模型。“如果只泛泛地说‘使用小模型与大模型差距不大’，这纯粹是胡说。”</p><p></p><p>刘聪解释道，在撰写文案、代码编写等方面，一个中小规模的模型即可搞定，讯飞将这类应用定义为一般任务；中等任务涉及行业内的很多知识库和行业深度内容，还有一些高难度任务，例如复杂推理、数学推理等，目前大模型都无法解决，更不用提小模型。</p><p></p><p>“我们一直强调大、小模型时代，并不意味着不再关注大模型了。核心技术原理是先找到大模型的天花板，再优化小模型。小模型的不断进步依赖于大模型的发展。”刘聪说道。</p><p></p><p>另外，讯飞研究院更重要的一项任务是围绕“N”中的刚需场景，把大模型应用做透彻，因此深入解决系统化问题变得非常关键。</p><p></p><p>不过在众多的基础能力上，讯飞也是有选择地进行研发。比如在通用任务中，讯飞最关注的能力之一是数学，因为在刘聪看来，数学能力与推理结合是大模型聪明的表现。</p><p></p><p>但是，不同于有的公司有专门的文生图产品，讯飞的文生图是在星火统一入口里面使用。刘聪明确称，“在文生图方面，我的优先级较低，甚至不专门制作文生视频。虽然我们与视频关系不大，但是我们会制作虚拟人、加强语音能力，我们必须做好语音交互。”</p><p></p><p>在刘聪看来，大模型底座是向多模态拓展的，对讯飞而言多模态的能力逐步提高最重要，但没有必要在一些业务关联度低、资源投入过大的方面做太多投入。在多模态中，刘聪会将重点放到 OCR（Optical Character Recognition，光学字符识别）上，“确保 OCR 做到最好，这与我的实际工作紧密相关。”</p><p></p><p>基于此，讯飞今年的重点虽然还是大模型通用能力的打造，但讯飞不会选择样样争第一，而是在自己认为的最重要的方向发力，比如交互能力等。</p><p></p><p></p><h4>应用，选择更加熟悉的方向</h4><p></p><p></p><p>讯飞研究院的研发工作与业务紧密相连，在研发之前，研究院要与业务部门达成深度共识，比如某个功能达到什么程度、完成客观技术指标后能为用户带来什么价值等。</p><p></p><p>达成共识之后，从研究院内部的算法研发部门、工程引擎部门、服务平台部门和资源部门，再到产品研发部门，整个过程需要一起对齐。无论发布产品、然后不断迭代，还是创新性研发一个产品，都是这样的过程。</p><p></p><p>讯飞被外界认为是较少能真正将技术实现产品落地的企业，刘聪认为这背后的核心原因是讯飞更加深入场景。</p><p></p><p>“我们找 PMF 之所以准确，是因为过去对行业场景和技术的积累。坚持阶梯原则，我们了解大模型在哪个节点可以适配、哪个场景可以发挥价值。”刘聪说道。“此外，讯飞也有深厚的场景资源和用户基础。”</p><p></p><p>以学习机为例，讯飞过去十几年从事学校工作，每天在学校里与老师打磨，持续了解中国教育政策以及未来发展趋势。老师的教学环境如何、不同年龄段的孩子是否有时间额外学习等，如果仅凭想象和拍脑袋是很难定义出来的。教育行业讲究因材施教，而非图文等技术。</p><p></p><p>落地中，选择在已积累的行业优势基础上进行大模型探索，是大多数相对成熟公司会选择的风险相对较低的策略。“自我造血非常重要，所以我们更加关注相对熟悉的方向，例如教育、医疗、办公、汽车和金融。”刘聪说道。</p><p></p><p>而什么时候完成应用则与大模型发展阶段有关系。围绕刚需场景，什么技术可以支撑、支撑度如何等都需要考虑。比如技术阅卷，之前是判断填空、选择题，后来扩展到了解答题并全学科阅卷，这都对技术要求越来越高。有了大模型后，直观的表现之一就是作文批改比之前做得更好。</p><p></p><p>讯飞业务中，硬件是不可忽略的一部分，比如有面向教育的学习机、批阅机等。讯飞业务的特点之一就是每个行业都有软硬件的差异。比如学习机不断将软件功能加到硬件上，以此提升硬件附加值。同时，硬件模式又能助力软件，例如翻译机和办公本都有一些大模型应用来升级体验，这不仅仅是单纯利用大模型的 API 连接，而是形成了适合硬件场景的独特功能。</p><p></p><p>而对外服务中，刘聪观察到，大模型的应用范围已经逐渐变大，比如金融这样的代表性场景已经往央国企拓展。“对应用大模型的企业来说，产品价值最重要的是能否降本增效。”刘聪说道。</p><p></p><p>讯飞在对 B 端业务服务过程中，发现算力统一难和整个数据管理难等问题。另外，在对外服务过程中，由于很多企业是私有化部署，因此讯飞在底座模型应用和场景开发中，对用户的场景并不清楚。为此，讯飞通过智能体平台这样的服务来解决。</p><p></p><p>“N 的逻辑必须落地。现在的阶段与去年不同，去年我们的 1+N 有些冗余，需要继续梳理。今年我们将主要的 N 梳理清楚后，一和 N 的协同变得更加系统。”刘聪说道。</p><p></p><p>根据实践观察，刘聪总结了两点经验：</p><p></p><p>第一，不必专门针对“N”，可以将其合入“1”的能力中。一个场景下的常用能力可以满足，或者在 1 基础上做某个智能体就能满足，合入“1”里就可以，这是减少重复开发的逻辑。</p><p></p><p>第二点，统一模型接口和数据接口。这里的 N 可能是业务线主导，有的是研究院主导，但一个公司内部的每个业务数据标注体系如果都不同，那将它们合并汇总到主模型就会相当困难。完成模型接口后，需要标注数据、SFT 数据和强化学习数据，形成一个技术体系。在此框架下，用户可以自行寻找专家进行标注，这样既能优化流程，又能将这些 N 的数据回流到数据库中。</p><p></p><p></p><h4>结束语</h4><p></p><p></p><p>对于今年讯飞的“1”和“N”而言，刘聪表示，“虽然是动态发展的，但是不能放弃。如果不演进，三个月就不行了。”</p><p></p><p>不过，随着模型规模的增大，研发周期会逐渐拉长，因此刘聪认为大模型技术后续可能不一定还那么卷。“GPT-5 底座大模型投入巨大，升级周期会变长，局部亮点可能会不断出现，但可能很难有 GPT-3.5 到 4 那么大的提升。”</p><p></p><p>在大模型争夺战中，讯飞给自己的定位是“综合能力是头部，在自己擅长的地方保持耐心和耐力”，因为一个很现实的问题就是大模型的企业同质化严重，但其实想要在每个领域都做到最好很难，OpenAI 和谷歌都做不到。</p><p></p><p>“我们还要给用户习惯的时间，通过产品培养用户和客户的耐心。”刘聪说道。</p><p></p><p>内容推荐</p><p></p><p>在这个智能时代，AI 技术如潮水般涌入千行百业，深度重塑生产与生活方式。大模型技术引领创新，精准提升行业效率，从教育个性化教学到零售精准营销，从通信稳定高效到金融智能风控，AI 无处不在。它不仅是技术革新的先锋，更是社会经济发展的强大驱动力。在 AI 的赋能下，我们正迈向一个更加智能、便捷、高效的新未来，体验前所未有的生活变革与行业飞跃。关注「AI 前线」公众号，回复「千行百业」获取免费案例资料。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c0/c0779541886d6212211f10391187b0f5.png" /></p><p></p><p>会议推荐</p><p></p><p>InfoQ 将于 10 月 18-19 日在上海举办 QCon 全球软件开发大会 ，覆盖前后端 / 算法工程师、技术管理者、创业者、投资人等泛开发者群体，内容涵盖当下热点（AI Agent、AI Infra、RAG 等）和传统经典（架构、稳定性、云原生等），侧重实操性和可借鉴性。现在大会已开始正式报名，可以享受 8 折优惠，单张门票立省 960 元（原价 4800 元），详情可联系票务经理 &nbsp;17310043226 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/df/dfd31ee989a7951439a77fec138d4cf8.png" /></p><p></p><p>今日荐文</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621559&amp;idx=1&amp;sn=35db58c708c2a1ab0ab3bb3307014d2b&amp;chksm=fbeba278cc9c2b6e27f50c0361eb0202485480e2e2a0a033e14c819f406fef31599a0a2fab1a&amp;scene=21#wechat_redirect">“创业一年，人间三年”，李沐亲述 LLM 创业第一年的进展、纠结和反思</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621514&amp;idx=1&amp;sn=d61a90572d1ece086f4c8238e82b2073&amp;chksm=fbeba245cc9c2b53f02c6ae3a2a3dd0714e5571ac5e12f5dccbb42bd4b5223b1e2979cb34269&amp;scene=21#wechat_redirect">刚刚，OpenAI又双叒叕鸽了！没等来“草莓”发布，只敷衍发了评测集，网友：拿这来抢谷歌发布会风头？</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621462&amp;idx=1&amp;sn=7fdb125768fc1da501d6ddc64efb7fce&amp;chksm=fbeba299cc9c2b8f23b1924c2c3ce0715ee8b2f3dc4a2b34bbf36b7e6d1927401d767b479774&amp;scene=21#wechat_redirect">三年亏损51亿元，去年卖出22台车！文远知行被爆赴美IPO，估值超360亿元</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621417&amp;idx=1&amp;sn=c2527e66bd2f71ae9f8502f019ad02b7&amp;chksm=fbeba2e6cc9c2bf0aa096968c96e502b20da1bef64c5e0be38d2ffc1ed740b9eb1b2b8bfbe21&amp;scene=21#wechat_redirect">一年前还看好，现在却急刹车？国内资本动辄数十亿投资，华尔街却不敢给了</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621293&amp;idx=1&amp;sn=f7a1e3bd450fdb11e2f23d3019804a9d&amp;chksm=fbeba362cc9c2a74471529947135ee0e3883cd5933487a07bf3f77d3bdd1d37932518d2203d3&amp;scene=21#wechat_redirect">京东发行稳定币；AI服务器大厂豪气分红115.2亿；小米二期工厂附近挖出古墓？王化：假的｜AI周报</a>"</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c5/c548af7ad39632ca346cd454eef4d2a8.gif" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/rBVmYQGeaHJCxyTks1pX</id>
            <title>顺丰揭秘：大模型技术如何重塑物流供应链</title>
            <link>https://www.infoq.cn/article/rBVmYQGeaHJCxyTks1pX</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/rBVmYQGeaHJCxyTks1pX</guid>
            <pubDate></pubDate>
            <updated>Thu, 22 Aug 2024 10:08:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>物流与供应链系统的庞大规模、环节的复杂多变、数据的复杂性、场景的多样性，使得物流与供应链系统的建设和运行变得更加复杂。而大模型技术作为 AI 的一项重要成果，在物流供应链领域具有无限的潜力和广阔的应用前景，并在推动物流供应链领域数字化、智慧化变革中扮演着不可忽视的角色。</p><p></p><p>在 8 月 18 日 -19 日的 <a href="https://aicon.infoq.cn/2024/shanghai">AICon 全球人工智能开发与应用大会</a>"上，InfoQ 荣幸地邀请了顺丰顺丰科技人工智能总工程师高磊，他为我们分享了大模型在物流和供应链场景中的应用，以及顺丰相关技术体系与产品体系的建设思路与实践经验。本文会会前采访文章，期待你对了解大模型在物流行业的应用有所帮助！</p><p></p><p>以下为采访正文～</p><p></p><h5>InfoQ：顺丰在建设物流决策大模型技术体系时，采用了哪些具体的技术手段和方法？这些技术是如何与现有的物流和供应链系统进行融合的？</h5><p></p><p></p><p>高磊： 当前 AIGC 技术主要以文本、图片、语言、视频等模态为主，因此在这些信息相对富集以及以这些模态作为主要信息载体的领域更容易落地，比如售前的营销素材的生成，售后的智能客服，以及办公领域的 FAQ、信息摘要等场景。</p><p></p><p>但是我们所关注的供应链运营和决策优化领域中，如何利用大模型与其背后的技术去解决供应链运营过程中问题，提升决策质量和效率，帮助客户业务更好的降本增效，目前并未看到很好的行业实践和落地案例。我们结合对于大模型技术的理解与顺丰的业务实践，逐渐摸索出三个方向：</p><p></p><p>结合顺丰沉淀的业务 know-how 与已有技术能力，构建行业智能体，相关技术被应用于供应链智能控制塔产品中突破文本、图片等模态的限制，构建物流决策大模型，让大模型技术直接作用于核心决策问题，相关技术被应用于供应链执行优化产品中基于多模态大模型的能力构建多层级多通道需求预测模型，解决消费供应链领域中需求预测的难点与痛点问题，相关技术被应用于供应链计划产品中</p><p></p><h4>需求预测模型在供应链计划产品中的应用案例</h4><p></p><p></p><h5>InfoQ：可以介绍一下什么是基于多模态大模型的能力构建多层级多通道需求预测模型吗? 它解决哪些痛点与难点问题？</h5><p></p><p></p><p>高磊： 我们构建这个模型的初衷是为了解决消费供应链领域中商品蚕食效应、新品新店预测等业界难题。</p><p></p><p>首先，需求预测在供应链计划中非常重要，因为他是需求计划、供应计划、生产计划等诸多计划的源头，准确的需求预测对于提升计划准确性，提升供应链效率而言至关重要。但是需求预测本身难度很大，尤其是消费供应链，受到诸多因素的影响，比如新品上市、老品下架、蚕食效应、促销、节假日、季节、天气等。其中蚕食效应，新品和新店的预测一直是行业普遍存在的难题，传统的算法模型难以有效解决这些问题。</p><p></p><p>以商品蚕食效应和新品上市为例，比如某个门店一直卖 10 种蛋糕，平时所有蛋糕的销量总和是大约 100，然后某一天突然上市了一个新的蛋糕，并做了促销，那么这里会出现两个问题：1. 新蛋糕的销量该如何预测，2. 老蛋糕的销量会受到多大影响？</p><p></p><p>传统的需求预测模型从单一商品视角建模，在解决这两个问题上存在较大困难：在第一个问题上，因为缺乏历史销量数据，很难建模，往往预测偏差很大，在第二个问题上，单一商品视角的建模难以有效捕捉商品之间的关联关系与相互影响，在上新期间难以捕捉到蚕食效应造成老品系统性的偏高。</p><p></p><p>为了解决这些行业难点问题，我们设计了基于多模态大模型的能力构建多层级多通道需求预测模型，从特征角度，我们通过预训练好的多模态模型将商品的文字描述如商品名，商品描述，配料表，价格等和商品的图片提取为表征商品内在属性的 Embedding 向量。通过选择合适的多模态大模型，我们发现提取出的 embedding 能够很好的表达商品之间内在的一些相关性。</p><p></p><p>提取了多模态特征之后，为了更好的学习商品之间的关联性，我们设计了一种多层级多通道的需求预测模型。</p><p></p><p>这里解释一下层级的概念，消费供应链预测中往往存在多种层级，比如时间层级：日到月到年；空间层级：门店到 RDC 到 CDC；品类层级：具体的 SKU 到二级品类到一级品类，往往在各种层级上都要输出预测结果，并且层级之间的结果应该能够对应上，比如某个门店内所有商品的总销量预测应该等各个商品预测之和。</p><p></p><p>多层级多通道的需求预测模型能够很好地学习同一层级内的商品之间的内在关联性，以及层级之间的关联性，从而更好的得到预测结果。</p><p></p><p></p><h5>InfoQ：这个模型的实际应用效果如何？</h5><p></p><p></p><p>高磊： 我们在某个实际客户的场景下做了测试，整体上，新的模型可以在预测准确性上提升绝对值 5 个百分点，这个是我们在传统方式下做了很久也没有难达到的程度。同时得益于多层级多通道统一建模极大的减少了模型的数量，以及 GPU 的使用，在计算性能方面实现了 120 倍的提升，对机器资源的需求也减少了 5 倍。</p><p></p><p>我们也着重验证了一下新模型在新品等场景下的预测表现，得益于多模态信息的引入与多层级多通道学习机制，新模型能够有效的捕捉到新品和老品之间的相关性与蚕食效应，可以在上新期间取得显著的的新老品预测准确度的提升。</p><p></p><h4>供应链智能体在供应链智能控制塔产品中的应用细节</h4><p></p><p></p><h5>InfoQ：什么是供应链智能体？它具备一些什么样的能力？解决什么问题？</h5><p></p><p></p><p>高磊： 我们知道供应链运营是一个专业程度很高，并且非常严谨的领域，因为任何数据或者决策建议的错误都可能带来比较严重的损失。大模型本身存在一些固有的缺陷如不擅长精确数值计算，幻觉，专业程度不够高等问题，限制了其在供应链运营领域的应用。</p><p></p><p>比如前端时间公众号上有个比较火的文章，讲得是问大模型 9.11 和 9.8 哪个更大，绝大多数大模型都回答 9.11。再比如把过去一段时间的历史销量和库存数据丢给大模型，让它去做库存优化，大模型也很难去做这种专业的事情。为了解决以上问题，我们的解决思路是结合大模型和专业小模型，以及顺丰多年沉淀的供应链实践，去构建供应链的行业智能体。</p><p></p><p>具体来说，我们通过 RAG 技术结合我们沉淀的业务知识库，让大模型具备更深入的供应链知识，同时我们将丰智云体系中沉淀的各种算法能力，比如预测、仿真、运筹优化、归因分析等，抽象成工具并交给大模型调用。由此构建出具备供应链行业知识的业务专家智能体与以及具备专业算法能力的算法专家智能体，并通过这些智能体的协作，去服务具体的业务场景，如销售分析，库存优化等场景。通过以上方式，可以有效的改善和缓解大模型在供应链场景下存在固有缺陷。</p><p></p><p></p><h5>InfoQ：在供应链智能控制塔产品中，顺丰如何集成供应链智能体的能力？</h5><p></p><p></p><p>高磊： 我们知道在供应链控制塔中，有一块很重要的能力是供应链诊断与分析能力，传统方式下，我们需要建立大量的报表来呈现业务指标与各种问题，但是这种形式是相对静态的，当出现新的场景和问题的时候往往还是需要手动获取数据、分析数据或者开发新的报表，难以敏捷的响应新的需求。</p><p></p><p>另外，从数据分析角度来看，大致存在 3 种类型的分析:</p><p></p><p>描述性分析：对数据进行整体概括和总结，以了解数据的基本特征和趋势，形成对业务现状的整体认识诊断性分析：通过深入挖掘数据的背后原因，解释数据异常或变动的原因，并为问题提供决策依据预测性分析：利用历史数据和模型来预测未来事件或趋势的发展，为决策提供先见之明</p><p></p><p>目前传统的控制塔还是以描述性分析为主，在诊断性分析和预测性分析方面提供的支持较少。</p><p></p><p>通过将供应链智能体融入到丰智云塔产品当中，通过多个智能体的协作，针对履约、库存、销售等领域的问题，提供从指标查询与分析到异常识别与归因再到提供优化建议的完整的服务支持，从而为客户提高更敏捷与高效服务。而在这些服务的背后，智能体利用的是成熟、专业的预测、仿真、运筹优化等模型工具，来确保输出结果的准确与可靠。</p><p></p><p></p><h4>物流决策大模型实际效果</h4><p></p><p></p><p></p><h5>InfoQ：什么是物流决策大模型？他与语言大模型等有什么区别和联系？</h5><p></p><p></p><p>高磊： 我们知道语言大模型是一个通过 Transformer-Like 的架构，利用自回归的形式进行文字序列生成的模型，而很多人不知道的是物流中的很多问题，其实也可以认为是一个序列生成或者说是序列决策的问题，比如去 3 家门店 a、b、c 送货的一个路径规划问题，可以认为是一个决定先去哪，再去哪，最后去哪的序列生成问题。再比如装箱问题，10 个物品要装到箱子里，也可以认为是一个先装哪个物品，并以什么样的姿态装进去，再装哪个物品这样的问题。</p><p></p><p>所以，本质上，物流中的很多问题和语言生成的问题一样，都是序列生成的问题，因此均可以采用相同的技术架构来解决。这是他们相同的地方。</p><p></p><p>不同的地方显而易见，就是模态的不同，不同于语言模型生成的是文字，物流决策模型生成的就是决策本身。另外不相同的点是目标不同，语言模型的目标是生成文字的合理性与有效性，能够符合语言规律并有效解决用户的问题。物流决策模型除了生成决策要合理外，还有优化目标在里面，比如生成的线路成本越低越好。</p><p></p><p>丛技术角度来说，我们知道语言大模型本身基于两大关键技术，Transformer 和 RLHF，其中 Transformer 在很多算法场景下的成功应用已经充分证明了其能力的强大，而 RLHF 技术因其解决了人类价值观与偏好对齐等问题，将大模型的实用程度推上了前所未有的程度。在物流大决策模型中，我们也是基于这两大技术进行了构建，以路径规划场景为例，通过 Transformer 架构并结合顺丰海量的场景以及规划数据，构建了路径规划的基座模型，并通过 RLHF 技术来解决与业务偏好和具体业务场景对齐的问题。</p><p></p><p></p><h5>InfoQ：如何将物流决策大模型应用到供应链优化产品中，它能够带来一些什么样的优势? 具体落地效果如何？</h5><p></p><p></p><p>高磊： 总体来讲物流决策大模型带来两方面的显著优势，第一个是计算性能方面，传统的运筹模型主要基于搜索的机制，在一定引导下在一个巨大的解空间里面尽可能的搜索较好的解，当问题规模变大，解空间指数级别增长时，往往搜索到较高质量的解需要相对较长的时间，而物流决策模型基于序列生成的方式，在训练的较好的情况下，能够快速将较高质量的结果直接生成出来，再经过 GPU 高速并行计算的加持，能够很快的得到结果。</p><p></p><p>以我们实际鲁多的某客户装箱优化场景举例，目前我们可以平均 20ms 的时间内计算出一个使用传统运筹方法需要 10 分钟才能计算出来的订单，并且得到的解还能略微超过传统运筹方法。</p><p></p><p>另外一方面的优势来自于 RLHF 微调技术，通过 RLHF 我们可以让我们的模型有能力学习到业务在特定场景下的业务偏好与特殊需求。这将我们的产品在面对业务变化与新的算法场景时候可以从定制开发方式转向数据驱动的方式。</p><p></p><p>具体来说，在传统方式下，当业务变化或者新的场景出现时，我们需要我们的算法工程师不断的和业务沟通并理解业务，然后设计针对性的算法，并做很多 POC 试验，输出结果给到业务进行验证，往往这个过程会反复很多次并持续很久，因为往往业务无法将所有影响因素和潜在的业务规则一次性说清楚，很多时候碰到问题才解决问题。</p><p></p><p>使用 RLHF 微调技术，我们可以以数据驱动的方式解决很多问题，当输出结果不满足业务预期时，用户可以自己对结果进行调整，我们的产品会记录调整过程，逐渐积累业务偏好数据，并使用业务偏好数据不断进一步优化我们的模型，使输出的结果越来越符合业务实际需要。</p><p></p><p>当然这里面需要额外考虑的问题是并不是所有的业务调整或者业务偏好都是合理的，因此我们在产品里面设计了偏好与优化效果之间权衡机制，用户可以自己调整更偏向于“像人”还是优化。</p><p></p><p></p><h5>InfoQ：您认为大模型技术在未来供应链管理中的潜在应用有哪些预期或愿景？</h5><p></p><p></p><p>高磊： 以上三个工作是目前我们决策大模型技术在供应链管理中的应用方面进行的初步探索，我觉得还远远没有完全发挥出大模型技术的所有潜力，也还有很多潜在的应用场景没有被挖掘，我们希望能够和业界的生态合作伙伴与友商一起，持续深耕这样一个领域，为提升供应链的数智化水平、实现行业共同进步方面添砖加瓦。</p><p></p><p>嘉宾介绍：高磊， 顺丰科技人工智能总工程师，拥有 10 年 + 机器学习与运筹优化算法经验，研究方向为 NLP、运筹优化、强化学习等。2016 年加入顺丰，现任顺丰科技人工智能总工程师，曾主导顺丰集团内部多个数智化项目的研发与落地工作，涉及领域包括业务量预测、陆运干支线规划与调度、航空规划与调度、运力规划、场站选址、物资调拨等。目前主要负责集团智慧供应链体系建设相关工作。期间带领团队获得十余项发明专利，中物联物流技术创新奖、CCF BDCI 一等奖、最具商业价值奖，运筹帷幄年度行业实践奖与学术应用奖等荣誉。</p><p></p><p>活动推荐</p><p></p><p>AI 应用开发正在逐步成为各行业内的核心创新驱动力，CUI 式的对话助手、串联业务流程的 Agent 或是内嵌在原有业务逻辑中的 AI 模块，都在不断拓展面向用户的新应用场景。我们惊喜地看到从中小创业公司到大型企业，都在利用计算机视觉、自然语言处理、个性化推荐、对话式交互等 AI 能力提升业务效率、优化用户体验，显著增强了产品的市场竞争力。10 月 18-19 日，来 QCon 全球软件开发大会（上海站），了解更多成功应用 AI 技术的案例与最佳实践。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/df/dfd31ee989a7951439a77fec138d4cf8.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/zJzgvWzv7N9cGHjBjJiS</id>
            <title>MiniMax 基于 Apache Doris 升级日志系统，PB 数据秒级查询响应技术实践</title>
            <link>https://www.infoq.cn/article/zJzgvWzv7N9cGHjBjJiS</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/zJzgvWzv7N9cGHjBjJiS</guid>
            <pubDate></pubDate>
            <updated>Thu, 22 Aug 2024 09:44:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><blockquote>作者｜MiniMax 基础架构研发工程师 Koyomi、香克斯、Tinker</blockquote><p></p><p></p><p></p><blockquote>导读：早期 MiniMax 基于 Grafana Loki 构建了日志系统，在资源消耗、写入性能及系统稳定性上都面临巨大的挑战。为此 MiniMax 开始寻找全新的日志系统方案，并基于 Apache Doris 升级了日志系统，新系统已接入 MiniMax 内部所有业务线日志数据，数据规模为 PB 级， 整体可用性达到 99.9% 以上，10 亿级日志数据的检索速度可实现秒级响应。</blockquote><p></p><p></p><p>MiniMax 是领先的通用人工智能科技公司，自主研发了不同模态的通用大模型，其中包括拥有万亿参数的 MoE 文本大模型、语音大模型以及图像大模型。MiniMax 以“与用户共创智能”为愿景，通过对大模型持续迭代，MiniMax 在国内率先完成核心 MoE 算法技术路线的突破。2024 年 4 月，公司推出国内首个上线商用的 MoE 架构、包含万亿参数的大语言模型——“MiniMax-abab 6.5”，模型性能接近国际领先水平。</p><p></p><p>随着模型复杂度以及模型调用量的不断提升，模型训练及推理产生的运行日志也在激增，这些数据对于 AI 应用的运行监控、优化及问题定位至关重要。早期 MiniMax 基于 Grafana Loki 构建了日志系统，在资源消耗、写入性能及系统稳定性上都面临巨大的挑战。为此 MiniMax 开始寻找全新的日志系统方案，并对业界具有代表性的技术栈 Apache Doris 和 Elasticsearch 进行了对比，Apache Doris 在性能、成本以及易用性等方面均优于 Elasticsearch，因此最终选择了 Apache Doris 来构建日志系统。</p><p></p><p>目前基于 Apache Doris 的新系统已接入 MiniMax 内部所有业务线日志数据，数据规模为 PB 级， 整体可用性达到 99.9% 以上，10 亿级日志数据的检索速度可实现秒级响应。</p><p></p><h2>问题及痛点</h2><p></p><p>MiniMax 早期日志系统架构基于 Loki 搭建，Loki 是由 Grafana Labs 团队开发的开源日志聚合系统，设计思想受 Prometheus 启发，不使用传统索引结构、仅对日志标签和元数据构建索引，核心模块包括 Loki、Promtail、Grafana 三个部分，其中 Loki 是主服务器、负责日志存储和查询，Promtail 是代理层、负责采集日志并发送给 Loki，而 Grafana 则用于 UI 展示。</p><p></p><p>在实际 Grafana Loki 使用中，每个集群中单独部署一套完整的日志采集器 + Loki 日志存储/查询服务。Loki 采用 Index + Chunk 的日志存储设计，写入时按日志标签的哈希值将不同日志流分散到各个 Ingester 上实现负载均衡，由 Ingester 负责将日志数据写入对象存储。查询时，Querier 从对象存储取出 Index 对应的 Chunk 后进行日志匹配。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0c/0c4fe6ea65ca8ef30e0f570533b6fc7e.png" /></p><p></p><p>尽管 Grafana Loki 定位为轻量级、水平可拓展和高可用的日志系统，但其在实际业务使用过程中仍存在一些问题：</p><p></p><p>查询资源消耗过大： Loki 未对日志内容创建索引，只能按照标签粒度对日志进行初步过滤。如果想要实现日志内容搜索功能，需使用 Query 对全量日志数据进行全文正则匹配， 而该操作会带来巨大的突发资源消耗，包括 CPU、内存、网络带宽。当查询的数据量和 QPS 越来越大时，Loki 的资源消耗及其稳定性问题也变得越来越不可忍受。Loki 架构复杂繁多： Loki 除了上图涉及模块之外，还有 Index Gateway、 Memcache、 Compactor 等模块，过多的架构组件给系统运维和管理带来很高的难度，配置起来也非常复杂。维护成本及难度高： MiniMax 部署集群数量较多，且每个集群的系统、资源、存储、网络等环境都有差异， 如果在每个集群中部署一套独立的 Loki 架构，维护成本及运维难度都非常高。</p><p></p><h2>为什么选择 Apache Doris</h2><p></p><p>根据 AI 场景的数据特点及业务需求，MiniMax 对新日志系统提出了以下要求：</p><p></p><p>日志数据规模庞大：由于 AI 业务场景具备链路长、上下文数据多、单次请求数据量大等特点，其产生的日志体量远远高于相同用户量级的其他互联网产品，这要求系统能够以较低的成本、稳定可靠的存储这些数据。查询性能要求高：业务对日志查询速度有较高的要求， 比如 1 亿条数据需要在秒级返回查询结果。分析灵活：要求系统能够支持日志指标查询、如某些关键词的统计曲线，同时能够提供日志告警服务。低成本：由于日志原始数据量达到 PB 级，而且还在不断增加，存储和计算的成本需要控制在合理范围内。</p><p></p><p>MiniMax 参考了当前业界成熟的日志系统架构解决方案，发现主流的日志系统一般包含以下几个关键组件：</p><p></p><p>采集端：负责从服务的标准输出采集日志，并将数据推送到中心消息队列。消息队列：负责解耦上下游、削峰填谷。在下游组件不可用时，仍然能保留一段时间的数据，保证系统稳定性。存储查询中间件：负责日志数据的存储和查询，在日志系统场景下，一般要求该中间件具备倒排索引能力，来支持高效的日志检索。</p><p></p><p>根据上述方案组成，MiniMax 决定在新日志系统中：采集端使用 iLogtail、消息队列使用 Kafka、存储中间件为 Apache Doris。在存储中间件的选择上，对比了业界具有代表性的 Apache Doris 和 Elasticsearch 这两个技术栈：</p><p></p><p><img src="https://static001.geekbang.org/infoq/dc/dc7c4c2f127bdfbf6ea247defc5a64fd.png" /></p><p></p><p>Apache Doris 在成本、写入性能、查询性能这几大维度均有较好的表现，尤其在存储效率、写入吞吐、聚合分析等方面有突出的优势，同时兼容 MySQL 的 SQL 语法也更加易用，因此最终选择 Apache Doris 作为存储中间件。</p><p></p><h2>Aapche Doris 日志系统升级实践</h2><p></p><p><img src="https://static001.geekbang.org/infoq/49/4940e10abd9a52783cc30df5d49c5c09.png" /></p><p></p><p>新日志系统（Mlogs）更加简洁，一套架构即可服务全部集群。上层为日志系统的控制面， 包括日志查询接口封装以及配置自动生产与下发模块。 下层是日志系统的数据面， 从左到右依次是日志采集端、消息队列、日志写入器、Doris 数据库。</p><p></p><p>集群服务产生的日志数据由 iLogtail 采集并推送到 Kafka，一部分会经由 Mlogs Ingester 从 Kafka 拉取并通过 Doris 的 Stream Load 写入到 Doris 集群中，另一部分则由 Doris 通过 Routine Load 直接实时订阅拉取Kafka 的消息流 。最后由 Doris 承担全量日志数据的存储与查询，无需每套集群单独部署。</p><p></p><p>在具体的应用落地方面：</p><p></p><p>在日志导入上： 新架构同时使用了 Doris Routine Load 和 Stream Load 方式。Routine Load 开箱即用，可直接处理不需要额外解析处理的 JSON 格式日志。而对于需要过滤与处理的复杂日志， MiniMax 在 Kafka 和 Doris 之间增加了日志写入器 Mlogs Ingester，由其解析和处理后，再通过 Stream Load 写入 Doris 中。在日志检索上： 主要使用了 Doris 倒排索引分词查询能力以及全文正则查询能力。倒排索引分词查询能力：分词查询性能较好， 场景覆盖度较广，主要采用倒排索引查询MATCH 和 MATCH_PHRASE。全文正则查询能力：正则查询精度更高，性能低于比分词查询， 适合小范围查询且对查询精度要求较高的场景，主要使用正则查询 REGEXP。在性能提升上：为进一步提升性能，实现了查询截断功能。当前日志数据按照时间顺序呈线性排列， 如果用户选择的查询范围过大， 会消耗较大的计算存储网络资源， 从而导致查询超时甚至系统不可用。 因此，对用户的查询进行了时间范围截断， 避免查询范围过大；并提前统计所有表的每 15 分钟的数据量， 动态地预估用户在不同表中最大可查询的时间长度。在成本控制上： 使用了 Doris 的冷热数据分层能力， 将 7 天内的数据定义为热数据，7 天之前的数据为冷数据。冷数据存储到对象存储， 以降低存储成本；同时对 30 天之前的对象存储数据进行归档， 仅在必要时恢复归档数据， 这也极大地降低了存量数据的存储成本。</p><p></p><h2>使用收益</h2><p></p><p>目前基于 Apache Doris 的新架构已接入 MiniMax 内部所有业务线日志数据，数据规模为 PB 级， 整体可用性达到 99.9% 以上， 同时也带来以下收益：</p><p></p><p>架构简化：新架构部署简单、一套架构即可服务全部集群，降低了整体系统维护及管理的复杂度，节省了大量的运维人力及成本投入。秒级查询响应： 基于 Apache Doris 的倒排索引能力及查询拦截功能，性能显著提升的同时系统也更加稳定。从 10 亿数据中查询单个关键字以及进行聚合分析，基本可以在 2s 内完成，对于日志数据的分析，大部分场景也可以做到秒级响应。写入性能高：当前系统规格可以实现 10 GB/s 级别的日志写入吞吐，能够在满足持续高吞吐写入的同时满足实时性要求，数据延迟控制在秒级。存储成本低： 数据压缩率较高达到 1:5 倍以上，因此存储空间占用较原本架构极大幅度降低。对于冷数据使用 Doris 冷热分层能力进一步降低数据的存储成本，存储成本节省超过 70%。</p><p></p><h2>未来规划</h2><p></p><p>未来 MiniMax 将持续迭代日志系统， 并重点从以下几方面发力：</p><p></p><p>丰富日志导入预处理能力：增加日志采样、结构化等预处理能力，进一步提升数据的可用性及存储性价比。增加 Tracing 能力：尝试将监控、告警、Tracing、日志等各方面的可观测性系统打通，以提供全方位的运维洞察。扩大 Doris 使用范围：除日志场景之外，Doris 逐步被引入数据分析和大数据处理场景下，助力后续构建数据湖仓能力。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/THI3VfBS9MVCmuO75Y9Y</id>
            <title>“印度马斯克”要发印度第一款 AI 芯片，号称超越英伟达！CEO 要“狼性”，但把数十亿美元打水漂</title>
            <link>https://www.infoq.cn/article/THI3VfBS9MVCmuO75Y9Y</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/THI3VfBS9MVCmuO75Y9Y</guid>
            <pubDate></pubDate>
            <updated>Thu, 22 Aug 2024 08:36:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AI市场迎来又一位新玩家。</p><p>&nbsp;</p><p>以电动踏板车而闻名的 Ola Electric 正在大胆进军人工智能硬件领域。该公司已宣布计划开发印度首款AI芯片系列，首批产品将于 2026 年通过其基础模型系列发布。</p><p></p><h2>印度发布首款AI自研芯片，号称性能媲美英伟达</h2><p></p><p>&nbsp;</p><p>作为印度最大的电动两轮车制造商之一，Ola Electric刚刚公布其计划推出的AI芯片。预计这三款芯片将于2026年投放市场，另外一款则将于2028年与广大用户见面。这些将成为印度推出的首批AI芯片，用以满足印度国内对于此类算力设备的旺盛需求。总部位于印度的数据中心与服务器公司Yotta已经为明年订购了1.6万张英伟达GPU，另有1.6万张上个月已经完成交付。</p><p>&nbsp;</p><p>Ola的首批三款芯片分别是Bodhi 1、Ojas和Sarv 1。第四款Bodhi 2则是首款印度AI芯片的继任者。Bodhi 1专为AI推理和微调而设计，主要应用于大语言模型和视觉模型，，将满足万亿参数 AI 模型的需求。Ola还强调称，Bodhi 1将超越目前最先进的技术，同时消耗更少的电量，这也是当今AI处理领域所面临的最大挑战之一。这一技术进展可能会加速印度各个行业对 AI 的采用。</p><p>&nbsp;</p><p>除此之外，还有专为特定应用而设计的Ojas Edge AI芯片。该公司可以根据多种应用场景对这款芯片做出定制，包括汽车、移动、物联网等。Ola还计划在其下一代电动汽车中部署这款芯片，以帮助运行充电、ADAS等系统。当然，随着AI计算的巨大需求，该公司还推出了Sarv 1，使用到为数据中心构建的Arm指令集。</p><p>&nbsp;</p><p>该公司在演示中披露，其原型芯片的性能与能效均比英伟达GPU更好。但目前尚不清楚他们具体是在与哪款GPU进行比较，例如RTX 4090还是H200，唯一明确的就是其运行功率为200瓦。此外，该公司也没有说明这些芯片将在哪里生产。</p><p>&nbsp;</p><p>Ola 的野心还不止于Bodhi 1。Ola表示还将随即推出 Bodhi 2 芯片，该芯片计划于 2028 年问世。这款更先进的芯片旨在支持具有超过 10 万亿个参数的模型的训练、推理和微调。该公司将其设想为百亿亿次超级计算的基石。</p><p>&nbsp;</p><p>除了专用于 AI 的芯片外，Ola 还在开发基于 ARM 架构的通用服务器 CPU 和用于智能手机和可穿戴设备等消费设备的富 AI 芯片。</p><p></p><h2>“印度马斯克”和他的“狼性”企业文化</h2><p></p><p>&nbsp;</p><p>据公开资料显示，Ola ElectricOla Electric 成立于 2017 年。创始人是 Bhavish Aggarwal，他之前曾是 Ola Cabs 的联合创始人，也是大语言模型AI公司 OlaKrutrim 的创始人。Aggarwal 毕业于孟买印度理工学院，他的职业生涯始于微软。</p><p>&nbsp;</p><p>Bhavish Aggarwal也常被人称为“印度马斯克”，也不仅是因为其在电动车领域的卓越成就，也因Bhavish Aggarwal的为人处事和管理公司的风格和马斯克很相似。</p><p>&nbsp;</p><p>过去十年，印度的一些分析师、评论家、媒体等用各种标签和形容词来描述这位 2010 年开启 Ola 创业之旅的创始人，有的标签是“傲慢”、有的标签是“咄咄逼人”，有人形容他是“工作狂”、“特立独行者”或者是“坚持不懈的创业者”。</p><p>&nbsp;</p><p>面对外界的种种声音，Bhavish Aggarwal称，“这就是梦想远大的代价，我说的都是真心话”。</p><p>&nbsp;</p><p>也如马斯克一样，Bhavish Aggarwal同样推崇“狼性文化”。</p><p>&nbsp;</p><p>Bhavish Aggarwal曾在接受采访时称，“Ola 并不适合每个人，Ola 是雄心勃勃的人的天堂”。Ola 是那些有理想、有抱负之人的最佳去处。在 Ola，他们感觉如鱼得水，因为我们公司充满了创业精神。我们不会过度管理员工。我们会告诉他们你必须做这项工作，请认真完成它”。</p><p>&nbsp;</p><p></p><blockquote>“一旦他们完成工作，他们自己就会超出预期。Ola 就是这样的地方，这样的人才是 Ola 真正闪耀的原因。但在这一过程中，有些人发现这里并不是最适合他们的地方。没关系，不是每个地方都适合每个人，我们的文化是影响力和目标驱动的，以业务建设为导向”。</blockquote><p></p><p>&nbsp;</p><p>Ola的企业文化也常被拿来撰写和讨论。Bhavish Aggarwal坦言，他们的文化就是这样。他们诚实对待自己的目标，不会向任何人隐瞒这一点。</p><p>&nbsp;</p><p>Bhavish Aggarwal表示他会公开告知员工“你别在这里奢望获得早九晚五的工作，如果你想图清闲，就别来Ola上班。如果你想创造终生难忘的故事，并告诉你的后辈你是如何为印度电气化和能源独立之旅的贡献了一份力量，那么 Ola 就是你该来的地方。我们都是诚实的人，有着诚实的目标。”</p><p></p><h2>主营业务还没盈利，Ola又盯上一块新“蛋糕”</h2><p></p><p>&nbsp;</p><p>在Bhavish Aggarwal的带领下，Ola的发展蒸蒸日上。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/93/934f10667ae5e7e3282c55486b15b21c.png" /></p><p></p><p>&nbsp;</p><p>Ola Cabs 首席执行官兼 Ola Electric 创始人 Bhavish Aggarwal 在首次公开募股 (IPO) 前的新闻发布会上发表讲话。</p><p>&nbsp;</p><p>2019 年 5 月 6 日，Ola Electric 宣布 Tata Sons 首席执行官、掌控96家公司的印度资本巨鳄拉坦塔塔已向该公司投资了一笔未公开的金额。此前，塔塔先生也投资了 Ola 的出租车业务。</p><p>&nbsp;</p><p>2020 年 5 月，Ola Electric 收购了位于阿姆斯特丹的电动踏板车制造商“Etergo”。Etergo 制造了一款使用可更换电池的踏板车，续航里程高于标准。这款踏板车的设计、技术和效率令人印象深刻，所以Ola Electric 也被称为“电动车界的特斯拉”。同年12 月，Ola Electric 公司与泰米尔纳德邦政府签署了一份谅解备忘录，宣布计划在泰米尔纳德邦建立全球最大的两轮车工厂（名为 Ola Futurefactory），耗资 240 亿卢比（约合20.46亿人民币）。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/f5/f50e50865e9fed15f079ebdd0c3abfff.png" /></p><p></p><p>&nbsp;</p><p>Ola Futurefactory 占地 500 英亩，是世界上最大的两轮车工厂。</p><p>&nbsp;</p><p>2022 年 3 月，Ola 对以色列电池技术公司“StoreDot”进行了战略投资。Ola Electric 将采用 StoreDot 的快速充电电池技术，并将其用于印度未来的汽车上。StoreDot 是超快速充电 (XFC) 电池的先驱，它克服了主流电动汽车采用的两个关键障碍：续航里程焦虑和充电时间长。</p><p>&nbsp;</p><p>收购完成并充分处理文书工作后，Ola Electric 宣布要进军电动汽车领域，不仅要在国内推出电动汽车，也要让其走向国际。随后2022年6月，Ola Electric 发布了其首款电动汽车。Ola 表示，其目标是一次充满电后行驶超过 500 公里，仅需 4 秒即可完成 0-100 次加速。</p><p>&nbsp;</p><p>虽然Ola Electric的发展一路高歌猛进，但由于处于迅速增长阶段，Ola Electric 的现金消耗异常巨大，亏损不断增加。电动滑板车销售收入是 Ola Electric 的唯一收入来源，而电池销售收入在本财年第一季度仅贡献了一小部分。</p><p>&nbsp;</p><p>在运营方面，Ola Electric也存在其他问题。Ola Electric公司表示，电动汽车所用零部件可能会出现缺陷、质量问题或供应中断或价格上涨，从而增加材料成本和 Ola 电动汽车的价格，这将影响预计的制造和交付时间表，也会从一定程度上削减公司营收。</p><p>&nbsp;</p><p>而此次Ola Electric宣布推出自研AI芯片后，也在印度社交网络上引发热议。看好和看衰两种声音势均力敌。</p><p>&nbsp;</p><p>一些网友认为Ola Electric成立多年并未取得什么重大成果，发布芯片也只为了融资和炒作。</p><p>&nbsp;</p><p>ID名为SaiSS961的用户在Youtube平台Ola Electric 发布芯片视频下方评论称：“这家公司得了数十亿美元的资金，却没有取得什么成果，一切都只是炒作而已。”</p><p>&nbsp;</p><p></p><blockquote>“Ola 只是购买外部技术并将其标榜为自己的技术。就像他迄今为止所做的一切一样，这只是复制粘贴。”</blockquote><p></p><p>&nbsp;</p><p>ID名为arpanshome6328的用户表示：“看起来他很快就厌倦了现有的业务。他没有从任何一项业务中赚取任何利润，就跳入了下一项业务。他们不知道自己想要实现什么。希望投资者的钱是安全的。”</p><p>&nbsp;</p><p>另一位ID为ankittiwari6716的用户也认为Ola Electric 应该专注于主营业务，而不是乱花钱在其他事情上。</p><p>&nbsp;</p><p></p><blockquote>“Ola Electric 最好专注于电动汽车，而不是在有限的预算下做所有事情。这家伙在欺骗无辜的印度人民来抬高他的股票。他的公司从欧洲获得了电动汽车设计，进行了外观改造并将其出售给印度人。他怎么能押注高端芯片制造业务？这是在欺骗所有人。只要和你认识的任何一位前 Ola 员工谈谈就会知道，几乎没人使用 krutrim AI ，甚至班加罗尔的人们更喜欢 Rapido 而不是 Ola。”</blockquote><p></p><p>&nbsp;</p><p></p><h2>印度正大力推动芯片行业发展</h2><p></p><p>&nbsp;</p><p>市场研究与咨询公司MarketsandMarkets 近期对 AI 芯片市场进行了全面分析，预测 2024 年至 2030 年间将出现大幅增长。根据他们的报告，全球 AI 芯片市场预计到 2030 年将达到 930 亿美元，复合年增长率为 25.6%。这一增长是由汽车、医疗保健和金融等各个领域越来越多地采用 AI 技术推动的。</p><p>&nbsp;</p><p>这块巨大的蛋糕早已被印度盯上。作为印度向全球领先经济体转型的一部分，印度总理莫迪曾设定了一个目标，即到 2029 年，印度将从几乎一无所有的基础发展成为全球五大计算机芯片制造商之一。</p><p>&nbsp;</p><p>伊利诺伊大学香槟分校的拉凯什·库马尔 (Rakesh Kumar) 表示，各国寻求半导体自给自足的主要驱动力有两个。第一个是，在疫情最严重时期，芯片短缺引发了人们的担忧，人们已经意识到了芯片现在对一个国家的安全和工业的重要性；第二个是，各国希望在这个庞大且不断增长的行业中分一杯羹，去年全球芯片行业市场总价值为5269 亿美元。</p><p>&nbsp;</p><p>作为世界上人口最多的国家，印度拥有大量科技人才，善加利用无疑能够推动其AI技术的发展。而由于英伟达和阿斯麦尔等多家企业被禁止向中国出售其尖端技术，这些厂商也许很乐意为自家产品在印度开辟新的市场空间。</p><p>&nbsp;</p><p>这一举措对印度的技术格局而言是重大的一步，有望减少对外国芯片制造商的依赖，并促进国内人工智能硬件的创新。如果成功，Ola 的芯片系列将使印度成为全球人工智能硬件市场的关键参与者。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href="https://theaiinsider.tech/2024/08/16/ola-unveils-ambitious-plan-for-indias-first-ai-chip-family/">https://theaiinsider.tech/2024/08/16/ola-unveils-ambitious-plan-for-indias-first-ai-chip-family/</a>"</p><p><a href="https://iotworldmagazine.com/2024/08/15/2360/a-review-of-top-ai-chips-market-share-report-2024-2030-in-the-uk-europe-asia-and-india">https://iotworldmagazine.com/2024/08/15/2360/a-review-of-top-ai-chips-market-share-report-2024-2030-in-the-uk-europe-asia-and-india</a>"</p><p><a href="https://www.cnbc.com/2024/08/09/ola-electric-shares-rise-20percent-in-india-ipo-valuing-firm-at-4point8-billion.html">https://www.cnbc.com/2024/08/09/ola-electric-shares-rise-20percent-in-india-ipo-valuing-firm-at-4point8-billion.html</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/O49yVYCK9ufG2d4q4QhP</id>
            <title>探索安全边界：出海合规与大模型实践 | QCon</title>
            <link>https://www.infoq.cn/article/O49yVYCK9ufG2d4q4QhP</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/O49yVYCK9ufG2d4q4QhP</guid>
            <pubDate></pubDate>
            <updated>Thu, 22 Aug 2024 03:10:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>进入 2024 年以来，企业在全球商业舞台上迎来了两个显著的潮流：业务出海和大模型技术的广泛应用。业务出海已成为企业扩大市场版图、提升全球竞争力的关键战略。企业不再局限于本土市场，而是积极向国际领域迈进，探索新的增长机遇和市场空间。这一过程中，如何确保合规与安全，成为企业出海战略中不可忽视的一环。同时，大模型技术的运用正深刻影响着企业的运营模式和创新方向，安全人员也开始考虑利用大模型技术优化安全解决方案，提升安全防护效率，减少安全风险。</p><p></p><p>面对不断演进的趋势，这些都是重点关注和丞待解决的挑战。在即将于 10 月 18 -19 日召开的 QCon 上海站，我们策划了【<a href="https://qcon.infoq.cn/2024/shanghai/track/1717">探索安全边界：出海合规与大模型实践</a>"】专场，将邀请不同公司的数据安全合规专家，分享他们在各自的业务场景中的出海合规实践经验，以及借助大模型助力应用场景落地，实现运营效率和效果的双重提升的方法。目前是 <a href="https://qcon.infoq.cn/2024/shanghai/apply">8 折购票</a>"最后优惠期，感兴趣的同学前往了解。</p><p></p><h3>精彩演讲抢先看</h3><p></p><p></p><p><a href="https://qcon.infoq.cn/2024/shanghai/presentation/6026">演讲主题：大模型在商业敏感数据中的分类分级实践</a>"</p><p></p><p>讲师：刘明 （携程信息安全部数据安全总监）</p><p></p><p>演讲摘要</p><p></p><p>分类分级是数据安全的一项基础性工作，高效准确地对数据进行分类分级打标，是后续进行有效分级保护控制的先决条件，以往分类分级工作中存在专家特征规则识别质量局限，人工识别耗时费力跟不上业务变化的问题，随着大语言模型的兴起，我们看到了新的机会，也取得了不错的可行性验证成果，本次演讲将分享携程逐步实现高质量、高效率的敏感数据分类分级的思路和经验，希望能给听众带来一些启发和思考。</p><p></p><p>演讲提纲</p><p></p><p>1. 背景</p><p></p><p>目前数据分类分级手段在商密数据场景的局限大语言模型能带来的潜在提升与场景适配研究</p><p></p><p>2. 分类分级可行性验证过程</p><p></p><p>找到正确的提问方式大语言模型分类分级过程中遇到的问题：模型幻觉、数据安全性</p><p></p><p>3. 在控制成本的基础上找到平衡应用落地方式</p><p></p><p>在现有分类分级产品中引入大语言模型能力分类分级准确率和召回率数据情况</p><p></p><p>4. 挑战与展望</p><p></p><p>分类分级大模型的常态运营</p><p></p><p>实践痛点</p><p></p><p>大语言模型存在幻觉问题，偶发性会出现脱离框架的答案，同时大语言模型使用成本目前相较专家特征规则和自建模型没有优势，因此成本控制问题会限制其应用规模。</p><p></p><p>演讲亮点</p><p></p><p>商业秘密数据特征模糊，使用专家特征规则无法进行有效识别，而大语言模型可充分利用上下文和常识知识更加准确的识别分类商业秘密数据。</p><p></p><p>听众收益</p><p></p><p>了解携程在使用大语言模型进行商密数据分类分级的实践经验了解大语言模型在数据安全分类分级基础性工作上提升效率质量的价值</p><p></p><p><a href="https://qcon.infoq.cn/2024/shanghai/presentation/6111">演讲主题：百度基于大模型安全运营的质效提升实践</a>"</p><p></p><p>讲师：包沉浮（百度杰出架构师，安全技术委员会主席 ）</p><p></p><p>演讲摘要</p><p></p><p>百度作为一家业务复杂的大型互联网企业，同时又是关键基础设施，随着网络安全威胁的日益加剧，传统的安全运营手段在效率和效果上都面临巨大挑战。本次分享将介绍百度如何基于大模型构建深度安全推理智能体框架，实现运营效率和效果的双重提升，并展示包括告警自动研判和漏洞事件分析在内的实践经验，希望能给听众带来一些大模型安全领域应用最佳实践的启示。</p><p></p><p>演讲提纲</p><p></p><p>1. 背景和挑战</p><p></p><p>大模型开始逐步应用于安全运营场景百度安全运营面临的双效（效率 + 效果）提升需求</p><p></p><p>2. 架构设计</p><p></p><p>‍设计目标：基于深度安全推理智能体框架，实现双效提升设计考虑：人机协同的工作流设计（运营流程梳理、质量标准定义、人机交互模式）、模型能力边界与拓展（模型结果可信度和可解释性、知识和工具依赖）、实施成本整体‍架构（自底向上）：底座模型的知识补充RAG、CoT、Function calling流程编排智能体 Review 机制</p><p></p><p>3. 实践案例</p><p></p><p>告警自动 / 辅助研判 + 事件处置漏洞事件自动分析 + 处置</p><p></p><p>4. 未来展望</p><p></p><p>大模型原生的安全运营中心</p><p></p><p>实践痛点</p><p></p><p>明确目标，围绕安全运营场景的风险偏好，制定更贴合实际的落地目标，避免直接盲目追求大而全的零职守无人干预以数据驱动能力迭代，缺少可用数据时应当从实际场景中提升标准化和自动化水平，引入业务的数据活水，避免直接使用脱离业务的合成数据</p><p></p><p>演讲亮点</p><p></p><p>从架构设计层面剖析安全运营场景双效提升应遵循的必要准则，提供构建深度安全推理智能体框架的完整视角细粒度展现告警研判、漏洞分析处置等实际场景的双效提升最佳实践</p><p></p><p>听众收益</p><p></p><p>了解互联网大厂的安全运营需求痛点与大模型实践经验了解规模化且对效果要求较高的安全运营场景下，大模型智能体设计考虑与整体架构</p><p></p><p><a href="https://qcon.infoq.cn/2024/shanghai/presentation/6064">演讲主题：安全大模型的最后一公里实践：智能决策与自动响应</a>"</p><p></p><p>讲师：傅奎（雾帜智能联合创始人 &amp; CTO）</p><p></p><p>演讲摘要</p><p></p><p>主流安全大模型及应用场景侧重于非结构化数据的整理、总结、分析和建议，但还缺少最后一步——如何让大模型参与安全响应的决策，并在决策后自动化完成动作的执行。本议题将介绍，安全专家如何借助大模型，自动生成网络安全响应流程（安全剧本），并自动完成剧本的执行，由此在安全运营场景最后一公里完成大模型应用场景落地。</p><p></p><p>演讲提纲</p><p></p><p>1. 大模型在网络安全领域应用</p><p></p><p>发布 SecGPT 的安全厂商大模型在安全领域的应用场景共性不足（重分析，轻决策）</p><p></p><p>2. 安全大模型在智能决策领域应用探索</p><p></p><p>模型是否有能力给出合理建议如何让模型给出更高质量的决策模型决策结果的潜在风险</p><p></p><p>3. 安全大模型实战应用实践案例</p><p></p><p>OWASP TOP 10 典型场景大模型在 Web 攻击攻击领域的应用效果降低模型决策风险的实践思路</p><p></p><p>4. 未来展望</p><p></p><p>让模型设计剧本 VS 让模型选择剧本大模型落地安全最后一公里（能力调度）如何实现终极目标：零值守无人安全运营中心</p><p></p><p>实践痛点</p><p></p><p>针对特定性的安全事件，如何设计响应策略人工智能设计的安全策略是否可以实现全自动执行距离真正零值守还有哪些问题没有解决</p><p></p><p>演讲亮点</p><p></p><p>不仅仅使用大模型对安全事件做分析，还通过安全大模型对安全事件响应作出决策，安全大模型完全决策，并最终付诸实施通过安全能力实现安全策略的落地，该环节减少对人工的依赖，减少对安全专家的依赖，是未来零值守安全运营中心的重要基础。</p><p></p><p>听众收益</p><p></p><p>传统安全运营的场景，痛点和困境有别于安全大厂的安全运营智能化实践安全大模型最后一公里所解决的问题和价值</p><p></p><p><a href="https://qcon.infoq.cn/2024/shanghai/presentation/6044">演讲主题：全球视野下的合规之道：携程海外数据安全管理实践</a>"</p><p></p><p>讲师：胡立平（携程数据安全合规负责人）</p><p></p><p>演讲摘要</p><p></p><p>出海成为众多国内企业实现业绩新增长曲线的选择，然而随着数据的重要性提升，法律及监管关注度也在增强，携程作为在线旅行行业较早布局海外业务的企业，在海外数据安全合规风险上也有所积累。本次演讲将分享携程海外数据安全合规风险管理的思路和经验，希望能给相关出海企业企业带来一些合规实践上的启示。</p><p></p><p>演讲提纲</p><p></p><p>出海面临的数据安全合规挑战</p><p></p><p>法律法规近些年主要变化及监管挑战从数据视角深度剖析出海合规风险携程应对策略及实践携程的海外合规整体策略设计如何通过 GRC 平台形成风险管理闭环如何保障旗下 Trip.com 产品的隐私合规</p><p></p><p>海外数据安全合规未来展望和应对思考</p><p></p><p>实践痛点</p><p></p><p>合规风险管理线上化需要建立在标准化的风险管理、优秀的产品设计、合理的内部运营流程等基础上，才能实现控制域的完备性、控制方法的准确性、关键控制的有效性、审计覆盖的充分性等关键指标。</p><p></p><p>演讲亮点</p><p></p><p>结合合规实战介绍部分法域的合规挑战介绍携程自研 GRC(Governance, Risk and Compliance ) 平台如何融合监管情报、外规内化、审计整改等多个治理环节，解决出海过程中面临多法域、多品牌的风险管理挑战</p><p></p><p>听众收益</p><p></p><p>帮助了解现有海外数据安全相关合规的整体风险态势帮助了解标准化及线上化在多法域数据安全合规风险管理中的价值</p><p></p><p><a href="https://qcon.infoq.cn/2024/shanghai/presentation/6094">演讲主题：跨国经营中的企业数据合规之道</a>"</p><p></p><p>讲师：陈晓芳（ vivo 数据合规专家）</p><p></p><p>演讲摘要</p><p></p><p>各大企业，尤其是跨国企业，由于业务拓展或人力管理等因素，不可避免地会涉及数据出境。与此同时，数据出境相关监管规范和管理机制日益完善，对企业管控数据出境行为提出了新的挑战。本次演讲将分享 vivo 在数据出境管理方面的实践经验，希望能给相关出海企业带来一些合规启示。</p><p></p><p>演讲提纲</p><p></p><p>监管框架：解析数据出境合规路径</p><p></p><p>数据出境的立法背景和监管趋势数据出境的三种合规路径：安全评估、标准合同备案（SCC）、个人信息保护认证探讨网信办规定的数据出境豁免条件及其适用性</p><p></p><p>2. 企业视角：企业如何管控数据跨境</p><p></p><p>介绍企业如何建立数据出境监控和排查机制阐述企业在数据出境过程中的合规流程和全周期管理措施</p><p></p><p>3. 实战案例：vivo 的数据出境安全评估</p><p></p><p>分享 vivo 申报数据出境安全评估的经验，着重于省网信办材料审核的重点，打回材料的理由等真实案例</p><p></p><p>4. 数据出境合规未来的挑战与展望</p><p></p><p>分析数据出境合规管理中的主要困难，提供相应应对策略和建议</p><p></p><p>实践痛点</p><p></p><p>系统的数据出境风险管理体系，需要完备全面的数据出境监测体系、精确的风险触发机制和合理的出境备案应对方案，才能实现企业数据出境的全面管控。</p><p></p><p>演讲亮点</p><p></p><p>vivo 在应对数据出境合规挑战过程中的管控措施，以及申报安全评估时的经验。</p><p></p><p>听众收益</p><p></p><p>帮助了解数据出境方向整体的合规风险及应对措施了解 vivo 在数据出境管控方面的措施及安全评估申报过程中的相关经验</p><p></p><p>更多精彩内容将在 10 月 18 - 19 日 QCon 上海站为您现场呈现，期待与您共赴这场技术之约。如果您有好的技术实践案例想要与我们分享，欢迎点击<a href="https://jsj.top/f/EbrZFg">链接</a>"提交演讲申请。</p><p></p><p>【会议推荐】</p><p></p><p>InfoQ 将于 10 月 18-19 日在上海举办 <a href="https://qcon.infoq.cn/2024/shanghai/">QCon 全球软件开发大会</a>" ，覆盖前后端 / 算法工程师、技术管理者、创业者、投资人等泛开发者群体，内容涵盖当下热点（AI Agent、AI Infra、RAG 等）和传统经典（架构、稳定性、云原生等），侧重实操性和可借鉴性。现在大会已开始正式报名，可以享受 8 折优惠，单张门票立省 960 元（原价 4800 元），详情可联系票务经理 &nbsp;17310043226 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/df/dfd31ee989a7951439a77fec138d4cf8.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/kLvNa2lBEouaaN0LlgyU</id>
            <title>不要掉入“AI 工程就是一切”的陷阱</title>
            <link>https://www.infoq.cn/article/kLvNa2lBEouaaN0LlgyU</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/kLvNa2lBEouaaN0LlgyU</guid>
            <pubDate></pubDate>
            <updated>Thu, 22 Aug 2024 02:37:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>常有人错误地将这样一句话归因于一些领导者，尽管它可能完全是虚构的：“外行谈论战略和战术，内行关注运营。”从战术的角度看，我们面对的是一系列独特的问题，从运营角度，我们看到的是需要解决的组织功能失调模式。在战略视角看到的是机会，在运营视角看到的是需要应对的挑战。</p><p></p><p>在本系列文章的第一部分，我们介绍了 <a href="https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247619519&amp;idx=2&amp;sn=a468ebed03640e82c4a4d3e14d8434c6&amp;scene=21#wechat_redirect">LLM 的战术性操作</a>"。接下来，我们将拓宽视野，深入探讨长期的战略规划。在这一部分，我们将讨论构建 LLM 应用程序的运营层面，这些应用程序是战略与战术的桥梁，将理论与实际应用紧密结合。</p><p></p><p>在运营 LLM 应用程序过程中，我们遇到了一些似曾相识的问题，这些问题在传统软件系统的运营中也常常出现，不同的是它们也带来了一些新的挑战，使得探索过程充满了趣味。此外，运营 LLM 应用程序还带来了一些全新的问题。我们将这些问题及其答案归纳为四个部分：数据、模型、产品和人。</p><p></p><p>对于数据，我们将探讨这几个问题：如何以及多久需要重新审视一次 LLM 的输入和输出？如何测量并有效减少测试环境与生产环境之间的偏差？</p><p></p><p>对于模型，我们将探讨这几个问题：如何将语言模型集成到现有的技术栈中？如何看待模型的版本控制以及如何在不同模型和版本之间进行平滑迁移？</p><p></p><p>对于产品，我们将探讨这几个问题：设计应该在何时介入应用程序的开发过程，为什么要“尽早介入”？如何设计能够充分吸纳人类反馈的用户体验？在面对相互冲突的需求时如何安排优先级？如何校准产品风险？</p><p></p><p>最后，对于人，我们将探讨这几个问题：选择哪些人才来构建成功的 LLM 应用程序，以及何时招募他们？如何培养正确的实验性文化？如何利用现有的 LLM 应用程序来辅助开发自己的 LLM 解决方案？哪一个更关键：流程还是工具？</p><p></p><p></p><h3>运营：LLM 应用程序的构建和开发团队</h3><p></p><p></p><p></p><h4>数据</h4><p></p><p></p><p>正如精选的食材能够成就一道佳肴，高质量的输入数据同样对机器学习系统的表现起着决定性作用。此外，系统的输出是评估其是否正常工作的唯一方式。所有人都紧密关注数据，他们每周都会花几个小时细致地分析输入和输出，以便更好地理解数据分布：模式、边缘情况以及模型的局限性。</p><p></p><p></p><h4>检查开发与生产偏差</h4><p></p><p></p><p>在传统机器学习流程中存在的一个普遍问题是训练与服务之间的偏差。这种情况通常发生在模型训练时使用的数据与模型在实际应用中遇到的数据不一致时。尽管我们可以无需训练或微调就能够使用 LLM，从而避免了训练集的问题，但开发与生产环境之间的数据偏差问题依然存在。关键在于，在开发阶段测试系统时所用的数据应与系统在生产环境中实际面对的数据相一致。如果不是这样的话，我们可能会发现生产环境中的模型准确性会受影响。</p><p></p><p>LLM 开发与生产偏差可以分为两种类型：结构性偏差和基于内容的偏差。结构性偏差包括格式不一致，比如 JSON 字典与 JSON 列表之间的差异、不一致的大小写以及错误，如错别字或不完整的句子片段。这些错误可能导致模型性能不可预测，因为不同的 LLM 是基于特定的数据格式训练的，而提示词对微小变化都非常敏感。基于内容的偏差（或“语义”偏差）指的是数据的含义或上下文的差异。</p><p></p><p>正如传统的机器学习一样，对 LLM 的输入和输出进行定期的偏差检测是非常有必要的。输入和输出的长度或特定格式要求（例如，JSON 或 XML）等指标是跟踪变化最直接的方式。对于更“高级”的漂移检测，可以采用更高级的方法，如聚类输入 / 输出对的嵌入向量可用于检测语义漂移：如果用户讨论的主题发生变化，这可能表明他们正在探索模型以前没有接触过的领域。</p><p></p><p>在测试变更时，例如提示词工程，确保保留数据集是最新的，并且能够反映用户交互的最新类型。例如，如果错别字在生产环境的输入中很常见，那么它们也应该出现在保留数据中。除了进行数值偏差检查之外，对输出进行定性评估也很有用的。定期检查模型输出——俗称“氛围检查”——可以确保结果符合预期并满足用户需求。最后，将非确定性纳入偏差检查中——通过多次运行测试数据集中的每个输入并分析所有输出，可以增加捕捉那些可能仅偶尔发生异常情况的可能性。</p><p></p><p></p><h4>每天检查 LLM 的输入和输出样本</h4><p></p><p></p><p>LLM 是动态且持续进化的。尽管它们具有令人印象深刻的零样本学习能力，并且经常能够生成令人满意的输出，但它们的失败模式却非常难以预测。对于自定义任务，定期审查数据样本有助于培养对 LLM 性能的直观理解。</p><p></p><p>生产环境的输入输出对是 LLM 应用程序的“现场证据”，它们不会被替换。最近的研究表明，开发者对什么构成“好”和“坏”输出的看法会随着他们与更多数据的交互而发生变化（即所谓的标准漂移）。虽然开发者可以预先设定一些标准来评估 LLM 输出，但这些预定义的标准通常不够全面。例如，在开发过程中，我们可能会更新提示词，以增加获得良好响应的概率，并降低获得不良响应的概率。这种评估、重新评估和标准更新的迭代过程是必不可少的，因为在没有直接观察输出的情况下，很难预测 LLM 的行为或人类的偏好。</p><p></p><p>为了有效地管理大型语言模型，我们需要记录 LLM 的输入和输出。通过每天检查这些日志样本，我们能够及时识别并适应新的模式或故障模式。在发现新问题时，我们可以立即编写断言或制定评估策略来应对这些问题。同样，对故障模式定义的更新都应实时反映在评估标准中。这些“氛围检查”可以帮助我们捕捉到不良输出的信号，而通过编写代码和断言，我们能够将这些检查操作化，使之成为可执行的过程。最后，这种态度需要在团队中得到普及，例如通过在值班轮换中加入对输入和输出的审查或注释环节。</p><p></p><p></p><h3>调用模型</h3><p></p><p></p><p>在使用 LLM API 时，我们确实可以依靠少数几家技术供应商的智能成果。虽然这为我们提供了便利，但同时也带来了一些权衡，包括性能、延迟、吞吐量和成本等方面。此外，随着更新、更好的模型（在过去一年中几乎每个月都会有新模型发布）的发布，我们需要随时准备好更新我们的产品，以弃用旧模型并迁移到新模型。在这一章节，我们将分享在使用这些我们不能完全控制的技术时的经验，特别是关于如何管理那些我们无法自托管的模型。</p><p></p><p></p><h4>生成结构化输出，简化下游集成</h4><p></p><p></p><p>对于大多数现实世界的场景，LLM 的输出需要通过机器可读的格式提供给下游应用程序。例如，Rechat，一个房地产 CRM 系统，需要结构化的响应来在前端显示小部件。同样，Boba，一个用于生成产品策略想法的工具，需要输出包含标题、摘要、可信度得分和时间范围字段的结构化信息。LinkedIn 通过限制 LLM 生成 YAML 格式的数据，用于决定使用哪种”技能“，并提供调用这些技能所需的参数。</p><p></p><p>这种应用模式体现了 Postel 定律的极致：在接收时宽容（接受任意自然语言），在发送时保守（输出类型化、机器可读的对象）。因此，我们期望这种方法具有很高的稳定性和可靠性。</p><p></p><p>目前，Instructor 和 Outlines 是从 LLM 中提取结构化输出的实际标准。如果你在使用 LLM API（比如 Anthropic 或 OpenAI），请优先选择 Instructor；而如果你在使用自托管的模型（例如 Hugging Face），则推荐使用 Outlines。</p><p></p><p>为不同模型修改提示词是一种痛苦</p><p></p><p>有时，我们精心编写的提示词在一种模型上表现出色，但在另一种模型上却表现平平。这种情况可能在我们更换不同模型供应商时发生，也可能出现在同一模型的不同版本升级过程中。</p><p></p><p>例如，Voiceflow 在从 gpt-3.5-turbo-0301 迁移到 gpt-3.5-turbo-1106 时，他们的意图分类任务性能下降了 10%。（幸运的是，他们进行了评估！）同样，GoDaddy 注意到了一个积极的变化，升级到 1106 版本缩小了 gpt-3.5-turbo 和 gpt-4 之间的性能差距。（或者，如果你是一个乐观的人，可能会对 gpt-4 的领先优势在这次升级中有所减少感到失望。）</p><p></p><p>因此，如果我们不得不在模型之间迁移提示词，预计这将是一个比简单更换 API 端点更耗时的过程。不要想当然地认为使用相同的提示词能够得到相似或更好的结果。此外，拥有一个可靠的自动化评估系统，可以在迁移前后有效地衡量任务性能，并显著减少所需的手动验证工作。</p><p></p><p></p><h4>版本控制和固定你的模型</h4><p></p><p></p><p>在机器学习管道中，“改变一点，影响全局”是一个普遍现象。这一点在我们依赖自己未参与训练的组件，例如大型语言模型（LLM）时，显得尤为突出，因为这些模型可能会在不被我们察觉的情况下发生变化。</p><p></p><p>幸运的是，许多模型供应商提供“锁定”特定模型版本（例如，gpt-4-turbo-1106）的选项。这样，我们可以使用特定版本的模型权重，确保它们保持不变。在生产环境中锁定模型版本有助于防止模型行为发生意外变化，从而减少因模型更新可能导致的问题（例如过于冗长的输出或其他不可预见的故障模式）。</p><p></p><p>此外，可以考虑维护一个影子管道，这个管道镜像了生成环境的设置，但使用的是最新的模型版本。这为实验和测试新版本提供了一个安全的环境。一旦确认这些新模型的输出在稳定性和质量上符合标准，就可以自信地升级生产环境中的模型版本。</p><p></p><p>选择能够完成任务的最小模型</p><p></p><p>在开发新应用程序时，使用最强大的模型往往具有极大的吸引力。然而，一旦我们确认了技术可行性，就很有必要尝试一下使用更小的模型是否能够产生同样优质的结果。</p><p></p><p>小模型的优势是较低的延迟和成本。虽然在性能上可能略显逊色，但通过诸如思维链、n-shot 提示词和上下文学习等先进技术的应用，它们完全有可能超越自身的限制。除了调用 LLM API，针对特定任务进行微调也能够显著提升性能。</p><p></p><p>综合考虑，一个精心设计的工作流，即使使用较小的模型，通常也能匹敌甚至超越单个大型模型的输出质量，同时还具备更快的处理速度和更低的成本。例如，这个推文分享了 Haiku 结合 10-shot 提示词的表现优于零样本的 Opus 和 GPT-4。从长远来看，我们期望看到更多流程工程的案例，使用较小的模型实现输出质量、响应时间和成本之间的最佳平衡。</p><p></p><p>作为另一个典型案例，我们来看一下那些看似简单的分类任务。轻量级的 DistilBERT（6700 万参数）模型居然是一个出人意料的强大基线。在开源数据上进行微调后，拥有 4 亿参数的 DistilBART 更是一个不错的选择——它在识别幻觉方面的 ROC-AUC 值达到了 0.84，在延迟和成本方面增加不到 5%，超越了大多数大型语言模型。</p><p></p><p>重点是，我们不要轻视那些模较小的模型。尽管人们往往倾向于对各种问题都应用庞大的模型，但通过一些创新思维和实验探索，我们常常能够发现更为高效的解决方案。</p><p></p><p></p><h3>产品</h3><p></p><p></p><p>虽然新技术为我们带来了新的可能性，但构建卓越产品的核心原则始终不变。因此，即使是在第一次面临新挑战时，我们也无需在产品设计方面重新发明轮子。将我们的 LLM 应用程序开发建立在坚实的产品理念之上，这将使我们能够为用户带来真正的价值。。</p><p></p><p></p><h4>及早并频繁地进行设计</h4><p></p><p></p><p>设计师的参与有助于推动你深入思考如何构建和向用户展示产品。我们有时会将设计师简单定义为美化事物的人。然而，除了用户界面之外，他们还会全面思考如何改进用户体验，甚至是打破现有的规则和范式。</p><p></p><p>设计师擅长将用户需求转化为各种各样的形式。这些形式有些更容易实现，而有些则为 AI 技术提供了更多或更少的施展空间。与许多其他产品一样，构建 AI 产品应该以要完成的任务为中心，而不是驱动这些任务的技术。</p><p></p><p>问问自己：“用户期望这个产品为他们完成哪些任务？这些任务是聊天机器人擅长的吗？能够使用自动完成功能？也许可以尝试一些不同的方案！”审视现有的设计模式，思考它们与要完成的任务之间的联系。这些是设计师为团队能力带来的宝贵贡献。</p><p></p><p></p><h4>以 HITL 为导向设计用户体验</h4><p></p><p></p><p>一种提升注释质量的方式是将 Human-in-the-Loop（HITL）融入到用户体验（UX）设计中。通过让用户轻松地提供反馈和更正，我们不仅能即时优化输出，还能收集有洞察力的数据来改进我们的模型。</p><p></p><p>设想一个电子商务平台，用户需要上传并分类他们的商品。我们可以从多个角度来设计用户体验：</p><p></p><p>用户手动选择产品类别；LLM 定期检查新产品并在后端更正分类错误。用户不选择产品类别；LLM 定期在后端对产品进行分类（可能存在错误）。LLM 提供实时产品类别建议，用户可以根据自己的判断进行验证和更新。</p><p></p><p>虽然这三种方法都利用了 LLM，但它们提供了非常不同的 UX。第一种方法将初始责任放在用户身上，并将 LLM 作为后续的辅助。第二种方法减少了用户的负担，但不提供透明度或控制权。第三种方法找到了二者之间的平衡点。LLM 提前建议类别，减少了用户的认知负担，他们无需深入了解复杂的分类体系。同时，用户可以审查和修改这些建议，他们对如何分类产品有最终的决定权，将控制权牢牢掌握在手中。作为一个额外的好处，第三种方法为模型改进创建了一个自然反馈循环。好的建议会被接受（正反馈标签），不好的建议会被更新（负反馈标签转成正反馈标签）。</p><p></p><p>这种建议、用户验证和数据收集的模式在多个应用领域中都得到了广泛应用：</p><p></p><p>编码助手：用户可以接受建议（强烈正反馈）、接受并调整建议（正反馈）或忽略建议（负反馈）。Midjourney：用户可以选择放大并下载图像（强烈正反馈）、修改图像（正反馈）或生成一组新图像（负反馈）。聊天机器人：用户可以对响应点赞（正反馈）或不点赞（负反馈），如果响应真的很差，选择重新生成响应（强烈负反馈）。</p><p></p><p>反馈可以是显式或隐式的。显式反馈是用户对产品提出的意见或评价，隐式反馈是我们需要从用户交互中捕捉的信息，无需用户有意提供。编码助手和 Midjourney 是隐式反馈的例子，而点赞和不点赞是显式反馈。如果我们能够像编码助手和 Midjourney 那样设计 UX，就可以收集到大量的隐式反馈来改进我们的产品和模型。</p><p></p><p></p><h4>调整需求层次的优先级</h4><p></p><p></p><p>在准备将演示转化为实际应用时，我们需要仔细考虑以下几个关键要素：</p><p></p><p>可靠性：确保 99.9% 的正常运行时间，同时遵循结构化输出标准；无害性：避免生成攻击性、NSFW 或其他有害的内容；事实一致性：忠实于提供的上下文，不虚构信息；实用性：与用户的需求和请求相关；可扩展性：延迟 SLA，支持高吞吐量；成本效益：需要考虑预算限制；其他：安全性、隐私保护、公平性、GDPR 合规性、DMA 合规性等。</p><p></p><p>如果我们试图同时解决所有这些要求，我们将永远无法完成产品交付。因此，我们必须进行优先级排序，并且要果断。这意味着我们要清楚哪些是没有商量余地的（例如，可靠性、无害性），没有这些我们的产品就是不可行的。关键在于识别出最基本的产品功能。我们必须接受第一个版本不会完美的事实，并通过不断迭代来改进。</p><p></p><p></p><h4>根据用例校准风险承受能力</h4><p></p><p></p><p>在选择语言模型及其审查标准时，我们需要根据应用场景和目标受众来做出判断。对于那些提供医疗或财务咨询的聊天机器人，我们必须设定极高的安全和准确性标准。因为任何错误或不当的输出都可能造成严重的后果，并且会严重损害用户对我们的信任。然而，对于不那么关键的应用，比如推荐系统，或者那些仅供内部使用的应用程序，如内容分类或摘要，过分严格的要求可能会拖慢开发进度，却不会为提升价值带来太大帮助。</p><p></p><p>这与最近发布的 a16z 报告中的观点相吻合，许多公司在内部 LLM 应用方面比外部应用进展得更快。通过在内部生产力工具中引入 AI，组织可以在更加受控的环境中实现价值，同时学习如何有效地管理风险。然后，随着他们信心的增强，可以逐步扩展到面向客户的应用场景。</p><p></p><p></p><h3>团队与角色</h3><p></p><p></p><p>定义工作职能不是件容易的事，而在这个新兴领域编写工作描述比其他领域更具挑战性。我们决定不再使用交叉工作职能的文氏图或工作描述的建议。相反，我们将引入一个新的职位——AI 工程师——并探讨其在组织中的位置。同时，我们也将讨论团队其他成员的角色以及如何合理分配责任，这至关重要。</p><p></p><p></p><h4>专注于流程，而不是工具</h4><p></p><p></p><p>面对新兴的范式，例如大型语言模型，软件工程师们往往更倾向于采用各种工具。这种偏好有时会导致我们忽视了这些工具本应解决的问题和优化的流程。结果，许多工程师不得不应对由此产生的偶然的复杂性，对团队的长期生产力构成了负面影响。</p><p></p><p>例如，这篇文章讨论了某些工具如何为大型语言模型自动生成提示词。文章认为（在我看来是正确的），那些在没有先理解问题解决方法或流程的情况下使用这些工具的工程师最终会累积不必要的技术债务。</p><p></p><p>除了偶然的复杂性，许多工具还常常存在规格不足的问题。以不断壮大的 LLM 评估工具行业为例，它们提供所谓的“即插即用”的 LLM 评估服务，涵盖毒性、简洁性、语调等通用评估指标。我们发现许多团队在没有深入分析其领域特有的失败模式的情况下，就盲目采纳了这些工具。与此形成鲜明对比的是 EvalGen，它通过深度参与用户的每一个环节——从定义标准到标注数据，再到评估检查——引导用户构建适合特定领域的评估体系。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/73/73d82d6652c837c82745f6d8a7e174e5.png" /></p><p></p><p>Shankar, S. 等人（2024）“谁来验证验证器？将 LLM 辅助评估 LLM 输出与人类偏好对齐”。来源：<a href="https://arxiv.org/abs/2404.12272">https://arxiv.org/abs/2404.12272</a>"</p><p></p><p>EvalGen 引导用户通过遵循最佳实践来制定 LLM 评估标准，即：</p><p></p><p>定义特定领域的测试（通过提示词自动引导）。它们可以是带有代码的断言，或者是采用“LLM 即评委”的形式。强调将测试与人类判断对齐的重要性，使用户能够验证测试是否确实捕捉到了既定的标准。随着系统（如提示词内容等）的变化不断迭代和优化测试标准。</p><p></p><p>EvalGen 为开发人员提供了评估构建过程的框架性理解，而不是将他们限制在特定工具的使用上。我们发现，一旦 AI 工程师获得了这种宏观视角，他们往往会选择采用更简洁的工具，或者根据自己的需求自行开发解决方案。</p><p></p><p>LLM 的组成部分远不止提示词编写和评估，其复杂性无法在此一一列举。关键在于 AI 工程师在采用工具之前要深入理解其背后的流程和原理。</p><p></p><p></p><h4>持续地实验</h4><p></p><p></p><p>机器学习产品与实验密切相关。不仅涉及 A/B 测试、随机对照试验，还包括频繁尝试修改系统的最小组件并进行离线评估。人们热衷于评估的真正原因并非仅仅为了可靠性和信心——而是为了让实验成为可能。你的评估越精确，就能越迅速地进行实验，进而更快地发现系统的最佳配置。</p><p></p><p>尝试采用不同的方法解决同一个问题是一种很常见的做法，因为现在的实验成本很低。收集数据和训练模型的高昂成本已经得到有效控制——提示词工程的成本仅略高于人力投入。确保你的团队成员都掌握了提示词工程的基础知识。这不仅能激发他们进行实验的热情，还能促进组织内部不同观点的交流与碰撞。</p><p></p><p>此外，实验不仅仅是为了探索，而是要学会利用它们。如果你手头有一个新的任务，可以考虑让团队的其他成员从不同的视角来处理它。尝试寻找更高效的方法，探索如思维链或 few-shot 提示词等技术，以提高工作质量。不要让工具限制了你的实验；如果是这样，那就重新构建它们，或者购买新的工具。</p><p></p><p>最后，在产品或项目规划阶段，务必留出足够的时间来构建评估机制并进行多项实验。在考虑工程产品的规格时，为评估过程设定明确的标准。在制定路线图时，不要低估了实验所需的时间。要预见到在生产交付之前，可能需要进行多轮的开发和评估迭代。</p><p></p><p></p><h4>让每个人都能使用新的 AI 技术</h4><p></p><p></p><p>随着生成式 AI 采用率的增加，我们希望整个团队——不仅仅是专家——都能理解并自信地使用这项新技术。没有比亲自实践更好的方式去培养对大型语言模型工作原理的直观理解了，比如它们的响应延迟、故障模式和用户体验。LLM 相对容易使用：你无需编码技能就可以为流程管道提升性能，每个人都可以通过提示词工程和评估做出实质性的贡献。</p><p></p><p>教育是关键环节，可以从提示词工程的基础开始，如利用 n-shot 和思维链等技术，引导模型生成期望的输出。拥有这方面知识的人还可以教授更技术性的内容，例如大型语言模型本质上是自回归的。换句话说，虽然输入可以并行处理，但输出是顺序的。因此，生成延迟更多地取决于输出的长度而非输入的长度——这是在设计用户体验和设定性能预期时需要考虑的一个关键因素。</p><p></p><p>我们还可以提供更多实践和探索的机会，比如举办一次黑客马拉松。虽然让整个团队投入数日时间在探索性项目上看起来成本较高，但最终的成果可能会超出你的预期。我们见证了一个团队通过黑客马拉松，在短短一年内就实现了他们原本计划三年完成的路线图。另一个团队则通过黑客马拉松，引领了一场用户体验的范式转变，这种转变现在因为大型语言模型的加入而成为可能。</p><p></p><p></p><h4>不要掉入“AI 工程就是一切”的陷阱</h4><p></p><p></p><p>随着新职位名称的出现，人们往往容易过分夸大这些角色的能力。这通常会导致在实际工作职责变得逐渐明确时，人们不得不去做一些痛苦的调整。新入行的人和负责招聘的经理可能会夸大声明或抱有不切实际的期望。在过去的十年里，这类显著的例子包括：</p><p></p><p>数据科学家：“在统计学方面比任何软件工程师都强，在软件工程方面比任何统计学家都强的人”机器学习工程师（MLE）：以软件工程为中心的机器学习视角</p><p></p><p>最初，许多人认为数据科学家单枪匹马就能驾驭数据驱动的项目。然而，现实情况已经清晰地表明，为了有效地开发和部署数据产品，数据科学家必须与软件工程师和数据工程师紧密合作。</p><p></p><p>这种误解在 AI 工程师这一新兴角色上再次出现，一些团队误以为 AI 工程师就是他们需要的一切。实际上，构建机器学习或 AI 产品需要一个由多种专业角色 组成的团队。我们与十多家公司就 AI 产品进行了深入咨询，发现他们普遍都陷入了认为“AI 工程就是一切”的陷阱。这种认知导致产品往往难以越过演示阶段，因为公司忽视了构建产品所涉及的关键方面。</p><p></p><p>例如，评估和度量对于将产品从单一的领域检查阶段扩展到广泛应用阶段来说至关重要。有效的评估能力与机器学习工程师通常所具备的优势相辅相成——一个完全由 AI 工程师组成的团队可能缺乏这些技能。Hamel Husain 在他最近的研究中强调了这些技能的重要性，包括监测数据漂移和制定针对特定领域的评估标准。</p><p></p><p>以下是在构建 AI 产品的过程中你需要的不同类型角色，以及他们在项目各个阶段大致的参与时机：</p><p></p><p>首先，专注于构建产品。这个阶段可能涉及 AI 工程师，但并非必须。AI 工程师在快速原型设计和迭代产品方面具有显著的价值（用户体验、数据处理管道等）。随后，通过系统化地收集和分析数据，为产品打下坚实的基础。根据数据的性质和体量，你可能需要平台工程师或数据工程师。你还需要建立查询和分析数据的系统，以便快速定位问题。最后，你将致力于优化 AI 系统。这并不一定涉及训练模型，包括设计评估指标、构建评估系统、执行实验、优化 RAG 检索、调试随机性问题等。机器学习工程师非常擅长这些工作（尽管 AI 工程师也可以通过学习掌握这些技能）。但如果你没有完成前面的基础步骤，招聘机器学习工程师可能并不明智。</p><p></p><p>除此之外，你始终需要一个领域专家。在小型企业，这通常是创始团队的成员；而在大型企业，产品经理也可以担任这一角色。角色的介入时机至关重要。在不恰当的时间（例如，过早让机器学习工程师介入）招聘人员或介入顺序不对，不仅浪费时间和金钱，还会导致频繁的人员更替。此外，在前面两个阶段定期与机器学习工程师沟通（但不全职让他们介入）将有助于公司为未来的成功打下坚实的基础。</p><p></p><p>原文链接：</p><p>https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-ii/</p><p></p><p>声明：本文由 InfoQ 翻译，未经许可禁止转载。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/46068541f35de5382cda6ed9f</id>
            <title>「模型量化技术」可视化指南：A Visual Guide to Quantization</title>
            <link>https://www.infoq.cn/article/46068541f35de5382cda6ed9f</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/46068541f35de5382cda6ed9f</guid>
            <pubDate></pubDate>
            <updated>Thu, 22 Aug 2024 02:37:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><blockquote>编者按：随着大语言模型（LLMs）规模的不断扩大，如何在有限的计算资源下高效部署这些模型成为了一个迫切需要解决的问题。模型量化作为一种有效的模型压缩技术，在保持模型性能的同时大大降低了计算和存储开销，因此广受关注。但对于许多人来说，模型量化的具体原理和实现方法仍然是一个“黑盒”。我们今天为大家带来的这篇文章，通过可视化图示详细解析各种模型量化技术的原理和实现方法，为各位读者提供一个全面且直观的模型量化技术指南。本文旨在帮助各位读者涉猎以下技能领域：理解模型量化技术的基本原理和作用掌握多种模型量化方法及其优缺点学会如何选择合适的量化方法，并根据实际场景进行调整我们分享这篇全面且深入的技术解析，期望各位读者不仅能够理解模型量化的基本原理，还能洞察该领域的最新发展趋势。随着模型量化技术的不断进步，我们有理由相信，未来将会出现更加高效、更轻量级的大语言模型，为 AI 技术的更广泛应用铺平道路。</blockquote><p></p><p></p><p>作者 🕶 | Maarten Grootendorst</p><p></p><p>编译 🐣 | 岳扬</p><p></p><h1>目录🧾</h1><p></p><p>01 第 1 部分：LLMs 存在的“问题”</p><p></p><p>1.1 参数数值（value）的表示方法</p><p></p><p>1.2 内存限制问题</p><p></p><p>02 第 2 部分：模型量化技术简介</p><p></p><p>2.1 常用的数据类型</p><p></p><p>2.1.1 FP16</p><p></p><p>2.1.2 BF16</p><p></p><p>2.1.3 INT8</p><p></p><p>2.2 对称量化 Symmetric Quantization</p><p></p><p>2.3 非对称量化 asymmetric quantization</p><p></p><p>2.4 取值范围的映射与裁剪</p><p></p><p>2.5 校准过程 Calibration</p><p></p><p>2.5.1 权重（和偏置项） Weights (and Biases)</p><p></p><p>2.5.2 激活值</p><p></p><p>03 第 3 部分：Post-Training Quantization</p><p></p><p>3.1 动态量化（Dynamic Quantization）</p><p></p><p>3.2 静态量化（Static Quantization）</p><p></p><p>3.3 探索 4-bit 量化的极限</p><p></p><p>3.3.1 GPTQ</p><p></p><p>3.3.2 GGUF</p><p></p><p>04 第 4 部分：Quantization Aware Training</p><p></p><p>4.1 1-bit LLM 的时代：BitNet</p><p></p><p>4.2 权重的量化 Weight Quantization</p><p></p><p>4.3 激活值的量化 Activation Quantization</p><p></p><p>4.4 反量化过程 Dequantization</p><p></p><p>4.5 所有 LLMs 实际上均为 1.58-bit</p><p></p><p>4.5.1 The Power of 0</p><p></p><p>4.5.2 Quantization 量化过程</p><p></p><p>05 Conclusion</p><p></p><p>Resources</p><p></p><p>文中链接🔗</p><p></p><p>顾名思义，大语言模型（Large Language Models，LLMs）的特点就是庞大，以至于普通的消费级硬件都难以承载。这些模型的参数量级可达数十亿，而且在进行推理时，往往需要依赖拥有大量显存（VRAM）的 GPU 来加快推理速度。</p><p></p><p>鉴于此，越来越多的研究者将目光投向如何通过优化训练方法、使用适配器（adapters）等技术来缩小模型体积。在这一领域，模型量化（quantization）技术成为了一个重要的研究方向。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5b/5bec304c38c40cbad59e1bde74b40389.png" /></p><p></p><p>本篇文章将带领大家深入了解语言模型领域的量化技术，并逐一探讨相关概念，帮助大家建立起对这一领域的直观认识。我们将一起探索不同的量化方法、实际应用场景，以及模型量化技术的基本原理。</p><p></p><p>本文将提供许多图表（visualizations）来帮助各位读者更好地理解和掌握模型量化技术这一概念，希望大家能够直观、深入地理解模型量化技术。</p><p></p><h1>01 第 1 部分：LLMs 存在的“问题”</h1><p></p><p>大语言模型之所以被称为“大”，是因为其参数数量十分之庞大。目前，这类模型的参数数量通常能够达到数十亿之巨（主要是指权重参数（weights）），这样的数据量其存储成本无疑是一笔巨大的开销。</p><p></p><p>在模型的推理过程中，激活值（译者注：activations，神经网络中某个层对输入数据应用激活函数后产生的输出值。）是通过输入数据（input）与模型权重（weights）相乘等一系列步骤来生成的，这些激活值的数据量也可能非常庞大。</p><p></p><p><img src="https://static001.geekbang.org/infoq/87/874c90d9ab6031252d37d67a6c96a99c.png" /></p><p></p><p>因此，我们的目标是找到一种尽可能高效的方式来表达数十亿个参数，以减少存储每个参数所需的空间。</p><p></p><p>在开始对这些参数进行优化之前，我们从最基本的部分入手，先探讨一下参数数值（value）在计算机中最初是如何表示的。</p><p></p><h2>1.1 参数数值（value）的表示方法</h2><p></p><p>在计算机科学中，特定的数值（value）通常都以浮点数的形式来表示，即带有正负号和小数点的数字。</p><p></p><p>这些数值是由 “bits” 组成的，也就是由二进制数字表示。根据 IEEE-754 标准[1]，这些 “bits” 可以用来表示三个不同的部分，从而构成一个完整的数值（value）：符号位、指数部分以及小数部分（也称为尾数）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/eac7a3488b10f273b59c0f8582d0d2fb.png" /></p><p></p><p>这三个部分结合起来，就能根据一组特定的 “bit” 值来计算出一个具体的数值（value）：</p><p></p><p><img src="https://static001.geekbang.org/infoq/89/8910b5c2c918c30a60a5e3f168f529b5.png" /></p><p></p><p>一般来说，用来表示数值（value）的 “bit” 越多，得到的数值（value）精确度就越高：</p><p></p><p><img src="https://static001.geekbang.org/infoq/0a/0a78c3a6c4125a0718f7ec579381c17d.png" /></p><p></p><h2>1.2 内存限制问题</h2><p></p><p>可用的 “bits” 数量越多，所能表示的数值范围就越大。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b6/b691578d8233df9a0b1240945498e402.png" /></p><p></p><p>一个特定的数值表示法能够表示的所有数值的区间被称为动态范围（dynamic range） ，而相邻两个数值之间的间隔则被称为精度（precision） 。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a2afc167e089cb51af4550eddaf9148e.png" /></p><p></p><p>使用这些 “bits” 的一个有趣功能是，我们可以计算出存储一个特定数值（value）所需的设备内存量。由于一个字节（byte）占 8 位（bits），我们可以为大多数浮点表示形式（floating point representation）制定一个基本的计算公式。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bd3bcd4f7e5d455d67c3008bc8bbf8c4.png" /></p><p></p><p></p><blockquote>Note：在实际应用中，模型推理阶段所需的显存（VRAM）量还受到诸多因素的影响，比如模型处理上下文的大小和模型架构设计。</blockquote><p></p><p></p><p>假设我们有一个拥有 700 亿参数的模型。通常情况下，这些模型默认使用 32 位浮点数（常称为全精度）进行表示，仅加载模型就需要 280GB 内存。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5f/5feb84da28fdf626a4d1142fda57782f.png" /></p><p></p><p>因此，尽可能地减少用于表示模型参数的 “bits” 数量（包括模型训练过程中也是如此）是非常有必要的。但是，有一点必须注意，精度的降低往往会导致模型准确性下降。</p><p></p><p>我们的目标是减少用于表示模型参数的 “bits” 数量，同时又不损害模型的准确性…… 这就是模型量化技术的作用所在！</p><p></p><h1>02 第 2 部分：模型量化技术简介</h1><p></p><p>模型量化的核心在于将模型参数的精度从较高的位宽（bit-widths）（例如 32 位浮点数）降低到较低的位宽（bit-widths）（例如 8 位整数）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2e/2e2942d6873011ce1e76a1d9312e11c5.png" /></p><p></p><p>在减少参数的 “bits” 数量时，通常会出现一定的精度损失（即丢失一些数值细节）。</p><p></p><p>为了更直观地说明这种影响，我们可以尝试将任意一张图片仅用 8 种颜色来表示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/3f/3f32efe21ee0931ff0c1f3bd816cc874.png" /></p><p></p><p>该图像基于 Slava Sidorov 的原作[2]进行了修改</p><p></p><p>观察放大区域，我们可以发现它比原始图片看起来更加“粗糙”，因为使用的颜色种类减少了。</p><p></p><p>模型量化的主要目的就是减少表示原始参数所需的 “bits” 数量（在上述案例中即为颜色种类），同时尽可能保留原始参数的精度。</p><p></p><h2>2.1 常用的数据类型</h2><p></p><p>首先，我们来看看一些常见的数据类型，以及它们与 32-bit（全精度（full-precision）或 FP32 ）表示法相比的影响。</p><p></p><h3>2.1.1 FP16</h3><p></p><p>以从 32-bit 转换到 16-bit（半精度或 FP16 ）的浮点数为例：</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fd8835bd8d53bdf7e7391e7bf931225e.png" /></p><p></p><p>可以看到，FP16 的数值范围比 FP32 要窄得多。</p><p></p><h3>2.1.2 BF16</h3><p></p><p>为了保持与原始 FP32 相似的数值范围，引入了 bfloat 16 这一数据类型，它类似于“截断版的FP32”：</p><p></p><p><img src="https://static001.geekbang.org/infoq/46/469cd166116fdca24bde632ede084922.png" /></p><p></p><p>BF16 虽然使用的 “bits” 数量与 FP16 相同，但能表示的数值范围更广，因此在深度学习领域内得到了广泛应用。</p><p></p><h3>2.1.3 INT8</h3><p></p><p>当我们需要再进一步减少 “bits” 的数量时，就到了整数表示法施展身手的领域，而不再是浮点数表示法。例如，从 FP32 转换为仅有 8 bits 的 INT8，其占用的 bits 数量仅仅是原来的四分之一：</p><p></p><p><img src="https://static001.geekbang.org/infoq/c1/c126c6b65515e139ec623c19b568e08c.png" /></p><p></p><p>有些硬件优化了整数运算，因此在这些硬件上整数运算可能会更高效。然而，并不是所有硬件都进行了这样的优化。不过，一般来说，使用较少的 “bits” 数量，计算速度通常会更快一些。</p><p></p><p>每减少一个 bits ，就需要进行一次映射（mapping）操作，将原本的 FP32 表示形式“压缩”到更少的 “bits” 数量。</p><p></p><p>在实际应用中，我们并不需要将 FP32 所表示的全部数值范围 [-3.4e38, 3.4e38] 都映射到 INT8。我们只需找到一种方法，将数据（即模型参数）范围映射到 INT8 即可。</p><p></p><p>常用的压缩（squeezing）和映射（mapping）方法包括对称量化（symmetric quantization）和非对称量化（asymmetric quantization），它们都是线性映射（linear mapping）的不同形式。</p><p></p><p>接下来，我们将探讨一下这些将 FP32 量化为 INT8 的方法。</p><p></p><h2>2.2 对称量化 Symmetric Quantization</h2><p></p><p>在对称量化过程中，原本浮点数的值域会被映射到量化空间（quantized space）中一个以零为中心的对称区间。从前面的例子可以看出，量化前后的值域都是围绕零点对称的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cd/cdd17a97e7873a0cf8e16883d9552697.png" /></p><p></p><p>这就意味着，在浮点数中表示零的值，在量化空间中仍然是正好为零。</p><p></p><p>对称量化（symmetric quantization）有一种经典方法是绝对最大值（absmax，absolute maximum）量化。</p><p></p><p>具体操作时，我们会从一组数值中找出最大的绝对值（α），以此作为线性映射的范围（译者注：从 -α 到 +α）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2d/2db8527e594edc4119a16580463ac83a.png" /></p><p></p><p></p><blockquote>Note：值域 [-127, 127] 代表的是受限制🚫的范围，而 8-bit 整数可以表示的完整范围是[-128, 127]，选择哪种范围取决于所采用的量化方法。</blockquote><p></p><p></p><p>由于这是一种以零为中心的线性映射（linear mapping），所以计算公式相对简单。</p><p></p><p>我们首先根据以下公式计算比例因子（s）：</p><p></p><p>b 是我们想要量化到的字节数（译者注：原文为“Byte”，此处保留原义，译为字节数，译者认为可能为 bits 数量）（这里是 8 ），α 是最大绝对值，</p><p></p><p>接着，我们用这个比例因子 s 来量化输入值 x：</p><p></p><p><img src="https://static001.geekbang.org/infoq/24/240961e13b3e5728e990d753f5c9d7cb.png" /></p><p></p><p>将这些数值代入公式后，我们将得到以下结果：</p><p></p><p><img src="https://static001.geekbang.org/infoq/7a/7a7c60a541213997ad4a360470f488aa.png" /></p><p></p><p>为了恢复原始的 FP32 值，我们可以使用之前计算出的比例因子（s）来对量化后的数值进行反量化（dequantize）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d3/d3d07f698f22b5a221408135478f9017.png" /></p><p></p><p>先量化后再反量化以恢复原始值的过程如下所示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/34/34815a8bd34226bf3f7e97c56c1c7667.png" /></p><p></p><p>我们可以观察到，某些值（如 3.08 和 3.02 ）在量化到 INT8 后，都被分配了相同的值 36。当这些值反量化（dequantize）回 FP32 时，会丢失一些精度，变得无法再区分。</p><p></p><p>这种现象通常被称为量化误差（quantization error） ，我们可以通过比较原始值（original values）和反量化值（dequantized values）之间的差值来计算这个误差。</p><p></p><p><img src="https://static001.geekbang.org/infoq/82/82677db0532256d3058fb219f8be2d6b.png" /></p><p></p><p>一般来说，“bits” 的数量越少，量化误差往往越大。</p><p></p><h2>2.3 非对称量化 asymmetric quantization</h2><p></p><p>与对称量化（symmetric around）不同，非对称量化并不是以零为中心对称的。 它将浮点数范围中的最小值（β）和最大值（α）映射到量化范围（quantized range）的最小值和最大值。</p><p></p><p>我们在此要探讨的方法称为零点量化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/46/464b86629735821898e9f1be4cc30d5c.png" /></p><p></p><p>各位注意到 0 的位置是如何移动的吗？这正是它被称为“非对称量化”的原因。在区间 [-7.59, 10.8] 中，最小值和最大值与零点之间的距离是不相等的。</p><p></p><p>由于零点位置的偏移，我们需要计算 INT8 范围的零点来进行线性映射（linear mapping）。与之前一样，我们还需要计算一个比例因子（s），但这次要使用 INT8 范围（ [-128, 127] ）的两个端点之间的差值。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c6/c6e743c31b1a53698b7ad26cd8782a1a.png" /></p><p></p><p>请注意，由于需要计算 INT8 取值范围中的零点（z）来调整权重，这个过程稍微复杂一些。</p><p></p><p>和之前一样填入公式：</p><p></p><p><img src="https://static001.geekbang.org/infoq/92/92e3f6c54fc905c152d03c71ee1af1c7.png" /></p><p></p><p>要将从 INT8 量化后的数值反量化回 FP32 ，需要使用之前计算的比例因子（s）和零点（z）。</p><p></p><p>除此之外，反量化过程则相对比较直接：</p><p></p><p><img src="https://static001.geekbang.org/infoq/95/95802965a73d91442d99da714f149ba9.png" /></p><p></p><p>当我们将对称量化和非对称量化放在一起对比时，我们可以迅速看出这两种方法之间的差异：</p><p></p><p><img src="https://static001.geekbang.org/infoq/8e/8eae78513534b90caa4d92b0c5080af4.png" /></p><p></p><p></p><blockquote>Note：请注意对称量化（symmetric quantization）以零点为中心的特性，以及非对称量化（asymmetric quantization）存在的零点偏移。</blockquote><p></p><p></p><h2>2.4 取值范围的映射与剪裁</h2><p></p><p>在前文所举的例子中，我们研究了如何将向量中的数值映射到更低的位表示形式（lower-bit representation）中。虽然这样使得向量的全范围都能被映射，但有一个明显的缺点，那就是有离群值（outlier）时不太好处理。</p><p></p><p>假设有一个向量，其值如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/2f/2ff9cbba82e0666a5db066e5ce6af23f.png" /></p><p></p><p>请注意，如果其中一个数值（value）远大于其他所有数值，该数值就可以被视作离群值（outlier）。 如果我们要映射这个向量的全部数值，那么所有较小的数值都将映射到相同的较低位表示，并因此失去它们的独特特性：</p><p></p><p><img src="https://static001.geekbang.org/infoq/08/0857ab4220800c53bd863aaf7f59b1c1.png" /></p><p></p><p>这就是我们之前使用的 absmax 方法。请注意，如果我们不进行剪裁（clipping），非对称量化也会出现这样的问题。</p><p></p><p>另一种选择是裁剪（clip）掉某些数值。裁剪（Clipping）操作会为原始值设定一个不同的动态范围，这样所有离群值都会被映射到相同的值。</p><p></p><p>在下文给出的案例中，如果我们手动将动态范围设置为 [-5, 5] ，所有超出这个范围的数值无论其原始值是多少，都将被映射为 -127 或 127 ：</p><p></p><p><img src="https://static001.geekbang.org/infoq/52/52bf417d28b12186a1fbab60e4c38ecb.png" /></p><p></p><p>这种方法的主要优点是，显著减少了非离群值的量化误差。然而，离群值的量化误差却增加了。</p><p></p><h2>2.5 校准过程 Calibration</h2><p></p><p>在前文的示例中，我展示了一种简单方法 —— 即任意选择一个取值范围 [-5, 5]。这个过程被称为校准（calibration），其目的是找到一个能够包含尽可能多数值（values）的范围，同时尽量减少量化误差（quantization error）。</p><p></p><p>对于不同类型的参数，执行校准步骤的方法并不相同。</p><p></p><h3>2.5.1 权重（和偏置项） Weights (and Biases)</h3><p></p><p>在 LLMs 中，我们可以将权重（weights）和偏置项（Biases）视为预先确定的静态值，因为这些值在运行模型之前就已经确定了。例如，Llama 3 的约 20 GB 文件[3]中大部分都是其权重和偏置项。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c1/c1578dc2cc2c80e84de0f893123c1c05.png" /></p><p></p><p>由于偏置项的数量（以百万计）远少于权重（以数十亿计），偏置项通常被保留在更高的精度（如 INT16 ），而量化的主要工作则集中在权重的处理上。</p><p></p><p>因为权重是静态且已知的，所以对其的量化技术可以有：</p><p></p><p>手动选择输入范围的百分位数优化原始权重和量化权重之间的均方误差（MSE）最小化原始值和量化值之间的熵（KL 散度）</p><p></p><p><img src="https://static001.geekbang.org/infoq/79/79f2fb01e27903e852897f738bd17d70.png" /></p><p></p><p>例如，第一种方法（手动选择输入范围的百分位数）会导致出现与前文我们看到的相似的裁剪（clipping）行为。</p><p></p><h3>2.5.2 激活值</h3><p></p><p>在 LLMs 中， 那些在整个推理过程中持续更新的输入（input）通常被称为“激活值”（activations）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1f/1fb614e36b50e5312f25dbe2e3ba5573.png" /></p><p></p><p>请注意，这些值之所以被称为激活值，是因为它们经常需要经过某些激活函数处理，比如 sigmoid 或 relu。</p><p></p><p>与权重不同，激活值会随着每次输入数据的改变而变化，因此很难对其进行精确量化。</p><p></p><p>由于这些值在每个隐藏层之后都会更新，因此我们只能在输入数据通过模型时才能预测它们在推理过程中的具体数值。</p><p></p><p><img src="https://static001.geekbang.org/infoq/24/24fa106202ae3f693b209d3d55e4e4a6.png" /></p><p></p><p>一般来说，校准权重和激活值的量化方法主要有两种：</p><p></p><p>Post-Training Quantization（PTQ）  — 训练完成后进行量化Quantization Aware Training（QAT）  — 训练/微调过程中同时进行量化</p><p></p><h1>03 第 3 部分：Post-Training Quantization</h1><p></p><p>在众多量化技术中，post-training quantization（PTQ）是最为流行的一种。这种方法是在训练完模型之后对模型的参数（包括权重和激活值）进行量化。</p><p></p><p>对于权重值的量化可以采用对称量化（symmetric quantization） 或非对称量化（asymmetric quantization） 两种方式。</p><p></p><p>至于激活值，由于我们不知道其范围，因此需要通过模型的推理来获取它们的 potential distribution（译者注：指的是在不同的输入数据和模型参数下，激活值可能出现的一系列数值。了解这个分布有助于我们选择一个能够包含大部分激活值范围的量化级别，从而减少量化误差。），然后再进行量化。</p><p></p><p>激活值的量化主要有两种形式：</p><p></p><p>动态量化（Dynamic Quantization）静态量化（Static Quantization）</p><p></p><h2>3.1 动态量化（Dynamic Quantization）</h2><p></p><p>当数据通过隐藏层时，其激活值会被收集起来：</p><p></p><p><img src="https://static001.geekbang.org/infoq/3c/3c275b1ab05e53f145786aefaf994e04.png" /></p><p></p><p>随后，利用这些激活值的分布（distribution of activations）来计算量化输出值所需的零点（z）和比例因子（s）值：</p><p></p><p><img src="https://static001.geekbang.org/infoq/0e/0ee7b2af280ecf602e42f70d2201f749.png" /></p><p></p><p>每次数据通过一个新模型层时，都要重复上述过程。因此，每个模型层都有其独特的 z 值和 s 值，因此也有不同的量化方案。</p><p></p><h2>3.2 静态量化（Static Quantization）</h2><p></p><p>与动态量化不同，静态量化在模型推理过程中不实时计算零点（z）和比例因子（s），而是在模型训练或校准过程中提前计算。</p><p></p><p>为了找到这些值，会使用一个校准数据集，并让模型处理这些数据，以便收集可能的激活值分布（potential distributions）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/52/522fd381b0daca69636a4bb0af5a2468.png" /></p><p></p><p>收集到这些数值后，我们就可以计算出必要的 s 值和 z 值，以便在推理过程中进行量化。</p><p></p><p>在实际推理过程中，s 值和 z 值不需要重新计算，而是被应用于所有激活值，实现全局量化。</p><p></p><p>通常情况下，动态量化技术可能会稍微更精确一些，因为它为每个隐藏层计算一次 s 值和 z 值。不过，由于需要计算这些值，因此可能会增加计算时间。</p><p></p><p>相比之下，静态量化虽然准确度稍低，但由于事先已知用于量化的 s 值和 z 值，因此在推理时更为高效。</p><p></p><h2>3.3 探索 4-bit 量化的极限</h2><p></p><p>将量化位数降至 8-bit 以下是一项艰巨的任务，因为每减少一个 bit，量化误差（quantization error）就会增加。 幸运的是，有几种巧妙的方法可以将量化位数进一步降低到 6-bit、4-bit，甚至 2-bit （不过不建议低于 4-bit ）。</p><p></p><p>接下来将探讨两种在 HuggingFace** 上常用的方法：</p><p></p><p>GPTQ — 全模型在 GPU 上运行。GGUF — 将一部分模型层从 GPU 转移到 CPU 上执行。</p><p></p><h3>3.3.1 GPTQ</h3><p></p><p>GPTQ 无疑是实际应用中最著名的 4-bits 量化方法之一。1</p><p></p><p>它采用非对称量化（asymmetric quantization），并逐层处理，每一层都经过独立处理，然后再继续处理下一层：</p><p></p><p><img src="https://static001.geekbang.org/infoq/89/89b382ce26ee082118e6beeac9250c92.png" /></p><p></p><p>在这个逐层量化的过程中，首先将模型层的权重转换为 Hessian 矩阵（译者注：Hessian 矩阵是二阶偏导数矩阵，用于描述函数在其输入变量上的局部曲率。对于多变量函数，Hessian 矩阵可以帮助我们了解函数在某一点上的凹凸性，以及函数值对输入变量的变化有多敏感。）的逆矩阵。它是模型损失函数的二阶导数，它告诉我们模型输出对每个权重变化的敏感程度。</p><p></p><p>简单来说，该过程展示了模型层中每个权重的重要性（或者说是权重的影响程度）。</p><p></p><p>与 Hessian 矩阵中较小值相关的权重更为重要，因为这些权重的微小变化可能会对模型的性能产生重大影响。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8c/8c2dd2e30c6bb6ac0daf7cb5df10c66f.png" /></p><p></p><p>在 Hessian 矩阵的逆矩阵中，数值越低，权重越 “重要”。</p><p></p><p>接下来，我们对权重矩阵的第一行权重进行量化，再进行反量化：</p><p></p><p><img src="https://static001.geekbang.org/infoq/4a/4a0c400c3c448b95ebd8101d7c06e6c0.png" /></p><p></p><p>通过这一过程，我们可以计算出量化误差 (q)，我们可以用之前计算的 Hessian 矩阵的逆矩阵（h_1）来调整这个误差。</p><p></p><p>换句话说，我们是在根据权重的重要性来构建加权量化误差（weighted-quantization error）：</p><p></p><p><img src="https://static001.geekbang.org/infoq/53/53a34232861680a798cf0c791da8838e.png" /></p><p></p><p>接着，我们将这个加权的量化误差重新分配到该行的其他权重上。这样做可以保持神经网络的整体功能（overall function）和输出（output）不变。</p><p></p><p>例如，如果要对第二个权重（如果它是 0.3（x_2））进行此操作，我们就会将量化误差（q）乘以第二个权重的 Hessian 矩阵的逆矩阵（h_2）加上去。</p><p></p><p><img src="https://static001.geekbang.org/infoq/91/91f9678765a1f1107d700f3b1e42d5ac.png" /></p><p></p><p>我们可以对第一行中的第三个权重进行同样的处理：</p><p></p><p><img src="https://static001.geekbang.org/infoq/15/15e2b1bb9656ce813a1a071b6f7d9f97.png" /></p><p></p><p>重复这个重新分配加权量化误差的过程，直到所有值都被量化。</p><p></p><p>这种方法之所以行之有效，是因为权重之间通常是相互关联的。因此，当一个权重出现量化误差（quantization error）时，与之相关的权重也会相应地更新（通过 Hessian 矩阵的逆矩阵）。</p><p></p><p></p><blockquote>NOTE：本文作者[4]采用了几种技巧来加快计算速度并提高性能，例如在 Hessian 矩阵中添加阻尼因子（dampening factor）、“懒惰批处理（lazy batching）”，以及使用 Cholesky 方法预先计算信息（precomputing information）。我强烈建议各位读者观看这个视频[5]。</blockquote><p></p><p></p><p></p><blockquote>TIP：如果你想要一种可以优化性能和提高推理速度的量化方法，可以查看 EXL2[6] 这个项目。</blockquote><p></p><p></p><h3>3.3.2 GGUF</h3><p></p><p>虽然 GPTQ 是一种在 GPU 上运行完整 LLMs 的最佳模型量化方法，但我们可能很多时候没有这种条件。于是我们可以使用 GGUF 将 LLM 的某些模型层放到到 CPU 上进行处理。2</p><p></p><p>这样，当 VRAM 不足时，就可以同时使用 CPU 和 GPU。</p><p></p><p>量化方法 GGUF 仍不断在更新，并且其性能可能会根据量化位数的不同而有所变化。其基本原理如下：</p><p></p><p>首先，给定模型层的权重被分割成包含一组“子”块的“超级”块（“super” blocks）。</p><p></p><p>我们从这些 blocks 中提取比例因子（s）和 α（α）：</p><p></p><p><img src="https://static001.geekbang.org/infoq/12/1246fede5eb200f62a671dcb1c9d0d01.png" /></p><p></p><p>为了量化给定的“子”块（“sub” block），我们可以使用之前介绍的 absmax 量化方法。这种方法会将给定权重乘以比例因子（s）：</p><p></p><p><img src="https://static001.geekbang.org/infoq/12/12068be791d838e0e804b2052beee053.png" /></p><p></p><p>比例因子是通过“子”块的信息计算出来的，但量化时使用的是“超级”块的信息，后者有自己的比例因子：</p><p></p><p><img src="https://static001.geekbang.org/infoq/4f/4ffb580e47ff3f0e077b5817f169bd7b.png" /></p><p></p><p>这种基于块（blocks）的量化方法使用“超级”块的比例因子（s_super）来量化“子”块的比例因子（s_sub）。</p><p></p><p>每个比例因子的量化级别可能会有所不同，“超级”块的比例因子通常比“子”块的比例因子有更高的精度。</p><p></p><p>为了更直观地理解，观看下图进一步了解这几个量化级别相关信息（ 2-bit、4-bit 和 6-bit ）：</p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ec74f774e44d8550c13fef3072b08a5d.png" /></p><p></p><p></p><blockquote>NOTE：在某些量化方法中，为了保持量化后的模型性能，可能需要一个额外的最小值来调整零点，以确保模型能够正确处理极端值。这个最小值和比例因子一样，都是量化过程中的关键参数，它们需要被正确地量化，以确保量化后的模型能够保持原有的性能。</blockquote><p></p><p></p><p>各位读者可以查看这个 PR[7] ，了解所有量化级别的详细信息。此外，还可以查看这个 PR[8]，获取更多关于使用重要性矩阵（importance matrices）进行量化的信息。</p><p></p><h1>04 第 4 部分：Quantization Aware Training</h1><p></p><p>在第 3 部分中，我们了解到如何在训练完成后对模型进行量化。这种方法的不足之处在于，量化过程并未考虑到实际的训练过程。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a4/a4cd3392c6c382dd611bb49b8742135e.png" /></p><p></p><p>于是 Quantization Aware Training（QAT）就有了用武之地。与训练后使用 post-training quantization（PTQ）技术对模型进行量化不同，QAT 的目标是在训练过程中学习量化过程。</p><p></p><p>QAT 通常比 PTQ 更准确，因为在训练过程中已经考虑了量化。其工作原理如下：</p><p></p><p>在训练过程中，引入所谓的“伪”量化。比如先将权重量化到例如 INT4 等形式，然后将它们反量化回 FP32 ：</p><p></p><p><img src="https://static001.geekbang.org/infoq/d7/d7edb3a5a342b5c3a4f32b501b7d9887.png" /></p><p></p><p>这一过程使得模型在训练阶段进行损失值计算和权重更新时能够考虑到量化误差。</p><p></p><p>QAT 尝试探索损失函数中的“宽”最小值区域，以尽可能减少量化误差，因为“窄”最小值区域往往会导致更大的量化误差。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b18915a9133feb2df4b53cbb1b5b738a.png" /></p><p></p><p>例如，假设我们在反向传播过程（backward pass）中没有考虑量化误差。我们将根据梯度下降法（gradient descent）选择损失值（loss）最小的权重。但是，如果它位于“窄”最小值区域，可能会引入更大的量化误差。</p><p></p><p>相反，如果我们考虑到量化误差，我们将选择在“宽”最小值区域中的不同权重进行更新，量化误差会小得多。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9b9626cf089b44f7f5cf02f111c4d194.png" /></p><p></p><p>因此，虽然 PTQ 在高精度（例如，FP32）下具有较小的损失值，但 QAT 在低精度（例如， INT4 ）下的损失值较小，这正是我们追求的目标。</p><p></p><h2>4.1 1-bit LLM 的时代：BitNet</h2><p></p><p>正如前文所述，将量化位数降低到 4-bit 已经非常小了，但如果我们还要进一步降低呢？</p><p></p><p>这就是 BitNet[9] 的用武之地了，它使用 1-bit 表示模型的权重，每个权重都使用 -1 或 1 表示。3</p><p></p><p>它通过直接将量化过程整合到 Transformer 架构中来实现这一点。</p><p></p><p>Transformer 架构是大多数 LLMs 的基础，它依赖于线性层来处理序列数据，并在模型中执行关键的计算操作：</p><p></p><p><img src="https://static001.geekbang.org/infoq/6f/6f867c6a46703c68e9e06dd6fa9b95d3.png" /></p><p></p><p>这些线性层（linear layers）通常使用更高的精度，如 FP16，它们也是大部分权重所在的地方。</p><p></p><p>BitNet 将这些线性层替换为他们称之为 BitLinear 的模型层：</p><p></p><p><img src="https://static001.geekbang.org/infoq/55/557e9606923e81e470a5df4632cb7947.png" /></p><p></p><p>BitLinear 层的工作原理与普通线性层相同，根据权重（weights）和激活值（activation）的乘积计算输出值（output）。</p><p></p><p>BitLinear 层使用 1-bit 来表示模型的权重，并使用 INT8 来表示激活值：</p><p></p><p><img src="https://static001.geekbang.org/infoq/91/91df839add366efd6e3e2651ffe14330.png" /></p><p></p><p>类似于 Quantization-Aware Training（QAT）技术，BitLinear 层在训练过程中执行一种 “伪” 量化，以便用来分析权重和激活值的量化效果：</p><p></p><p><img src="https://static001.geekbang.org/infoq/10/1007ae8c8f69d93cea8fbd87155b1204.png" /></p><p></p><p></p><blockquote>NOTE：在论文中使用的是 γ 而不是 α ，但由于在本文中所举的例子一直使用 α ，所以我使用 α 。此外，请注意此处的 β 与前文在零点量化（zero-point quantization）中使用的 β 不同，它是基于平均绝对值（average absolute value）计算得出的。</blockquote><p></p><p></p><p>让我们一步一步来学习 BitLinear 。</p><p></p><h2>4.2 权重的量化 Weight Quantization</h2><p></p><p>在训练过程中，权重以 INT8 的形式存储，然后使用一种称为 signum 函数的基本策略，将其量化到 1-bit。</p><p></p><p>这种方法的核心在于，它将权重分布（distribution of weights）重新调整到以 0 为中心，然后将所有小于 0 的值（左侧）设置为 -1 ，将所有大于 0 的值（右侧）设置为 1 ：</p><p></p><p><img src="https://static001.geekbang.org/infoq/a5/a572f0074ff0f4f07f9f2cb8aa4c24f2.png" /></p><p></p><p>此外，它还会跟踪记录一个值 β（平均绝对值（average absolute value）），我们稍后会用到它来进行反量化（dequantization）。</p><p></p><h2>4.3 激活值的量化 Activation Quantization</h2><p></p><p>为了量化激活值，BitLinear 利用 absmax 量化方法将 FP16 格式的激活值转换为 INT8 格式，因为矩阵乘法 (×) 需要更高精度的激活值。</p><p></p><p><img src="https://static001.geekbang.org/infoq/75/759b252e32eeb8e9c45cd165cf5e3a67.png" /></p><p></p><p>同时，它还会跟踪记录 α（最高绝对值），我们将在后续的反量化过程中使用该值。</p><p></p><h2>4.4 反量化过程 Dequantization</h2><p></p><p>我们跟踪记录了 α（激活值的最高绝对值）和 β（权重的平均绝对值），因为这些值将在后续的反量化过程中帮助我们把激活值从 INT8 格式恢复到 FP16 格式。</p><p></p><p>输出激活值（output activations）通过 {α, γ} 进行缩放，然后进行反量化将其恢复到原始精度：</p><p></p><p><img src="https://static001.geekbang.org/infoq/26/26616211bcaeca9ae19b5375d0832b6c.png" /></p><p></p><p>就是这样！这个过程相对简单，只需用两个值（-1 或 1）来表示模型。</p><p></p><p>根据这一流程，作者发现随着模型规模的扩大，1-bit 形式和 FP16 形式训练的模型之间的性能差异逐渐缩小。</p><p></p><p>不过，这只适用于较大型的模型（参数超过 300 亿（30 B）），而对于较小型的模型，这个性能差距仍然很大。</p><p></p><h2>4.5 所有 LLMs 实际上均为 1.58-bit</h2><p></p><p>BitNet 1.58b[10] 就是为了解决之前提到的扩展性问题而提出的。4</p><p></p><p>在这种新方法中，模型的每一个权重不仅可以是 -1 或 1 ，还可以取 0 ，从而成为了一个三元模型。有趣的是，仅仅添加了 0 这一可取值就极大地提升了 BitNet 的性能，并使得计算速度大大提升。</p><p></p><h3>4.5.1 The Power of 0</h3><p></p><p>那么，为什么就添加了一个可取值 0 就能带来如此大的提升呢？</p><p></p><p>这与矩阵乘法的原理紧密相关！</p><p></p><p>首先，让我们了解一下矩阵乘法的一般工作原理。在计算输出值时，我们将权重矩阵（weight matrix）与输入向量（input vector）相乘。下图展示了权重矩阵第一层与输入向量相乘的过程：</p><p></p><p><img src="https://static001.geekbang.org/infoq/81/81c5cbcfe6e18476ba32cf775c3c103a.png" /></p><p></p><p>请注意，这一过程包含两个步骤：首先将每个权重与输入值相乘，然后将所有乘积相加。</p><p></p><p>与此不同，BitNet 1.58b 则省略了乘法这一步骤，因为三元权重（ternary weights）实际上传达了这样的信息：</p><p></p><p>1: 我想要加上这个值0: 我不需要加上这个值-1: 我想要减去这个值</p><p></p><p>因此，当权重量化到 1.58 bit 时，只需要执行加法运算：</p><p></p><p><img src="https://static001.geekbang.org/infoq/55/554c56dfeb18190e341e0bab7fb44995.png" /></p><p></p><p>这样不仅可以大大加快了计算速度，还可以进行特征过滤（feature filtering）。</p><p></p><p>将某个权重设置为 0 后，我们就可以选择忽略它，而不是像 1-bit 表示法那样要么加上要么减去权重。</p><p></p><h3>4.5.2 Quantization 量化过程</h3><p></p><p>在 BitNet 1.58b 中，进行权重量化（weight quantization）时采用了 absmean 量化方法，这是之前看到的 absmax 量化方法的一种改进形式。</p><p></p><p>这种方法通过压缩权重的分布，并利用权重的绝对平均值（α）来进行数值（value）的量化。之后，这些数值会被归整到 -1、0 或 1 ：</p><p></p><p><img src="https://static001.geekbang.org/infoq/7a/7ad4ab53c81e5ba650d48e2384ae1472.png" /></p><p></p><p>相较于 BitNet，激活值的量化过程基本相同，但还是有一点不同。激活值不再被缩放到 [0, 2ᵇ⁻¹] 区间，而是通过 absmax 量化方法被调整到了 [-2ᵇ⁻¹, 2ᵇ⁻¹] 区间。</p><p></p><p>就是这样！1.58-bit 量化主要需要两种技巧：</p><p></p><p>通过添加可取值 0 构建三元数值表示法 [-1, 0, 1]对权重实施 absmean 量化方法。</p><p></p><p>“13B BitNet b1.58 在响应延迟、内存占用和能耗方面，相较于 3B FP16 LLM 更高效。”</p><p></p><p>由于仅需 1.58 个 bits ，计算效率高，我们得以构建出更为轻量的模型！</p><p></p><h1>05 Conclusion</h1><p></p><p>我们的量化之旅到此告一段落！但愿本文能帮助你更深入地认识到量化技术、GPTQ、GGUF 以及 BitNet 的巨大潜力。未来模型的体积又将能够缩小到何种程度？真是令人期待啊！</p><p></p><p>如果要查看更多与 LLMs 相关的可视化内容，并希望支持我们，不妨留意一下我和 Jay Alammar 正在编写的新书。该书即将发行！</p><p></p><p><img src="https://static001.geekbang.org/infoq/00/00fe6928ffcd52e2a95b114cb38e1050.png" /></p><p></p><p>你可以在 O’Reilly 网站[11]上免费试读此书，或者直接在亚马逊[12]上预订。我们还会将所有相关代码同步更新到 Github[13] 上。</p><p></p><h1>Resources</h1><p></p><p>Hopefully, this was an accessible introduction to quantization! If you want to go deeper, I would suggest the following resources:</p><p></p><p>A HuggingFace blog about the LLM.int8()[14] quantization method: you can find the paper here[15]. （译者注：LLM.int8() 量化方法）Another great HuggingFace blog about quantization for embeddings[16].（译者注：嵌入向量的量化问题）A blog about Transformer Math 101[17], describing the basic math related to computation and memory usage for transformers.（译者注：介绍了与 Transformer 的计算和内存使用相关的基本概念）This[18] and this are two nice resources to calculate the (V)RAM you need for a given model.（译者注：计算特定模型所需（V）RAM 的数量）If you want to know more about QLoRA5, a quantization technique for fine-tuning, it is covered extensively in my upcoming book: Hands-On Large Language Models[19].（译者注：QLoRA 技术的学习资料）A truly amazing YouTube video[20] about GPTQ explained incredibly intuitively.（译者注：GPTQ 技术的学习资料）</p><p></p><p>脚注：</p><p></p><p>Frantar, Elias, et al. "Gptq: Accurate post-training quantization for generative pre-trained transformers." arXiv preprint arXiv:2210.17323 (2022).You can find more about GGUF on their GGML repository here[21].Wang, Hongyu, et al. "Bitnet: Scaling 1-bit transformers for large language models." arXiv preprint arXiv:2310.11453 (2023).Ma, Shuming, et al. "The era of 1-bit llms: All large language models are in 1.58 bits." arXiv preprint arXiv:2402.17764 (2024).Dettmers, Tim, et al. "Qlora: Efficient finetuning of quantized llms." Advances in Neural Information Processing Systems 36 (2024).</p><p></p><p>Thanks for reading!</p><p></p><p>Hope you have enjoyed and learned new things from this blog!</p><p></p><p>Maarten Grootendorst</p><p></p><p>Data Scientist | Psychologist | Writer | Open Source Developer (BERTopic, PolyFuzz, KeyBERT) | At the intersection of Artificial Intelligence and Psychology</p><p></p><p>END</p><p></p><h1>🔗文中链接🔗</h1><p></p><p>[1]https://en.wikipedia.org/wiki/IEEE_754</p><p></p><p>[2]https://pixabay.com/users/slava_web-designer-39623293/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=8668140</p><p></p><p>[3]https://huggingface.co/meta-llama/Meta-Llama-3-8B/tree/main</p><p></p><p>[4]https://arxiv.org/pdf/2210.17323</p><p></p><p>[5]https://www.youtube.com/watch?v=mii-xFaPCrA</p><p></p><p>[6]https://github.com/turboderp/exllamav2</p><p></p><p>[7]https://github.com/ggerganov/llama.cpp/pull/1684</p><p></p><p>[8]https://github.com/ggerganov/llama.cpp/pull/4861</p><p></p><p>[9]https://arxiv.org/pdf/2310.11453</p><p></p><p>[10]https://arxiv.org/pdf/2402.17764</p><p></p><p>[11]https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/</p><p></p><p>[12]https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961</p><p></p><p>[13]https://github.com/HandsOnLLM/Hands-On-Large-Language-Models</p><p></p><p>[14]https://huggingface.co/blog/hf-bitsandbytes-integration</p><p></p><p>[15]https://arxiv.org/pdf/2208.07339</p><p></p><p>[16]https://huggingface.co/blog/embedding-quantization</p><p></p><p>[17]https://blog.eleuther.ai/transformer-math/</p><p></p><p>[18]https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator</p><p></p><p>[19]https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961</p><p></p><p>[20]https://www.youtube.com/watch?v=mii-xFaPCrA</p><p></p><p>[21]https://github.com/ggerganov/ggml/blob/master/docs/gguf.md</p><p></p><p>本文经原作者授权，由 Baihai IDP 编译。如需转载译文，请联系获取授权。</p><p></p><p>原文链接：</p><p></p><p>https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/UNmjGDyGK5XBLIAEt7Ui</id>
            <title>揭秘谷歌搜索排名的工作原理</title>
            <link>https://www.infoq.cn/article/UNmjGDyGK5XBLIAEt7Ui</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/UNmjGDyGK5XBLIAEt7Ui</guid>
            <pubDate></pubDate>
            <updated>Thu, 22 Aug 2024 02:29:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>从现有的资料来看，谷歌文档泄露事件与反垄断听证会公开的谷歌搜索排名文件并未直接揭开谷歌搜索排名的全部运作细节。</p><p></p><p>随着机器学习技术的深入应用，有机搜索结果背后的机制变得极其复杂，即便是谷歌内部负责排名算法的专业人士，也难以精确阐述为何某个特定结果会位居榜首或次席。我们尚不清楚这些众多影响因素的具体权重及它们之间错综复杂的相互作用关系。</p><p></p><p>然而，深入理解搜索引擎的整体架构仍然至关重要。这不仅能帮助我们理解为何某些精心优化的网页未能获得高位排名，还能揭示为何一些看似简单且未经刻意优化的结果却能脱颖而出。更为关键的是，这促使我们拓宽视野，重新审视并识别出真正影响排名的核心要素。</p><p></p><p>所有已披露的信息均指向这一点。对于任何关注搜索引擎优化（SEO）的人来说，都应将这些新发现融入自己的思考框架中。这将促使我们以全新的视角审视自己的网站，并在分析、规划与决策过程中引入更多维度的考量标准。</p><p></p><p>坦诚而言，要精确勾勒出这些复杂系统的全貌实属不易。网络上关于此类信息的解读往往存在分歧，即便是讨论同一主题，所用术语也可能大相径庭。</p><p></p><p>举个例子，负责优化搜索结果页面（SERP）布局的系统，在某些谷歌文档中被称为 “Tangram”，而在其他文档中则换上了 “Tetris” 这一名称，这或许是对那款经典游戏的巧妙借喻。</p><p></p><p>经过数周的深入研究，我反复查阅、分析、整理、筛选并重组了近百份相关文档。本文虽非尽善尽美或绝对权威，但确系我基于现有知识与理解，以类似侦探福尔摩斯般的细致精神，竭尽所能完成的成果。呈现在你面前的，便是我个人视角下的探索总结。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/43/43793919af8d3782159dc1e8c537fd44.jpg" /></p><p></p><p>作者创作的谷歌排名工作原理的图解概览</p><p></p><p></p><h3>一份新文档等待谷歌爬虫访问</h3><p></p><p></p><p>当你发布一个新网站时，它并不会立即被谷歌索引。谷歌需要首先发现这个网站的 URL，这通常是通过更新站点地图或是由一个已知 URL 上的链接引导来实现的。</p><p></p><p>对于像首页这样频繁被访问的页面，它们往往会更快地将新链接的信息传递给谷歌。</p><p></p><p>谷歌的 Trawler 系统负责抓取新内容，并跟踪何时重新访问这些 URL 以检查是否有更新。这一过程由调度器精心管理，而存储服务器则负责决定是转发这些 URL 供进一步处理，还是将它们暂时放置在所谓的 “沙盒” 中。尽管谷歌官方否认了沙盒机制的存在，但最近的泄露信息却暗示，那些被怀疑为垃圾或低质量的网站确实有可能被置于这样的环境中进行观察。值得注意的是，谷歌似乎还会转发一些垃圾内容，这可能是为了深入分析，以进一步优化其算法。</p><p></p><p>假设某个文档成功通过了这一系列筛选，那么文档中的外部链接将被提取出来，并被分类为内部链接或外部链接。这些链接信息随后会被其他系统用于进行链接分析和 PageRank 计算（关于这一点，我们稍后会详细阐述）。</p><p></p><p>而对于指向图像的链接，它们则会被专门转发给 ImageBot 进行处理。这个过程有时可能会遇到显著的延迟。ImageBot 会调用这些链接，并将图像与相同或相似的图像一起存储在图像数据库中。此外，Trawler 还会根据它自己的 PageRank 评估结果来调整对网站的抓取频率。简单来说，如果一个网站的访问量较大，那么 Trawler 对它的抓取频率也会相应提高，这被称为 ClientTrafficFraction（客户端流量比例）的影响。</p><p></p><h3>Alexandria：伟大的索引库</h3><p></p><p></p><p>谷歌的索引系统名为 Alexandria，它巧妙地为每一份内容分配一个独一无二的 DocID。若内容已存在于系统中，比如在处理重复内容时，系统不会生成新的 ID，而是会将新发现的 URL 与已存在的 DocID 相关联，实现内容的统一管理和识别。</p><p></p><p>值得注意的是，谷歌严格区分 URL 与文档的概念。一个文档可以涵盖多个 URL，这些 URL 虽然指向不同位置或包含细微差异（如不同语言版本的页面），但只要它们的内容相似且被正确标记，就会被视为同一文档的不同表现形式。同时，来自其他域的 URL 也会在这一体系下被合理分类。所有这些 URL 所携带的信息和信号，都会通过它们所关联的同一个 DocID 来整合处理，确保内容的一致性和准确性。</p><p></p><p>在处理重复内容时，谷歌会精心挑选一个规范版本作为搜索结果的主要展示对象。这也解释了为什么我们有时会看到多个 URL 在搜索结果中排名相近 —— 它们实际上都指向了同一个文档的不同入口。而 “原始”（即规范）URL 的确定并非一成不变，它可能会随着谷歌算法的更新和内容的演变而有所调整。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/30/302a5ea9487c77d751a9ceae87b4f41a.jpg" /></p><p></p><p>图 1: Alexandria 收集文档的 URL</p><p></p><p>由于我们的文档在网络上独一无二，因此被赋予了一个专属的 DocID。</p><p></p><p>网站的不同部分会被搜索引擎细致扫描，寻找相关关键词短语，并将这些信息推送至搜索索引中。在这一过程中，页面上的所有关键词 “亮点”（即 “命中列表”）首先会被送往直接索引，该索引负责整合页面上重复出现的关键词。</p><p></p><p>随后，这些关键词短语会被精心编织进倒排索引的词汇表中。以 “铅笔” 为例，这个词及其所有包含它的关键文档，都已被纳入索引体系之中。</p><p></p><p>简而言之，由于我们的文档中 “铅笔” 一词频繁出现，它现在在词汇索引中占据了 “铅笔” 条目的位置，并与对应的 DocID 紧密相连。</p><p></p><p>与 “铅笔” 相关联的 DocID 会获得一个通过精密算法计算出的 IR（信息检索）分数，该分数将在后续用于搜索结果列表中的排序。值得注意的是，若 “铅笔” 一词在我们的文档中被加粗显示，或位于 H1 标签中（这些信息存储在 AvrTermWeight 中），这些都会作为提升 IR 分数的积极信号。</p><p></p><p>谷歌会将视为重要的文档迁移至其核心存储系统 ——HiveMind，即主存储器。这里融合了高速 SSD 与传统 HDD（称为 TeraGoogle），后者用于长期存储非即时访问的数据。文档和信号都存储在主存储器中。</p><p></p><p>据专家估算，在人工智能热潮兴起之前，全球大约半数的网络服务器均由谷歌托管。这一庞大的互联集群网络，使得数百万个主存储单元能够高效协同工作。甚至有谷歌工程师在会议中提及，理论上，谷歌的主存储器容量足以涵盖整个互联网的信息量。</p><p></p><p>有趣的是，存储在 HiveMind 中的链接，包括反向链接，似乎被赋予了更高的权重。例如，来自权威文档的链接将获得更多重视，而存于 TeraGoogle（HDD）中的 URL 链接则可能权重较低，甚至被忽略不计。</p><p></p><p>提示：为你的文档提供准确且一致的日期信息至关重要。无论是源代码中的日期（BylineDate）、从 URL 和 / 或标题中提取的日期（syntaticDate），还是从内容中解析的日期（semanticDate），都将被综合考虑。随意更改日期以营造时效性的假象可能导致搜索引擎降权处理。lastSignificantUpdate 属性精确记录了文档最后一次重大更新的时间，细微的修改或拼写更正并不会触动这一计数器。</p><p></p><p>每个 DocID 的附加信息与信号都被动态存储在 PerDocData 库中，供多个系统在优化搜索结果相关性时调用。此外，文档的最近 20 个版本都会被保存在历史记录中（通过 CrawlerChangerateURLHistory 实现），使谷歌能够评估并追踪内容随时间的演变。</p><p></p><p>若你计划彻底改变一个文档的内容或主题，理论上需通过创建一系列过渡版本逐步过渡，以覆盖并替换旧的内容信号，这一过程可能需持续发布多达 20 个版本。这解释了为何复活过期域名（即曾活跃后废弃的域名）并不总能带来排名上的优势。</p><p></p><p>当域名的管理权发生变更，同时内容主题也大幅调整时，谷歌系统能够敏锐地捕捉到这些变化，并将所有相关信号重置，使得旧域名在排名上不再享有特殊优待，与全新注册的域名站在同一起跑线上。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b6/b6e6a29aa4abae2a87e3eb9fd413468b.jpg" /></p><p></p><p>图 2：除了泄露的信息外，美国司法部门对谷歌的审判和听证会提供的证据文件也是进行深入研究的宝贵资源。这些文件中还包含了内部电子邮件。</p><p></p><p></p><h3>QBST: 搜索 “铅笔” 的详细过程</h3><p></p><p></p><p>当你在谷歌中输入 “铅笔” 进行搜索时，QBST 系统便立刻启动，开始处理这一请求。系统首先会细致地分析搜索关键词，如果搜索短语由多个词汇组成，这些词汇会被精准地传递到词汇索引中，进行深入的检索。</p><p></p><p>接下来，术语加权过程会登场，这是一个复杂而精密的步骤，它涉及到了 RankBrain、DeepRank（原名 BERT）以及 RankEmbeddedBERT 等多个先进的系统。在这些系统的协同作用下，与 “铅笔” 紧密相关的词汇会被进一步传递给 Ascorer，进行更深层次的处理。</p><p></p><p></p><h3>Ascorer: 构建 “绿色环”</h3><p></p><p></p><p>Ascorer 的工作是从倒排索引中筛选出与 “铅笔” 最相关的前 1000 个文档（DocID），并按照信息检索（IR）评分进行排序。这个排序后的文档列表，我们称之为 “绿色环”，在行业内也被广泛称为发布列表或 posting list。</p><p></p><p>Ascorer 作为 Mustang 排名系统的重要组成部分，还会通过一系列精细的过滤手段，如去重（利用 SimHash 技术）、段落分析以及识别原创和有价值的内容等，对这 1000 个候选文档进行进一步的筛选和优化，最终目的是将这 1000 个候选项精炼成用户眼前所见的 “10 个蓝色链接” 或 “蓝色环”。</p><p></p><p>关于铅笔的文档，在当前的发布列表中排名第 132 位。如果没有其他系统的进一步介入，那么这将是它在搜索结果中的最终位置。</p><p></p><p></p><h3>Superroot: 从千中选优，打造 “蓝色环”</h3><p></p><p></p><p>然而，Superroot 系统并不会让事情就此定格。它的任务是将 “绿色环” 中的 1000 个文档重新排序，通过更加精确和细致的算法，将这庞大的数量精确地缩减到仅包含 10 个结果的 “蓝色环”。</p><p></p><p>在这个过程中，Twiddlers 和 NavBoost 等系统扮演着关键角色，它们负责执行具体的筛选和排序任务。尽管可能还有其他系统也参与其中，但由于信息有限，我们无法一一详述其具体细节。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f1/f1f2d1693f47249dee39d6fceabdf975.jpg" /></p><p></p><p>图 3：Mustang 生成 1,000 个潜在结果，随后由 Superroot 将这些结果筛选至 10 个最终结果。</p><p></p><p>尽管 “谷歌咖啡因（Caffeine）” 这一名称仍被提及，但其最初作为独立系统的形式已不复存在，仅作为历史记忆保留。如今，谷歌构建了一个庞大的微服务架构，这些微服务紧密协作，共同为网页文档生成各种关键属性。这些属性不仅是不同排名和重排系统的核心信号，还助力神经网络模型进行更精准的预测。</p><p></p><p></p><h3>过滤器中的多面手：Twiddler 系统</h3><p></p><p></p><p>当前，谷歌正运用着成百上千个 Twiddler 系统，它们的作用类似于 WordPress 插件，但专注于搜索引擎内部的优化任务。每个 Twiddler 都肩负着特定的过滤使命，这种模块化设计不仅简化了创建过程，还避免了直接干预 Ascorer 中复杂排名算法的必要性，后者一旦修改，可能引发连锁反应，需要周密的规划与编程工作。</p><p></p><p>Twiddler 系统以其灵活性和独立性著称，它们可以并行或顺序工作，彼此间无需知晓对方的操作细节。根据工作特性的不同，Twiddler 大致分为两类：</p><p></p><p>PreDoc Twiddlers：这类 Twiddler 能够高效处理大规模的 DocID 集合，因为它们对额外信息的需求极低，从而在处理初期就能显著缩减发布列表的条目数量，为后续步骤打下基础。“Lazy” 类型 Twiddlers：相比之下，这类 Twiddler 则更为复杂，它们需要额外信息，如从PerDocData数据库中提取的数据，这使得处理过程更加耗时。</p><p></p><p>因此，它们通常在 PreDoc Twiddlers 完成初步筛选后才介入。</p><p></p><p>通过这种分阶段处理策略，谷歌极大地优化了计算资源的利用效率，节省了宝贵的时间。</p><p></p><p>不同的 Twiddler 对文档的最终排名产生着直接或间接的影响。有的 Twiddler 通过调整信息检索（IR）评分来提升或降低文档的排名权重；而另一些则直接干预排名位置。例如，对于新入库的文档，一个专注于提升新文档排名的 Twiddler 可能会将 IR 评分大幅提升 1.7 倍，从而将文档从第 132 位迅速推升至第 81 位。</p><p></p><p>此外，为了提升搜索结果页面（SERP）的多样性，有 Twiddler 会专门降低内容相似文档的权重，这进一步促使我们的铅笔文档排名上升了 12 位，达到第 69 位。更有甚者，一个专门限制特定查询下博客页面数量的 Twiddler，将我们的文档排名进一步提升至第 61 位。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/74/74e92857a206e0000a24659a08aa4577.jpg" /></p><p></p><p>图 4：两种类型的 Twiddler—— 超过 100 个 Twiddler 用于减少潜在搜索结果，并对这些结果进行重新排序。</p><p></p><p>在我们的页面中，CommercialScore属性得到了零分（即被标记为 “是”），这表示 Mustang 系统在分析过程中检测到了销售意图。谷歌可能注意到，“铅笔” 搜索后经常会跟随如 “买铅笔” 这样的具有明确商业购买意图的搜索，这表明用户有交易倾向。因此，一个专门识别并响应此类意图的 Twiddler 会介入，通过添加相关商业结果，将我们的页面排名提升了 20 位，最终排在第 41 位。</p><p></p><p>随后，另一个 Twiddler 启动，实施了所谓的 “页面三惩罚”，旨在将疑似垃圾内容的页面排名限制在搜索结果的前三页之内（即最大排名为第 31 位）。这一限制由BadURL-demoteindex属性控制，该属性为页面排名设定了上限。类似DemoteForContent、DemoteForForwardlinks和DemoteForBacklinks等属性也用于实现内容降级的目的。因此，在排除了我们上方三个被降级的文档后，我们的页面排名进一步上升至第 38 位。</p><p></p><p>尽管我们的文档有可能受到降级的影响，但为了简化讨论，我们假设它未受影响。接下来，我们考虑一个通过评估嵌入内容来判断我们铅笔页面与网站主题相关性的 Twiddler。由于我们的网站专注于书写工具，这一特点对我们极为有利，导致另外 24 个与主题关联度不高的文档受到负面影响。</p><p></p><p>举个例子，假设有一个内容多样化的价格比较网站，其中有一页专门介绍铅笔，虽然内容丰富，但因其主题与网站整体内容大相径庭，该页面可能会因此 Twiddler 而被降级。</p><p></p><p>siteFocusScore和siteRadius等属性反映了页面内容与网站主题的紧密程度。得益于此，我们的信息检索（IR）评分再次获得提升，而其他一些结果则因相关性较低而排名下降，最终我们的页面排名跃升至第 14 位。</p><p></p><p>正如之前所述，Twiddler 的功能极为广泛且灵活。开发人员可以不断尝试新的过滤规则、调整乘数或设置特定的排名限制，甚至能够精确控制某个结果在页面上的具体排列顺序。</p><p></p><p>值得注意的是，一份谷歌内部泄露的文件发出警告，指出某些 Twiddler 功能应由专家谨慎使用，并在与核心搜索团队充分沟通后实施。</p><p></p><p></p><blockquote>“即便你认为自己已经洞悉了这些系统的运作奥秘，相信我，那也只是冰山一角。我们自己也尚未能完全参透。”—— 摘自 泄露的《Twiddler 快速入门指南 – Superroot》文档</blockquote><p></p><p></p><p>此外，还有一类专门的 Twiddler，它们负责创建注释并将这些注释附加到文档 ID（DocID）上，从而在搜索结果页面（SERP）中直观展示。比如，它们可能会在摘要中嵌入图片，或动态调整标题及描述内容，以优化用户体验。</p><p></p><p>如果你在疫情期间好奇为何你所在国家的卫生部门（比如美国的卫生与公共服务部）在 COVID-19 相关搜索中总是稳居榜首，答案很可能就藏在一个特定的 Twiddler 里。这个 Twiddler 通过识别查询语言和国家代码，利用特定的算法提升了官方资源的排名权重。</p><p></p><p>虽然用户对于 Twiddler 如何具体调整搜索结果排序的控制力有限，但了解其工作机制无疑能帮助我们更好地理解排名的波动或那些看似 “难以捉摸” 的排名现象。因此，定期检查 SERP，并留意结果类型的多样性显得尤为重要。</p><p></p><p>举例来说，你是否发现，无论搜索词如何变化，论坛讨论和博客文章的数量在搜索结果中似乎总是保持不变？你可以进一步思考：这些结果中，交易性、信息性或导航性的内容各占多少比例？相同的域名是否会频繁出现在不同但相近的搜索查询结果中？</p><p></p><p>如果你观察到搜索结果中在线商店的数量寥寥无几，那么试图通过类似商店网站来提升排名可能并非明智之举。相反，将重心转向创作更多信息丰富的内容可能更为有效。当然，在做出决策之前，我们还需深入探讨 NavBoost 系统的作用，因为它同样在搜索结果排序中扮演着重要角色。</p><p></p><p></p><h3>谷歌的质量评估员和 RankLab</h3><p></p><p></p><p>谷歌在全球范围内聘请了数千名质量评估员，他们负责审视特定的搜索结果，并在新算法或过滤器正式启用前进行初步测试。谷歌方面澄清：“这些评估结果并不直接决定搜索排名。” 尽管此言非虚，但这些评估在间接层面对排名产生了显著影响。</p><p></p><p>评估员的工作流程大致如下：他们会接收到网址或搜索短语（即待评估的搜索结果），并在移动设备上回答一系列预设问题。例如，他们可能会被问及：“这篇内容的作者是谁？写作时间是什么时候？作者在其领域内是否具备专业知识？” 这些回答随后会被记录下来，作为训练机器学习算法的重要数据。算法通过分析这些数据，能够辨别出哪些页面质量上乘、值得信赖，而哪些则相对逊色。</p><p></p><p>这一机制的核心在于，搜索排名的标准并非由谷歌搜索团队直接设定，而是通过深度学习技术，从人工评估中提炼出模式与规律。为了更直观地理解，我们可以设想一个场景：如果大众普遍认为，包含作者照片、全名及 LinkedIn 个人简介链接的内容更具可信度，那么缺乏这些元素的页面在可信度上自然会大打折扣。当神经网络在训练过程中接触到这些特征及相应的评估结果时，它会将这些特征视为影响排名的关键因素。经过多轮正面验证，通常这一过程会持续至少 30 天，网络可能会开始将这些特征作为重要的排名信号。因此，具备这些特征的页面可能会获得排名上的优势，而缺失这些特征的页面则可能面临排名下降的风险。</p><p></p><p>值得注意的是，尽管谷歌官方可能并未特别强调作者信息的重要性，但泄露的信息显示，如 isAuthor 等属性以及通过 AuthorVectors 实现的 “作者指纹识别” 技术，实际上能够识别并区分出作者独特的语言风格（即个体用词和表达方式）。</p><p></p><p>评估员的反馈会被汇总成 “信息满意度”（IS）评分。尽管参与评估的人数众多，但 IS 评分主要集中应用于少数网址。对于其他具有相似特征的页面，系统会采用外推的方式，利用这些评分来辅助排名决策。谷歌指出：“许多文档可能并未获得大量点击，但它们依然具有重要意义。” 当外推方法不适用时，系统会将相关文档自动提交给评估员进行评分。</p><p></p><p>在提及 “黄金” 一词时，它常与质量评估员相关联，暗示着可能存在某种文档或文档类型的最高标准。可以合理推测，符合评估员期望的文档有可能达到这一黄金标准。此外，某些特定的 Twiddler 可能会为被视为 “黄金” 级别的 DocID（文档标识符）提供显著的排名提升，使其跻身搜索结果的前列。</p><p></p><p>值得一提的是，这些质量评估员往往并非谷歌的全职员工，他们可能通过外部公司参与工作。而谷歌的专家则在 RankLab 中致力于实验与研发，不断推出新的 Twiddler，并评估其是否能有效提升搜索结果的质量，或是仅仅起到过滤垃圾信息的作用。经过严格验证并证明有效的 Twiddler 将被整合到 Mustang 系统中，该系统利用复杂、计算密集型且相互关联的算法，对搜索结果进行精细化的处理与优化。</p><p></p><p></p><h3>但是用户想要什么？</h3><p></p><p></p><p>NavBoost 可以解决这个问题！</p><p></p><p>我们的铅笔文档尚待进一步完善。在 Superroot 系统中，NavBoost 这一核心系统占据了决定搜索结果排序的关键位置。NavBoost 采用 “切片” 技术，以灵活管理移动端、桌面端及本地搜索等多样化的数据集。</p><p></p><p>尽管谷歌官方坚称未将用户点击数据纳入排名考量，但 FTC 文件中一封内部邮件的披露却揭示了点击数据处理过程的保密性要求，这在一定程度上引发了外界遐想。</p><p></p><p>这并不意味着谷歌的做法存在不妥，其否认背后实则蕴含双重考量。首要的是，一旦承认使用点击数据，可能会触发媒体对隐私问题的强烈关注，将谷歌置于 “数据巨头” 的舆论漩涡中，被指责为无孔不入地追踪用户在线行为。然而，实际上，点击数据的运用旨在获取具有统计学意义的信息，以优化搜索体验，而非针对个体用户的监控。尽管数据保护倡导者可能对此持保留意见，但这一解释无疑为谷歌的否认立场提供了合理解释。</p><p></p><p>FTC 文件的记载进一步印证了点击数据在排名中的实际作用，而 NavBoost 系统在此过程中更是频频被提及（仅在 2023 年 4 月 18 日的听证会上就被提及了 54 次）。此外，回溯至 2012 年的官方听证会，也已明确指出了点击数据对搜索排名产生的实际影响。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8d/8d8aa0883eac608f016338c364ff02bd.jpg" /></p><p></p><p>图 5：自 2012 年 8 月以来（！），官方已经明确点击数据会改变排名。</p><p></p><p>研究表明，搜索结果中的用户点击行为以及网站或网页的流量情况都会对其在搜索引擎中的排名产生影响。谷歌能够直接在搜索结果页面（SERP）上监控和评估用户的搜索行为，包括搜索操作、点击选择、重复搜索以及重复点击等行为。</p><p></p><p>有一种观点认为，谷歌可能通过其自家的谷歌分析（Google Analytics）工具来推测域名的流量数据，这导致部分用户选择避免使用该系统。然而，这一观点存在局限性。首先，Google Analytics 并不提供对所有交易数据的全面访问权限，限制了其推测能力的准确性。更为关键的是，由于超过 60% 的用户使用的是谷歌 Chrome 浏览器（其用户数量已超过三亿），谷歌能够收集到海量的网络活动数据。这使得 Chrome 在分析网络动态中扮演着至关重要的角色，这一点在相关听证会上也得到了明确强调。此外，Core Web Vitals 的数据也是通过 Chrome 进行收集的，并最终汇总为 “chromeInTotal” 值，用于评估网站的性能。</p><p></p><p>关于 “监控” 的负面舆论是谷歌否认使用点击数据的一个原因。另一个原因是，担心评估点击和流量数据可能会激励垃圾邮件发送者和骗子使用机器人系统伪造流量，从而试图操控搜索排名。虽然谷歌的这种否认态度可能会让人感到沮丧，但其背后的担忧和理由却是可以理解的。</p><p></p><p>在存储的指标中，包括了 “badClicks”（坏点击）和 “goodClicks”（好点击）等评估标准。这些评估通常会考虑搜索者在目标页面上的停留时间、他们浏览了多少其他页面以及这些页面的浏览时间（这些数据来源于 Chrome）如果搜索者在搜索结果中短暂偏离后又迅速返回并点击了其他结果，这种行为可能会增加 “坏点击” 的数量。而在一个搜索会话中，最后一次被认为是 “好” 点击的搜索结果则会被记录为 “lastLongestClick”（最长点击）。为了确保数据的准确性和防止被操控，这些数据会经过压缩处理以在统计上进行标准化。如果某个页面、一组页面或一个域名的首页通常具有良好的访问指标（这些数据同样来源于 Chrome），那么这将会通过 NavBoost 产生积极效果。通过分析在一个域名内或跨域名的流动模式，甚至可以评估网站导航的用户引导效果。由于谷歌能够监测整个搜索会话过程，因此在极端情况下它甚至可能识别出与搜索查询完全不同的文档也适合该查询。例如如果搜索者在搜索过程中离开了他们最初点击的域名并访问了另一个域名（可能是通过该域名中的链接跳转过去的）并在新域名上停留较长时间那么这个作为搜索 “结束” 的文档在未来就有可能通过 NavBoost 被推到更前面的位置前提是它在选择范围内。当然这需要大量搜索者提供有力的统计信号作为支持。</p><p></p><p>接下来我们来详细分析搜索结果中的点击情况。在每个搜索结果页面（SERP）中不同排名位置的结果都有一个平均预期点击率（CTR）作为性能评估的基准。例如根据 Johannes Beus 在今年柏林 CAMPIXX 会议上的分析结果显示排名第一的自然搜索结果平均可以获得 26.2% 的点击率而排名第二的结果则只能获得 15.5% 的点击率。</p><p></p><p>如果某个搜索结果的实际点击率显著低于预期值那么 NavBoost 系统会记录这一差异并据此调整该结果的排名位置（即 DocID 的排名）。相反如果某个结果的实际点击量在历史上一直明显多于或少于预期值 NavBoost 也会相应地调整该文档的排名位置以确保搜索结果的相关性和准确性（见图 6 所示）。</p><p></p><p>这种方法是合理的因为点击率从本质上反映了用户对搜索结果相关性的评价这些评价又是基于搜索结果的标题、描述以及域名等因素得出的。这一概念在谷歌的官方文档中也有详细说明（如图 7 所示）从而进一步证明了其合理性和科学性。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/fc/fccc5c008e2e60078614ea7859ce5648.jpg" /></p><p></p><p>图 6：如果 “预期 _CRT” 与实际值有显著差异，则排名会相应调整。（数据源：J. Beus，SISTRIX，带编辑覆盖）</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7c/7cbf2c5b548d6b6599fb359411fc4576.jpg" /></p><p></p><p>图 7：谷歌演示文稿中的幻灯片（来源：审判证据 - UPX0228，美国及原告州诉谷歌公司）</p><p></p><p>由于我们的铅笔文档刚刚发布不久，因此目前还缺乏具体的点击率（CTR）数据。对于这类无数据的新文档，系统是否会忽略 CTR 偏差尚不明朗，但从其融入用户反馈的设计初衷来看，这种可能性是存在的。另一种推测是，系统可能会依据其他相关指标对 CTR 进行初步估算，这与谷歌 Ads 中处理质量因子的方式有异曲同工之妙。</p><p></p><p>SEO 专家和数据分析师在长期实践中发现，一旦他们全面监控点击率，便会注意到一个规律：当某个文档首次跻身搜索结果前 10 名，而其实际 CTR 显著低于预期时，其排名往往会在几天内（具体时间取决于搜索频率）出现下滑。相反，若 CTR 远高于预期，则排名有望攀升。面对 CTR 表现不佳的情况，快速调整文档的摘要信息（如优化标题和描述）以吸引更多点击至关重要，否则排名下滑后恢复难度将大幅增加。这一现象被普遍视为系统测试机制的一部分，即文档若表现优异则稳固高位，若不符用户期待则可能被剔除。至于这是否与 NavBoost 系统直接相关，目前尚无确凿证据。</p><p></p><p>根据泄露的信息，谷歌在估算新页面信号时，似乎高度依赖于页面 “环境” 中的海量数据。例如，新页面在初期可能会继承主页的 PageRank（称为 HomePageRank_NS），直至其建立起自己的 PageRank。同时，pnavClicks 可能用于预测通过导航链接到新页面的点击概率。</p><p></p><p>鉴于计算和更新 PageRank 的复杂性及高计算成本，谷歌可能采用了 PageRank_NS 指标作为过渡方案。“NS” 代表 “最近种子”，意味着相关页面共享一个临时的 PageRank 值，该值会根据需要长期或短期地应用于新页面。</p><p></p><p>此外，邻近页面的信号也可能对其他关键指标产生影响，助力新页面在缺乏高流量或反向链接的情况下提升排名。值得注意的是，许多信号的反映并非即时，而是存在一定的延迟。</p><p></p><p>谷歌在听证会上展示了 “新鲜度” 在搜索结果中的实际应用。以搜索 “Stanley Cup” 为例，平时搜索结果多聚焦于这一著名奖杯的介绍，但在斯坦利杯冰球比赛期间，NavBoost 会根据搜索和点击行为的变化，优先展示与比赛紧密相关的信息。这里的 “新鲜度” 并非指文档的新旧，而是指搜索行为和兴趣点的动态变化。谷歌每天处理的搜索行为超过十亿次，每一次搜索和点击都在为谷歌的学习提供宝贵数据。这意味着，谷歌对搜索意图的捕捉和响应远比我们想象的细腻和及时，而非仅仅局限于对季节性变化的简单预测。</p><p></p><p>最新数据显示，文档的点击指标会被存储并评估长达 13 个月之久（每年有一个月的数据与前一年重叠，以便进行对比分析）。鉴于我们的假设域名拥有强大的访问指标和显著的广告直接流量，作为知名品牌（这是一个正面信号），我们的新 “铅笔” 文档自然能够从前期的成功页面中获益。因此，NavBoost 系统成功将我们的排名从第 14 位提升至第 5 位，使我们跻身 “蓝色环” 或前 10 名之列。这前 10 名的文档将与其他九个自然搜索结果一同被转发至谷歌的网络服务器。</p><p></p><p>值得注意的是，谷歌实际提供的个性化搜索结果并不像人们普遍预期的那样丰富。测试表明，通过模拟用户行为并进行相应调整往往能带来更优化的搜索结果，而非单纯依赖于评估个别用户的偏好。这一发现极具启示意义 —— 神经网络的预测能力已经超越了我们的个人浏览和点击历史记录所能提供的个性化程度。当然，对于特定偏好（如对视频内容的喜好），个性化搜索结果仍会予以体现。</p><p></p><p></p><h3>谷歌网络服务器：一切终结与新开始的地方</h3><p></p><p></p><p>谷歌网络服务器（GWS）是构建和呈现搜索结果页面（SERP）的核心，这个页面上包含了诸多元素：十个蓝色链接的自然搜索结果、广告、图片、谷歌地图视图、“人们也在问” 板块等。</p><p></p><p>为了优化这些元素在有限页面空间内的布局，谷歌采用了 Tangram 系统。该系统负责计算每个元素所需的空间大小，并智能决定在给定的 “框架” 内能容纳多少结果。紧接着，Glue 系统会将这些元素精确无误地安置到它们应有的位置上，确保页面既美观又高效。</p><p></p><p>目前，我们的 “铅笔” 文档在自然搜索结果中排名第五，但值得注意的是，CookBook 系统拥有在搜索结果展示前的最后一刻进行微调的能力。这个系统内部集成了 FreshnessNode、InstantGlue（能在 24 小时内快速反应，但通常会有约 10 分钟的延迟）和 InstantNavBoost 等组件。这些组件如同 “幕后英雄”，在最终页面呈现之前，迅速生成与搜索结果时效性紧密相关的信号，并可能据此对排名进行动态调整。</p><p></p><p>想象一下这样的场景：一档关于 Faber-Castell 品牌 250 周年纪念以及 “铅笔” 这一关键词的德国电视节目突然热播。在节目播出的几分钟内，成千上万的观众可能会迅速拿起他们的智能手机或平板电脑进行搜索。这时，FreshnessNode 便会敏锐地捕捉到 “铅笔” 搜索量的激增，并智能地分析出用户的搜索意图是寻求信息而非直接购买。基于这一判断，系统会相应地调整搜索结果的排名。</p><p></p><p>具体来说，InstantNavBoost 会立即采取行动，将所有与交易相关的结果暂时移除，转而用更加信息丰富、与当前热点紧密相关的结果来替代。同时，InstantGlue 也会迅速更新 “蓝色环” 内的结果排序，导致我们原本可能以销售为导向的文档因为不够相关而被更合适的结果挤出前列。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7e/7e35991f5946e6ec8885cf5c4483905c.jpg" /></p><p></p><p>图 8：一档关于 “铅笔” 一词起源的电视节目，以庆祝德国知名铅笔制造商 Faber-Castell 成立 250 周年。</p><p></p><p>尽管我们假设的排名故事以遗憾暂告段落，但它深刻揭示了一个核心真理：获得并维持高排名，绝非仅凭出色的文档或高效的 SEO 策略就能一蹴而就。</p><p></p><p>排名是一个多因素交织的复杂结果，它受到搜索行为波动、新文档信号的融入以及外部环境不断变化等多重影响。因此，认识到高质量文档与优化的 SEO 策略仅是排名动态系统中的一环，且至关重要，这一点尤为重要。</p><p></p><p>搜索结果的生成过程犹如精密的机械运作，背后涉及数以千计的信号和复杂算法。SearchLab 通过 Twiddler 进行的实时测试，甚至可能间接影响到文档的反向链接权重，从而引发连锁反应。</p><p></p><p>这些文档的命运可能因此发生转折，它们可能被从 HiveMind 这一核心存储系统迁移到优先级较低的存储层级，如 SSD 或 TeraGoogle，这一变动将直接削弱或消除它们对排名的正面影响，即便文档内容本身并未有丝毫改动。</p><p></p><p>谷歌的 John Mueller 曾明确指出，排名的下滑并不总是意味着你的策略有误。用户行为模式的转变、新兴趋势的涌现或是其他外部因素，都可能成为影响排名表现的不确定因素。</p><p></p><p>比如，当用户开始倾向于寻求更详尽的信息或偏好简短明了的文本时，NavBoost 系统便会迅速响应，调整排名以匹配这些新的搜索偏好。然而，值得注意的是，这种调整并不会触动 Alexandria 系统或 Ascorer 中的 IR 评分，后者更多地是基于文档本身的固有质量进行评估。</p><p></p><p>这一切都向我们传达了一个重要启示：SEO 工作应当置于更广阔的视角下进行审视。如果文档内容与用户的搜索意图存在偏差，那么即便是再精妙的标题优化或内容调整，其效果也会大打折扣。</p><p></p><p>更为关键的是，Twiddler 和 NavBoost 等系统对排名的干预力度，往往超越了传统的页面优化手段，包括页面内、页面上以及页面外的优化措施。一旦这些系统对文档的可见性进行了限制，那么无论我们在页面上如何努力优化，都可能难以扭转乾坤。</p><p></p><p>但请放心，我们的故事并不会就此陷入低谷。关于铅笔的电视节目效应终究只是短暂的喧嚣。随着搜索热度的逐渐退却，FreshnessNode 的临时影响也将烟消云散，我们的排名有望重新回升至第五位。</p><p></p><p>当我们重新开始收集点击数据时，根据 SISTRIX 的 Johannes Beus 的预测，第五位的平均点击率（CTR）大约在 4% 左右。只要我们能够稳定保持这一 CTR 水平，我们就有信心继续稳坐前十的宝座。未来可期，一切都将朝着更好的方向发展。</p><p></p><p>SEO 的关键要点</p><p></p><p>流量来源多元化：确保你的网站流量不仅仅依赖于搜索引擎，而是从多种渠道汇聚而来，包括社交媒体平台等非传统渠道，这些都能带来宝贵的访问量。即便谷歌的爬虫无法触及某些页面，它依然能通过 Chrome 浏览器或直接 URL 追踪到你的网站访客数量。强化品牌与域名认知：不断提升你的品牌或域名知名度至关重要。品牌越为人熟知，用户在搜索结果中点击你网站的几率就越大。通过优化针对多种长尾关键词的排名，可以有效提升域名的可见度。据透露，“站点权威性” 可能是影响排名的一个关键因素，因此增强品牌声誉对提升搜索排名大有裨益。深入理解搜索意图：为了更好地满足访客需求，深刻理解他们的搜索意图及路径至关重要。利用 Semrush、SimilarWeb 等工具分析访客来源及其行为，审视这些域名是否提供了你页面所缺失的信息，并据此逐步补充，使你的网站成为访客搜索路径上的 “终极目的地”。谷歌能够追踪相关搜索会话，精准把握搜索者的需求与历史。优化标题与描述，提升点击率：审视并调整当前标题与描述的吸引力，通过大写关键词汇使其在视觉上更为突出，可能有助于提高点击率。标题在决定页面排名中扮演关键角色，因此应优先考虑其优化。评估隐藏内容效果：若采用手风琴等形式隐藏重要内容，需留意这些页面的跳出率是否偏高。当访问者无法迅速定位所需信息，需多次点击时，可能产生负面点击信号。精简无效页面：对于长期无人问津或排名不佳的页面，应考虑删除，以避免对邻近页面造成不利影响。新文档若发布在 “劣质” 页面群组中，其表现机会将大打折扣。“deltaPageQuality” 指标用于衡量域名或页面集群中单个文档的质量差异。优化页面布局：清晰的页面结构、流畅的导航以及令人印象深刻的首页设计，对于跻身排名前列至关重要，这往往得益于 NavBoost 等系统的助力。增强用户互动：延长访客在网站上的停留时间，能发出积极的域名信号，惠及所有子页面。致力于成为访客的 “一站式” 信息源，提供全面信息，减少其他搜索需求。深化而非泛化内容：更新并丰富现有内容往往比不断创建新内容更为有效。“ContentEffortScore” 评估文档创作难度，高质量图片、视频、工具及独特内容均对此有正面贡献。标题与内容一致：确保标题准确概括后续内容，利用文本向量化等先进技术进行主题分析，较单纯词汇匹配更为精准地判断标题与内容的一致性。利用网页分析工具：借助谷歌 Analytics 等工具，有效追踪访客互动情况，及时发现问题并予以解决。特别关注跳出率，若异常偏高，需深入调查原因并采取措施改善。谷歌通过 Chrome 浏览器获取这些数据，实现深度分析。聚焦低竞争关键词：初期可优先针对竞争较小的关键词进行优化，更易于建立正面用户信号。构建高质量反向链接：重视来自 HiveMind 中最新或高流量页面的链接，因其传递的信号价值更高。避免链接至流量稀少或参与度低的页面。同时，来自同国别且内容相关的反向链接更具优势。警惕 “有毒” 反向链接，以免损害评分。关注链接上下文：在评估链接价值时，不仅要考虑锚文本本身，还需关注其前后文本的自然流畅性。避免使用 “点击这里” 等通用短语，因其效果已被证实不佳。理性看待 Disavow 工具：该工具用于屏蔽不良链接，但据泄露信息显示，它并未被算法直接采用，更多用于文档管理和反垃圾邮件工作。强调作者专业性：若使用作者引用功能，应确保其在外界享有良好声誉并具备专业知识。少数高资质作者往往优于众多低信誉作者。谷歌能根据作者的专业知识评估内容质量，区分专家与非专家。创作独特、实用、全面的内容：对关键页面尤为重要，展现你的专业深度，并提供有力证据支持。尽管可以聘请外部人员填充内容，但若缺乏实质质量和专业知识支撑，则难以企及高排名目标。</p><p></p><p>原文链接：</p><p></p><p><a href="https://searchengineland.com/how-google-search-ranking-works-445141">https://searchengineland.com/how-google-search-ranking-works-445141</a>"</p><p></p><p>声明：本文为 InfoQ 翻译，未经许可禁止转载。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ba2e971f73b94be4959ea71a1</id>
            <title>AICon 全球人工智能开发与应用大会参会有感</title>
            <link>https://www.infoq.cn/article/ba2e971f73b94be4959ea71a1</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ba2e971f73b94be4959ea71a1</guid>
            <pubDate></pubDate>
            <updated>Wed, 21 Aug 2024 14:58:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><h1>目录</h1><p></p><p>引言大会背景大会议程参会体验会后感想结束语</p><p></p><h1>引言</h1><p></p><p></p><blockquote>在数字化浪潮席卷全球的今天，人工智能开发与应用已成为推动社会进步和产业升级的关键力量。作为一名对AI技术非常感兴趣的开发者，在 8 月 18 日至 19 日这两天，我有幸参加了由极客邦科技旗下 InfoQ 中国主办的 AICon 全球人工智能开发与应用大会，个人觉得这场盛会就如同科技领域的璀璨星辰，吸引了众多行业精英和技术开发者齐聚上海，共同见证人工智能相关的又一盛会。在人工智能的浪潮中， AICon 全球人工智能开发与应用大会无疑是技术领域的一次盛会，在这次大会上我不仅了解到了AI技术的最新发展，还深入了解了AI在各行各业的实际应用。在这篇文章中，将分享我的参会体验和AI技术的前沿动态，以及它如何影响我们的未来。</blockquote><p></p><p><img src="https://static001.geekbang.org/infoq/82/821d9837ddc41f82cdbd724def5f7f49.png" /></p><p></p><h1>大会背景</h1><p></p><p>先来了解一下本次大会的背景， AICon 全球人工智能开发与应用大会是一个专注于AI技术的国际性盛会，是由极客邦科技旗下InfoQ中国主办的技术盛会，旨在为各行业的AI技术爱好者提供一个交流和学习的平台。大会聚集了来自世界各地的AI领域专家、学者、企业家以及开发者，共同探讨AI技术的发展趋势、应用实践和未来挑战。</p><p><img src="https://static001.geekbang.org/infoq/3b/3b729ea10567776ece148fd9ba4162c6.png" /></p><p></p><h1>大会议程</h1><p></p><p>再来分享一下本次大会的议程，大会的议程丰富多样，涵盖了AI领域的多个方面，具体如下所示：</p><p>AI基础理论研究机器学习算法创新深度学习与神经网络自然语言处理（NLP）计算机视觉AI在金融、医疗、教育等行业的应用AI伦理与法规</p><p><img src="https://static001.geekbang.org/infoq/43/43250663d8b5e92ebe4c17ceeeff1679.png" /></p><p></p><h1>参会体验</h1><p></p><p>当我踏入大会现场的那一刻，便被现场的氛围所感染，会场内人头攒动，来自各个领域的技术开发者怀揣着对人工智能的热情和期待，交流着彼此的见解和经验，作为一名开发者，我对AI技术的应用实践和技术创新特别感兴趣。</p><p><img src="https://static001.geekbang.org/infoq/16/16db377a7be1e0c51b810714f6638633.png" /></p><p>大会的第一天，众多顶尖企业与研究机构的资深专家纷纷登台，分享了他们在人工智能领域的最新研究成果和实践经验：</p><p>来自字节跳动的专家深入探讨了人工智能在内容推荐系统中的应用，他们通过巧妙地运用深度学习算法，实现了对用户兴趣的精准预测，从而为用户提供了更加个性化、精准的内容推荐。华为的专家则分享了他们在人工智能与 5G 通信技术融合方面的研究成果。通过利用人工智能的智能优化算法，实现了对 5G 网络资源的高效分配和管理，大大提升了网络的性能和覆盖范围。这让我看到了人工智能在通信领域的广阔应用前景，以及其对推动整个通信行业发展的巨大潜力。阿里巴巴的专家带来了关于人工智能在电商领域的创新应用案例。他们利用图像识别和自然语言处理技术，实现了商品的智能识别和搜索推荐，极大地提高了消费者的购物效率和体验。这使我明白了人工智能在电商行业的深度融合，能够为企业带来显著的竞争优势和业务增长。微软亚洲研究院的专家展示了他们在人工智能基础研究方面的最新突破，特别是在强化学习和生成对抗网络方面的研究成果。这些前沿的研究为人工智能的未来发展提供了坚实的理论基础和技术支持，让我对人工智能的未来充满了信心。智源研究院的专家分享了关于人工智能伦理和社会影响的思考。他们强调了在人工智能快速发展的背景下，我们需要关注技术带来的伦理问题，如算法偏见、数据隐私等，并积极探索相应的解决方案。这让我意识到，人工智能的发展不仅要追求技术的进步，还要注重其对社会和人类的影响，确保技术的发展是有益和可持续的。上海人工智能实验室的专家介绍了他们在城市智能管理方面的应用实践。通过利用人工智能技术，实现了对城市交通、环境、能源等方面的智能监测和优化管理，提升了城市的运行效率和居民的生活质量。这让我看到了人工智能在改善城市生活方面的巨大潜力，以及其对未来智慧城市建设的重要意义。蔚来汽车的专家分享了人工智能在自动驾驶领域的最新进展。他们通过融合多种传感器数据和深度学习算法，实现了车辆的自动驾驶和智能决策，为未来的出行方式带来了革命性的变革。这使我对自动驾驶的未来充满了期待，同时也让我认识到在实现自动驾驶的过程中，还需要解决许多技术和法律方面的挑战。小红书的专家讲述了他们在内容创作和社交互动方面的人工智能应用。通过利用图像生成和自然语言处理技术，为用户提供了更加丰富和有趣的内容创作工具，同时也提升了用户之间的社交互动体验。这让我感受到了人工智能在社交媒体领域的创新应用，以及其对用户参与和内容传播的积极影响。零一万物的专家展示了他们在人工智能芯片研发方面的最新成果。他们研发的高性能人工智能芯片，为人工智能算法的高效运行提供了强大的硬件支持，大大提升了人工智能系统的性能和效率。这让我认识到硬件的创新对于推动人工智能发展的重要性，以及芯片研发在人工智能产业链中的关键地位。</p><p><img src="https://static001.geekbang.org/infoq/df/dfb26b0b6ef3d95ce77c6aa61122e017.png" /></p><p>在第一天的会议中，我不仅了解到了人工智能在各个领域的最新应用和技术突破，还深刻感受到了人工智能技术的快速发展和广泛应用给我们的生活和社会带来的巨大变革。同时，我也意识到在人工智能的发展过程中，我们需要关注技术的伦理和社会影响，确保技术的发展是有益和可持续的。在大会上，我有机会听到了来自业界领袖的精彩演讲，参与了深入的技术研讨会，还与来自不同领域的专家进行了交流，这让我深刻认识到，人工智能不仅能够提升用户体验，还能为企业创造巨大的商业价值。</p><p><img src="https://static001.geekbang.org/infoq/61/61239e1fc2ae2cad5a8257afd22bfd3e.png" /></p><p>大会的第二天，这一天的分享更加侧重于人工智能的产业化和商业化动态，以及在实际落地场景中的挑战和解决方案，同样的，来自不同企业的专家们分享了他们在将人工智能技术从实验室推向市场的过程中所面临的困难和挑战：</p><p>数据质量和数据标注的问题成为了许多企业共同面临的难题。高质量的数据对于训练有效的人工智能模型至关重要，但获取、清洗和标注大量的数据需要耗费大量的时间和资源。此外，模型的可解释性和透明度也是一个亟待解决的问题，尤其是在一些对安全性和可靠性要求较高的领域，如医疗和金融。在产业化方面，一些企业分享了他们如何构建人工智能团队和建立有效的研发流程。他们强调了跨学科合作的重要性，包括数据科学家、工程师、产品经理和业务专家之间的紧密协作。同时，建立敏捷的开发流程和持续的优化机制也是确保项目成功的关键因素。在商业化方面，专家们探讨了如何将人工智能技术转化为实际的商业价值。他们分享了一些成功的案例，如通过人工智能优化供应链管理，降低成本并提高效率；利用人工智能进行精准营销，提高客户满意度和销售额。同时，他们也提到了在商业推广过程中面临的挑战，如客户对新技术的接受程度、法律法规的限制等。</p><p><img src="https://static001.geekbang.org/infoq/44/44993ae13f9e8fa1711dfed9fd9a160a.png" /></p><p>除了主题演讲和案例分享，大会还设置了互动环节和小组讨论。在这些环节中，我有机会与其他参会者深入交流，分享彼此的经验和见解。我结识了许多来自不同行业的技术大佬，与他们的交流让我拓宽了视野，获得了许多新的思路和灵感。</p><p></p><p>尤其是在小组讨论中，我们围绕着“人工智能在医疗领域的应用与挑战”这一话题展开了热烈的讨论。医疗行业对准确性和安全性的要求极高，因此在应用人工智能技术时需要格外谨慎，我们交流了如何确保人工智能诊断系统的准确性和可靠性，如何解决数据隐私问题，以及如何让医疗机构和患者更好地接受和信任人工智能技术。通过参与这些互动环节，我不仅加深了对会议内容的理解，还建立了宝贵的人脉资源。这些人脉将为我未来在人工智能领域的学习和工作提供有力的支持。</p><p><img src="https://static001.geekbang.org/infoq/24/24e09697cc3734c2e7516c60de090c31.png" /></p><p></p><h1>会后感想</h1><p></p><p>回顾这两天的参会经历，我深感收获颇丰，个人觉得AICon 全球人工智能与机器学习技术大会不仅是一个技术交流的平台，更是一个激发创新思维、促进合作的机会，我不仅接触到了最前沿的技术动态，了解到了行业的发展趋势，同时也结识了许多志同道合的朋友。</p><p>人工智能作为引领未来的关键技术，将继续在各个领域发挥重要作用，我相信通过不断的学习和实践，我们能够更好地驾驭这一强大的技术，为人类创造更美好的生活。</p><p>在今后的工作中，我将把在大会上学到的知识和经验应用到实际项目开发中，然后不断探索和创新，而且我也将继续关注人工智能领域的发展动态，积极参与相关的技术交流活动，与其他开发者共同成长，争取在人工智能领域占一席地！</p><p><img src="https://static001.geekbang.org/infoq/50/50c413f2f4bd966b662e60320025949b.png" /></p><p></p><h1>结束语</h1><p></p><p>参加 AICon 全球人工智能开发与应用大会是一次宝贵的学习经历，我不仅学到了对AI技术的新的认识，还对AI的未来发展有了更深的理解。AI技术正以前所未有的速度改变着世界，作为开发者，我们需要不断学习新技术，探索新应用，从而推动AI技术的健康发展。在AI的征途上，我们既是探索者，也是建设者。让我们携手共进，用AI技术创造更美好的未来。最后，我要感谢极客邦科技旗下 InfoQ 中国主办了这样一场精彩的大会，为开发者们提供了如此宝贵的学习和交流机会，非常期待下一次的 AICon 大会能够带来更多惊喜！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/5Jx040lfFB5d1ioIRtK5</id>
            <title>《黑神话：悟空》太狠了！Steam 瘫痪、多家公司放假，英伟达老黄又要躺在新的印钞机上数钱了</title>
            <link>https://www.infoq.cn/article/5Jx040lfFB5d1ioIRtK5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/5Jx040lfFB5d1ioIRtK5</guid>
            <pubDate></pubDate>
            <updated>Wed, 21 Aug 2024 12:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><h1>《黑神话：悟空》太狠了！Steam 瘫痪、多家公司放假，英伟达老黄又要躺在新的印钞机上数钱了</h1><p></p><p><img src="https://static001.geekbang.org/wechat/images/66/66d8817eb95f472a457ad1cc44627f34.png" /></p><p></p><p>作者｜小褚、华卫、冬梅</p><p></p><p></p><h1>挤爆 Steam 服务器，多家公司给员工放假玩游戏</h1><p></p><p></p><p>8 月 20 日，互联网上似乎所有的热搜都集中在了《黑神话：悟空》上线这件事上。</p><p></p><p>上午 10 点，备受期待的游戏《黑神话：悟空》正式上线，然而，一瞬间挤进上百万人后，这款游戏背后的服务器 Steam 不堪重负，遭遇了短暂的崩溃。</p><p></p><p>众多玩家进入游戏时遭遇了阻碍，无法顺利启动游戏。有玩家在社交媒体上调侃称：“服务器爆了，Steam 好久没被干爆过了吧。”幸运的是，这一问题在大约十分钟内得到了解决，玩家得以继续他们的游戏体验。</p><p></p><p>网友纷纷摩拳擦掌时，却遭遇“漫漫解压路”：“八十一难第一难，开始解压”“比下载时间都长”“解压打断了我的大圣梦”……但依旧挡不住大家的热情。</p><p></p><p>据悉，《黑神话：悟空》的上线人数位居 Steam 同时在线人数历史第四位，这一人数仅次于 PUBG、幻兽帕鲁、CSGO。</p><p></p><p>此外，游戏玩家社区平台小黑盒也报告了崩溃情况。许多玩家通过小黑盒购买了《黑神话：悟空》，此次崩溃事件可能对他们的购买体验造成了一定影响。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b4/b4d8cbe00ff2dc1ab411271afdf4cf48.png" /></p><p></p><p>《黑神话：悟空》到底火到什么程度？在游戏上线首日，甚至出现了不少公司给员工放假去体验游戏的情况。</p><p></p><p>四川木子杨科技有限公司 8 月 19 日发布通知，决定在《黑神话：悟空》上线的当天 8 月 20 日给全体员工放假一天，让员工尽情体验《黑神话：悟空》带来的视觉盛宴和游戏乐趣，与同事、朋友一起分享这款国产大作的精彩瞬间。公司表示，这次放假是对国产游戏行业的一份支持。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/0f/0ff0c0cf10df7f6ed900cd7e1ce9a13c.png" /></p><p></p><p>游戏发行商 Gamera Game 宣布 8 月 20 日放假，公司还表示，“为了避免各位同事因临时暂停工作而感到不知所措，将送给每位同事一份《黑神话：悟空》数字豪华版”。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b4/b4ff5439e76c0872e760b1c61ab1816b.png" /></p><p></p><p>行业媒体“游戏茶馆”也公告，为让员工们更好地体验《黑神话：悟空》，决定 8 月 20 日放假半天，同时为大家提供一份报销该游戏费用的机会。</p><p></p><p>“作为苦逼打工人，也是靠加班赚来的年假 + 病假凑足的请假时间来体验《黑神话：悟空》，我的初体验是‘绝对值得’”有网友评价道。“要不是我现在已经是一个成熟稳重分得清轻重缓急的成年人，我现在也去玩游戏了！”还有网友提到。正式上线后的《黑神话：悟空》好评如潮。</p><p></p><p>那么，这款游戏为何能火到如此程度？</p><p></p><p></p><h1>《黑神话：悟空》为啥这么火？</h1><p></p><p></p><p>4 年前的今天，也是 8 月 20 日，《黑神话：悟空》发出第一条宣传预告，自此之后这一游戏的热度便一直居高不下。据 Steam 商店的官方介绍，《黑神话：悟空》是由游戏科学制作的以中国神话为背景的动作角色扮演游戏，游戏中玩家将扮演一位“天命人”，为了探寻昔日传说的真相，踏上一条充满危险与惊奇的西游之路。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8b/8bdfef422204c751f50d7ebaa5dba588.png" /></p><p></p><p>今年 6 月 10 日，《黑神话：悟空》实体版正式开启全款预售。Steam 中国大陆区标准版售价 268 元，数字豪华版售价 328 元。目前这款游戏的全球媒体评分已解禁，IGN 中国给予 10 分评价、IGN 海外给予 8 分评价。截至 8 月 17 日凌晨，52 家全球媒体平均给出了 82 分的评价。</p><p></p><p>而论《黑神话：悟空》获得如此高市场反响的原因，首先不得不提的是西游记的 IP 加成，基于西游记而产生的影视作品层出不穷，最近一部的《大圣归来》也唤起了大家记忆中那个踏碎凌霄的齐天大圣，孙悟空更是许多人童年时候心目中的英雄。</p><p></p><p>其次便是同行的“衬托”了。《黑神话：悟空》并不是第一个做西游记 IP 的游戏，此前市面上有不少同类型的游戏，但其仅从宣传片画面便赢得了众多玩家对其的期待和青睐。据介绍，《黑神话：悟空》游戏里的取景，几乎都有现实存在的原型，能够让玩家感受到从游戏到现实的无缝切换。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/18/1863774968b2642250c7fb3d7e63bcff.gif" /></p><p></p><p>更重要的是，《黑神话：悟空》寄托了许多游戏玩家对第一个国产 3A 大作的希望。中国的游戏单机市场寂寥已久，一众玩家对于国内游戏的宽容度不低。有专业博主分析认为：“国人渴求中国文化背景的游戏，平心而论尽管国外一些游戏的制作水平比《黑神话：悟空》高，但它们的情感冲击绝对没有它强。”</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8f/8f01473d8992120c88551acf3beb6761.gif" /></p><p></p><p>此外，自首个宣传片放出之日起，《黑神话：悟空》便不断对外同步演示视频，从开场动画到游戏表现形式、人物动作流畅度，玩家们都见证着其在这 4 年中的进步。</p><p></p><p>有市场分析显示，《黑神话：悟空》的预售表现超出预期，一个月内销量达到 120 万份，销售额近 4 亿元，最后的收入肯定更远不止如此。随着《黑神话：悟空》的终极 PV 发布，玩家纷纷喊话，“这次我要做自己的齐天大圣。”</p><p></p><p></p><h1>强大的英伟达 AI 作支撑</h1><p></p><p></p><p>《黑神话：悟空》基于虚幻引擎 5，并采用新的 RTX 技术。在 PC 端，《黑神话：悟空》的视觉效果经过全景光线追踪技术的增强，成为迄今为止发布的沉浸感更强、技术更先进的游戏之一。</p><p></p><p>为了迎接这款游戏到来，NVIDIA 英伟达上周宣布为《黑神话：悟空》推出 Game Ready 驱动，这是 NVIDIA 首次为中国游戏做专属优化。此次 Game Ready 驱动重点是优化 RTX 40 系列的性能。</p><p></p><p>全景光线追踪技术对硬件要求更高，但可以高度准确地渲染光线及其在场景中的效果。这种先进的光线追踪技术也被称为路径追踪，视觉效果艺术家们可利用它打造以假乱真的电影和电视画面。《黑神话：悟空》中的全景光线追踪技术提高了光照、反射和阴影的保真度和质量。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/cb/cb57c378272f3c49a7ea25b06ac1cfc2.png" /></p><p></p><p>借助多次反射光线追踪间接照明，自然色彩光线可反射多达两次，营造出更逼真的间接光照和遮蔽效果。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/45/456d955536ab06bc197a9a92f84a40c4.png" /></p><p></p><p>为了提高质量并增强沉浸感，特效通常包含大量的单体粒子，而使用传统光线追踪方法会对性能带来巨大的压力。《黑神话：悟空》采用一种新技术，使用两级光线追踪为大量面片系统进行画序无关的透明渲染，从而在实时反射中高效地渲染游戏的粒子系统。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/44/44821f2a0b7af3641af8abcf9efdbc9c.png" /></p><p></p><p>要获得此效果，玩家可以在游戏中开启“Full Ray Tracing (全景光线追踪) ”设置，然后将“Full Ray Tracing Level (全景光线追踪水平)”设置为“Very High"。</p><p></p><p>NVIDIA DLSS 是英伟达的 AI 渲染技术，可通过 GeForce RTX GPU 上的专用 Tensor Core AI 处理器提高游戏和应用的图形性能。</p><p></p><p>在《黑神话：悟空》中，通过支持全景光线追踪技术并将每项设置为最大值，DLSS 3 可带来性能的成倍提升。这使得 GeForce RTX 4080 SUPER 的用户能够在《黑神话：悟空 》的基准测试中达到每秒近 74 帧的帧率。此外，GeForce RTX 4070 Ti SUPER 的用户可以在每秒 66 帧的帧率下享受 4K 的乐趣。</p><p></p><p>此前，官方公布的推荐配置显示，玩家在 1080P 高画质下需要至少 RTX 2060 或 RX 5700XT 级别的显卡，而中画质则可选择 GTX 1060 或 RX 580。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b9/b91017444854a18dc9b73e2b95f3b8f2.png" /></p><p></p><p>值得注意的是，这些配置均是在启用了 DLSS/FSR/Xess 等图形优化技术的前提下给出的。若玩家希望体验原生 1080P/ 高画质的游戏效果，可能需要具备更高性能的显卡，如 RTX 3060 及以上。目前已经有玩家表示，3060 使用起来还会有卡顿出现。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/55/5506b4d3713356c2c6d8a6bd02db2719.png" /></p><p></p><p>此外，微星、技嘉、七彩虹、索泰、影驰、映众、万丽、耕升等企业陆续推出了与《黑神话：悟空》联名的显卡产品。</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>