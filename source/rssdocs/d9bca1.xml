<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/YcV5XvSriI5kPOkxCYuW</id>
            <title>大模型之争深水期，企业如何真正实现产业级落地？</title>
            <link>https://www.infoq.cn/article/YcV5XvSriI5kPOkxCYuW</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/YcV5XvSriI5kPOkxCYuW</guid>
            <pubDate></pubDate>
            <updated>Sat, 28 Sep 2024 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>大模型之争，已经进入了深层阶段。</p><p></p><p>随着"百模大战"逐渐平息，整个模型市场步入了一个更为冷静的时期。但表面的宁静下，实则暗流涌动，竞争已经从单一的模型能力转向了更为复杂的生态系统建设。真正的较量在于谁能将模型的力量转化为实际应用，谁能为企业解锁更大的价值。</p><p></p><p>在这场转型的浪潮中，企业面临的挑战是全方位的，从垂直模型的开发到模型的调用，再到应用的开发，每一步都充满了难题。成本、效率，这些关键词在当下的环境中变得尤为突出。企业对于模型的需求不断增长，而大模型所蕴含的潜力，正在推动着生产力的革新。</p><p></p><p>在中国，随着国内大模型行业的快速发展，我们已经迈过了早期的混战阶段。现在，随着国内企业的数智化转型需求日益迫切，大模型的落地应用正成为行业发展的新焦点。各行各业对于模型的需求呈现出爆炸性的增长。</p><p></p><p>对于模型厂商而言，这是一个充满机遇的新时代。9 月 25 日，2024 百度云智大会上，百度智能云不仅展示了其在大模型产业落地方面的最新实践，还发布了包括基础设施、大模型、开发工具链、AI 原生应用等多个云与 AI 产品。在这次大会上，百度智能云千帆大模型平台 3.0 的发布成为了焦点。基于“加速企业大模型产业落地”的理念，千帆 3.0 旨在帮助企业更高效地实现产业 AI 化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2d/2d90bee1cb8500865573d4cecb6076c5.webp" /></p><p></p><p>那么，百度智能云如何利用千帆助力企业落地大模型？千帆 3.0 相比前代产品，又带来了哪些显著的升级？其又将如何帮助企业克服落地大模型过程中的重重困难？</p><p></p><p></p><h2>1企业大模型落地，到底有多难？</h2><p></p><p></p><p>企业落地大模型，到底需要克服哪些困难？</p><p></p><p>首先，算力问题是企业落地大模型的关键瓶颈。高性能的硬件资源不仅成本高昂，而且对大多数企业来说，如何有效管理和优化这些资源以支持大模型的运行和训练，成为了一项技术和经济上的双重挑战。企业在构建大模型时，往往需要投入大量资金购买高性能的计算设备，这对于资金实力有限的中小企业来说无疑是一个巨大的负担。</p><p></p><p>此外，算力的管理和优化也需要专业的技术团队进行维护和调整，进一步增加了企业的运营成本。因此，全套的大模型落地基础设施已经成为企业落地大模型的重中之重，算力瓶颈和成本问题。</p><p></p><p>其次，平台的兼容性问题同样不容忽视。不同系统和框架之间的集成往往需要额外的开发工作和技术支持，这进一步增加了企业的技术负担。许多企业在实施大模型时，发现现有的 IT 基础设施与新引入的模型技术之间存在兼容性问题，导致集成过程复杂且耗时。</p><p></p><p>除了算力和平台兼容性，企业在大模型落地过程中还需要关注开发层和服务层的需求。开发层涉及到模型的构建、训练和优化，而服务层则包括模型的部署和维护。企业在这两个层面上都需要切实可行的服务方案，以提升整体生产力。</p><p></p><p>在开发层，许多企业在实际业务场景中需要同时使用大模型和垂直模型。例如，在教育行业，企业可能需要结合大模型的自然语言处理能力和垂直模型的知识图谱，以实现个性化的学习体验。然而，开发这些模型通常需要较高的技术门槛和丰富的行业知识，这对企业的技术团队提出了更高的要求。</p><p></p><p>在服务层，企业需要确保模型在实际应用中的稳定性和可靠性。大模型的维护和更新是一个持续的过程，企业需要定期对模型进行监控和调整，以适应不断变化的业务需求和市场环境。这就要求企业具备灵活的平台架构，以便快速响应市场变化。</p><p></p><p>可以看到，企业在大模型落地过程中面临的挑战是多方面的，涉及模型调用、模型开发和应用开发等多个层面。为了有效应对这些挑战，企业需要借助像百度智能云千帆这样的整合平台，提供全方位的支持与服务。尤其在 AI 应用开发这一环节，企业迫切的需要一个工具甚至是平台，帮助他们触达最前沿的模型，调用最强的能力，实现最高效的开发。百度智能云的千帆 3.0 正是为此而生。正如云智大会上所说，千帆 3.0 为企业提供了一整套从模型开发到模型服务再到应用开发的全流程工具，层层结合，每一层都迎来了全新的能力升级。</p><p></p><p><img src="https://static001.geekbang.org/infoq/25/25a8405535f7b6bf866a9ad865226503.webp" /></p><p></p><p>根据 IDC 发布的《2023 中国大模型平台市场份额》报告，中国大模型平台及相关应用市场规模已达 17.65 亿元，百度智能云在这一领域占据了市场的领先地位。也正是在千帆调用量指数级增长下，百度智能云通过实践找寻到了大模型重构企业业务的方向，逐渐构建出千帆 3.0 这一能够满足企业级场景的“生产力工具”。</p><p></p><p></p><h2>从模型开发到调用服务，百度智能云如何帮助企业高效能、低成本落地使用大模型</h2><p></p><p></p><p>想象一下，如果你是一家制造企业的 CTO，你的公司需要利用大模型技术来提升产品质量，优化生产流程，但你却不知从何入手，到底是选择自己开发垂直模型，还是对主流开源模型进行调用，该如何将其接入企业内部的智能化系统，对于 AI 化技术刚刚入门的你来说，或许十分头疼。</p><p></p><p>千帆 3.0 正在为这样的企业搭建一个专属于模型开发层的产业级开发工具链。许多企业在实际业务中需要同时运用大模型和垂直行业模型，比如在金融领域，可能需要一个能够理解复杂查询的大型语言模型，同时也需要一个能够精准识别表格的 OCR 模型。千帆 3.0 提供的全套产业级开发工具链，正是为了解决这一需求而设计的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7f/7f7177af13f36a891aea7504cae5037c.webp" /></p><p></p><p>那么千帆 3.0 是如何做到帮助企业进行高效模型开发的？</p><p></p><p>首先，千帆 3.0 提供一站式模型开发服务，涵盖从数据处理、模型训练到服务部署的整个生命周期。其数据管理工具能够高效地处理和清洗大规模数据集，确保数据质量；模型训练方面，千帆 3.0 结合百度智能云百舸等基础设施，通过强大的算力资源与异构计算能力，显著提升了模型训练的速度和效果。千帆 3.0 还构建了完整的模型开发工具链，实现数据、模型、算力资源的统一纳管和调度，从而提高资源利用率和开发效率。此外，千帆 3.0 还提供了自动化的模型管理工具，支持模型版本控制、模型评估和模型优化等功能。这些工具能够帮助企业更好地管理和迭代模型，确保模型的稳定性和可靠性。</p><p></p><p>而在模型服务层，千帆 3.0 则遵循一个原则，那就是：灵活调用，按需服务。</p><p></p><p>千帆 3.0 在模型服务层进行了升级，提供文心大模型系列“全家桶”的调用服务，除了适用于复杂场景的旗舰大模型外，还包含了最适合精调的主力大模型、适合端侧的轻量级大模型，以及适合某些特定场景的垂直场景模型模型，并在此基础上，新增语音系列能力模型和视觉系列模型，使得大模型与传统模型充分协同，解决用户更复杂的场景需求。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9d/9de61636a90ca9764771f6276a33677b.webp" /></p><p></p><p>通过提供从模型开发到模型服务的全流程支持，千帆 3.0 不仅为企业提供了一个高效、灵活的解决方案，更重要的是其真正在降低企业实现 AI 化的门槛。其真正可以做到企业需求全覆盖，企业在模型开发与调用的过程中的基本场景均可满足，同时在“高性价比”的支持下，让模型能力不再被“束之高阁”，真正走到万千企业中去。在这个过程中，企业能够更专注于自身的核心业务，借助 AI 技术提升竞争力。</p><p></p><p></p><h2>AI 应用开发趋势已起，如何构建模型应用繁荣生态？</h2><p></p><p></p><p>在如果说千帆 3.0 的模型开发以及模型服务解决了企业 AI 化的问题，那么 AI 应用开发则是真正走到了“落地”这一步。如何将这些前沿技术转化为实际可用的应用，解决实际问题，提升用户体验，百度智能云的千帆 3.0 在 AI 应用开发方面实现了重大升级。</p><p></p><p>在过去的几年里，百度智能云千帆在 AI 应用开发领域不断发力，千帆 2.0 就已经开始在模型服务、模型开发的基础上，进一步为用户提供非常易用的 AI 原生应用开发工具，进一步催动了国内 AI 原生应用生态的日渐繁荣。</p><p></p><p>千帆 3.0 则不仅仅只是简单的功能升级，其已经真正进化成为“企业级”的 AI 应用开发平台。它提供了一整套的工具和框架，使得企业能够利用大模型的能力，快速构建和部署 AI 应用。这对于那些缺乏 AI 开发经验，但又急需智能化升级的企业来说，无疑是一个福音。</p><p></p><p>而深入进此次的功能升级，在千帆 3.0 中，企业级 Agent 开发是一大亮点。</p><p></p><p>什么是企业级 Agent？通常是指为企业提供服务的智能软件应用，它能够模仿人类的行为和决策能力，以提高企业的运营效率和改善客户体验。企业级智能体可以应用于多种场景，包括但不限于客户服务、内部业务流程自动化、数据分析和决策支持等。比如，在客户服务领域，企业可以利用这些工具开发出能够理解复杂用户查询并提供精准答案的智能客服系统。在制造业，企业可以开发出能够预测设备故障并提出维护建议的智能监控系统。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2d/2ddf322b84c1038690dea325ddc6470d.webp" /></p><p></p><p>但在过去的很长一段时间里，企业业务场景下 Agent 往往存在着很多问题，比如基于模型幻觉问题导致的“已读乱回”，缺乏长期记忆导致的多轮对话失效，以及碍于客观条件导致的能力欠缺，一系列问题之下，企业用户难以将 Agent 融入进业务中，企业级 Agent 也最终成为了一个“伪命题”。</p><p></p><p>但基于千帆 3.0 搭建的企业级智能体则能最大限度的避免这些问题。首先是极致的效果，目前基于千帆 3.0 搭建的企业级 Agent 可以灵活配置大模型以及垂类模型，规划调度准确率能够达到 95% 以上。同时借助人工编排功能，可以最大程度的降低大模型幻觉，从而更稳定地还原业务 SOP，让企业级 Agent 实现标准化，解决以往的失控问题。</p><p></p><p>除此之外，在长效记忆、知识注入、周边工具的丰富性上，千帆 3.0 都实现了全面升级，千帆企业 Agent</p><p>的记忆准确率可以达到 96%+，并且可以保持持续学习，持续进化，而 80+ 的高质量组件更能够进一步扩展 Agent 的边界，3D 数字人、语音对话等多模态功能，也进一步让企业级 Agent 的功能更加丰富，使用体验也同步提升。</p><p></p><p>借助千帆 3.0，智能体的开发不再需要从头开始编写复杂的代码，企业只需根据自己的业务需求进行简单的配置和调整，就可以快速生成所需的智能体。</p><p></p><p>千帆 3.0 的另一个核心工具是 AI 速搭，这是一个端到端的应用开发工具，它通过自然语言处理技术，允许企业通过自然语言描述来创建应用。这种方式极大地降低了 AI 应用的开发门槛，使得非技术背景的业务人员也能够参与到 AI 应用的开发中来。</p><p></p><p>例如，一个业务人员想要开发一个客户反馈收集系统，他只需要用自然语言描述这个系统应该具备的功能，AI 速搭就能够根据这样的描述，自动生成一个初步的应用框架。这个框架还可以通过低代码 GUI 的方式进行调整和完善，使得最终的应用能够完全符合企业的实际需求。</p><p></p><p>基于企业级 Agent 开发工具与 AI 速搭，千帆 3.0 进一步降低了 AI 应用开发的门槛。在当前 AI 应用开发人才紧缺的情况下，这样的普世化工具显得尤为重要。它们不仅降低了企业对于专业 AI 开发人才的依赖，也使得更多的业务人员能够参与到 AI 应用的开发中来。这不仅提升了企业的创新能力，也为企业的数字化转型提供了强大的动力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/21/2166330b82048a436463a8bd7adb9eb5.webp" /></p><p></p><p>千帆 3.0 的推出，是百度智能云对企业智能化需求的深刻理解和积极响应。它不仅提供了强大的模型开发和调用能力，更提供了一整套的应用开发工具，使得企业能够快速构建和部署 AI 应用。在这个过程中，企业能够更专注于自身的核心业务，借助 AI 技术提升竞争力，实现智能化转型。</p><p></p><p>与此同时，其为企业构建 AI 生态提供了坚实的基础。企业不仅可以开发出满足自身需求的智能应用，还可以将这些应用整合到更大的业务流程中，构建出一个完整的 AI 生态系统。</p><p></p><p></p><h2>产业实践为先，大模型如何切实解决行业发展问题？</h2><p></p><p></p><p>而在产品之外，AI 技术落地需要考虑更多复杂的场景，面对不同行业，不同需求，如何提高模型利用的效率，解决实际问题，则是全行业都在解决的问题。百度智能云的优势也正在于此，作为全国市场份额第一位的 AI 技术厂商，在汽车、能源电力、港口、钢铁、教育等行业，百度智能云均有落地标杆案例，基于这些行业实践经验，千帆 3.0 也同步提供了八大行业的场景解决方案，针对性的帮助企业解决实际问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/eb/eb5915dcfaf315530e83c56c4a729018.webp" /></p><p></p><p>以考试宝为例，作为一款服务于广大学习者的在线学习平台，它面临的挑战是显而易见的。在教育行业，题目解析是一项既耗时又耗力的工作。借助千帆大模型平台，考试宝能够自动解析用户上传的题目，快速生成准确的答案和解析，极大地提升了用户体验和效率。</p><p></p><p>在过去，用户上传一道题目后，往往需要等待一段时间才能得到反馈。而借助大模型的能力，使得考试宝能够实现近乎实时的解析，用户即刻就能得到题目的解析和答案。这种速度的提升，不仅让用户感到惊喜，也使得学习过程变得更加流畅和高效。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a6/a613e3b2b5c7cc0154c8ddb1f2a288aa.webp" /></p><p></p><p>对于企业来说，成本控制同样重要。千帆 3.0 的大模型技术，通过自动化的题目解析，大幅度降低了对人工审核的依赖，从而减少了人力成本。同时，自动化流程的高效性，也减少了因等待解析而造成的潜在机会成本。</p><p></p><p>除了教育行业，人事、企业服务、社交文娱、办公、电商营销、智能硬件、医疗、汽车等行业场景解决方案的推出，将为企业提供了一条条可行的智能化转型升级之路。以医疗行业为例，千帆能够辅助医生进行病例分析，提供辅助诊断建议，从而提升医疗服务的质量和效率。在汽车行业，借助千帆能够进行智能驾驶系统的开发，提高车辆的智能水平和安全性。</p><p></p><p>千帆 3.0 的目标是降低企业使用大模型的门槛，让更多企业能够享受到 AI 技术带来的红利。通过提供全栈的基础设施和工具，千帆 3.0 让企业无需从头开始构建复杂的 AI 系统，而是可以直接利用现有的模型和工具，快速实现业务的智能化。</p><p></p><p></p><h2>结语</h2><p></p><p></p><p>在数字化转型的浪潮中，百度智能云的千帆 3.0 正成为企业数智化转型的强大助推器。它不仅提供了全栈的基础设施支持，还通过一站式解决方案，大幅降低了企业在 AI 领域的入门门槛。千帆 3.0 的设计理念，是将复杂的 AI 技术封装成简单易用的工具，让企业能够快速地将 AI 能力应用到实际业务中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/75/7546147c4e09b6bf1c11f0e7d2870f1f.webp" /></p><p></p><p>随着技术的不断进步和实践的深入，千帆 3.0 将继续赋能企业，推动行业向更智能、更高效的未来迈进。正如在 2024 百度云智大会上所强调的，“智能科技是推动产业升级的核心动力”，千帆 3.0 正是这一理念的完美体现。它不仅推动了大模型的广泛应用，也为构建繁荣的 AI 应用生态奠定了坚实基础。未来，千帆 3.0 将继续引领企业在智能化的道路上走得更远、更快。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/dgkW2Y83r09pOKIAljCW</id>
            <title>老程序员有责任培养新人拯救行业！专访世界编程大师 Uncle Bob：不懂编程只会用 AI 助手是行业灾难！</title>
            <link>https://www.infoq.cn/article/dgkW2Y83r09pOKIAljCW</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/dgkW2Y83r09pOKIAljCW</guid>
            <pubDate></pubDate>
            <updated>Fri, 27 Sep 2024 10:39:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>在当今快速发展的软件行业中，不断涌现的新理念、技术和工具对开发者的工作方式产生了深远的影响。那么，敏捷开发在现代软件开发中还适用吗？测试驱动开发（TDD）是事倍功半还是物有所值？实践代码整洁之道对企业有哪些好处？</p><p></p><p>针对这些问题，近期 InfoQ 有幸专访了著名软件工程师、作家和讲师“Uncle Bob”——Robert C. Martin，与读者朋友们共同探讨。</p><p></p><p>Uncle Bob 是敏捷开发宣言 17 个奠基人之一，在软件开发社区中享有极高的声誉。并出版了多本关于软件开发的书籍，其中包括《Clean Code》、《The Clean Coder》等。目前，他的新书《Functional Design》也已在中国上市。《Clean Architecture》一书，经过国内4位知名的架构师精心翻译，将于金秋10月由机械工业出版社出版，欢迎关注。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/67/6786bf84b8e909b73dc410e6ff9ff032.jpeg" /></p><p></p><p>部分精彩观点如下：</p><p></p><p>软件最好在短周期内生产，同时拥有大量的反馈和团队互动。越想快速前进，代码就必须越整洁。并不是软件科学退化了，而是这个领域因大量尚未学习明白它的年轻学生涌入而被淹没。人工智能不是人类智能的替代品。使用 AI 来编写测试的程序员会发现 AI 只是重复了他们的错误。业界真正需要的是一个完善的学徒制度。软件行业最大的变化其实是硬件领域缺乏变革。人工智能发展的瓶颈已经隐约可见。</p><p></p><p>以下内容基于书面采访整理，经过不改变原意的编辑。</p><p></p><p></p><h2>个人经历</h2><p></p><p></p><p>InfoQ：非常高兴看到您的新书《Functional Design》在中国上市，中国的程序员们都非常关注这本书。我们了解到，过去近 30 年您参与撰写并出版的编程类书籍已经超过十本。最初您是因为什么样的契机开始写书的？是什么推动着您持续产出这么多书籍？</p><p></p><p>Uncle Bob： 说实话，我真不知道自己为什么会成为一名作家。我几乎从记事起就开始写作了。小时候，我每天都会写东西。我写过许多故事，也写过很多非虚构作品。当我成为一名程序员后，继续写作对我来说是很自然的事情。怎么说呢？我就是喜欢写作！</p><p></p><p>InfoQ：您是如何开始您的软件开发生涯的？在早期的职业生涯中，有哪些关键的经历或项目对您影响最大？您的昵称 uncle bob 有何含义吗？</p><p></p><p>Uncle Bob： 我的第一个程序是为一台小型塑料计算机编写的，那是我十二岁生日时母亲送给我的礼物。那台计算机叫做 Digi-Comp I。它有三个塑料滑块，代表三个比特位。它有弹簧加载的金属杆，起到与门的作用。简而言之，它是一个由 6 个三输入与门驱动的三位有限状态机。通过将小塑料管插入三个滑块上的插脚来编程，管子会阻挡与门的金属杆。这台机器可以被编程来从零数到七，或者从七倒数到零。你可以编程让它进行二进制加法，将两个比特相加并产生和位和进位。还有许多其他的小程序可以让这台机器执行。最终，我被深深吸引。我知道了我想要用我的余生做什么——我想要让那样的机器工作。</p><p></p><p>多年来，我参与了许多项目。我从事过金融、工厂自动化、嵌入式实时系统、计算几何、电话、电信、高能物理等领域的工作。可能最具影响力的是那些使用 8080 微处理器控制电子设备以测量电话线路质量的电话项目。</p><p></p><p>“Uncle Bob”这个名字是在 80 年代末由一位同事给我起的，很快就被大家记住了，我也认为这个名字很不错。</p><p></p><p></p><h2>Bob 大叔的编程哲学</h2><p></p><p></p><p>InfoQ：您提出了多项影响深远的软件设计理念，比如敏捷开发、代码整洁之道等，您能谈谈是哪些经历或人物帮助您塑造了您的编程哲学吗？</p><p></p><p>Uncle Bob： 在我的生活和职业生涯中，有很多人对我产生了影响：Grady Booch、Martin Fowler、Kent Beck、Jim Newkirk、Tim Ottinger、Michael Feathers、Jim Coplien……我可以继续列举下去，值得提及的人实在太多了。</p><p></p><p>至于塑造我哲学观的经历，我参与过许多不同类型的软件项目。在每一个项目中，我都注意到了软件设计与架构方面的相似问题。多年来，这些问题逐渐凝聚成了一些通用原则。</p><p></p><p>InfoQ：作为敏捷宣言的创始人和签署者之一，您认为敏捷开发对软件开发行业最大的贡献是什么？</p><p></p><p>Uncle Bob： 敏捷软件开发的最大贡献，简单来说，就是重新唤醒了一个理念：软件最好在短周期内生产，同时拥有大量的反馈和团队互动。这个理念在 50 年代就已经被知晓和实践，但在 70 年代初，一种极端的前期规划理念取代了它。这种替代是由于程序员群体的人口结构变化：随着大量拥有计算机科学学位的大学毕业生涌入，程序员的平均年龄下降了近 20 岁。</p><p></p><p>InfoQ：去年的时候，敏捷早期采用者 <a href="https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651155396&amp;idx=1&amp;sn=60d270d7f7103ba691ff9c504b43391c&amp;scene=21#wechat_redirect">Capital One 裁掉了整个敏捷团队</a>"，认为这是降低“遗留技术成本”。有统计发现如今的主流大厂也认为敏捷阻碍了交付，<a href="https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651119406&amp;idx=1&amp;sn=a9bf04b2adf23de56bceb6927778ebe9&amp;scene=21#wechat_redirect">并摒弃了 Scrum 框架</a>"。那么您对这些企业的敏捷实践有何看法？为什么敏捷会衰落？敏捷开发的正确方法是什么？</p><p></p><p>Uncle Bob： 敏捷，像所有软件理念一样，可能会成为营销炒作和稀释的牺牲品。敏捷开发是由程序员为程序员创建的，但早期它就被一群寻求 Scrum Master 认证的项目管理者侵入，然后被曲解。这群人教授和实践了他们所谓的“敏捷”，但这与原始的敏捷原则并不一致。我对 Capital One 的案例不熟悉。但对我来说，一些公司会放弃被曲解的 Scrum 和敏捷版本并不奇怪。</p><p></p><p>敏捷的正确方法是遵循敏捷宣言中的四项声明。Kent Beck 的《极限编程解释》一书中很好地描述了一套最佳的敏捷纪律。</p><p></p><p>InfoQ：您提出的 SOLID 原则对面向对象设计有着深远的影响。您能解释一下这些原则背后的基本思想吗？</p><p></p><p>Uncle Bob： 所有的 SOLID 原则都是管理依赖关系的技术。总体目标是将软件系统划分为一组组件，这些组件的相互依赖关系被组织得如此之好，以至于变化不会从一个组件传播到另一个组件。</p><p></p><p>InfoQ：有人认为在微服务等技术盛行之后，SOLID 原则并不完全适用了（https://www.infoq.com/news/2021/11/solid-modern-microservices/）。那么您认为现代软件设计是否需要有一套新的原则？</p><p></p><p>Uncle Bob： 在我职业生涯的几十年里，给我留下深刻印象的一件事是，软件设计与架构的原则是不变的。我们并不需要一套新的原则来处理“现代”软件实践——因为“现代”软件实践与不“现代”的软件实践并没有那么大的不同。</p><p></p><p>微服务就是一个很好的例子。几十年来，软件开发人员一直在将组件隔离成可以独立执行的服务。微服务的概念一点也不新。而且，它也不是许多系统的特别合适的方法。对于那些适合使用微服务的系统来说，SOLID 原则为服务的设计以及整个系统提供了一个很好的指导。</p><p></p><p>InfoQ：您从 1970 年开始从事软件行业，那时候甚至还没有 C 语言，而现在主流的 Java、Python 语言出现得更晚，Rust、Go 等语言甚至是在您的“Clean Code”出版之后。对于 C 可能非常易于使用“Clean Code”的一些原则，但是否也适用于 Java、Rust、Swift、Python 等这些后期出现的静态、动态语言？</p><p></p><p>Uncle Bob：Clean Code 的原则适用于所有编程语言。你可以在 Rust 中实践它们，在 COBOL 中，在 Swift 中，在 FORTRAN 中，在 Clojure 中，在 Assembler 中。它们适用于静态语言如 Kotlin，动态语言如 Ruby，堆栈语言如 Forth，以及逻辑语言如 Prolog。</p><p></p><p>InfoQ：如今的企业希望做到快速推出产品并快速试错，那么相对以前，现在的开发者们应该如何构建干净的代码，哪些原则是必须坚守的，哪些是可以妥协的？干净代码能给企业以及想盈利的产品带来哪些好处？</p><p></p><p>Uncle Bob：“早失败，常失败”是敏捷原则之一，它与 Clean Code 完全兼容。实践代码整洁之道并不慢——它很快，代码整洁比代码混乱时更快。你越想快速前进，你的代码就必须越整洁。</p><p></p><p>InfoQ：对于一些在 IT 行业从事了 20 年或以上的人来说，其实都在目睹“软件科学”的不断退化。二十年前，我们写软件会用到“设计模式”、会用 UML 做设计、做完美的需求分析和画流程图。如今，我们看到许多年轻开发者不再热衷于学习和应用设计模式、UML 等。您认为造成这种现象的原因是什么？如何平衡过度设计和功能需求？</p><p></p><p>Uncle Bob： 世界上程序员的数量大约每五年翻一番。自 60 年代以来一直如此，而且在未来几十年内很可能仍然如此。这意味着世界上有一半的程序员经验不足五年。这就解释了问题中的所有情况。并不是软件科学在退化，而是这个领域因大量尚未学习明白它的年轻学生涌入而被淹没。 这对设计模式、敏捷、SOLID、UML 等都是如此。</p><p></p><p>解决这个问题的方法是提升教学质量。我们这些有 10 年、15 年和 20 年经验的人必须承担起责任，教育涌入我们领域并稀释我们职业的大量年轻程序员。</p><p></p><p>InfoQ：您提到过您理想情况下的 TDD 是每行代码编写一行免费的测试代码，并达到 100% 的覆盖率。但实际有一些软件，特别是互联网企业，并不重视测试，比如说 Tiktok 作为一款全球现象级的应用，似乎并没有严格遵循 TDD 的原则，他们<a href="https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651107616&amp;idx=1&amp;sn=cada729aad574eabc09557caef99aff6&amp;scene=21#wechat_redirect">用 QA 代替了单元测试</a>"，但实际上 Tiktok 运行还比较稳定。您如何看待这样的一些实践？您认为在现实的软件开发中，尤其是互联网企业，这种状态是否可行并可持续？为什么？</p><p></p><p>Uncle Bob： 我很推荐 TDD。有些人不遵循这一建议，那是他们的选择。然而我猜想，一个不太重视测试的软件团队，必然花很多时间在调试上，并且会因为调试而显著减慢速度。我个人更喜欢通过避免调试来快速推进。</p><p></p><p>我还认为，一个不太重视测试的软件团队，也不会太重视重构。没有一套测试，重构是非常危险的，系统必然会因为开发者害怕通过重构来保持代码整洁而退化。最终，随着时间的推移，这些团队将不得不处理越来越混乱的系统，他们的速度会因此减慢。</p><p></p><p>InfoQ：作为测试驱动开发的坚定支持者，您如何回应那些认为 TDD 会降低开发速度的观点？对于那些不重视测试，甚至认为测试是浪费时间的年轻企业，您有哪些建议？如何说服他们认识到 TDD 的重要性？</p><p></p><p>Uncle Bob：TDD 提高开发速度，因为它减少了调试，并使得重构成为可能。重构允许系统的设计与代码持续被清理和改进，这使得团队中的每个人都能更快地工作。做快的唯一方式是做好。</p><p></p><p>InfoQ：如今，生成式 AI 越来越强大，围绕各类编程类 Copilot 有很多讨论。与我们之前倡导的“提升专业化水平”方向不同的是，这些工具厂商都在鼓动不懂或不精通编程的非专业人士进入软件行业，并编写自己的软件，您认为这会对 IT 行业造成哪些影响？</p><p></p><p>Uncle Bob： 灾难。人工智能不是人类智能的替代品。人工智能可以是很好的工具，但只有在知道如何使用这些工具的人手中才是如此。</p><p></p><p>InfoQ：许多开发人员已开始将 ChatGPT 和 Copilot 等工具集成到他们的日常工作流程中。那么，在生成式 AI 时代，TDD 是否比以前更为重要？我们注意到已经有一些利用大模型生成测试的案例，您如何看待“使用 AI 为代码生成测试”？您有没有利用 AI 进行测试的建议方法？</p><p></p><p>Uncle Bob：TDD 相当于会计实践中的复式记账。会计人员将每一笔交易输入两次，一次作为资产，第二次作为负债。这两笔交易遵循不同的数学路径，直到它们在资产负债表上相互抵消至零。遵循 TDD 的程序员将每一段代码编写两次，一次作为测试，第二次作为生产代码。它们遵循互补的执行路径，直到它们零和测试失败，在错误报告上相遇。</p><p></p><p>TDD 和复式记账的要点是让个人输入两次，以捕捉该个人犯的错误。我们不希望自动化系统，如人工智能，来编写我们的测试，因为那违背了让个人陈述和重申他们意图的目的。使用 AI 来编写测试的程序员会发现 AI 只是重复了他们的错误。</p><p></p><p></p><h2>“想一直写代码直到去世”</h2><p></p><p></p><p>InfoQ：您现在仍在软件设计和工程实践领域深耕，没有停止对编程的研究，这五十年来，您是如何一直保持对编程的热爱的？</p><p></p><p>Uncle Bob： 我在十二岁时就爱上了编程，并且从未回头。我想一直写代码直到我去世，而且我不想很快就去世。总有一天，他们会发现我，我的鼻子卡在笔记本电脑的键盘之间，屏幕上显示着一个失败的测试。</p><p></p><p>InfoQ：您经常提到软件工匠（Software Craftsman）和工匠精神。您能解释一下何为软件工匠精神，并分享一些如何培养这种精神的建议吗？</p><p></p><p>Uncle Bob： 软件工匠精神就是你每天下班回家后，站在镜子前，对自己说：“我今天干得不错。” 很多程序员下班后却需要洗个澡，因为他们那天制造的混乱。软件工匠精神关乎对工作的自豪，知道自己的代码会被他人看到并赞赏，你是如何精心打造代码的，以及你对细节的关注。</p><p></p><p>InfoQ：您曾表示程序员要持续学习，比如在一门新的编程语言还没流行之前就提前感知到并去学习它。您最近还在学习什么新语言吗？在您的职业生涯中，想必使用过多种编程语言。您认为选择编程语言时应考虑哪些因素？程序员如何才能提前感知到未来可能变得重要的新语言的来临？</p><p></p><p>Uncle Bob： 最近我又开始温习一些编程语言：写了一些 Python 和 Go 代码——这些语言我已经多年没碰了。也稍微研究了一下 Rust，尽管我现在还不能算是一个 Rust 程序员。几年前，我花了点时间学习 Elixir，而且我经常玩玩 Lua 。</p><p></p><p>学习新语言时，最重要的考虑因素是它是否能让你感到乐趣。当学习变得有趣时，你不仅会学得更快，而且学习本身就是目的。随着时间的积累，你会逐渐明白新语言对你的职业生涯是否有帮助。但在职业生涯的初期，这一点并不是特别重要。等到你能够在团队和组织的方向上发表意见时，实用性和实际效益的考量就会变得更加重要。</p><p></p><p>InfoQ：您认为当前的编程教育是否足够培养出优秀的软件开发者？有哪些改进的建议？</p><p></p><p>Uncle Bob： 不，我觉得大多数编程教育都远远不够。业界真正需要的是一个完善的学徒制度。新程序员应该在资深程序员的亲自指导下，通过实践操作和严格监督来学习——就像医生和律师那样。</p><p></p><p>InfoQ：在您看来，软件开发领域过去十年最大的变化是什么？未来十年最值得期待的变化又是什么？</p><p></p><p>Uncle Bob： 过去几十年，软件行业最大的变化其实是硬件领域缺乏变革。摩尔定律大约在 2005 年就走到了尽头，自那以后，计算机的速度并没有显著提升。存储技术在接下来的十年里持续进步，但现在这种进步也放缓了。我成长的时代，机器的性能以指数级的速度提升，变得更快速、更小巧、更经济、存储容量更大。每隔一两年，我们就得更新换代，因为旧设备无法胜任新软件的需求。每隔一两年，我们的视野就会因为新技术而拓宽，探索更多可能性。但今天的软件开发者面临的是一个技术发展的瓶颈期，机器的性能不会再像以前那样指数级增长了。虽然可能会有一些逐步的改进，但摩尔定律带来的迅猛发展已经结束。</p><p></p><p>人们很容易误以为人工智能是摩尔定律那样的技术革命，但实际上并非如此。人工智能需要巨大的资源投入，而这些资源是有限的，因此人工智能不可能无限制地指数级增长。一些技术改进，比如专用芯片，可能会带来一定的进步，但人工智能发展的瓶颈已经隐约可见。</p><p></p><p>在未来十年，我们应该更多地关注提升我们的专业技能，而不是单纯依赖技术的进步。 我们应该致力于成为更优秀、更专业的程序员。我们应该努力建立和完善软件行业的纪律、标准和道德规范。</p><p></p><p>InfoQ：您还有什么话想对中国的开发者们说？</p><p></p><p>Uncle Bob： 开发者都是我的同胞，我向世界各地的开发者致以崇高的敬意。对每一位开发者，我都要说一句：保持代码的整洁，认真做好每一份工作，无论是在编程还是在生活中都要细心谨慎。当你提交代码时，确保它比你检出时更加整洁。</p><p></p><p>今日好文推荐</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651215576&amp;idx=1&amp;sn=9e32dbb2529df6e21aa9c0b4cc0a68d0&amp;chksm=bdbbae8b8acc279d1f214d6fc675e341ad83a4da021659b53138242a7c187a3c28e51d0aa2e6&amp;scene=21#wechat_redirect">下载量超 5000 万的知名应用，开发团队“全军覆没”，从此发版人唯剩老板一个</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651220167&amp;idx=1&amp;sn=812b1ddaba6c1f846c8095d0427d0525&amp;chksm=bdbb90948acc1982cd836b8ba08bd48b8f6f385b496a8c406a6e130e2b5e57d88d494f8b9220&amp;scene=21#wechat_redirect">一次 App 更新差点要了这家 22 年老牌公司的命：忽视技术债、疯狂裁员降本，CEO 为开发加速无视员工警告</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651220019&amp;idx=1&amp;sn=eede282c476b1d534607b7fa41b26a08&amp;chksm=bdbb90608acc1976aa6d13cce486c95642a8922cf518dd9a2a892029ce394b34a6aa39cba4f3&amp;scene=21#wechat_redirect">开源 9 年后，词频数据库 wordfreq 宣布停止更新，创始人：网上全是垃圾，OpenAI 和谷歌要为此付出代价</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651219703&amp;idx=1&amp;sn=6be957dc64bd47d16c7ff5046578b602&amp;chksm=bdbb9ea48acc17b209e2ee998dd25fc55fae213f1b577dcd5b3d4de6592129101e4337f5132f&amp;scene=21#wechat_redirect">突发！高通被曝有意收购英特尔；思科N+7裁员，员工称人性化；百度最高奖发出2800万！李彦宏：再苦不能苦技术人 | Q资讯</a>"</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/zJuC2GK3R95NjyUzkpqD</id>
            <title>MLPerf 存储基准测试发布：焱融存储斩获多项世界第一</title>
            <link>https://www.infoq.cn/article/zJuC2GK3R95NjyUzkpqD</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/zJuC2GK3R95NjyUzkpqD</guid>
            <pubDate></pubDate>
            <updated>Fri, 27 Sep 2024 10:23:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><h3>摘要：</h3><p></p><p>9月25日，全球权威 AI 基准测评组织 MLCommons® 公布了 MLPerf® v1.0 存储性能基准测试的结果。焱融科技在此次测试中表现出色，焱融全闪存储产品在 3D-Unet、ResNet50 和 CosmoFlow 三种 AI 深度学习模型的评估中均展现了卓越的性能和效率。</p><p></p><p>焱融科技作为中国自主研发的高性能存储领导者，与 DDN、Nutanix、Weka、Hammerspace、Solidigm 和 Micron 等众多国际优秀厂商同场竞技，测试结果显示，在带宽、模拟 GPU 数量以及 GPU 利用率等关键性能指标上，焱融科技的产品获得了多项世界第一。</p><p><img src="https://static001.geekbang.org/infoq/ed/eda6b205c472bc8a92d592d8143771c3.webp" /></p><p>在 MLPerf® Storage v1.0 的基准测试中，焱融全闪存储显著提升了 GPU/ML 工作负载的处理速度，这表明焱融高性能存储产品具备支持各种 AI 模型训练和高性能计算场景的能力。在 AI 领域，尤其是在大规模模型训练方面，焱融全闪存存储解决方案发挥着至关重要的作用，为 AI 技术的发展和应用提供了强有力的支持。</p><p></p><h3>MLPerf® Storage  全球首个且唯一的 AI/ML 存储基准测试</h3><p></p><p>MLPerf 是由图灵奖得主大卫·帕特森（David Patterson）联合谷歌、斯坦福大学、哈佛大学等顶尖学术机构共同发起的国际权威 AI 性能基准测试，被誉为全球 AI 领域的“奥运会”。MLCommons 组织在 2023 年首次推出了 MLPerf 存储基准测试（MLPerf Storage Benchmark），这是首个也是目前唯一一个开源、公开透明的 AI/ML 基准测试，旨在评估存储系统在 ML/AI 工作负载中的表现。这一基准测试为 ML/AI 模型开发者选择存储解决方案提供了权威的参考依据，帮助他们评估合适的存储产品。</p><p></p><p>MLPerf Storage 基准测试目前有两个版本：v0.5 和 v1.0。2023 年发布的 v0.5 版本初步包含了 Unet-3D 和 BERT 两个模型，并仅支持模拟 NVIDIA v100 GPU。而今年最新发布的 v1.0 版本进行了重大更新，引入了更具代表性的测试模型，这些模型在业界具有广泛的应用，能够更好地代表实际工作负载。</p><p><img src="https://static001.geekbang.org/infoq/9f/9f672cdb139bb38d1827eb8ae9919072.webp" /></p><p>为确保测试结果的可靠性，MLPerf Storage v1.0 基准测试的规则非常严格，关键要求如下：</p><p>1. 高 GPU 利用率</p><p>在 U-Net 3D 和 ResNet-50 模型测试中，GPU 利用率需维持在 90% 以上。CosmoFlow 模型的 GPU 利用率需达到 70% 以上。</p><p>2. 禁止缓存</p><p>在基准测试开始前，不能在主机节点上缓存训练数据。连续测试运行之间，必须清除主机节点中的缓存以确保测试的准确性。整体数据集的大小务必远超过主机节点的内存大小。</p><p></p><h3>国内唯一全面参与所有模型测试的厂商，荣登多项世界第一</h3><p></p><p>本次焱融科技参与 MLPerf 测试使用了<a href="http://mp.weixin.qq.com/s?__biz=MzIzMzY1NTM4Mw==&amp;mid=2247491564&amp;idx=1&amp;sn=c63d949a309d21a0d3ca420e395df177&amp;chksm=e88302d4dff48bc215c953a69a21e1f9b43ac84477211ea3d12b2c61c1ab56ddaa0c07aff7d8&amp;scene=21#wechat_redirect">最新发布的 F9000X 全闪分布式一体机产品</a>"。F9000X 每个存储节点搭载最新的英特尔® 至强® 第 5 代可扩展处理器，存储介质采用 10 块 Memblaze PCIE 5.0 NVMe 闪存 ，同时配备 2 块 NVIDIA ConnectX-7 400Gb NDR 网卡。测试环境网络拓扑如图所示：</p><p><img src="https://static001.geekbang.org/infoq/9f/9f51570df9cbbba2331758f672edc69f.webp" /></p><p>部署环境架构图</p><p></p><p>为了深入理解 MLPerf Storage 基准测试内容，我们先解释两个核心概念：</p><p>ACC：即 Accelerators（加速器），MLPerf Storage Benchmark 测试工具通过 accelerator emulation，来模拟真实的 GPU，如：NVIDIA A100、H100 等。在无需真实 GPU 的情况下就能进行大规模的存储性能压测，用以评估存储系统在 AI 模型训练场景的适用性。</p><p></p><blockquote>说明：在 MLPerf Storage Benchmark v1.0 版本中 ACC 可以模拟 NVIDIA A100 和 H100 两款 GPU 型号。本次MLperf 测试焱融提交了基于 H100的测试数据。</blockquote><p></p><p></p><p>AU：Accelerator Utilization（AU，加速器利用率）在 MLPerf Storage Benchmark 中，通常被定义为 GPU 计算时间占整个基准测试运行时间的百分比。这是一个衡量加速器在给定任务中的有效利用程度的关键指标，GPU 计算时间占比越高，代表存储速度越快，GPU 越能被充分利用。AU 太低，说明存储性能不足以支撑 GPU 高效运行，只有 AU 高于指定值时，提交的数据才是有效数据。</p><p></p><p></p><h3>焱融在 MLPerf&nbsp; Storage &nbsp;v1.0 的测试表现</h3><p></p><p></p><p></p><h4>最全面最完整，国内唯一一家参加了全部模型测试的存储厂商</h4><p></p><p>焱融科技是国内唯一一家参与了 MLPerf Storage 全部模型测试的存储厂商，这些测试包括&nbsp;3D-Unet、CosmoFlow 和 ResNet 50。在本次测试环节，焱融追光全闪存储一体机 F9000X 展现了卓越的性能，全面覆盖目前主流模型应用数据负载需求。F9000X 不仅能够处理大规模的数据集，还可以根据 AI 集群规模弹性扩展，完美匹配 GPU 算力性能。</p><p></p><p></p><h4>在分布式训练集群场景，平均每个计算节点 ACC 数量最多，存储带宽最高</h4><p></p><p>MLPerf Storage 基准测试规则定义可以采用单个计算节点（客户端）运行多个 ACC（GPU 加速器），进行相应模型应用测试，同时支持大规模分布式训练集群场景，多个客户端模拟真实数据并行的方式并发访问存储集群。其中平均每个客户端能够运行的 ACC 数量越多，则代表该节点的计算能力越强，能够处理任务的数量也就越多，而对于存储数据并发访问性能要求也就越高。测试结果显示，在分布式训练集群场景，焱融存储在所有三个模型的测试中，能够支撑的每个计算节点平均 ACC 数量和存储带宽性能均排名第一。</p><p><img src="https://static001.geekbang.org/infoq/9f/9f8bff0a045e9141ea6fc76d5e921ef6.webp" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/2e/2e49df7c096e8f55ccff98579c21e4fa.webp" /></p><p></p><h4>存储性能随计算规模同步线性增长</h4><p></p><p>随着计算规模的扩大，存储性能应实现线性增长以满足 AI 训练的需求。以 3D-Unet 三维图像分割模型为例，其单个图像样本大小约为 146MB，而在多节点集群环境中，每秒处理的训练样本数可超过 1100 个，这导致训练数据的读取带宽需求超过 160GB/s。</p><p>在针对三个模型的测试中，焱融全闪存储一体机 F9000X 展现了出色的性能。测试结果显示：随着并发计算节点（ACC）数量的增加，存储系统的带宽性能保持明显的线性增长能力。此外，存储的可用性（AU）也始终保持在测试基准要求的范围内，确保了训练过程的高效和稳定。</p><p><img src="https://static001.geekbang.org/infoq/83/8386cbc56f2982507c8e77434056deb3.webp" /></p><p>目前在 3D-Unet 模型应用的测试中，使用 3 个计算节点，共 60 个 ACC，可达到 160GB/s 的存储带宽性能。F9000X 3 节点存储集群实测最大可以达到 260GB/s 以上的带宽性能，这表明在实际业务环境中焱融全闪存可以支撑更多的 GPU 的计算节点。</p><p>以下是焱融全闪存储在 ResNet50、CosmoFlow 这两款模型测试的存储的可用性（AU）及带宽性能表现：</p><p><img src="https://static001.geekbang.org/infoq/b5/b590bfcd89151a36da60c37faf78d52d.webp" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/41/41f8ae95007d3de718f1a937421e34e2.webp" /></p><p></p><h3>小结：</h3><p></p><p>在进行 MLPerf Storage 基准测试时，我们发现为了满足 AI 计算存储的性能需求，存储系统需要具备以下关键特性：</p><p>高性能设备支持：MLPerf Storage 需要高带宽，因此存储系统必须支持如 200Gb 和 400Gb InfiniBand 或以太网等高性能网络设备。MultiChannel 网络带宽聚合：YRCloudFile 支持在 InfiniBand 或 RoCE 网络上使用 MultiChannel 功能，以充分利用双卡的性能，实现数据读写的高效性。🔗 更多了解，详情请见《<a href="https://mp.weixin.qq.com/s?__biz=MzIzMzY1NTM4Mw==&amp;mid=2247489291&amp;idx=1&amp;sn=f824abadc01c8aeeca7ff21ffd342cbd&amp;scene=21#wechat_redirect">90GBps 性能顶流！焱融科技发布最新 AI 大模型存储方案</a>"》全链路 direct&nbsp;I/O：为了避免内存缓存导致的性能瓶颈，YRCloudFile 支持在计算节点上部署 kernel client，允许数据读写直接绕过内存缓存，通过 direct I/O 方式访问后端存储。🔗 更多了解，详情请见《<a href="https://mp.weixin.qq.com/s?__biz=MzIzMzY1NTM4Mw==&amp;mid=2247491417&amp;idx=1&amp;sn=8b7091dde8aa6d7d678a289e4afd5e3e&amp;scene=21#wechat_redirect">CPU 使用率飙升，Buffer IO 引发的性能问题</a>"》）NUMA&nbsp;优化：内存性能对存储性能至关重要。YRCloudFile支持全链路NUMA优化，确保服务进程与NVMe SSD绑定到同一NUMA节点，优化数据传输路径。🔗 更多了解，详情请见《<a href="https://mp.weixin.qq.com/s?__biz=MzIzMzY1NTM4Mw==&amp;mid=2247488301&amp;idx=1&amp;sn=1a34d5c6068763799093da491798711a&amp;scene=21#wechat_redirect">YRCloudFile V6.8.0 发布：向全闪时代迈进</a>"》</p><p>焱融分布式文件存储 YRCloudFile 通过上述技术亮点，能够在本次 MLPerf Storage 测试中接近硬件性能极限，为 AI 计算提供所需的高性能存储解决方案。在实际测试中，YRCloudFile 已经展现出能够支持大规模 AI 训练任务的能力，即使在极端条件下也能保持系统的稳定性和性能。</p><p></p><p>引用链接：</p><p>[1] MLPerf Storage Benchmark Suite Results:&nbsp;https://mlcommons.org/benchmarks/storage/</p><p>[2] MLPerf Storage rules:&nbsp;</p><p>https://github.com/mlcommons/storage/blob/main/Submission_guidelines.md</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/MKfOlVLfzfVN4Xk5Rws3</id>
            <title>Sam Altman罕见发表预言长文：我们距离AI超级智能可能只有“几千天”</title>
            <link>https://www.infoq.cn/article/MKfOlVLfzfVN4Xk5Rws3</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/MKfOlVLfzfVN4Xk5Rws3</guid>
            <pubDate></pubDate>
            <updated>Fri, 27 Sep 2024 10:21:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>本周，OpenAI公司CEO山姆·奥特曼在一篇题为《智能时代》的最新个人博文中，概述了自己对于AI驱动的技术进步与全球繁荣未来的愿景。这篇文章描绘了AI加速人类进步的可能性，奥特曼还指出超级AI有可能会在未来十年之内出现。</p><p>&nbsp;</p><p>他在文中写道，“我们距离超级超级智能可能只有几千天；也许实际时间会更长，但我坚信我们将会达成这个目标。”</p><p>&nbsp;</p><p>目前尚不清楚 Altman 的这篇帖子是新产品发布或重大公告的先兆，还是仅仅是对“智能时代”的预测。尽管如此，Altman 表示乐观强调这个新时代的一个决定性特征是大规模的繁荣。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/f6/f64a98907fc30492269ef715d2349085.jpeg" /></p><p></p><p>&nbsp;</p><p></p><h2>准备好迎接“智能时代”？！</h2><p></p><p>&nbsp;</p><p>OpenAI的当前目标在于建立通用人工智能（AGI）。这是一个设想出来的技术概念，指在执行多种任务时能够与人类智能相媲美、且无需接受特定训练的AI。相比之下，超级智能要比通用人工智能更进一步，可以理解成一种假想的机器通知水平，可以在任意智力任务上远远超越人类、甚至达到不可思议的程度。</p><p>&nbsp;</p><p>超级智能（也被称为「ASI」，即「人工超级智能」）是机器学习社区中一个流行、但也有些极端的议题，多年来一直受到关注和质疑。矛盾的集中爆发，则源自哲学家Nick Bostrom于2014年撰写的争议论著《超级智能：路径、危险与策略》。OpenAI公司联合创始人兼前首席科学家Ilya Sutskever于今年6月离开OpenAI之后，也创办了一家以该术语命名的公司：Safe Superintelligence——安全超级智能。与此同时，奥特曼本人则至少从去年开始就一直在谈论开发超级智能。</p><p>&nbsp;</p><p>那么，“几千天”到底是有多长？没人能给出确切的数字。奥特曼之所以选择这样一个模糊的数字，可能也是因为他也不确定AI超级智能究竟何时会来，只是隐隐感觉到可能会在未来十年之内实现。具体算来，2000天大约是5.5年，3000天大约是8.2年，而4000天则接近11年。</p><p>&nbsp;</p><p>我们当然可以批评奥特曼的语焉不详，毕竟没有人能够真正预测未来。但身为OpenAI公司的掌门人，奥特曼很可能掌握着一些尚未被公众所广泛知晓的AI研究技术。因此哪怕是“几千天”这样一种宽泛的时间表述，考虑到说法来自AI领域值得关注的消息来源，我们似乎也有必要认真加以对待——尽管这位消息人士属于利益相关者，投入了大量资金来确保AI发展不致陷入停滞。</p><p>&nbsp;</p><p>而且并非所有人都认同奥特曼表现出的乐观和热情。计算机科学家、时常批评AI技术的Grady Booch引用了奥特曼“几千天”的预测，在X上发帖评论称：“我对一切AI炒作感到极度厌倦：它们没有任何现实依据，只会抬高期待、激怒公众、登上媒体头条，并分散人们对于计算领域实际工作的注意力。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/f4/f4acb4058515c9a3e9dbd132b42d1650.jpeg" /></p><p></p><p></p><blockquote>我同意萨姆的看法！假设他使用的是16进制计数法……</blockquote><p></p><p>&nbsp;</p><p>尽管受到批评，但值得注意的是，这家可能是当下最具代表性的AI厂商的CEO确实在对未来技术能力做出宽泛的预测——哪怕目的可能是为了继续努力筹集资金。现如今，建设基础设施来支持AI服务，已经成为许多科技企业CEO们最关心的问题。</p><p>&nbsp;</p><p>奥特曼在文章中写道，“如果我们想让尽可能多的人们掌握AI，就必须降低计算成本并丰富其实际功能（这需要大量能源和芯片）。如果我们不建设充足的基础设施，AI就将成为一种极其有限的资源。争夺之战将因此打响，AI也会主要成为富人们的工具。”</p><p>&nbsp;</p><p></p><h2>奥特曼的“智能时代”愿景</h2><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/34/34acfd6176eedc0c4121d59474c8ad54.png" /></p><p></p><p>OpenAI公司CEO山姆·奥特曼于2024年1月11日走过华盛顿特区美国国会大厦众议院廊下。</p><p>&nbsp;</p><p>在文章的其余部分，奥特曼将我们现在的时代描述为“智能时代”的开端，即人类历史上继石器时代、农业时代和工业时代之后的又一个变革性技术时代。他将深度学习算法的成功描述成这个新时代的催化剂，并简单总结称：“我们是如何迈向下一次繁荣飞跃的门槛的？概括讲，就是深度学习的确有效。”</p><p>&nbsp;</p><p>奥特曼的观点似乎基于该公司最近推出的 "o1" AI 模型，据称该模型能够推理解决以前模型难以处理的问题。</p><p>&nbsp;</p><p>许多批评该公司的声音认为，深度学习——驱动像 ChatGPT 和 Google 的 Gemini 这样的模型的特定类型 AI——无法通过扩展来创造出具有人类水平的人工智能。</p><p>&nbsp;</p><p>然而，在他最新的博客文章中，奥特曼直接反驳了这一观点：</p><p></p><blockquote>“用15个字概括：深度学习有效，随着规模的扩大可预测地变得更好，我们为其投入了越来越多的资源。”</blockquote><p></p><p>&nbsp;</p><p>这位OpenAI掌门人设想AI助手将变得越来越强大，最终形成“个人AI团队”，能够帮助个人完成他们所能想象的几乎一切事务。他预测AI技术将在教育、医疗保健、软件开发及其他诸多领域取得突破。</p><p>&nbsp;</p><p>虽然承认AI带来的潜在负面影响与劳动力市场混乱，但奥特曼在AI对于人类社会的整体影响方面仍持乐观态度。他在文中写道，“繁荣本身并不一定能让人们幸福——有很多富人其实生活得相当悲惨——但其仍将显著改善世界各地人民的生活。”</p><p>&nbsp;</p><p>即使像SB-1047这样的AI监管条例已经成为当下的热门话题，奥特曼也没有在文中特别提及AI技术带来的科幻性质的风险。在X上，彭博专栏作家Matthew Yglesias写道，“值得注意的是，奥特曼甚至不再口头讨论生存风险的问题，他唯一担忧的AI弊端就是对劳动力市场造成的冲击。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/7f/7fb2e4272591b5f218de663ba7d9c470.jpeg" /></p><p></p><p>&nbsp;</p><p>虽然对于AI科技的潜力充满热情，但奥特曼同样呼吁应当谨慎行事，只是语气比较含糊。他写道，“我们需要明智且坚定地采取行动。智能时代的到来标志着重大的发展里程碑，必然伴随着极其复杂的高风险挑战。整个过程绝不会完全积极且正向，但其中的收益又是如此巨大，我们有责任为自己和未来考虑该如何应对可能面临的风险。”</p><p>&nbsp;</p><p>可除了对劳动力市场造成的冲击之外，奥特曼并没有提及智能时代可能造成的其他负面影响，只是用因技术变革而消失的过时职业作为类比给文章作结。</p><p>&nbsp;</p><p>他写道，“我们当下所做的许多工作，在几百年前的人们看来可能既微不足道又浪费时间。但往者不可谏，来者犹可追，最重要的不是沉溺于过去、而在于怎样开启一个新的时代。如果当时的人们有机会看到今天的世界，一定认为周遭的繁荣景象远远超过了其想象。而如果我们从今天开始向后快进一百年，那种勃勃生机、万物竞发的情景也将同样令人难以想象。”</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://ia.samaltman.com/">https://ia.samaltman.com/</a>"</p><p><a href="https://arstechnica.com/information-technology/2024/09/ai-superintelligence-looms-in-sam-altmans-new-essay-on-the-intelligence-age/">https://arstechnica.com/information-technology/2024/09/ai-superintelligence-looms-in-sam-altmans-new-essay-on-the-intelligence-age/</a>"</p><p><a href="https://news.ycombinator.com/item?id=41628167">https://news.ycombinator.com/item?id=41628167</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/kMThQLtXZIYcvLJF0x7J</id>
            <title>广东首个国产TPU智算中心怎么建起来的？</title>
            <link>https://www.infoq.cn/article/kMThQLtXZIYcvLJF0x7J</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/kMThQLtXZIYcvLJF0x7J</guid>
            <pubDate></pubDate>
            <updated>Fri, 27 Sep 2024 10:10:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>9 月 9 日，广东地区首个采用国产 TPU 技术的智算中心成立。该项目一期由 32 个算力节点通过高效互联构建而成，后期将扩容至千卡规模，形成训推一体化的枢纽，成为中国联通在深圳的核心智算高地的重要组成部分。</p><p></p><p>深圳作为全国科技创新的前沿阵地，一直走在人工智能产业发展的前列。据深圳联通副总经理赵桂标介绍，此次中昊芯英与深圳联通联合进行的高性能 AI 智算中心项目合作，不仅是对国家智算能力布局要求的积极响应，也为深圳乃至全国的人工智能产业发展注入强劲动力。智算中心的主要客户一是政企客户、二是工业制造方面的客户、三是金融客户、四是医疗客户。</p><p></p><p>那么，这样一个重要的智算中心是怎么建成的？其中有两个重要组成部分：TPU 芯片和算力调度。</p><p></p><p>TPU 架构，专为 AI 深度学习设计。相较于 CPU 的 if else 类的逻辑开销，GPU 用于光线追踪的计算开销，TPU 则专注于针对深度学习的主要计算方式（如非线形计算）进行硬件优化设计，这些特定的计算方式和硬件算子是 CPU 和 GPU 所不具备的，这就使得 TPU 在用于 AI 深度学习时更有算力性能优势。而这一性能优势在集群层面更甚，多 TPU 芯片系统的构建方式，也是针对深度学习在模型训练和推理过程中所需要的数据流特征，而构建的专用的网络形态和网络基础架构。这样的网络形态没有向前兼容的负担，所以它比英伟达的 NVlink 更适合跑大模型的应用。无论是单芯片还是系统级，TPU 芯片都有特定的技术路线优势来实现 AI 场景中进行算法运行时的算力性价比的巨大提升。</p><p></p><p>本次智算中心的 AI 计算底座选择了搭载中昊芯英自主研发的高性能 TPU 架构 AI 芯片“刹那®”的人工智能服务器及大规模 AI 计算集群系统“泰则®”。中昊芯英创始人兼 CEO 杨龚轶凡表示，“我们想把 TPU 架构做成 AI 界的 X86。”</p><p></p><p>同样是由前谷歌的 TPU 团队核心成员创办的 Groq，最近推出了新的 AI 加速芯片 LPU。杨龚轶凡解释称，从 Groq 的论文中可以看出 LPU 就是类 TPU 架构，本质上和 TPU 没有太大区别。杨龚轶凡曾在 Google 负责 TPU 芯片研发工作，也曾在 Oracle 参与、主导过 12 款高性能服务器级别 CPU 芯片的设计与研发，中昊芯英核心研发团队成员都是一批来自于谷歌、微软、三星、甲骨文的 AI 软硬件设计专家，具备从 28nm 到 7nm 各代先进制程工艺下大芯片设计与优化完整方法论。</p><p></p><p>杨龚轶凡说：“芯片的设计研发的确是集结了很多人心血的系统化工程，也是集结了人类社会最顶尖生产工艺的过程。在这个过程中，耗费的人力和脑力是很多的，经历的时间周期也很长。这也是为什么中昊芯英第一代芯片产品“刹那®”经历了 4 年半的设计和生产周期才能完成，它其实没有一个真实世界的对应参照物做验证，大部分的东西都是在想象和想象的过程中完成。但是当有了第一代芯片，之后的迭代就会顺畅些。”</p><p></p><p>关于智能算力落地应用，杨龚轶凡表示，芯片与系统集成的挑战尤为显著。随着芯片数量的激增，通信效率成为一大难题。协调难度骤增、背景噪音干扰严重、信息传递效率急剧下降……如何设计高效的信息交换协议与物理链路，从而实现千到万乃至十万级别核心间的顺畅交流，成为亟待解决的技术瓶颈。</p><p></p><p>而 TPU 以独特的片间互联能力展现出强大的可拓展性优势。它能够轻松实现千片以上芯片片间互连，形成数据网络，并支持节点间的灵活交互与通信。这一特性使得 TPU 在构建大规模集群时更为简便，谷歌第 6 代 TPU 已能内部连接 16000 个芯片，无需依赖外部以太网，为万卡至百万卡级别的集群部署奠定了坚实基础。</p><p></p><p>此外，智算中心另外一个特点就是，智算中的单机密度和功耗密度越来越高，原来机架的功耗是 4 千瓦、6 千瓦、8 千瓦、20 千瓦，接下来可能 40 千瓦，功耗会越来越高。</p><p></p><p>对此赵桂标表示，对于能耗的控制主要依赖于两个方面：首先，在规划和建设层面，要大胆拥抱新技术，采用高效能的设备，比如液冷、磁悬浮冷机和模块化的电源等。通过采用高效能的设备利用自然冷源来降低能耗；其次，在运营管理层面上，不断积累精细化管控能耗的经验，持续优化、提升降低 PUE 来达到降低能耗目的。智算中心最后就是电力的竞争，不断降低能耗是整个行业要面对和不断攻克的问题。</p><p></p><p>中国联通以国家智算能力布局为导向，为推动全国范围内的人工智能产业发展，将深圳作为这一布局中的核心智算高地，同时，该项目将搭载联通云自研“星罗”算力管理平台，实现多元异构算力的适配和服务编排，形成“通算 + 智算 + 超算”的融合调度能力，可面向客户提供一体化的算力运营服务，也可用于企业私有化部署的智能算力网络搭建及运营管理。根据介绍，智算中心的优势在于算力的共享，避免单个企业因业务需求波动导致的算力闲置或不足问题，提高算力整体利用率，降低运营成本。</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Yd1RldcC4gZgPKGloH7C</id>
            <title>大模型让汽车“开窍”了：吉利汽车强势入场，3 年AI布局从车圈里“杀出来”</title>
            <link>https://www.infoq.cn/article/Yd1RldcC4gZgPKGloH7C</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Yd1RldcC4gZgPKGloH7C</guid>
            <pubDate></pubDate>
            <updated>Fri, 27 Sep 2024 07:21:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p></p><blockquote>出品｜InfoQ 《大模型领航者》访谈主持｜霍太稳，极客邦科技创始人兼 CEO访谈嘉宾｜陈勇，吉利汽车研究院人工智能中心主任作者｜褚杏娟</blockquote><p></p><p></p><p>“开车下班回家，路上可能没有什么事，那我可以跟大模型聊聊天，聊聊技术发展怎样、今天过得如何，可以做很多互动交流。我觉得这就是大模型上车后能够带来的一个方向转变。”吉利汽车研究院人工智能中心主任陈勇说道。</p><p></p><p>实际上，聊天只是开始。大模型一定程度上重新定义了汽车：汽车从原来的出行属性变成了社交属性，这背后带来的想象非常大。因此，大模型上车已经是车企的必答题，对于老牌车企吉利来说，亦是如此。</p><p></p><p>吉利汽车在 2021 年便开始策划自研大模型，当时的焦点还是基座模型，行业模型概念并不像现在一样成为共识。在各种不确定中，吉利汽车还是决定自己做大模型。</p><p></p><p>今年 1 月份，吉利便推出了全栈自研的全球首个汽车行业全场景 AI 大模型星睿大模型，该模型目前已经陆续应用在银河 L6、银河 L7 等车型中。从大模型研发到真正上车，吉利汽车用近 3 年的时间初步完成了这一探索。本期 InfoQ《大模型领航者》，吉利汽车向我们揭开了大模型上车神秘面纱的一角。</p><p></p><h3>做大模型的“基本功”</h3><p></p><p></p><p></p><blockquote>“之前几年对我们来讲，是一种历练和成长。”</blockquote><p></p><p></p><p>每个做大模型的开发者都要面临的难题是数据、算力、算法，吉利汽车也不例外。吉利汽车人工智能研发团队在构建自己的数据集上花费了很多精力。数据决定了大模型认知的天花板，而全面的数据才能训练出更加通用的模型，因此，这个数据集必须是高质量且全面的。“对于那个时候的我们来讲，构建数据集是非常难的一件事情，”陈勇坦诚道。</p><p></p><p>那么，大模型训练所需要的大量数据从哪里来？</p><p></p><p>以智能驾驶为例，智能驾驶的大部分数据是从实际道路上采集而来，但这种采集方式周期长、成本高、难度大。比如需要采集在上海下雪天的高架上前方有一辆车插进来的数据，这样的场景非常少难度也很高。</p><p></p><p>因此，吉利汽车开始思考利用大模型生成数据，前期用大量合成数据或虚拟生成数据训练模型，然后再用真实的道路数据做精调，来提升模型的准确率。这种方式在数据非常缺乏的产品开发冷启动阶段，也是适用的。</p><p></p><p>算力是模型研发的绝对制约因素。为解决这个问题，吉利大手笔构建了星睿智算中心。吉利的动作很快，从 21 年提出到 22 年建成，再到 23 年的正式揭牌，这个总投资 10 亿元、占地 52.12 亩、规划机柜 5000 架的智算中心，成为国内车企自建设备规模最大的智算中心之一。</p><p></p><p>根据官方披露的数据，星睿智算中心目前的云端总算力达 102 亿亿次每秒、通信网络传输速度达 800GB/s，存储带宽 4.5TB/s。结合算力调度管理算法和研发体系，吉利的整体研发效能得到了 20% 的提升，更是让星睿 AI 大模型训练速度直线提升 200 倍。</p><p></p><p>算法方面，吉利汽车组建了自己的算法团队，团队初期也踩了很多坑，比如在算法架构、加速框架的选择上就做了很多尝试，因为当时业内并没有确定的适用框架，团队需要不断试错找到最合适自己的。这个过程中，吉利汽车团队也与一些高校和机构开放合作，更快速地进行验证。</p><p></p><p>但在数据、算力和算法这大模型三要素之外，陈勇表示，更重要的是要把汽车行业的经验数据融入到大模型里，让大模型应用能够更符合业务发展需求。</p><p></p><h3>汽车模型还可以更“垂”</h3><p></p><p></p><p></p><blockquote>“可能我们会因为一个声音喜欢上一个车。”</blockquote><p></p><p></p><p>“虽然现在大模型企业很多，但 base 模型可能不需要那么多，垂类模型的需求会更多一点。”陈勇表示。</p><p></p><p>吉利本身做的是汽车行业的大模型，但在这个垂类下面，吉利做了更细化的分类。吉利星睿 AI 大模型可以看作是一个综合的模型平台，其中包含了语言大模型、多模态大模型、数字孪生大模型 3 大基础模型，并由此衍生出 NLP 语言大模型、NPDS 研发大模型、多模态感知大模型、多模态生成大模型、AI DRIVE 大模型、数字生命大模型 6 大能力模型。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a9/a97824fc2d6bb324c26aa99e55c281e9.png" /></p><p></p><p>“做垂类的模型，还是要结合场景去做。”陈勇表示。</p><p></p><p>以智能座舱为例，之前的语音回应，都只是很简单的“好的”等，但如今用大模型做语音交互后，人们可以跟系统聊天，这时对语音技术要求变高了：人们会希望它声色像真人、说话有感情等，也就是说，车里的情感交互变得十分重要。</p><p></p><p>吉利汽车团队今年 4 月发布的 HAM-TTS 语音合成大模型，就可以合成自然流畅、富有情感的语音，并根据声音样本复刻出逼真的声音。语音交互模型的迭代优化需求来自用户，毋庸置疑也是未来吉利重点推进的方向。</p><p></p><p>陈勇还提到了另一种垂类模型，就是可以识别特定场景下的用户意图，比如车上有小孩子睡着时，空调应该设置成什么样等等。汽车摄像头可以识别到车内人的状态等“多模态”输入，都是大模型理解的信息来源。</p><p></p><p>在汽车中构建各种垂类模型并落地应用也是当前汽车行业重点思考的方向。</p><p></p><p>而在模型的部署上，云端模型协同已经成为业内共识。云端模型协同使用一方面可以让系统运行得更好，另一方面也更具有经济价值。“现在的大模型不经济，训练出一个模型要花很多钱。”陈勇建议要理性消费大模型。“不是说大模型来了后就替代了原来所有的东西，一个产品的定位技术要跟它的产品定位匹配。”</p><p></p><p>具体实践中，根据吉利汽车的经验，高频、高价值、低时延的需求可以部署在端侧，一些丰富生态的需求则可以在云端部署。比如，像智能驾驶动力系统这种对时间响应要求比较高的需求就适合端侧，而娱乐性的需求更适合云端。另外，端侧更适合做实时推理，云端更适合离线训练，云端训练完可以下传到本地，让本地端做车云协同。</p><p></p><h3>自己先成为“受益人”</h3><p></p><p></p><p></p><blockquote>“用大模型讲笑话那就只能是讲笑话的钱。”</blockquote><p></p><p></p><p>虽然吉利汽车是大模型的提供方，但首批获益的人可能并不是其用户，而是吉利汽车自己。</p><p></p><p>在陈勇看来，好的产品是企业的竞争力，好的组织也是企业的竞争力。“产品都比较卷，推出一个新产品后市场上会快速出现一批类似产品，可模仿性比较强。但是一个组织的竞争力是很难被模仿和替代的。”陈勇说道。</p><p></p><p>“如果利用 AI 加强我们的组织竞争力，用 AI 文化、AI 流程、 AI 体系、 AI 开发等系列逻辑体系大大提升我们的效率，那这个组织的竞争力构建起来后，可复制性的难度是很大的。”陈勇进一步补充道。他认为，这种竞争力相当于每个人底层的逻辑和文化，难以被拿走和替代。</p><p></p><p>基于此，吉利汽车构建了星睿智能体平台来提升组织效能，这个平台上有几百个面向不同场景的智能体，包括办公领域、产品定义、营销领域、软件开发等。吉利汽车的员工可以使用智能体应用来更好地完成工作任务。</p><p></p><p>比如，原来的一些开发工作，工程师要自己写代码、做代码注释，还要去做测试等。现在可能大模型可以帮着生成一些代码，然后工程师检查是不是满足要求，不满足稍微再改一改，这样的话就提升很多效率了。</p><p></p><p>“我们的员工可以解放更多的精力，专注把产品设计、产品创新、产品体验做得更好。”陈勇说道。</p><p></p><p>陈勇对大模型的期待，不止于此。</p><p></p><p>“大模型刚出来的时候，大家思考人工智能会不会替代我们的工作，但我觉得人工智能其实是让我们变得更美好、更幸福。”</p><p></p><p>在陈勇看来，未来人机协同、人机共存会变成一种常态，机器可以辅助我们去做很多事情。其中，具身智能是陈勇个人比较看好的方向。</p><p></p><p>“之前，大家更强调机器人的运动能力，但现在大模型相当于给机器人加上了一个大脑，有五官的感知能力，然后会运动、会思考决策，这种状态就是具身智能。”陈勇说道。</p><p></p><p>在陈勇看来，具身智能未来会慢慢走进人们的生活，尤其是家庭陪伴。“现在全球的老龄化程度越来越严重，有了大模型加持的具身智能作为陪伴，体验会更好一些。”另外，在工业场景中，尤其是在重体力、重复性和充满危险的工作环境中，具身智能完全可以替代人类劳作。</p><p></p><p>审视自己所处的行业，陈勇认为，汽车一定程度上也可以看作是具身智能：汽车本身有能源、有感知系统，加上大模型也会思考做决策，这就是具身智能的一个雏形。</p><p></p><p>当把思维放得更开一些，陈勇还设想“大脑 + 脑机接口 + 大模型”这种双脑协同工作，两个思想可以随时碰撞，可能让人机协同的融合度更高。</p><p></p><h3>结束语</h3><p></p><p></p><p>大模型上车后，整个行业都在等一个现象级产品的出现，但现在没人知道是什么。陈勇预计，这样一款现象级产品可能要两、三年后才会出现。</p><p></p><p>如今，大模型上车更多还是解决旧场景里的旧问题，车企们正在思考如何在新场景解决新问题。但汽车行业的路就是这样一步步走出来的。大模型能够上车的一个重要前提是，行业自身已经完成了动力结构从燃油到新能源的转变，但在新能源汽车来临时没有人能设想到大模型上车。如今大模型上车了，也没人能预测出未来到底会给汽车带来哪些变革。</p><p></p><p>谁能率先交出大模型上车的答卷，我们拭目以待。</p><p></p><p>华卫对此文亦有贡献。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/iARi98XQCDLPVvmZIWWk</id>
            <title>最高配128核！英特尔至强6性能核处理器发布：运行Llama2-7B 快了3.08倍</title>
            <link>https://www.infoq.cn/article/iARi98XQCDLPVvmZIWWk</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/iARi98XQCDLPVvmZIWWk</guid>
            <pubDate></pubDate>
            <updated>Fri, 27 Sep 2024 07:11:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>“英特尔至强6性能核，将是英特尔有史以来性能最强大的至强处理器。”英特尔数据中心与人工智能集团副总裁、中国区总经理陈葆立在9月6日的发布会上说道。</p><p>&nbsp;</p><p>英特尔本次宣布上市的至强6900P系列处理器（代号Granite Rapids-AP），最高配备128个内核，支持高达每秒6400MT的DDR5内存、每秒8800MT的MRDIMM内存、6条UPI 2.0链路（速率高达每秒24 GT），96条PCIe 5.0或64条CXL 2.0通道、504MB的L3缓存，支持FP16数据格式的英特尔®&nbsp;高级矩阵扩展（英特尔®&nbsp;AMX），可为AI和科学计算等内存带宽敏感型工作负载提供MRDIMM选择，且新增对CXL 2.0的支持。</p><p></p><p>&nbsp;英特尔至强6900P系列具备三个计算单元和两个IO单元，其中计算单元里包含了最重要的X86内核、内存控制器和缓存，I/O单元里面包含了PCle、CXL、UPI等通用协议，也包括了英特尔独有的加速器。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/c0/c019b5e6e0d993827bc9d7ac15b026d2.png" /></p><p></p><p>&nbsp;</p><p>“性能装备从64核到128核，单核性能1.2倍提升，每一个核都比以前更快。上一代平台所需电量是350瓦，这一代需要更多的供电（500瓦），但我们在增加30%功耗的情况下，算力却拥有了双倍提升。所以我们相信，在综合考量范围下，我们能够跟厂商打造一个更新、更强大、更高效的平台，并帮助最终用户降低30%的TCO。”陈葆立说道。&nbsp;&nbsp;&nbsp;&nbsp;</p><p>&nbsp;</p><p>英特尔至强6性能核的处理器测试运行了12种不同的常见工作负载，包括通用计算、数据库、科学计算、AI大模型（包括Llama2，Llama3 ）等，结果显示，单颗CPU性能和每瓦特性能与上一代产品相比快了两倍以上。值得注意的是，70亿参数的Llama2 大模型推理在AMX加速下，至强6处理器相比前一代有了3.08倍的提升。</p><p></p><p><img src="https://static001.geekbang.org/infoq/35/35bcc8977f4afd230731f027ebf3a72d.png" /></p><p>&nbsp;</p><p>MRDIMM是业界备受关注的领先内存技术，利用组装其上的数据缓冲区，实现两个列的同步操作，从而允许一次向 CPU 传输 128 字节的数据，而传统 DRAM 模块一次传输 64 字节。英特尔至强6性能核处理器，一个是使用标配6400MT/s，一个是使用更快的MRDIMM内存，对内存非常敏感的工作负载，包括科学计算、AI等，有1.2-1.3倍的提升。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/92/920bb746e2abedabf47f6fe3c81f48a3.png" /></p><p></p><p>&nbsp;另外，CXL也是英特尔发起的一个全新的技术，通过CXL扩展可以在数据库或者大内存的场景里支持更多、更大的内存。当前，英特尔 CXL 2.0 支持多种设备类型（Type 1、Type 2 和</p><p>Type 3）且可向后兼容 CXL 1.1；支持链路分叉 (link bifurcation)，即使一个主机端口</p><p>可以对接多个设备；提供更强的 CXL 内存 (Type 3) 分层支持，可实现容量和带宽扩展，同时支持以受控热插拔的方式添加/移除设备。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/CO7aCwGvJLU6u8kSy8XD</id>
            <title>更快速度更高质量！开发代办事项 API ，看 Amazon Q 加速软件开发！</title>
            <link>https://www.infoq.cn/article/CO7aCwGvJLU6u8kSy8XD</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/CO7aCwGvJLU6u8kSy8XD</guid>
            <pubDate></pubDate>
            <updated>Fri, 27 Sep 2024 07:05:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>软件开发团队一直在寻求加速软件开发生命周期（SDLC）的方法，以更快地发布高质量软件。作为一款由生成式 AI 驱动的助手，<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;可以帮助软件开发团队在 SDLC 的各个阶段中实现更高效的产出。</p><p>软件开发团队在分析需求、构建、测试和运维应用程序时，往往会在一些非核心任务上花费大量时间。基于亚马逊云科技 17 年相关专业知识进行训练的&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;，可以革新您在亚马逊云科技上构建、部署和运维应用程序的方式。通过自动化常规任务，<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;让开发团队能够把更多时间投入到创新和研发当中。<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;可以加速<a href="https://aws.amazon.com/cn/getting-started?trk=cndc-detail">新手入门</a>"，减少上下文切换，以及加速亚马逊云科技上应用程序的开发。</p><p></p><p><img src="https://static001.geekbang.org/infoq/35/354d3dd85adc260a4f164e00016e26fa.png" /></p><p></p><p>本文将以开发一个待办事项的 API 接口项目为示例，讲解如何使用&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;来加速 SDLC 的各个阶段。我们将利用&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business&nbsp;以及&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer&nbsp;来帮助实现该项目。我们将介绍&nbsp;<a href="https://aws.amazon.com/cn/q/business/?trk=cndc-detail">Amazon Q Business</a>"&nbsp;在规划和研究阶段的常见用法，以及&nbsp;<a href="https://aws.amazon.com/cn/q/developer/?trk=cndc-detail">Amazon Q Developer</a>"&nbsp;在研究、设计、开发、测试和维护阶段的应用。</p><p></p><h3>计划</h3><p></p><p>作为产品负责人，需要花费大量时间进行需求分析和创建用户故事，同时还需要研究内部文档，如功能规格说明书和业务需求，以了解所需的功能和目标。手动筛选文档是一项耗时的工作，而现在可以利用&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business 快速从内部文档或 Wiki （如 Confluence）中提取相关信息。</p><p></p><p><img src="https://static001.geekbang.org/infoq/22/226d676eebdedb5dcaf6b17975f3d83b.png" /></p><p></p><p><a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business 可以快速地和您的业务数据、业务信息和业务系统进行连接，让您可以进行定制对话、解决问题、生成内容并采取与业务相关的行动。<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business 提供超过&nbsp;<a href="https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/supported-connectors.html?trk=cndc-detail">40 个内置连接器</a>"，可连接流行的企业应用程序和文档存储库，包括&nbsp;<a href="https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/s3-connector.html?trk=cndc-detail">Amazon S3</a>"、Confluence、Salesforce&nbsp;等，让您只需少量的配置即可创建生成式 AI 的解决方案。<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business 还提供了与第三方应用程序交互的<a href="https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/plugins.html?trk=cndc-detail">插件</a>"。这些插件支持读写操作，可帮助提高终端用户的生产力。</p><p>因此，您不需要深入研究内部文档，只需使用自然语言向&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business 询问需求，它就会立即为您提供相关信息，并帮助简化任务和加速问题解决。</p><p>以我们的待办事项 API 接口项目为例，假设业务需求记录在 Confluence 中，而 Jira 用于任务管理。您可以分别通过 Confluence 连接器和 Jira 插件，使&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business 与 Confluence 和 Jira 连接。为了了解需求，您可以询问&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business 关于用例概述、业务驱动因素、非功能性需求等相关问题。<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business 会从 Confluence 文档中提取相关细节，并以清晰简洁的方式呈现给您。这样可以节省收集需求的时间，让您更专注于用户故事的开发。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2b/2b8c9d709610b18a444ee713cc4c3bb7.png" /></p><p></p><p>在充分理解需求之后，您可以要求&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business 为您撰写用户故事，甚至直接在 Jira 中为您创建相应的任务。对于本文的 API 接口项目，<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business 会根据需求量身定制生成用户故事，并在 Jira 中为您创建对应的待办事项，为您的团队节省时间，确保项目工作流程高效运转。</p><p></p><p><img src="https://static001.geekbang.org/infoq/10/10594e43da0a759996b5bbfcd6763173.png" /></p><p></p><p></p><h3>研究和设计</h3><p></p><p>假设上述用户故事被分配给您，您需要根据 Confluence 页面中描述的技术栈来实现它。</p><p>首先，您可以询问&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business，了解组织开发指南中技术栈的相关信息。<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business 会立即从托管在 Confluence 上的内部开发指南文档中为您搜索相关详细信息，并附带参考资料和引用。</p><p>作为开发人员，您可以在集成开发环境（IDE）中使用&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 协助软件开发，包括代码解释、代码生成以及代码改进（如调试和优化）。<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 可以提供诸如分析需求、评估不同方法、创建实施计划和示例代码等协助。它还可以研究技术选型、权衡利弊、推荐最佳实践，甚至与您进行头脑风暴来优化设计。</p><p>让我们看看&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 是如何帮助分析用户故事、设计，并制定实施计划。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cb/cb9915edddfc672bea3200af428f8c9c.png" /></p><p></p><p>让我们在设计中进一步完善非功能性需求，如安全性和性能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/53/533704f728dcfe66fd217cd4137b377c.png" /></p><p></p><p></p><h3>开发和测试</h3><p></p><p><a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 可以根据您指定的业务和技术需求生成代码片段。您可以审查自动生成的代码，手动复制并粘贴到编辑器中，或使用选项“插入到光标处”直接将其合并到源代码中。这有助于您快速原型化和迭代应用程序的新功能。<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 使用对话的上下文来指导后续的响应，这使得您可以专注于构建应用程序，无需离开 IDE 即可获得答复和基于特定上下文的编码指导。</p><p></p><p><a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 对于回答以下领域的问题特别有用：</p><p>在亚马逊云科技上构建的相关问题，包括亚马逊云科技服务的选择、限制和最佳实践。通用的软件开发概念，包括编程语言语法和应用程序开发。编写代码，包括解释代码、调试代码和编写单元测试。使用<a href="https://aws.amazon.com/cn/q/developer/code-transformation/?trk=cndc-detail">用于代码转换的 Amazon Q Developer Agent</a>"&nbsp;升级和现代化现有的应用程序代码。</p><p>在&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 生成的用户故事设计的基础上，您可以要求&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 实现 API 接口，并根据其他要求和参数进行完善。让我们与&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 合作，将设计变成实现。您可以利用&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 的专业知识进行构思、评估选项，并得出最佳解决方案。<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 可以基于需求进行智能讨论，集思广益，创造新的测试用例。然后，它可以帮助构建实施计划，并高效地添加健壮、全面、以及对边缘例子覆盖度高的测试用例。</p><p>让我们要求&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 根据设计生成代码。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5e/5e8a002b2963c96900f361a9d1b41318.png" /></p><p></p><p>现在，让我们要求&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 实现&nbsp;<a href="https://aws.amazon.com/cn/lambda/?trk=cndc-detail">Amazon Lambda</a>"&nbsp;函数。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a1/a1d2329896915bcf44dde1335d3ef3dc.png" /></p><p></p><p><a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 可以提供代码示例和代码片段，展示如何实现设计。您可以审核生成的代码，获得&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 的反馈，并无缝地将其集成到项目中。与&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 的协作可以让您利用其知识快速迭代和丰富应用程序的功能，从而提高生产力。</p><p><a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 还可以审查代码，并根据性能和其他参数找到改进和优化的空间。让我们要求&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 为我们的示例项目找出需要改进的地方。</p><p></p><p><img src="https://static001.geekbang.org/infoq/51/51a912d0043c315baf9fa07461401e06.png" /></p><p></p><p></p><h3>调试和故障排查</h3><p></p><p><a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 可以在故障排查和调试方面为您提供协助。对于不熟悉的错误代码或异常类型，您可以要求&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 研究其含义以及常见的解决方案。<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 还可以通过分析应用程序的调试日志，标出任何异常、错误或警告，从而提示潜在的问题。</p><p><a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 可以对由错误配置导致的网络连接问题进行排查，提供简明的问题分析和解决建议。它还可以研究亚马逊云科技最佳实践，识别哪些地方与最佳实践不一致。对于代码问题，它可以在支持的 IDE 中回答问题和进行代码调试。利用其对亚马逊云科技服务及其交互的了解，<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 可以提供特定亚马逊云科技服务的指导。在亚马逊云科技控制台中，<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 可以对您在使用亚马逊云科技服务时所收到的错误（如权限不足、配置不正确、超出服务限制）进行故障排查。</p><p>让我们使用命令行工具 cURL 通过访问&nbsp;<a href="https://aws.amazon.com/cn/api-gateway/?trk=cndc-detail">Amazon API Gateway</a>"&nbsp;的端点对我们的待办事项 API 进行测试。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ffc4f7d54a408fe6c7058fd3c250b13c.png" /></p><p></p><p>由于&nbsp;<a href="https://aws.amazon.com/cn/api-gateway/?trk=cndc-detail">Amazon API Gateway</a>"&nbsp;端点在调用 Amazon Lambda 函数在&nbsp;<a href="https://aws.amazon.com/cn/dynamodb/?trk=cndc-detail">Amazon DynamoDB</a>"&nbsp;表插入记录时抛出了内部服务器错误，让我们转到 Amazon Lambda 控制台进一步排查问题，并通过为 POST 方法创建测试事件直接测试该函数。在亚马逊云科技控制台中，您可以利用&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 对不同的控制台错误进行故障排查。对于上述错误，<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;会分析其问题并帮助找到解决方案。<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;将直接在控制台上解释如何通过添加<a href="https://aws.amazon.com/cn/dynamodb/?trk=cndc-detail">&nbsp;Amazon DynamoDB</a>"&nbsp;表名的环境变量来修复此错误。</p><p></p><p><img src="https://static001.geekbang.org/infoq/24/247f060fbe5289a355861b7659e8de0f.png" /></p><p></p><p>现在，让我们在 IDE 中要求&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 生成代码来修复这个错误。<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 随后会生成一个代码片段，用于在&nbsp;<a href="https://aws.amazon.com/cn/cdk/?trk=cndc-detail">Amazon CDK</a>"&nbsp;中为 Amazon Lambda 函数设置所需的环境变量。</p><p></p><p><img src="https://static001.geekbang.org/infoq/30/305f98f8949f31877de3e62db63abd51.png" /></p><p></p><p></p><h3>总结</h3><p></p><p>相信通过本文的介绍，您能够了解到如何利用&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business 和&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 来简化软件开发生命周期，从而加快产品发布速度。凭借对代码和亚马逊云科技资源的深入理解，<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 使开发团队能够在研究、设计、开发、测试和审查等阶段高效工作。通过自动化常规任务、提供专家指导、生成代码片段、优化实现代码和故障排查，<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 让开发人员可以将注意力重新集中在推动创新的高质量的工作中。此外，通过&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business，团队可以借助生成式 AI 的力量，加快需求规划和研究阶段的进度。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/kUiU3VOE9N4lId0fwChz</id>
            <title>AI芯片或面临新一轮短缺，首席信息官们如何提前布局应对？</title>
            <link>https://www.infoq.cn/article/kUiU3VOE9N4lId0fwChz</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/kUiU3VOE9N4lId0fwChz</guid>
            <pubDate></pubDate>
            <updated>Fri, 27 Sep 2024 06:53:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>根据咨询研究机构贝恩公司本周发布的一份报告，随着 <a href="https://aicon.infoq.cn/202412/beijing/">AI </a>"计算需求的激增，数据中心芯片、个人电脑和智能手机的供应链将面临重大压力。其指出，持续的地缘政治紧张局势和其他供应风险可能会导致下一轮半导体短缺。</p><p></p><p>半导体的供需是一个微妙的平衡，过去几年的经历让业界对此深有感触。在此背景下，贝恩公司呼吁各方密切关注半导体供应链的复杂性——“当需求增加约 20% 或更多，很有可能打破平衡，导致芯片短缺’。”</p><p>从报告来看，其关键观察有以下几点：</p><p></p><p>数据中心及其专用芯片的支出依然强劲，主要云服务提供商预计在 2024 年的资本支出将同比增长 36%，这一增长主要源于对 AI 和加速计算的投资。如果数据中心对当前一代图形处理单元（GPU）的需求到 2026 年翻倍——鉴于当前的趋势，这是一个合理的假设，那么关键部件的供应商在某些情况下需要将产量增加 30% 或更多。为促进 AI 的增长，必须在建设数据中心、晶圆厂、先进封装技术和电力保障等方面整合复杂的供应链要素，确保获得先进的封装技术和充足的电力。</p><p></p><p>虽然报告的重点是购买芯片的组织需要做什么，但首席信息官们可以采取一些措施，以确保将来能获得所需的产品，或为价格剧烈波动做好准备。</p><p></p><p>Info-Tech 研究集团的研究主管 Scott Bickley 指出，先进的半导体供应链是全球最脆弱的供应链之一，必须有超过 5000 家供应商完美协作才能生产最先进的芯片。</p><p></p><p>他说，其中许多供应商“为单个公司供应单一的组件，如果没有它们，整个系统就会嘎然而止。单是技术障碍就令人瞠目结舌，更不用说台积电面临的地缘政治风险和物流管理的阻力了。”</p><p></p><p>Bickley 还表示，技术买家主要分为两类：一类是为大规模基础设施采购的买家，例如私有云环境...... 也可以说是财富 200 强规模的客户；另一类则是为小规模项目采购的买家，比如数据中心现代化、小规模的 LLM 内部模型，以及先进的 AI 功能 PC。</p><p></p><p>在私有云层面，Bickley 建议买家应立即制定技术战略。举例来说，你是否要大干一场，押注于英伟达下一代 Blackwell 系列 GPU，或选择第一代 H100 进行模型训练。数据中心基础设施的挑战不容小觑，尤其是在水冷环境和高密度 GPU 集群的设计上，以平衡能耗、性能和环境要求。</p><p></p><p>而传统企业环境中的技术买家面临的挑战则不同，他们由于规模较小，对供应商的影响力有限，在这种环境中，这些买家将不得不过度扩张，现在就下注，以确保以后的供应。“为生产延迟做提前规划可能需要买家承担一些昂贵的前沿技术产品库存，并且这些产品可能很快就会过时。”</p><p></p><p>Forrester Research 的高级分析师 Alvin Nguyen 补充道，谈到首席信息官可以做些什么来确保他们能够继续获得所需的产品，或者为价格的剧烈变化做好准备，他们需要考虑几个方面：</p><p>风险管理：Nguyen 说，生成式 AI 的进展速度以及对特定模型或方法的巨额投资，日后可能被证明是错误或非最佳的选择：“对于大多数希望利用 AI 而非推动 AI 市场发展的企业来说，规避风险，利用现有的 AI 服务，而不是大力获取大量 AI 基础设施，是最有意义的。”人员培训：首席信息官和技术高管“需要投资于 现有员工的培训 / 技能提升，以及为已知的可有效利用的 AI 用例（如代码开发）招聘具备基本 AI 技能的新人才。他们需要让他们的技术人员、架构师和工程师试验最新的 AI 技术，以确定他们需要做出的选择。如果你能够获取大量的 AI 基础设施，那么就在这里大量投资，以建立相对于他人的竞争优势。”AI 基础设施：目前 AI 加速器的需求超出供应，在未来几年内不太可能改变，因此 AI 加速器 /GPU 目前会有溢价。因此，企业可以考虑利用云服务提供商的 AI 服务。可持续性：生成式 AI 对更多能源和水资源的需求，以及其碳足迹，已经影响了一些组织实现其可持续性目标的能力。在对 AI 的需求持续增长的情况下，这种情况不太可能改变。因此首席信息官和技术高管需要从可再生能源中采购电力，并在可能的情况下采用可持续的建筑和运营实践（建筑材料的选择、施工方法、回收利用）。</p><p></p><p>参考链接：</p><p>https://www.cio.com/article/3540407/bain-warns-prepare-for-ai-chip-shortage.html</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Ifj6EXOfg6NjoN1lspMy</id>
            <title>英特尔 28 年老员工崩溃控诉：公司文化烂透了！员工数 5 倍于对手、市值却不及5%，何至如此？</title>
            <link>https://www.infoq.cn/article/Ifj6EXOfg6NjoN1lspMy</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Ifj6EXOfg6NjoN1lspMy</guid>
            <pubDate></pubDate>
            <updated>Thu, 26 Sep 2024 08:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>整理 ｜ 华卫、核子可乐</p><p></p><p>近日，美国芯片制造商英特尔一直处于风口浪尖上。</p><p></p><p>先是其要被高通收购的消息传得沸沸扬扬，后有私募机构试图来“救火”，美国资产管理公司 Apollo Global Management 提出愿意向英特尔投资几十亿美元，最多可达 50 亿美元。目前的最新进展是，高通未进一步向英特尔发出收购要约，英特尔和 Apollo 的交易也还在初期接触阶段，未达成任何结果。</p><p></p><p>就在今早，又有一位在英特尔工作 28 年、刚刚退休的项目经理 Lynn Coffin 发帖“控诉”了英特尔的企业文化，称“在 Pat 的领导下，公司的文化已经彻底腐烂了”。他在帖子中提到，英特尔内部频繁的重组、有害的工程经理、办公室政治和冗长而令人精疲力竭的夜间会议让人身心俱疲，有太多层级的有害管理者正在扼杀创新和进步。</p><p></p><p>全文如下：</p><p></p><p></p><blockquote>28 年了，和许多人一样，我很幸运地有资格享受退休金，并花了一些时间进行反思。我在英特尔工作多年，发现它的环境经常阻碍我的成长和幸福。频繁的重组、有害的工程经理、办公室政治和冗长而令人精疲力竭的夜间会议，让我身心俱疲。我很高兴现在能够专注于真正符合我激情的目标。对我来说，英特尔一直是一种达到目的的手段——一个养家糊口的稳定地方。我在这里工作期间遇到了我的丈夫、结婚并抚养了我们的女儿，为此，我很感激。但这始终只是一份工作，从来都不是灵感或目标的源泉。虽然我有幸与杰出的人一起工作，但工作场所已经发生了巨大的变化。我见证了许多变化，从 Andy Grove 开始，公司就完全不同了，但在 Pat 的领导下，公司的文化已经彻底腐烂了。透明度、动力和尊重都消失了，这主要归咎于高管领导层。太多层级的有害管理者正在建立封建领地，扼杀创新和进步。我记得在 2022 年与 Gordon Moore 的一次会议上，他告诉 Pat：“别搞砸了。”可悲的是，他的建议似乎被忽视了。像我这样的项目经理经常被忽视和削弱。Pat 称我们为"checkers"的评论是不幸的，也是误导性的。项目经理是项目的粘合剂，但多年来我们的价值一直被低估。我亲眼目睹了这种缺乏支持的情况是如何拖慢了英特尔的进展，并导致其错失良机的。</blockquote><p></p><p></p><p>在评论区里，疑似也是英特尔前员工的 Fernando J Santiago-Avila 表示对 Coffin 的发言深有同感：</p><p></p><p></p><blockquote>英特尔对我来说也是实现目标的一种手段。我先在美国工作，一年后站稳脚跟，再回到波多黎各。新工作（第一份！）开始 3 个月后，英特尔关闭了他们在波多黎各的工厂。实际上，我把英特尔当成了我的 “奇思妙想 ”项目：以某种方式让一家硬件公司关注其软件实践，尤其是测试。当然，当你的总经理把你和所有其他软件开发人员召集到一个小房间里，然后说：“你们最好干点别的，因为我现在就可以去钱德勒街挑选 10 名开发人员”时，我这个不切实际的人决定去一个重视软件工程的地方… 于是我去了 MS。</blockquote><p></p><p></p><p>这一切似乎都透露出，英特尔当下的内部运营状态和财务情况令人堪忧。</p><p></p><p></p><h1>5 倍于对手的团队规模，“只有偏执狂才能生存”的精神远去</h1><p></p><p></p><p>对英特尔的内部文化有意见的，并不止几位普通员工，还有董事会的高管层因此发生变动。</p><p></p><p>今年 8 月，据三位知情人士介绍，英特尔公司某位重要董事会成员突然辞职，原因是他与英特尔 CEO Pat Gelsinger 及其他董事对于臃肿的员工队伍、规避风险的保守文化以及落后的 AI 战略产生了分歧。</p><p></p><p>半导体行业资深专家陈立武（Lip-Bu Tan）在提交的监管文件中表示，他离开董事会的原因是其个人希望“重新整理承诺事务的优先顺序”，而且将仍然“支持英特尔公司及其重要工作”。据未获授权公开讨论此事的消息人士介绍，随着时间推移，陈立武对于英特尔公司庞大的员工队伍、按合约制造的僵化方式以及英特尔规避风险且官僚主义严重的企业文化感到失望。</p><p></p><p>这位芯片软件公司 Cadence Design 的前任 CEO 于两年之前加入英特尔董事会，作为恢复英特尔全球领先芯片制造商地位计划的重要一员。英特尔董事会还于 2023 年 10 月扩大了陈立武的职责，授权他督导芯片制造业务。陈立武的退出，导致英特尔董事会当中出现了芯片行业技术与商业指导的真空。投资者和半导体行业内部人士证实，英特尔董事会成员包括学术界和金融界的多位领袖，也有来自医疗、科技及航空航天行业的前任高管。</p><p></p><p>在陈立武和一部分前英特尔高管看来，目前公司的员工队伍似乎过于臃肿。据两位消息人士介绍，某些项目的团队规模已经达到 AMD 等竞争对手中从事类似项目团队的 5 倍。多位英特尔前任高管也表示，英特尔的员工人数比英伟达和台积电两家公司的员工总和还要多，这导致了一种自满且缺乏竞争力的文化氛围，与英特尔联合创始人 Andy Grove 一直强调的“只有偏执狂才能生存”的精神相去甚远。</p><p></p><p>作为重振计划的一部分，Gelsinger 于 2021 年上任，到 2022 年至少又为英特尔增加了 2 万名员工。根据今年 8 月公布的财务报告，英特尔在全球拥有近 12.53 万名员工。</p><p></p><p>一位英特尔前高管坦言，英特尔 8 月就应该把裁员比例再提高 1 倍。据介绍，英特尔自 2010 年以来已经收购了至少两家 AI 初创公司，希望打造出轰动一时的 AI 芯片。尽管收购 Habana 带来了前景光明的 AI 芯片方案，但随着该公司高层领导班子辞职并前往以色列建立新的同行企业，英特尔的项目也遭受到沉重打击。为了削减成本，英特尔今年 8 月宣布裁员 15% 以上，这是最近两年来的第二轮裁员。</p><p></p><p>据消息人士称，裁员计划也是陈立武同英特尔董事会之间紧张关系的一大根源。陈立武主张开展更有针对性的裁员，特别是裁撤那些已经不再为英特尔工程工作做出贡献的中层管理人员。陈立武明确表示，他认为英特尔已经被代表官僚体系的中层管理人员所吞没，这群人阻碍了英特尔服务器和台式机芯片部门的发展，所以裁员的大刀就应该向这帮人的头上砍去。</p><p></p><p></p><h1>剥离代工业务，裁减超 7500 名员工</h1><p></p><p></p><p>“英特尔正在经历其五十年发展史中最为惨淡的时期之一，也面临着激进派股东的猛烈攻击。”多位英特尔前任高管对外证实称。</p><p></p><p>8 月，英特尔在公布业绩并计划减少工厂建设资本支出时，暂停了数十年来一直支付的股息。第二天，投资者们用实际行动将这个芯片巨头的市值拉低了 300 多亿美元，缩水幅度超过四分之一。而与此同时，其竞争对手正乘着 AI 技术热潮大举投资和疯狂夺取市场份额。此前有消息称，英伟达已成为一家市值 3 万亿美元的 AI 芯片主导供应商；相比之下，英特尔的市值现已跌至 1000 亿美元以下。2018 年，英特尔还曾放弃了收购 ChatGPT 制造商 OpenAI 高达 30% 股份的机会。</p><p></p><p>9 月初，据知情人士透露，英特尔预计将在本月晚些时候向公司董事会提交一项计划，将包括通过出售可编程芯片部门 Altera 等多条业务线以削减总体成本，意味着英特尔无法从其曾经可观的整体利润中为这些业务抽调经营资金。</p><p></p><p>9 月 16 日，Gelsinger 在英特尔最新的股东大会之后发布了一封全员信，宣布英特尔正在采取措施将其芯片代工部门 Intel Foundry 转变为独立的子公司，还将“根据预期的市场需求”暂停其在波兰和德国的芯片制造项目两年，并考虑撤回其在马来西亚的芯片封装和测试业务。</p><p></p><p>据四位熟悉英特尔代工制造业务的消息人士介绍，如果没有 Tower Semiconductor，英特尔这家历来拥有芯片自产能力的公司根本拿不出与外部客户合作的专业知识，因此很难得到市场用户的青睐。去年，英特尔曾计划以 54 亿美元收购以色列芯片制造商 Tower Semiconductor，借此打入代工制造市场。但由于中方阻止了这笔交易，导致收购被迫搁置。</p><p></p><p>还有一位熟悉陈立武的消息人士反映，陈立武曾提出“让代工制造业务更多以客户为中心、同时消除不必要的官僚作风”的建议，但英特尔董事会没有听从。</p><p></p><p>此外，Gelsinger 还在全员信中对英特尔今年夏天公布的全球裁员计划作了回应。他表示，英特尔通过自愿提早退休与离职方案在年底前裁减 1.5 万名员工的目标，目前完成度超过一半。公司将持续推进计划，将于 10 月中旬通知受影响员工。同时，英特尔已经制定了减少或退出全球约三分之二房地产的计划，以进一步降低运营成本。</p><p></p><p>参考链接：</p><p></p><p><a href="https://finance.yahoo.com/news/exclusive-intel-board-member-quit-143944877.html?guce_referrer=aHR0cHM6Ly93d3cubGlua2VkaW4uY29tLw&amp;guce_referrer_sig=AQAAAFSkqYSYU_yWcKQXtUCjGFM_1NMeGJhFzJK4IoCb-vwbFHg6OVJV8ZivBIp3R_eewAw5R24FpW3guy0d4jbyeyE0tRJRCrHXvBUPiE2n_yxXYDdmqaT7Kca5RWD9zFd-ZpHlyB8A-NljxG4KIrt_ViwcTbqGiZQLOS9TS-_v_uhv&amp;guccounter=2">https://finance.yahoo.com/news/exclusive-intel-board-member-quit-143944877.html?guce_referrer=aHR0cHM6Ly93d3cubGlua2VkaW4uY29tLw&amp;guce_referrer_sig=AQAAAFSkqYSYU_yWcKQXtUCjGFM_1NMeGJhFzJK4IoCb-vwbFHg6OVJV8ZivBIp3R_eewAw5R24FpW3guy0d4jbyeyE0tRJRCrHXvBUPiE2n_yxXYDdmqaT7Kca5RWD9zFd-ZpHlyB8A-NljxG4KIrt_ViwcTbqGiZQLOS9TS-_v_uhv&amp;guccounter=2</a>"</p><p></p><p><a href="https://www.reuters.com/technology/intel-ceo-pitch-board-plans-shed-assets-cut-costs-source-says-2024-09-01/">https://www.reuters.com/technology/intel-ceo-pitch-board-plans-shed-assets-cut-costs-source-says-2024-09-01/</a>"</p><p></p><p><a href="https://www.linkedin.com/posts/lynn-coffin-46a8532_goodbye-intel-from-the-last-in-line-activity-7244369621506498563-lknw/?utm_source=share&amp;utm_medium=member_desktop">https://www.linkedin.com/posts/lynn-coffin-46a8532_goodbye-intel-from-the-last-in-line-activity-7244369621506498563-lknw/?utm_source=share&amp;utm_medium=member_desktop</a>"</p><p></p><p><a href="https://techcrunch.com/2024/09/16/intel-inks-ai-chip-deal-with-aws-pauses-plans-in-poland-and-germany/">https://techcrunch.com/2024/09/16/intel-inks-ai-chip-deal-with-aws-pauses-plans-in-poland-and-germany/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/VRAAlq3wA7aM40NIaeht</id>
            <title>空间智能公司「极佳视界」完成连续三轮融资，致力从视频生成走向4D世界模型</title>
            <link>https://www.infoq.cn/article/VRAAlq3wA7aM40NIaeht</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/VRAAlq3wA7aM40NIaeht</guid>
            <pubDate></pubDate>
            <updated>Thu, 26 Sep 2024 03:03:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>近日，空间智能公司「极佳视界」宣布完成近5000万元天使及天使+连续两轮融资，此两轮融资由北汽产投、奇绩创坛、华民投、龙鼎投资、清智资本、PKSHA Algorithm Fund等知名财务和产业投资机构投资。此前极佳视界已完成数千万元的种子轮融资，由辰韬资本投资。</p><p></p><p>「极佳视界」是一家空间智能公司，致力于将视频生成提升到4D世界模型，赋予AI大模型对于4D空间的理解、生成、常识和推理的能力，实现4D空间中的交互和行动，走向通用空间智能。通用空间智能对于影视游戏、元宇宙等虚拟空间的内容创作，以及自动驾驶、具身智能等物理空间的数据生成和认知推理能力，都有巨大的价值和作用。</p><p></p><p>在物理空间方向，极佳视界发布了全球首个物理世界驱动的自动驾驶世界模型——DriveDreamer系列，为自动驾驶、具身智能领域提供数据生成与闭环仿真等解决方案。目前公司已经签约定点合作多家头部主机厂，服务相关客户几十余家，携手行业加速走向物理世界通用智能。</p><p></p><p>在虚拟空间方向，极佳视界发布了中国首个模型原生16秒超长时长的视频生成模型「视界一粟YiSu」，并已与多家影视、游戏、电视台等行业客户展开深度合作，同时，也与多个AI影视创作公司达成了战略合作，携手共赴内容创作的新视界。</p><p></p><p>人工智能领域正经历爆发式的发展和深刻的变革，这场变革将深刻影响社会的各个层面，语言大模型已拉开通用智能的序幕，空间智能将是下一个革命性的事情。目前视频生成领域发展迅速，但在内容创作中依然存在物理不可控、时空不一致等亟待解决的本质问题；同时，具身智能方向则面临物理世界训练数据短缺、通用推理能力缺失等严重的制约因素。</p><p></p><p>极佳视界认为，将视频生成提升到4D世界模型，实现通用空间智能，是解决上述问题最有效的路径，对于虚拟空间的内容爆发，和具身智能的应用爆发，都具有决定性的作用。</p><p></p><p>「极佳视界」的创始人兼CEO黄冠是清华大学人工智能方向博士，拥有超过十年的AI经验，曾多次带领团队获得全球权威AI比赛世界冠军，发表多个知名AI成果；拥有AI和自动驾驶等方向连续创业经验，作为合伙人累计主导或参与融资数亿美金；同时还有微软、三星、地平线等知名企业丰富的产业经验，多次带领百人研发团队。</p><p></p><p>公司核心团队还包括前阿里云总监、前地平线数据闭环产品线总经理孙韶言博士；清华大学博士后、发表顶级AI论文超过50篇、引用超过1万次的朱政博士；前大厂T10架构师、前Apollo仿真技术负责人毛继明先生以及拥有丰富大模型算法和架构经验、获得多个AI世界冠军的陈新泽先生等业内顶尖专业人士。此外，还有多名驾驶具身、影视游戏等行业的资深产品和商务专家。</p><p></p><p>本轮融资后，极佳视界将继续加快技术研发和市场拓展。极佳视界希望通过世界模型和空间智能，打造新一代数据和内容引擎，携手创作者和开发者解锁新的视界。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ReaOufuNS4a9WwhAStss</id>
            <title>京东搜索重排：基于互信息的用户偏好导向模型</title>
            <link>https://www.infoq.cn/article/ReaOufuNS4a9WwhAStss</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ReaOufuNS4a9WwhAStss</guid>
            <pubDate></pubDate>
            <updated>Thu, 26 Sep 2024 02:07:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>本文入选顶会 SIGIR 2024，为你揭晓京东搜索重排多样性与效率平衡的解决方案！</p><p></p><p>京东零售搜推团队提出了一种基于互信息的偏好导向多样性模型（PODM-MI），该模型可使商品能够根据用户多样性偏好进行自适应排序，该模型可已在京东主搜全量部署，并在 UCVR 和多样性上均有显著收益。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ac/ac4ab8d73f8f3e09cdfeee32b7ffe2ad.png" /></p><p></p><p>SIGIR 24: A Preference-oriented Diversity Model Based on Mutual-information in Re-ranking for E-commerce Search</p><p></p><p>链接：<a href="https://xie.infoq.cn/link?target=https%3A%2F%2Fdl.acm.org%2Fdoi%2Fabs%2F10.1145%2F3626772.3661359">https://dl.acm.org/doi/abs/10.1145/3626772.3661359</a>"</p><p></p><p>摘要：重排是一种通过考虑商品之间的相互关系（上下午）来重新排列商品顺序以更有效地满足用户需求的过程。现有的方法主要提高商品打分精度，通常以牺牲多样性为代价，导致结果可能无法满足用户的多样化需求。而旨在提高多样性的方法可能会降低打分精度，无法满足商品打分精准性的要求。为了解决上述问题，本文提出了一种基于互信息的偏好导向多样性模型（PODM-MI），在重排过程中同时考虑准确性和多样性。具体而言，PODM-MI 采用基于变分推理的多维高斯分布来捕捉具有不确定性的用户多样性偏好。然后，我们利用最大变分推理下界来最大化用户多样性偏好与候选商品之间的互信息，以增强它们的相关性。随后，我们基于相关性得出一个效用矩阵，使商品能够根据用户偏好进行自适应排序，从而在上述目标之间建立平衡。该模型已在京东主搜全量部署，且在 UCVR 和多样性上均有显著收益。</p><p></p><h1>1、背景及现状</h1><p></p><p></p><p>在京东商城中，在用户从搜索到下单的过程中存在不同的决策阶段（买、逛等），用户不同的决策阶段对多样性也有不同需求。如下图所示，用户从搜索“连衣裙”到逐渐缩小范围到“荷叶边连衣裙”，这一过程中，他们的搜索意图从多样化逐渐变得明确和具体。</p><p></p><p><img src="https://static001.geekbang.org/infoq/41/4107dc421f3be944975949b82f982693.png" /></p><p></p><p>重排作为靠近后链路的一环，需要在效率指标和多样性之间找到平衡，并充分考虑用户的个性化需求。通常来说，用户的需求是多样化的，即在某些场景下，他们对排序结果的准确性要求较高，而在其他场景下，他们则更注重排序结果的多样性。因此，一个合适的重排算法应能够自适应地根据用户需求进行调整：当用户需要多样性时，搜索排序结果应包含尽可能多的不同商品，以满足用户的多种兴趣；而当用户需要准确性时，排序结果应优先展示最符合用户兴趣的单一类别商品。</p><p></p><p>在解决上述问题的过程中，我们面临两个主要挑战：</p><p></p><p>准确建模用户的决策意图：用户的意图是动态且复杂的，会在多次搜索和交互中逐渐演变。捕捉这种演变过程并准确建模用户的决策意图是一个关键挑战。</p><p></p><p>增强搜索结果与用户演变意图的匹配：即使我们能够成功建模用户的意图，如何确保搜索结果能够动态地与用户不断演变的意图相匹配也是一个难题。现有的排序算法通常固定在某种优化目标上，缺乏灵活性，难以在多样性和准确性之间进行自适应的权衡。</p><p></p><p>为了解决上述挑战，我们提出了 PODM-MI（基于互信息的偏好导向多样性模型）。</p><p></p><h1>2、PODM-MI</h1><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/a1/a1d013f21fb4f9242779ce61bc3f8c5d.png" /></p><p></p><p>PODM-MI 模型以待排序商品及其 sideinfo、用户行为流等作为输入。首先，我们使用 PON 捕捉用户的多样性偏好和候选商品的多样性表示。然后，SAM 增强用户多样性偏好与候选商品多样性之间的一致性。从这种增强的一致性中，我们得出一个效用矩阵，该矩阵会动态调整用户偏好，从而重新排序最终的排名结果以更好地满足用户需求。</p><p></p><h2>2.1 PON：用户偏好建模</h2><p></p><p></p><p>在京东的搜索场景中，历史 query 及其对应 session 的商品提供了用户意图随着 session 变化的重要表示。因此，我们的方法不仅包括常规的用户历史行为流，还加入了用户的 query 流，以更好地捕捉用户偏好。</p><p></p><p>在确定用户偏好建模特征之后，下一步是选择适当的建模方法来表示用户偏好。传统模型往往将用户偏好的动态性视为确定性的，在 embedding 空间中生成静态的用户嵌入。然而，这种方法在捕捉用户偏好的复杂性方面不够准确。相比之下，分布表示引入了不确定性，并提供了更大的灵活性。这种方法能够更好地适应用户偏好随时间和情境变化的复杂性。通过将用户偏好表示为一个概率分布，而非单一的固定嵌入，我们可以更准确地反映用户的多样化需求和行为模式。也有部分研究表明将用户偏好表示为分布而非嵌入具有显著的优越性。这些研究表明，分布表示不仅能够更好地捕捉用户偏好的动态变化，还能在实际应用中提供更高的预测精度和灵活性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/dd/dd3db941b74d7d7edd612124cfc0c551.png" /></p><p></p><p>因此，如上图所示，我们使用多维高斯分布来建模用户偏好的演变趋势。该分布由均值向量和对角协方差矩阵表征，使我们能够更好地捕捉用户偏好的动态特性。同样的，后续商品序列的多样性表征也用同样的方式进行表征。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ee/ee435989c8f297b92676606c5e8fb500.png" /></p><p></p><p>此外，高斯分布还可以用于测量收敛和发散趋势。较大的方差表示更均匀的分布，而较小的方差则表示更集中的分布。这个方差可以间接反映用户的偏好趋势。</p><p></p><h2>2.2 SAM 利用互信息优化排序结果</h2><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/3d/3d6e525d36dfa8f1c2d6b535eef7fa27.png" /></p><p></p><p>在建模用户偏好和候选商品的多样性之后，下一步是确保排序结果与用户意图紧密匹配。为此，我们可以使用互信息（一种衡量两个变量之间共享信息量的方法）来量化候选商品与用户偏好之间的相关性。通过最大化这两个因素（用户偏好和商品多样性）之间的互信息，我们确保候选商品的分布与用户意图的分布紧密对齐。</p><p></p><p>然而，直接估计和最大化互信息通常是不可行的。为了解决这一挑战，我们借鉴了变分推理的文献，引入了一个变分后验估计器。该方法允许我们为互信息目标推导出一个可行的下界，具体推导过程如下，这儿不再赘述。</p><p></p><p><img src="https://static001.geekbang.org/infoq/72/72ae4297c0db33a27446a04754f6f004.png" /></p><p></p><p>在增强一致性之后，我们设计了一个可学习的效用矩阵，以进一步使最终的排序结果与用户偏好对齐。通过调整矩阵的数值，我们可以控制不同商品和用户趋势在排序过程中的相对重要性。例如，如果某些商品更符合用户偏好，我们可以调整矩阵的数值，使其在排序过程中占据更大的权重。这种方法允许根据用户意图自适应地调整排名结果。</p><p></p><h2>2.3 优化函数及最终 loss：</h2><p></p><p></p><p>优化函数：</p><p><img src="https://static001.geekbang.org/infoq/df/df45f82fb554f3d6618dbbb439823797.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b32a681a1656fc6d72add28be0eb362e.png" /></p><p></p><p>最终 loss：</p><p>L_{total} = \lambda _{1}L_{CE}+\lambda_{2}L_{MI}</p><p>前者是 prm 分类 loss，后者是互信息 loss</p><p></p><p>方案总结：</p><p></p><p><img src="https://static001.geekbang.org/infoq/3e/3e75217b06d0d3be19a19e92ef9e3966.png" /></p><p></p><p></p><h2>2.4 实验结果及可视化分析</h2><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/66/660bbcbf0e11e3528b27ccde778b9211.png" /></p><p></p><p>为了验证 PODM-MI 的有效性，我们在京东电商搜索引擎中进行了在线 A/B 测试。PODM-MI 不仅提高了用户购买的可能性，还增加了搜索结果中商品的多样性。需要注意的是，每增加 0.10%的 UCVR 都会为公司带来巨大的收入，因此 PODM-MI 取得的提升是非常显著的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f4/f4d05cf8b52361e08acf5a506e0354a0.png" /></p><p></p><p>不同排序结果的熵值对应于不同的用户意图。为了评估排名结果是否与用户意图高度相关，我们使用 T-SNE [17] 降维方法可视化了根据用户意图分类的排名结果熵值分布。为了增强聚类效果，我们将熵值水平分为 8 个不同的组别。如上图所示，不同多样性趋势下的用户行为流聚类非常明显，用户意图分布聚类的边界清晰可见。这表明所提出的模型成功捕捉到了用户意图的潜在趋势，并相应地调整了排名结果。</p><p>值得注意的是，随着用户意图变得更加多样化，排序结果的熵值也随之增加，反映出排名结果的多样性更高。相反，当用户意图变得更加明确时，熵值会降低，表明排名结果的准确性更高。</p><p></p><p>案例一：</p><p></p><p>Query 流：switch,塞尔达，手机壳，榔头，油烟机，油烟排风管</p><p>当前 Query：果蔬脱水机</p><p></p><p><img src="https://static001.geekbang.org/infoq/14/140d04fc191c2978db3bd2ceec390264.png" /></p><p></p><p>案例二：</p><p>Query 流：连衣裙，连衣裙通勤，连衣裙 s，拉夏贝尔夏京东自营，</p><p>当前 Query：veromoda 官方旗舰店</p><p></p><p><img src="https://static001.geekbang.org/infoq/71/719d319d171fa69d190c7c53fe9f2e78.png" /></p><p></p><p>此外，我们还用一个更具体的案例来说明我们方法的有效性。当用户的历史搜索查询非常多样时，如：Switch，塞尔达，手机壳，锤子，油烟机，排气管，在这种情况下，当用户输入“蔬菜水果脱水机”后，我们的方法比基线方法产生了更多样化的结果。另外，还有一个收敛趋势的案例。当用户搜索“连衣裙”并访问相应的店铺后，再次输入该店铺时，我们的方法比基线方法产生的结果更加集中，并且更好地与用户的历史搜索记录相匹配。</p><p></p><h1>3、未来迭代方向</h1><p></p><p></p><p>• 引入更精细的特征，更好的建模用户的逛买意图</p><p>• 用户意图建模更新的进一步优化</p><p>• 用户意图建模显式影响</p><p></p><p>Note：</p><p></p><p>我们京东搜索算法部目前有大量的社招和实习机会，包括基于大模型的生成式召回/排序等，诚邀有志之士加入。无论您是技术专家还是新兴人才，我们都期待您的加入，共同推动技术的进步和创新。欢迎大家踊跃投递简历，期待与您在京东相遇！欢迎大家交流与探讨，简历投递和建议反馈可联系 wanghuimu1@jd.com, limingming65@jd.com。</p><p></p><p>团队最近相关工作：</p><p></p><p>1. Breaking the Hourglass Phenomenon of Residual Quantization: Enhancing the Upper Bound of Generative Retrieval （arxiv：2407.21488）</p><p>2. Generative Retrieval with Preference Optimization for E-commerce Search（arxiv：2407.19829）</p><p>3. A Preference-oriented Diversity Model Based on Mutual-information in Re-ranking for E-commerce Search（SIGIR 24 ACCEPTED）</p><p>4. MODRL-TA: A Multi-Objective Deep Reinforcement Learning Framework for Traffic Allocation in E-Commerce Search（CIKM 24 ACCEPTED）</p><p>5. Optimizing E-commerce Search: Toward a Generalizable and Rank-Consistent Pre-Ranking Model（SIGIR 24 ACCEPTED）</p><p>Adaptive Hyper-parameter Learning for Deep Semantic Retrieval EMNLP 2023 ACCETPED</p><p></p><p>分享嘉宾：</p><p></p><p>王彗木博士：中科院自动化所博士，亦城优秀人才，CCF 中国计算机学会专业会员，研究方向为大模型、强化学习，目前在京东从事主搜排序及生成式召排工作。</p><p></p><p>李明明博士：中科院信工所博士，亦城优秀人才，CCF 中国计算机学会专业会员，研究方向为大模型、语义检索，目前在京东从事主搜召回及生成式召排工作。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/WCvegJdzK9UUKapYR0ih</id>
            <title>北京银行如何构建全栈大模型应用体系？</title>
            <link>https://www.infoq.cn/article/WCvegJdzK9UUKapYR0ih</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/WCvegJdzK9UUKapYR0ih</guid>
            <pubDate></pubDate>
            <updated>Thu, 26 Sep 2024 01:33:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><blockquote>嘉宾｜代铁 北京银行软件开发中心副总经理编辑｜高玉娴策划｜FCon 全球金融科技大会</blockquote><p></p><p></p><p>近年来，以 ChatGPT 为代表的大模型技术迅速发展，为人工智能产业带来了新的变革，全球 AI 竞争日益激烈，通用智能成为竞争的焦点。国家层面鼓励构建人工智能增长引擎，去年两会工作报告中提出了“人工智能 +”的理念，强调深化大数据和人工智能研发应用，开展人工智能 + 行动，以打造具有国际竞争力的数字产业集群。监管机构如人行、监管总局等也提出了针对人工智能大模型的规范要求和指南。金融行业凭借其丰富的大规模数据积累和多元化应用场景，成为大模型应用的理想领域。</p><p></p><p>然而，金融大模型在行业应用中面临三大挑战：高端芯片受限、数据问题以及大模型应用风险。例如，美国政府限制 AI 芯片出口，ChatGPT 主要使用英文数据，而中文数据不足 1%，同时金融行业数据来源也较为单一。此外，大模型存在幻觉问题，模型输出的准确性问题限制了其在金融领域的应用，目前主要应用于边缘场景。</p><p></p><p>为应对这些挑战，北京银行近年来一直致力于探索人工智能在商业银行的应用，打造人工智能驱动的商业银行（AIB）。去年，北京银行提出了 B=IB+AIB 的理念，即投行驱动与人工智能驱动相结合的商业银行，强调加快企业级数字化转型，用 AI 构建面向未来的金融核心能力，提升经营质效和客户体验。</p><p></p><p>自 2020 年起，北京银行开始构建人工智能中台应用——京智大脑，重点发展以知识驱动的大模型应用体系，通过大模型与小模型的双轮驱动，提高运营效率，精准拓客营销，支持产品创新，优化客户服务，加强风险管理，积极赋能业务发展。</p><p></p><p>在日前举办的 <a href="https://fcon.infoq.cn/2024/shanghai/">FCon 全球金融科技大会</a>"上，北京银行软件开发中心副总经理代铁分享了北京银行在人工智能领域的应用平台建设与实践。</p><p></p><p>以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）：</p><p></p><p>在本次演讲中，我将分享<a href="https://www.infoq.cn/article/GItTCDMzzSsxMcojWydF">北京银行</a>"在人工智能应用平台的技术架构、技术特点以及行业应用情况。</p><p></p><h2>构建“4+N”的全栈国产化大模型应用体系</h2><p></p><p></p><h4>总体功能架构</h4><p></p><p></p><p>我们之前提到了京智大脑这一技术平台，它最初主要是基于数据驱动的小模型。近两年，我们重点聚焦于大模型的研发与应用，并成功构建了一个 “4+N”的全栈国产化大模型应用体系。</p><p><img src="https://static001.geekbang.org/infoq/8c/8c017955c1fccf7407d68b5fc17b9ec2.webp" /></p><p></p><p>这个体系中的“4”代表了四个核心组成部分：</p><p></p><p>1. 国产化的算力底座，集成了 CPU、GPU、NPU 等算力资源，为人工智能应用提供了强大的计算支持。</p><p></p><p>2. 企业级知识库，它包含了行内的操作手册、规章制度以及金融行业的基础知识，通过我们的构建，形成了一个全面的、立体的金融知识图谱，为企业提供了丰富的知识资源。</p><p></p><p>3.MaaS 平台，在通用大模型的基础上建立了模型训练和推理的平台，负责整个大模型的训练、推理和构建工作。</p><p></p><p>4. 基于 Agent 的应用平台，我们在上面构建了查询和内容生成等方面的 <a href="https://www.infoq.cn/article/9lBoqN5m9lmeyeFmY8EF">Agent </a>"能力，以支持多样化的业务需求。</p><p></p><p>“N”则代表了多个银行业应用场景的示范应用，通过这些应用，我们打造了一个“4+N”的应用体系，推动人工智能技术在银行业务中的深入应用和创新。</p><p></p><h4>应用架构</h4><p></p><p></p><p>从应用架构的角度来看，北京银行构建了一个覆盖前中后台的大模型应用体系，这个体系以私有化、通用化、行业化、专业化和普惠化为引领。整个架构从下到上分为五层：</p><p><img src="https://static001.geekbang.org/infoq/e8/e88b23863d38619aa64250d5198e0c89.webp" /></p><p></p><p>1. 基础大模型： 这是架构的最底层，我们采用了开源兼容的方式，集成了大约十个目前市场上可用的开源大模型，以此为基础打造了我们自己的基础大模型平台。</p><p></p><p>2. 行业大模型： 在这一层，我们在基础大模型的基础上，结合行内金融数据进行精细调整，形成了多个参数规格的行业金融大模型，包括 6B、7B、13B 以及 130B 等不同规模 的模型，以满足不同的应用场景需求。</p><p></p><p>3. 企业级知识库： 在这一层，我们结合总行、分行、支行各级的营销策略、产品信息、操作流程以及外部监管和内部风控审计的相关资料，进行了训练和向量化存储，构建了我们行自己的企业级大模型。</p><p></p><p>4. 场景赋能层： 基于企业大模型，我们建立了各种应用平台，围绕投资顾问、运营管理、行业研究、财务报告等 十多个业务场景，提供了人工智能助手，以支持业务的高效运作。</p><p></p><p>5. 个人应用层： 最顶层是针对个人员工的应用，北京银行非常重视 为一线员工赋能。我们以个人办公场景为抓手，为一线员工提供了包括 写作助手、营销助手、客户助手 等一系列 AI 工具，这些工具易于使用、高效且实用，旨在提高一线员工的工作效率，并提升全行的服务专业化水平。</p><p></p><h4>技术架构</h4><p></p><p></p><p>在技术架构方面，北京银行的人工智能平台从下到上分为五个主要层次：算力层、数据层、框架层、模型层和应用层。</p><p><img src="https://static001.geekbang.org/infoq/7a/7a0c4d1e8acaf7fc474ff4ffc09fb6df.webp" /></p><p></p><p>1. 算力层： 这一层由 GPU、NPU、CPU 等组成，为大模型的训练和推理提供必要的计算支持。</p><p></p><p>2. 数据层： 数据层涵盖了三个方面的数据资源。首先是行内数据，包括规章制度、操作指引、监管政策和培训资料等；其次是通过外部途径获取的企业信息，如工商、司法、税务、招投标等；最后是互联网上收集的金融资讯、百科知识和行业数据。这些数据共同构成了我们的数据和知识库。</p><p></p><p>3. 框架层： 在这一层，我们依托金智大脑进行大模型的训练和推理，包括数据清洗、提示词工程应用和搜索引擎增强等。模型的训练、管理和发布都在这一层进行控制。</p><p></p><p>4. 模型层： 我们内嵌了大约 10 多个基础大模型，这些模型可以根据需要进行增补。基于这些基础大模型，我们结合行内和金融业的数据，形成了不同参数规模的行业金融模型。这些模型针对不同的业务场景，如知识抽取、实体识别、意图识别、知识问答和查询等。</p><p></p><p>5. 应用层： 针对不同的业务场景，我们设立了相应的应用，以支持银行各种业务层面的需求。整个平台构建了一种多模型适配、多任务插件以及多元知识互补 的交付能力和应用体系，为业务赋能。</p><p></p><h2>人工智能应用平台七大技术特点</h2><p></p><p></p><p>第一，我们建设了全栈国产化的算力基座和训练框架，以提升自主可控能力。响应国家的号召，我们致力于增强自主研发和信息技术应用创新的能力。面对芯片供应的挑战，我们基于华为的 910B 芯片打造了国产化的算力，并通过 AI、物联网和边缘计算技术构建了智算网络。这个网络允许总行集中训练模型，分行进行推理、边端应用，实现了在算力资源紧张的情况下，对全行算力资源的统一调度，提高了资源利用效率，更好地服务于经营单位和业务流程。</p><p></p><p>第二，我们构建了可信的金融训练集，结合行内的规章制度、营销策略、监管要求以及行业数据，进行了相关的训练。通过与自动化研究所合作，我们提出了两项关键技术：一是多维度金融数据集的智能过滤技术，利用启发式规则、困惑度评分等方法，从数据采集、分析、清洗到去重过滤，形成高质量的金融数据集；</p><p><img src="https://static001.geekbang.org/infoq/50/506d16061fa075c513cd2c6ea5aab664.webp" /></p><p></p><p>二是高可信混合式金融指令数据生成技术，通过种子指令扩展、背景数据混合等技术，针对金融领域数据更新快、内容忠实性要求高、风险厌恶的特点，提高了指令数据的抗干扰能力。目前，我们已经形成了约 500 亿 token 规模的数据集，包含了 300 万条指令集。</p><p></p><p><img src="https://static001.geekbang.org/infoq/67/671d43d285f436b7bced1df4e8f0bd02.webp" /></p><p></p><p>第三，我们构建了金融领域的混合专家模型。银行业的应用场景众多且复杂，我们的模型规格、参数和版本也相当多样。如果将应用场景与模型强绑定，将导致训练更新成本高昂，应用效果不佳。因此，我们建立了一个混合专家模型矩阵，通过自动路由和任务规划技术，确保最适合的模型解决具体的银行应用场景。这种方法不仅节约了推理资源，让最合适的模型处理相应的问题，还提高了模型的准确性。整个模型架构灵活且可扩展，可以进行拆卸和组合，基于这种混合专家模型实现分布式计算，使模型的分工更加专业化。</p><p><img src="https://static001.geekbang.org/infoq/59/59298acb1b9567227ea978b656a4b4d2.webp" /></p><p></p><p>第四，我们建设了大模型服务平台，旨在降低大模型的训练和推理成本。我们内置集成了十多个基础大模型，能够快速进行二次性能增强。同时，我们打造了基于训练和推理的全面工具链，包括模型的基础配置、数据管理、模型训练、模型管理和模型部署等全栈式工具链，使我们能够迅速针对业务场景进行模型训练和推理，并快速部署，节约了训练和推理成本。我们也考虑了安全性，内置了安全算子和高危词过滤，确保模型的输出在数据安全和结果安全方面都经过了严格的考量和安排。</p><p></p><p>第五，我们建设了 Agent 智能体应用能力，以实现大模型应用能力的快速对接。Agent 是一种智能实体，它能够感知外界环境、进行推理，并执行动作。其核心功能依赖于大模型的驱动，除此之外，Agent 还具备规划、知识存储和工具调用的能力，使其能够快速感知外界环境变化并作出反应。Agent 可以根据设定的目标进行独立思考，调用并组合工具，以提升人机交互体验。这有助于更好地发挥银行业务流程的价值。</p><p></p><p>我们的 Agent 平台提供了流程编排、插件开发和部署等功能，支持知识问答、知识检索、数据分析、任务执行 等多种业务场景的调用。这使得大模型能够方便地与银行业务系统进行对接，尤其是在信贷和风控等核心业务领域，Agent 技术的应用使得大模型的智能应用成为可能，我们也在不断探索其在这些领域的应用。</p><p></p><p>第六，自研搜索引擎的建设，旨在构建多元化的金融知识库，以提升大模型的安全性。我们已经建立了一个全面立体的金融知识图谱，这为大模型的决策和内容生成提供了坚实的数据基础。同时，我们自研了基于正向和逆向索引的搜索引擎，使我们能够迅速定位大模型所需的金融知识点，确保获取的知识既最新又权威，从而提高模型的准确性。通过构建这种多元化的金融知识库，我们能够满足银行业的监管要求，特别是对人工智能技术应用的可解释性和准确性的明确要求。大模型通常被视为“黑盒子”，而 结合多元化金融知识库的技术，我们能够将“黑盒子”打开，将闭卷考试转变为开卷考试，既满足了监管要求，也使得大模型的应用更加安全可信。</p><p><img src="https://static001.geekbang.org/infoq/50/50ce7bb2a9f3a0a1e3900b96e3d64ae3.webp" /></p><p></p><p>第七，建设数据安全标注环境，以满足大模型数据标注的安全需求。近年来，数据安全保护的要求日益严格。为此，我们在数据中心建设了自己的安全屋，通过一系列物理措施，包括监控和门禁设备，确保数据标注的安全性和可控性。这不仅防止了数据外泄，还支持了大模型的持续研发和升级。</p><p><img src="https://static001.geekbang.org/infoq/60/6005d58c67322f2943b8ab22153b95e2.webp" /></p><p></p><h2>在报告撰写、会议记录、报告审校等场景落地应用</h2><p></p><p></p><p>最后，我想简单介绍一下我们基于<a href="https://qcon.infoq.cn/2024/shanghai/">大模型的一些具体应用</a>"。</p><p></p><p>“京信妙笔”智能报告工具：这个工具整合了 OCR、NLP 以及大模型的生成式写作能力，主要用于贷款领域的报告撰写。它主要围绕贷款三查，即贷前的计算报告、贷中审批以及贷后的审查报告。使用这个工具的步骤很简单：首先选择信贷报告模板，然后上传相关报告和客户扫描的营业执照等信息，工具会自动进行识别和关键内容提取。接下来，可以一键生成初审报告的初审材料，并通过问答方式补充材料内容，方便地进行编辑和追加。此外，还有一键润色功能，可以进行改写、扩写以及风格化处理，以及一键校对功能，筛查语法、关键字或敏感词，极大提升了信贷领域报告编写的工作效率。</p><p></p><p>智能会议助手工具：这个工具结合了大模型的内容生成能力、语音识别和智能提取功能。它可以与行内视频会议系统对接，会议结束后，通过导入视频录像或录音，自动进行文本识别，并生成会议内容摘要。它还能识别发言人的情绪，并通过声纹识别确定发言人身份，提取相关信息，总结会议内容和讲话重点。此外，它还能对以往的会议纪要进行智能化搜索，方便引用。目前，这个工具 主要应用于审贷会的会议纪要，大大减少了会议纪要的工作量。未来，我们计划在其他会议场景中逐步推广和应用，不断提升会议纪要的质量和效果。</p><p></p><p>智能校对工具： 旨在服务一线员工，减轻他们频繁写报告和审核校对的工作负担。这个工具与自动化所合作，具备三大类校对能力：一是文字标点的应用差错校对，包括错别字、语义、语法等；二是关键词或敏感词的识别，如领土完整、涉黄涉黑等词汇的提示；三是常识性错误校对，如地理名词、职务信息、领导人讲话引用的正确性等。这个工具在一线试用后反响良好，校对速度达到每秒 1000 次，基本满足一线应用需求。</p><p></p><h4>嘉宾介绍</h4><p></p><p>代铁，现任北京银行软件开发中心副总经理，分管数据研发及人工智能领域。毕业于清华大学计算机应用技术专业，硕士学位，高级工程师。曾就职于中国建设银行银行，拥有逾 20 年银行 IT 从业经历。</p><p></p><h4>内容推荐</h4><p></p><p>大会 PPT 获取通道已开启，关注数字化经纬公众号，后台回复“PPT”，即可获取 PPT 下载地址（由于讲师所在企业限制，部分 PPT 不对外公布，详情见大会官网日程）&gt;&gt;&gt;<a href="https://fcon.infoq.cn/2024/shanghai/schedule">https://fcon.infoq.cn/2024/shanghai/schedule</a>"</p><p><img src="https://static001.geekbang.org/infoq/53/53e3ad2b9af87b0f24155bdf37243add.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/3lOSp9UOA1qwhmQ9NbRe</id>
            <title>百度智能云旗舰模型一年降价超90%，万卡集群有效训练时长达99.5%</title>
            <link>https://www.infoq.cn/article/3lOSp9UOA1qwhmQ9NbRe</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/3lOSp9UOA1qwhmQ9NbRe</guid>
            <pubDate></pubDate>
            <updated>Wed, 25 Sep 2024 07:57:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>9月25日，百度智能云宣布分别针对算力、模型、AI应用，全面升级了百舸AI异构计算平台4.0、千帆大模型平台3.0两大AI基础设施，并升级代码助手、智能客服、数字人三大AI原生应用产品。</p><p>&nbsp;</p><p>“目前在千帆大模型平台上，文心大模型日均调用量超过7亿次，累计帮助用户精调了3万个大模型，开发出70多万个企业级应用。过去一年，文心旗舰大模型降价幅度超过90%，主力模型全面免费，最大限度降低了企业创新试错的成本。”百度集团执行副总裁、百度智能云事业群总裁沈抖说道。</p><p>&nbsp;</p><p></p><h4>升级百舸4.0：模型训练有效时长达99.5%，可高效管理十万卡集群</h4><p></p><p>&nbsp;</p><p>“如今，整个云业务的增长正在从传统的云计算向所谓的GPU云以及异构算力进行转换。”百度副总裁谢广军在接受媒体采访时说道。</p><p>&nbsp;</p><p>沈抖介绍称，GPU集群有三个特征：极致规模、极致高密和极致互联。这些“极致”带来了两个严峻的挑战：第一，巨额的建设、运营成本。建一个万卡集群，单是GPU的采购成本就高达几十亿；第二，运维的复杂性急剧增加。硬件不可避免地会出故障，而规模越大出故障的概率就越高，比如Meta训练llama3的时候，用了1.6万张GPU卡的集群，平均每3小时就会出一次故障。在这些故障中，绝大多数是由GPU引起的。</p><p>&nbsp;</p><p>“过去一年，我们感受到客户的模型训练需求猛增，需要的集群规模也越来越大。与此同时，大家对模型推理成本的持续下降的预期也越来越高。这些都对GPU管理的稳定性和有效性提出了更高要求。”沈抖表示，为此百度智能云大幅升级并发布百舸AI异构计算平台4.0。</p><p>&nbsp;</p><p>最下面是资源层，支持异构芯片管理、高速互联、高效存储；组件层解决的是大规模集群稳定和性能的问题；加速层是面向客户大模型训练、推理的需求设计；最上面的工具层是一套管理界面。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/c9/c90ee10f26cfc92176040d4f0f30348c.png" /></p><p>&nbsp;</p><p>百度百舸AI异构计算平台4.0</p><p>&nbsp;</p><p>在集群创建阶段，企业通常需要进行大量复杂、琐碎的算力配置和调试工作。沈抖介绍道，百舸4.0预置了主流的大模型训练工具，能够实现工具层面的秒级部署，并将万卡集群运行准备时间从几周缩减至1小时，极大地提升部署效率，缩短业务上线周期。</p><p>&nbsp;</p><p>在开发实验阶段，企业需要针对业务目标对不同架构、参数的模型进行多次测试，进而制定最佳模型训练策略，保障后续训练的性能和效果。百舸4.0全新升级的可观测大盘，能够对多芯适配、集群效能、任务自动容错等方面进行全方位监测，提供直观决策依据。</p><p>&nbsp;</p><p>在模型训练阶段，稳定和高效是衡量GPU集群水平的“金指标”、“硬通货”。一张GPU出现故障就会导致集群整体停摆，大量时间、成本浪费在故障恢复和数据回滚上。为此，百舸4.0支持自动筛查集群状态，并基于对GPU故障的精准预测，及时转移工作负载，降低故障发生频次。此外，百舸独有的故障秒级感知定位、Flash Checkpoint模型任务状态回滚等技术，能够大幅减少集群故障处置时间，实现接近无损的集群容错。</p><p>&nbsp;</p><p>据介绍，目前百舸在万卡集群上实现了有效训练时长占比99.5%以上，此外，据悉百舸4.0通过在集群设计、任务调度、并行策略、显存优化等一系列创新，大幅提升了集群的模型训练效率，整体性能相比业界平均水平提升高达30%。</p><p>&nbsp;</p><p>在模型推理环节，百舸则通过架构分离、KV Cache、负载分配等一系列加速方法，实现了模型推理的降本提效，尤其在长文本推理方面，推理效率提升超过1倍。</p><p>&nbsp;</p><p>沈抖认为，大模型的Scaling Law将在一段时间内持续有效，很快就会有更多的十万卡集群出现，但是管理十万卡的难度与管理万卡有着天壤之别。</p><p>&nbsp;</p><p>首先，在物理空间方面，十万卡集群需要占据大概10万平方米空间，相当于14个标准足球场的面积；在能耗方面，每天则要消耗大约300万千瓦时的电力，相当于北京市东城区一天的居民用电量。这种对于空间和能源的巨大需求，远远超过了传统机房部署方式的承载能力，跨地域机房部署又会给网络通信带来巨大挑战。此外，十万卡集群中的GPU故障将会非常频繁，有效训练时长占也将迎来新的挑战。</p><p>&nbsp;</p><p>为此，百舸4.0已经构建了十万卡级别的超大规模无拥塞HPN高性能网络、10ms级别超高精度网络监控，以及面向十万卡集群的分钟级故障恢复能力。“百舸4.0正是为部署十万卡大规模集群而设计的。今天的百舸4.0，已经具备了成熟的十万卡集群部署和管理能力，就是要突破这些新挑战，为整个产业提供持续领先的算力平台。”沈抖说道。</p><p>&nbsp;</p><p></p><h4>发布千帆3.0：三大服务全面升级，一句话即可生成企业级应用</h4><p></p><p>&nbsp;</p><p>“模型开发尤其是大模型开发，在toB市场上的需求比直接调用的需求来得晚。”谢广军表示，“随着应用本身的深入落地，也会越来越多，越来越广。”</p><p>&nbsp;</p><p>为了满足企业客户对模型调用、模型开发、应用开发三方面的需求，百度智能云发布千帆大模型平台3.0。根据介绍，升级后的千帆平台可以调用包括文心系列大模型在内的近百个国内外大模型，还支持调用语音、视觉等各种传统的小模型。同时在价格方面，文心旗舰大模型过去一年降价幅度超过90%、主力模型全面免费。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/59/593970705b1d2c90c5071e3ec6730b67.png" /></p><p>&nbsp;</p><p>文心大模型家族全景图</p><p>&nbsp;</p><p>对于需要定制、微调专属模型的用户，千帆3.0提供了一系列大模型工具链，支持CV、NLP、语音等传统模型的开发，并实现数据、模型、算力等资源的统一纳管和调度。模型投入使用后，千帆平台还支持企业将应用中产生的数据，经过采样评估、人工标注、对齐或微调等方式反馈给模型，形成数据飞轮，持续优化模型效果。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/a9/a9f7af4cc1bdc17dc3c73c276a5a7a81.png" /></p><p>&nbsp;</p><p>千帆平台大模型工具链</p><p>&nbsp;</p><p>&nbsp;</p><p>在应用开发方面，针对企业落地大模型的高频应用场景，千帆3.0从检索效果、检索性能、存储扩展、调配灵活性四方面对企业级检索增强生成（RAG）进行了全面升级；针对企业级Agent的开发，千帆3.0增加了业务自主编排、人工编排、知识注入、记忆能力以及百度搜索等80多个官方组件支持。</p><p>&nbsp;</p><p>工具平台的不断完善，也促进了过去一年大模型产业落地的爆发式增长。据悉，目前在千帆平台上，文心大模型日均调用量超过7亿次，千帆平台累计帮助用户精调了3万个大模型，开发出70多万个企业级应用。</p><p>&nbsp;</p><p>此外，千帆行业增强版提供了体系化的工具和组件，支持行业客户、合作伙伴在千帆通用底座上不断添加行业特色，从而更方便地开发适合自己的行业应用。目前，千帆平台上已经沉淀了包括制造、能源、交通，政务、金融、汽车、教育、互联网在内的八大行业解决方案。</p><p>&nbsp;</p><p></p><h4>代码助手、智能客服、数字人全面升级</h4><p></p><p>&nbsp;</p><p>随着大模型产业落地逐渐走向深水区，AI原生应用也将迎来爆发式增长，为了满足企业直接选购成熟AI应用的需求，百度智能云面向数字人、智能客服、代码提效三大领域，升级了三大AI原生应用产品。</p><p>&nbsp;</p><p>基于文心大模型重构的AI原生智能客服应用“客悦”，在用户复杂意图理解、多模态信息交流等方面实现了大幅提升，让智能客服变得更聪明、更拟人。据介绍，“问题自助解决率”是智能客服领域最关键的考核指标，当前业内平均水平是80%，升级后的客悦将这一指标提升至92%，实现业界领先。目前，客悦已累计帮助企业客户服务超过1.5亿人次，交互超过5亿次。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/1e/1e4ed3430d6ea908d3d86dda8a83d68e.png" /></p><p></p><p>&nbsp;</p><p>基于大模型能力，新升级的曦灵数字人4.0支持根据文字快速生成不同妆造、不同行业特色的3D数字人形象和视频。本次大会期间，曦灵平台宣布：将3D超写实数字人的价格从万元大幅降价至199元，达到业内最低价。</p><p>&nbsp;</p><p>曦灵数字人4.0全新升级的4D（3D+时间维度）自动绑定技术和创新模态迁移技术，还解决了传统2D数字人动作僵硬的问题，可以实现人物在不同角度、形体、表情的高度一致。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ef/ef9caa300852855a7f918f0daf238961.png" /></p><p>&nbsp;</p><p>全新升级的全流程AI代码提效工具“文心快码”，聚焦研发全生命周期的业务流，实现了从项目接手到最终交付，全流程编码开发效率与质量的双重提升。</p><p>&nbsp;</p><p>文心快码业界首发“企业级代码架构解释”、“企业级代码审查”，两项全新功能。企业级代码架构解释能在项目接手初期，实现工程架构的智能解读，帮工程师快速理解业务逻辑；而企业级代码审查则能传承资深工程师的编码经验，智能辅助程序员查缺补漏。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b1e9be1a3a1217f4cf6e3a1f04462578.png" /></p><p></p><p>&nbsp;</p><p>此外，针对市面上通用代码助手缺乏对企业历史代码库的理解的痛点，文心快码全新升级的“企业级代码辅助能力”能够深度理解企业代码库，快速学习企业过往的代码与规范，让生成的代码更贴近企业的要求。</p><p>&nbsp;</p><p>目前，文心快码已经服务超过1万家企业客户，帮助数百万中国开发者提升编码效率，整体提升研发效率20%。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ox8UjdiyqTrrGRbWN1Kv</id>
            <title>借助 AI 实现高效的 DevSecOps 工作流程</title>
            <link>https://www.infoq.cn/article/ox8UjdiyqTrrGRbWN1Kv</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ox8UjdiyqTrrGRbWN1Kv</guid>
            <pubDate></pubDate>
            <updated>Wed, 25 Sep 2024 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DevSecOps 是一种强大的软件开发方法，可以加快交付速度并提高效率。</p><p></p><p>在 2024 年伦敦 QCon 的演讲中，我探讨了团队在他们的 DevSecOps 流程中是如何面对不同程度的低效率，从而阻碍了进步和创新。</p><p></p><p>我强调了一些常见的问题，比如调试时间过长、工作流程效率低下等，同时还展示了人工智能（AI）是如何成为简化这些流程并提高效率的强大工具的。</p><p></p><h2>云原生——DevSecOps</h2><p></p><p></p><p>让我们来探索 DevSecOps 及其与云原生的联系。在我们开启 DevSecOps 之旅时，请先考虑你当前所处的阶段。你是处于部署、自动化测试、利用临时环境阶段，还是要从头开始？</p><p></p><p>在整个讨论过程中，我鼓励大家找出自己当前所面临的最低效的任务。是问题创建、编码、测试、安全扫描、部署、故障排除、根本原因分析还是其他什么？</p><p></p><p>想象一下人工智能在提高效率方面的潜力。然而，认识到工作流程多样性的重要性是至关重要的。稍后我们将会探讨具体的示例。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/56/5603538b6f3e6af7d480e52300960b8a.png" /></p><p></p><p>为人工智能建立护栏至关重要。确保数据安全，防止环境泄漏。此外，衡量人工智能的影响也至关重要。不要仅仅是因为别人实施了人工智能我们也要实施。构建一个令人信服的案例并展示其价值。我们也将深入探讨这一方面。</p><p></p><h2>开发工作流程中的 AI</h2><p></p><p></p><p>在快节奏的软件开发世界中，简化工作流程对于提高效率并取得成功至关重要。2024 年，70% 的受访者表示，他们组织中的开发人员需要一个多月的时间才能融入团队并提高生产力，而 2023 年这一比例为 66%（数据来源：GitLab 2024 年全球 DevSecOps 调查）。人工智能已准备好重组我们的工作方式。通过在我们的工作流程中使用人工智能，我们可以获得很多好处，包括提高效率、减少花在重复任务上的时间、增强对代码的理解、增加协作和知识共享，简化新员工入职培训过程。</p><p></p><p>在软件开发方面，人工智能提供了许多可能性来增强每个阶段的工作流程——从将团队划分为开发、运维和安全等专业角色，到促进规划、管理、编码、测试、文档和审查等典型步骤。</p><p></p><p>人工智能驱动的代码建议和生成功能可以自动执行自动完成、识别缺失的依赖项等任务，从而提高了编码效率。此外，人工智能还可以提供代码解释、总结算法、提出性能改进建议，并将长代码重构为面向对象的模式或其他不同的语言。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/88/8824f240f289e7b0613bb402103ddbf9.png" /></p><p></p><p>人工智能的影响不仅限于开发，还延伸到了运维领域。通过分析较短的描述，人工智能可以生成全面的问题描述，从而节省了宝贵的时间和资源。它还可以总结冗长的讨论和问题描述，使团队成员更容易了解情况并参与其中。</p><p></p><h2>现实世界中的 AI 应用实例</h2><p></p><p></p><p>Anthropic Claude Workbench 是一个功能强大的工具，可用于开发和运行在底层大语言模型（LLM）上的 AI 提示查询。例如，一个简单的提示可以生成启动 Golang 项目的全面指导，包括 CLI 命令、GitLab 的 CI/CD 配置，甚至 OpenTelemetry 仪表盘。这消除了筛选无数标签和资源的需要，节省了时间并提高了效率，特别是对于新团队成员来说。</p><p></p><p>此外，人工智能可以帮助制定详细的问题描述，将一个简短的想法转化为一个全面的提案。在提供的示例中，该工具探索了是使用 SDK 还是使用适当的自动检测来检测源代码。这是启动讨论并探索不同解决方案的好方法。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7c/7c30d2fc77a9b17b927c650340e3a7ec.png" /></p><p></p><p>此外，人工智能在总结冗长的讨论和计划方面也具有重要价值，可以快速理解复杂的问题。通过将内容粘贴到 Anthropic Claude 3 Workbench 中，它可以有效地压缩信息，从而实现更快的决策和更有针对性的方法。</p><p></p><p>另一方面，人工智能通过总结冗长的问题描述、在 Go 中生成 Kubernetes 的可观测性 CLI、将 Go 代码重构为 Rust 以及为合并请求推荐审查者来展示它的多功能性。</p><p></p><p></p><h2>AI 驱动的运维：事件响应、可观测性和成本优化</h2><p></p><p></p><p>将重点从开发转移到运维，让我们探讨人工智能是如何彻底改变根本原因分析、可观测性、错误跟踪、性能和成本优化的。一个常见的痛点是 CI/CD 管道停滞不前，就像修改后的 XKCD 303 漫画中描绘的一样。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/eb/ebf22b22c134e082fbd9130a10723dd1.png" /></p><p></p><p>无需手动筛选作业日志，人工智能即可对作业日志进行分析，并提供可操作的见解，甚至提出修复建议。通过改进提示并与人工智能进行对话，开发人员可以快速诊断并解决问题，甚至获得优化建议。</p><p></p><p>安全性是至关重要的，因此在分析之前必须过滤掉密码和凭据等敏感数据。一个精心设计的提示可以指示人工智能以任何软件工程师都能理解的方式解释根本原因，从而加速故障排除。这种方法可以显著提高开发人员的效率。</p><p></p><p>对于云原生部署，Kubernetes 的故障可能会是一场噩梦。然而，像 CNCF 沙箱项目 k8sgpt 这样的工具利用 LLM 来分析部署，并从 SRE 或效率的角度提供建议。它适用于各种 LLM，甚至可以在 MacBook 上与 Ollama 一起本地运行。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/44/4439d51f1ad1771db666171603aeded7.png" /></p><p></p><p>可观测性是运维的另一个关键方面，人工智能可以简化日志分析。在故障发生期间，人工智能可以总结大量的日志数据，更快地查明根本原因，从而帮助快速解决问题。Honeycomb 将人工智能集成到它们的产品中，即体现了这种方法，为复杂的可观测性任务提供了查询助手和其他人工智能驱动的功能。</p><p></p><p>最后，可持续性监测正在获得越来越多的关注，Kepler 等工具使用 eBPF 和机器学习来预测 Kubernetes 环境中的能耗。这使组织能够优化成本并获得可持续性，从而减少碳足迹。</p><p></p><p>这些示例展示了人工智能如何改变运维，提高效率，并推动各个领域的创新。</p><p></p><h2>安全工作流程中的 AI</h2><p></p><p></p><p>将我们的重点转移到安全工作流程上，人工智能可以成为理解并减轻漏洞、增强安全扫描并解决供应链问题的强大盟友。67% 的开发人员表示，他们所开发的代码中有四分之一或更多来自于开源库，但目前只有 21% 的组织使用了软件物料清单（SBOM）来记录构成其软件组件的成分（来源：GitLab 2024 年全球 DevSecOps 报告）。</p><p></p><p>回顾过去的某次安全事件，其中开源工具中的 CVE 导致了意想不到的后果，很明显，更深入地了解漏洞及其长期修复至关重要。</p><p></p><p>人工智能可以通过用简单的术语解释漏洞，澄清诸如格式字符串漏洞、命令注入、定时攻击和缓冲区溢出等概念来提供帮助。通过了解恶意攻击者如何利用漏洞，开发人员可以在不引入回归或损害代码质量的情况下实施有效的修复。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/40/4073495599f7be5659393da73ba1f121.png" /></p><p></p><p>使用诸如“以软件安全工程师的身份解释此漏洞”之类的提示，人工智能可以分析代码片段，提供潜在的漏洞示例，并提出可靠的修复建议。此外，人工智能甚至可以生成带有拟议代码更改的合并请求或拉取请求，从而自动执行修复过程，并能确保安全扫描和 CI/CD 管道的验证修复。</p><p></p><p>这种简化的方法不仅节省了时间，还降低了人为错误的风险，使漏洞管理更加有效且高效。</p><p></p><h2>人工智能护栏——隐私、数据安全、性能、验证</h2><p></p><p></p><p>将我们的注意力转移到人工智能护栏上，解决隐私、数据安全、性能以及人工智能在我们工作流程中的整体适用性至关重要。首先，必须仔细检查数据使用情况。由于潜在的泄漏，我们的数据（包括源代码）不应用于训练 AI 模型。专有数据不应发送给外部提供商进行分析，特别是在银行或政府机构等受监管的环境中。</p><p></p><p>此外，如果人工智能的功能使用了聊天历史记录，那么数据保留政策和删除实践应该是透明的。必须就数据使用和隐私发表公开声明，并向你的 DevOps 或 AI 提供商询问他们的政策，这一点至关重要。</p><p></p><p>安全性是另一个至关重要的问题。应控制对人工智能功能和模型的访问，并建立治理机制来定义谁可以使用它们。此外，应实施保障措施，防止敏感内容被发送到提示中。</p><p></p><p>验证提示响应对于避免被利用至关重要。明确的指导方针和对团队成员要求对于确保负责任且合乎伦理道德地使用人工智能工具是必要的。</p><p></p><p>透明度是关键。应随时提供有关人工智能使用、开发和更新的文档。此外，制定解决人工智能故障或模型更新的计划对于保持生产力也至关重要。像 GitLab 这样的人工智能透明度中心可以提供有价值的见解和信息。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/00/002295cb6beb5c37f8ee80eb8a646232.png" /></p><p></p><p>无论我们使用的是 SaaS API、自管理的 API 还是本地的 LLM，性能监控都是至关重要的。OpenLLMetry 和 LangSmith 等可观测性工具可以帮助我们跟踪人工智能在工作流程中的行为和性能。</p><p></p><p>最后，由于可能产生幻觉，大模型（LLM）的验证至关重要。测试框架和评估指标对于确保人工智能生成的响应的质量和可靠性至关重要。</p><p></p><p>By diligently addressing these considerations, you can harness the power of AI while minimizing risks and ensuring responsible and effective integration into your workflows.</p><p></p><p>通过努力解决这些问题，我们可以利用人工智能的力量，同时最大限度地降低风险，并确保将负责任且有效的人工智能整合到我们的工作流程中。</p><p></p><h2>AI 的影响</h2><p></p><p></p><p>从护栏过渡到影响，衡量人工智能对开发工作流程的影响提出了一个新的挑战。超越传统的开发人员生产力指标并探索其替代方法是至关重要的。</p><p></p><p>可以考虑将 DORA 指标与团队反馈及满意度调查结合起来。此外，需要监控代码质量、测试覆盖率和 CI/CD 管道失败的频率。检查发布时间是减少了还是保持一致。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/50/50f1b8ba138d64c979a21a6132e20405.png" /></p><p></p><p>无论是通过像 Grafana 这样的工具还是其他平台，构建全面的仪表板来跟踪这些指标都是至关重要的。通过分析这些见解，我们可以更深入地了解人工智能是如何影响我们的工作流的，并确定需要改进的领域。虽然通往精确测量的道路仍在持续探索中，但对我们方法的不断探索和完善将使我们更全面地了解人工智能对生产力和整体发展成果的影响。</p><p></p><h2>AI 的采用</h2><p></p><p></p><p>虽然将人工智能集成到工作流程中是很有前景的，但重要的是要考虑安全、隐私和数据使用的防护措施，同时还要验证人工智能的影响。有一些先进的技术，如检索增强生成（Retrieval Augmented Generation，RAG），可以增强人工智能的能力。</p><p></p><p>RAG 通过整合文档或知识库等外部信息源，解决了在旧数据上训练的 LLM 的局限性。通过将这些资源加载到向量存储中并将其与 LLM 集成，用户可以访问最新的特定信息，甚至可以访问当前的 Rust 开发或伦敦天气等主题的信息。</p><p></p><p>RAG 有实际的应用，例如为 Discord 或 Slack 等平台创建的知识库聊天机器人。即使是像 GitLab 手册这样的复杂文档也可以有效地加载和查询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/fc/fc6a9001fd20352e044267088af3887b.png" /></p><p></p><p>有了像 LangChain 这样的工具和像 Ollama 这样的本地 LLM 提供商，我们可以构建自己的 RAG 驱动的解决方案。这使我们能够在不依赖外部 SaaS 提供商的情况下利用专有的数据，从而确保了数据安全和隐私。</p><p></p><h2>AI/LLM 代理</h2><p></p><p></p><p>另一个值得关注的领域是 AI/LLM 代理，它们正在迅速发展的。它们可以动态收集数据来回答复杂的问题，从而提高准确性和效率。虽然该技术仍在开发阶段，但它对 DevSecOps 具有巨大的潜力。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c4/c4605980f65ce0a37676727cadc6ffde.png" /></p><p></p><p>此外，考虑针对特定用例的自定义提示和模型。经过内部数据训练的本地 LLM 提供了安全和隐私方面的优势。在定制领域，探索代理调优是替代全面重新训练的一种经济有效的方案。这些先进的技术可以进一步优化我们的 DevSecOps 工作流程。</p><p></p><h2>结论</h2><p></p><p></p><p>总之，为了确保高效地实施 DevSecOps，有几个关键的考虑因素。首先，从工作流的角度来看，重复性任务、低测试覆盖率和缺陷可以通过利用代码建议、生成测试和使用聊天提示来解决。其次，在安全领域，解决延迟发布的安全回归问题需要漏洞解释、解决方案和团队知识建设。</p><p></p><p>最后，从运维的角度来看，在失败的部署上花费了过多时间的开发人员可以从根本原因分析和 k8sgpt 等工具中受益。通过解决这些问题，组织可以增强其 DevSecOps 实践，并简化其软件开发和交付流程。</p><p></p><p>你可以在点击此处访问公开的演讲幻灯片，此外，它还提供其他的网址和参考资料。</p><p></p><p></p><h3>作者介绍</h3><p></p><p>Michael Friedrich 是 GitLab 的开发人员倡导者，专注于通过 AI 提高 DevSecOps 效率。他对学习计算机工作原理的渴望使他从硬件 / 软件系统工程到 DNS，从开源监控开发到编写 Git/GitLab 培训再到 DevOps 工作流。Michael 喜欢教导每个人，并定期在行业活动和聚会上发表演讲。他喜欢迎接意想不到的挑战，并创建了最佳实践教程和实时编程课程。当他不周游世界或远程工作时，他喜欢搭建乐高模型，并热衷于探索嵌入式硬件。</p><p></p><p>原文链接：</p><p><a href="https://www.infoq.com/articles/efficient-devsecops-workflows/">https://www.infoq.com/articles/efficient-devsecops-workflows/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/KQQVrPLNI91lviqQVoGQ</id>
            <title>垂直领域革新者：浩鲸科技“鲸智大模型” 重磅发布</title>
            <link>https://www.infoq.cn/article/KQQVrPLNI91lviqQVoGQ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/KQQVrPLNI91lviqQVoGQ</guid>
            <pubDate></pubDate>
            <updated>Tue, 24 Sep 2024 09:30:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>9月20日，“垂直大模型，全面释放数字生产力——浩鲸科技·鲸智大模型发布会”在云栖大会期间举行。作为垂直大模型的场景落地践行者和价值发挥者，浩鲸科技重磅推出“1+1+4+5+X”鲸智大模型技术体系，加速赋能行业数字化转型。</p><p>&nbsp;</p><p></p><h3>强强联合，做大模型落地践行者</h3><p></p><p>作为全球化的数字化转型技术服务提供商，浩鲸科技密切关注行业趋势、技术进步和用户需求的变化，聚焦于人工智能技术在垂直领域能力和行业应用场景，实现商业价值。并重视行业标准化的建设和产学研结合，当前已与国内外主流标准组织、高校院所建立了紧密且深度的战略合作关系，以促进人工智能技术和应用的创新。</p><p></p><p>浩鲸科技董事长、总经理鲍钟峻表示，大模型的出现为产业带来很多希望，或许会再次验证IT领域最容易被忽视的规律，“我们总是高估未来两年的变化，却低估未来十年的变化”，大模型的未来任重道远。</p><p></p><p><img src="https://static001.geekbang.org/infoq/43/43dd49688ef87026216647b4ec458553.png" /></p><p>&nbsp;</p><p>中国信通院人工智能研究所所长魏凯表示：大模型正引领人工智能技术的革新，其强大的多模态感知能力和自主认知能力使其在诸多领域展现出了通用人工智能的潜力。同时，大模型的应用正在加速各行各业的数智化转型，特别是在金融、工业等领域的实践证明了其在降本提效方面的价值。大模型的发展也促进了政策制定、技术开发和市场应用之间的良性循环。中国信通院致力于构建大模型的标准体系，推动技术评估和产业应用研究，并将联合产业各界力量打造一个健康发展的大模型生态体系。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/54/546d13c5cd067a04f2961e7d3878ab1d.png" /></p><p>&nbsp;</p><p>现场，在各位专家及领导的共同见证下，浩鲸科技与中国信通院人工智能研究所正式签署战略合作框架协议。双方重点就人工智能、大模型等领域进行全面深入合作，共同推进相关领域的技术创新、标准研制、测试验证、应用示范、产业化推广、咨询交流、生态共建及联合市场推广等合作，充分利用各自的优势资源，推动AI技术创新和产业发展。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/b8/b8fbeba8c2b67350194d1c225f53e098.png" /></p><p></p><p></p><h3>重磅发布，鲸智大模型技术体系及应用</h3><p></p><p>浩鲸科技董事、云智能总裁杨名正式发布鲸智大模型，提出“1+1+4+5+X”技术体系，并详细阐述了浩鲸科技鲸智大模型全栈能力。这不仅是浩鲸科技技术创新的重要成果，也为行业数字化转型注入了强劲动力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0b/0b454ac9851ab1d01fc7f0e4117e97b6.png" /></p><p></p><p>浩鲸科技“1+1+4+5+X ”鲸智技术体系具备算力生态的高效适配、模型生态的灵活兼容、数据供给的成本降低、高精度场景的准确性提升、下沉场景的敏捷支撑这五大特性，帮助企业客户在生产、运营、管理中，提升效率、质量、安全的同时，降低数字化与业务运营的成本。同时，基于20余年的行业服务经验和丰富的大模型落地实践，浩鲸科技能够充分理解客户所需，从更专业、更贴合具体问题解决的角度携手合作伙伴帮助客户构建更加贴近业务的垂直大模型，全面释放数字生产力。</p><p>随后，浩鲸科技创新中心总经理王玉木、数据智能首席专家吴名朝、创新中心首席专家张林，分别就鲸智文档大模型、鲸智BI大模型、鲸智代码大模型做详细介绍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/09/0940e72d885d85ee3c141190a68903fe.png" /></p><p></p><p>协同创新，共筑垂直大模型良性生态</p><p>当前，浩鲸科技面向垂直行业领域，基于大模型产品工具、成熟落地方法论与服务体系，助力企业快速构建企业级、领域级、场景级各类智能化应用，已深度赋能至电信、政务、能源电力、烟草、泛零售等行业。</p><p>发布会期间，上海交通大学计算机科学与工程系主任、国家杰青吴帆博士，阿里云智能副总裁、解决方案部研发部总经理曾震宇，阿里云公共云业务大模型技术服务负责人王德山，海南省海口市营商环境建设局党组成员、海口市政务中心主任曹献平，贵州习酒数字与信息化产业部资深专家岳世彬等出席会议并发言。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8c/8c46f3f02ebffac73666f6cd451f2dbd.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/a7/a7431da99f0af7238eb1225331e449f6.png" /></p><p></p><p>发布会的最后，浩鲸科技携手中国信通院、中国社科院、阿里云、海口市营商建设局、贵州习酒等伙伴，共同发起大模型产业生态合作倡议。该倡议旨在汇聚大模型产业链上下游的精英力量，携手并进，深入探索并加速推动大模型技术在市场中的广泛应用、技术层面的深度合作以及人才培育体系的建立健全。</p><p></p><p><img src="https://static001.geekbang.org/infoq/39/3964240456f5a4ca01280f7aa73fd63f.png" /></p><p></p><p>随着产业数智化时代的全面到来，大模型成为引领行业前行的关键力量。浩鲸科技匠心打造的鲸智行业大模型，精准对接政企行业需求，为行业企业提供量身定制的一站式大模型产品及服务，助力企业加速智能化转型步伐。展望未来，浩鲸科技将携手各方力量，持续优化升级大模型产品与服务，致力于为各行各业政企客户带来更加高效、便捷、易用的数字化转型方案，共同推动社会经济的数字化转型与高质量发展。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/J5pekjwxTFI20oqQCT9K</id>
            <title>豆包视频生成大模型正式发布，首次突破多主体互动难关</title>
            <link>https://www.infoq.cn/article/J5pekjwxTFI20oqQCT9K</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/J5pekjwxTFI20oqQCT9K</guid>
            <pubDate></pubDate>
            <updated>Tue, 24 Sep 2024 08:39:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>字节跳动正式宣告进军AI视频生成。9月24日，字节跳动旗下火山引擎在深圳举办AI创新巡展，一举发布了豆包视频生成-PixelDance、豆包视频生成-Seaweed两款大模型，面向企业市场开启邀测。</p><p>&nbsp;</p><p>多动作多主体交互能力示例：</p><p></p><p></p><p>一致性切镜能力示例：</p><p></p><p></p><p>&nbsp;</p><p>“视频生成有很多难关亟待突破。豆包两款模型会持续演进，在解决关键问题上探索更多可能性，加速拓展AI视频的创作空间和应用落地。”火山引擎总裁谭待表示。</p><p>&nbsp;</p><p>据火山引擎介绍，豆包视频生成模型基于DiT架构，通过高效的DiT融合计算单元，让视频在大动态与运镜中自由切换，拥有变焦、环绕、平摇、缩放、目标跟随等多镜头语言能力。全新设计的扩散模型训练方法更是攻克了多镜头切换的一致性难题，在镜头切换时可同时保持主体、风格、氛围的一致性。</p><p>&nbsp;</p><p>据悉，豆包视频生成模型经过剪映、即梦AI等业务场景打磨和持续迭代，来达到具备专业级光影布局和色彩调和、画面视觉极具美感和真实感的目的。深度优化的Transformer结构则大幅提升豆包视频生成的泛化能力，支持3D动画、2D动画、国画、黑白、厚涂等多种风格，适配电影、电视、电脑、手机等各种设备的比例，不仅适用于电商营销、动画教育、城市文旅、微剧本等企业场景，也能为专业创作者和艺术家们提供创作辅助。</p><p></p><p>目前，新款豆包视频生成模型正在即梦AI内测版小范围测试，未来将逐步开放给所有用户。剪映和即梦AI市场负责人陈欣然认为，AI能够和创作者深度互动，共同创作，带来很多惊喜和启发，即梦AI希望成为用户最亲密和有智慧的创作伙伴。</p><p>&nbsp;</p><p>此外，豆包大模型不仅新增视频生成模型，还发布了豆包音乐模型和同声传译模型，已全面覆盖语言、语音、图像、视频等全模态，全方位满足不同行业和领域的业务场景需求。</p><p>&nbsp;</p><p>火山引擎在这次发布会上还披露了豆包大模型的使用量。据悉，截至9月，豆包语言模型的日均tokens使用量超过1.3万亿，相比5月首次发布时增加十倍，多模态数据处理量也分别达到每天5000万张图片和85万小时语音。</p><p>&nbsp;</p><p>此前，豆包大模型公布低于行业99%的定价，引领国内大模型开启降价潮。谭待认为，大模型价格已不再是阻碍创新的门槛，随着企业大规模应用，大模型支持更大的并发流量正在成为行业发展的关键因素。</p><p>&nbsp;</p><p>谭待介绍，业内多家大模型目前最高仅支持300K甚至100K的TPM（每分钟token数），难以承载企业生产环境流量。例如某科研机构的文献翻译场景，TPM峰值为360K，某汽车智能座舱的TPM峰值为420K，某AI教育公司的TPM峰值更是达到630K。为此，豆包大模型默认支持800K的初始TPM，客户还可根据需求灵活扩容。</p><p>&nbsp;</p><p>“在我们努力下，大模型的应用成本已经得到很好解决。大模型要从卷价格走向卷性能，卷更好的模型能力和服务。”谭待说道。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/8uvqICaWuKGYPAB0Ttia</id>
            <title>300 亿 L4 自动驾驶独角兽 CEO 跑路！大裁员 40%、被投资人抛弃，赚钱业务都停了</title>
            <link>https://www.infoq.cn/article/8uvqICaWuKGYPAB0Ttia</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/8uvqICaWuKGYPAB0Ttia</guid>
            <pubDate></pubDate>
            <updated>Mon, 23 Sep 2024 16:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>整理 | 华卫、核子可乐</p><p></p><p>自动驾驶行业一直充满活力，同时又瞬息万变。就在上周（9 月 18 日），估值近 300 亿人民币的 L4 自动驾驶公司 Motional 宣布了领导层的大变动：CEO 突然自愿“下线”了。</p><p></p><p>据其公告称，其总裁兼首席执行官 （CEO） Karl Iagnemma 将转任高级战略顾问一职；Motion 的首席技术官 （CTO） Laura Major 将担任临时首席执行官兼首席技术官，担任更大的领导角色。</p><p></p><p>Motional 并未公布此番人事变动的原因，但一位知情人士表示，这次调整并无幕后矛盾。Iagnemma 也表示，“我期待着以顾问的身份继续支持 Motional 的使命，并对团队的未来感到兴奋。”</p><p></p><p></p><h1>11 年创业心血，大牛 CEO 毅然让权</h1><p></p><p></p><p>Motional 最初的班底，就来自 Iagnemma 和苏黎世联邦理工学院动态系统与控制专业教授 Emilio Frazzoli 于 2013 年联合建立的初创企业 Nutonomy。</p><p></p><p>尽管 Nutonomy 一直没能像 Waymo 等其他规模更大、资金更雄厚的自动驾驶公司备受关注，但其于 2016 年 8 月在新加坡率先部署自动驾驶汽车服务的公开试验时，还是在广大投资者以及汽车与科技行业当中掀起了一股风潮。</p><p></p><p>短短一年多之后，Nutonomy 就被 Delphi 以 4.5 亿美元收购。后来，Delphi 拆分为两家公司：其动力总成业务成为 Delphi Technologies，而 Aptiv 则专注于设计和生产电子系统、先进安全技术以及自动驾驶汽车所需的硬件和软件，NuTonomy 被并入 Aptiv。</p><p></p><p>在 Aptiv 担任自动驾驶汽车事业部总裁期间，Iagnemma 还领导发布了首个面向公众开放的自动驾驶数据集 nuScenes，包括来自波士顿和新加坡各地的 140 万张图像、39 万个 LiDAR 扫描和 140 万个人工标注的 3D 物体边界框。</p><p></p><p>两年后，随着对自动驾驶汽车的炒作和承诺达到顶峰，现代汽车和 Aptiv 成立了一家 L4 自动驾驶合资企业（后更名为 Motional），原本担任 Aptiv 自动驾驶汽车事业部总裁的 Iagnemma 被选为 Motional 的负责人。当时，两家公司表示，对 Motional 的合并投资将总计达到 40 亿美元（合人民币 281.9 亿元），包括合并工程服务、研发和知识产权的价值。</p><p></p><p>而 Iagnemma 不仅是 Motional 的核心人物，还是自动驾驶汽车行业的重量级先驱。凭借其机器人与自动驾驶汽车的研究成果，Iagnemma 在学术领域享有盛誉。作为曾经的麻省理工学院团队的成员，他曾于 2007 年参与了 DARPA 发起的自动驾驶汽车研发项目“城市挑战赛”。</p><p></p><p>有如此成就的行业元老级人物，就这样突然平静地“放手”了自己一手创办至今已 11 年的公司。这背后的原因，令人不得不深究。</p><p></p><p></p><h1>被投资人中途放弃，赚钱的业务都暂停了</h1><p></p><p></p><p>事实上，Motional 在发布自动驾驶出租车方面取得了一定进展，但同时也面临着严峻的资金困境，导致其商业计划被迫推迟。</p><p></p><p>今年 1 月，Aptiv（合资公司中的一方）宣布将不再为该项目继续注资。“虽然我们的 Motional 合资企业继续在其技术路线图上取得进展，但我们已经决定不再向 Motional 分配资金，并正在寻求替代方案以进一步减少我们的所有权权益。”Aptiv 董事长兼 CEO Kevin Clark 表示。</p><p></p><p>Clark 补充说：“从移动按需市场采用的角度来看，主要在硬件内部和周围提供技术相关的成本确实使其具有挑战性。”换句话说，建立自动驾驶出租车服务的成本很高，而收回这些宝贵资金的时间太长了，Aptiv 无法等待。</p><p></p><p>3 月，有消息称，Motional 获得了一笔未披露金额的过桥贷款作为救命稻草，这家自动驾驶初创公司的下一轮长期融资也得以顺利达成。</p><p></p><p>最终是现代汽车挺身而出，于今年 5 月同意再向 Motional 投资 10 亿美元。其中，现代汽车决定向 Motional 直接投资了 4.75 亿美元，并斥资 4.48 亿美元收购 Aptiv 方面 11% 的普通股权益。这笔交易让现代汽车掌握了多数股权，同时也为这家自动驾驶初创公司提供了维持运营所必需的资金。</p><p></p><p>但这一切并非没有代价。Motional 公司的商业运营思路，包括通过 Uber 和 Lyft 网络在拉斯维加斯使用自动驾驶的现代 Ioniq 5 车辆提供出租车服务。而作为企业重组计划的一部分，Motional 将裁员约 550 人，暂停其商业运营，停止在圣莫尼卡使用自动驾驶汽车为 Uber Eats 客户提供送餐服务，并将使用现代 Ioniq 5 机器人出租车推出下一代机器人出租车服务的计划推迟到了 2026 年。</p><p></p><p>从 Aptiv 今年第一季度的收益报告可以明确看到，在业务前景相对不太乐观的情况下，该公司正在努力管理风险并优化财务状况，但 Aptiv 的撤退与现代汽车的多数股权接手还是引发了人们对于 Motional 未来命运的担忧和质疑。</p><p></p><p>熟悉此番内情的消息人士表示，这次重组是希望在保留资本的同时，在核心技术和商业模式方面取得新的突破。在新计划中，Motional 打算投入更多资源打磨其核心技术。一位知情人士表示，这意味着开展更多测试，包括可能考虑将车辆部署到更多其他城市。此前，Motional 一直在波士顿、匹兹堡和拉斯维加斯测试其自动驾驶技术。</p><p></p><p></p><h1>L4 的商业部署仍极具挑战</h1><p></p><p></p><p>而 Motional 的财务情况转变，正值自动驾驶出租车行业持续面临不确定性之际。这家初创公司虽然一直在缓慢走向商业化，并且至少在五个城市启动了试点项目，但尚未开始收取乘车或者配送费用。</p><p></p><p>宣布重组计划时，Motional 的员工们也被告知，由于商业运营成本高昂加上自动驾驶汽车技术的组件成本居高不下，导致当下部署商业用例仍然极具挑战性。这之后不久，Iagnemma 还发表了一篇博文，概述其重组计划希望“将资源集中在我们核心无人驾驶技术的持续开发和推广之上，同时不再强调近期的商业部署和辅助驾驶用例。”</p><p></p><p>Iagnemma 在博文中解释称，“随着技术发展成熟，更重要的是当自动驾驶部署的商业用例足够明确时，无人驾驶汽车就会正式进入市场。虽然我们对技术进步的速度感到兴奋，而且我们最初的商业部署也获得了宝贵见解，但大规模部署自动驾驶汽车仍然不是短期能够实现的目标，还需要更长的时间周期作为积累。”</p><p></p><p>在 L4 自动驾驶市场当中，通用汽车旗下 Cruise 的商业落地业务也陷入了停滞，原因是在 2023 年 10 月发生的一起事故中，一名行人被该公司的自动驾驶出租车压在车下并拖行。但通用方面已经开始在凤凰城重新绘制地图，选择以缓慢且更为慎重的方式重新将车辆派上公共道路。</p><p></p><p>但与此同时，也有多家 L4 自动驾驶正处在商业部署的上升期。Waymo 继续在美国旧金山、洛杉矶和凤凰城扩展其完全无人驾驶的付费机器人出租车服务，并计划于今年晚些时候登陆奥斯汀。百度 Apollo 正在全面推进全无人自动驾驶规模化应用，已在北京、重庆、武汉多个城市实现全无人自动驾驶商业化运营和测试。</p><p></p><p>参考链接：</p><p></p><p><a href="https://techcrunch.com/2024/09/18/ceo-of-self-driving-startup-motional-is-stepping-down/?guccounter=1">https://techcrunch.com/2024/09/18/ceo-of-self-driving-startup-motional-is-stepping-down/?guccounter=1</a>"</p><p></p><p><a href="https://techcrunch.com/2024/05/02/hyundai-is-spending-close-to-1-billion-to-keep-self-driving-startup-motional-alive/">https://techcrunch.com/2024/05/02/hyundai-is-spending-close-to-1-billion-to-keep-self-driving-startup-motional-alive/</a>"</p><p></p><p><a href="https://techcrunch.com/2024/01/31/autonomous-vehicle-company-motional-is-about-to-lose-a-key-backer/">https://techcrunch.com/2024/01/31/autonomous-vehicle-company-motional-is-about-to-lose-a-key-backer/</a>"</p><p></p><p><a href="https://techcrunch.com/2024/05/07/motional-delays-commercial-robotaxi-plans-amid-restructuring/">https://techcrunch.com/2024/05/07/motional-delays-commercial-robotaxi-plans-amid-restructuring/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/K7cZrEco7Jpu9QMc1BYi</id>
            <title>大模型时代下的新一代广告系统</title>
            <link>https://www.infoq.cn/article/K7cZrEco7Jpu9QMc1BYi</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/K7cZrEco7Jpu9QMc1BYi</guid>
            <pubDate></pubDate>
            <updated>Mon, 23 Sep 2024 09:11:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>京东零售广告部承担着京东全站流量变现及营销效果提升的重要职责，广告研发部是京东最核心的技术部门，也是京东最主要的盈利来源之一。作为京东广告部的核心方向，我们基于京东海量的用户和商家数据，探索最前沿的深度学习等算法技术，创新并应用到业务实践中，赋能千万商家和数亿消费者的消费连接，不断拓展中国乃至全世界的数字经济边界。 </p><p></p><p>在这里，你将与各业务、产品、工程团队紧密合作，深入京东亿量级的数据与丰富的广告业务场景，进行前沿 AI 算法和工程架构的研究与应用工作。通过 AGI 算法创新和行业领先的广告技术，赋能京东多个业务线的广告投放和管理需求，帮助商家实现精准营销，同时提升用户购物体验，推动京东的商业增长，创造数以亿计的业务贡献。</p><p></p><p>大模型时代的到来，新一代广告系统中，我们目前重点攻坚以下五个方向，欢迎敢于挑战、有梦想的同学，和我们一起共事。让我们一起来看看新一代广告系统中如何实现大模型时代的流量价值预估、流量售卖机制、生成式推荐、智能创意以及承载它的算法工程体系。</p><p>﻿</p><p>文末有最新的机会哟~</p><p>﻿</p><p></p><h1>一、流量价值预估——更好的人货场理解能力</h1><p></p><p></p><h2>1、广告用户意图理解</h2><p></p><p></p><p>Query 意图识别是电商搜索中离用户最近和最基础的一个模块，主要的功能是精确地理解用户的搜索意图，为下游的召回/相关性/排序提供决策信息和特征。Query 意图识别主要是做分词、纠错、NER、品牌识别、类目预测和 query 改写等，需准确捕捉用户意图辅助下游决策，是供需匹配和用户体验的基础。</p><p></p><p>当前 query 意图识别训练样本的产生逻辑导致约 85%的 query 预测的类目都是单类目，且多标签样本的标签量较少。因此亟需在保持现有的类目精准度情况下，提升类目的召回率。通过分析，主要存在以下类型的 query 的高相关召回率不足：</p><p></p><p>•泛词的多意图：侧重知识类，词与具体商品之间需要知识关联，例如：水果，生日礼物，灯；</p><p>•歧义词的多意图：多意图 query 下，基于样本生成逻辑，会偏向主意图，弱化甚至丢失次意图，导致召回问题，例如：小米（粮食 or 手机？），苹果（水果 or 手机？）；</p><p>•长尾类目冷启：由于用户点击数据的马太效应，使得大量的长尾类目没有曝光机会，类目下商品无法获得点击，加深了模型无法得到长尾类目训练数据的问题，例如： 服务类，健康类，工业品类；</p><p>•长尾 query 的多意图：由于用户背景和表达习惯不同，对同类商品需求，会有多种表达方式，产生很多长尾 query。模型给出的类目不准，因此产生的点击数据也不够准确。</p><p>﻿</p><p></p><h3>生成-判别模型增强长尾类目训练数据</h3><p></p><p></p><p>模型的训练依赖 query 点击商品的类目来作为监督信号。像这些偏冷启动类目的商品，我们希望通过增加商品曝光来让它们获得更多点击。有了点击数据，就能正向影响下次模型迭代，让模型下一次可以预测的更准。从而让整个模型迭代的流程形成良性循环，而不是马太效应的恶性循环。</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/d9/d9421e02e427c7af63c8d3ca932f5c4f.png" /></p><p>﻿﻿</p><p>解决方案：针对训练样本的类目高度不平衡问题，我们设计了生成-匹配模型，预训练一个 query 生成模型+query-SKU 匹配模型，生成模型用来根据 SKU 的标题/属性信息生成 query，匹配模型用于计算生成 query 和原 SKU title 的相关性分数，卡掉低质量的 query，保证生成 query 的质量。Sku 的类目作为生成 query 的类目，补充到类目预测的训练样本中，平衡和缓解训练集类目失衡问题，让模型能够学习到用户 query 中的长尾类目需求，从而让长尾类目商品有一定的曝光机会。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9a/9a3ac716e694e646f6258cc25f0cb046.png" /></p><p>﻿﻿</p><p>生成数据样例：</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/45/458e46cec4b0cbd02cb9c7401e86f3b8.png" /></p><p>﻿﻿</p><p>基于搜索日志数据预训练的生成-匹配模型不仅可以在类目预测中使用，也可以用在其他相关业务线。例如 query suggestion 和 query 改写业务，根据 sku title 生成的 query 可以作为两者的 query 召回源。</p><p>﻿</p><p></p><h3>先验知识注入模型解决中长尾类目召回不足导致的商家获量困难问题</h3><p></p><p></p><p>算法训练以用户点击 sku 的类目为标签。但由于马太效应，高点击商品的类目才能获得展现。模型的更新，反而会加剧马太效应，形成恶性循环。</p><p>•用户反馈信号被高频类目主导，需打破仅依赖用户反馈的马太效应闭环。例如：用户搜“耳机”，相关类目包含 862-手机耳机，842-蓝牙耳机... 等 9 个三级类目。由于马太效应，系统只能展现出 1~2 个高点击类目的商品，中长尾类目下商品无展现。</p><p></p><p>•业界最新算法，也高度依赖后验反馈信号，无法召回中长尾类目。</p><p>﻿</p><p>解决方案：通过引入先验知识和模型的优化，增强模型对电商知识的感知，弱化模型对后验反馈的依赖：引入先验知识：类目语义知识、类目共现/语义关系图。通过提取类目名、类目的产品词等，代表类目侧的语义表征。通过类目关系图，反映类目共点击和语义相似关系，实现头部类目带长尾（相关）类目来提升召回率。学习先验知识：设计新模型，以 BERT 为文本编码器，学习 Query 和类目表征。以多通道 GCN 为图结构编码器，学习类目之间的关系。设计半监督 Loss，通过 query-类目语义匹配分数，作为监督信号增强类目标签。</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/56/567042528053f2a8922d546fd608d0a6.png" /></p><p>﻿﻿</p><p><img src="https://static001.geekbang.org/infoq/85/85218181b15ca08606636704994092e3.png" /></p><p>﻿﻿</p><p>算法方案发表于 WWW 2024《A Semi-supervised Multi-channel Graph Convolutional Network for Query Classification in E-commerce》</p><p>﻿</p><p></p><h2>2、广告多模态内容理解</h2><p></p><p></p><p>随着富媒体时代的到来，商业广告已告别了纯文本广告时代，图文广告、视频广告已成为广告主进行创意表达的新型方式。目前京东 APP 中的推荐和搜索页面均包含大量图像、视频形式的商品展示。在此场景下，传统单模态 or 少模态的建模方式，有以下问题：</p><p></p><p>•无法建模视觉信息对用户行为的影响，用户对商品展示效果的偏好无法建模。</p><p>•只局限在文本/ID 特征上，无法对商品细节进行精准建模。</p><p>•大量使用物料 ID 特征会带来模型记忆性的问题，使得整个广告系统对广告物料的换血能力会比较差，新物料无法在系统中快速生效。</p><p></p><p>针对上述问题，我们在广告场景下实现了多方位的多模态表征能力建设，并在召回及创意等环节进行了应用，取得了显著的线上效果提升。</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/55/55918285e6a37f1cb8f737a28c1973a9.png" /></p><p>﻿﻿图 1.电商场景下的多模态商品展示</p><p>﻿</p><p></p><h3>多模态表征在召回环节的应用</h3><p></p><p></p><p>当前线上的向量化召回模型，过于依赖于 sku 的切词特征、品牌、类目特征等客观特征，对于刻画 sku 的全局属性、主观信息等方面的能力是缺失的。引入 sku 的图像信息，相当于从另一个维度增加了图片的信息，对于 query-item 本身的 match 任务是有正向增益效果的。在未来的发展趋势中，单纯基于文本匹配的方式已经缺乏了优势，图像、视频、虚拟现实 &amp;人机交互等途径的冲击对传统的搜索、推荐任务提出了新的挑战。</p><p></p><p>基于图像 &amp;文本多模态的商品表征 本工作中，我们基于双流模型 pipeline，分别利用预训练的文本表征提取网络和图像表征提取网络，提取京东站内商品的视觉与文本表征，并通过不同的对齐流程得到用于下游推荐任务的多模态商品表征。整体流程包括：内容模态表征提取-&gt;内容模态对齐-&gt;推荐空间对齐三个主要部分。内容模态表征提取对于文本模态信息，基于商品标题+品牌词+三级类目，使用预训练的 BGE-large-zh1.5 模型提取隐层表征，对于视觉模态信息，基于商品主图，使用预训练 ViT-CLIP-base 提取视觉表征。内容模态对齐：对基于预训练 backbone 提取到的隐层表征，使用基于 CLIP 的对比学习方式训练一个 projection head，对文本和视觉模态进行对齐和降维处理。推荐空间对齐：在对齐到推荐任务的语义空间时，首先构造不同模态的商品关系图，之后利用 Gate-GNN 的特征聚合能力，在 item-item 关系图上进行基于商品活跃度的聚合，得到混合模态的商品表表征。</p><p>﻿</p><p></p><h3>多模态表征在创意优选环节的应用</h3><p></p><p></p><p>创意优选环节的多模态理解与排序等环节存在较大区别，排序任务的目标是建模同一用户在多个候选 sku 之间的排序关系，用多模态理解作 sku 信息精细化建模的信息增益来源，更好地建模商品信息，以实现不同商品之间的对比。创意可以表征很多高阶的结构化信息。基于这一点，在创意优化的特征工程上，方向大致是：强化 User/Context，弱化 Item/POI，通过引入多模态的创意表征，来个性化地学习到创意中的卖点信息，从而实现创意层面的最优排序。</p><p></p><p>基于图像模态的商品表征 目前商详主图中存在一定的噪声，因此对于全图的表征往往会受到噪声的干扰，之前的做法往往先对主商品进行抠图，之后再进行特征提取，但是这种两阶段的特征提取依赖主图区域的准确标注，并会带来误差累积的问题，不适合缺乏标签的电商图像预训练任务。我们考虑直接进行图像自监督方法（DINO）进行预训练，在模型训练的同时端到端提取可靠的图像主体表征，具体流程如下图所示：</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/9c/9c93412ab1fead0746be8c3737d5c0e7.png" /></p><p>﻿﻿</p><p>无监督模型方案</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/d1/d17afff33a409da2206486bec2e36974.png" /></p><p>注意力图可视化</p><p>﻿</p><p></p><h1>二、流量售卖机制——更优的机制能力</h1><p></p><p></p><h2>1、ListVCG：基于强化学习的序列拍卖机制</h2><p></p><p></p><p>推荐信息流广告是典型的多品拍卖场景，业界通用方案 GSP 在理论、效率上均不是最优解，VCG 多品拍卖机制是我们的理想方案。但是 VCG 仅仅是一个理论上的解决方案，他的前提是需要高效的找到最佳组合拍卖结果。与此同时，推荐业务复杂，是典型的多目标优化场景，但是标准 VCG 是追求社会福利最大化的机制，因此在由 GSP 切换到 VCG 时，平台收益在短期内会显著下降，这也是业界公认的 VCG 机制切换难题。因此如何将 VCG 与多目标优化进行结合也是我们面临的主要挑战。结合京东的实际应用场景，我们提出了 ListVCG 拍卖机制，来解决上述问题。</p><p></p><p>首先面临要解决的是 700 选 4 的排列组合问题，序列的搜索空间上千亿，我们将此定义成一个强化学习的问题，借鉴了经典的 Actor-Critic 架构，Actor 输出概率矩阵，通过采样的手段去求解排列组合问题，同时我们利用用户的真实反馈去提升 Critic 的评估水平，挑选出的最优组合会利用策略梯度的方式指引 Actor 学习。通过这种互相迭代自提升的方式去高效逼近最优组合。</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/f5/f54594fea0c68e6728db7192243151b0.png" /></p><p>﻿﻿</p><p>VCG 下的多品拍卖同时是一个经济学问题，需要满足激励相容的拍卖理论约束来保证长期的生态健康发展，然而常见的多目标问题的优化思路会使得无法使用 vcg 计费。因此我们在 Listvcg 中对于 ECPM 价值进行了参数化的变形，在保证可计费的同时通过可学习的参数来满足平台收益、社会福利、用户体验以及物料整体价值多目标优化的诉求。</p><p></p><p>为了更好地对流量长期价值进行建模，我们自然地引入了强化学习的方式，起初我们尝试了传统 off-policy 的 Q-Learning 算法如 DDQN 等，然而，由于后验反馈的奖励稀疏，模型训练效果不稳定，因此，我们尝试引入 reward shaping 以及 curriculum RL 的思想，通过加入稠密先验奖励缓解数据侧的奖励稀疏，并让模型在相对简单的单步决策任务（如序列曝光、点击、单步价值预估等）收敛后，再学习长期决策任务，使得模型效果有了显著提升，在优化长期竞价环境的同时，实现了短期收入和广告主 roi 的上升。</p><p></p><p><img src="https://static001.geekbang.org/infoq/64/64b101efa94f993cfe3210f4067d1415.png" /></p><p>﻿﻿</p><p></p><h2>2、基于强化学习的多智能体博弈</h2><p></p><p></p><h3>多智体在拍卖机制的博弈环境</h3><p></p><p></p><p>目标层面：机制和出价智能体联合优化是行业发展趋势，出价与机制智能体具有一致的整体目标。</p><p>算法层面：我们从算法视角分析出价与机制的策略如何影响广告收入和 tcharge。</p><p></p><p>•平台一段时间的收入由以下三个因素决定：</p><p>1.流量价值分布：一段时间请求数量，广告主数量，以及每个请求 pctr、pcvr、tcpa</p><p>2.广告主调价策略：bid ratio （假设这段时间不变）</p><p>3.平台机制策略：分配以及计费规则</p><p>•具体的，我们有（假设 100 个请求，10 个广告主）</p><p>﻿</p><p>TC=∑i=1100∑j=110xij⋅pctrij⋅pcvrij⋅tcpaj⋅bid_ratiojTC=∑i=1100​∑j=110​xij​⋅pctrij​⋅pcvrij​⋅tcpaj​⋅bid_ratioj​</p><p>﻿</p><p>期望 revenue=∑i=1100∑j=110xij⋅pctrij⋅cpcij 期望 revenue=∑i=1100​∑j=110​xij​⋅pctrij​⋅cpcij​</p><p>﻿﻿</p><p></p><blockquote>机制、出价与用户（环境）的交互关系如下图所示，事实上，在单智能体强化学习下，机制和出价互为环境</blockquote><p></p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/c0/c0c5f07a586f965f4b731c56d19ba210.png" /></p><p>﻿﻿</p><p></p><h3>基于强化学习的多智能体博弈的研究和落地工作</h3><p></p><p></p><p>基于 MPC 和强化学习的出价算法优化点主要在于对未来一段时间请求环境奖励（tcharge、达成率）的预测，以及根据当前的状态（展现、点击、消耗、达成情况）来决定下一步动作（bid ratio）；同样的，基于强化学习的机制策略也需要对未来一段时间请求环境奖励（广告收入）进行预测，并且根据当前的状态（历史收入、预算情况等）来决定下一步动作（分配 &amp;计费）。</p><p>﻿</p><p>机制和出价对未来一段时间奖励预测越准确，动作选择越准确，会带来越多的收入和达成提升。为此，我们根据不同阶段对多智能体技术就行研发：</p><p></p><p>（1）第一阶段：基于离线请求数据的模拟</p><p>•出价和机制智能体一侧固定，通过离线模拟尽可能还原线上策略，进行模型训练</p><p>•难点：</p><p>◦缺少精确的离线模拟环境，目前只有部分精排队列还原，复杂逻辑难以还原</p><p>◦计算量级大；新的机制还在不断迭代中</p><p>（2）第二阶段：基于离线仿真环境的模拟</p><p>•出价和机制智能体通过感知 自身不同动作下对方的反馈，对未来奖励预估更准确</p><p>•风险：</p><p>◦模拟误差累计增大（无法模拟部分/用户行为模拟偏差）</p><p>◦实验评估难以进行</p><p>﻿</p><p>以机制为例，收益本质上来自于对广告主未来行为的预测，比如在某个流量上 bidder 由于 bid ratio 高（但是 cvr 低）获得了流量，虽然平台当次请求收入最大，但是会影响后续 bid ratio 调节，整体收入非最优。通过在仿真环境下寻找更优均衡（需考虑离在线不一致的问题），可以避免广告主（比如某个类目）的出价收敛到对平台整体收入不利的均衡。</p><p>﻿</p><p></p><h1>三、广告生成式推荐——更颠覆的推荐范式</h1><p></p><p></p><p>在京东广告场景，我们面临了如下的挑战：用户行为复杂、平台数据边界、数据稀疏性高、冷启动问题、场景理解困难、多样性和新颖性。由于现实系统中的商品数量巨大，传统 RS 通常采用多级过滤范式，包括召回、粗排、精排、重排等流程，相较于传统 RS，生成式推荐系统具备如下的优势：1）简化推荐流程。实现从多级过滤范式（discriminative-based，判别式）到单级过滤范式（generative-based，生成式）的变迁。2）具备更好的泛化性和稳定性。利用 LLM 中的世界知识和推理能力，在具有新用户和商品的冷启动和新领域场景下具备更好的推荐效果和迁移效果。</p><p>﻿</p><p></p><h2>1、方案</h2><p></p><p></p><p>生成式推荐涉及两个接地（grounding）过程，“将语言空间接地到推荐空间”和“将推荐空间接地到实际商品空间”。为了实现这两个过程，我们的方案如下：</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/f2/f2972ca0dfe6cbdf2ce4c1b666bd75cc.png" /></p><p>﻿﻿</p><p>步骤一：商品量化表示阶段</p><p>选取高点击商品的标题、类目等语义信息，经由编码器模型获得向量表示，再利用 RQ-VAE 对向量进行残差量化，从而获得商品的语义 ID。例如，商品“ThinkPad 联想 ThinkBook 14+ 2024 14.5 英寸轻薄本英特尔酷睿 ultra AI 全能本高性能独显商务办公笔记本电脑”可表示为：</p><p></p><p>步骤二：继续预训练阶段</p><p>（1）量化 token 扩展大模型词表并完成初始化</p><p>将商品量化表示的底层 token 集合，加入到大模型中，进行微调对齐训练，使得模型“理解"这些底层 token</p><p>（2）语义 ID 和商品文本信息互译任务</p><p><code lang="text">提示词:
请告诉我,商品的四元组表示为{input_turple}的标题是什么？
输入:

输出:
华为（HUAWEI）旗舰手机mate60 pro+ 16G+512GB 宣白

提示词:
请告诉我,商品的标题是{input_title}, 它的四元组表示是什么?
输入:
华为（HUAWEI）旗舰手机mate60 pro+ 16G+512GB 宣白
输出:
</code></p><p></p><p>步骤三：微调阶段</p><p></p><p><code lang="text">提示词:
用户历史浏览的商品序列的文本语义信息为{input_text1, input_text2, ..., input_text_N},
请帮我预测用户下一个要浏览的商品是什么？
输入:
华为（HUAWEI）旗舰手机mate60 pro+ 16G+512GB 宣白,
华为（HUAWEI）旗舰手机mate60 pro+ 16G+1TB 宣白,
华为（HUAWEI）旗舰手机mate60 pro+ 16G+512GB 砚黑，
华为（HUAWEI）旗舰手机mate60 pro+ 16G+1TB 砚黑,
华为
输出:


</code></p><p></p><p><code lang="text">提示词:
用户历史浏览的商品序列的文本语义信息为{input_text1, input_text2, ..., input_text_N},
请帮我预测用户下一个要浏览的商品是什么？
输入:
华为（HUAWEI）旗舰手机mate60 pro+ 16G+512GB 宣白,
华为（HUAWEI）旗舰手机mate60 pro+ 16G+1TB 宣白,
华为（HUAWEI）旗舰手机mate60 pro+ 16G+512GB 砚黑，
华为（HUAWEI）旗舰手机mate60 pro+ 16G+1TB 砚黑,
华为
输出:


</code></p><p></p><p><code lang="text">提示词:
用户历史浏览的商品的四元组序列为{input_tuple1, input_tuple2, ..., input_tupleN},
请帮我预测用户下一个要浏览的商品是什么？
输入:
，, , 
输出:


</code></p><p></p><p>步骤四：DPO 阶段</p><p></p><p><code lang="text">提示词:
用户历史浏览的商品的四元组序列为{input_tuple1, input_tuple2, ..., input_tupleN},
请帮我预测用户下一个要浏览的商品是什么？
输入:
, , , 
正例: 

负例:


</code></p><p></p><h2>2、效果</h2><p></p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/16/161f1908ca751c1e97cacbacfb9e4418.png" /></p><p>﻿﻿</p><p>我们将上述方案应用于京东站内和站外广告的推荐流程，取得了显著的效果提升。</p><p>﻿</p><p></p><h1>四、广告智能创意——更生动的视觉冲击</h1><p></p><p>﻿</p><p>广告创意不仅能够抓住消费者的眼球，还可以传递品牌核心价值和故事，建立起与消费者之间的情感联系。在电商场景下，创意内容是影响用户点击的重要因素，对广告收入有着重要的影响。为了满足千人千面的用户偏好，我们在大模型时代借助其强大的生成能力，产出以下一系列的创意内容：</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/3d/3d9c64553611bc91eb20bae8760aaa53.png" /></p><p>﻿﻿</p><p>尽管最近 AIGC 技术蓬勃发展，使得创意制作摆脱了成本和效率的限制。然而，大模型在广告创意的应用上还存在诸多问题。如下方图片所示，现有的图片生成模型会产出空间失调/大小失调/商品不显著和形状幻觉等 bad case：</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/f0/f04f87ae3f0c4d59224a2a6aaf49d293.png" /></p><p>﻿﻿</p><p>为了解决上述问题，我们提出一种多模态可靠反馈网络（RFNet），用于自动审核生成的图片，并将其应用于递归生成过程中，从而提高可用广告图片的数量。此外，我们通过一致性条件正则化（Consistent Condition regularization）微调扩散模型，利用 RFNet 的反馈，显著提升了生成图片的可用率，减少了递归生成的尝试次数，同时保持了高效的生产过程和视觉吸引力。我们还构建了一个包含超过一百万张人工标注生成广告图片的 RF1M 数据集，帮助训练 RFNet 准确评估图片的可用性。这项工作发表在计算机视觉顶级会议 ECCV2024。</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/52/52cd372359f425b588e67b462d43f5bd.png" /></p><p>﻿﻿</p><p></p><h1>五、广告大模型算法工程体系——更极致的算法基建</h1><p></p><p></p><p>大模型尤其以 LLM、AIGC 类的典型模型为例，其模型参数通常在 0.5B ~ 72B 之间，在广告场景上带来最直观的挑战是：超大规模模型的训练推理挑战、复杂业务链路的融合。</p><p></p><p>推理上，广告链路跟传统的对话系统不同，其延迟要求极高，通常请求到计算完毕返回之间的耗时仅有 100ms，因此，耗时约束下的推理能力是一个极大的挑战。此外，单请求的推理成本也是业界大模型服务公司挥之不去的追求点。京东广告已经可以做到 1.5B 体积模型，百万 Token 成本较行业成本更低。</p><p>训练上，不论是开源模型再微调和在训练，还是以 Transformer 为核心的自行搭建的模型结构，对片上网络、存内计算、空间时间编排的脉动计算模式等技术要求都有成倍的要求提升。</p><p></p><p>业务链路上，最典型的模型服务以模型内逻辑+外部链路逻辑整合而成，而一个 DAG（RAG）服务是一种不错的融合方式。</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/46/46740b949d9fafdaaa253cb81b7fc38b.jpeg?x-oss-process=image%2Fresize%2Cp_80%2Fauto-orient%2C1" /></p><p>﻿﻿</p><p>京东广告算法工程团队在人工智能领域持续深耕，不仅致力于 LLM（Large Language Model）训练推理技术的前沿探索，力求突破自然语言处理的瓶颈，提升模型的语义理解和生成能力。同时，我们也充分认识到硬件基础设施对于大规模模型运行的重要性，因此积极与业界领先的芯片制造商和网络服务提供商展开深度合作。</p><p>﻿</p><p>我们从底层的物理拓扑结构开始优化，确保数据传输的高效性和稳定性，为模型的高速运行奠定坚实基础。接着，针对不同的芯片特性进行定制化的适配工作，让模型能够在各种硬件环境下发挥出最优性能。我们深知，只有软硬件完美结合，才能真正释放 AI 的潜能。</p><p>﻿</p><p>此外，京东广告算法工程团队还对训练框架进行了全方位的优化。我们引入了最新的并行计算技术和分布式存储方案，使得大规模数据的处理和模型的训练变得更加迅捷。同时，我们也在推理服务上狠下功夫，通过缓存策略、负载均衡等手段，显著提升了模型的响应速度和并发处理能力。</p><p>﻿</p><p>这一切的努力，都是为了能够支撑起下一代的超大规模模型，使其能够应对百万级 QPS 的严苛挑战，为用户提供更快速、更精准的广告推荐服务。京东广告算法工程团队将持续创新，以技术驱动业务发展，为实现更智能、更个性化的广告体验而不懈奋斗。</p><p>﻿</p><p></p><h2>TO 亲爱的朋友：</h2><p></p><p>﻿</p><p></p><blockquote>京东广告研发部致力于提供全方位的广告技术服务，包括广告排序、出价、创意算法、广告投放平台建设、大数据生产和数据挖掘、广告质量控制和广告产品创新。我们全天候保障京东广告系统的稳定运行，不断优化广告系统全链路基础能力，持续提升研发效率和交付能力。通过 AGI 算法创新和行业领先的广告技术，赋能京东多个业务线的广告投放和管理需求，帮助商家实现精准营销，同时提升用户购物体验，推动京东的商业增长，创造数以亿计的日均广告收入。在这里，你将与各业务、产品、工程团队紧密合作，深入京东亿量级的数据与丰富的广告业务场景，进行前沿 AI 算法和工程架构的研究与应用工作。</blockquote><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/xWDfxLTL2iIL8fvTwWe1</id>
            <title>大模型拜师学艺！422位专家、学者加入百度“文心导师”计划</title>
            <link>https://www.infoq.cn/article/xWDfxLTL2iIL8fvTwWe1</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/xWDfxLTL2iIL8fvTwWe1</guid>
            <pubDate></pubDate>
            <updated>Mon, 23 Sep 2024 08:01:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月20日，百度文心大模型与上海东方明珠广播电视塔达成合作意向，双方将在AI+文旅领域展开深入合作。截至目前，百度已招募422位各领域专家，在知识传授、结果评估和反馈迭代等方面为文心大模型提供帮助。</p><p></p><p>百度集团副总裁、深度学习技术及应用国家工程研究中心副主任吴甜表示，大模型已成为人工智能主流的发展方向。新一代大模型在理解、生成、逻辑、记忆等通用能力大幅提升，可以进行思考、规划，甚至行动，将推动产业变革，重塑产业未来发展方向。</p><p></p><p>公开资料显示，2018年底，百度就开展了大模型技术研发，2019年3月正式发布文心大模型1.0，今年6月百度发布了文心大模型 4.0 Turbo，基础模型持续迭代。在此基础上，百度借鉴人的思考系统，在基础模型和产业应用场景之间构建了智能体技术。</p><p></p><p>吴甜进一步介绍到，文心智能体在基础模型之上进一步进行思考增强训练，构建了思考模型，具备更强的理解、规划、反思与进化能力，能根据各类需求场景自主调用工具解决问题。智能体技术全面降低了大模型应用的开发门槛。</p><p></p><p>据了解，基于智能体技术构建的文心智能体平台，目前已吸引60万开发者，构建的智能体覆盖上百个应用场景，在为用户提供更好内容和服务的同时，也在产业中越来越深入，助力产业智能化升级。</p><p> </p><p></p><p>百度集团副总裁、深度学习技术及应用国家工程研究中心副主任 吴甜</p><p>随着大模型技术的发展和应用的推进，百度在大模型产业当中的实战经验拓展到更多的领域，推动各行业智能化升级。</p><p></p><p>百度在2023年12月发起了“文心导师”计划。文心导师指导文心大模型学习特定行业、场景专业知识，评定文心大模型生成结果的质量，校准文心大模型生成内容的专业度，帮助文心一言加强在各个专业领域的认知，为用户提供更具思想深度和广度的支持。</p><p></p><p>2024年7月，“文心导师”计划正式对外公开招募。截至目前，已有包括科技、金融、文化、教育、医疗、能源等十余个重点行业在内的422位顶尖专家、学者加入“文心导师”计划。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/8oDE9Ygmg31V33OGcC5W</id>
            <title>成立7年融资近10亿元 ，这家AI创企将被英伟达收入囊中！AI大佬趣评：估值应该仅能让投资人回本</title>
            <link>https://www.infoq.cn/article/8oDE9Ygmg31V33OGcC5W</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/8oDE9Ygmg31V33OGcC5W</guid>
            <pubDate></pubDate>
            <updated>Mon, 23 Sep 2024 07:20:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><h2>英伟达将斥资约10亿元收购初创AI公司OctoAI</h2><p></p><p>近日，外媒 The Information 的 Anissa Gardizy、Aaron Holmes 和 Stephanie Palazzolo 援引发给股东的一份消息称，英伟达已就收购 OctoAI 展开了深入谈判，OctoAI 是一家销售软件的初创公司，旨在提高人工智能模型的运行效率。根据 OctoAI 发给股东的一份文件，英伟达提议以约 1.65 亿美元收购该公司，这还不包括该公司的债务和其他费用。</p><p>&nbsp;</p><p>据公开资料显示，OctoAI（前身为 OctoML）成立于2019年，公司联合创始人兼首席执行官Luis Ceze是一位出生于巴西的美国计算机科学家、商人和学者，也是华盛顿大学Paul&nbsp;G. Allen 计算机科学与工程学院的计算机科学教授，后因其在 Apache TVM&nbsp;和仿生数据存储系统方面的杰出成就而闻名。OctoAI 总部位于华盛顿州西雅图，成立7年间获得了Madrona Venture Partners、Amplify Partners、Tiger Global 和 Addition Capital等机构资金支持。</p><p>&nbsp;</p><p>OctoAI的使命是让人工智能更易于访问和可持续，以便它可以用来改善生活。OctoAI 平台为应用程序构建者提供了完整的堆栈，以便在云端或本地运行、调整和扩展他们的人工智能应用程序。借助适用于 SDXL、Mixtral 和 Llama2 等热门模型的超快推理 API、端到端开发人员解决方案和世界一流的 ML 系统，企业可以专注于构建让客户赞叹的应用程序，而无需成为人工智能基础设施专家。</p><p>&nbsp;</p><p>此前，OctoAI曾从包括Tiger Global Management、Madrona Venture Group和Amplify Partners在内的投资者哪里筹集了1.32亿美元，最终在2021年获得了约9亿美元的估值。</p><p>&nbsp;</p><p>英伟达没有立即回应的置评请求。</p><p>&nbsp;</p><p></p><h2>为什么能被英伟达看上？</h2><p></p><p>&nbsp;</p><p>早在今年4月，OctoAI公司就宣布推出了软件平台OctoStack，该平台可让公司在其内部基础设施上托管人工智能模型。据悉，这是业界首个可在任何地方为生成式 AI 模型提供服务的完整技术堆栈。</p><p>&nbsp;</p><p>在早期，OctoAI 几乎只专注于优化模型，使其运行更有效。基于Apache TVM机器学习编译器框架，该公司随后推出了 TVM 即服务平台，并随着时间的推移，将其扩展为一个成熟的模型服务产品，将其优化功能与 DevOps 平台相结合。随着生成式人工智能的兴起，该团队随后推出了完全托管的 OctoAI 平台，帮助其用户服务和微调现有模型。OctoStack 的核心就是 OctoAI 平台，但用于私人部署。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/eaee75f147f4e2d5bc72e84a40d833ef.png" /></p><p></p><p>&nbsp;</p><p>图片来源：OctoAI</p><p>&nbsp;</p><p>OctoAI 首席执行官兼联合创始人Luis Ceze曾在接受媒体采访时称，该公司平台上有超过 2.5万名开发人员和数百名在生产中使用该平台的付费客户。Ceze 说，这些公司中很多都是 GenAI 原生公司。不过，想要采用生成式 AI 的传统企业市场要大得多，因此 OctoAI 现在想通过OctoStack 为这些企业提供服务。</p><p>&nbsp;</p><p>许多大型语言模型都是通过基于云的API（应用程序编程接口）提供的。此类模型托管在各自开发人员的基础设施上，这需要客户将其数据发送到该基础设施进行处理。在内部硬件上托管神经网络无需与外部提供商共享数据，这可以简化企业的网络安全和法规遵从性。</p><p>&nbsp;</p><p>OctoAI 表示，其新的 OctoStack 平台使在公司内部基础设施上托管 AI 模型变得更加容易。该平台可以在本地硬件、主要公共云和 AI 优化的基础设施即服务平台（如 CoreWeave）上运行。OctoStack 同样适用于英伟达集团和 Advanced Micro Devices Inc. 的多种 AI 加速器，以及 Amazon Web Services 中提供的 AWS Inferentia 芯片。</p><p>&nbsp;</p><p>在创建神经网络的初始版本后，开发人员可以通过各种方式对其进行优化，来提高性能。其中一种技术是运算符融合，它能够将人工智能执行的一些计算压缩为更少、更高效的硬件计算。另一种技术是量化技术，它减少了神经网络必须处理的数据量，能够产生准确的结果。</p><p>&nbsp;</p><p>此类优化并不总是适用于不同类型的硬件。因此，针对一款显卡优化的 AI 模型不一定能在不同芯片制造商的处理器上高效运行。OctoStack 采用的开源技术 TVM 可以自动优化不同芯片的神经网络。</p><p>&nbsp;</p><p>OctoAI 表示，这一平台可以帮助客户更高效地运行 AI 基础设施。据该公司称，由 OctoStack 驱动的推理环境提供的显卡利用率是自建 AI 集群的四倍。该公司还承诺将运营成本降低 50%。</p><p>&nbsp;</p><p>OctoAI 联合创始人兼首席执行官 Luis Ceze 表示：“要让客户能够构建可行且面向未来的生成式 AI 应用程序，需要的不仅仅是经济实惠的云推理。硬件可移植性、模型入门、微调、优化、负载平衡——这些都是需要全栈解决方案的全栈问题。这正是我们在 OctoAI 构建的。借助 OctoStack，客户可以根据自己的情况利用我们行业领先的 GenAI 系统，控制他们的数据、模型和硬件。”</p><p>&nbsp;</p><p>OctoStack 的主要优点：</p><p>快速运行任何模型：&nbsp;企业选择理想的开源模型组合（例如 Llama、Mistral、Mixtral 等）、定制模型和专有模型，同时最大限度地提高性能。在任何环境中运行：在用户选择的云中的云虚拟专用连接 (VPC) 中运行：AWS、Microsoft Azure、Coreweave、Google Cloud Platform、Lambda Labs、OCI、Snowflake 等。选择任何硬件目标：&nbsp;在广泛的硬件上运行模型，包括 NVIDIA 和 AMD GPU、AWS Inferentia 等。专业知识和创新：受益于 OctoAI 在独立于硬件的全栈推理优化方面无与伦比的专业知识，这些专业知识经过多年的专门研究和开发磨练而成。持续优化：本地客户可以获得类似于 SaaS 优势的持续更新，包括订阅新优化的模型和对其他硬件类型的支持，确保他们的 AI 能力始终保持领先地位。</p><p>&nbsp;</p><p>OctoStack 支持流行的开源 LLM，例如 Meta 公司的Llama 和初创公司 Mistral AI 开发的Mixtral专家混合模型。公司还可以运行内部开发的神经网络。据 OctoAI 称，OctoStack 可以随着时间的推移更新推理环境中的 AI 模型，而无需对其支持的应用程序进行重大更改。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/HPgEAkkw1hjgIDsxWNl0</id>
            <title>一口气发 100 个开源模型、主力模型再大降价！阿里：不然谈什么应用爆发？！</title>
            <link>https://www.infoq.cn/article/HPgEAkkw1hjgIDsxWNl0</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/HPgEAkkw1hjgIDsxWNl0</guid>
            <pubDate></pubDate>
            <updated>Mon, 23 Sep 2024 06:23:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>整理 | 华卫、褚杏娟</p><p></p><p>这两天，阿里在云栖大会上又开“卷”大模型了。一口气上架 100 多个开源模型、主力模型再大降价，“量多”又“价低”的策略又给大家带来了一点“小小”的震撼。</p><p></p><p></p><h1>100 多个开源模型“量大管饱”</h1><p></p><p></p><p>阿里云 CTO 周靖人发布了通义千问新一代开源模型 Qwen2.5，Qwen2.5 全系列总计上架了 100 多个模型，涵盖多个尺寸的大语言模型、多模态模型、数学模型和代码模型，其中每个尺寸都有基础版本、指令跟随版本和量化版本，创造大模型开源史上之最。</p><p></p><p>“这些（模型）不是竞争的关系，而是把选择留给我们的开发者。”周靖人在回答媒体提问时说道，“今天，开发者会基于自己的业务场景去做模型能力增强和推理效率增强的权衡与选择，很多时候我们不能帮大家选择。”</p><p></p><p>周靖人表示，Qwen2 在最开始时只推出两款，7B 和 70B，之后团队得到整个开发者社区的反馈，希望阿里能推出更多版本，包括 14B、32B 和 3B。“这次我们就把整个模型系列推出来，将更多选择权留给开发者。”</p><p></p><p>另外，周靖人谈到，从去年开始，阿里非常坚定不移做开源，这跟其做云计算的初心是密切相关的。“一方面，我们希望生态的发展。另一方面，我们还是希望能更有效服务到企业。”</p><p></p><p>据了解，Qwen2.5 在语言模型方面开源了 7 个尺寸，0.5B、1.5B、3B、7B、14B、32B、72B。型号的设定考虑了下游场景的不同需求，比如 3B 是适配手机等端侧设备的黄金尺寸；32B 是受开发者期待的“性价比之王”，可在性能和功耗之间获得最佳平衡；而 72B 是 Qwen2.5 系列的旗舰模型，阿里在多个核心任务的测评上，以不到 1/5 的参数超越了拥有 4050 亿巨量参数的 Llama3.1-405B。</p><p></p><p>专项模型方面，阿里用于编程的 Qwen2.5-Coder 在当天开源了 1.5B 和 7B 版本，据悉未来还将开源 32B 版本；专门用于数学的 Qwen2.5-Math 当天开源了 1.5B、7B、72B 三个尺寸和一款数学奖励模型 Qwen2.5-Math-RM。</p><p></p><p>多模态模型方面，广受期待的视觉语言模型 Qwen2-VL-72B 也正式开源。Qwen2-VL 能识别不同分辨率和长宽比的图片，理解 20 分钟以上长视频，具备自主操作手机和机器人的视觉智能体能力。</p><p></p><p>根据阿里的统计，截至 2024 年 9 月中旬，通义千问开源模型下载量突破 4000 万，Qwen 系列衍生模型总数超过 5 万个，成为仅次于 Llama 的世界级模型群。</p><p></p><p>在大模型开源领域，Llama 依然领先。根据 Meta 的最新数据， Llama 系列模型的下载量已接近 3.5 亿次（比去年同期增长了 10 倍多），仅在上个月，下载量就超过 2000 万次，使 Llama 成为领先的开源模型家族。Hugging Face 上有超过 60,000 个衍生模型，开发者会根据自己的用例对 Llama 进行微调。</p><p></p><p>“Llama 的成功得益于开源的力量。通过公开我们的 Llama 模型，我们看到了一个充满活力和多样化的 AI 生态系统，开发人员拥有比以往更多的选择和能力。从突破新界限的初创公司到使用 Llama 在本地或通过云服务提供商构建的各种规模的企业，创新范围广泛且迅速拓展。”Meta 官方表示。</p><p></p><p>对比之下，OpenAI 的 GPT 系列、Anthropic 的 Claude 大模型、百度的文心大模型、华为的盘古大模型等都选择了闭源。这反映了业内对大模型发展的判断有很大的不同，但到底哪条路是可以一直走下去的，目前还没有答案。</p><p></p><p></p><h1>大模型还太贵了，“资费降下来才能爆发”</h1><p></p><p></p><p>过去两年，模型的尺寸已增长数千倍，但模型的算力成本正在持续下降，企业使用模型的成本也越来越低。继 5 月首次大幅降价后，阿里云在大会上宣布，通义千问三款主力模型再次降价，Qwen-Turbo 价格直降 85%，低至百万 tokens 0.3 元，Qwen-Plus 和 Qwen-Max 分别再降价 80% 和 50%。</p><p></p><p>“我们希望企业和开发者能以最低的成本做 AI、用 AI，让所有人都能用上最先进的大模型。”阿里云智能集团首席技术官周靖人指出，只有这样才能带动整个行业的发展，激发更多产业级的创新。今天模型的运用、迭代和各种创新都还在早期阶段，这个时候如果把模型推理放到昂贵级别，会有大量开发者无法有效地批量化使用 AI。</p><p></p><p>他表示，阿里降价主要是通过技术的优化，不光是模型本身在快速迭代，模型推理架构、系统优化和云基础设施也在不断提升，而这些都能把模型的整体成本进一步降下来。</p><p></p><p>今年上半年，AI 大模型行业多番掀起价格战。除阿里外，百度、科大讯飞、字节、智谱等多家企业都先后大幅降价，甚至已经打到了“负毛利”的状态。谈及大模型降价的底线，周靖人直言，今天的大模型价格不存在“已经足够低”的说法，相对未来庞大的应用来说，还太贵了。</p><p></p><p>“这是 AI 基础设施全面革新带来的技术红利，我们会持续投入先进 AI 基础设施的建设，加速大模型走进千行百业。”周靖人表示，阿里云正在围绕 AI 时代，树立一个 AI 基础设施的新标准，全面升级从服务器到计算、存储、网络、数据处理、模型训练和推理平台的技术架构体系，让数据中心成为一台超级计算机，为每个 AI 和应用提供高性能、高效的算力服务。</p><p></p><p>大会现场，周靖人展示了 AI 驱动的阿里云全系列产品家族升级。最新上线的磐久 AI 服务器，支持单机 16 卡、显存 1.5T，并提供 AI 算法预测 GPU 故障，准确率达 92%；阿里云 ACS 首次推出 GPU 容器算力，通过拓扑感知调度，实现计算亲和度和性能的提升；为 AI 设计的高性能网络架构 HPN7.0，可稳定连接超过 10 万个 GPU ，模型端到端训练性能提升 10% 以上；阿里云 CPFS 文件存储，数据吞吐 20TB/s，为 AI 智算提供指数级扩展存储能力；人工智能平台 PAI，已实现万卡级别的训练推理一体化弹性调度，AI 算力有效利用率超 90%。</p><p></p><p>用阿里云副总裁、公共与客户沟通部的总经理张启的话说，“阿里现在在疯狂的搞 AI 大基建，把资费降下来，才有可能去谈未来应用的爆发。”</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/W187D2zUrvg3XvA2YBcL</id>
            <title>一年砸 10 亿只是开始，被“神话”的端到端，在中国自动驾驶圈的“最佳实践”能挣钱吗？</title>
            <link>https://www.infoq.cn/article/W187D2zUrvg3XvA2YBcL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/W187D2zUrvg3XvA2YBcL</guid>
            <pubDate></pubDate>
            <updated>Mon, 23 Sep 2024 06:15:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>作者 ｜ 华卫</p><p></p><p>预计再过不到半年，特斯拉 FSD 就将正式入华了。9 月 5 日，特斯拉宣布，FSD 将于 2025 年第一季度在中国和欧洲推出。</p><p></p><p>而前不久，基于端到端的特斯拉 FSD V12 版本在推送后得到了业内外的众多好评。就连曾多次与特斯拉公开“互怼”的小鹏汽车董事长何小鹏，都发文评价特斯拉自动驾驶“表现极好”，还激动地表示“2025 会是完全自动驾驶的 ChatGPT 时刻！”</p><p></p><p>以 GPT 为代表的大模型正以其前所未有的创新速度和技术架构，深刻影响着自动驾驶领域的方案研究与发展模式，并且全球的行业版图都在迅速响应这一热潮。从目前国内车企的发力重点来看，端到端也已成为其新一代自动驾驶技术路线。</p><p></p><p>华为、小鹏、小马智行、Momenta、极佳科技、地平线等乘用车自动驾驶企业都在积极跟进，纷纷推出了面向量产的端到端自动驾驶解决方案和车型。在商用车方面，零一汽车也公布了端到端大模型上车的明确时间规划。理想汽车创始人兼 CEO 李想也公开声称，理想汽车将在三年内依靠端到端和世界模型实现 L4 级自动驾驶。</p><p></p><p>就连此前遭遇“寒流”的 L4 自动驾驶市场，也因端到端技术的到来有所回暖。依靠这一技术概念拿下 10 亿美元级别融资的 Wayve，便是一大例证。辰韬资本投资经理刘煜冬表示，“端到端为 L4 商业化开启了第二个成长曲线。”</p><p></p><p>凭借端到端令 FSD 能力飞跃的特斯拉还宣布，将在 10 月 10 日推出 Robotaxi 车型。何小鹏也公开透露，小鹏汽车将在 2026 年推出 Robotaxi。然而，近期车企和自动驾驶厂商一心通过端到端方案实现量产 L4 的动作和预期，引来了不少自动驾驶从业者的质疑：端到端是否被过度“神话”？</p><p></p><p></p><h1>端到端为何突跃智驾圈“顶流”？</h1><p></p><p></p><p>端对端并非这一两年内才横空出世，早在 2017 年就有不少公司在探索这种技术路线的可能性。今年“端到端”在自动驾驶圈翻红并被业内视为杀手锏技术，除 ChatGPT 等大语言模型带来的革新外，与其自身的“魅力”也有必不可分的关系。</p><p></p><p>“端到端模型的诞生，是自动驾驶技术通向大规模商业化的必经之路。”小马智行联合创始人、首席技术官楼天城表示，端到端模型最大的优点之一是泛化性，泛化性能够提高自动驾驶商业化的速度，加速自动驾驶的普及。</p><p></p><p>而据零一汽车智驾负责人王泮渠介绍，相比端到端，传统非端到端的自动驾驶系统不仅泛化性较差，而且在向新场景扩展时，很多之前所用的基于规则的方案会失效，新增加的代码又会使系统的可维护性变差，继而导致边际成本的上升。</p><p></p><p>除此之外，传统自动驾驶系统还存在两方面的劣势。第一是架构的复杂性，多模块的系统不仅开发成本更高，由于每个模块所分配到的计算资源较少，其性能上限也比较低，模块间的通信还会带来很多工程上的优化问题。第二是架构复杂带来的高成本问题，每个模块都需要做开发、维护、项目管理和集成等工作，这也是传统自动驾驶公司的团队规模都非常大的原因。</p><p></p><p>“在我看来，端到端能很好地解决这些问题。”王泮渠表示，从架构来说，端到端只有一个模块，可以很好地解决架构复杂问题，同时也具有降本增效的优势。基于数据甚至知识驱动的端到端泛化性非常强，很有可能快速实现量产，不仅可以将 L2 适配各种车型的成本降得非常低，还能够帮助 L4 减少适配不同场景的时间。</p><p></p><p>另外，楼天城指出，端到端最大的好处是防止不同模块和功能之间信息的丢失。极佳科技工程副总裁毛继明也谈到了这一方面，并解释道，模块之间涉及信息的有效传递问题，上下游模块的接口定义了传输信息的上限，但无论多么精细的接口设计，都会存在信息损失。而端到端的统一模块方案不存在这种信息损失，有助于提升最终的算法效果。</p><p></p><p>同时，毛继明还谈到了端到端架构拥有的更多其他优势。首先是模块误差方面，由于端到端在一个模块下，不存在多模块的误差放大效应，整体智驾算法的能力上限也得以最大程度的提升。其次，多模块架构中，每个模块都有单独的研发节奏和优化目标，并不总能严格对齐整个智驾系统的全局优化目标，导致了潜在的无效优化以及研发资源的投入浪费；而端到端架构只有一个模块，优化目标明确统一，可以有效避免这种内耗式的优化过程。</p><p></p><p>还有一点是，模块化架构的组件之间天然容易构成多个规则驱动的“域”，带来一系列维护挑战以及 corner case 解决困境；而端到端作为典型的完全数据驱动架构，会促使开发人员更积极主动的开始从数据驱动、模型驱动的思维范式去考虑问题和解决问题，提升整个算法团队的认知水平。</p><p></p><p>“整体来讲，端到端系统的开发效率更高，资源消耗也更少。”刘煜冬表示，端到端纯数据驱动的开发范式会减少很多原来的重工程资源投入，并将企业的资源重心转向数据驱动方面的高人才密度以及数据积淀的投入。</p><p></p><p>值得一提的是，端到端带来的用户价值也备受关注。刘煜冬指出，第一，在长尾场景的处理上，端到端系统能够比原来的系统覆盖更多的极限场景，如常识处理能力。第二，自动驾驶系统的行为更加拟人化，也能够更强地建立消费者和系统之间的信任，端到端在博弈性比较强的场景里更像人类司机。</p><p></p><p></p><h1>上限高、下限低，自动驾驶的“终局”还没到？</h1><p></p><p></p><p>尽管端到端的技术优势显著，且一众车企和自动驾驶企业都在积极跟进端到端的应用，但对于所谓“终局模式”的说法，业内至今仍众说纷纭。</p><p></p><p>坚定派如王泮渠表示，“我相信端到端一定是实现自动驾驶的最终形式，但端到端只是一个技术框架，不过具体实现的方式其实有非常多的选项，目前业界还没达成共识。”</p><p></p><p>理智派如毛继明指出，端到端方案具有“上限高，但下限低”的特点。通俗来说就是，做得好可以达到很好的效果，做的不好比传统方案更差。在毛继明看来，是否选择端到端方案要看具体的应用场景。对 L5 级无人驾驶而言，端到端就是唯一解；但对于 L2 和 L3 来说，端到端就只是其中一个可行方案。并且，端到端在应用时需要与其他技术方案进行组合搭配。</p><p></p><p>“端到端给自动驾驶的快速大规模普及提供了很好的技术路径，是否是终局还有待时间验证。”楼天城也有类似看法，认为目前无论是 L2 级还是 L4 级自动驾驶都已经在实现，但实现的质量如何、在多大范围实现，对技术有着不同的要求与标准。</p><p></p><p>对 L2 级别自动驾驶来说，端到端技术是目前的更优路径；对 L4 级无人驾驶来说，端到端可以帮助其快速开拓新区域。但 L4 对安全性要求更高，要达到人类驾驶员的 10 倍以上，因此除使用端到端外，还需结合驾驶意图、应用场景融入高确定性的指令，如交通法规、驾驶偏好等。</p><p></p><p>刘煜冬则给出了更为谨慎的论断：“目前来看端到端是可以预见的未来时间段内自动驾驶的终局，但是更长周期的技术演变有各种可能性。就像三年前我们想不到会有 ChatGPT 这样的技术出现，两三年之后也可能有新的技术架构出现颠覆现在的 ChatGPT。”</p><p></p><p></p><h1>100% 端到端还未出现，何为“最佳实践”？</h1><p></p><p></p><p>虽然尚不能明确端到端是否为自动驾驶的终局方案，但其落地应用显然已成为智能驾驶行业的共识方案。然而，业内对于端到端自动驾驶技术路径的选择仍存在诸多争议。</p><p></p><p>目前，零一汽车正沿着基于多模态大语言模型的端到端路线前进，不仅在一些公开的数据集上做出了效果，还在今年上海人工智能实验室联合 CVPR 等举办的自动驾驶国际挑战赛上，凭借纯视觉的自动驾驶解决方案，在端到端自动驾驶赛道的 143 支国际团队中拿到第二的成绩。</p><p></p><p>王泮渠认为，模块化端到端相当于是一个前期的探索，能更快地去做落地，目前学术界和工业界也有了相对成熟的方案。而采用基于多模态大模型的端到端技术路线有把自动驾驶变成赚钱生意的发展潜力，并且只有强泛化性的基座模型才能带来自动驾驶领域所需要的知识注入和融会贯通。</p><p></p><p>简单来说，大模型的强泛化性会为整个端到端系统带来性能的优势，也会为未来实现大规模量产的可盈利高阶自动驾驶带来可能。并且，未来分别基于多模态大模型和世界模型的这两条端到端技术路线可以互相复用。</p><p></p><p>刘煜冬表示，从原理上讲，one model 更加接近其他领域的 AGI 形态，而世界模型目前主要是数据生成的工具，能否用作自动驾驶系统还需要更长时间来观察。未来两年之内，落地的端到端方案主要有两种：一是模块化的端到端，典型代表是上海人工智能实验室的 UniAD；二是以多模态大模型为主要基础的 one model 端到端，如 Wayve 的 LINGO-2 和理想最近推出的 DriveVLM。</p><p></p><p>而对于世界模型，毛继明持有不同看法。他认为，世界模型才是端到端的合理解决思路。基于世界模型，智驾算法拥有了理解场景并对未来进行合理预测的能力，并基于这些信息做出决策，这才是更符合人类思维逻辑的方案。</p><p></p><p>极佳科技联合创始人、首席科学家朱政进一步补充道，one model 训练起来非常耗资源和时间，对于数据的规模和质量都有非常高的要求。而端到端是利用模型预测能力来进行场景感知和驾驶行为的决策，同人类的驾驶行为和习惯比较一致。据其介绍，目前极佳已经有了基于世界模型的端到端基础原型系统，正在和一家车厂做上车的联合验证，很快就会公开一些进展。</p><p></p><p>去年 8 月，小马智行将感知、预测、规控三大传统模块打通，统一成 one model 端到端自动驾驶模型，目前已同步搭载到 L4 级自动驾驶出租车和 L2 级辅助驾驶乘用车。在楼天城看来，目前无论是模块化端到端还是 one model，都处于早期阶段，还未经过量产交付的验证。预计未来 1 至 2 年内，端到端的技术路线才会从分歧走向共识。</p><p></p><p>“长远来看，端到端的终局终究要步入到 one model 形态。”毛继明表示，就当前状态，华为、小鹏等公司所采用的“两段式”端到端还都属于半端到端的实现，或者说处在端到端的过渡态而非完整态。</p><p></p><p>前不久，极越汽车 CEO 夏一平也公开谈到，“现在市场上没有任何一家是眞的端到端，都是营销的噱头。”据了解，目前极越的端到端智驾方案采用的也是“两段式”技术架构。</p><p></p><p></p><h1>“黑盒”属性是误会，可以做成类似灰盒或白盒</h1><p></p><p></p><p>端到端方案的一系列优势，源于其将多个模块融合在一起的架构，但这种设计也使得系统相较于原先的可理解的“白盒”更接近“黑盒”了，从而具备了更多“不可解释性”。</p><p></p><p>楼天城认为，不可解释性是端到端系统天然的缺陷，但是否会限制端到端自动驾驶技术的发展，要分情况看。对于 L2 来说，不可解释性并不影响端到端的应用，比如模块化端到端仍保留了各个主要功能模块，中间的输出特征可以被进一步提取为可解释的数据。</p><p></p><p>而对 L4 来说，其对安全性和确定性的要求是远高于 L2 的。因此，需要在模型中融入规则性的指令，如交通法规、驾驶偏好等，帮助端到端自动驾驶模型更好理解驾驶意图。与此同时，也需要升级模型能力，以对外输出驾驶意图，进一步提高可解释性。</p><p></p><p>而在朱政看来，虽然从产品层面和最终研发形态来看，端到端确实是一个黑盒，但以工程师和产品设计包括用户的角度来看，端到端是可以做成类似于灰盒或者白盒的。</p><p></p><p>第一，模块化联合端到端详细地区分了感知、预测和规划三个模块，任意一个 planning 结果都可以和前面的某个中间模块关联起来。第二，one model 可以输出模块化的中间结果，标注该结果用来做中间监督可以让 one model 收敛得更好，也可以把模式化中间结果拿出来给工程师或用户看。第三，世界模型最重要的是预测能力，而它的预测结果也可以和模式化的中间结果关联起来。</p><p></p><p>毛继明谈到，目前端到端的“黑盒”说法是对整个模型的训练推理细节的误解。只要把研发认知变成可对外解释的形态呈现出来，就不再是黑盒了。</p><p></p><p>王泮渠同样认为，不可解释性的提出反映的是公众对技术的信任度，即技术本身性能是否达到了大家可以接受的标准。随着数据驱动、算法设计，大模型安全等相关技术的发展，在未来一到两年端到端的性能与可靠性一定会有非常大的飞跃。再通过性能的大规模测试与充分验证，其可解释性不再会是关键问题。</p><p></p><p></p><h1>端到端上车“高峰”将到来，商用车更快落地</h1><p></p><p></p><p>“模块化端到端规模化上市就在最近一年内，基于大语言模型的端到端还额外需要 1 到 2 年的时间。”王泮渠指出，商用车的 L4 自动驾驶一定比乘用车的落地速度要快，原因是能大规模量产的高阶自动驾驶系统非常挑落地场景的难度，而商用车场景比乘用车简单，且单个场景容易商业闭环，也方便做场景渐近。</p><p></p><p>刘煜冬则更为乐观，认为明年模块化端到端和 one model 的端到端就会开始比较密集地开始推送。此外，刘煜冬站在技术发展的激进程度和人才集聚、技术迭代速度和技术应用难度上表示，端到端在商用车和乘用车真正落地的时间可能会差不多，但乘用车的落地范围会比商用车大，商用车要到后期再慢慢起来。</p><p></p><p>“端到端量产前必须要跨过这几道关，首先就是车端算力的准备，第二是端到端算法的迭代，第三是云端数据规模，第四是算力规模，第五是验证方案。”毛继明表示。</p><p></p><p>在他看来，目前特斯拉以及国内的蔚小理、华为等头部主机厂和公司，在车端算力、云端数据规模、和云端算力规模这三项上已经都齐备了。今年年底到明年上半年，几家头部车企的端到端算法就能够达到规模化上车；明年下半年起，行业就会迎来端到端量产上车的井喷状态。</p><p></p><p></p><h1>入局端到端，意味着“重新来过”？</h1><p></p><p></p><p>端到端系统的开发和采用，无疑会对整体智驾方案带来技术革命。那么，入局端到端要对之前的技术推倒重来吗？</p><p></p><p>刘煜冬认为，原来的自动驾驶技术并不会被完全颠覆，端到端会与其共用某些算法和软件方面的积累。</p><p></p><p>一是感知部分，现在很多端到端的前端摄像头信息处理部分会用到 BEV 的做法，如 backbone 或 encoder 之类。二是规控部分，原来在规控的一些 knowhow 可以迁移到端到端系统里。三是数据基建，这是企业未来做端到端所需的重要能力，能做好 BEV 方案的公司数据基建也比较强。</p><p></p><p>而在毛继明看来，是否会推翻，取决于之前的技术方案是什么。他表示，端到端以纯数据驱动的多模态大模型为核心，如果某智驾公司之前的技术方案有很多规则，那这些规则基本上就都要被推翻了；如果之前的技术方案就已经大部分改为模型驱动了，那这部分代码大概率能以某种形式重用。</p><p></p><p>需要强调的是，端到端算法所带来的研发模式更改，才是每个主机厂和自动驾驶公司需要关注的重点，也是最痛苦的地方。</p><p></p><p>王泮渠还谈到，除了模型端，端到端也需要做更多数据方面的工作：第一需要重构数据闭环体系及其迭代效率，第二是端到端的测试和验证，整个仿真平台的传感器输入都要做得非常真实，这是目前很有挑战的技术问题。但人力成本上，端到端智驾系统整体花费是低于非端到端的，因为端到端只有很少的模块，核心团队有 20-30 个工程师应该就够了。</p><p></p><p>此外，毛继明指出，从传统的模块化架构到端到端模式，智驾方案的成本构成也会发生转变：大量写各种规则的研发专家的人力成本会迁移到数据方面。这对有量产能力的主机厂来说是件好事，由于其获取数据的成本较低，智驾方案的整体成本实际会进一步显著下降。</p><p></p><p>算力投入方面，楼天城表示，短期来看，购买大算力芯片确实会增加当下成本。但长期来看，一旦端到端技术成熟应用，前期投入成本会逐渐摊薄。</p><p></p><p></p><h1>纯端到端算力投入小于模块化架构，一年至少一到两个亿</h1><p></p><p></p><p>“想要端到端模型达到比较好的训练程度，一年至少需要一到两个亿的算力资金投入，乘用车赛道数字肯定会更加可观。”</p><p></p><p>据王泮渠介绍，端到端所需的算力分训练和部署两方面。部署相当于要采购多少块域控，这部分成本固定且比较低，与单车成本相关。最大的成本是训练成本，分自建买卡和跟云服务商合作两种。对订单量比较大的车企来说，自己造数据中心是合算的选择；但对订单量没有那么大或处在前期研发阶段的车厂来说，找云服务商租服务器是较好的选择。</p><p></p><p>此前，理想汽车智能驾驶副总裁郎咸朋曾公开透露，目前理想每年投在算力训练上的花费是 10 亿人民币，预计未来每年都花费要到 10 亿美金。“如果一年拿不出 10 亿美金来去做训练的话，可能会在将来的自动驾驶竞争当中被淘汰。”</p><p></p><p>算力规模上，楼天城认为，如果只是简单的一次端到端自动驾驶模型训练，上百张大算力的 GPU 就可以支持。如果要长期投入，并保证端到端质量，各个自动驾驶公司的训练算力规模基本在上千卡级别，车企投入会更多。</p><p></p><p>毛继明则给出更为具体的端到端算力需求：整个系统至少需要两台英伟达 Orin 或者单台英伟达 Thor。他表示，纯端到端系统的算力需求小于模块化架构的总算力需求，但量产端到端除了主系统外，往往还有一个旁路系统，其算力需求一般与之前的模块化架构的相当。</p><p></p><p>但王泮渠认为，随着车端计算芯片能力的上升，算力并不会成为未来端到端上车的障碍。楼天城持相同看法，表示从经典架构到端到端，总的代码数量会显著降低，端到端神经网络带来的计算资源消耗相比 BEV 模型并不一定会显著提升。</p><p></p><p>“对更高算力的渴望更多来自于模型参数量和模型性能的提升，而不是来自于端到端的转变。”另外，他指出，从端到端落地应用角度出发，相关企业更应该思考的是，如何充分利用现有的芯片算力资源提升利用效率。</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/8cvb1qBo4dzX9zRYYDif</id>
            <title>100T极致算力+全链路开发支持，地瓜机器人为具身智能造“基座”</title>
            <link>https://www.infoq.cn/article/8cvb1qBo4dzX9zRYYDif</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/8cvb1qBo4dzX9zRYYDif</guid>
            <pubDate></pubDate>
            <updated>Mon, 23 Sep 2024 02:53:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月20日，以“加速智能生长”为主题的“2024地瓜机器人开发者日”活动在深圳成功举办。作为业界领先的机器人软硬件通用底座提供商，地瓜机器人重磅推出面向“机器人+”时代的软硬件产品全家桶，包括专为新一代通用机器人而生的旭日5智能计算芯片、极致易用全能开发首选RDK X5机器人开发者套件、具身智能全场景算力核心RDK S100，以及赋能全链条全生命周期的机器人云端开发环境，软硬结合、端云一体，让开发更简单、让机器更智能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/46/467088105000950dc4ff0bd30ee113ad.png" /></p><p></p><p>伴随大模型和具身智能技术的突破，机器人有望在2030年成为下一轮产业升级的重要引擎。统一的软硬件通用开发底座，将为机器人赋能千行百业夯实根基。站在“机器人+”时代的爆发前夜，地瓜机器人将以全场景覆盖的机器人开发套件全家桶、全链路机器人开发基础设施、全产业链协同的生态布局，加速机器人智能升级，并为具身智能从“实验室”走向“应用场”提供最优解。</p><p></p><p>活动上，地瓜机器人重磅宣布RDK S100将由星动纪元、逐际动力、求之科技、睿尔曼、国讯芯微等数家行业顶级合作伙伴率先搭载。地瓜机器人CEO王丛表示：“AI大模型正为具身智能赋予更通用、更泛化的智慧大脑，国内完善的机器人产业链为具身智能发展提供了肥沃的土壤，而地瓜机器人在软硬件开发基础设施领域的长期积淀，能够为机器人开发构建更简单、更高效的创新路径。正是这样的‘天时、地利、人和’，地瓜机器人有信心成为具身智能发展的通用底座，携手广大生态伙伴和开发者共同孕育‘机器人+’的全新时代。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/31/3140523afaf4c89b989a81537975d5d1.png" /></p><p>地瓜机器人CEO王丛</p><p></p><p></p><h2>打造机器人开发者套件全家桶，让更智能的机器人开发更简单</h2><p></p><p></p><p>当前，机器人行业处于硬件成熟度低、算法尚未收敛、底层工具重复造轮普遍的松散发展阶段，面临效率低、成本高的困境。地瓜机器人致力于打造“让机器人创造更简单的全链路开发基础设施”，从端侧的旭日智能计算芯片和RDK机器人开发者套件，到广泛兼容的机器人操作系统，再到云侧的机器人云端开发环境和机器人算法中心，为机器人设计、开发、测试、仿真验证到量产、迭代的每一个环节降本增效。</p><p></p><p>专为新一代通用机器人而生的旭日5智能计算芯片基于BPU贝叶斯架构而来，拥有高达10 TOPs算力，并提供充沛异构计算资源，能够为Transformer、BEV、Occupancy等复杂模型和最新算法提供极致的计算效率，仅3W功耗即可完成全栈计算任务，充分满足扫地机、割草机等消费级机器人对智能化升级的强大性能需求，以及恶劣使用环境下对功耗的严格要求。目前，地瓜机器人已经和诸多行业头部客户达成合作，共同推动基于旭日5的消费级机器人产品的量产落地。</p><p></p><p><img src="https://static001.geekbang.org/infoq/67/679ee2bac0280d2bb8b017e98e631c78.png" /></p><p></p><p>以不断迭代进化的智能计算芯片为核心，地瓜机器人RDK一站式智能机器人开发者套件全家桶在本次活动上全面亮相。横向覆盖从消费级机器人，到商清、巡检、AMR，再到人形/四足机器人乃至前沿科研的全场景需求；纵向配套RDK OS + NodeHub + RDK Studio + Cloud的全链路机器人开发平台，全方位助力机器人开发与规模化落地。</p><p></p><p><img src="https://static001.geekbang.org/infoq/50/500f491f4fa994d1c678813c87ebb0e4.png" /></p><p></p><p>RDK X5面向中小创客和个人开发者，提供极致算力性价比与极简开发体验，是千元内最佳智能计算平台。其拥有10 TOPs超大算力，只需一根Type-C线便能轻松实现全栈开发流程，同时接口丰富、拓展灵活。RDK X5还配套NodeHub的多种先进大模型和机器人算法，更有100+配套件自由选择，助力开发者一路通关，快速完成从0到1的产品创新。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/24/24106366920305d9bd550864c2de8ff6.png" /></p><p></p><p>RDK S100是面向具身智能前沿创新与应用突破的全场景算力核心，基于新一代BPU纳什架构设计，拥有百TOPs级算力，专为大参数Transformer优化，可以满足人形机器人、仿生机器人等具身智能应用场景对感知精度和泛化能力的高阶需求。RDK S100还提供丰富的图像传感器接入方式和多种接口拓展板，为具身智能机器人的开发提供灵活拓展性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/03/0372565789be5650e6131d93bf051810.png" /></p><p></p><p>地瓜机器人全链路开发平台将全面支持RDK X5和RDK S100，赋能机器人开发的全生命周期。全新的Cloud云端开发环境带来了行业内首个基于大模型的机器人设计开发助手RDK-Copilot，可辅助进行代码补全，并提供高效智能的问答助手；Data-Copilot可帮助客户实现高效的数据处理和利用，克服样本数量不足和样本获取难度大的挑战。</p><p><img src="https://static001.geekbang.org/infoq/f2/f2bfcd800aaec207554a905a02bd241c.png" /></p><p></p><p>地瓜机器人算法中心NodeHub亦全新升级，集成200+机器人开源算法与应用程序，涵盖Transformer、RWKV、CLIP、Mobile-SAM等多种端侧精调大模型，以及双目深度、Occupancy、AI+VSLAM、3D点云计算等自动驾驶级别感知算法，可为机器人赋予更具泛化性和通用性和强大感知、决策能力，并支持一键安装调用。RDK Studio让开发者可以通过直观易懂的可视化界面轻松上手机器人开发，仅需10分钟就能快速完成算法和应用部署，提升全流程开发体验。</p><p></p><p>地瓜机器人开发者生态负责人胡春旭表示：“机器人开发是一项庞杂的系统工程，我们的目标是打造‘好、快、多、省’的机器人开发新范式，让机器人开发更简单。从算力高效的好用开发者套件、快人一步的操作流程、丰富多样的算法资源，到极具吸引力的性价比，真正实现机器人开发的效率跃升。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/4a/4abf41866cc42c0bfd5adfb88050ae46.png" /></p><p>地瓜机器人开发者生态负责人胡春旭</p><p></p><h2>共建智能机器人产业繁荣生态，引领机器人体验变革与创新孵化</h2><p></p><p></p><p>以旭日智能计算芯片和RDK机器人开发者套件为核心，地瓜机器人致力于与行业客户、合作伙伴、创客、个人开发者携手共建软硬一体、产学研深度协同的智能机器人产业生态圈，引领消费级机器人体验变革、加速创新产品应用孵化，商业和生态成果硕果累累。</p><p></p><p>在商业成果方面，新一代旭日5智能计算芯片发布即爆款，率先应用于扫地机、割草机、机械臂、家庭陪伴、四足狗、工业相机、视频会议等领域多家行业头部客户的重点产品中。基于数百万的交付验证，旭日智能计算芯片以强大的计算性能、完善的开发支持和丰富的生态支持，在扫地机、割草机等消费级市场中份额持续领跑，已经成为消费级机器人智能升级的标配之选。</p><p></p><p>在生态成果方面，地瓜机器人基于独一无二的大规模量产实践，构建了从人才培养、创意孵化、产品加速到运营增长的全生命周期开发者赋能体系，已累计服务200+中小创客及50000+个人开发者，实现50+机器人品类覆盖。同时还携手200+合作院校，推出100+校园课程，建立1000+赛事队伍，组织100+场开发者活动，为机器人行业人才培养聚集产学研融合势能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bf/bfd2c0213e2e4ef672af554c83746b36.png" /></p><p></p><p>本次开发者日汇聚了近400名机器人开发者的热情参与，并举办了RDK X5开发工作坊、开发者TalkShow、DUP教育生态联盟研讨会、具身智能前沿论坛以及极客夜话等多场平行活动。一场场充实的上手开发实践、创新成果汇报、创业干货分享和前沿趋势探讨，让每一位开发者收获满满，更好地了解机器人创新、创业所需的一切。活动现场，更有基于地瓜机器人生态的各式机器人产品展出，一应俱全的产品形态和新奇功能，展现了机器人+时代的繁花似锦和无限可能。</p><p></p><p>承载着“成为机器人时代Wintel”的初心梦想，地瓜机器人如今以焕然新生的品牌形象，开启“机器人+”时代的全新旅途，将聚合更优质的产业资源，筑牢具身智能软硬件通用底座，构建智能机器人时代的母生态，加速机器智能进化，促进人机和谐伴生。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/js2cvHZ2r4FbH0g7EXgk</id>
            <title>内部数百工程师可随意摄取OpenAI先进模型！OpenAI前员工揭露：谏言即被开除，祈祷公司不报复</title>
            <link>https://www.infoq.cn/article/js2cvHZ2r4FbH0g7EXgk</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/js2cvHZ2r4FbH0g7EXgk</guid>
            <pubDate></pubDate>
            <updated>Mon, 23 Sep 2024 01:13:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>就在OpenAI新模型发布不久还被广泛赞誉的时候，OpenAI举报人对其悄悄“背刺一刀”。</p><p>&nbsp;</p><p>“我在OpenAI工作期间，运营体系有很长一段时间都存在漏洞，这些漏洞可能允许我或者公司内的其他数百名工程师绕过访问控制、窃取包括GPT-4在内的最先进AI系统。”OpenAI 举报人 William Saunders 在近日参加的一场关于人工智能监管的听证会上说道。</p><p>&nbsp;</p><p>去年年初，一名黑客入侵了OpenAI 的内部消息系统，并窃取了该公司 AI 技术设计的详细信息。这件事大家是最近才知道的。OpenAI 高管于 2023 年 4 月在公司旧金山办公室举行的全体会议上向员工透露了这一事件，并通知了董事会。但高管们决定不对外公开这一消息，因为没有关于客户或合作伙伴的信息被窃取。另外高管层也认为这与所谓国家安全没有关系，因为他们相信黑客是一名个人，与外国政府没有联系。</p><p>&nbsp;</p><p>Saunders在过去三年，一直担任OpenAI公司的技术人员。“OpenAI一直强调自己正在进步，但我和其他已经辞职的员工都怀疑他们能不能跟得上发展的节奏。”Saunders说道。</p><p>&nbsp;</p><p>他以OpenAI 最新模型o1为例称，这是首个向着生物武器风险迈进的系统，因为它能够帮助专家规划并重现已知的生物威胁。“如果不是经过了严格测试，o1项目的开发者很可能忽略掉这种危险的能力。虽然OpenAI率先开展了相关测试，但公司的总体思路仍然是以部署为优先、而非以严格监管为优先。因此我认为，OpenAI确实有可能遗漏未来AI系统当中某些重要的危险能力。”</p><p>&nbsp;</p><p>“我之所以从OpenAI辞职，是因为我不再相信他们会以负责任的态度制定AGI相关决策。”看得出来，他抨击的OpenAI 不止不关心AI安全，自家的安全也是一点都不担心。</p><p>&nbsp;</p><p></p><h2>批评OpenAI，是要被“惩罚”的</h2><p></p><p>&nbsp;</p><p>今年6 月有媒体曝出，OpenAI 员工如果想离开公司，将面临大量的、限制性极强的离职文件。如果他们在相对较短的时间内拒绝签字，就可能被威胁失去公司既得股权。这种做法在硅谷并不常见。这项政策迫使离职员工在放弃他们已经赚到的数百万美元和同意不批评公司之间做出选择，而且没有截止日期。</p><p>&nbsp;</p><p>这一消息在 OpenAI 内部引起了轩然大波。与许多硅谷初创公司一样，OpenAI 的员工通常以股权的形式获得大部分的预期薪酬。员工们往往认为，一旦按照合同中规定的时间表“归属”，这些股权就属于他们了，公司无法收回。</p><p>&nbsp;</p><p>外媒报道后的第二天，首席执行官 Sam Altman就发文道歉，总体意思是：我不知道我们有一些威胁公平的条款，我保证我们不会再这样做了。OpenAI 部分高管在公司内部也表达了同样的歉意。OpenAI 首席战略官 Jason Kwon 承认，该规定自 2019 年以来就已实施，但“团队确实在一个月前就发现了这个问题。这么久了才被发现，是我的错。”</p><p>&nbsp;</p><p>但外媒指出，公司领导层的道歉存在问题。终止文件中的离职信中写道：“如果您拥有任何既得的权益单位…您必须在60天内签署一份放弃索赔协议，以便保留这些权益单位。”该协议由 Kwon 和 前OpenAI 人力副总裁 Diane Yoon签署。这份秘密的超限制性保密协议仅针对已既得股权的“对价”，由首席运营官 Brad Lightcap 签署。</p><p>&nbsp;</p><p>用威胁既得股权的方式让前员工签署极其严格的保密协议只是一部分，这里面还涉及更多细节。OpenAI 发出的冗长而复杂的解雇文件有效期只有七天，这意味着前员工只有一周的时间来决定是接受 OpenAI “封杀”，在无限期内无法发声批评OpenAI，还是承担失去数百万美元的风险。但时间非常紧迫，他们几乎没有时间去寻找外部顾问。</p><p>&nbsp;</p><p>“我们希望确保你们明白，如果不签署，可能会影响你们的股权。这对每个人来说都是如此，我们只是按规矩办事。”OpenAI的代表如是说道。</p><p>&nbsp;</p><p>大多数前员工在压力之下屈服了。对于拒绝签署第一份终止协议并寻求法律顾问的员工，OpenAI改变了策略：没有说要取消股权，而是说阻止其出售股权。“你必须明白，此外你将没有资格参与未来我们赞助或促成的招标活动或其他流动性机会。”</p><p>&nbsp;</p><p>此外，公司文件中还包含，“根据公司的全权酌情决定权”，任何被公司解雇员工的既得股权可以减至零。还有条款规定，公司可以全权决定哪些员工可以参与出售其股权的招标要约。</p><p>&nbsp;</p><p></p><h2>解雇一个员工，找个理由就可以</h2><p></p><p>&nbsp;</p><p>可能有人还记得，今年OpenAI 因涉嫌泄露信息解雇了两名员工，其中一个就是Leopold Aschenbrenner。Aschenbrenner 后来解释了他为什么会被解雇，并透露了更多细节。</p><p>&nbsp;</p><p>“去年某个时候，我写了一份关于未来 AGI 道路上需要做的准备、安全和保障措施的头脑风暴文件。我与三位外部研究人员分享了这份文件以征求反馈。这就是泄密的内容。”Aschenbrenner 解释称，“当时在 OpenAI，与外部研究人员分享安全理念以获得反馈是完全正常的。这种情况一直都有。文档中有我的想法。在我分享之前，我审查了它是否有任何敏感内容。内部版本提到了未来的集群，我在外部副本中删去了它。有一些内部PPT的链接，但对于外部人员来说，这是一个无效链接。PPT也没有与他们分享。”OpenAI给他的回复是“计划在 2027-2028 年实现 AGI”，但公司并未设定准备时间表。</p><p>&nbsp;</p><p>但这并不是他被解雇的真相，真正的原因是他写了一份关于 OpenAI 安全性内部备忘录并与董事会分享了该备忘录。</p><p>&nbsp;</p><p></p><blockquote>去年，我写了一份关于 OpenAI 安全性的内部备忘录，我认为这份备忘录严重不足，无法防止外国参与者窃取模型权重或关键算法机密。我与几位同事和几位领导分享了这份备忘录，他们大多表示这份备忘录很有帮助。&nbsp;几周后，发生了一起重大安全事件。这促使我与几位董事会成员分享了这份备忘录。几天后，我清楚地知道，领导层对我与董事会分享这份备忘录非常不满。显然，董事会就安全问题向领导层提出了质问。&nbsp;我因与董事会分享备忘录而收到人力资源部门的正式警告。人力资源部的人告诉我，担心间谍活动是种族主义行为，而且没有建设性。我可能没有发挥出最佳外交能力，本可以更精通政治。我认为这是一个非常重要的问题。安全事件让我非常担心。&nbsp;我之所以提起这件事，是因为当我被解雇时，他们明确表示安全备忘录是我被解雇的主要原因。他们说：“这是解雇而不是警告，因为这份安全备忘录。”</blockquote><p></p><p>&nbsp;</p><p>&nbsp;</p><p>Aschenbrenner 透露，在被解雇前，他被拉到一边跟律师交谈，但双方很快就对峙了起来。律师问了他对人工智能发展的看法、对 AGI 的看法、AGI 的适当安全级别、政府是否应该参与 AGI、我和超级联盟团队是否忠于公司，以及他在 OpenAI 董事会活动期间做了什么。然后律师还和Aschenbrenner 的几个同事谈了谈，然后回来告诉就告他被解雇了。“他们查看了我在 OpenAI 工作期间的所有数字文件，然后发现了泄密事件。”</p><p>&nbsp;</p><p>此外，律师团队还提出了其他几项指控。其中一件是，Aschenbrenner 在调查期间不愿透露与谁分享了头脑风暴文件，他表示自己确实不记得了，只记得曾与一些外部研究人员讨论过这些想法。“这份文件已经有六个多月了，我花了一天的时间研究它。”Aschenbrenner 表示，“它根本就不值得关注，因为它根本不是什么问题。”</p><p>&nbsp;</p><p>Aschenbrenner表示，OpenAI 还声称不喜欢他参与政策的方式。</p><p>&nbsp;</p><p>“他们引用了我曾与几位外部研究人员谈过的观点，即 AGI 将成为政府项目。事实上，我当时正在与该领域的许多人讨论这一观点。我认为这是一件值得思考的重要事情。所以他们找到了我五、六个月前写给一位同事的 DM，他们也引用了这一点。”Aschenbrenner说道。“我曾认为，与该领域的外部人士讨论有关 AGI 未来的高层问题是符合 OpenAI 规范的。”</p><p>&nbsp;</p><p>OpenAI公司内部的其他员工都对这样的事情表示惊讶。</p><p>&nbsp;</p><p></p><blockquote>从那时起，我和几十位前同事谈过这件事。他们的普遍反应是“这太疯狂了”。我也感到惊讶。几个月前我才刚刚升职。当时，Ilya 对我升职一事的评论是：“Leopold 太棒了。我们很幸运能拥有他。”</blockquote><p></p><p>&nbsp;</p><p>“从某种意义上说，这是合理的。我有时在安全问题上可能很烦人，这惹恼了一些人。我反复提到这一点，也许并不总是以最圆滑的方式提出来的。尽管有压力要求我在董事会活动期间签署<a href="https://www.nytimes.com/interactive/2023/11/20/technology/letter-to-the-open-ai-board.html">员工信，但我并没有签署。</a>"”Aschenbrenner。</p><p>&nbsp;</p><p>Saunders 和其他OpenAI员工此前签署了关于“对先进人工智能发出警告的权利”公开信。他们认为，只要政府还没有对AI企业施以有效监督，那就只有现任及前任员工来负起责任。然而，广泛的保密协议却阻止了他们表达自身担忧的权利，除非企业主动承认他们无力解决这些问题。”</p><p>&nbsp;</p><p>Saunders在听证会上隐隐透露出担心被报复的想法。他表示，“鉴于整个行业的历史上曾经发生过极端情况，我们中有一部分人可能担心受到各种形式的报复。我们并不是第一批遇到、或者决定正视这些问题的人。”</p><p>&nbsp;</p><p></p><p>参考链接：</p><p><a href="https://www.judiciary.senate.gov/imo/media/doc/2024-09-17_pm_-_testimony_-_saunders.pdf">https://www.judiciary.senate.gov/imo/media/doc/2024-09-17_pm_-_testimony_-_saunders.pdf</a>"</p><p><a href="https://www.vox.com/future-perfect/351132/openai-vested-equity-nda-sam-altman-documents-employees">https://www.vox.com/future-perfect/351132/openai-vested-equity-nda-sam-altman-documents-employees</a>"</p><p><a href="https://www.dwarkeshpatel.com/p/leopold-aschenbrenner?open=false#%C2%A7what-happened-at-openai">https://www.dwarkeshpatel.com/p/leopold-aschenbrenner?open=false#%C2%A7what-happened-at-openai</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ag5uZyWgd2ofRQbPlsMy</id>
            <title>投资等于捐赠！？65 亿美元已经砸给 OpenAI，微软、苹果、英伟达争夺入场券，最新估值 1500 亿美元创历史</title>
            <link>https://www.infoq.cn/article/ag5uZyWgd2ofRQbPlsMy</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ag5uZyWgd2ofRQbPlsMy</guid>
            <pubDate></pubDate>
            <updated>Mon, 23 Sep 2024 01:09:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>&nbsp;</p><p>据知情人士透露，OpenAI的最新一轮融资即将完成，潜在投资者将于本周五内得知是否有资格参与交易。该公司已要求投资者至少出资2.5亿美元。</p><p>&nbsp;</p><p>知情人士表示，OpenAI 的65亿美元融资已经超额认购，也就是说投资者希望投入的资金超过了该公司准备接收的金额。其中一位知情人士还提到，超额认购金额高达数十亿美元，部分投资者将于上周五得知自己是否入选。OpenAI方面拒绝对此发表置评。</p><p>&nbsp;</p><p>这位知情人士表示，包括OpenAI最大支持者微软公司以及新投资方英伟达及苹果在内的多家战略投资方，有望顺利获得融资参与资格。其中一位知情人士表示，这些科技公司在这轮融资中所占的份额预计将达到20至30亿美元，但目前还不清楚他们是投入现金，还是现金和其他资源（如计算能力）的混合投入。此前消息，英伟达有意向在OpenAI新一轮融资中投入1亿美元。</p><p>&nbsp;</p><p>此外，这笔交易还将推动OpenAI的估值达到1500亿美元（大约10000亿人民币），且该数字还不包含本轮新投资。OpenAI在此前一笔融资交易中的估值为860亿美元，这次融资让OpenAI估值提高了 74%。</p><p>&nbsp;</p><p>知情人士指出，原投资方Thrive Capital将领投本轮融资，并开出12.5亿美元的支票。Thrive Capital方面同样拒绝发表置评。</p><p>&nbsp;</p><p>此外，至少将有一家知名原OpenAI投资方不会参与最新一轮融资，即红杉资本。红杉最近支持了OpenAI的竞争对手AI厂商Safe Superintelligence，后者由OpenAI联合创始人Ilya Sutskever创立，并于今年早些时候离开了Sam Altman领导的OpenAI。红杉没有立即回应置评请求。</p><p>&nbsp;</p><p>此轮融资一旦最终敲定，OpenAI 就会成为硅谷历史上最有价值的科技初创公司之一，超过支付公司 Stripe在 2021 年私募融资中实现的 950 亿美元估值，还将使这家初创公司获得全球三家最有价值科技公司的资金支持。</p><p>&nbsp;</p><p>在本轮融资之前，OpenAI 已经从微软那里获得了100亿美元的B轮融资，OpenAI融资后的利润分配分为4个阶段：优先保证马斯克等首批投资者收回资本，保证他们不亏钱；微软将有权获得OpenAI 75%的利润，直到收回前后共130亿美元的投资；在OpenAI的利润达到920亿美元后，微软的利润分配比例将下降到49%，剩余49%的利润由其他风险投资者和OpenAI的员工作为有限合伙人分享；在利润达到1500亿美元后，微软和其他风险投资者的股份将无偿转让给OpenAI的非营利基金。本次融资后的利润分配还未可知。</p><p>&nbsp;</p><p>此前还有报道称，OpenAI 讨论改变其公司结构，使其变得更加有利于投资者。此前投资者必须签署一份运营协议，其中规定：“以捐赠的精神看待对[OpenAI 营利性子公司]的任何投资是明智的”，并且 OpenAI“永远不会盈利”。但这并没有阻碍大家的投资热情。目前还不知道OpenAI 的是否会改变结构，但此前考虑的一个选择是取消对营利性子公司投资者的现有利润上限。</p><p>&nbsp;</p><p>OpenAI 的一位投资者表示，转向更简单的营利结构将受到投资者的欢迎。“所有优先投资者都有利润上限，很多人都在谈论将其变成一种更传统的投资，这样我们的上行空间就不会受到限制。”</p><p>&nbsp;</p><p>值得注意的是，在OpenAI本月完成新一轮融资之前，国内大模型厂商已经率先进行了一轮融资：月之暗面在8月进行了3亿美元的融资，目前估值达到33亿美元，这是最近6个月里月之暗面的第三次融资，三次融资总额达13亿美元；同样在8月，零一万物完成新一轮数亿美元的融资，最新估值为104亿元人民币；百川智能于7月已完成50亿元人民币的A轮融资，并且以200亿元估值开启B轮融资；智谱AI在今年1月、7月分别完成一轮股权融资。</p><p>&nbsp;</p><p></p><h2>网友们高唱“泡沫”</h2><p></p><p>&nbsp;</p><p>“嗨，投资者们，只要支付‘区区’2.5 亿美元，你就能成为 AGI 的创造者之一。”有网友调侃道。此轮融资也正值人们对AI领域泡沫的担忧日益加剧之时，随着构建和训练AI模型的成本不断上升，打造人工智能模型的公司面临着投资者要求其创造回报的压力。</p><p>&nbsp;</p><p>OpenAI 在最近的时间里发布产品虽然保持了一定的频率，但直到o1之前的模型大多都反响平平。“在 o1 之前，ChatGPT 已经变得无关紧要。我认为他们匆忙推出它是为了再次引起人们的关注。”有网友评价并晒出了下面的走势图。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/18/183abba71be9af90f57121356a71282f.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>“泡沫……但我确信泡沫会破裂！”有网友看到OpenAI最新融资消息后说道。也有网友表示，“对我来说，在看到真正的进展之前，这都是 HypeAI。o1-preview对我来说是真正的进展吗？不是。也许到明年他们会有所进步，我会认为那是真正的进步，但目前我还没看到。这是 OpenAI 和一些用户的炒作。”</p><p>&nbsp;</p><p>“我几乎可以肯定其他公司也在研发与草莓相当的产品。”有网友提出，对此有网友表示，“没关系，现在每个人都向 OpenAI 投入资金的原因有两个：一是他们正在放弃非营利的心态；二是他们拥有显著的领先优势，他们所建造的东西的价值很快就会超过地球上几乎所有的东西。”</p><p>&nbsp;</p><p>也有网友指出，投资 OpenAI 的大公司也在投资 Anthropic 和其他一批人工智能公司。但“OpenAI 已经领先并将继续领先。它需要的只是更多的资金来巩固其领先地位，现在它已经拥有了实现这一目标的资本。”</p><p>&nbsp;</p><p></p><h2>大额融资背后是运营压力</h2><p></p><p>&nbsp;</p><p>超强融资背后是OpenAI不得不面临的运营压力。The information 此前还预估，OpenAI 今年亏损将达 50 亿美元，即将破产。</p><p>&nbsp;</p><p>OpenAI 的成本主要分成推理成本、训练成本和人工成本三大块，总运营成本预计将达到惊人的85亿美元。</p><p>&nbsp;</p><p>具体看，推理成本方面，截至今年3月，OpenAI已斥资近40亿美元用于租用微软的服务器资源，全力支撑ChatGPT及其底层大模型的运行。训练成本（包括支付数据使用费用）方面也呈现显著增长态势，预计今年将攀升至约30亿美元。据悉，该公司去年以超乎预期的速度推进新人工智能模型的培训，导致原本规划的8亿美元预算大幅超出，预计今年的训练成本将会翻倍，因为OpenAI在持续迭代旗舰大模型，并训练新一代模型。</p><p>&nbsp;</p><p>人力成本方面，OpenAI的扩张同样迅猛，7月份被曝有大约1500名员工，且团队规模仍在不断壮大。为吸引并留住顶尖技术人才，OpenAI在薪酬及福利上的投入不容小觑，今年预计人力成本开支将达到15亿美元。</p><p>&nbsp;</p><p>此前，一张美国科技大厂的薪酬表曝出，OpenAI以500万起薪领先于谷歌等一众大厂。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ab/ab6f4e6d7e23ed32ceeff1c29a0d87ca.png" /></p><p></p><p>&nbsp;</p><p>值得一提的是，Anthropic、特斯拉、OpenAI、Google Brain 和亚马逊等公司提供了最高的初始报价（并愿意协商更高的最终交易），但提高报价最多的公司是 Google Research、Microsoft Research、Bloomberg AI、IBM Research 和 TikTok。从中可以得出的结论是，即使你的初始报价很低，你也总有空间利用你作为高技能研究人员的优势来获得更好的报价。</p><p>&nbsp;</p><p>此前还有知情人士透露，OpenAI曾预计其2023年的劳动力成本为5亿美元。而到当年年底，其员工人数增加了一倍，达到800人左右。从那以后，该公司的员工人数再次增加了近一倍。加之其官网公布的近200个空缺职位，预示着2024年下半年将迎来更多员工。实际上，OpenAI目前确实还在招人。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/6b/6b8c8c8e36b5be0de407f2a41592762f.png" /></p><p></p><p>&nbsp;</p><p>总之，上述运营成本并未通过大约 35 亿美元的收入来满足。值得注意的是，该公司已经进行了七轮融资，筹集了超过 110 亿美元。据悉，OpenAI 运行接近满负荷，其 350,000 台服务器中有 290,000 台专用于支持 ChatGPT。</p><p>&nbsp;</p><p>另外，有消息称，OpenAI 可能会在 9 月 24 日进一步推广 ChatGPT 的高级语音模式，甚至可能是直接正式发布。有网友在OpenAI 更新的高级语音模式 FAQ 页面的代码中发现“hasSeenAdvancedVoice/2024-09-24”的字符串，暗示会邀请部分移动用户体验高级语音模式。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/96/9681de72c35387cdec78a093ca60ff38.png" /></p><p></p><p>&nbsp;</p><p>OpenAI能否在激烈的竞争中保持自己的技术优势和创新能力，我们拭目以待。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href="https://www.bloomberg.com/news/articles/2024-09-19/openai-to-decide-which-backers-to-let-into-6-5-billion-funding">https://www.bloomberg.com/news/articles/2024-09-19/openai-to-decide-which-backers-to-let-into-6-5-billion-funding</a>"</p><p><a href="https://www.teamrora.com/post/ai-researchers-salary-negotiation-report-2023">https://www.teamrora.com/post/ai-researchers-salary-negotiation-report-2023</a>"</p><p><a href="https://www.ft.com/content/841f6e58-b1bb-4c8e-bce0-a4c0b46ee2f8">https://www.ft.com/content/841f6e58-b1bb-4c8e-bce0-a4c0b46ee2f8</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/bJM8KS3be5cFMhgJbjqP</id>
            <title>高通洽购英特尔，英特尔中国回应；IBM秘密裁员数千人，年龄在50-55岁；《黑神话：悟空》收入超67亿创纪录 | AI周报</title>
            <link>https://www.infoq.cn/article/bJM8KS3be5cFMhgJbjqP</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/bJM8KS3be5cFMhgJbjqP</guid>
            <pubDate></pubDate>
            <updated>Mon, 23 Sep 2024 01:04:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p></p><h2>行业热点</h2><p></p><p>&nbsp;</p><p></p><h4>曝IBM针对云服务部门秘密裁员，涉及数千名员工，年龄集中在50-55岁之间</h4><p></p><p>&nbsp;</p><p>9月19日消息，据外媒当地时间本周三报道，一位IBM员工透露，IBM云服务部门IBM cloud过去数日经历了一次大规模裁员，影响数千名员工。此外本次裁员还是秘密进行的：受裁员工必须签署NDA，不得对外谈论裁员具体细节。</p><p>&nbsp;</p><p>具体而言，本次秘密裁员主要针对高级的程序、销售和支持人员，受影响的员工大多在50~55岁年龄段，工龄位于20~24年之间，职级集中在L7、L8、L9，裁员前拥有相当高的薪资收入。同时，爆料的员工称，其工作岗位将转往印度，这反映了IBM美国的招聘将冻结。但据说IBM在印度的招聘工作正在进行中。此前IBM 也有将工作岗位转移到印度的历史。</p><p>&nbsp;</p><p>IBM发言人表示，这家科技巨头今年早些时候披露了一笔劳动力再平衡费用，这笔资金将影响IBM整体员工团队中的“非常低个位数百分比”，IBM仍然预计其2024年末的员工总数将同年初大致相当。</p><p>&nbsp;</p><p>IBM 本财年一季度的财报中提到了一笔4亿美元（当前约 28.36 亿元人民币）的裁员赔偿资金，按IBM在2023年以3亿美元的开支裁员3900人估算，本年度IBM将有约5200名员工被裁，相当于整体员工团队的1.8%。</p><p>&nbsp;</p><p></p><h4>大连思科裁员正式落地，思科今年第二轮裁员占7%，补偿方案N+7</h4><p></p><p>&nbsp;</p><p>美国科技巨头公司思科在2024年进行了今年第二轮大规模裁员，涉及约5600名员工，占其员工总数的7%。此前，思科在2024年2月已经裁减了4000名员工。</p><p>&nbsp;</p><p>据最新消息，高层给员工开会，大连思科年内第二波裁员策略已经正式落地。据知情人透露，上午已经收到内部邮件，日韩业务要撤出中国，思科大连蓝牌（正编）员工开始裁员。赔偿方案两种选择：1.N+7直接走人，2.N+5，延迟两个月。</p><p>&nbsp;</p><p>此次裁员不仅涉及常规运营部门，还波及了思科旗下的Talos Security团队。Talos是思科的情报与安全研究部门，负责保护全球客户免受网络攻击。然而，即便这一关键部门也难以避免要裁员。更让人不解的是，8月份思科宣布了新一轮裁员计划，一直拖到9月16日才通知受影响的员工，期间未给出明确解释。据员工透露，在此期间公司内部的工作氛围变得异常奇怪，员工普遍感到焦虑不安和不满。</p><p>&nbsp;</p><p>思科称，裁员是为了“投资关键增长机会，并提升运营效率”。该声明与其全年财报的发布时间一致。尽管进行大规模裁员，思科在2024财年的年收入仍接近540亿美元，是公司历史上表现第二好的财年。</p><p>&nbsp;</p><p></p><h4>英特尔 CEO宣布40年来最重要转型：年底前裁员15000人、将抛掉2/3的房地产</h4><p></p><p>&nbsp;</p><p>9月17日消息，英特尔 CEO Pat Gelsinger 发布全员信，宣布了公司下一段的战略目标，以及多项降本增效计划。重点包括“继续采取紧急行动，建立更具竞争力的成本结构，实现我们上个月宣布的100亿美元的节约目标”等三个方面。基辛格还在信中提到，董事会计划将英特尔代工厂设立为英特尔内部的独立子公司，以最大限度地实现增长和股东价值创造。</p><p>&nbsp;</p><p>另外，基辛格透露，通过自愿提前退休和离职，英特尔已经完成今年年底裁员约15000人目标的一半以上（7500人）。“我们仍需做出艰难的决定，并将于10月中旬通知受影响的员工。此外，我们正在实施计划，在年底前减少或退出全球约三分之二的房地产。”基辛格写道。他强调：“这是英特尔四十多年来最重要的转型。自内存向微处理器转型以来，我们从未尝试过如此重要的事情。当时我们成功了——我们将迎接这一时刻，在未来几十年打造更强大英特尔。”</p><p>&nbsp;</p><p></p><h4>亚马逊CEO官宣：恢复一周5天办公室上班，管理层开始裁员</h4><p></p><p>&nbsp;</p><p>亚马逊CEO Andy Jassy官宣，从现在到明年1月2日，亚马逊要求员工逐步恢复到疫情前一周5天上班的工作状态。据了解，此前亚马逊已经要求员工一周至少有三天回到办公室上班，现在亚马逊打算从3天提高到5天，完全恢复成疫情之前的状态了。此外，Andy Jassy还要求亚马逊的组织扁平化，并且要求IC到manager的比例至少达到15%。然而，扁平化管理势必会导致组织合并，其中一部分中层经理失去位置，这也被认为是亚马逊在进行管理层裁员。</p><p>&nbsp;</p><p></p><h4>《黑神话：悟空》收入超67亿，国产3A游戏新纪录诞生！</h4><p></p><p>&nbsp;</p><p>《黑神话：悟空》自8月20日清晨10点正式解锁以来，犹如齐天大圣孙悟空般腾空而起，迅速在Steam平台上掀起热潮，其影响力与关注度在全球范围内持续攀升，成为游戏界热议的焦点。</p><p>&nbsp;</p><p>时至今日（9月20日），这款国产3A游戏上线满月之际，据国外知名数据分析公司VG Insights最新披露，《黑神话：悟空》在Steam上的销量已惊人地突破2000万大关，总收入更是跃升至9.61亿美元（相当于人民币超67.9亿元），标志着国产3A游戏领域新里程碑的诞生。</p><p>&nbsp;</p><p></p><h4>高通洽购英特尔？英特尔中国：对于传言，不予置评</h4><p></p><p>&nbsp;</p><p>9月21日电，芯片巨头高通被曝正在洽购芯片代工厂商英特尔。9月21日，有外媒报道称，高通已就收购事宜接洽英特尔。该报道援引知情人士消息称，交易还远未确定。高通并未正式向英特尔提出收购要约，且达成交易的障碍仍很大。但如果交易能够顺利完成，这将是近年来规模最大、深刻影响市场的交易之一。</p><p>&nbsp;</p><p>对此，英特尔中国相关人士向媒体表示，对于传言，我们不予置评。另外有消息称，博通目前没有在评估向英特尔发出收购要约，该公司曾评估过是否寻求交易，顾问在继续向博通提出建议。</p><p>&nbsp;</p><p></p><h4>前苹果设计总监 Jony Ive 确认正与 OpenAI 开发一款新设备，iPhone 元老级人物加盟</h4><p></p><p>&nbsp;9 月 22 日消息，今年 4 月曾有消息称，OpenAI 首席执行官山姆・阿尔特曼（Sam Altman）携手前苹果设计总监乔纳森・伊夫（Jony Ive），联合设计面向个人的 AI 硬件，目前正寻求外部投资。</p><p>&nbsp;</p><p>对此，伊夫本人在纽约时报 9 月 21 日的一篇文章中证实了这一点。报道称，伊夫是通过 Airbnb 的首席执行官 Brian Chesky 认识阿尔特曼的，该项目则由伊夫和劳伦娜・鲍威尔・乔布斯（乔布斯遗孀）的公司 Emerson Collective 资助。</p><p>&nbsp;</p><p>报道提到，到今年年底，该新公司可能会筹集 10 亿美元（当前约 70.55 亿元人民币）的资金，但报道没有提到软银首席执行官孙正义，去年曾有传言称孙正义向该项目投资 10 亿美元。</p><p>&nbsp;</p><p>该项目目前只有 10 名员工，但其中包括 Tang Tan 和 Evans Hankey，他们是与伊夫一起开发 iPhone 的两个关键人物。至于产品本身是什么，去年有传言说它的灵感来自触摸屏技术和初代 iPhone，不过这一消息暂未证实。</p><p>&nbsp;</p><p>部分厂商称已无法下单英伟达H20芯片</p><p>&nbsp;</p><p>9月20日消息，部分厂商已无法下单英伟达H20芯片。一位产业链人士表示，“英伟达上月开始不接H20订单，但没有明文通知。”另一AI厂商人士亦表示，“近期确实存在英伟达不接部分厂商H20订单的情况。”包括互联网厂商、大模型厂商、芯片供应商在内多位产业链人士表示，一直有听到H20将停售的消息，但英伟达方面还在争取。不</p><p>&nbsp;</p><p>过亦有多家厂商反馈，“近期仍有H20大批到货，年内到货已超出了全年约40万颗的出货预期。”</p><p>&nbsp;</p><p></p><h4>字节跳动回应与台积电合作AI芯片：报道不实</h4><p></p><p>&nbsp;</p><p>针对9月18日媒体报道的字节跳动计划与台积电就AI芯片开展合作，字节方面回应称，报道不实，字节跳动在芯片领域确实有一些探索，但还处于初期阶段，主要是围绕推荐、广告等业务的成本优化，所有项目也完全符合相关的贸易管制规定。</p><p>&nbsp;</p><p>此前，据外媒报道，两位直接知情人士透露，字节跳动计划与台积电合作，在 2026 年前量产其自主设计的两款半导体，预计字节跳动将预定数十万枚芯片的产量。知情人士表示，生产这些芯片可以减少字节跳动对价格高昂的英伟达芯片的依赖，从而开发和运行 AI 模型。字节跳动正在加快步伐制造自己的 AI 芯片，以期在中国 AI 聊天机器人市场上领先于竞争对手。</p><p>&nbsp;</p><p></p><h4>奥特曼离开OpenAI安全委员会，OpenAI将转型为更传统的盈利性公司</h4><p></p><p>&nbsp;</p><p>9月17日，据外媒消息报道，OpenAI 首席执行官 Sam Altman 将离开 OpenAI 于 5 月成立的内部委员会，该委员会负责监督与公司项目和运营相关的关键安全决策。</p><p>&nbsp;</p><p>OpenAI在博文中表示，安全与保障委员会将成为由卡内基梅隆大学教授Zico Kolter担任主席的独立董事会监督小组，成员包括 Quora 首席执行官 Adam D'Angelo、美国退役陆军将军 Paul Nakasone 和前索尼执行副总裁 Nicole Seligman，他们都是 OpenAI 董事会的现任成员。OpenAI 在帖子中指出，委员会对OpenAI 的最新 AI 模型o1进行了安全审查——尽管 Altman 仍参与其中。该公司表示，该小组将继续定期接受 OpenAI 安全团队的简报，并保留推迟发布的权力，直到安全问题得到解决。</p><p>&nbsp;</p><p>OpenAI 在帖子中写道：作为其工作的一部分，安全委员会将继续定期收到有关当前和未来模型的技术评估报告，以及持续发布后监测报告。他们正在基于模型发布流程和实践，建立一个综合的安全框架，明确定义模型发布的成功标准。</p><p>&nbsp;</p><p>此外，Sam Altman 在最近的公司周会上宣布，OpenAI 将于明年调整其复杂的非营利性企业结构，转型为更传统的营利性公司。尽管如此，OpenAI 仍将保留一个非营利部门，并继续致力于构建造福所有人的 AI。OpenAI 成立于2015年，主要依赖捐赠者的资金，但多年来仅筹集了1.305亿美元，难以满足其核心研究所需的计算能力和人才成本。Altman 在周会上未透露更多细节，但强调非营利性仍是公司使命的核心，并将持续存在。</p><p>&nbsp;</p><p></p><h4>小米三折叠屏手机专利曝光：华为同款Z字形方案，2022年就布局</h4><p></p><p>&nbsp;</p><p>9月18日消息，根据国家知识产权局9月3日公示的清单，小米公司获得了一项“手机及其主体”的外观设计专利，展示了小米三折叠手机设计。查询专利报告页面，显示小米于2022年12月21日提交了该设计专利，展示了2种设计方案，但整体都是与华为一致的Z字形折叠方式，并分享了多张设计草图。根据显示的专利草图，小米三折叠手机背面采用水平放置的摄像头方案，水平放置了3个摄像头，并配有一个LED闪光灯。</p><p>&nbsp;</p><p>不过，这款机型有可能并不存在，只是小米在进行三折的相关技术研发和专利储备。另外值得注意的是，其实早在2019年年初，小米联合创始人林斌就曾公布了小米首款双折叠智能手机，其实就与如今的“三折叠”概念异曲同工。只不过当时更多的是在进行折叠屏技术储备，机器被晒出来多是以炫技为目的，折叠方式比较鸡肋，很难正常使用。当时林斌介绍，小米双折叠手机攻克了柔性折叠屏技术、四驱折叠转轴技术、柔性盖板技术以及MIUI适配等一系列技术难题，这应该是全球第一台双折叠手机。</p><p>&nbsp;</p><p></p><h4>苹果市值蒸发千亿：iPhone 16 首周销量下降 12.7%</h4><p></p><p>&nbsp;</p><p>天风国际分析师郭明錤表示，iPhone16 系列在首周末预购销量预估约 3700 万部，较去年 iPhone15 系列首周末销量同比减少约 12.7%，关键在于 iPhone16Pro 系列低于预期。郭明錤指出，iPhone16Pro 系列出货时间显著低于 15Pro 系列，除预购前备货量增加外，从首周末销量同比减少来看，关键还是在于需求低于预期。</p><p>&nbsp;</p><p>最近一周内苹果市值蒸发 690 亿美元（约合 4900 亿元人民币）。在一些国内商家和用户看来，创新在苹果手机上越来越弱，而今年表现的尤为明显，特别是同天华为三折叠推出。据报道，华为三折叠手机 10 天预约量达 625.9 万，而 9 月 20 日其也将迎来开卖，据说备货有 100 万台。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>微软、贝莱德等宣布成立超 300 亿美元 AI 基础设施投资基金，英伟达提供专业知识支持</h4><p></p><p>&nbsp;</p><p>9 月 18 日消息，微软联合多方成立了一只名为“全球 AI 基础设施投资伙伴关系”的基金，旨在投资 AI 基础设施，以建设数据中心和能源项目。</p><p>&nbsp;</p><p>微软宣布联合贝莱德（BlackRock）、全球基础设施合作伙伴（GIP）及阿联酋 AI 投资公司 MGX 成立全球 AI 基础设施投资伙伴关系（GAIIP），以投资新兴及扩建中的数据中心，满足对不断增长的算力需求，并投资能源基础设施，为这些设施提供新的能源来源。英伟达还将为 GAIIP 提供支持，包括在 AI 数据中心和 AI 工厂方面的专业知识，以惠及 AI 生态系统。</p><p>&nbsp;</p><p>据称，这些基础设施投资主要集中在美国，推动 AI 创新和经济增长，其余部分将投资于美国的合作伙伴国家。微软透露，该合作伙伴关系最初将寻求逐步释放 300 亿美元的私募股权资本，来自投资者、资产所有者和企业，加上债务融资，最终有望撬动总计 1000 亿美元（当前约 7094.48 亿元人民币）的投资潜力。</p><p>&nbsp;</p><p></p><h4>美团王兴：去年获得收入的骑手约 745 万，报酬 800 亿元</h4><p></p><p>&nbsp;</p><p>9月18日消息，中秋假期期间，美团 CEO 王兴对内发布全员信，信中提到，过去 3 年每年招聘超过 5000 名应届毕业生，2025 届计划招募 6000 名，同时，内部提拔比例高达 69%。去年在美团平台获得收入的骑手约 745 万，获报酬超过 800 亿。</p><p>&nbsp;</p><p></p><h2>大模型一周大事</h2><p></p><p>&nbsp;</p><p></p><h3>大模型发布</h3><p></p><p>&nbsp;</p><p></p><h4>阿里云宣布开源 Qwen2.5，上架超 100 个模型</h4><p></p><p>&nbsp;</p><p>9 月 19 日举办的 2024 云栖大会上，阿里云 CTO 周靖人发布通义千问新一代开源模型 Qwen2.5。据悉，Qwen2.5 全系列涵盖多个尺寸的大语言模型、多模态模型、数学模型和代码模型，每个尺寸都有基础版本、指令跟随版本、量化版本，总计上架 100 多个模型，其中旗舰模型 Qwen2.5-72B 性能超越 Llama 405B。</p><p>&nbsp;</p><p>相比 Qwen2，Qwen2.5 全系列模型都在 18T tokens 数据上进行预训练，整体性能提升 18% 以上，拥有更多的知识、更强的编程和数学能力。此外，在多模态模型方面，阿里云还宣布了视觉语言模型 Qwen2-VL-72B 开源，Qwen2-VL 能识别不同分辨率和长宽比的图片，理解 20 分钟以上长视频，具备自主操作手机和机器人的视觉智能体能力。</p><p>&nbsp;</p><p></p><h4>阿里国际发布最新版多模态大模型Ovis</h4><p></p><p>&nbsp;</p><p>9月19日消息，阿里国际AI团队宣布发布多模态大模型Ovis。据介绍，Ovis能够在数学推理问答、物体识别、文本提取和复杂任务决策等方面展现出色表现。例如，Ovis可以准确回答数学问题，识别花的品种，支持多种语言的文本提取，甚至可以识别手写字体和复杂的数学公式。Ovis 1.0、1.5的数据、模型、训练和推理代码都已全部开源，可复现。Ovis1.6系列中的Ovis1.6-Gemma2-9B也已开源权重。</p><p>&nbsp;</p><p></p><h4>巨人网络发布两款“游戏+AI”自研大模型应用</h4><p></p><p>&nbsp;</p><p>9月19日，巨人网络携多项“游戏+AI”新成果首次参展云栖大会，两款自研大模型GiantGPT、BaiLing-TTS应用首发，巨人摹境、AI数字人等AI新技术亮相。据介绍，GiantGPT是专注于游戏业务的垂类大模型，结合高质量自有数据与互联网公共数据训练，并针对角色演绎、情景推理、长期记忆等基础能力进行深度优化。BaiLing-TTS则是行业内首个支持多种普通话方言混说的语音大模型。</p><p>&nbsp;</p><p></p><h4>字节跳动豆包大模型 9 月 24 日发布视频生成模型</h4><p></p><p>&nbsp;</p><p>字节跳动宣布，豆包大模型将于 9 月 24 日发布视频生成模型，并带来更多模型家族的能力升级。9 月 24 日 14:30，2024 火山引擎 AI 创新巡展・深圳站将于深圳举行，字节跳动将在大会上公布火山引擎整体介绍及豆包大模型进展，还有火山引擎 AI 产品最新进展和多个行业企业 AI 落地实践分享。</p><p>&nbsp;</p><p></p><h4>腾讯发布DepthCrafter，提升开放世界视频的画面质量</h4><p></p><p>&nbsp;</p><p>9月18日消息，腾讯与香港科技大学联合开发的DepthCrafter模型，是首个专为生成开放世界视频游戏而设计的扩散变换器模型。通过模拟广泛的游戏引擎特性，如创新角色、动态环境、复杂动作和多样事件，该模型实现了高质量、开放世界的生成。这此外，它还提供了交互式可控性，从而允许游戏玩法模拟。</p><p>&nbsp;</p><p></p><h4>快手可灵 AI 面向全球发布 1.5 模型</h4><p></p><p>&nbsp;</p><p>可灵 AI&nbsp;全球升级发布，新增可灵 1.5 模型和“运动笔刷”功能，提升视频生成质量与控制能力。自 6 月发布以来，已进行 9 次迭代，超过 260 万人使用，生成视频超 2700 万部、图片 5300 万张。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h3>企业应用</h3><p></p><p>&nbsp;</p><p>9 月 19 日，“微信派”公众号宣布，微信朋友圈发布实况照片功能正式推出。用户打开朋友圈-从手机相册选择，即可选中并发布实况照片，照片将包含动态画面和声音。发布时，也可以点击关闭实况效果，支持静态照片与实况照片混合发布。待内容发布后，朋友圈图片将出现实况圈圈效果。目前，功能正在 iOS 8.0.51 及以上版本逐步覆盖中，用户更新完并被覆盖后即可使用。Android 手机目前还不支持，微信派官方对此回应称：“再等等”。9 月 18 日，谷歌宣布为 YouTube 带来一系列 AI 相关功能，有望改变视频制作的方式乃至视频本身。YouTube 正在引入一系列 AI 工具，以帮助创作者更高效地制作视频内容。其中包括一个名为“灵感”的新选项卡，这个 AI 驱动的功能可以为创作者提供视频概念、推荐标题、缩略图，甚至编写视频大纲。此外，YouTube 还推出了 Veo，这是一个集成了谷歌 DeepMind 视频模型的工具，能够生成视频背景和最长 6 秒的完整视频片段。9 月 18 日，谷歌正计划推出一项技术，用于识别照片是用相机拍摄的，还是用Photoshop等软件编辑的，抑或是由人工智能生成模型生成的。谷歌正在使用的系统是内容出处和真实性联盟（C2PA）的一部分，该联盟是试图解决人工智能生成图像问题的最大团体之一。9 月 17 日，特斯拉似乎正在稳步推进其在中国推出的全自动驾驶 (FSD) 服务的计划。最近，已经有部分在国内生产销售的特斯拉汽车更新了一个按钮，以启用 FSD。同时，该按钮注明了是在“驾驶员监督下：的测试版，并且目前还不能启用。同时，根据截图上的信息显示，FSD 需要更新导航地图才能启用，此前有消息称，特斯拉正在和百度就相关地图数据事宜进行沟通。而Elon Musk在特斯拉 2024 年第二季度财报电话会议上表示，他预计 FSD 将在今年年底前在中国获得批准。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/kcp5IFmv7B8gYqBJxluZ</id>
            <title>阿里云发布首个AI多模数据管理平台DMS，助力业务决策提效10倍</title>
            <link>https://www.infoq.cn/article/kcp5IFmv7B8gYqBJxluZ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/kcp5IFmv7B8gYqBJxluZ</guid>
            <pubDate></pubDate>
            <updated>Sat, 21 Sep 2024 06:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月20日，2024云栖大会上，阿里云瑶池数据库宣布重磅升级，发布首个一站式多模数据管理平台DMS：OneMeta+OneOps。该平台由Data+AI驱动，兼容40余种数据源，实现跨云数据库、数据仓库、数据湖的统一数据治理，帮助用户敏捷、高效地提取并分析元数据，业务决策效率可提升10倍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/32/32e0c3fcf153c23a1a30e0f8baf036e7.jpeg" /></p><p></p><p>阿里云副总裁、数据库产品事业部负责人李飞飞</p><p></p><p>“数据是生成式AI的核心资产，大模型时代的数据管理系统需具备多模处理和实时分析能力，以数据驱动决策和创新，为用户提供‘搭积木’一样易用、好用、高可用的使用体验。”阿里云副总裁、数据库产品事业部负责人李飞飞表示。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/ab/ab91507c148beebff7e17bddc742ebc3.png" /></p><p></p><p>图：阿里云推出多模数据管理平台DMS：OneMeta+OneOps</p><p></p><p>当前，近80%的企业在建设数据平台时采用多种数据引擎、多数据实例组合的策略，AI兴起也带来了非结构化数据的指数级增长，给企业对数据的高效检索和分析管理提出了更大挑战。此次，阿里云重磅推出由“Data+AI”驱动的多模数据管理平台DMS：OneMeta+OneOps，助力构建企业智能Data Mesh（数据网格），提升跨环境、跨引擎、跨实例的统一元数据管理能力。</p><p></p><p>DMS创新设计了统一、开放、跨云的元数据服务OneMeta及DMS+X的多模联动模式OneOps。OneMeta首次打通不同数据系统，可支持全域40余种不同数据源，提供数据血缘和数据质量的一站式数据治理。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/ad/ad49f198fa6c7ec433e77aca75ca93a0.png" /></p><p></p><p></p><p>OneOps则基于数据开发平台DataOps和AI数据平台MLOps，将不同数据库引擎（关系型数据库、数据仓库、多模数据库等）集结到统一平台，让用户“开箱即用”，实现全链路的数据加工和计算能力。</p><p></p><p>自上线以来，DMS已服务超过10万企业客户。借助跨引擎、跨实例管理和开发以及数据智能一体化，DMS将帮助企业从分散式数据治理升级至开放统一数据智能管理，可降低高达90%的数据管理成本，业务决策效率提升10倍。</p><p></p><p>李飞飞表示：“这是自云原生数据库2.0后，阿里云瑶池数据库又一次里程碑式的改造升级。DMS：OneMeta+OneOps为企业提供了全域数据资产管理能力，让业务数据‘看得清、查得快、用得好’。”</p><p></p><p>据介绍，极氪汽车采用DMS+Lindorm一站式多模数据解决方案，实现32万在线车辆上万车机信号数据的弹性处理分析，开发效能提升2倍，降低50%云资源成本。在大模型领域，此方案支撑月之暗面构建AI智能助手Kimi，帮助Kimi准确理解用户的搜索意图、整合与概述多种信息源，实现精准和全面的信息召回，提升用户交互体验。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/ea542c860a7b87c2dfe54757dd6ae8b8.png" /></p><p></p><p></p><p>此外，云原生数据库PolarDB今年首次提出基于“三层解耦, 三层池化”（存储、内存、计算）、AlwaysOn架构的多主多写和秒级Serverless能力，解决了多主架构中冲突处理和数据融合、以及Serverless秒级弹性租户隔离的难题。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/e9/e95f675bc1decd38d160c65fbe921c6e.png" /></p><p></p><p>本次云栖大会，阿里云瑶池还正式发布了云原生内存数据库Tair Serverless KV服务，是阿里云首个基于NVIDIA TensorRT-LLM的推理缓存加速云数据库产品。Tair采用NVIDIA TensorRT-LLM一起进行了深度优化。相比开源方案，该服务可实现PD分离/调度优化吞吐30%的提升 ，预计成本可降低 20%*注。</p><p></p><p>*注：基于Qwen2 7B模型在长上下文场景构造实验环境数据测试，最终效果以实际产品和场景测试数据为准。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/HOKcm1CVYKUq1aOIwFqb</id>
            <title>JetBrains 与阿里云合作推出 AI Assistant，聚焦中国市场开发者</title>
            <link>https://www.infoq.cn/article/HOKcm1CVYKUq1aOIwFqb</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/HOKcm1CVYKUq1aOIwFqb</guid>
            <pubDate></pubDate>
            <updated>Sat, 21 Sep 2024 04:54:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9 月 19 日，在 2024 云栖大会上，全球软件开发工具提供商 JetBrains 发布基于阿里云通义大模型的 JetBrains AI Assistant，在完善其开发工具产品生态方面迈出了重要一步。</p><p></p><p>JetBrains《2023 开发者生态系统现状报告》调查结果显示，79% 的开发者认为处理代码是其工作中最耗时的环节。而 60% 的开发者已经开始使用、熟悉 AI 代码生成工具，用 AI 协助完成理解代码、检测并修正代码等繁琐的工作，让开发者专注于核心的编码任务，显著提升开发效率。</p><p></p><p>据介绍，JetBrains AI Assistant 与多款 JetBrains 产品深度集成，能够以高度的适配性完成代码生成与重构、回答和解释代码相关问题、撰写文档和提交信息等工作，助力中国本土开发者提升效率和代码质量。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b9/b988354041f18d4f4fabd8048961425c.png" /></p><p></p><p>并且，JetBrains AI Assistant 充分考虑了开发者的使用习惯，将 AI 辅助融入开发者的工作流中，同时实现了与 IDE 的无缝集成，将 AI 功能深刻融入对代码和上下文的理解，全面增强开发环境。在实际操作中，JetBrains AI Assistant 能够提供详尽的上下文信息和代码解释，帮助用户清晰理解 AI 的决策逻辑，从而显著提升操作的透明度和可追溯性。</p><p></p><p>在与阿里云的联合下，为中国本土用户量身打造的 AI Assistant 深度融合了中文自然语言处理技术，实现了中文指令与系统的直接交互，显著降低了使用门槛并提高了使用者的工作效率。此外，运用本土数据进行模型训练，能够在优化模型性能的同时，大幅降低训练成本。基于本土语言大模型，确保了数据处理的合规性和安全性，提供更加安全信赖、持续可靠的服务。</p><p></p><p>此外，JetBrains AI Assistant 在设计之初就将用户数据隐私与安全置于核心位置，不断深化安全性研究，严格遵守数据保护政策，未经授权不收集或泄露任何敏感信息，确保用户信息的安全。</p><p></p><p>大会上，JetBrains 除了带来全新首发的 JetBrains AI Assistant，还在展台上准备了丰富的互动和内容。展台特设“码脑讲堂”，邀请技术专家、KOL 通过直观演示和技术分享，解读 AI 在软件开发领域的实际应用与未来蓝图；解决方案展示区呈现了游戏开发、跨平台开发和服务器端开发的最新解决方案；趣味互动墙区域则通过创新装置提供沉浸式互动体验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/ea2fd5b299d0919a7b716486644380a4.png" /></p><p></p><p>JetBrains 中国区总裁李玥萱表示，“中国市场在 JetBrains 的全球版图中占据举足轻重的地位。JetBrains 积极拥抱市场变化，确保我们的产品和服务始终与开发者的需求同频共振，助力他们突破效率极限。未来，我们期待与阿里云继续深化战略合作，不断探索和创新，为中国市场的用户提供更加强大、更加智能的工具和解决方案。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/dd/dd91385fcb5521cfb12ca761250facf3.png" /></p><p>图：JetBrains 中国区总裁李玥萱</p><p></p><p>阿里云智能集团资深副总裁、公共云事业部总裁刘伟光表示：“今天，我们见证了云和 AI 的碰撞带来的一波全新浪潮。我们非常坚信，AI 的应用会席卷和渗透到各行各业。阿里云和 JetBrains 的战略合作发挥了衔接国际技术与中国市场的独特桥梁价值，让更多前沿的创新功能落地本土，我们愿与 JetBrains 共同开启 AI 在开发领域落地的新篇章。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/10/101abc01ca887189e95cbc99445abc2e.png" /></p><p>图：阿里云智能集团资深副总裁、公共云事业部总裁刘伟光</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/lrhWq2us1bD3Vvwdeea5</id>
            <title>阿里云首次推出云原生NDR产品 提升全流量威胁防御能力</title>
            <link>https://www.infoq.cn/article/lrhWq2us1bD3Vvwdeea5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/lrhWq2us1bD3Vvwdeea5</guid>
            <pubDate></pubDate>
            <updated>Sat, 21 Sep 2024 04:22:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月20日，在杭州云栖大会上，阿里云宣布云原生安全能力全线升级，首次发布云原生网络检测与响应产品NDR（Network Detection Response，简称NDR）。同时，阿里云还宣布将持续增加免费的安全防护能力，帮助中小企业客户以极低投入完成基础的云上安全风险治理。</p><p></p><p>云时代复杂的IT体系、碎片化的安全工具和传统的防护思路，以及新技术和新威胁带来的多重变化，让安全运营难以应对挑战。阿里云安全产品负责人欧阳欣表示，阿里云基于多年经验，创新性提出“三体”安全建设思路，将基础设施安全一体化、安全技术域一体化、以及办公安全和生产安全一体化贯彻到安全运营中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/47/479adc185863338978a64cc3ca88438a.jpeg" /></p><p></p><p>此次推出的阿里云云原生NDR，即是在此背景下的创新。NDR是基于公共云环境原生化部署的威胁检测与响应产品，全面提升了云环境全流量防御能力。与传统第三方产品不同在于，它无需部署即可即时开通，并通过创新的自动留存技术，可以针对攻击事件及攻击发生前后5分钟的流量进行取证保存，兼顾留存需要与成本投入，进而进行溯源和关联分析，帮助客户更快发现高级网络威胁。</p><p></p><p><img src="https://static001.geekbang.org/infoq/36/36b6336f76c2c28a67e8fb32a142b407.jpeg" /></p><p></p><p>基于基础设施安全一体化，阿里云还加强了WAAP、云安全中心、DDoS防护等能力，并且对数据库、网络CDN、计算、存储等云原生产品的安全能力也进行全新升级。</p><p></p><p>比如数据库与安全产品在数据安全上进行全面融合与能力共建，发布列加密与原生审计技术，可一键开通，增强自动化的安全能力。在CDN安全方面，阿里云将安全功能融入边缘网络，实现一键开启DDoS防护、WAF、Bot管理、API安全、SSL证书等功能，通过全球3200+节点提供原生安全能力，为用户提供边缘云网安全防护体验。</p><p></p><p>目前，阿里云已经成为Forrester、Gartner、IDC三大国际权威机构认可的全球安全能力最完整的厂商之一。</p><p></p><p><img src="https://static001.geekbang.org/infoq/28/28a3eb882c0ea7b69192ba6d9c1d2cf0.jpeg" /></p><p></p><p>欧阳欣表示，“在做好平台安全建设同时，阿里云也免费开放更多的安全能力额度，包括云安全中心、内容安全、数据安全中心，让中小企业客户能够增强安全防护，同时还在安全体验上增加一键检测、一键修复等功能，帮助客户共同加入到云上安全维护中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/eb/ebdb1fcbdda73c7a113745bf83a2b8e3.jpeg" /></p><p></p><p>面向AI，阿里云全新升级了安全体系，通义大模型基于阿里云的安全基座建设了生成式人工智能安全保障的最佳实践，将内容安全能力覆盖到大模型全生命周期中。同时，阿里云安全为百炼平台的专属部署模式设计了VPC安全保障方案，让客户在私域环境中也能获得数据确权归属等系列安全服务。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>