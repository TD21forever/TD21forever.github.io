<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/O324HBbsMqKFC0ohcZWq</id>
            <title>FCon 演讲视频：数字人民币（e-CNY）赋能支付业态发展</title>
            <link>https://www.infoq.cn/article/O324HBbsMqKFC0ohcZWq</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/O324HBbsMqKFC0ohcZWq</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 03:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 全球数字货币, 中国数字人民币, 金融技术变革, 数字货币的安全性
<br>
<br>
总结: 随着全球数字货币的兴起，特别是中国数字人民币（e-CNY）的发展，我们正见证一个重大的金融技术变革。数字人民币的推出不仅仅是一种新型支付方式的出现，更是对现有金融生态系统的重塑。了解这些关键点，将有助于我们更好地理解数字货币的未来发展趋势及其可能带来的影响。 </div>
                        <hr>
                    
                    <p>随着全球数字货币的兴起，特别是中国数字人民币（e-CNY）的发展，我们正见证一个重大的金融技术变革。数字人民币的推出不仅仅是一种新型支付方式的出现，更是对现有金融生态系统的重塑。在这个变化中，既有机遇也有挑战，特别是在数字货币的安全性、普及性和监管方面。了解这些关键点，将有助于我们更好地理解数字货币的未来发展趋势及其可能带来的影响。在<a href="https://fcon.infoq.cn/2023/shanghai/presentation/5517">FCon全球金融科技大会</a>"上，我们邀请了苏州银行网络金融部高级产品经理<a href="https://fcon.infoq.cn/2023/shanghai/presentation/5574">金一松</a>"，他以主题为《数字人民币（e-CNY）赋能支付业态发展》展开了分享，以下为重点内容概述：</p><p></p><p>数字人民币的定义与特性：详细讨论了数字人民币的定义、设计特点，包括与传统货币的区别、发行和流通方式，以及它在支付体系中的角色。数字人民币的母子钱包体系和软硬钱包形态：探讨了数字人民币的钱包体系，包括母子钱包体系的结构和软硬钱包的不同形态。无网无电支付能力与智能合约应用：强调了数字人民币在无网络和无电源环境下的支付能力，以及智能合约在数字货币中的应用。数字人民币的未来应用前景：展望了数字人民币未来的发展方向，包括在不同场景中的应用潜力和可能的创新应用。</p><p></p><p>通过深入了解这些重点内容，我们可以更全面地认识数字人民币的影响力及其在未来金融生态中的潜在角色。详细内容，请观看完整视频：</p><p></p><p></p><p></p><p>活动推荐：</p><p>QCon 全球软件开发大会（上海站）即将在 12 月 28-29 日开始，届时将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。现在购票，享 9 折优惠，立减￥680！咨询购票请联系：18514549229（微信同手机号）。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/d4rIQ0vpsW9b06qToiji</id>
            <title>夸克大模型通过备案 将升级通识、健康、创作等搜索产品与智能工具</title>
            <link>https://www.infoq.cn/article/d4rIQ0vpsW9b06qToiji</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/d4rIQ0vpsW9b06qToiji</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 10:16:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 阿里智能信息事业群, 夸克大模型, AIGC 创新应用, AI助手
<br>
<br>
总结: 阿里智能信息事业群自研的夸克大模型已通过备案，将在通识、健康、创作等领域升级内容产品与智能工具，并落地一系列 AIGC 创新应用，借助大模型能力全面升级夸克，提升用户在学习、工作、生活上的效率。夸克App将在自研大模型的助力下，加速迈向年轻人的AI助手。 </div>
                        <hr>
                    
                    <p>日前，记者获悉阿里智能信息事业群自研的夸克大模型已通过备案，将陆续在通识、健康、创作等领域升级内容产品与智能工具，并落地一系列 AIGC 创新应用，借助大模型能力全面升级夸克，提升用户在学习、工作、生活上的效率。</p><p></p><p>今年下半年，国内多款大模型已经完成备案且能力水平部分超过 GPT-3.5，广大用户也都期待爆款产品的出现以更好地解决方方面面的实际问题。作为深受年轻人喜欢的信息服务产品，夸克App将在自研大模型的助力下，加速迈向年轻人的AI助手。</p><p></p><p>今年11月中旬，阿里巴巴智能信息事业群发布全栈自研、千亿级参数的夸克大模型，将应用于通用搜索、医疗健康、教育学习、职场办公等众多场景。夸克大模型也凭借四大优势，接连登顶 C-Eval 和 CMMLU 两大权威榜单。同时在法律、医疗、问答等领域的性能评测中夺冠，成为了名副其实的“学霸”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a2026ddaf74dacff9fec92c89fb31921.png" /></p><p></p><p>清华大学新闻学院教授、博士生导师沈阳认为，依托搜索平台，夸克大模型拥有高质量的各类数据，在中文语境下，模型能力处在行业领先水平。在教育、医疗等垂直领域中，夸克在对话、解题上的能力取得了新的突破，是国产自研大模型的优秀代表之一。</p><p></p><p>夸克相关负责人表示，夸克大模型是面向搜索、生产力工具和资产管理助手的应用型大模型。在搜索应用中，将通过图文多模理解、专业知识生成、交互方式创新进一步拓宽应用场景，提升用户体验。同时，在健康等垂直领域中，夸克将依托大模型能力，提供更加实用的信息服务。</p><p></p><p>目前，夸克 App 已经为数千万 95 后职场人和大学生提供了跨场景的智能效率工具。根据 QuestMobile发布的《2023年轻人群智能效率应用研究》报告显示，夸克 App 在泛学生人群和新生代职场人群的用户占比最高，年轻用户使用时长位列行业第一。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/qcE019AscmDYTUJUfIHX</id>
            <title>国内首份“图风控”报告发布：图风控成应对新型网络安全风险的关键性技术</title>
            <link>https://www.infoq.cn/article/qcE019AscmDYTUJUfIHX</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/qcE019AscmDYTUJUfIHX</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 09:51:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 图风控技术, 数据关联性特征, 风险识别, 图智能技术
<br>
<br>
总结: 《图风控行业技术报告》指出，图风控技术是应对AI时代复杂风险的关键技术，利用数据关联性特征实现了大规模时序关系图的构建和实时风险识别。图智能技术以直观、高效、智能的方式分析实体之间的复杂交互关系，提升风险识别的准确性和及时性。图风控技术已在支付、信贷、电商等领域得到广泛应用，成为金融机构和科技公司关注的新发展趋势。 </div>
                        <hr>
                    
                    <p>12 月 8 日，国内首份《图风控行业技术报告》（以下简称“报告”）在北京发布，指出智能风控迈入“全图时代”，图智能应用于<a href="https://www.infoq.cn/article/TDdJaEAY6dBu474REtL0?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">风控</a>"领域形成的“图风控技术”成为应对 AI 时代复杂风险形势的下一代风控基础设施和关键性技术。报告认为，图风控充分利用了海量数据时代的数据关联性特征，实现了大规模时序关系图的高效构建及全周期实时风险识别，在解决黑产复杂隐蔽、信息孤岛等挑战，挖掘更多隐藏风险等方面提供了强大的技术功能和应用价值。</p><p>&nbsp;</p><p>据了解，该报告由<a href="https://www.infoq.cn/article/bjCH8kMloxFUfp00WQIX?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">蚂蚁集团</a>"、清华大学、北京邮电大学、中山大学、上海交通大学、复旦大学、之江实验室和<a href="https://www.infoq.cn/article/EE2bAVOOWa0K_g5lLh7j?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">阿里巴巴</a>"淘天集团联合编写，中国人民大学国际货币研究所（IMI）、金融科技50人论坛（CFT50）提供学术支持，详细呈现了新型数字风险态势、图风控算法技术、图数据库等底层基础设施，并提供了丰富的行业应用案例。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/ed/ed12501a3e99fa960e9b079af7002bb6.png" /></p><p>图：《图风控行业技术报告》发布现场</p><p>&nbsp;</p><p>数字化智能化的颠覆性变革正在带来全新的安全挑战。尤其是 AI 大规模渗透应用引发新一轮智能化浪潮，带来新型数字经济网络中数据复杂度和关联性呈几何倍增，也带来了更加复杂、隐蔽、强对抗和更具破坏力的安全威胁。传统的风控方式已难以抵御多样化的风险形势，越来越多的场景需要更智能化的技术利器。图风控技术的出现，提供了一种解决问题的利器。</p><p>&nbsp;</p><p>课题组专家、北京邮电大学教授、博士研究生导师石川在报告中指出，智能风控技术历经专家策略、机器学习和深度学习的演进，如今图智能技术正逐渐成熟。在金融、电商、安全、社交等领域，风险涉及多个实体之间的复杂交互关系。图智能技术以更直观、高效、智能的方式表达和分析这些交互关系，助力系统发现潜在风险中的隐藏模式和异常，进而提升对潜在风险的准确性和及时性识别。</p><p>&nbsp;</p><p>具体来说，“图”是一种以点和边来表示实体和关系的数据结构；“图智能技术”指包括图数据库、图计算引擎、图神经网络、图可解释等一系列和图有关的人工智能技术通称，是最适应大数据海量、动态等特征的技术之一；应用于风险控制领域而形成的“图风控技术”，可以聚合风险事件、交易属性、关系图谱、专家特征等各类动态变化的风险数据，结合图结构数据的可解释性，实现对风险全链路、基于关系视角的刻画，为风控从业者提供更加全面、可见、实时的风险监测并及时决策。因此，运用图技术提升风控系统能力，正成为行业的新发展趋势。</p><p>&nbsp;</p><p>图风控技术目前在业界已有成熟应用，涵盖支付风控、信贷风控、电商风控，以及供应链、网络安全和基础设施安全等多个领域，是金融机构、安全服务商、新兴初创企业，以及大型科技公司逐浪的“风控风口”。</p><p>&nbsp;</p><p>蚂蚁集团副总裁、大安全事业群总裁赵闻飙在报告中表示，数字经济时代，安全的重要性日益凸显。图风控技术作为蚂蚁集团重点研发投入的创新技术之一，现已成为强化风险管理的利器，对构筑坚固的安全防线作出了重要贡献。</p><p>&nbsp;</p><p>报告显示，蚂蚁集团从 2015 年开始探索图技术，推出了底层自研的大规模图风控基础设施 TuGraph。基于 TuGraph 布局的全图风控体系，打造了万亿级点边规模的全域风险大图，目前已全面应用在业务场景中，不仅实现了支付过程的毫秒级极速风控，支撑了高频交易的高精准度识别，还显著降低了资损率，提高了反欺诈和反洗钱等安全业务的效率。</p><p>&nbsp;</p><p>据了解，全图风控是蚂蚁集团智能风控体系“IMAGE”的重要组成部分。该体系还包括交互式主动风控、端边云协同风控、多方安全风控、智能对抗，支撑了支付宝资损率连续三年低于亿分之一，为解决风控的智能化、主动性、可预测性、隐私保护等世界级难题提供了新突破，获得 CCF 科学技术奖、吴文俊人工智能科学技术奖、浙江省科学技术奖等多个权威奖项。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/OkAUyMNBwo5FHSVxsMST</id>
            <title>FCon 最新演讲视频：大模型在金融领域的落地探索</title>
            <link>https://www.infoq.cn/article/OkAUyMNBwo5FHSVxsMST</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/OkAUyMNBwo5FHSVxsMST</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 08:42:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 技术发展, 大数据, 大模型, 金融行业
<br>
<br>
总结: 在金融行业，随着技术的快速发展，大数据和大模型正在逐渐成为推动行业创新的重要力量。这种变革不仅在风险管理和预测方面展现出巨大潜力，而且在促进金融机构与科技公司之间的合作、推动数字化转型，以及优化数据管理和治理方面也显示出其独特价值。 </div>
                        <hr>
                    
                    <p>在金融行业，随着技术的快速发展，大数据和大模型正在逐渐成为推动行业创新的重要力量。这种变革不仅在风险管理和预测方面展现出巨大潜力，而且在促进金融机构与科技公司之间的合作、推动数字化转型，以及优化数据管理和治理方面也显示出其独特价值。然而，在这一进程中，行业也面临着如可解释性、社会智能等一系列挑战。在<a href="https://fcon.infoq.cn/2023/shanghai/presentation/5517">FCon全球金融科技大会</a>"上，我们邀请了光大信托信息技术部副总经理、数据中心总经理祝世虎 博士，为你分享了大模型在金融领域的应用及其带来的机遇与挑战。以下为分享的重要内容：</p><p></p><p>大数据、大模型与风控的关系：探讨了大数据和大模型如何影响金融领域的风险控制，特别是如何通过数据分析和模型预测来管理和减少风险。大合作与创新：讨论了金融机构与科技公司之间的合作以及这种合作如何促进创新，特别是在开发和应用大型模型方面。关注的问题：提出了金融行业在采用大型模型时面临的一些挑战和问题，例如可解释性、社会智能等。数字化转型对大模型的助力：分析了数字化转型如何助力大模型在金融行业的发展和应用。数据信托与大模型：讨论了数据信托如何帮助管理和优化大模型，特别是在处理和保护数据方面。大模型的治理：探索了在金融行业中应用大模型时需要考虑的治理问题，包括伦理和法律方面的考量。</p><p></p><p>详细内容，请观看完整视频：</p><p></p><p></p><p>活动推荐：</p><p>QCon 全球软件开发大会（上海站）即将在 12 月 28-29 日开始，届时将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。现在购票，享 9 折优惠，立减￥680！咨询购票请联系：18514549229（微信同手机号）。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/N6cVIbuu90KyyRVP2Q7G</id>
            <title>如何看待 OpenAI Q* 谣言</title>
            <link>https://www.infoq.cn/article/N6cVIbuu90KyyRVP2Q7G</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/N6cVIbuu90KyyRVP2Q7G</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 07:55:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, Q*, AI模型, 数学问题
<br>
<br>
总结: OpenAI取得了一项重大技术突破，开发出了名为Q*的AI模型，具备解决全新数学问题的能力。这一突破可能标志着迈向具有一般推理能力的通用人工智能的重要一步。通过分步推理技术，大语言模型可以更好地解决复杂的数学问题。OpenAI的研究还包括训练验证器模型来评估每个步骤的正确性，以提高推理结果的准确性。 </div>
                        <hr>
                    
                    <p>作者 ｜ Timothy B Lee</p><p>译者 ｜ 核子可乐</p><p>策划 ｜ Tina</p><p>&nbsp;</p><p></p><blockquote>OpenAI仍未明确解释Q*究竟是什么，但透露的线索倒是相当不少。</blockquote><p></p><p>&nbsp;</p><p>11月22日，就在OpenAI决定解雇（后又重新聘用）CEO Sam Altman的几天之后，技术媒体The Information报道称OpenAI取得了一项重大技术突破，使其能够“开发出更强大的AI模型”。新模型被命名为Q*（音为「Q star」），“具备解决全新数学问题的能力。”</p><p>&nbsp;</p><p>路透社也发表了类似的报道，但细节同样含糊不清。</p><p>&nbsp;</p><p>两篇报道都将这项突破与董事会解雇Altman的决策联系起来。路透社在报道中指出，几名OpenAI员工向董事会发函，“警告称这项强大的AI发现可能对人类构成威胁。”然而，“路透社未能拿到这封信的副本”，随后的报道也没有继续将Altman下台与Q*一事联系起来。</p><p>&nbsp;</p><p>The Information指出，今年早些时候，OpenAI开发出“能够解决基本数学问题的系统，攻克了这一对现有AI模型来说颇为艰巨的任务。”路透社则表示Q*“具备小学生水平的数学计算能力。”</p><p>&nbsp;</p><p>为了避免妄下结论，我们又花了几天时间搜集相关内容。OpenAI确实没有公布Q*项目的详细信息，但发表了两篇关于其解决小学数学问题的论文。在OpenAI之外，不少研究人员（包括Google DeepMind的研究人员）也一直在这方面开展探索。</p><p>&nbsp;</p><p>我个人怀疑Q*正是指向通用人工智能（AGI）的关键技术突破。虽然不一定会对人类构成威胁，但这可能标志着迈向具有一般推理能力的AI的重要一步。</p><p>&nbsp;</p><p>在本文中，我们将一同了解AI研究领域的这一重大事件，并解释专为数学问题设计的分步推理技术如何发挥关键作用。</p><p>&nbsp;</p><p></p><h1>分步推理的力量</h1><p></p><p>我们首先考虑以下数学问题：</p><p></p><blockquote>John给了Susan五个苹果，之后又给了她六个。之后Susan吃掉其中三个，又给了Charlie三个苹果。她把剩下的苹果给了Bob，Bob吃掉一个。接下来，Bob把手中半数苹果给了Charlie。John给了Charlie七个苹果，Charlie将手中三分之二的苹果给了Susan，最后Susan又把其中四个还给了Charlie。问，现在Charlie还剩几个苹果？</blockquote><p></p><p>&nbsp;</p><p>大家可以先试着自己算算。</p><p>&nbsp;</p><p>其实我们都在小学阶段学过简单的加减乘除，所以看到问题里说“John给了Susan五个苹果，之后又给了她六个”，就知道这时候Susan有11个苹果。</p><p>&nbsp;</p><p>但对于更复杂的问题，那人类在尝试解决时就需要借助笔算或者心算了。比如在此问题中，先有5+6=11，之后是11-3=8，接着8-3=5，以此类推。通过一步步思考，我们最终会得到正确答案：8。</p><p>&nbsp;</p><p>同样的技巧也适用于大语言模型。在2022年1月发表的著名论文中，谷歌研究人员指出，如果大语言模型能按照提示词分步进行推理，就会产生更好的结果。以下是论文中的一份关键图表：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/e8/e8aa3c3a12e1e619a196d0456cebedb1.png" /></p><p></p><p>&nbsp;</p><p>这篇论文的发表时间还早于“零样本”提示技术，因此研究人员通过给出示例答案的方式来提示模型。在左图中，系统会提示模型直接给出最终答案，但结果是划的。而在右侧，系统会一步步提示模型并最终推理出正确答案。谷歌研究人员将这项技术称为“思维链提示法”，且至今仍被广泛应用。</p><p>&nbsp;</p><p>对于大语言模型来说，“五”和“六”这样的数字只是token，跟“这”、“那”或者“猫”没什么区别。这些模型之所以能把大写数字转换成5+6=11，是因为这个token序列曾经在训练数据中出现过。但大模型的训练数据中可能并不包含长计算示例，比如((5+6-3-3-1)/2+3+7)/3+4=8，所以如果要求模型直接给出计算结果，那它就很可能搞不清状况并生成错误答案。</p><p>&nbsp;</p><p>或者用另一种思路来解释，大语言模型没有可用于记忆中间结果（例如5+6=11）的外部“临时空间”。而思维链推理使得大模型能够有效使用自己的输出作为暂时记忆空间，从而将复杂问题拆分成更多步骤——每个步骤都可能与模型训练数据中的示例相匹配。</p><p>&nbsp;</p><p></p><h1>解决更复杂的数学难题</h1><p></p><p>&nbsp;</p><p>在谷歌发表关于思维链提示法论文的几个月前，OpenAI曾经推出一套包含8500道小学数学应用题的GSM8K数据集，以及一篇描述问题解法新技术的论文。OpenAI没有让模型逐一给出答案，而是要求其一次性给出100个思路答案，再通过名为验证器的另一套模型对各个答案进行评分。在这100条回复中，系统将只返回评分最高的答案。</p><p>&nbsp;</p><p>乍看起来，训练验证器模型也需要大费周章，难度不啻于训练大语言模型来生成正确答案。但从OpenAI的测试结果来看，情况并非如此。OpenAI发现只需小型生成器与小型验证器的组合，就能提供与单独使用超大生成器模型（参数是前者的30倍）相当的结果。</p><p>&nbsp;</p><p>2023年5月的一篇论文介绍了OpenAI在该领域的最新研究情况。OpenAI已经跨越小学数学，开始研究更具挑战性的MATH数据集。OpenAI现在不再让验证器对完整答案打分，而是训练验证器具体评估各个步骤，具体参见论文给出的下图：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/30/308ab3a9a944e10fa406a17b643ca6ed.png" /></p><p></p><p>&nbsp;</p><p>每一步都有一个绿色笑脸符号，代表该步骤处于正确的思路之上，直到最后一步模型得出“x=7”，这时打出的是红色的皱眉符号。</p><p>&nbsp;</p><p>文章得出的结论是，在推理过程中的各个步骤上都使用验证器，其结果比直接验证最终答案更好。</p><p>&nbsp;</p><p>这种逐步验证方法的最大缺点，就是更难实现自动化。MATH训练数据集中包含每个问题的正确答案，因此很容易自动检查模型是否得出了正确的结论。但OpenAI未能找到更好的方法来自动验证中间步骤。于是，该公司只能聘请了一些审查员，为7.5万个解题思路的共80万个计算步骤提供反馈。</p><p>&nbsp;</p><p></p><h1>求解路漫漫</h1><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/0b/0be88a7f03a5274850c9978a79424c1e.png" /></p><p></p><p>&nbsp;</p><p>需要注意的是，GSMK8K和MATH数据集中的问题至少还可以通过分步方式简单解决。但在实际应用中，相当一部分数学问题根本无法拆解，例如：</p><p>&nbsp;</p><p>你正在筹划一场分五张餐桌、每桌三位客人的婚宴。</p><p></p><blockquote>Alice不想跟Bethany、Ellen或者Kimmie一起坐。Bethany不想跟Margaret一起坐。Chuck不想跟Nancy一起坐。Fiona不想跟Henry或者Chuck一起坐。Jason不想跟Bethany或Donald一起坐。Grant不想跟Ingrid、Nancy或Olivia一起坐。Henry不想跟Olivia、Louise或Margaret一起坐。Louise不想跟Margaret或Olivia一起坐。要如何安排客人座位，才能充分满足他们的要求？</blockquote><p></p><p>&nbsp;</p><p>在把这样的提示词输入GPT_4时，它开始分步进行问题推理：</p><p>餐桌1：Alice、Chcuk和Donald。餐桌2：Bethany、Fiona和Ellen。餐桌3：Jason、Grant和Ingrid。</p><p>&nbsp;</p><p>但到第四张餐桌时，它就卡住了。这时候Henry、Margaret和Louise还没有入座，他们彼此都不想坐在一起，但接下来只剩两张桌子可以安排。</p><p>&nbsp;</p><p>在这个问题中，我们不知道GPT-4具体错在哪个具体步骤上。它在前三张桌子的安排上完全满足规则，但这些前期选择也导致余下的客人没办法正确入座。</p><p>&nbsp;</p><p>这就是计算机科学家们所说的NP难题，即不存在通用算法以线性方式加以解决。唯一的办法就是尝试一种可能的安排，看看是否符合要求，如果不行则推倒重来。</p><p>&nbsp;</p><p>GPT-4可以通过在上下文窗口中添加更多文本来完成回溯，但其扩展能力仍然有限。更好的方法是为GPT-4提供一个“退格键”，这样它就能删除最后一个或几个推理步骤，然后重试。为此，系统还需要一种方法来跟踪它已经尝试过的组合，避免重复尝试。如此一来，大语言模型就能探索下图所示的可能性树：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/9e/9ed8f78ee2b5f43a4ff715f21bb0f4e1.png" /></p><p></p><p>&nbsp;</p><p>今年5月，普林斯顿大学和Google DeepMind的研究人员共同发表论文，提出一种名为“思路树”的方法。思路树不再用单一推理链来解决问题，而是允许大模型系统探索一系列指向不同方向的推理链“分支”。</p><p>&nbsp;</p><p>研究人员发现，该算法在解决某些传统大语言模型难以解决的问题上表现良好。其中不仅包括所谓“24点游戏”（即通过添加运算符号将随机给出的几个数字计算为24），还实现了创意写作能力。</p><p>&nbsp;</p><p></p><h1>AlphaGo模型</h1><p></p><p>以上，就是OpenAI和DeepMind迄今为止发表过的所有研究成果，可以看到他们都在让大语言模型更好地解决数学问题方面付出了不懈努力。现在，我们一起来推测这项研究最终可能会走向何方。当然，这些猜测没有任何依据，大家也可以根据自己掌握的情况做出展望。</p><p>&nbsp;</p><p>今年10月，播客Dwarkesh Patel曾就通用人工智能开发计划采访过DeepMind联合创始人兼首席科学家Shane Legg。Legg认为，迈向AGI的关键一步就是把大语言模型跟搜索可能响应的树结构结合起来：</p><p></p><blockquote>这些基础模型属于某种世界模型，通过搜索方式实现问题的创造性解决能力。以AlphaGo为例，它那惊人的棋路到底是从何而来？是学习了人类棋手的经验，还是参考了原有数据？不，根本没有。它其实是选择了一个非常罕见、但也极为合理的棋步，再通过搜索过程思考这步棋会造成怎样的后续影响。也就是说，要想获得真正的创造力，必须探索可能性空间并找出隐藏其中的最佳答案。</blockquote><p></p><p>&nbsp;</p><p>Legg在这里提到了著名的“第37手”，即2016年DeepMind AlphaGo软件与顶尖棋手李世石第二场比赛中的一步。大多数人类选手最初都觉得AlphaGo在这步棋上出现了失误，但其最终刻了比赛，且复盘分析发现这是一手强棋。换言之，AlphaGo表现出了超越人类棋手的布局洞察力。</p><p>&nbsp;</p><p>AlphaGo能够根据当前棋盘状态模拟出数千种可能的后续发展，从而获取类似的见解。对于计算机来说，潜在棋序实在太多，根本不可能一一检查，所以AlphaGO使用神经网络来简化整个过程。</p><p>&nbsp;</p><p>其中的策略网络能够预测出哪些棋路最有希望，值得进一步做模拟分析。而价值网络则负责估算棋盘的当前状态是对白方有利、还是对黑方有利。根据这些估算，AlphaGo再逆向计算下面一步该怎么走。</p><p>&nbsp;</p><p>Legg的观点是，这类树搜索方法有望提高大语言模型的推理能力。大语言模型要预测的不只是单个最可能出现的token，而应在给出回答之前探索数千种不同的响应。事实上，DeepMind的思维树论文似乎就是朝这个方向迈出的第一步。</p><p>&nbsp;</p><p>前文提到，OpenAI曾经尝试使用生成器（生成潜在答案）与验证器（估算这些答案是否正确）组合来解决数学问题。这与AlphaGo明显有几分相似，同样可以理解成策略网络（生成潜在棋步）与价值网络（估算这些棋步能否导向更有利的盘面状态）。</p><p>&nbsp;</p><p>如果将OpenAI的生成器/验证器网络与DeepMind的思维树概念相结合，就能得到一套与AlphaGo非常相似的语言模型，同时保留AlphaGo的强大推理能力。</p><p>&nbsp;</p><p></p><h1>为何命名为Q*</h1><p></p><p>在AlphaGO之前，DeepMind曾在2013年发表过一篇关于训练神经网络以打通雅达利电子游戏的论文。DeepMind并没有手动录入每款游戏的规则，而是让网络不断游玩这些游戏，通过反复试验自行理解玩法。</p><p>&nbsp;</p><p>参考早期强化学习技术Q-learning，DeepMind将这套雅达利解决方案命名为Deep Q-learning。DeepMind的雅达利AI中包含一个Q函数，用于估算任意特定操作（例如向左或向右推操纵杆）可能获得的奖励（比如更高的得分）。当系统游玩雅达利游戏时，它会不断优化Q函数，提升获取更佳得分的估算能力。</p><p>&nbsp;</p><p>DeepMind 2016年在AlphaGo论文同样使用字母Q来表示AlphaGo中的棋步价值函数——该函数用于估算任意给定棋步有多大可能通往对局胜利。</p><p>&nbsp;</p><p>AlphaGo和DeepMind的雅达利AI都属于强化学习的范畴，这是一种从经验中学习知识的机器学习技术。在大语言模型兴起之前，OpenA也I一直将强化学习作为关注重点。例如，OpenAI曾在2019年使用强化学习让机械臂在自行探索中学会解开魔方。</p><p>&nbsp;</p><p>参考这些背景，我们似乎可以对Q*做出有理有据的解读：它是将大语言模型同AlphaGo式搜索能力相结合的产物，而且应该是在以强化学习的方式进行混合模型训练。其重点就是找到一种在困难的推理任务中“自我较量”的方式，借此改进语言模型的实际能力。</p><p>&nbsp;</p><p>其中一条重要线索，就是OpenAI今年早些时候决定聘请计算机科学家Noam Brown。Brown在卡耐基梅隆大学获得博士学位，并在那里开发出首个能够超越人类水平的扑克AI。之后Brown加入Meta，并开发出玩《强权外交》桌游的AI。这款游戏的成功秘诀在于同其他玩家结成联盟，因此AI必须把战略思维与自然语言能力结合起来。</p><p>&nbsp;</p><p>由此看来，这似乎就是帮助大语言模型提高推理能力的绝佳案例。</p><p>&nbsp;</p><p>Brown今年6月在推文中表示，“多年以来，我一直在研究扑克和〈强权外交〉桌游中的AI自我对弈和推理课题。现在，我想探索如何将成果转化为普适性能力。”</p><p>&nbsp;</p><p>AlphaGo和Brown扑克AI中使用的搜索方法，明显只适用于这些特定游戏。但Brown预测称，“如果我们能发现一个通用版本，则必然带来巨大的收益。没错，推理速度可能会降低至千分之一且成本迅速膨胀，但如果能够发现新的抗癌药物、或者证明黎曼猜想，这一切难道不值得吗？”</p><p>&nbsp;</p><p>而在Brown于今年早些时候离职之后，Meta公司首席AI科学家Yann LeCun表示，他认为Brown研究的就是Q*。</p><p>&nbsp;</p><p>LeCun在11月的推文中指出，“看起来OpenAI更进一步的探索就是Q*，他们还聘请了Noam Brown来协助解决这个问题。”</p><p>&nbsp;</p><p></p><h1>两大挑战</h1><p></p><p>&nbsp;</p><p>如果大家跟科学家或者工程师共事时，就会注意到他们特别喜欢用白板。当我自己在研究生院学习计算机科学时，我们就经常站在白板前面绘制图表或者议程。随后在谷歌的实习经历，也让我意识到技术大厂里同样到处都是白板。</p><p>&nbsp;</p><p>白板确实很有启发意义，因为面对极为困难的技术问题，人们刚开始根本不知道该如何下手。他们可能会花几小时勾勒出了种潜在的解决思路，却发现根本就不适用。之后他们就擦掉一切，从零开始找个不同的切入角度。或者，他们也可能觉得方案的前半部分还行，于是擦掉后半部分再换条新的探索路线。</p><p>&nbsp;</p><p>这本质上就是一种智能树搜索：对多种可能的解决方案进行迭代，直到找出一个似乎可以实际解决问题的路线。</p><p>&nbsp;</p><p>OpenAI和DeepMind之所以对大语言模型加AlphaGo搜索树感到如此兴奋，就是因为他们希望计算机也能执行同样的开放式智能探索。到那个时候，我们只需要把充满挑战的数学问题输入给大语言模型，然后安心上床睡觉。第二天早上醒来，它已经考虑了几千种可能的解决方案，并最终给出一些可能有希望的探索方向。</p><p>&nbsp;</p><p>这当然是个鼓舞人心的愿景，但OpenAI至少还要克服两大挑战才能将其转化为现实。</p><p>&nbsp;</p><p>首先，就是找到一种让大语言模型进行“自我对弈”的方法。AlphaGo就是通过自我对弈完成了对顶尖人类棋手的碾压。OpenAI也在模拟物理环境中进行魔方实验，通过判断魔方是否处于“解开”状态来判断哪些操作有正向作用。</p><p>&nbsp;</p><p>而他们的梦想就是建立起一套大语言模型，通过类似的自动化“自我对弈”方式提高推理能力。但这就需要一种能够自动检查特定解决方案是否正确的办法。如果系统还需要人类来检查每条答案正确与否，那么训练规模将非常有限、难以带来可与人类匹敌的推理水平。</p><p>&nbsp;</p><p>就在2023年5月发表的论文中，OpenAI还在聘用审查员来核对数学答案的正确性。所以如果真的出现了突破，那肯定是发生在过去这几个月间。</p><p>&nbsp;</p><p></p><h1>学习是个动态的过程</h1><p></p><p>&nbsp;</p><p>我认为第二个挑战才是根本：通用推理算法，必须在探索各种可能性时表现出动态学习能力。</p><p>&nbsp;</p><p>当人们尝试在白板上推衍解题思路时，他们并不是在机械地迭代各种可能路线。相反，每试过一个失误的路线，人们对问题的理解也就又加深了一步。在推理过程中，他们的心理模型也在不断演进，逐渐生出能快速判断哪种方法更好的强大直觉。</p><p>&nbsp;</p><p>换句话说，人类内心的“策略网络”和“价值网络”并非一成不变。我们在同一个问题上花费的时间越多，在思考潜在答案时的判断能力也就增强，自然更善于预测当前思路是否有效。如果没有这种实时学习能力，我们一定会迷失在无穷无尽的潜在推理步骤当中。</p><p>&nbsp;</p><p>相比之下，目前大多数神经网络在训练和推理之间保持着严格的边界。一旦训练完成，AlphaGo的策略和价值网络就被固定下来了——后续任何比赛过程都不会产生改变。这对围棋来说没有问题，因为这项游戏的规则足够简单，可以在自我对弈的过程中体验各种可能的情况。</p><p>&nbsp;</p><p>但现实世界要比方寸棋枰复杂得多。从定义上讲，研究者想要解决的是以往未能解决过的问题，所以实际情况很可能与训练期间遇到的任何问题都存在巨大差异。</p><p>&nbsp;</p><p>因此，通用推理算法的实现必须在推理过程中持续获取见解，以便在模型解决问题的同时不断增强后续决策质量。然而，目前的大语言模型完全通过上下文窗口来维持状态，而思维树方法在现有模型的一个分支跳往另一分支时，之前的记忆信息会被新的上下文窗口直接删除。</p><p>&nbsp;</p><p>一种可能的解决方案，就是使用图搜索来取代树搜索。今年8月的一篇论文就提到这种方法，尝试让大语言模型将来自多个“分支”的见解结合起来。</p><p>&nbsp;</p><p>但我高度怀疑，真正的通用推理引擎恐怕需要在底层架构上做根本性创新。语言模型必须借助新的方法来学习超越训练数据的抽象概念，并利用这些不断发展的抽象概念强化探索潜在解决方案空间时的具体选择。</p><p>&nbsp;</p><p>我们都知道这绝非妄言，毕竟人类的大脑就能做到这一点。而OpenAI、DeepMind乃至其他厂商可能还需要一段时间，才能搞清楚如何把这种方法照搬到硅芯片之上。</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://www.understandingai.org/p/how-to-think-about-the-openai-q-rumors">https://www.understandingai.org/p/how-to-think-about-the-openai-q-rumors</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/I7oTGIxwXsUk2SGfQAyf</id>
            <title>多场开发实战课，百度智能云技术大咖现场教学！</title>
            <link>https://www.infoq.cn/article/I7oTGIxwXsUk2SGfQAyf</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/I7oTGIxwXsUk2SGfQAyf</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 07:42:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型技术, 智能化跨越, 百度云智大会, AI原生应用
<br>
<br>
总结: 大模型技术正在推动各行业的智能化跨越，百度云智大会是一个重要的活动，旨在引领智能计算技术创新，传递最新实践与突破。AI原生应用的构建和实际落地是关键，开发者需要将AI技术与实际应用场景相结合，开发有用、有价值的产品和服务。 </div>
                        <hr>
                    
                    <p>大模型技术正在以前所未有的速度推动各行业的智能化跨越。对于身处这个时代的开发者来说，他们不仅需要不断学习新知识，还要探索如何将 AI 更好地融入实际应用场景。</p><p></p><p>面对崭新的时代，开发者若想找到一条提升思维认知和开发效率的最短路径，百度智能云每年举办的百度云智大会·智算大会是不容错过的：</p><p></p><p>智能计算大会是百度智能云面向“云计算产品与技术”的重磅活动之一，以引领智能计算技术创新为目标，传递百度智能云产品与技术的最新实践与突破。历经 3 载，从 AI 原生云到深入产业，百度智能云传递着创新的火种，描绘着智能计算的未来。2023 年，智能计算大会全新起航，将以“重构云计算·Cloud for AI”为主题，结合大模型技术以及 MaaS 服务，碰撞最前沿的技术与产品，开启全新的智能计算时代。</p><p></p><p>“工欲善其事，必先利其器”。对于开发者来说，百度云智大会·智算大会是你不可或缺的技术盛宴，它不仅是探索前沿科技的窗口，更是开发实战的“课堂”。</p><p></p><p>2023 百度云智大会·智算大会将于 12 月 20 日在北京落地，本次大会以“重构云计算·Cloud For AI”为主题，汇集了百度集团副总裁侯震宇、IDC 中国区副总裁兼首席分析师武连峰、百度副总裁谢广军等多位行业大咖，聚焦 AI 和云，解读智能计算带来的万千可能和全新图景，带开发者窥见 AI 原生时代的技术创新重构。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a9/a9f9eea7c6ff5cd2c5ecdb72404cb32b.png" /></p><p></p><p>仅是让大家了解 AI 原生时代的趋势还不够！为了让你获得知行合一的参会感受，下午特别开设了「2023 百度云智大会·智算大会 开发者沙龙」，旨在为开发者提供切实有效的开发技能。 今年 9 月，李彦宏曾在 2023 百度云智大会上强调 AI 原生应用的重要性，他表示，AI 原生应用要能解决过去解决不了、解决不好的问题，应用才是大模型存在的意义。这意味着在 AI 大模型时代，AI 原生应用的构建和实际落地是关键。对于所有开发者而言，则需要能在先进技术和模型的基础上，将 AI 技术与实际应用场景相结合，开发出有用、有价值的产品和服务。</p><p></p><p>为了让开发者能够实操跟练，下午场的「2023 百度云智大会·智算大会 开发者沙龙」活动，由百度智能云主任架构师吴多益、百度资深工程师 &amp; 百度 Comate 产品架构师徐晓强等技术大咖担任分享讲师。</p><p></p><p>实践课程设置方面，由浅入深地涵盖了从编码到应用开发的内容，帮助开发者通过现场实战，学习热门产品及技术、提升软件开发效率，打破在技术与实际应用场景结合方面的障碍。期待参与其中的你，不仅能够掌握提升开发效率的方法，还能建立起构建 AI 原生应用的思维方式。</p><p></p><p>本次沙龙，还为开发者准备了丰富的互动礼品，完成任意一场课程及实验，即可获得精美礼品！线下席位有限，抓紧扫码占位！12 月 20 日 13:00，我们在「2023 百度云智大会&nbsp;· 智算大会 开发者沙龙」不见不散！</p><p></p><p><img src="https://static001.geekbang.org/infoq/e8/e8203ab57b2d095c1230776b92aabdf3.jpeg" /></p><p></p><p>                                                           一起掌握开发“金手指”</p><p></p><p>                                                        提升开发效率，准时下班吧！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/VG9Loxtgp3eHdPdDec49</id>
            <title>大语言模型加速信创软件 IDE 技术革新</title>
            <link>https://www.infoq.cn/article/VG9Loxtgp3eHdPdDec49</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/VG9Loxtgp3eHdPdDec49</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 07:38:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 智能化信创软件 IDE, 信创, 智能化, 大语言模型
<br>
<br>
总结: 本文介绍了智能化信创软件 IDE 的重要性和意义。智能化技术的发展使得软件开发工具更加强大，例如自动化重构、代码翻译和自动化文档生成等功能。智能化信创软件 IDE 的目标是实现核心技术的可掌控和可研究，以规避信息安全、供应链安全、技术依赖和经济风险。通过信创化，可以解决现有技术的问题并实现超越。同时，文章还介绍了华为云开发工具和效率领域首席专家王亚伟的观点和团队的研发工作。在 QCon 全球软件开发大会上，王亚伟和他的团队将分享关于大语言模型、AI 编码辅助和下一代 IDE 平台架构等技术的内容。 </div>
                        <hr>
                    
                    <p>什么是智能化信创软件 IDE？为什么它很重要？</p><p>&nbsp;</p><p><a href="https://qcon.infoq.cn/2023/shanghai/schedule">QCon 全球软件开发大会（上海站）</a>"将于 12 月 28-29 日举办，会议特别策划「智能化信创软件 IDE」专题，邀请到华为云开发工具和效率领域首席专家、华为软件开发生产线 CodeArts 首席技术总监<a href="https://qcon.infoq.cn/2023/shanghai/track/1598">王亚伟</a>"担任专题出品人，为专题质量深度把关。作为拥有云和开发工具领域近 20 年经验的老兵，华为公司软件开发工具领域的领军人物，20 多项软件开发技术发明专利的拥有者，王亚伟对于「智能化新创软件 IDE」这个专题有着怎样的理解？在会议即将开幕之际，王亚伟与 InfoQ 分享了他的核心观点。</p><p></p><p><img src="https://static001.geekbang.org/infoq/29/29e75f26136948c06ee3f9bfd82139f8.jpeg" /></p><p></p><p>&nbsp;</p><p>“信创”是信息技术应用创新的简称，其本质是发展国产替代技术，实现核心技术的可掌控、可研究、可发展等。</p><p>&nbsp;</p><p>相比“信创”，“智能化”在过去 5 年中被业界反复提起，智能化技术的发展必然会使诸如 IDE 这样的软件开发工具更加强大。随着大语言模型的诞生，IDE 除了可以自动地完成一些重复性工作之外，还可以协助开发人员在软件的设计和开发过程中完成更多创新性的工作，比如：</p><p>自动化重构：将一段复杂的代码分解为更小、更易于管理的函数或类。开发者可以描述他想要实现的重构目标，然后让模型生成相应的代码代码翻译：大语言模型可以将一种编程语言的代码翻译成另一种编程语言，再配合 IDE 的语法高亮和错误检查功能，可以帮助开发者使用不熟悉的编程语言编写代码自动化文档生成和更新：大语言模型可以根据代码和注释生成相应的文档，或者在修改代码时自动更新文档。大语言模型是 IDE 的智能化加速度</p><p>&nbsp;</p><p>IDE 的”信创“化旨在将基础软件开发的核心技术实现自主可控，在拥抱开源的同时逐步建立基于自有技术内核的架构和标准，形成自有开放生态。信创化的目的是为了规避可能或已经发生的风险：</p><p>信息安全和供应链安全风险：在关键时刻，国外的产品和技术可能会面临供应链中断的风险。此外，国外产品或开源技术可能会存在安全漏洞或后门，基于这些技术打造的商业解决方案会威胁用户的信息安全 - 2020 年 3 月发生的 SolarWinds 攻击事件导致业界领先的开发工具公司 JetBrains 遭受牵连技术依赖风险：如果完全依赖于外国的技术，那么我们在软件开发核心技术领域的研究、发展和创新能力就会受制于人，最终导致落后经济风险：技术上依赖意味着我们需要持续支付大量的许可费用</p><p>&nbsp;</p><p><a href="https://www.infoq.cn/article/d4613NRodWJEAXqRblEu">延伸阅读：被逼出来的自主可控，从华为自研看国产 IDE 的未来和商业模式</a>"</p><p>&nbsp;</p><p>“信创”化不意味着重复造轮子或为了与现有技术不同而进行盲目创新，而是目标实现核心技术可控的前提下，解决现有技术的问题，从而对现有技术实现某些方面的超越。举个例子，代码索引是 IDE 的文件查找、代码提示等功能的基础数据源，现有商业 IDE 代码索引的创建、存储和访问效率并不高，索引数据基于对象存储访问时，一个只有 8 字节（2 个 int）内容的数据封装成对象后要占据至少 24 个字节的存储空间。同时，由于内存读写速率要远低于缓存，如果在存储和访问索引时没有以一种缓存友好的方式进行，读写效率甚至 100 倍下降。我们团队在代码索引存储和访问领域提出了一种基于内存压缩的索引自动化存储和访问技术，可以做到 50 倍以上的综合效率提升，该技术已经被评选为华为云高价值专利，并应用到 IDE 内核、运行时优化、云编译等多个领域。” <a href="https://www.infoq.cn/article/ubciEs8NPH06CwlpEvtf">延伸阅读：生成的代码会出错、质量差？面对 AI 编程工具的老大难问题，华为这群人打算这样做</a>"</p><p>&nbsp;</p><p>技术的积累需要时间，产品研发更需要打磨。王亚伟介绍道：“从 2019 年初开始，我们逐步组建了一支数百人的软件研发专家队伍，分布在中国、俄罗斯、欧洲等国家地区，其中一半成员来自于业界顶尖的软件和工具公司，超过 40% 的成员是开源社区的 Committer 和 Contributor，整个团队都围绕着‘做最好的产品’展开工作，我们建立了从产品、运营、UX 到开发、测试的完整专业的产品研发流程，每月一个小版本、三个月一个大版本，基于内外部用户的反馈快速迭代。过去五年我们真正做到了深耕软件开发工具这个专业领域。”</p><p>&nbsp;</p><p>同时，王亚伟也坦言：“虽然从产品成熟度上看我们的信创化工具跟业界成熟的商用工具相比还有差距，但‘信创’本身绝不意味着竞争力弱，体验打折。我们会继续秉持着‘做最好的产品’的信念，不断前进。”</p><p>&nbsp;</p><p>在今年的「智能化信创软件 IDE」专题上，王亚伟带领他的专家团队将围绕大语言模型、AI 编码辅助、下一代 IDE 平台架构、动态语言类型推理等技术，给大家带来一场技术盛宴。</p><p>&nbsp;</p><p>议题<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5567">《AI 开道，让编程体验“一路狂飙》</a>"，详细介绍华为云 <a href="https://www.huaweicloud.com/devcloud/">CodeArts</a>" 团队应用大模型开发的 AI 辅助编程的技术 - CodeArts Snap，讲师程啸从博士阶段开始就对代码生成、RAG、代码克隆检测等领域有较深入的研究，他这次也是代表 Snap 团队进行分享。</p><p>&nbsp;</p><p>另外三个英文议题是来自于 CodeArts 俄罗斯的专家团队。</p><p>&nbsp;</p><p><a href="https://qcon.infoq.cn/2023/shanghai/presentation/5568">Applying Machine Learning in IDE Challenges and Insights</a>"将会系统讨论 AI 技术在 IDE 中的应用研究以及如何深远改变我们的开发，测试和调试代码的方式。讲师 Pavel 是俄研院新西伯利亚实验室主任，20 年开发者工具构建经验，机器学习专家、Eclipse IDE 的专家和 Committer。</p><p>&nbsp;</p><p>议题 <a href="https://qcon.infoq.cn/2023/shanghai/presentation/5575">Evolution of IDE Platforms</a>" 会紧密围绕其在构建 IDE 平台时面临的问题和挑战比如分布式代码模型架构下如何确保前后端组件可以高效交互、如何直接从后端内核画出前端复杂 UI，以及我们如何做出艰难的架构和设计决策，同时分享对下一代 IDE 平台的架构和设计展望。Denis 是俄罗斯新西伯利亚实验室的首席架构师，20 多年的工具研发经验，精通编译器、DSL、编程框架，Eclipse 社区 Committer。</p><p>&nbsp;</p><p>静态语言如 Java，C# 等，它的类型推理主要通过编译器完成，代码模型可以通过类型绑定（通常存在于程序的元数据 metadata 中）获得所需要的类型信息。而动态语言的类型推理主要由 IDE 完成，由于缺少编译元数据的支持，动态语言的类型推理是一个业界难题。以 Python 为例，其有一个完全动态严格的类型系统，类型（type）在运行时动态绑定到变量（variable），变量和类型都可以在运行时动态被改变 – 这增加 Python IDE 进行可靠类型推理的难度。议题 <a href="https://qcon.infoq.cn/2023/shanghai/presentation/5576">Type inference engine</a>" 会介绍该团队在做动态语言类型推理时设计和实现的技术细节，并讨论未来该领域的发展方向。Nikolai 是俄罗斯圣彼得堡实验室的首席软件工程师，拥有 15 年 IDE 研发经验，是前 JetBrains Intellij IDEA 和 Scala 项目负责人，精通 Compiler、Program Language Design、Code Analysis 等技术。</p><p>&nbsp;</p><p>据了解，QCon 上海还邀请到了<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5623">中国科学院外籍院士、国际数据库专家樊文飞院士</a>"，<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5586">英特尔大数据技术全球 CTO 戴金权</a>"等大咖会亲临现场分享大数据、芯片、架构等方向的前沿洞见。这次会议主要探讨大模型的全面技术架构的进化，不仅有跟大模型本身相关的推理加速、AI Agent、GenAI，还有架构的演进思路、性能优化，以及以智能代码助手为代表的研发效能提升等方向，邀请<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5599">阿里巴巴的通义星尘</a>"、魔搭社区开源 <a href="https://qcon.infoq.cn/2023/shanghai/presentation/5673">ModelScope-Agent</a>" 框架、百度文心大模型驱动下的智能代码助手等团队核心技术骨干前来分享，目前大会日程已上线，<a href="https://qcon.infoq.cn/2023/shanghai/schedule">可点击下方图片查看详情。</a>"</p><p></p><p><img src="https://static001.geekbang.org/infoq/66/66a620fc97b46ea55958f9b172195701.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/0a/0a6b81251700257adb01b1334e7d49f9.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/SKtUWjqpsK9DV0gFaR5W</id>
            <title>从业务应用挑战出发，火山引擎专家深度拆解“弹幕互动方案”的全新实践</title>
            <link>https://www.infoq.cn/article/SKtUWjqpsK9DV0gFaR5W</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/SKtUWjqpsK9DV0gFaR5W</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 04:15:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 互联网, 视频化时代, 火山引擎, 弹幕互动玩法
<br>
<br>
总结: 互联网正在进入视频化时代，火山引擎作为一种视频云服务，与NVIDIA合作推出了《云上新视界》线上课程，其中分享了弹幕互动玩法的解决方案与应用实践。弹幕互动玩法是一种通过弹幕、送礼物等互动操作控制直播画面中互动内容的直播方式，具有即开即玩、多人互动等特点。弹幕互动经历了PC端开播、云游戏方案和云游戏+RTC方案三个核心演进阶段。火山引擎通过优化方案解决了弹幕互动延时和外放回声的问题。 </div>
                        <hr>
                    
                    <p>从互联网到全行业视频化时代，营销、商品、知识与空间的体验正在被重塑和创新，<a href="https://www.infoq.cn/article/z1CW0cFhLxLi2KYk258t?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">火山引擎</a>"视频云以“面向体验，驱动创新”为核心，特别与 NVIDIA 团队合作推出《<a href="https://www.infoq.cn/article/OHhA89XUrsQtm7T5Ts43?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">云上新视界</a>"》线上课程。第五期课程中，火山引擎 RTC 商业化解决方案团队负责人郭健为大家分享了当前热门玩法“弹幕互动”的解决方案与应用实践。</p><p></p><h2>一、什么是“弹幕互动玩法”?</h2><p></p><p></p><p>弹幕互动玩法是依托直播间（直播连麦、语聊房等互娱核心场景），观众可以通过弹幕、送礼物等互动操作，控制直播画面中的互动内容的一种直播方式，具备即开即玩、多人互动等特性，兼具观众互动性强、直播内容游戏化趣味化等特点。</p><p></p><p>从 2014 年的《Plays Pokémon》到 2021 年尾《修勾夜店》爆火，弹幕互动几经翻红。今年开始，弹幕互动受到各大平台的广泛关注，从玩法上线后效果看弹幕互动玩法的户观看人数 / 时长、营收等核心指标都有很好的收益。</p><p></p><h2>二、弹幕互动方案的 3 个核心演进阶段</h2><p></p><p></p><p>弹幕互动经历了 PC 端开播、云游戏方案、云游戏 + <a href="https://www.infoq.cn/article/Ue0E2ZXpr2BwaYxlQ0fL?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">RTC </a>"方案三个阶段。</p><p></p><p>第一个阶段，PC 端开播。传统开播流程需要主播先在 PC 端安装程序和开播工具，互动玩法在主播 PC 上运行和渲染。同时，主播使用 PC 端直播工具（比如 OBS）对本地画面和主播直播画面混流，再推送到直播间。观众进入直播间发送弹幕或者发送礼物参与互动。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f6/f65bdaf68b7489076c5d4af003097847.png" /></p><p></p><p>这种方式存在一定局限性，比如：</p><p></p><p>弹幕互动内容本身需要实时计算渲染，对设备硬件配置如显卡计算能力有较高要求，甚至堪比 3A 大作性能要求，开播设备性能不足，就会导致弹幕无效甚至内容本身卡死，影响直播间用户体验；越来越多的主播更习惯在移动端随时开播，而只能运行在 PC 端的弹幕互动程序，会大大增加开播门槛，也降低平台玩法覆盖度；移动端开播还可以与平台其他玩法相结合，但如果单独为弹幕玩法准备 PC 端 OBS 开播，既增加了维护成本，也难以进行推广。</p><p></p><p>第二阶段，在直播 / 语聊的基础上引入云游戏。主播进入连麦房间推拉 RTC 流的同时，也需要进入云游戏的房间拉取互动玩法音视频。然后业务层把 H5 引擎拉取到的视频流和业务层采集到的摄像头流在端上合流后，推入直播房间。</p><p></p><p><img src="https://static001.geekbang.org/infoq/07/07a7368a086b934ca4812074f93dbffb.png" /></p><p></p><p>这个方案解决了开播平台限制和开播设备的限制，但是有一些方案接入和体验问题。从方案层面看，业务逻辑复杂接入相对麻烦。从体验看，存在嘉宾 / 观众侧主播解说和互动画面会有轻微的不同步、画面延时大、有回声等问题。其中，RTC 引擎订阅云游戏音频观众侧有回声主要是因为游戏流的声音或者麦克风会采集到本地播放的游戏声音。</p><p></p><p>为了解决上个方案的几个问题，火山引擎视频云首推“云游戏 +RTC 方案”方案，而弹幕互动方案也正式进入了第三阶段——火山引擎 RTC 与云游戏产品在服务侧和引擎侧做了深度协同优化。在服务侧，优化了调度方案，保证用户连接的云游戏 pod+RTC 媒体服务器在同一个机房、云游戏音视频流可直接送入 RTC 房间。在引擎侧，云游戏引擎直接依赖宿主侧的火山 RTC 引擎、云游戏引擎裁剪场景无关功能。</p><p></p><p>在具体操作中，首先主播通过云游戏引擎开启互动完成程序，云游戏启动 pod 并创建火山 RTC 房间。完成后，Pod 集成云游戏引擎和 RTC 引擎向火山云游戏房间推音视频流，火山云游戏房间跨房转推音视频流到两个直播 / 语聊房间，嘉宾和观众通过 RTC 直接拉取直播流和云游戏流即可。</p><p></p><p><img src="https://static001.geekbang.org/infoq/91/918deed71d6b8a03257846fafd65c7d3.png" /></p><p></p><p>云游戏和 RTC 内部深度协作，缩短数据流转链路在接入直播 / 语聊的基础上，仅需接入 veGameSDK 启动游戏、业务端通过 OpenAPI 同步弹幕 / 礼物数据到云游戏服务器两步即可完成场景“升级”，大大简化业务逻辑，缩短接入周期减少工作量。</p><p></p><h2>三、火山引擎是怎么解决历史方案问题的？</h2><p></p><p></p><p>此前弹幕互动方案所存在的观众弹幕互动延时、主播外放有回声等体验问题，火山引擎方案是如何解决的？</p><p></p><h4>&nbsp;1. 弹幕互动延时问题</h4><p></p><p></p><p>未优化的云游戏方案观众端发送弹幕后，由于传统 RTMP 直播流延迟较大，观看云游戏观众侧会有 3~5 秒延时，并且都会有轻微的互动画面与解说的不同步，体验较差。这些在普通常见的场景可能影响不大，但是在对战场景，战场形势瞬息万变，可能最后一秒的延时失去被“偷家”导致战斗失败。</p><p></p><p>优化后，使用全 RTC 方案，可以让用户参与玩法整体延时&lt;400ms 。</p><p></p><h4>&nbsp;2. 外放回声消除</h4><p></p><p></p><p>在未优化方案中，云游戏的声音在经过扬声器播放后，会被近端用户的麦克风采集到并产生回声问题，需要参考扬声器播放的声音进行回声消除技术处理，云游戏和 RTC 独立运行，云游戏音频无法给到 RTC 引擎，所以容易产生回声。</p><p></p><p>在优化方案中，云游戏音频可以直接跨房转推到 RTC 房间，场景内音频播放通过音频托管的方式统一由 RTC 进行音频播放，有参考信号，可以彻底消除回声，以确保对端收到清晰的声音。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b7/b7615c34243048d5d4edabe66d99ef81.png" /></p><p></p><p></p><h2>四、弹幕互动方案在业务应用中的挑战与实践</h2><p></p><p></p><h4>&nbsp;1. 卡顿优化</h4><p></p><p></p><p>弹幕互动场景有一个特点就是画面极致高清，一般是高清 1080P、 帧率 30fps、高码率 8Mbps。同时，主播、观众均为移动端设备，随时开播与参与，用户网络环境复杂且不稳定。在这种高分辨率高码率、且网络不稳定情况下极其容易造成卡顿劣化。</p><p></p><p>要优化这种情况，首先把线上 H264 升级为自研 ByteVC1 编解码，在 PSNR（视频质量客户评价）画质质量优于原方案 2dB 时，还能节约 10% 码率。此时对于线上情况码率可能仍较大，火山引擎 RTC 采用智能流控协议 (VISC)，它基于 Simulcast 和 SVC 策略优化而来、更加智能的一种传输协议，它可以综合考虑音视频通话中每个订阅者的个性化需求，在网络情况、终端性能发生变化的时候，自动调整音视频流的配置，最大限度地让每个参与者的个性化需求得到满足，为用户提供更流畅的互动体验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5a/5a57bdf9d82120b552297cf2fbbe2456.png" /></p><p></p><h4>&nbsp;2. 操作延时优化</h4><p></p><p></p><p>云游戏在所有的云计算相关应用中，对延时要求最为苛刻，火山引擎 RTC 针对云游戏与 RTC 场景相结合的应用场景，进行全链路延时优化。</p><p></p><p>阶段一，边缘机房阶段。保证用户连接的云游戏 Pod 和 RTC 服务器调度到同一个机房，使用更高效传输方式优化，首帧时长减少约 30ms；降低延时 50ms；编码前优化采集和格式转换，使用 OpenGL 转换替换 libyuv 转换，优化延迟 15ms;阶段二，级联服务优化。减少级联服务器和优化信令传输，优化 20ms;阶段三，订阅端。针对云游戏下行音视频调整 jitterbuffer 大小，降低延时 60~260ms，有优化的处理，可以不影响直播 / 语聊体验；针对不同的硬件解码器做优化，最多优化延迟 90ms；内部渲染替代外部渲染降低延迟 5ms，整体云游戏到端延时可以达到小于 75ms。</p><p></p><h4>&nbsp;3. 性能优化</h4><p></p><p></p><p>弹幕互动玩法可以在个人直播、直播连麦或者跨房 PK 中等场景中加入。在语聊房跨房 PK+ 弹幕互动玩法场景中，假设每个语聊房会有 9 人，两个房间 PK 时，单个用户最多需要拉 18 路音频流和云游戏音视频流，性能压力大，玩法准入机型门槛高，设备发热严重。</p><p></p><p>因此，为减少对手机性能消耗，火山引擎 RTC 使用 RTC 公共流不进房拉流方案。这个方案中，本房间内拉流方式不变，PK 房间的音频流合流后推一路公共流，对比普通语聊模式单个用户只多拉一路音频流和一路云游戏流。两个房间 PK，每个房间 1 位主播、8 位嘉宾、100 位观众流数评估，单房间减少（1+8+100）*8 约 872 路、单用户减少 8 路流，有效优化用户拉流性能，减少 50% 流数量。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3c/3c3b679655ff084ab5807c48eece0a0f.png" /></p><p></p><p>独立集成云游戏 SDK 包体增量一般 9M 左右，9M 的包增量对客户来说是不可接受的。弹幕互动方案中云游戏直接复用火山引擎 RTCSDK 传输能力，云游戏 SDK 精简包只需操控信令和选路部分，精简包给整体带来增量仅 610KB。</p><p></p><h2>五、写在最后</h2><p></p><p></p><p>总体来说，火山引擎弹幕互动方案有五大优势：</p><p></p><p>不限设备、不限场景，零门槛开播：无论是个播还是多人互动，移动端即可随时随地“云开启”弹幕互动玩法，无需高性能 PC，消除互动内容本身对用户终端算力的限制；热门弹幕互动内容全适配：云游戏支持直接部署基于 UE/Unity 框架的互动内容，底层多种类型 IaaS 和对应 GPU 配置，满足不同等级算力要求的弹幕互动玩法；无惧弹幕高并发，渲染画面高清流畅：云游戏支持 ARM、x86 以及定制化 GPU 等多样化计算资源，并采用自研 ByteVC1 编解码结合动态码率技术，保证互动画面流畅体验同时节约带宽消耗，互动画面 100ms 卡顿率低于 2%；主播解说与玩法进程实时同步：通过火山引擎 RTC 媒体节点和 云游戏 Pod 端同机房调度，超低延时体验，操作延时小于 90ms，主播讲解和内容画面实时同步，保障观众沉浸互动体验；应用最小包增量引入：弹幕互动方案中云游戏可直接复用火山引擎 RTC SDK 传输能力，云游戏 SDK 精简包只需操控信令和选路部分，精简包增量仅 KB 级。</p><p></p><p>而本期课程中介绍的弹幕互动玩法的解决方案技术实践只是“小试牛刀”，如果想要了解更多，可以扫描下方二维码，有更加详细的弹幕互动解决方案和获取弹幕互动 Demo！</p><p></p><p><img src="https://static001.geekbang.org/infoq/eb/ebba54e6aafb6fea8351787b6285c768.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/TqNGiNCf3yfomTXbrwm9</id>
            <title>英特尔软件与先进技术事业部 / 首席工程师胡宁馨确认出席 QCon 上海，分享 WebNN，Web 端侧推理的未来</title>
            <link>https://www.infoq.cn/article/TqNGiNCf3yfomTXbrwm9</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/TqNGiNCf3yfomTXbrwm9</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, WebNN, W3C 标准, AI推理
<br>
<br>
总结: QCon 全球软件开发大会将在上海举行，胡宁馨将分享关于WebNN API的主题演讲，探讨WebNN API的W3C标准进展以及对CNN、Transformer和生成式AI模型的支持情况和计划，以及在浏览器的实现进展。WebNN API提供了Web应用访问AI加速器的途径，以获得更好的性能和更低的功耗。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1208&amp;utm_content=huningxin">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。英特尔软件与先进技术事业部 / 首席工程师胡宁馨将发表题为《<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5646?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1208&amp;utm_content=huningxin">WebNN，Web 端侧推理的未来</a>"》主题分享，探讨 WebNN API 的 W3C 标准进度，对 CNN，Transformer 以及更广泛的生成式 AI (Generative AI) 模型的支持情况和计划，以及在 Chrome，Edge 等浏览器的实现进展。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/presentation/5646?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1208&amp;utm_content=huningxin">胡宁馨</a>"，就职于 Intel 软件与先进技术事业部，专注于 Web 技术，W3C 机器学习工作组 Web Neural Network API (WebNN) 规范的发起者和联合编辑，Chromium 项目 Code Committer，WebNN 模块负责人。他在本次会议的演讲内容如下：</p><p></p><p>演讲：WebNN，Web 端侧推理的未来</p><p></p><p>AI PC 以及 AI Mobile 的新兴时代已经到来，越来越多的设备集成了强大的神经处理单元 NPU，以实现高效的人工智能加速，这对需要端侧推理的应用至关重要。除了通过 CPU 和 GPU 进行推理之外，Web Neural Network API (WebNN) 提供了 Web 应用访问此类专有 AI 加速器 NPU 的途径，以获得卓越性能及更低功耗。</p><p></p><p>本次演讲将会给大家分享 WebNN API 的 W3C 标准进度，对 CNN，Transformer 以及更广泛的生成式 AI (Generative AI) 模型的支持情况和计划，以及在 Chrome，Edge 等浏览器的实现进展。作为 JavaScript ML 框架的后端，WebNN 将会在几乎不更改前端代码的前提下，为 Web 开发者及他们的产品带来相较于 Wasm，WebGL 更为优异的性能体验。</p><p></p><p>演讲提纲：</p><p></p><p>当前 Web AI 发展概况主流硬件加速器的发展（CPU，GPU，NPU)WebNN 设计与架构WebNN 代码演示WebNN 浏览器（Chromium）实现WebNN 机器学习框架集成（ONNXRuntime 和 TensorFlowLite)WebNN Transformers 支持WebNN 性能</p><p></p><p>听众收益点：</p><p></p><p>○ 了解 Web 平台对异构处理器的支持</p><p>○ 了解基于 Web 的机器学习模型硬件加速</p><p>○ 了解 Chromium 实现内部细节</p><p></p><p>除上述演讲外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！ 现在购票，享 9 折优惠，立减￥680！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/MNLwdrkZ7upG9yd2KXTY</id>
            <title>如何使用 Cluster Autoscaler 将批处理作业的节点扩容到 2000 个</title>
            <link>https://www.infoq.cn/article/MNLwdrkZ7upG9yd2KXTY</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/MNLwdrkZ7upG9yd2KXTY</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 02:13:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 火山引擎容器服务, VKE, Kubernetes, Cluster Autoscaler
<br>
<br>
总结: 本文介绍了火山引擎容器服务(VKE)作为云上Kubernetes平台的经历和挑战。文章首先解释了Cluster Autoscaler(CA)的概念和工作机制，包括自动调整集群大小和节点使用率的调度。然后介绍了CA的扩容和缩容逻辑，以及在实际客户场景中的应用。最后给出了一些建议，帮助实现集群弹性和避免类似的问题。 </div>
                        <hr>
                    
                    <p>本文将分享火山引擎容器服务 <a href="https://mp.weixin.qq.com/s?__biz=MzIzNjUxMzk2NQ%3D%3D&amp;chksm=e8d7e80adfa0611c8447800f02c4c482f8746bad0d541c80b22615aa09000dbe3b6fd28b2761&amp;idx=1&amp;mid=2247489480&amp;scene=27&amp;sn=3b54e854a6756c5191ca1032e4d1189b&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">VKE</a>" 作为云上<a href="https://mp.weixin.qq.com/s?__biz=MzI5ODQ2MzI3NQ%3D%3D&amp;chksm=eca7f1acdbd078baf17571575d642a09391401c945ddae152443ea51963b8eaca11b540c3364&amp;idx=1&amp;mid=2247502056&amp;scene=27&amp;sn=ef7a6deb9a63551cac10322b6b059aad&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect"> Kubernetes</a>" 平台，在帮助客户实现集群资源弹性过程中的一些经历和挑战，共分为以下几个部分：</p><p></p><p>第一部分介绍什么是 CA，以及它内部的流程和实现方式，帮助大家更好地理解其工作机制；第二部分简要说明客户批处理作业的使用场景；第三部分把重心放在客户在使用 Cluster Autoscaler 的过程中，碰到的问题和挑战，以及我们是如何解决的；最后将给出一些建议，帮助大家更好地实现集群弹性，避免踩到类似的坑。</p><p></p><h2>什么是&nbsp;Cluster Autoscaler(CA)</h2><p></p><p></p><p>从 <a href="https://xie.infoq.cn/article/d26b441cebb5b229a6efa35f4?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Cluster Autoscaler </a>"项目的 README 文档中，可以看到它包括几个方面：</p><p></p><p>自动调整集群大小，即扩缩容因为集群中资源不足，才会扩容缩容时由于集群中的节点使用率低于阈值，这个低使用率的节点上的 Pod 可以调度到其他节点上去</p><p></p><p>下图展示了用户视角下 CA 扩容的情况。当集群中出现 Pending Pod，没有节点能让这些节点调度上去时，CA 就会触发扩容，往集群中加入新的节点，让 Pod 调度上去。</p><p></p><p><img src="https://static001.geekbang.org/infoq/21/219ece266705fec98c75af3dc71ab8d3.png" /></p><p></p><p>而节点的使用率较低，比如图中的低于 50%，CA 就会把这个节点删除，Pod 被重新调度到其他的空闲节点上。这样一来，集群中工作负载的数量不变，但是节点数减少了，剩余节点和集群整体的使用率就提高了，对用户来说，这相当于降本增效。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4d/4de3555a3ae6af17767f51424697db9b.png" /></p><p></p><p>CA 是一个定期重复执行的过程，如果简化一下，它大致可以分为以下几个部分：</p><p></p><p>准备工作，CA 会先从集群中获取相关的数据，比如节点、集群的状态、需要调度的 Pending Pod、清理创建失败的节点、过滤还没 ready 的 GPU 节点等；扩容逻辑；缩容逻辑；结束；等待一段时间后，再从头开始。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0136fcaa26a5f89d7082fabc55ff2d61.png" /></p><p></p><p>在扩容阶段，CA 会先找到集群中无法调度的 Pending Pod，然后试着把这些 Pending Pod 和节点池做匹配，看看每个节点池都满足哪些 Pending Pod 的调度要求：有的节点池可能扩容了也不满足调度要求，这些节点池就被排除了；有的节点池能调度一部分 Pending Pod，那这些节点池就会保留下来。</p><p></p><p>对于这些保留下来的节点池，CA 会计算需要扩容多少个节点才能满足这些 Pending Pod 的资源用量，接着从这些节点池中按照设置的扩容策略选一个最合适的节点池。扩容策略可能是随机选择、也可能是优先级，或者最小浪费，这些都是由用户配置的。选择出最合适的节点池之后，CA 就会调用接口，告知云厂商需要扩容的数量，云厂商完成具体的 ECS 创建、加入集群等动作。</p><p></p><p>而在缩容阶段，CA 会找到使用率低于阈值的节点，查看这些节点上是否还有 Pod，如果没有 Pod 了，就认为这个是空节点，会被优先批量删除。删除完空节点以后，CA 再判断这些非空的节点上，Pod 是否可以调度到其他节点上去：如果可以调度，CA 也会把这个非空节点删除，节点上的 Pod 被驱逐、然后在别的节点上被重建。</p><p></p><p>这大概就是 CA 的整个过程，虽然省去了很多细节，但大家应该可以理解几个关键点：一个是 CA 中的逻辑，是定期运行的；第二个是在整个流程中，有扩容和缩容两个阶段，这两个阶段相互独立，扩容需要计算新增的节点数量、按照扩容策略选节点池，缩容就只看节点的使用率和上面的 Pod 是否可被重调度。</p><p></p><h2>客户场景</h2><p></p><p></p><p>我们遇到过这样一个案例，客户有自己的任务分发平台，不同计算任务通过任务平台下发到 Kubernetes 集群中，每批计算任务对应一堆的 Pod。而他们的业务存在这几个特点：</p><p></p><p>任务种类多，不同的任务所需的资源不同，CPU 用量各异，有的也会使用 GPU；不同任务对应的 Pod 数量也不同，峰值时整个集群超过 2w Pod；一般业务高峰期是在晚上，从凌晨开始跑，一直跑到早上；整体耗时长，不同批次任务耗时有长有短；Pod 的镜像也非常的大，拉取耗时长。</p><p></p><p>在这样的业务场景下，为了节省成本，客户很自然地使用了 Cluster Autoscaler，期望在计算任务下发后，节点池能自动扩容，添加新的节点到集群中，让 Pod 调度上去。在计算任务跑完以后，节点空闲下来，Cluster Autoscaler 再把节点删除，避免资源浪费。为了提高装箱率减少资源碎片，客户会对某些类型的任务，设置 Pod 的 resource request 和节点规格一致，尽量让这种任务的 Pod 独占一个节点</p><p></p><h2>问题与解决方案</h2><p></p><p></p><p>问题一：扩容成功率低</p><p></p><p>在客户上量过程中，我们碰到的第一个问题，是在大规模扩容过程中出现的大量扩容失败。CA 触发节点池扩容后，一部分节点创建成功，调度了部分 Pod，另一部分节点创建失败，在随后的过程中又被 CA 删除。由于还有部分 Pod 处于 Pending 状态，又触发 CA 扩容，然后又失败，周而复始。</p><p></p><p>这就给客户带来了非常糟糕的体验，一是看到很多失败的扩容记录，使其对云厂商的信任度降低；二是增加了不必要的成本，因为这些创建失败的节点并没有加入集群，不能被客户使用，但是节点对应的云服务器是实实在在被创建出来了，客户花了钱，但资源又没用上，就增加了无谓的成本。</p><p></p><p>经过仔细排查，我们发现节点扩容失败是因为云服务器在初始化 Kubernetes 组件的过程中，写入磁盘的速度特别慢，很久都不能加入集群，超过了预设的超时限制，我们判定这是一个异常的节点。异常节点随后又被 CA 清理删除，那我们就很好奇，为什么 ECS 的云盘写入这么慢？经过进一步的调研，我们发现主要原因是云盘服务的压力太大：</p><p></p><p>一方面，云服务器自身在初始化 Kubernetes 组件的时候，比如安装系统软件包、从对象存储上拉取 Kubernetes 的安装包再解压等动作，是有磁盘写入的，一个节点可能还好，当几百个节点同时处于这个阶段的时候，云盘服务的整体写入压力会大幅上升。</p><p></p><p>另一方面，在于容器镜像的拉取。在已经正常创建的节点上，用户的 Pending Pod 会调度上去，然后开始拉取镜像，由于这个客户的镜像很大，拉的耗时也很久，如果很多节点都处于这个阶段，那会有大量的写入操作，导致整个云盘服务的写入吞吐量被打到一个较高的位置，新的节点在初始化的时候，因为要争抢写带宽，所以写入速度就降低了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9b7d5ce610842e7597f02b75c0be7027.png" /></p><p></p><p>为了解决这个问题，我们的想法是对同时扩容的节点数量做一个限制。虽然社区的 CA 中并没有对同时扩容的节点数有什么限制，但任何系统都存在上限，通过对系统做合理的限制，不仅能提供稳定的服务，从全局上也有助于提升性能。</p><p></p><p>我们根据云盘的吞吐能力，估算了一个可被接受的同时扩容节点数，比如限制是 100，这样一来，用户看到的就是 100 一批 100 一批的扩容，节点都能扩容成功。虽然扩容的批次增加了，但扩容成功率也提高了，整体的云盘写入流量更加平滑，整体的扩容速度也比之前提升了很多。</p><p></p><p>问题二：容器镜像大，扩容速度慢</p><p></p><p>我们碰到的第二个问题，是极致的性能问题，我们先讲扩容的性能问题。在批处理场景下，客户使用的镜像会比较大，并且客户对扩容端到端速度要求会比较高，比如要求在 5min 内扩容出 500 个节点，并且 Pod 都能运行起来，这是一件非常有挑战的事情。</p><p>在客户视角下，他们计算任务的启动延迟，大概分为 5 个阶段：</p><p></p><p>第一阶段：下发任务，集群中出现因资源不足而导致 pending 的 Pod；第二阶段：CA 感知到这些&nbsp;Pending Pod，触发节点池扩容。这个阶段一般是秒级的，如果是使用了 GPU 的 Pod，由于 CA 自身的策略，会导致最多延迟 30s 再扩容。这里 CA 不立马扩容要等几秒，是因为如果最新的 Pending Pod，创建时间离现在比较近，很有可能还会有新的 Pending Pod 被创建出来。比如 deployment 的副本数从 0 改到 1000，可能就需要 10 多秒才能全部创建完，所以 CA 宁愿多等一会儿等所有 Pod 都被创建了才执行扩容；第三阶段：云厂商接收到扩容请求，去创建云服务器、注册到集群中。这个阶段是分钟级别的，不同云厂商的耗时可能会略有差别；第四阶段：把这些 Pending Pod 调度到节点上，如果 Pod 数量和集群规模不大，Pod 的调度条件不复杂，相对整个过程来说，这阶段的耗时可以忽略不计；第五阶段：节点上的 Pod 开始拉取镜像、启动。这个阶段的耗时是不太稳定的，比如同时扩容的节点数量比较多，容器镜像又比较大，就很有可能会打满云厂商的限速，对整个端到端的影响比较大。</p><p></p><p>比如在这张图里，在多个节点同时扩容时，除了用户的计算任务的 Pod，节点上还有很多系统 daemonset 的 Pod，比如网络组件、device plugin、日志采集组件等等，这些 Pod 的镜像也会大量的、同时的从镜像仓库拉取，很容易就达到网络瓶颈，或者给云盘服务带来写入压力。如果 500 个节点同时扩容，每个节点上都在争抢带宽或者磁盘的写入，是无法达到刚刚说的性能要求的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1ab394d6fb7af7b1f7b51a8eb9895e4e.png" /></p><p></p><p>在这种极致的性能要求下，我们采用了自定义系统镜像方案。这个自定义系统镜像是指云服务器的系统镜像，我们先在云服务器中把容器镜像预先拉取下来，然后把云服务器导出为自定义系统镜像，把业务的容器镜像固化到系统中去，这样在后续扩容的时候，我们用这个自定义系统镜像去创建云服务器，云服务器作为节点加入集群后，容器镜像就已经在节点上了，不需要再去镜像仓库拉取，Pod 可以做到秒级启动。</p><p></p><p>但这个方案也有一些弊端，比如我们可以把整个容器镜像固化到系统中后，后续容器镜像发生了变化，这个自定义系统镜像也需要重新制作，比较麻烦，如果容器镜像变化比较频繁，就要频繁的制作自定义系统镜像。所以我们也可以把镜像做一下拆分，把数据量比较大的、又不怎么更新的静态数据，打包到基础镜像中，然后把这个基础镜像再固化到系统中，这样节点在启动以后，拉取的数据量也会大大减小。</p><p></p><p>在使用这个方案前，如果客户扩容 500 节点，在单批次运行最多 70 个节点扩容的情况下，每个节点上 1 个 10GiB 的容器镜像，那从下发到 Pod 全部运行，大概需要 22min。</p><p></p><p>而如果使用自定义镜像，因为不需要拉取容器镜像，所以刚刚说的云盘服务的压力就减轻了，所以我们直接放开扩容数量的限制，直接从 0 到 500 做扩容，从 Pod 下发到最终 Running，可以在 5min 以内完成，并且云盘服务整体的写入流量，可以从峰值的 14GB/s 下降到 6GB/s，大幅减少数据写入。</p><p></p><p>这个方案对于需要快速扩容、对扩容时的端到端耗时非常敏感的业务，是一个可行的解决思路。</p><p></p><h2>问题三：多节点干扰，缩容速度慢</h2><p></p><p></p><p>客户因为计算任务的不同，会触发不同节点池的扩容。比如客户先进行&nbsp;GPU 计算任务，触发了节点池 A 的扩容（节点池 A 是 GPU 节点），在计算任务 A 快结束的时候，可能会下发新的计算任务，触发节点池 B 的扩容。</p><p></p><p>那按照客户的预期，这时节点池 A 的这些 GPU 节点，因为上面没有 GPU 计算任务、节点使用率已经降低，需要在任务结束的一段时间内很快就被缩容掉。但实际情况是节点池 A 的缩容会被推后较长的时间，这就造成了一些资源浪费。</p><p></p><p>所以为什么节点池 A 的缩容会被推迟呢？</p><p></p><p>CA 内部的缩容流程中，有一个冷却时间，表示扩容后多久时间内，是不能对节点做缩容的，这个值由用户来设定。这个计时是集群级别的，就是任何一个节点池扩容了，这个计时器都会被重置，重新计算。在大规模、多节点池扩容的情况下，如果用户分批扩容，那每次扩容都会做一次重置，导致扩容过程中，空闲的节点池无法被缩容，造成资源的空跑。</p><p></p><p>当前社区对此已经有解决方案，但代码还处于草稿阶段，具体的解决思路就是把计时器改成节点池级别，每个节点池只针对自己的扩容过程做倒计时，不受其他节点池干扰。</p><p></p><p><img src="https://static001.geekbang.org/infoq/83/83eb92b64349a7dbbc8ac5f8c732fc8d.png" /></p><p></p><p>我们在生产环境上对社区的方案做了验证，确实很好的解决了我们的问题，在计算任务结束后，节点池 A 就会很快被缩容。那这个缩容时间的缩短，非常显著地降低了客户的使用成本。</p><p></p><h2>问题四：Pending Pod 过多导致未扩容</h2><p></p><p></p><p>最后我们再来看一下由规模带来的问题。</p><p></p><p>如前文所述，客户用自己的任务分发平台将计算任务转换成 Pod 下发到 Kubernetes 集群，有时候并不能非常好地控制任务的下发速度。峰值时期，整个集群中有 2w 多个 Pod，其中&nbsp;Pending Pod&nbsp;的数量高达 1.8w 个。面对如此大的规模，CA 难免“力不从心”。</p><p></p><p>下图展示了集群中 Pod 的数量情况和 CA 的日志分布情况，可以发现在 Pod 数激增的那段时间里，CA 基本上没有输出日志，集群中的节点池也没有扩容，客户的计算任务被大量堆积、阻塞。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2d/2d975e99ab55cfe3844e9622b4e5052d.png" /></p><p></p><p>经过调查我们发现，CA 主要卡在调度预测阶段，在这一阶段，CA 会计算每个节点池需要扩容多少个节点才能满足这些&nbsp;Pending Pod&nbsp;的资源用量。为了复现这个问题，我们做了一些压测，期望能找到影响这个耗时的主要因素，方便针对客户的场景做一些优化。</p><p></p><p>一开始我们想到的就是 Pending Pod 的数量。为此我们使用两个不同规格的节点池，然后往集群中下发大量的&nbsp;Pending Pod，这些&nbsp;Pending Pod&nbsp;通过资源用量期望调度到其中一个节点池上。</p><p></p><p><img src="https://static001.geekbang.org/infoq/42/423f92f0da89e08ec47fa5967cb6efea.png" /></p><p></p><p>我们发现随着集群中 Pending Pod 数量的增长，单个节点池的整个计算耗时，是不断上升的，在 2.2w Pod 时，单个节点池的计算耗时会到 400s，而从 Pod 视角来看，单个 Pod 的平均耗时，也是线性增长的。虽然 Pending Pod 的数量规模达到了，但实际 CA 僵死的时间是远比我们测出来的 400s 多，所以我们继续接近客户的使用方式，将 Pod 的调度逻辑做了修改，从之前的默认调度约束，改为了使用节点亲和性。</p><p></p><p>我们发现在不使用节点亲和性的情况下，整体的耗时和第一次压测的是一样的，而如果使用了节点亲和性，在&nbsp;Pending Pod&nbsp;数量在 1.8w 的时候就达到了 700s。下方右侧这张图中蓝色的那条曲线也说明，单个 Pod 的平均计算时间，比之前不使用节点亲和性的场景增长得快，整条线上升的速度更快、斜率更高。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4b/4be635025e18911d4f0dd002c5a3b6ac.png" /></p><p></p><p>除此以外，我们继续控制变量，调整 Pod 的 request，将之前的单个节点上只跑 1 个 Pod，改为单个节点上能跑 8 个 Pod，这样修改后，预期添加到集群中的节点数量是之前的 1/8，同时整个计算耗时，相比之前的曲线，也是接近水平了。</p><p></p><p>从上面的 3 次压测中，我们可以得出一些结论：</p><p></p><p>Pending Pod 越多，需要计算的耗时越久，且平均每个&nbsp;Pending Pod&nbsp;的耗时随总数的增加而增加；使用了 Node Affinity 的&nbsp;Pending Pod，在做调度预测时，会耗时更久；预估节点数量越多，调度预测越久；可被调度的节点池数量越多，调度预测越久。</p><p></p><p>这是我们从压测中得到的实验结论，那真实的技术理解应该是怎样的呢？</p><p></p><p>CA 在估算节点池需要扩容多少个节点的时候，内部有一个快照，一开始这个快照包含了集群中的节点和节点上的 Pod。如果集群中有多个节点池，CA 会先对每个节点池做一下计算，看看哪些 Pending Pod 能调度到节点池上。因为如果节点池不能满足 Pod 的调度要求，即使扩容了也没有用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/29/2930b38dff9fca2b6eeb2b1b7ccd06e7.png" /></p><p></p><p>比如这张图里，集群中一共有 8 个&nbsp;Pending Pod，节点池 A 能满足所有&nbsp;Pending Pod&nbsp;的调度要求，节点池 B 只能调度 6 个。这个过程的复杂度是 O(n^2)，跟 Pod 的数量、节点池的数量有关，当然也跟 Pod 的调度条件有关系，调度条件越复杂，这个耗时也会更久一点。在做完这一步之后，CA 会再根据节点池和节点池上的这些&nbsp;Pending Pod，去计算需要扩容多少个节点</p><p></p><p>比如节点池 A 能满足 8 个&nbsp;Pending Pod&nbsp;的调度条件，CA 会先对这些&nbsp;Pending Pod&nbsp;和快照里的节点做一轮调度模拟，跑一下 scheduler framework 中的 prefilter 和 filter 阶段，看能不能正常通过，如果能通过就表示这些&nbsp;Pending Pod&nbsp;可以调度到快照的节点上，如果不能通过，就会根据节点池 A 的规格信息，构建出一个虚拟的 Node，放到快照里，然后再做刚刚的调度模拟，此时这些&nbsp;Pending Pod&nbsp;是可以调度到这个新加的虚拟的 Node 里的。</p><p></p><p>CA 重复这个过程，直到这里所有的&nbsp;Pending Pod&nbsp;都能加入到快照中，此时快照里新增了多少个虚拟的 Node，其实就是节点池 A 需要多扩容的节点数。只要集群里新增了这些节点，这些因资源不足而无法调度的&nbsp;Pending Pod&nbsp;就能真正的跑起来。</p><p></p><p>节点池 B 也是类似的，只不过在我们的例子中，节点池 B 的规模会比节点池 A 小。根据我们刚刚的分析，整个过程的复杂度是接近 O(n^3) 的，跟&nbsp;Pending Pod&nbsp;的数量、快照中的节点数量、节点池得到数量相关。</p><p></p><p>这也跟我们的压测结论是一样的：Pending Pod&nbsp;的数量越多、节点池越多、预估的节点数量越多、调度条件越复杂，整个扩容的耗时就越久。</p><p></p><p>对此，CA 社区主要提出了两个改进点：</p><p>限制节点数量的上限，就是减少快照中的节点数量，这个跟我们刚刚提到的观点是类似的，如果对扩容的节点数量不加限制，其实是不太稳妥的；对单个节点池整体的计算耗时做限制，比如不能超过 10s，如果这个过程超过了 10s，我们就截断这个过程。</p><p></p><p>如果你的 CA 版本还比较老，低于 v1.25 的，可能就没法使用社区的解法了。</p><p></p><h2>资源弹性建议</h2><p></p><p></p><p>如果业务对扩容的延迟比较敏感，期望能更快的让 Pod 启动，可以考虑将静态的、较大的容器镜像，打包进云服务器的系统镜像里，加速扩容。</p><p></p><p>推荐在业务侧就开始控制集群中的&nbsp;Pending Pod 的数量，数量过多不但会增大集群自身的压力，也会影响 CA 扩容的稳定性，将数量保持在一个稳定的水位，控制好扩容的节奏，会更好。</p><p></p><p>第三个是对于不需要弹性能力的节点池，关掉弹性伸缩功能，避免 CA 在这些节点池上消耗算力。</p><p></p><p>相关产品：www.volcengine.com/product/vke</p><p></p><p>视频回放：关注【字节跳动云原生】公众号，在后台回复“KubeCon CN 2023”</p><p></p><p>相关服务咨询：</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b3b1a8f60be8d5405bb4dc31095389f8.png" /></p><p>扫码咨询</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Xcu7VoHdktaHGrbvbEcu</id>
            <title>刚发布就被质疑？超过GPT-4的“最强”大模型Gemini、“最高效”训练加速器，谷歌到底行不行</title>
            <link>https://www.infoq.cn/article/Xcu7VoHdktaHGrbvbEcu</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Xcu7VoHdktaHGrbvbEcu</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 02:04:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 谷歌, AI模型, Gemini, 多模态提示
<br>
<br>
总结: 谷歌发布了功能强大的AI模型Gemini，该模型通过多模态提示实现对文本和图像的理解和反应。 </div>
                        <hr>
                    
                    <p>当地时间12 月 6 日，谷歌发布了自己“迄今为止功能最强、通用性最高”的AI模型Gemini。</p><p></p><p>谷歌及Alphabet&nbsp;CEO桑达尔·皮查伊 (Sundar&nbsp;Pichai)表示，首个Gemini 1.0针对不同规模进行优化，具体分为Ultra、Pro和Nano三个版本。“这是Gemini时代的首批模型，也是我们今年早些时候重组Google DeepMind时所表达愿景的首个实现。此模型代表着谷歌作为一家企业，在AI新时代下所做出的最重要的科学与工程努力之一。”</p><p></p><p>但刚发布不久，科技专栏作家Parmy Olson 指出，其中一个AI实时对人类的涂鸦和手势动作给出评论和吐槽的视频被曝出“不是实时或以语音方式进行的”。还有<a href="https://twitter.com/noguestein/status/1732927393466040617">网友吐槽</a>"整个互动过程“特别慢，跟演示视频完全不同。”</p><p></p><p>这个视频主要是演示“多模态提示”（multimodal prompting），即为大模型提供不同模式的组合（在本例中为图像和文本），并让其通过预测接下来会发生什么来做出反应。</p><p></p><p></p><p></p><p>对此，Google DeepMind 研究与深度学习主管副总裁 <a href="https://twitter.com/OriolVinyalsML/status/1732885990291775553">Oriol Vinyals</a>"表示，“视频中的所有用户提示和输出都是真实的，只是为简洁起见进行了缩短剪辑。”但网友对此并不买账，认为谷歌在玩营销手段，误导大家。</p><p></p><p>在谷歌发布的<a href="https://developers.googleblog.com/2023/12/how-its-made-gemini-multimodal-prompting.html?m=1">一篇文章</a>"里，详细介绍了效果实现经过，可以看出是使用静态图片和多段提示词拼凑训练。</p><p></p><p><img src="https://static001.geekbang.org/infoq/87/879c43b3919928ef014c63fd299e9cbb.png" /></p><p></p><p></p><h2>看看谷歌的测试</h2><p></p><p></p><p>Gemini 被称为谷歌迄今为止最灵活的模型，能够从数据中心到移动设备实现高效运行，帮助开发人员与企业客户显著增强在利用AI进行构建和扩展时的操作方式。谷歌针对三种不同体量优化了Gemini 1.0（首个正式模型版本），分别为：</p><p></p><p>Gemini Ultra&nbsp;— 最大、功能最强的模型，适用于高度复杂的任务。Gemini Pro&nbsp;— 可处理各种任务类型的最佳模型。Gemini Nano&nbsp;— 能够在多种设备上高效运行的任务处理模型。</p><p></p><p>值得注意是，本次尚未发布最强大的Gemini Ultra，距离正式发布还需要几个月的时间。目前Gemini Ultra正在进行全面的信任与安全检查，包括由受信的外部合作方进行红队审查，并在广泛应用前通过微调和基于人类反馈的强化学习（RLHF）对其做进一步完善。</p><p></p><p>Gemini Pro和Gemini Nano已分别集成到了聊天机器人Bard和智能手机Pixel 8 Pro上。此外，自12月13日开始，开发者和企业客户都可通过Google AI Studio或者Google Cloud Vertex AI中的Gemini API访问Gemini Pro模型。在未来几个月间，Gemini将逐步登陆谷歌更多产品及服务，包括搜索、广告、Chrome浏览器以及Duet AI等。</p><p></p><p>谷歌说得很厉害，那Gemini 1.0 的实力到底如何？</p><p></p><p><img src="https://static001.geekbang.org/infoq/96/9677686c03434f3f8237cd371682bc06.png" /></p><p>﻿</p><p>根据谷歌测试结果，从自然图像、音频和视频理解再到数学推理，在大语言模型（LLM）研发领域的32种常见学术基准测试中，Gemini Ultra的性能一举创下30项最佳新纪录。</p><p></p><p>在MMLU（大规模多任务语言理解）中Gemini Ultar的得分高达90.0%，成为首个超越人类专家的模型。这项测试结合了数学、物理、历史、法律、医学和伦理学等57个科目，旨在测试AI模型掌握知识和解决问题的能力。</p><p></p><p>Gemini在文本和编码等一系列基准测试中表现超过GPT-4：</p><p></p><p><img src="https://static001.geekbang.org/infoq/69/691456eef49288cbb54143ec862c3dc2.png" /></p><p></p><p>Gemini Ultra还在新的MMMU基准测试中取得了59.4%的最高得分。这项基准测试涵盖跨越不同领域、需要深思熟虑的一系列多模态推理任务。</p><p></p><p>根据谷歌测得的图像基准，Gemini Ultra的性能优于以往最先进的模型，且无需借助从图像中提取文本以供进一步处理的对象字符识别（OCR）系统的辅助。谷歌表示，这些测试结果凸显出Gemini的天然多模态优势，也证明Gemini已经表现出具备复杂推理能力的早期特征。</p><p></p><p>Gemini在一系列多模态基准测试中均创下性能新纪录，全面超越GPT-4V：</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/eabc1941f73a277dcac4574c7de7d681.png" /></p><p></p><h2>多模态推理能力</h2><p></p><p></p><p>到目前为止，创建多模态模型的标准方法主要是针对不同模态训练单独的组件，再将其组合起来以粗略模仿相应能力。由此实现的模型虽然比较擅长执行某些特定任务，例如描述图像内容，但却难以处理概念性更强、复杂度更高的推理任务。</p><p></p><p>在Gemini的起始阶段就将其定位为原生多模态形式，针对不同模态开展预训练。之后，谷歌又使用额外的多模态数据对其进行微调，希望进一步完善其有效性。现在，Gemini可以同时识别和理解文本、图像、音频、视频和代码五种信息。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ab/abb5095053fbb457004d5057561f4555.png" /></p><p></p><h4>理解文本、图像、音频等各种素材</h4><p></p><p></p><p>Gemini 1.0拥有精妙的多模态推理能力，可以帮助理解复杂的书面与视觉信息，展现出了在大量数据中提取重要知识的独特能力。比如，Gemini 在阅读、过滤和理解信息的过程中，可以从数十万份文档中提取见解并进行分析。</p><p></p><p>Gemini 1.0在训练之后，能够同时识别并理解文本、图像、音频等各种素材，因此可以把握住更加微妙的信息，并回答与复杂主题相关的更多问题。这使得它特别擅长解释数学、物理等复杂学科的推理过程。</p><p></p><p>比如，Gemini 可以识别学生的手写物理题答案，并验证正确性：</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/09/0993347ceccee6452d2a0f3248905fd5.png" /></p><p></p><p>基于视觉线索进行推理：</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/91/9122b89b1f3140bc6e1a323e529acd5a.png" /></p><p></p><p>音频方面，可以看下Google DeepMind 研究科学家 Adrià Recasens Continente 演示 Gemini 能够理解来自多个扬声器的不同语言的音频，并结合视觉、音频和文本，在厨房做饭时提供帮助的场景：</p><p></p><p></p><p></p><h4>高级编码能力</h4><p></p><p></p><p>谷歌介绍，首个Gemini正式版能够理解、解释并生成基于目前各种流行编程语言（例如Python、Java、C++和GO）的高质量代码。其表现出的跨语言工作和复杂信息推理能力，也使得Gemini成为世界领先的编码基础模型之一。</p><p></p><p><img src="https://static001.geekbang.org/infoq/62/62fd21473ee722a5f3eb12414a0ad27d.png" /></p><p></p><p>Gemini&nbsp;&nbsp;的多模式推理功能生成用于重新排列子图的matplotlib代码</p><p></p><p></p><p>Gemini Ultra在多项编码基准测试中表现出色，包括HumanEval（用于评估编码任务性能的重要行业标准）和 Natural2Code（谷歌内部保留的数据集），此数据集使用作者专门创作的源素材、而非来自网络的信息。</p><p></p><p>Gemini还能作为更高级编码系统的引擎。谷歌两年之前发布了ALphaCode，这也是首个在编程竞赛中表现出一定竞争力的AI代码生态系统。使用Gemini的专用版本，谷歌推出更加先进的代码生成系统AlphaCode 2。除了编码场景之外，它还擅长解决涉及复杂数学和理论计算科学的更多编程难题。</p><p><img src="https://static001.geekbang.org/infoq/10/108a0fe125a7ab615f9e83a23e82c6e7.png" /></p><p></p><p>面对与初代AlphaCode相同的评估场景，AlphaCode 2表现出巨大的性能改进，其解决的问题数量几乎达到初版的两倍，谷歌估计其成绩优于85%的竞赛参与者，而AlphaCode成功解决问题的比例只接近50%。因此当程序员通过代码示例来定义某些属性，并借此向AlphaCode 2寻求帮助时，其表现会更好。</p><p></p><p></p><h2>“专为训练顶尖AI模型而生”的TPU系统</h2><p></p><p></p><p>在介绍自家大模型的同时，谷歌顺势推出了了自己的AI训练基础设施。</p><p></p><p>谷歌使用内部设计的张量处理单元（TPU）v4和v5e在AI优化的基础设施之上，完成了Gemini 1.0的大规模训练任务。</p><p></p><p>在TPU上，Gemini的运行速度明显快于其他更早、更小且功能较差的模型。这些定制设计的AI加速器一直是谷歌AI产品的核心，负责为搜索、YouTube、Gmail、谷歌地图、Google Play和Android等服务的数十亿用户提供支持。它们也使得世界各地的其他企业也能经济高效地训练出自己的大规模AI模型。</p><p></p><p>如今，谷歌宣布推出迄今为止“最强大、最高效且可扩展”的TPU系统Cloud TPU v5p，专为训练顶尖AI模型而生。谷歌表示，作为下一代TPU，它将加速Gemini开发，帮助开发者和企业客户快速训练大规模生成式AI模型，将新产品和新功能更快交付至客户手中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ee/ee5fb38056fa5ac7026cfc835d0eb72a.png" /></p><p></p><p>谷歌数据中心内的Cloud TPU v5p AI加速器超级计算机</p><p></p><p>此外，在安全问题上，谷歌表示，Gemini拥有迄今为止所有谷歌AI模型当中最全面的安全评估机制，包括偏见与有毒内容检测。谷歌还对网络攻击、说服与自主判断等潜在风险领域开展了新颖研究，并应用谷歌研究院领先的对抗性测试技术抢在部署之前帮助发现Gemini中的重大安全隐患。</p><p></p><p>为了诊断Gemini训练阶段的内容安全问题，并确保其输出结果符合政策，谷歌使用诸如真实毒性提示词Real Toxicity Prompts在内的多种基准。这是一组从网络提取的、包含不同程度毒性内容的10万条提示词，由艾伦AI研究所的专家们提供。为了限制伤害，谷歌还构建了专门的安全分类器，用以识别、标记并整理涉及暴力或负面刻板印象的内容。</p><p></p><p>附 Sundar&nbsp;Pichai 公开信内容：</p><p>&nbsp;</p><p></p><blockquote>每一次技术变革都代表着推动科学发现、加速人类进步和改善生活品质的机遇。我相信我们现在所见证的AI转变，将成为我们一生当中最具深远意义的事件，甚至远远超越之前的移动或者Web革命。AI有望为全球各地的人们创造前所未有的日常生活体验和非凡的职业发展空间，将掀起新一波的创新与经济进步，并以前所未见的规模提升知识、学习、创造力与生产力。&nbsp;这也让我感到兴奋，期待通过AI技术为各国各地的每一个人提供帮助。&nbsp;作为一家AI优先的厂商，我们已经走过近八年历程，而前进的步伐只会不断加快：数百万用户正在我们的产品中运用生成式AI完成一年之前还难以想象的工作，包括为更加复杂的问题寻求答案、使用新工具协作与创新等等。与此同时，开发人员也在使用我们的模型与基础设施构建出新的生成式AI应用程序，世界各地的初创企业和组织正利用我们的AI工具不断拓展业务。&nbsp;这是一股令人难以置信的发展态势，而且我们才刚刚开始触及这无限可能性的最表层。我们正以大胆且负责任的态度开展这项工作。这意味着我们既需要追求雄心勃勃、能够为人类和全社会带来巨大收益的技术成果，同时也要建立保障措施并与政府和专家合作，应对AI发展过程中带来的种种风险。我们将继续投资打造更好的工具、基础模型和底层设施，并在我们AI原则的指导下将其引入自己的产品及其他方案当中。</blockquote><p></p><p></p><p></p><p>相关链接：</p><p><a href="https://blog.google/technology/ai/google-gemini-ai/#availability">https://blog.google/technology/ai/google-gemini-ai/#availability</a>"</p><p><a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf">https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ZRC4RXO1SXy7HfqBzFCg</id>
            <title>KubeCon | 使用 KubeRay 和 Kueue 在 Kubernetes 中托管 Ray 工作负载</title>
            <link>https://www.infoq.cn/article/ZRC4RXO1SXy7HfqBzFCg</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ZRC4RXO1SXy7HfqBzFCg</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 09:51:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: KubeCon CN 2023, Open AI + 数据 | Open AI + Data, KubeRay, Kueue
<br>
<br>
总结: 本文介绍了在 KubeCon CN 2023 上的专题演讲，演讲内容包括使用 KubeRay 和 Kueue 在 Kubernetes 中托管 Ray 工作负载的方法。文章还介绍了 Ray 的概念和特点，以及字节跳动在 KubeRay+Ray 应用实践中的经验。 </div>
                        <hr>
                    
                    <p>在 KubeCon CN 2023 的「 &nbsp;Open AI + 数据 | Open AI + Data」专题中，火山引擎软件工程师胡元哲分享了《使用 KubeRay 和 Kueue 在 Kubernetes 中托管 Ray 工作负载｜Sailing Ray workloads with KubeRay and Kueue in Kubernetes》议题。以下是本次演讲的文字稿。</p><p></p><p>本文将从 Ray 为何得到 AI 研究者们的青睐，在字节如何使用<a href="https://xie.infoq.cn/article/072696d697dfb06be06ff536c?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search"> KubeRay</a>" 来托管 Ray 应用，Kueue 如何管理和调度 RayJob 三个方面进行介绍。</p><p></p><h2>什么是 Ray</h2><p></p><p></p><p><a href="https://www.infoq.cn/article/sF4obWPGiLbJrMaFpf42?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Ray </a>"起源于 UC Berkeley 的 RISElab 实验室，其定位是一个通用的分布式编程框架，能帮助用户将自己的程序快速分布式化。Ray Core 提供了 low level 的分布式语法，如 <a href="https://xie.infoq.cn/article/78669fdfcde61be0aa3560481?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">remote func</a>"、remote class，上层 Ray AIR 提供了 AI 场景的相关库。</p><p></p><p><img src="https://static001.geekbang.org/infoq/34/34787337399ecc1e765504db1e864553.png" /></p><p></p><p>Ray 的GitHub repo 如今已有 27K star，其发起者也成立了 Anyscale 公司来管理开源社区以及商业化。在 Anyscale 刚举办的 Ray Summit 2023 上，相关数据显示 Ray 已被 OpenAI/Uber/Amazon/字节跳动/蚂蚁金服等众多企业所使用。基于 Ray，Anyscale 也推出了自己的 LLM 相关商业化产品，并以成本和易用性等方向作为卖点。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b8/b8361e5f4380ff6e8d420122607e7ab0.png" /></p><p></p><p>上图右侧展示了 Ray cluster 的基本架构：</p><p></p><p>每个框是一个 Ray 的节点，节点是虚拟的概念，比如在 K8s 集群上，每个节点就对应一个 pod。所有的节点中，有一个节点的角色不同，就是最左边的 head 节点，它可以理解成整个 Ray cluster 的调度中心，head 节点上有 GCS 存储集群节点的信息、作业信息、actor 的信息等等，head 节点上还有 dashboard 等组件。除了 head 节点以外的都是 worker 节点，worker 节点主要是承载具体的工作负载。每个节点上有一个 raylet 守护进程，raylet 也是一个本地调度器，负责 task 的调度以及 worker 的管理，同时 raylet 中还有 object store 组件，负责节点之间 object 的传输，整个 Ray cluster 中的所有 object store 构成一个大的分布式内存。</p><p></p><p>为了提供简洁的分布式编程体验，Ray Core 内部做了非常多工作，比如 actor 调度和 object 的生命周期管理等，上图左侧展示了如何使用 Ray Core 编写一个简单的分布式程序，square 函数和 Counter 类通过 Ray 的语法糖，变成了一些在远程运行的对象，其计算过程会被异步调用并存储在 object store 中，最后通过 ray.get 来获取到本地。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7c/7c877c94ec9bb1dadd4d6efccbc19d6c.png" /></p><p></p><p>除了 Ray Core 提供的底层分布式能力，其上层 Ray AI Runtime（Ray AIR）针对算法场景也实现了一系列工具：</p><p></p><p>ray.data&nbsp;集合了数据读写、流式处理、shuffle 等功能，给离线推理、数据预处理等场景提供了灵活 API 和异构的调度功能ray.train&nbsp;和&nbsp;ray.tune&nbsp;可以将 xgboost、pytorch 等训练代码快速改写成基于 Ray 的分布式训练应用ray.serve&nbsp;是一套在线服务的部署调用框架，支持复杂模型编排，可以灵活扩缩实例</p><p></p><p>可以说，Ray 的生态打破了过去 AI 工程中每个模块都是固定范式的传统——</p><p></p><p>在过去，提到数据处理，大家会想到 Spark；提到训练，会想到 Torch DDP、MPI；提到推理，会想到 deployment、service；而 Ray 能够给予你足够的自由度和想象力，可以将 AI 的 pipeline 糅合在一个框架甚至一串代码中实现，其强大异构调度能力以及友好的上手调试感受。这也是很多 AI 从业者越来越多地选择 Ray 的原因。</p><p></p><h2>字节跳动 KubeRay+Ray 应用实践</h2><p></p><p></p><p>KubeRay 简介</p><p></p><p>KubeRay 是由字节跳动技术团队牵头，由 AnyScale、蚂蚁金服、微软等公司共同参与建设的开源 Ray 部署集成工具集，目前已成为在 Kubernetes 集群上部署 Ray 应用的事实标准。</p><p></p><p>如果不使用 KubeRay，直接在物理机来托管 Ray 集群会有什么问题呢？</p><p></p><p><img src="https://static001.geekbang.org/infoq/b7/b707e841aefee595160b801434c14aea.png" /></p><p></p><p>首先，head 和 worker 需要直接通过 ip 和 port 连接，集群的拉起、节点的增删会比较复杂，可恢复能力也较弱。其次，RayJob submit 脚本提交作业的模式在大规模生产环境下很难管理，除此之外，也没有 K8s 生态可以给予你的监控、报警、Ingress、HPA/VPA 等能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/07/07d270af9180692caf6729bcd1caa8aa.png" /></p><p></p><p>KubeRay 采用了经典的 operator 设计，提供了 RayCluster，RayJob，RayService 这三个 CRD：</p><p></p><p>RayCluster：负责 Ray 集群的搭建RayJob：负责提交作业到一个伴生集群中，并同步状态RaySevice：负责将 RayServe 应用快速部署到云原生环境中</p><p></p><p>在 operator 实现中，cluster 的 controller 更侧重集群的拉起、恢复、与 Ray autoscaler 配合等，Job Service 的 controller 侧重作业提交和状态更新，并且它俩分别对应了离线和在线两个典型场景。</p><p></p><p>除此之外 KubeRay 还提供了 APIServer 等 client 库来负责 CRD 的增删改差，方便对接上层平台。</p><p></p><p>RayCluster</p><p></p><p><img src="https://static001.geekbang.org/infoq/5f/5f489b9dd05d34d3284b6b480ae02d1f.png" /></p><p></p><p>如果说 Ray 本身提供了 actor 重启、task 重试等能力来增强代码的高可用性，那么 KubeRay 就是真正让 Ray 在集群维度成为真正高可用的应用。</p><p></p><p>首先 RayCluster CRD 提供了 pod 的恢复能力以及集群粒度的热更新，可以非常方便地管理集群；其次 head 和 worker 通过 service 进行连接，通过将集群 metadata 挂到远程存储中，配合 service 可以做到无感知的 head 节点恢复，同时 Ray autoscaler 可以实现基于集群负载动态伸缩集群规模，有效缩减成本。当然 Cluster CRD 还提供了 metric， 集群状态等的透出。</p><p></p><p>RayJob</p><p></p><p><img src="https://static001.geekbang.org/infoq/de/dec060fcf86cb2faec661053c31241a6.png" /></p><p></p><p>RayJob 是生产环境管理 Ray 作业的解决方案，支持批式调度器，创建伴生 Ray 集群或者选择已有的 Ray 集群，提交作业，并更新作业状态，最后删除 Ray 集群。在字节跳动，我们优化了作业状态机转移，增加了超时、等待节点数等功能。</p><p></p><p>RayService</p><p></p><p><img src="https://static001.geekbang.org/infoq/b5/b5e3d0c66271b55e0252b61191f1342c.png" /></p><p></p><p>RayService 把 CRD 中的 serve 配置部署到集群上，并通过 service 把 serve agent 的端口透出，实现了 Ray serve 的云原生化。它支持热更新 Serve 配置，通过 pending cluster 的滚动更新实现 Serve 无感知迁移。</p><p></p><p>Ray 在字节跳动的托管</p><p></p><p><img src="https://static001.geekbang.org/infoq/cd/cdb059d61461e2dc7aa3068cec72bbaf.png" /></p><p></p><p>在字节跳动，我们给用户提供了丰富的 Ray 相关生态。首先站内所有的 Ray 集群都由 KubeRay 去管理，我们基于开源版本做了相关适配和增强来支持大规模作业提交以及一些额外特性；我们在平台层支持用户创建常驻 Ray 集群用来调试作业，也支持 single-job 形式让平台托管创建 RayJob；除此之外还提供了平台鉴权、historyserver、notebook 等周围的能力。</p><p></p><p>如今字节跳动内部的相关业务包含了图计算、离线推理、大模型、并行计算等方向，涵盖了离线、在线等场景。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c4/c4f0a634454c047746f4c8fbea89197d.png" /></p><p></p><p>上图展示了站内某业务在使用常驻集群的场景，其需求是希望尽量利用不同 K8s 集群上的低优 spot 资源提供给用户用于运行、调试作业，同时希望大多数作业感知不到外界资源的抖动。</p><p></p><p>我们的方案是在每个 K8s 集群中创建一个大资源量的低优 pod 组成的 Ray 集群，operator 层面会基于每天 quota 的规律性浮动，并配合 Ray autoscaler 主动调整集群规模，尽量减少被 K8s 去主动驱逐 pod 的情况。</p><p></p><p>同时在上层，用户的脚本会感知每个大集群的剩余资源量决定分发到哪个集群去执行。每个集群内部我们实现了一个简单的排队功能，收到作业请求后先将作业放入 dashboard 内部的队列中，通过 placement group 来实现资源 gang 调度，确保作业需要的 GPU、CPU 资源到位后才开始真正运行作业。</p><p></p><p><img src="https://static001.geekbang.org/infoq/99/99d3a607fe41a081d42ddc9bc3ca8b75.png" /></p><p></p><p>在用户结束调试之后，可以通过平台来托管创建 RayJob CR，KubeRay 会负责集群拉起、作业提交、结束销毁。作业运行过程中，Ray 集群的重要信息会以 event 的形式 dump 到远程存储，我们仿照了 spark history server 的设计思路，用户在作业运行结束之后可以通过 Ray UI 界面来直接查看历史的作业的日志、metric 等信息。</p><p></p><p>场景案例</p><p></p><p>场景一：图计算</p><p></p><p><img src="https://static001.geekbang.org/infoq/68/68e6db08dca1ee104e8aa7c8d4a1798f.png" /></p><p></p><p>在图计算场景，我们使用 Ray Core 来改造字节跳动内部的图计算引擎，每个图算子通过 Ray Actor 拉起，这些算子会基于初始化的 rank 利用 MPI 进行通信。通过 Ray 的分布式能力和 KubeRay 的编排能力，可以实现端到端的容错，如果 worker 挂掉，可以再次被拉起，从 Pmem 或者 SSD 存储中恢复 checkpoint 信息。</p><p></p><p>场景二：大规模离线推理</p><p></p><p><img src="https://static001.geekbang.org/infoq/5a/5af5c2f988d1af61bb004c75c5299d6c.png" /></p><p></p><p>如图所示，上述作业同时包含数据读取处理和模型推理，同时需要消耗大量计算资源做分布式计算。相比在线推理，离线推理对延迟要求不高，但是对吞吐和资源利用率要求很高。我们使用 Ray dataset 的流式推理能力来处理这个场景，是因为相比 Spark，Ray 的编程更加灵活，同时将处理和推理放在异构 actor 并 pipeline，可以做流水线并行、模型并行等操作。我们还增加了 actor pool 扩缩、端到端容错的一些优化。</p><p></p><p>这些场景都已在 Anyscale 发表过博客，有兴趣可以查看：</p><p></p><p>www.anyscale.com/blog/how-bytedance-scales-offline-inference-with-multi-modal-llms-to-200TB-datawww.anyscale.com/blog/7-must-attend-ray-summit-sessions-rl-powered-traffic-control-infra-less-ml</p><p></p><h2>Kueue 如何管理/调度&nbsp;RayJob</h2><p></p><p></p><p>随着作业规模的增大，如何有限资源下调度不同优先级的作业，让大家都能稳定有序去使用 GPU 等资源是一个非常重要的问题。除了字节跳动所给出的一些经验，在开源社区侧，另一位分享人殷纳（Kante Yin）也介绍了如何使用 Kueue 这样一个作业调度器去管理 RayJob 来解决这些问题。</p><p></p><p>作业管理和调度框架 Kueue</p><p></p><p><img src="https://static001.geekbang.org/infoq/78/78f30abfc2cfef60d89e0a714a3742fe.png" /></p><p></p><p>Kueue&nbsp;是去年由 K8s 社区发起的作业管理和调度框架，提供作业层面的队列调度，支持入队优先级、抢占、资源配额等能力。相比其它拥有队列调度能力的开源组件，Kueue 从设计上希望更多复用 K8s 原生的调度能力，尽量不重复造轮子。Kueue 已经原生支持了 BatchJob、RayJob、TFJob 等作业类型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a2cfea87eb444d3a942c37f76120bea1.png" /></p><p></p><p>从 Kueue 的架构来看，ResourceFalvor 提供了节点的抽象，它通过 nodeLabel 的方式与具体的 node 进行绑定。ClusterQueue 是资源池的抽象，定义这个集群总资源量，ClusterQueue 中存在多个 localQueue，它们之间的资源会共享。一个作业会被提交到一个具体的 localQueue 进行调度。不同 clusterQueue 可以通过 Cohort 的机制共享资源。</p><p></p><p><img src="https://static001.geekbang.org/infoq/30/3083964610f312f72792d6c9a7e92d0c.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/26/26938b4822e135e3a76f57e9fb319e40.png" /></p><p></p><p>对于管理员，需要创建 ResouceFlavor、ClusterQueue、LocalQueue 来定义资源和机器之间的划分关系，以及资源池中的 quota 分配。</p><p></p><p><img src="https://static001.geekbang.org/infoq/54/545c2ff3bbb9eb8113d93ba15cf3f6f8.png" /></p><p></p><p>用户提交作业，需要指定自己作业所属于的 localQueue，job 在进入 Kueue 中会进入一个挂起状态，排队过程基于 quota，优先级等信息满足需求后放行，如果总当前资源不够，也有可能触发集群规模的 autoscale 机制。</p><p></p><p><img src="https://static001.geekbang.org/infoq/77/77b63af20bc07d061ccc12973b23e8b8.png" /></p><p></p><p>KubeCon 活动现场还展示了相关 Demo：两个优先级不同的 queue 中，随着优先级和 quota 的变化，来触发多个 RayJob 的抢占和恢复流程。</p><p></p><p>视频回放：关注【字节跳动云原生】公众号，在后台回复“KubeCon CN 2023”</p><p></p><p>相关服务咨询：</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b3b1a8f60be8d5405bb4dc31095389f8.png" /></p><p>扫码咨询</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/wCkOH34JXviAGmTEJwvO</id>
            <title>降本增效的秘密：抖音集团如何实践潮汐混部</title>
            <link>https://www.infoq.cn/article/wCkOH34JXviAGmTEJwvO</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/wCkOH34JXviAGmTEJwvO</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 09:34:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 抖音集团, 在线业务, 离线业务, 弹性伸缩
<br>
<br>
总结: 抖音集团的业务类型具备多元化的特点，根据业务对实时性要求的区别，可以将这些业务划分为在线业务和离线业务两个业务体系。在线业务通常服务于终端用户，对实时性要求高；离线业务包含临时查询、定时报表、模型训练、数据分析等作业，对实时性要求较低。为了提高资源利用率，抖音集团采用弹性伸缩的方式，在低谷时回收在线业务的资源，并将其分配给离线业务使用。这样可以在保证服务稳定性的前提下，充分利用资源。 </div>
                        <hr>
                    
                    <p><a href="https://xie.infoq.cn/article/569ed60e7361db04888f00e6a?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">抖音集团</a>"的业务类型具备多元化的特点，根据业务对实时性要求的区别，我们可以将这些业务划分为在线业务和离线业务两个业务体系，其中：</p><p></p><p>在线业务体系通常服务于终端用户，包含 <a href="https://mp.weixin.qq.com/s?__biz=MzI5ODQ2MzI3NQ%3D%3D&amp;chksm=eca43495dbd3bd83c5cc13f7668b32deea1cf70190cb850f949140d37276ce718c6bdbd2728b&amp;idx=1&amp;mid=2247486929&amp;scene=27&amp;sn=7ce020046ec24fe3eb991bc689523021&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">Web 服务</a>"，算法服务，有状态服务，视频编解码、FaaS 服务等，这些服务通常对 RPC 调用延迟比较敏感，对实时性要求高。离线业务体系包含临时查询、定时报表、模型训练、数据分析等作业，这些服务的特点是它们可以承受一定程度的排队或等待，在合理时间得到合理结果即可。</p><p></p><p>对于大部分的在线服务来说，业务的访问量具备明显波峰波谷的<a href="https://www.infoq.cn/article/experiments-data-innovation?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">潮汐变化</a>"。以抖音为例，绝大部分用户会在晚高峰时段使用抖音，这样就会导致抖音相关服务的整体流量都上涨到一个比较高的水平。而到了凌晨，因为用户使用抖音的次数和频率下降，该时段业务访问的流量会出现比较明显的波谷。</p><p></p><p>在线服务访问量的变化也导致了这些服务资源使用量的变化。下图展示了抖音集团内部在线业务的天级 CPU 使用情况。</p><p></p><p><img src="https://static001.geekbang.org/infoq/84/843e8e62e105c50bf1b965ab984a5f89.png" /></p><p>在线业务的天级 CPU 利用率</p><p></p><p>可以看到，在线业务凌晨时段的 CPU 使用量只有晚高峰时段的 20% - 30%。这种潮汐现象对头部服务而言会更加明显，它们波峰波谷间利用率的差距可能高达到 40%。但是为了保证在线业务在高峰时段的稳定性，在业务部署时，业务方必须按照峰时的流量来预估申请资源，这部分资源在谷时就会被闲置下来，造成严重的资源浪费。</p><p></p><p>在抖音集团的在线业务体系中，无状态服务如 Web 类微服务有着很高的资源保有量。而 Kubernetes 原生就提供了 HPA 的概念，可以根据 workload 的实际资源使用情况来扩缩无状态服务的实例数。如果我们可以通过弹性伸缩，在业务处于低谷时，通过回收业务副本数的方式来回收这部分资源，然后在业务处于峰值时，重新恢复业务的副本数，那么我们就能够在保证服务的 SLA 几乎不受影响的前提下，回收大量在线低谷时段的弹性资源。</p><p></p><p>在通过弹性伸缩回收了在线低谷时的资源后，下一步需要做的事情就是将这部分弹性资源进行二次的分配和利用，否则集群整体的利用率并没有因为服务弹性伸缩而得到本质的提升。但是在凌晨时段，几乎所有在线业务都会面临使用频次减少的情况，所以无法通过扩容在线的方式来利用这部分资源。而抖音集团也有很多离线的任务同样需要资源进行调度，例如视频转码和模型训练等，这些任务对资源的需求相对来说没有特定的时间约束，所以天然能够利用闲置资源。在这样的背景下，我们就开启了通过弹性伸缩来实现在离线业务的混部，即分时弹性混部。</p><p></p><h4>弹性伸缩</h4><p></p><p></p><p>大部分的在线服务都属于无状态服务，可以直接通过水平扩展来增加副本数。抖音集团内部并没有使用原生的 Deployment 描述在线的无状态服务，也没有使用社区原生的 HPA 体系，而是在上面构建了一层 &nbsp;HPAGroup 用于控制多个 Deployment 支持小流量或者 AB 发布，同时也方便我们在原生能力上针对某些场景做能力增强。整体架构如下图所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/37/377abe2258921b6bc3fbb47bbbb74df5.png" /></p><p></p><p>从图中可以看到，Agent 负责采集业务各种数据，包括业务指标如 QPS 、P99 延迟等，以及系统维度指标如 &nbsp;Load、CPU 利用率等。这些数据最终会由两个接收方进行消费，一方面它会通过中心式采集的组件进入到实时数据的存储系统，另一方面它会通过一个消息队列进入离线算法模型中。</p><p></p><p>中心式的 Controller 负责消费这两种数据，并在这些数据的基础上决定当前的扩缩容行为。因此扩缩容行为是由 Controller 调整 HPAGroup 的 replica 数，最终进入到 K8s 调度体系中产生 Pod，完成最终的调度。</p><p></p><p>在弹性伸缩的流程中最重要的就是实时性和稳定性，对于在线业务来说，我们需要保证它们被缩容的实例在流量上涨之后能够被迅速地扩容，同时需要保证有足够的资源可以让在线业务完成扩容，否则就有可能造成单实例流量过高影响服务的 SLA，严重时可能会导致服务不可用，甚至影响到整个请求调用链路。所以我们需要底层系统的配合来提供一整套的机制进行保证，主要包括几个方面：</p><p></p><p>监控体系：需要一套集群维度的监控体系，为弹性伸缩提供稳定实时的利用率数据。Quota 体系：需要一套 Quota 系统保证业务在伸缩的过程中，集群整体的资源量是可控的，不能出现在波谷时将服务的副本数缩容后，它所对应的 Quota 被别的服务占用且无法归还的情况。</p><p></p><h4>监控体系</h4><p></p><p></p><p>从上文中描述的弹性伸缩过程可以看出，控制面强依赖于从监控系统中获取服务的实时资源利用率情况，需要尽可能避免利用率采集出错或者延迟太高，导致服务在需要扩容时扩不上去的问题。抖音集团在实际生产中没有采用 K8s&nbsp;原生的 Metrics Server，主要是基于以下的考虑——</p><p></p><p>首先， Metrics Server 只能代理实时数据，不存储历史数据。如果希望在弹性伸缩中根据历史数据做一些更平滑的策略，基于原生 Metrics Server 无法很好的实现。其次，由于抖音集团的弹性伸缩能力可以基于多集群的联邦，所以需要在联邦层上得到服务资源使用情况的汇聚数据。最后，不是只有弹性伸缩依赖监控系统，业务也需要实时通过内部的监控管理系统查询业务的实时数据和历史数据，所以监控系统内还需要将数据同步到内部的离线分析系统。</p><p></p><p>为了更好地支持这些特殊逻辑，研发团队借鉴 Metrics Server 实现了一套监控系统，主要包括以下的组件，其中 Metrics Agent 负责提供单机上 Pod 聚合数据，并以标准的 Prometheus 格式对外暴露数据；Collector 通过轮询收集每台机器上的数据，然后写入到 Metrics Store 中；Metrics Store 是一个基于内存的数据库。</p><p></p><p><img src="https://static001.geekbang.org/infoq/dd/dd7e8584dbff35438ca853300eae84f6.png" /></p><p></p><p>另外在提高整个数据采集链路的高可用和高性能方面，我们也做了一定的优化，例如对所有组件都采用多实例 stand by 的方式部署。Collector 支持根据 Node 做分片采集，Metrics Store 支持根据 Metrics 分片存储；Custom Metrics APIServer 从 Store 中读写数据时采用 quorum 机制保证数据的准确性；Store 自身支持故障恢复和数据同步；每个 Store 实例都在内存中以 podName 为 key 构建 B 树以提高查询效率，并对数据内容进行压缩来降低存储压力。同时 Store 还支持一些控制面计算逻辑的下发，例如直接返回服务的平均利用率等信息，由于抖音集团的部分服务可能规模非常庞大，单 deployment 副本数可以达到 5000+，所以计算逻辑放在 metrics 系统中实现能够大大降级数据传输的量和延迟。目前这套监控系统单次查询延迟在 30ms 左右，单次采集延迟在 60s 左右，基本能够满足弹性伸缩的需求。</p><p></p><p>Quota 体系</p><p></p><p>Kubernetes 原生提供了简单的 Quota 实现，但是由于抖音集团内部的服务有特定的组织形式，组织内部存在着比较复杂的嵌套关系，套用原生的 Quota 系统会非常难以维护，同时也无法对计算类服务所需求的定制化资源进行更好地支持。</p><p></p><p>我们从零开始构建了自己的 Quota 计量体系，具体来说，服务大致会按所属的业务划分到不同组中，我们使用 CRD 对象来记录各个组中所有服务的总体资源可用量和使用量的信息，然后通过旁路的 Controller 不断轮询更新对象的内容。当业务方对服务副本数进行修改时，APIServer 的请求处理链中会通过 validation webhook 对该服务的资源进行校验和准入控制。尤其需要说明的是，当服务开启弹性伸缩后，Quota 系统将通过扩容的实例数上限进行资源的预留，从而保证资源弹性伸缩过程中资源量可控。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cd/cd3c8ac3952d63631b1cbc6a24e2c640.png" /></p><p>伸缩策略</p><p></p><p>目前我们支持根据 CPU、内存、GPU 等多个资源维度进行弹性伸缩，另外我们还补充了一些新的特性，这里有两个重点特性值得一提：</p><p></p><p>支持根据时间段设置不同的配置、支持设置服务级别的对利用率小幅波动的容忍度、支持单步扩缩容的步长。关于这个配置的背景主要是因为一些算法相关的服务在启动和退出时需要进行数据的同步操作，如果单次扩缩容实例数较多，可能会对底层的存储组件造成较大的瞬时压力，所以需要将一次较大的扩缩容行为拆分为多次较小扩缩容行为来做一个缓冲，使得服务副本数的变化更加平滑。使用每个服务小时级别的历史数据作为保底的策略，以应对监控系统异常的情况。这里我们还是利用了服务天级的利用率比较稳定的特性，在监控系统出现问题导致无法获取监控数据时，控制面可以使用该服务昨天相同时段的利用率数据来作为指导扩缩容的兜底策略。</p><p></p><h4>分时弹性混部实践</h4><p></p><p></p><p>系统架构资源出让与回收</p><p></p><p>在开启了大规模弹性伸缩后，在线业务就具备了生产和使用弹性资源的能力。正如背景介绍中提到的，为了能够充分地使用弹性资源，我们采取的方案是和离线作业进行混合部署。</p><p></p><p>最开始开展混部项目的时候，我们的底层隔离能力还并不十分完善，如果将在线和离线业务同时摆放在一台节点上运行，容易出现在离线业务之间的互相影响，导致在线业务的 SLA 受损。</p><p></p><p>所以我们早期采取的是 “0/1” 的方式进行混部。具体来说，就是把在线业务波谷时产生的弹性资源折合成同等资源规模的整机出让给离线业务使用，使得同一时段不会同时有在线业务和离线业务运行在同一台机器上；当在线波峰来临时对弹性资源进行回收。</p><p></p><p>为了实现这个逻辑，我们引入了集群部署水位的概念（见下图）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ac/ac8d08a1ee81117d852706f786cb4bd4.png" /></p><p></p><p>初始情况下，在线服务没有进行弹性伸缩，集群中整体的资源部署处于满载状态。当在线服务的波谷来临后，几乎所有服务都会因为弹性缩容而导致副本数降低。从整体上看，集群里节点上的 Pod 会变得非常稀疏，集群整体资源部署水位也随之降低。</p><p></p><p>当集群的部署水位低于设置的阈值后，控制面会通过一定规则选取部分在线节点，将该节点上的 Pod 驱逐到别的节点上，并标记该节点不可调度，最后将离线服务调度到该节点上实现资源的出借。</p><p></p><p>同理，当在线服务的波峰来临后，会发生一个逆向的控制过程，控制面在通知并确保离线任务撤离后，重新将节点设置为在线可调度状态，实现资源的回收。在实际操作中，集群的水位阈值通常会定义在 90% 的水平上，这就意味着集群在常态下会始终维持在一个接近满载的部署状态中，从而能够最大限度地提升资源利用率。</p><p></p><p>分时弹性混部的控制面最主要的职责就是控制节点的动态出让和回收，即节点的动态伸缩。可以用一个状态机来描述节点的转移状态，其中 Online 和 Offline 分别表示节点处于在线使用和离线使用的状态。而当节点被选择进行出让和回收时，会分别先进入到 OnlineToOffline 或 OfflineToOnline 的中间状态。在此状态下在离线都无法调度新的任务到节点上来，控制面会负责清理残留的在线的 Pod 和离线任务，并执行用户所配置的 hook 函数，只有当这些工作都处理完成后，才会真正完成节点的出让和回收逻辑。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b8/b859e14b474a0542760417cf003c3946.png" /></p><p></p><p>离线业务稳定性保证</p><p></p><p>弹性资源最大的特点是它整体的资源供应量不确定，当在线服务出现抖动时，我们需要优先保证在线服务的稳定性，极端情况下需要通过杀死离线业务来为在线服务腾挪资源。而离线任务通常运行的时间长，频繁杀死和重跑任务对离线业务来说也会造成较大的影响。因此如何在不稳定的资源供应基础上保证离线业务的稳定性也十分重要。</p><p></p><p>资源不稳定性主要来自以下两个方面：</p><p></p><p>弹性资源的供应量是不稳定的。弹性资源的供应量受制于在线业务的扩缩容情况，这会导致整体的资源的总量、资源供应的时间，甚至每一天资源所对应的具体的机器环境是不一样的。因此我们没有办法让离线业务针对弹性资源做一些提前的资源规划，同时当在线业务发生任何抖动时，我们会随时进行资源回收，这对整个训练作业的体验并不好。</p><p></p><p>弹性资源的需求量是不稳定的。离线作业存在 min/max 语义，例如在一个 PS-Worker 离线训练中，Worker 的数量其实是不确定的，离线业务整体的资源描述也并非确定值。同时我们还需要解决一个问题，即在提高单个作业的训练速度和满足更多训练作业之寻求平衡。</p><p></p><p>那么在抖音集团内部，研发团队如何解决上述问题？</p><p></p><p>在资源供应方面：我们在执行缩容操作的过程中，引入了 deletion cost 机制定义实例缩容的优先级。比如我们可以尽可能地缩容整机，甚至尽可能地保证这些缩容出来的资源处于同一个 Pod 或者使用了同质的 GPU ，从而减少资源碎片的问题。在资源分配方面：对于一些离线业务例如离线训练来说，因为作业在调度和非调度的过程中，可能会执行很多次 checkpoint dump 和 reload 操作，这个操作过程需要从 HDFS 上实现完整模型的上传和下载，非常耗时。如果我们运行更多的作业，虽然在一定程度上可以优化用户的体验，但是弹性资源在归还给在线业务时会触发多次无效的 checkpoint 操作占据大量时间，从而降低资源的利用效率。因此对于离线训练业务，我们更倾向于提高单个作业的加速比，而不是运行更多的作业。在资源回收方面：为了解决资源回收的过程中无脑地杀死离线业务的问题，研发团队构建了弹性资源的优先级，基于优先级实现资源回收。目前的弹性资源大概分为三级，如下图所示。以 PS-Worker 架构的离线分布式训练为例，PS 作业可能会处于一个 High 的优先级；能够满足基本运行的 Min 的 Worker 处于中优的优先级；为了进行弹性加测的 Worker 处于 low 的优先级。这样做的好处是当我们在线进行资源回收时，我们可以定制一些调度器的抢占策略，使得在线服务总是倾向于去抢占低优先级的作业资源。除此之外，我们在分时弹性混部的控制面引入提前通知的机制，在提前通知的时间窗口内，不会再调度新的离线任务，同时尽可能保证那些已经被调度的任务顺利跑完，从而将任务杀死率维持在一个可接受的范围内。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bd794e7eacfe8cd9a320cd6634b1742c.png" /></p><p>Priority Mapping</p><p></p><p></p><h4>案例分析</h4><p></p><p></p><p>视频编解码使用弹性资源</p><p></p><p>场景一是视频编解码服务和在线服务进行并池。我们基于 Kubernetes 生态系统构建了一套适用于视频计算的解决方案，支持视频相关任务的调度、运行与结果回调全流程。简化的架构如下图所示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/ad/add2c5887b8453dfe735bca7fdbab260.png" /></p><p></p><p>其中：</p><p></p><p>我们定义了一种 GroupCRD 的自定义资源，用来管控一类具有完全相同的任务执行环境（镜像、资源规格、环境变量等）的 Pod。Task scheduler 负责从上游接收用户的函数任务，并将任务分配到正确的 Pod 上执行，并完成任务执行结果的回调。每个 pod 都是执行视频编解码任务的 executor，它从任务调度模块获取到需要执行的函数任务，完成任务执行之后发起任务执行的回调用户过程。Scaler 可以根据 pod 状态、任务状态、资源状态等信息调整 GroupCRD 中的 Pod 副本数，实现弹性伸缩</p><p></p><p>在这个场景中，视频编解码任务具备了两个特点：</p><p></p><p>任务之间相对独立，可以利用有弹性能力的业务架构来快速扩容。每个任务的处理时间短，一般在分钟级以内，任务的杀死成本低。</p><p></p><p>这也使得视频编解码任务可以更平滑地接入弹性资源。</p><p></p><p>Ring AllReduce 使用弹性资源</p><p></p><p>场景二是 NLP 和在线推理服务进行资源并池。通常来说， NLP 场景更适合使用 Ring AllReduce 的训练方案。我们可以在一个 GPU 的显存里完整地加载所有的模型参数，通过 Ring AllReduce 更加合理地使用整体带宽，从而达到较高的加速比。</p><p></p><p><img src="https://static001.geekbang.org/infoq/76/766fa5441e032da6ff07ad61d501ba37.png" /></p><p></p><p>框架说明：在具体实现中，研发团队基于社区的 horovod 和 et-operator 实现了 Ring AllReduce 框架弹性。从上图可以看到，框架引入了一个中心式 Launcher 负责 Worker 之间通讯环建立、Worker 健康状态检查、异常处理等逻辑。</p><p></p><p>该场景中的弹性思路是：将 Launcher 和满足基本训练需要的 Worker 运行在稳定资源上，同时将用于加速的弹性 Worker 运行在弹性资源上。在线推理模型通常来说比较大，一个推理模型的实例可能会占据一个整机，因此这种做法的好处是：缩容一个在线实例等于缩容一台机器，从而供给完整的机器给 Ring AllReduce 的 Worker 运行，规避单机硬件资源的隔离问题。</p><p></p><p>目前整套框架的弹性加速比可以达到 1:8&nbsp;水平，达到了对弹性资源充分利用的效果。</p><p></p><p>PS-Worker 使用弹性资源</p><p></p><p>场景三是 PS-Worker 和推广搜核心服务共用 CPU 和 GPU 资源的情形。通常来说，CTV/CVR 的训练模型非常稀疏，而且特征维度非常庞大，所以单机基本没有办法装上所有的特征参数，因此这类训练模式基本上只能使用 PS-Worker 架构。抖音集团内部对 CTV/CVR 这种训练模型的需求十分大，为了更好地支持这种训练任务，研发团队自研了一套 PS-Worker 框架进行异步训练。</p><p></p><p>与传统 PS-Worker 不同的是，自研框架中的 Worker 被拆分为 Sparse 部分和 Dense 部分。其中 Dense 部分主要负责稠密模型的训练，它能在一个完整的 GPU 卡上加载所有模型参数，从而实现更好的加速效果；而 Sparse 和 PS 通常运行在廉价的 CPU 上。</p><p></p><p>同时， PS、Worker、在线三种服务会可能同时运行在一台机器上，共享部分单机资源，需要我们提供一些隔离机制减少互相干扰。例如，我们采用双网卡方案，在单机上进行分流，在交换机侧通过流量优先级打标的方式保证在线稳定性；在 NUMA 分配策略上通过微拓扑感知的能力，针对不同的角色定制 NUMA 分配逻辑，例如 PS 与 Worker 不共享 NUMA，Worker 可以共享 NUMA。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0f/0f51a46ac6f193ee2fda3c0df706d33a.png" /></p><p></p><p>另外，PS-Worker 的弹性粒度是作业整体维度，不可避免会造成比较大的资源碎片，为了解决该问题，我们引入了视频编解码或者 Spark 的一些 batch 类作业来填充资源碎片。</p><p></p><p>目前，这套框架在抖音集团内部得到了广泛使用，每天可以出让大概 300 万核心乘以 7 小时的资源。</p><p></p><h4>总结</h4><p></p><p></p><p>对于由在线业务潮汐现象而导致的资源浪费，我们可以通过分时弹性混部，即将缩容在线实例而获得的资源以弹性资源的形式，折合成整机出让给离线业务的方式来实现资源效能提升。分时弹性混部的优点在于对基础设施能力要求较低。缺点一方面在于出让的粒度为整机，容易形成资源碎片；另一方面在于在离线业务对于潮汐过程都有一定程度的感知。</p><p></p><p>总体来说，分时弹性混部比较适合基础设施能力建设尚处于早期的用户，在现有环境中快速上量，实现资源效能提升。</p><p></p><p>相关开源项目：github.com/kubewharf/katalyst-core</p><p>相关服务咨询：</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b3b1a8f60be8d5405bb4dc31095389f8.png" /></p><p>扫码咨询</p><p></p><p>点击<a href="https://github.com/kubewharf">此处链接</a>"，体验字节跳动同款云原生技术！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Z5kqO0J3Yq7bt3IiUsCE</id>
            <title>手把手教你在 pycharm 中安装 Amazon CodeWhisperer – AI 代码生成器</title>
            <link>https://www.infoq.cn/article/Z5kqO0J3Yq7bt3IiUsCE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Z5kqO0J3Yq7bt3IiUsCE</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 06:48:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 文字编辑工作者, 多语言, 内容创作经验, <>括起来的文本
<br>
<br>
总结: 作为一位多语言的文字编辑工作者，我有丰富的文字内容创作经验。对于<>括起来的文本，我可以提取出关键词，并使用中文进行概括，包含原文核心思想和概念。 </div>
                        <hr>
                    
                    <p></p><blockquote>文章作者：这世上无所不能的阳～</blockquote><p></p><p></p><p><a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">Amazon CodeWhisperer 官网</a>"：<a href="https://aws.amazon.com/codewhisperer/">https://aws.amazon.com/codewhisperer/</a>"在 pycharm 使用中如果想使用&nbsp;Amazon CodeWhisperer 首先点击 File 选择setting</p><p><img src="https://static001.infoq.cn/resource/image/60/8f/60c8f2c6410033c30c43671505e87a8f.png" /></p><p></p><p>找到 Plugins 在文本框中搜索“Amazon Toolkit”接着搜索结果中就会出现 Amazon Toolkit 插件，我们点击图示标注的“Install”即可安装该插件了。</p><p><img src="https://static001.infoq.cn/resource/image/f7/d2/f74471310c6b5900d0c1f56cac01abd2.png" /></p><p></p><p>安装之后 pycharm 会进行自动重启，重启后再左下角就会出现 Amazon Toolkit，点击 Amazon Toolkit 后再点击“Add Connection to Amazon”按钮登录。</p><p><img src="https://static001.infoq.cn/resource/image/00/30/002df75c26b3c546895f8cae4d7ff730.png" /></p><p></p><p>如果是初次使用默认选择“Use a personal email to sign up and sign in with Amazon Builder ID”，接着点击“Connect”</p><p><img src="https://static001.infoq.cn/resource/image/47/3b/47c634ce8fd3def7af1967b94c661f3b.png" /></p><p></p><p>点击后接着会弹出下图所示的界面，我们点击图示标注所示的“Open and Copy Code”；</p><p><img src="https://static001.infoq.cn/resource/image/13/e7/1308f11540baf5490c7c034e6eab75e7.png" /></p><p></p><p>接着会跳转到网页浏览器中打开登录界面，我们在图中所示的 code 一栏中粘贴上验证码，点击"next"按钮</p><p><img src="https://static001.infoq.cn/resource/image/13/1c/13fce4f858e69b13e3d5916d0ca7081c.png" /></p><p></p><p>没有账号的会创建账户，在图中标注的位置中填写邮箱和姓名后点击"next"按钮</p><p></p><p><img src="https://static001.infoq.cn/resource/image/37/43/371fa0e0267b51bcea97a23a5de03e43.png" /></p><p></p><p>填写的邮箱会收到相应的验证码，将验证码进行复制</p><p></p><p><img src="https://static001.infoq.cn/resource/image/60/07/6047ebf3ff2b1f9bbb2b1c145347f507.png" /></p><p></p><p>将验证码填入图中点击"next"按钮，就会出现下图的设置密码，按照要求设置好密码后，点击“Create Amazon Builder ID”按钮</p><p></p><p></p><p>登录成功后如下图所示，我们点击图示标注所示的“Allow”按钮完成授权即可。</p><p><img src="https://static001.infoq.cn/resource/image/22/71/22d283d4ab630cb3fa1674d40d01b471.png" /></p><p></p><p>授权成功会弹出绿色的小框</p><p></p><p><img src="https://static001.infoq.cn/resource/image/62/d4/6253cfc3148f31f59ff44784813178d4.png" /></p><p></p><p>返回 pycharm 会进行提示，点击“yes”即可</p><p><img src="https://static001.infoq.cn/resource/image/47/c1/47ba03c9c2d91d69633c5f80f97656c1.png" /></p><p></p><p>图中表示以安装成功，这样我们就可以开始使用 CodeWhisperer 了。</p><p><img src="https://static001.infoq.cn/resource/image/d9/5c/d97e99584d850e712cfb6a9c8233c55c.png" /></p><p></p><p>在我们编写代码时，CodeWhisperer 会给出与当前光标位置相关的代码建议。如上图所示，我们在代码编辑窗口写上一个注释，例如“<a href="https://so.csdn.net/so/search?q=kmeans%E7%AE%97%E6%B3%95&amp;spm=1001.2101.3001.7020?trk=cndc-detail">kmeans 算法</a>"”，然后回车，我们就可以看到该插件为我们给出了相关代码建议，只需要直接可以通过单击鼠标来接受或拒绝建议即可，你可以选择“Next”继续生成，然后“Insert Code”插入代码。你的注释描述信息写的越精准，⽣成的代码质量越好。</p><p><img src="https://static001.infoq.cn/resource/image/3c/9c/3cb16yy579fc239f3ca3704e98b8d89c.png" /></p><p></p><p>此外，如果我们点击 "Run Security Scan"，我们可以让 CodeWhisperer 检查你的代码是否存在安全漏洞，并给我们提供代码建议来修复它们。我们还可以通过点击鼠标接受或拒绝这些建议，或者修改这些建议以满足实际使用要求。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/cWW5FDyHrJrlaXHeyIRe</id>
            <title>Katalyst Memory Advisor：用户态的 K8s 内存管理方案</title>
            <link>https://www.infoq.cn/article/cWW5FDyHrJrlaXHeyIRe</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/cWW5FDyHrJrlaXHeyIRe</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 06:03:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 混部场景, 内存管理, 性能影响, 内存利用率
<br>
<br>
总结: 在混部场景下，内存管理是一个很重要的话题。一方面，当节点或容器的内存紧张时，业务的性能可能会受到影响，比如出现时延抖动或者 OOM。另一方面，节点上可能存在一些较少被使用但未被释放的内存，导致可以出让给离线作业使用的内存量较少，无法实现有效的超卖。为了解决这些问题，字节跳动总结了一套用户态的Kubernetes内存管理方案Memory Advisor，并在资源管理系统Katalyst中开源。本文将介绍Kubernetes和Linux内核原生的内存管理机制及其局限，以及Katalyst如何通过Memory Advisor在提升内存利用率的同时，保障业务的内存服务质量。 </div>
                        <hr>
                    
                    <p>在混部场景下，内存管理是一个很重要的话题：一方面，当节点或容器的内存紧张时，业务的性能可能会受到影响，比如出现时延抖动或者 OOM（由于对内存进行了超卖，该问题可能会更加严重）。另一方面，节点上可能存在一些较少被使用但未被释放的内存，导致可以出让给离线作业使用的内存量较少，无法实现有效的超卖。</p><p></p><p>针对上述问题，<a href="https://www.infoq.cn/article/AsgjeVRM8isLSzo7IXzh?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">字节跳动</a>"将其在大规模在离线混部过程中积累的精细化的内存管理经验，总结成了一套用户态的 <a href="https://mp.weixin.qq.com/s?__biz=MzI5ODQ2MzI3NQ%3D%3D&amp;chksm=eca7f1acdbd078baf17571575d642a09391401c945ddae152443ea51963b8eaca11b540c3364&amp;idx=1&amp;mid=2247502056&amp;scene=27&amp;sn=ef7a6deb9a63551cac10322b6b059aad&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">Kubernetes</a>" 内存管理方案&nbsp;Memory Advisor，并在资源管理系统 <a href="https://xie.infoq.cn/article/3cbf7cf040ca79a9419edd89c?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Katalyst </a>"中开源。</p><p></p><p>本文将重点介绍 Kubernetes 和 Linux 内核原生的内存管理机制及其局限，以及 Katalyst 如何通过 Memory Advisor 在提升内存利用率的同时，保障业务的内存服务质量。</p><p></p><h2>原生方案的局限</h2><p></p><p></p><p></p><h4>内核原生的内存分配与回收机制</h4><p></p><p></p><p>由于访问内存的速度比访问磁盘快很多，Linux 使用内存的策略比较贪婪，采取尽量分配，当内存水位较高时才触发回收的策略。</p><p></p><p>内存分配</p><p></p><p>内核的内存分配方式主要包含 2 种：</p><p></p><p>快速内存分配：首先尝试进行快速分配，判断分配完成后整机的空闲水位是否会低于 Low Watermark，如果低于的话先进行一次快速内存回收，然后再判断是否可以分配。如果还不满足，则进入慢速路径。慢速内存分配：慢速路径中会首先唤醒 Kswapd 进行异步内存回收，然后尝试进行一次快速内存分配。如果分配失败，则会尝试对内存页进行 Compact 操作。如果还无法分配，则尝试进行全局直接内存回收，该操作会将所有的 Zone 都扫描一遍，比较耗时。如果还不成功，则会触发整机 OOM 释放一些内存，再尝试进行快速内存分配。</p><p></p><p>内存回收</p><p></p><p>内存回收根据针对的目标不同，可以分为针对 Memcg 的和针对 Zone 的。内核原生的内存回收方式包含以下几种：</p><p></p><p>Memcg 直接内存回收：如果一个 Cgroup 的 Memory Usage 达到阈值，则会触发 Memcg 级别的同步内存回收来释放一些内存。如果还不成功，则会触发 Cgroup 级别的 OOM。</p><p>全局快速内存回收：上文在介绍快速内存分配时提到了快速内存回收，其之所以快速，是因为只要求回收这次分配所需的页数量即可。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5a/5ab0f32db52e8d4cfff9cbd7f019075d.png" /></p><p></p><p>全局异步内存回收：如上图所示，当整机的空闲内存降到 Low Watermark 时，会唤醒 Kswapd 在后台异步地回收内存，回收到 High Watermark 为止。</p><p>全局直接内存回收：如上图所示，如果整机的空闲内存降到 Min Watermark，则会触发全局直接内存回收。因为该过程是同步的，发生在进程内存分配的上下文，对业务的性能影响较大。</p><p></p><h4>K8s 原生的内存管理机制</h4><p></p><p></p><p>Memory Limit</p><p></p><p>Kubelet 依据 Pod 中各个 Container 声明的 Memory Limit 设置 Cgroup 接口&nbsp;memory.limit_in_bytes&nbsp;，约束了 Pod 和 Container 的内存用量上限。当 Pod 或 Container 的内存用量达到该限制时，将触发直接内存回收甚至 OOM。</p><p></p><p>驱逐</p><p></p><p>当节点的内存不足时，K8s 将选择部分 Pod 进行驱逐，并为节点打上 Taint&nbsp;node.kubernetes.io/memory-pressure，避免将 Pod 再调度到该节点。</p><p>内存驱逐的触发条件条件为整机的 Working Set 达到阈值，即：</p><p></p><p>memory.available := node.status.capacity[memory] - node.stats.memory.workingSet</p><p></p><p>其中&nbsp;memory.available&nbsp;为用户配置的阈值。</p><p></p><p>在对待驱逐的 Pod 进行排序时，首先判断 Pod 的内存使用量是否超过其 Request，如果超过则优先被驱逐；其次比较 Pod 的 Priority，优先级低的 Pod 先被驱逐；最后比较 Pod 的内存使用量超过其 Request 的差值，超出越多则越先被驱逐。</p><p></p><p>OOM</p><p></p><p>如果全局直接内存回收仍然满足不了节点上的进程对内存的需求，将触发整机的 OOM。Kubelet 在启动容器时，会根据其所属 Pod 的 QoS 级别与其对内存的申请量，为其配置&nbsp;/proc//oom_score_adj，从而影响其被 OOM Kill 的顺序：</p><p></p><p>对于 Critical Pod 或 Guaranteed Pod 中的容器，将其&nbsp;oom_score_adj&nbsp;设置为 -997对于 BestEffort Pod 中的容器，将其&nbsp;oom_score_adj&nbsp;设置为 1000对于 Burstable Pod 中的容器，根据以下公式计算其&nbsp;oom_score_adj</p><p></p><p>min{max[1000 - (1000 * memoryRequest) / memoryCapacity, 1000 + guaranteedOOMScoreAdj], 999}</p><p></p><p>Memory QoS</p><p></p><p>K8s 从 v1.22 版本开始，基于 Cgroups v2 实现了 Memory QoS 特性[2]，可以为容器的内存 Request 提供保障，进而保障了全局内存回收在 Pod 间的公平性。</p><p>具体的 Cgroups 配置方式如下：</p><p></p><p>memory.min: 依据&nbsp;requests.memory&nbsp;配置。memory.high: 依据&nbsp;limits.memory * throttlingfactor&nbsp;(或&nbsp;nodeallocatablememory * throttlingfactor) 配置。memory.max: 依据&nbsp;limits.memory&nbsp;(或&nbsp;nodeallocatablememory) 配置。</p><p></p><p>在 K8s v1.27 版本中，对 Memory QoS 特性进行了增强。主要是为了解决以下问题：</p><p></p><p>当容器的 Requests 和 Limits 比较接近时，由于&nbsp;memory.high&nbsp;&gt;&nbsp;memory.min&nbsp;的限制，memory.high&nbsp;中配置的 Throttle 阈值可能不生效。按照上述方式计算出的&nbsp;memory.high&nbsp;可能较低，导致频繁的 Throttle，影响业务性能。throttlingfactor&nbsp;的默认值 0.8 过于激进，一些 Java 应用通常会用到 85% 以上的内存，经常被 Throttle。</p><p>因此进行了以下优化：</p><p></p><p>对&nbsp;memory.high&nbsp;的计算方式进行改进：</p><p></p><p>memory.high&nbsp;=&nbsp;floor{[requests.memory&nbsp;+&nbsp;memory&nbsp;throttling&nbsp;factor&nbsp;*&nbsp;(limits.memory&nbsp;or&nbsp;node&nbsp;allocatable&nbsp;memory&nbsp;-&nbsp;requests.memory)]/pageSize}&nbsp;*&nbsp;pageSize</p><p></p><p>将&nbsp;throttlingfactor&nbsp;的默认值调整为 0.9。</p><p></p><h4>局限</h4><p></p><p></p><p>从前两节的介绍中，我们可知 K8s 和内核原生的内存管理机制存在以下局限：</p><p></p><p>全局内存回收缺少公平性机制：当对内存进行超卖时，即使所有容器的内存使用量都显著低于 Limit，整机内存也可能触及全局内存回收水位线。在当前使用最广泛的 Cgroups v1 环境下，Container 声明的 Memory Request 默认不会体现在 Cgroups 配置上，仅作为调度的依据。因此，全局内存回收在 Pod 间缺少公平性保障，容器的可用内存不会像 CPU 一样按 Request 比例划分。</p><p></p><p>全局内存回收缺少优先级机制：在混部场景下，低优离线容器往往运行着资源消耗型任务，可能大量申请内存。而内存回收并不感知业务的优先级，导致节点上的高优在线容器进入直接内存回收的慢速路径，干扰到在线应用的内存资源质量。</p><p></p><p>原生驱逐机制的触发时机可能较晚：K8s 当前主要通过 kubelet 驱逐的方式保障内存使用的优先级与公平性，但是原生驱逐机制的触发时机可能发生在全局内存回收之后，不能及时生效。</p><p></p><p>Memcg 直接内存回收会影响业务性能：当容器的内存使用量达到阈值时，会触发 Memcg 直接内存回收，造成内存分配的延迟，可能导致业务抖动。</p><p></p><h2>Katalyst Memory Advisor</h2><p></p><p></p><h4>系统架构</h4><p></p><p></p><p>Katalyst Memory Advisor 的架构经过多次讨论和迭代，采用可插拔的设计，以框架加插件的模式便于开发者灵活扩展功能和策略。各组件或模块的职责如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/e3/e3c52ce402ae7d37240f37d95791f447.png" /></p><p></p><p>Katalyst Agent: 单机上的资源管理 Agent。本功能中涉及以下模块：</p><p></p><p>Eviction Manager: 带外对 kubelet 原生驱逐策略进行扩展的框架。在本功能中负责周期性地调用各驱逐插件的接口，获取驱逐策略计算的结果并执行驱逐动作。Memory Eviction Plugins: Eviction Manager 的插件。本功能中涉及以下插件：</p><p></p><p><img src="https://static001.geekbang.org/infoq/39/39bc44b931808671505046a2f7da475d.png" /></p><p></p><p>Memory QRM Plugin: 内存资源管理插件。在本功能中负责离线大框的 Memcg 配置，以及 Drop Cache 动作的实现。SysAdvisor: 单机上的算法模块，支持通过插件扩展算法策略。在本功能中涉及以下插件：</p><p></p><p><img src="https://static001.geekbang.org/infoq/9d/9dbbdb844f9c3e53717fdc672169b730.png" /></p><p></p><p>Reporter: 带外信息上报框架。在本功能中负责上报内存压力相关的Taint 到 Node 或 CustomNodeResource CRD 中。MetaServer: Katalyst Agent 中的元信息管理组件。在本功能中负责提供 Pod、Container 的元信息，缓存 Metrics，以及提供动态配置能力。</p><p>Malachite: 单机上的 Metrics 数据采集组件。在本功能中负责提供 Node、NUMA、Container 级别的内存指标。</p><p></p><p>Katalyst Scheduler:&nbsp;中心调度器。本功能涉及的插件：</p><p></p><p><img src="https://static001.geekbang.org/infoq/de/de46d8d425842005285fc4a8c03bf862.png" /></p><p></p><p></p><h4>详细方案</h4><p></p><p></p><p>多维度的干扰检测</p><p></p><p>Memory Advisor 通过周期性的干扰检测，提前感知内存压力，并触发对应的缓解措施。当前已支持下列维度的干扰检测：</p><p></p><p>整机和 NUMA 级别的内存水位：比较整机和 NUMA 级别的空闲内存水位和全局异步内存回收的阈值水位 Low Watermark 之间的关系，尽量避免触发全局直接内存回收。</p><p></p><p>整机的 Kswapd 回收内存的速率：如果全局异步内存回收的速率较高，并且持续较久的时间，那么说明此时整机的内存压力较大，后续极有可能会触发全局直接内存回收。</p><p></p><p>Pod 级别的 RSS 超用情况：通过超卖可以使节点的内存得到充分使用，但是无法控制超卖的内存被用作 Page Cache 还是 RSS。如果某些 Pod 使用的 RSS 远超过其 Request，可能造成节点内存水位过高且无法被回收。进而影响其他 Pod 无法使用足够的 Page Cache 而性能受损，或者可能导致 OOM。</p><p></p><p>QoS&nbsp;级别的内存资源满足度：通过比较节点 Relcaimed Memory 的供应量和该节点上&nbsp;reclaimed_cores&nbsp;QoS 级别总的 Memory 申请量，计算离线作业的内存资源满足度，避免离线作业的服务质量受到严重影响。</p><p></p><p>多层级的缓解措施</p><p><img src="https://static001.geekbang.org/infoq/51/51e9eb4a072c48ea8461d2ab859143bd.png" /></p><p>根据干扰检测反馈的异常级别不同，Memory Advisor 支持多层级的缓解措施。在避免高优 Pod 受到干扰的同时，尽量减轻对 Victim Pod 的影响。</p><p></p><p>禁止调度</p><p></p><p>禁止调度是影响程度最小的缓解措施。当干扰检测反馈任何程度的整机异常时，都会触发该节点的禁止调度，避免调度更多的 Pod 使情况进一步恶化。</p><p></p><p>当前 Memory Advisor 已通过 Node Taint 支持对所有 Pod 的禁止调度，后续我们将使调度器能够感知 CustomNodeResource CRD 中扩展的 Taint，从而实现针对&nbsp;reclaimed_cores&nbsp;Pod 的精细化禁止调度。</p><p></p><p>Tune Memcg</p><p></p><p>Tune Memcg 是一种对 Victim Pod 影响程度较小的缓解措施。当干扰检测反馈的异常程度较低时，会触发 Tune Memcg 操作，挑选部分&nbsp;reclaimed_cores&nbsp;Pod，并为其配置较高的内存回收触发阈值，使离线 Pod 尽早触发内存回收，释放出来一些内存，从而尽量避免触发全局直接内存回收。</p><p></p><p>Tune Memcg 因为需要配合 veLinux 内核开源的 Memcg 异步内存回收特性 [3] 一起使用，默认不会开启，不影响使用。</p><p></p><p>Drop Cache</p><p></p><p>Drop Cache 是一种对 Victim Pod 影响程度中等的缓解措施。当干扰检测反馈的异常程度中等时，会触发 Drop Cache 操作，挑选部分 Cache 用量较高的&nbsp;reclaimed_cores&nbsp;Pod，强制释放其缓存，从而尽量避免触发全局直接内存回收。</p><p>在 Cgroups v1 环境下，通过&nbsp;memory.force_empty&nbsp;接口触发缓存释放：</p><p></p><p>echo&nbsp;0&nbsp;&gt;&nbsp;memory.force_empty</p><p></p><p>在 Cgroups v2 环境下，通过向&nbsp;memory.reclaim&nbsp;接口写入一个较大的值触发缓存释放，比如：</p><p></p><p>echo 100G &gt; memory.reclaim</p><p></p><p>因为 Drop Cache 是一个比较耗时的操作，我们实现了一个异步的任务执行框架，避免阻塞主流程。这一部分的技术细节将在后续的技术文章中进行介绍。</p><p></p><p>驱逐</p><p></p><p>驱逐是一种对 Victim Pod 影响较大的措施，也是最为快速、有效的兜底措施。当干扰检测反馈的异常程度较高时，会触发整机或 NUMA 级别的驱逐 (或仅对&nbsp;reclaimed_cores&nbsp;Pod 的驱逐)，从而有效避免触发全局直接内存回收。</p><p></p><p>Memory Advisor 支持用户通过配置自定义待驱逐 Pod 的排序逻辑。如果用户未配置，默认的排序逻辑如下：</p><p></p><p>根据 Pod 的 QoS 级别排序，reclaimed_cores&gt;&nbsp;shared_cores/dedicated_cores。根据 Pod 的 Priority 排序，优先级低的先被驱逐。根据 Pod 的 Memory Usage 排序，Usage 高的先被驱逐。</p><p></p><p>基于“策略器插件化，执行器收敛”的设计理念，我们在 Katalyst Agent 中抽象出了一个 Eviction Manager 框架，将驱逐策略下放到 Plugin 中，将驱逐动作收敛在 Manager。具有以下优势：</p><p></p><p>Plugin 和 Manager 可以通过本地函数调用或远程 gRPC 协议通信，方便灵活启停插件。可以在 Manager 中方便地支持一些针对驱逐的治理操作，比如过滤、限流、排序、审计等。支持对插件进行 Dry Run，方便对策略进行充分验证后再使其真正生效。</p><p></p><p>离线大框</p><p></p><p>为了避免离线的容器过度使用内存影响到在线容器的服务质量，我们通过离线大框限制&nbsp;reclaimed_cores&nbsp;QoS 级别总的内存用量。</p><p></p><p>具体实现上，我们在单机算法组件 SysAdvisor 中扩展了一个 Memory Guard 插件，周期性地计算&nbsp;reclaimed_cores&nbsp;Pod 可以使用的内存总量，并通过 Memory QRM Plugin 将其下发到 BestEffort QoS 层级 Cgroup 的&nbsp;memory.limit_in_bytes&nbsp;文件中。</p><p></p><p>内存动态迁移</p><p></p><p>在 Flink 等业务场景下，服务的性能与内存带宽和内存延迟有较强的相关性，同时对内存容量也有一定规模的占用。默认的内存分配策略会优先从本地的 NUMA Node 分配内存，从而得到较小的内存访问延迟。但是另一方面，默认的内存分配策略可能会造成各个 NUMA Node 的内存使用不均衡，某些 NUMA Node 的压力过大成为热点，进而严重影响服务的性能，出现 LAG。</p><p>因此，我们通过 Memory Advisor 感知各个 NUMA Node 的内存水位，并动态调整容器绑定的 NUMA Node 进行内存迁移，避免某个 NUMA Node 成为热点。</p><p>在生产环境落地内存动态迁移功能的过程中，我们曾遇到可能导致系统 Hang 住的异常情况，因此对内存迁移的方式进行了优化。这一部分的实践经验将在后续的技术文章中展开介绍。</p><p></p><p>Memcg 差异化回收策略</p><p></p><p><img src="https://static001.geekbang.org/infoq/56/56219a7262525875db5ec13f5025f538.png" /></p><p></p><p>因 Memcg 直接内存回收对业务性能会造成较大影响，字节跳动内核团队为 veLinux 内核增强了 Memcg 异步内存回收特性，并已开源 [3]。</p><p></p><p>在混部场景下，在线业务主要的 IO 行为是读写日志，而离线任务读写文件更频繁，Page Cache 对离线作业的性能影响较大。因此，我们通过 Memory Advisor 支持了 Memcg 级别的差异化内存回收策略：</p><p></p><p>对于需要使用大量 Page Cache 的业务 (比如离线作业)，用户可以通过 Pod Annotation 为其指定一个相对较低的 Memcg 异步内存回收水位，使其内存回收更保守，从而可以使用更多 Page Cache；而某些业务更倾向于尽量避免触发直接内存回收造成性能抖动，则可以通过 Pod Annotation 为其配置相对激进的 Memcg 异步回收策略。</p><p></p><p>Memcg 差异化回收策略因为需要配合 veLinux 内核的开源特性一起使用，默认不会开启。</p><p></p><h2>后续规划</h2><p></p><p></p><p>在 Katalyst 后续版本中，我们将持续迭代 Memory Advisor，使其能够支持更多用户场景。</p><p></p><h4>将部分能力与 QoS 解耦</h4><p></p><p></p><p>Memory Advisor 在混部场景下扩展了一些增强的内存管理能力，其中一些能力本质上是与 QoS 正交的，在非混部场景下依然适用。</p><p></p><p>因此，我们后续会将 Memcg 差异化回收策略、干扰检测与缓解等功能与 QoS 解耦，打造成通用场景下的精细化内存管理能力，使非混部场景的用户也可以使用。</p><p></p><h2>OOM 优先级</h2><p></p><p></p><p>上文中介绍到 Kubernetes 会根据容器其所属 Pod 的 QoS 级别，为其配置不同的&nbsp;oom_score_adj。但是最终的 OOM Score 还会受到内存用量等其他因素的影响。</p><p></p><p>在潮汐混部场景下，在离线 Pod 属于相同的 QoS 级别，可能无法保证离线 Pod 一定早于在线 Pod 被 OOM Kill。因此，需要扩展一个 Katalyst QoS Enhancement：OOM 优先级。Memory Advisor 需要在用户态为属于不同 QoS 优先级的容器配置对应的&nbsp;oom_score_adj，严格保证在离线 Pod 的 OOM 顺序。</p><p></p><p>此外，字节跳动内核团队近期为 Linux 内核提交了一个 Patch [4]，期望通过 BPF 将内核的 OOM 行为可编程化，从而更加灵活地自定义 OOM 的策略。</p><p></p><h4>冷内存卸载</h4><p></p><p></p><p>节点上可能存在一些较少被使用的内存未被释放 (即冷内存)，导致可以出让给离线作业使用的内存量较少，无法实现有效的内存超卖。</p><p></p><p><img src="https://static001.geekbang.org/infoq/88/88c87ef7eeb8f6902620728476e401ae.png" /></p><p></p><p>为了获得更多的内存出让量，我们参考了 Meta 的 Transparent Memory Offloading (TMO) 论文 [5]，后续将使 Memory Advisor 在用户态通过 PSI 感知内存压力，当内存压力较小时提前触发内存回收。并通过内存冷热探测子模块 DAMON 统计内存热度信息，将冷内存换出到相对廉价的存储设备上，或通过 zRAM 将其压缩，从而节省内存空间，提高内存资源利用率。</p><p></p><p>该特性的技术细节将在后续的技术文章中进行介绍。</p><p></p><h2>总结</h2><p></p><p></p><p>在字节跳动，Katalyst 部署了超过 900,000 个节点，管理了数千万核，统一管理各种类型的工作负载，包括微服务、搜广推、存储、大数据和 AI 作业等。将天级资源利用率从 20% 提升至 60% 的同时，保障了各种类型的工作负载的稳定运行。</p><p></p><p>未来，Katalyst Memory Advisor 将持续迭代优化，冷内存卸载、内存迁移方式优化等更多技术原理将在后续的文章中进行解析，敬请期待。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/zIo3Yh3IRGUCfKiR2bqx</id>
            <title>Gödel：字节跳动在离线混部统一调度系统</title>
            <link>https://www.infoq.cn/article/zIo3Yh3IRGUCfKiR2bqx</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/zIo3Yh3IRGUCfKiR2bqx</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 03:42:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: SoCC 2023, 字节跳动, 云计算, Gödel
<br>
<br>
总结: 2023年10月30日至11月1日，SoCC 2023将在美国加州Santa Cruz举行。字节跳动基础架构-编排调度团队的研究成果被SoCC 2023接收，并受邀进行现场报告。SoCC是云计算领域顶级会议之一，代表了当前云计算领域的前沿水平。Gödel是字节跳动基础架构-编排调度团队自主研发的在离线统一调度系统，能满足字节各业务间混合部署、资源并池等部署要求，提高了资源利用率和任务灵活性。 </div>
                        <hr>
                    
                    <p>2023 年 10 月 30 日至 11 月 1 日，<a href="https://xie.infoq.cn/article/88cd8b925e2b572e492c1b924?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">SoCC</a>" 2023&nbsp;将在美国加州 Santa Cruz 举行。<a href="https://www.infoq.cn/article/AsgjeVRM8isLSzo7IXzh?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">字节跳动</a>"基础架构-编排调度团队的研究成果被&nbsp;SoCC&nbsp;2023 接收，并受邀进行现场报告。</p><p></p><p>SoCC 会议全称 Annual ACM Symposium on Cloud Computing，是云计算领域顶级会议之一，同时也是 ACM 所有会议当中唯一一个同时被 SIGMOD 和 SIGOPS 赞助的顶会，代表了当前云计算领域在学术界、工业界和开源社区的前沿水平。</p><p></p><p>SoCC 会议伴随着云计算的兴起而成立，至今已经举办到第 14 届。该会议每年吸引全球顶级研究机构和知名大公司投稿，对系统创新性、完整性、和有效性等方面都要求很高。今年，会议论文的接收率只有30%。</p><p></p><p>Gödel: Unified Large-Scale Resource Management and Scheduling at ByteDance</p><p></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzI4MTY5NTk4Ng%3D%3D&amp;chksm=eba41b74dcd3926223b55710fe5820796e1edbf01c859d7aef4131c75213fcc77ee5dd3abd93&amp;idx=1&amp;mid=2247489769&amp;scene=27&amp;sn=1457a1499ee7608a9b1502c8b287ce41&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">Gödel </a>"是字节跳动基础架构-编排调度团队自主研发、面向大规模云原生基础设施管理的在离线统一调度系统。</p><p></p><p>字节跳动旗下业务线在过去几年的飞速发展中对计算资源的需求与日俱增，在数据中心的不断膨胀和对计算资源的差异化需求中，原生的 Kubernetes 调度器对于各种在离线业务负载统一托管、资源统一运营都带来了一系列挑战。</p><p></p><p>在此背景下，Gödel 调度系统应运而生。和 Kubernetes 原生调度器相比，Gödel&nbsp;能同时在一套集群环境支持各类在离线、机器学习负载混合调度，同时具有高吞吐（up to 10X）、高弹性（sub-minute 资源流转）、高资源利用率（up to 60%）等特点，更好地满足了字节各业务间混合部署、资源并池等部署要求。在满足各形态业务负载 SLA 要求的同时，为计算集群资源统一运营提供了通用平台，进而提高了字节数据中心的资源利用率和任务灵活性，达到降本增效的目的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/df/df8ab73079a0351b07d565295f3967e1.png" /></p><p></p><p>Gödel 论文与现场报告将于 10 月底正式亮相 SoCC 2023。届时，字节跳动基础架构团队也将发布该论文的对应解读文章，欢迎持续关注。</p><p></p><p>目前，字节跳动在离线混部的另一核心组件——<a href="http://mp.weixin.qq.com/s?__biz=Mzk0NDMzNjkxNw==&amp;mid=2247485561&amp;idx=1&amp;sn=c5a10a4f5e692568a60f76fb3bab67c2&amp;chksm=c3277103f450f815423288c62b7f66d0a86a67f3820950c77acbf241cad0e2b56f1e0461bb5f&amp;scene=21#wechat_redirect">资源管控系统 Katalyst</a>"&nbsp;已开源，点击了解社区【<a href="https://mp.weixin.qq.com/s?__biz=Mzk0NDMzNjkxNw==&amp;mid=2247485631&amp;idx=1&amp;sn=93221fc76f80dcfeded9a34c344e5d7e&amp;chksm=c32771c5f450f8d3edab675f4ac2cb4d147aca9033182c22f4bbc930bbf42e060025f1e5f19f&amp;scene=21#wechat_redirect">编程挑战</a>"】！</p><p></p><p><img src="https://static001.geekbang.org/infoq/6d/6df04d6ca479f5bf430b0e635675cf14.png" /></p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/QHhWqeXmAQYQahEFMN9j</id>
            <title>AI 帮写爬虫，真的吗？ CodeWhisperer：当然！</title>
            <link>https://www.infoq.cn/article/QHhWqeXmAQYQahEFMN9j</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/QHhWqeXmAQYQahEFMN9j</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 03:38:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 2023年, AIGC, Amazon CodeWhisperer, AI辅助编程
<br>
<br>
总结: 本文介绍了2023年技术圈最火的AIGC技术以及其带来的新的编程方式，即用AI辅助编程。文章推荐了一款非常棒的人工智能编程工具Amazon CodeWhisperer，它可以大幅度提升开发人员的效率和生产力。通过使用CodeWhisperer，开发人员可以解决一些常见的编码难题，并且可以进行代码提示、代码翻译等功能。 </div>
                        <hr>
                    
                    <p>2023 年技术圈什么最火？答案毫无疑问是 AIGC，伴随该项技术的发展，新的编程方式也出现了，那就是用 AI 辅助编程，有了 AI 的加持，开发人员的效率和生产力可以得到大幅度的提升。今天我们就介绍一款非常棒的人工智能编程工具&nbsp;<a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">Amazon CodeWhisperer</a>"&nbsp;， 相信我，用上他之后，你的工作效率至少能翻一倍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/30/3071333e43a26ad220c7f589e4e393dc.png" /></p><p></p><p>在日常的编码工作中，你是否会碰到如下难题？接触了一款全新的 Python 模块，不知道如何开启 hello world；模块的某个方法，忘记了参数和返回值，反复切换手册会打断思路；不想写注释；写了一段代码，但是并不健壮，担心有难以发现的漏洞；……</p><p></p><p>如果你正在被这些问题困扰，那 Amazon CodeWhisperer 能非常好的解决上述问题。</p><p>下面跟随我的脚步，开启 CodeWhisperer 的实践吧！</p><p></p><h3>CodeWhisperer 初始化</h3><p></p><p></p><p>CodeWhisperer 支持的 IDE 包括 Visual Studio（VS）Code（本篇博客使用的 IDE） 和 JetBrains IDE（IntelliJ、PyCharm、CLion、GoLand、WebStorm、Rider、PhpStorm、RubyMine 和 DataGrip），安装过程只需要几分钟，这里我们不详细展开讲解，大家可以参考 官方文档 ，官方还贴心的准备的视频教程~</p><p></p><p>本篇博客我们使用 VSCode 完成一个爬虫项目实践，可以直接在 VSCode 插件中检索【CodeWhisperer】，直接安装即可（已经有将近 200W 的安装量啦，要抓紧跟上大家的节奏），安装完毕，在 VSCode 侧边栏中会出现 CodeWhisperer 插件图标，如下所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fd58707ab97f9bf0ab9c483b7fde5718.png" /></p><p></p><p>初始化过程中最重要的就是账号的链接，点击上图【Connect to ……】链接，之后按照步骤登录账号，一系列的操作之后，浏览器出现下图绿色状态提示界面，此时账号对接已经完成。</p><p></p><p><img src="https://static001.geekbang.org/infoq/06/068d396ec69476da61eaad645918d483.png" /></p><p></p><p>返回到 VSCode 中，会发现 Amazon CodeWhisperper 帮助手册已经打开，建议阅读一下 ，里面已经整理了插件的基础使用说明。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5b/5b6ba29acf55369185de5105cc45e891.png" /></p><p></p><p>与此同时，VSCode 左下角开发者工具（DEVELOPER TOOLS）也已经显示链接到 Builder ID。至此，我们的前置工作已经全部完成，下面可以开始进行工具使用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/48/4809824c0d5cfccbaf22c442ab7ec5f5.png" /></p><p></p><p></p><h3>CodeWhisperper 使用</h3><p></p><p></p><h4>简单逻辑示例</h4><p></p><p></p><p>CodeWhisperer 插件安装完毕，默认会开启 Auto-Suggesions（自动建议模式），此时当我们在编写注释或代码之后，CodeWhisperer 会自动给我们提供代码提示，这个过程就像超强版的语法提示，如果你还没有安装好插件，可以先看一下动图，整体感受一下。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6f/6fa339d2a4d6c3fd2ebb2b139065c5d1.gif" /></p><p></p><p>CodeWhisperer 的代码提示，在 VSCode 中可以使用左右方向键进行选择，使用&nbsp;Tab 进行确认，其他按键表示不采纳提示代码。初次体验下来，正如插件名称（Whisperer：低语者）所描述的一样，就像是编辑器在轻轻的和我们说着即将要敲入的代码，如果同意，可以一键采纳，不同意，直接忽略即可。</p><p></p><h3>代码翻译示例</h3><p></p><p></p><p>拥有此功能之后，很多简单的程序完全可以基于智能提示编写完毕，但这肯定不够，我们需要 CodeWhisperer 完成更有挑战的事情，在 Python 爬虫领域，经常需要将一段前端 JS 代码用 Python 重新实现。</p><p></p><p>下面提供一段 JavaScript 中生成&nbsp;UUID&nbsp;的代码，然后用 Python 复写。</p><p></p><p><code lang="text"> p = function(e) {
  var t = e || null;
  return null == t &amp;&amp; (t = "xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx".replace(/[xy]/g, (function(e) {
      var t = 16 * Math.random() | 0;
      return ("x" === e ? t : 3 &amp; t | 8).toString(16)
  }
  ))),
</code></p><p></p><p>在 VSCode 中直接输入你的需求，然后 CodeWhisperper 会给我们一个完整的实现示例，可以看到代码重写完全正确。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fa/fa312148d6faae8447f54ab7020557d0.gif" /></p><p></p><p></p><h4>加密函数示例</h4><p></p><p></p><p>除了翻译代码外，在编写爬虫案例的时候，如果你对某些加密函数使用详情有遗忘，可以让 CodeWhisperer 直接给出示例，快速唤醒自己的记忆。</p><p>下图为输入注释&nbsp;# 使用 Python 实现 hmac_sha256 加密函数&nbsp;获取函数的使用示例代码。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a72bc1591d39b07d63aea20d8075411.gif" /></p><p></p><p>如果在使用 CodeWhisperer 的过程中按错按键，即没有使用 Tab 确认代码，可以随时按下快捷键 Alt+C，CodeWhisperer 代码提示会再次出现，又可以继续加速你的开发效率了。</p><p></p><h4>混合加密示例</h4><p></p><p></p><p>如果将上述逻辑都定义为单逻辑，那下述需求就是一个复合逻辑示例了，在实际编码中，会碰到需要将两种加密混合使用的情况，需求如下：</p><p></p><blockquote>使用 Python 实现 Base64 + AES 加密字符串</blockquote><p></p><p></p><p>在 IDE 中输入上述注释信息，然后回车，CodeWhisperer 就会自动生成后续代码，其中部分逻辑可以一次性完成，效果如下，其中在核心逻辑部分，CodeWhisperer 表现非常优秀，答案秒输出，并且直接可用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/aa/aa0812325e01fe12923f46f82dea7ae3.gif" /></p><p></p><p></p><h4>算法模板示例</h4><p></p><p></p><p>在业务逻辑的编写过程中，有时会用到常见算法，例如快排、堆排、哈希等，这些算法很多都类似模板代码，在编写的时候，如果可以一键生成，能大幅度提高代码编写速度，包括算法改写速度，在 CodeWhisperer 中，可以通过注释快速生成。</p><p></p><blockquote>使用 Python 生成快排代码</blockquote><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/e8/e81689721f38323b56fde5ed41fa11b4.gif" /></p><p></p><p>体验 5 个示例之后，CodeWhisperer 的使用非常简单，而且无需切换编辑窗口，在 IDE 的代码文件中直接完成了 AIGC 的问答流程，响应速度非常快，代码准确性很高。</p><p></p><p>单独的案例对 CodeWhisperer 已经没有难度了，接下来我们尝试完成一个完整的 Python 爬虫案例（咱们看一下只写注释，能不能完成一个合格的爬虫采集程序）。</p><p></p><h3>CodeWhisperer 项目实战</h3><p></p><p></p><p></p><blockquote>目标采集站点为作者博客，无侵权问题。</blockquote><p></p><p>编写爬虫基础框架注释，包含如下内容：</p><p>程序使用的采集模块目标采集站点地址采集的目标标签数据存储到文件发送采集到的数据到指定邮箱</p><p></p><h4>第一步：输入如下注释，获取网页响应内容</h4><p></p><p></p><p></p><blockquote>使用 Python requests 模块采集&nbsp;<a href="https://blog.csdn.net/hihell?type=blog">https://blog.csdn.net/hihell?type=blog</a>"</blockquote><p></p><p></p><p><code lang="text"># 使用 Python requests 模块采集 https://blog.csdn.net/hihell?type=blog

import requests

url = 'https://blog.csdn.net/hihell?type=blog'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'
    }
response = requests.get(url, headers=headers)

print(response.text)
</code></p><p></p><p>写完注释，直接回车之后，CodeWhisperer 直接给我们生成了对应的代码，过程中只需要按下 回车和 Tab 键即可。</p><p></p><p>如果你觉得上述代码有些简单，可以在代码基础上继续完善，例如将请求头进行完善，增加 referer 和 host 参数，直接在需要修改的位置添加注释即可。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a770e2ee60a11dd5e2af601e4383985.gif" /></p><p></p><p></p><h4>第二步：编写目标数据采集函数</h4><p></p><p></p><p>这一步，我们要使用 lxml 模块中的 etree 提取目标标签，继续将我们的逻辑输入到注释注释中，然后回车。</p><p></p><blockquote>使用 etree 模块提取网页响应中所有的 article 标签</blockquote><p></p><p></p><p><code lang="text"># 使用 etree 模块提取网页响应中所有的 article 标签

from lxml import etree

html = etree.HTML(response.text)

article_tags = html.xpath('//article')

print(article_tags)
</code></p><p></p><p>此时目标博客标签已经被初步提取，下面要将标题和超链接地址进行再次提取，这里的注释需要尽可能编写清晰，如果你对 Python 爬虫技术栈有一定了解，到这里就会发现 CodeWhisperer 对代码编写提速效果。</p><p></p><p><code lang="text"># 循环 article_tags 提取其内部的超链接标签的 href 属性和 h4 标签文本
# 注意标签结构是 超链接 a 标签含后代 h4 标签
for article in article_tags:
    href = article.xpath('./a/@href')[0]
    title = article.xpath('./a//h4/text()')[0]
    print(href, title)
</code></p><p></p><h4>第三步：数据存入到 csv 文件中</h4><p></p><p></p><p>写入文件的逻辑属于常见操作，直接输入函数需求，相信 CodeWhisperer 会直接生成的，输入的参考注释如下：</p><p></p><blockquote>编写一个 csv 文件写入函数，其包含 2 个参数，分别是 title 和 href</blockquote><p></p><p></p><p><code lang="text"># 编写一个 csv 文件写入函数，其包含 2 个参数，分别是 title 和 href

def write_to_csv(title, href):
    with open('codewhisperper_demo.csv', 'a', encoding='utf-8') as f:
        f.write(title + ',' + href + '\n')</code></p><p>write_to_csv()&nbsp;函数调用放到上述循环中即可完成本步骤。再次运行代码，在爬虫目录会生成对应的文件，打开文件得到目标数据，效果图如下所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0e/0e8cd71556d94b2f927b26be4c94d45a.png" /></p><p></p><p>这里一个简单的爬虫写完了，但是我们的工作还没有完成，要继续优化这个程序。</p><p></p><h4>第四步：将采集到的数据，发送到 163 邮箱</h4><p></p><p></p><p>输入注释：# 编写一个邮件发送函数，将刚刚生成的文件 codewhisperper_demo.csv&nbsp;发送到指定邮箱，CodeWhisperer 瞬间就会帮助我们生成一个参考函数，实际效果如下图所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d7/d7b9d0ddc1c846a5640a4176db301926.gif" /></p><p></p><p></p><blockquote>将上述代码中的账号和密码修改为自己真实数据，即可实现一键发送邮件。</blockquote><p></p><p>打开收件箱，可以看到刚刚的邮件已经发送成功，CodeWhisperer 给我们提供了一种邮件发送的实现，你可以再其基础上进行改造，以满足个性化的需求。</p><p></p><p><img src="https://static001.geekbang.org/infoq/96/965750f21ea6216147673ed3b1a92c95.png" /></p><p></p><p></p><h3>CodeWhisperer 使用总结</h3><p></p><p></p><p>在博客开篇，我提出了编码过程中几个比较头疼的问题，尤其是第二个，程序员在编码过程中总是切出去查找代码示例和文档手册，从而打断编码思路，而 CodeWhisperer 非常完美的解决了该问题，在编码的过程中，Amazon CodeWhisperer 自动提供编码建议，同意就使用，不同意就舍弃，真正实现了沉浸式编程。</p><p></p><p>除此之外，CodeWhisperer 还可以基于代码和注释生成新的业务代码，尤其当我们 Python 工程师接触一个新的模块时，他可以快速的产出示例代码，而且生成的代码与我们编码风格非常相似，甚至编码风格和命名规则都可以完美学习到。在编码代码过程中，CodeWhisperer 还会自动为我们的代码提供注释参考，让我们将更多精力投入到业务逻辑中。</p><p></p><p>将 CodeWhisperer 用起来吧，几分钟之后，你就会深刻的感受到编码效率的提升！</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Pqu5weV2ssFwYADv9plm</id>
            <title>Katalyst：字节跳动云原生成本优化实践</title>
            <link>https://www.infoq.cn/article/Pqu5weV2ssFwYADv9plm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Pqu5weV2ssFwYADv9plm</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 03:30:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 字节跳动, 云原生化改造, 资源利用效率, 混合部署
<br>
<br>
总结: 字节跳动通过云原生化改造和混合部署的方式提高了资源利用效率。他们通过将在线和离线服务同时运行在相同节点上，充分利用两者之间的互补特性，实现了更好的资源利用。这种混合部署的方式使得字节跳动能够二次销售在线未使用的资源，并利用离线工作负载填补超售资源，从而保持资源利用效率在较高水平。通过不断迭代混合部署系统，字节跳动解决了资源利用率低、资源波动较大等问题，实现了在离线统一调度的混合部署。他们的混合部署系统Katalyst通过统一的资源联邦、统一调度器和统一资源管理器等组件，实现了在离线一体化资源管理能力，提高了资源利用效率。 </div>
                        <hr>
                    
                    <p>从 2016 年起，<a href="https://www.infoq.cn/article/AsgjeVRM8isLSzo7IXzh?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">字节跳动</a>"开始着手服务云原生化改造，截至今日，字节服务体系主要包含四类：传统微服务（大多是基于 Golang 的 RPC Web 服务）、推广搜服务（传统 C++ 服务）、机器学习和大数据服务以及各类存储服务。</p><p></p><p>在字节跳动，基础设施面临的是一个规模巨大且持续快速变化的业务场景，而云原生技术体系需要同时聚焦资源效率和研发效率。在资源效率上，云原生要解决的核心问题之一就是如何提高集群的资源利用效率。</p><p></p><p>以典型的在线服务的资源使用情况为例，下图深蓝色部分是业务实际使用的资源量，浅蓝色部分为业务提供的安全缓冲区。即使增加缓冲区，仍有很多资源处于业务已申请但未使用的状态，因此我们的优化重点是从架构的角度尽可能地利用这些未使用的资源。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7d/7de9d838ebc3160891eab4d440222711.png" /></p><p></p><p>针对上述情况，字节跳动内部尝试过若干不同类型的资源治理方案，包括</p><p>资源运营：定期帮助业务跑资源利用情况并推动资源申请治理，问题是运维负担重且无法根治利用率问题；动态超售：在系统侧评估业务资源量并主动缩减配额，问题是超售策略不一定准确且可能导致挤兑风险；动态扩缩：问题是如果只针对在线服务扩缩，由于在线服务的流量波峰波谷类似，无法充分实现全天利用率提升。</p><p></p><p>最终我们决定采用混合部署，将在线和离线同时运行在相同节点，充分利用在线和离线资源之间的互补特性，实现更好的资源利用。我们期望达到如下图的效果，即二次销售在线未使用的资源，利用离线工作负载能够很好地填补这部分超售资源，实现资源利用效率在全天保持在较高水平。</p><p></p><p><img src="https://static001.geekbang.org/infoq/70/706083699b7b560667e235b7c9b7c488.png" /></p><p></p><p></p><h2>字节跳动混部发展历程</h2><p></p><p></p><p>随着字节跳动各业务云原生化的推进，我们根据不同阶段业务需求和技术特点，选择合适的混合部署方案，并在此过程中不断迭代我们的混部系统。</p><p></p><h4>2.1 阶段一：在离线分时混部</h4><p></p><p></p><p>第一个阶段主要进行在线和离线的分时混合部署。</p><p></p><p>对在线：在该阶段我们构建了在线服务弹性平台，用户可以根据业务指标配置横向伸缩规则；例如，凌晨时业务流量减少，业务主动缩减部分实例，系统将在实例缩容基础上进行资源 Bing Packing 从而腾出整机；对离线：在该阶段离线服务可获取到大量 spot 类型资源，由于其供应不稳定所以成本上享受一定折扣；同时对于在线来说，将未使用的资源卖给离线，可以在成本上获得一定返利。</p><p>该方案优势在于不需要采取复杂的单机侧隔离机制，技术实现难度较低；但同样存在一些问题，例如：</p><p>转化效率不高，bing packing 过程中会出现碎片等问题；离线使用体验可能也不好，当在线偶尔发生流量波动时，离线可能会被强制杀死，导致资源波动较强烈；对业务会造成实例变化，实际操作过程中业务通常会配置比较保守的弹性策略，导致资源提升上限较低。</p><p><img src="https://static001.geekbang.org/infoq/fc/fcfb033a758a4cc26fca557c62d18396.jpeg" /></p><p></p><h4>2.2 阶段二：Kubernetes/YARN 联合混部</h4><p></p><p></p><p>为解决上述问题，我们进入了第二个阶段，尝试将离线和在线真正跑在一台节点上。</p><p></p><p>由于在线部分早先已经基于 Kubernetes 进行了原生化改造，但大多数离线作业仍然基于 YARN 进行运行。为推进混合部署，我们在单机上引入第三方组件负责确定协调给在线和离线的资源量，并与 Kubelet 或 Node Manager 等单机组件打通；同时当在线和离线工作负载调度到节点上后，也由该协调组件异步更新这两种工作负载的资源分配。</p><p></p><p>该方案使得我们完成混部能力的储备积累，并验证可行性，但仍然存在一些问题：</p><p></p><p>两套系统异步执行，使得在离线容器只能旁路管控，存在 race；且中间环节资源损耗过多；对在离线负载的抽象简单，使得我们无法描述复杂 QoS 要求；在离线元数据割裂，使得极致的优化困难，无法实现全局调度优化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/da/daa019815ae94d496d40130b63ed68ce.png" /></p><p></p><h4>2.3 阶段三：在离线统一调度混部</h4><p></p><p></p><p>为解决第二阶段的问题，在第三阶段我们彻底实现了在离线统一的混合部署。</p><p></p><p>通过对离线作业进行云原生化改造，我们使它们可以在同一个基础设施上进行调度和资源管理。该体系中，最上面是统一的资源联邦实现多集群资源管理，单集群中有中心的统一调度器和单机的统一资源管理器，它们协同工作，实现在离线一体化资源管理能力。</p><p></p><p>在该架构中，Katalyst 作为其中核心的资源管控层，负责实现单机侧实时的资源分配和预估，具有以下特点：</p><p></p><p>抽象标准化：在离线元数据打通，在 QoS 的抽象上更加复杂和丰富，更好地满足业务对性能的要求；管控同步化：在容器启动时下发管控策略，避免在启动后异步修正资源调整，同时支持策略的自由扩展；策略智能化：通过构建服务画像提前感知资源诉求，实现更智能的资源管控策略；运维自动化：通过一体化的交付，实现运维自动化和标准化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f5/f520bfeeabbb430442bc2b0c9c5ccf3f.png" /></p><p></p><p></p><h2>Katalyst 系统介绍</h2><p></p><p></p><p>Katalyst 引申自英文单词 catalyst，本意为催化剂，首字母修改为 K，寓意该系统能够为所有运行在 Kubernetes 体系中的负载提供更加强劲的自动化资源管理能力。</p><p></p><h4>3.1 Katalyst 系统概览</h4><p></p><p></p><p>如下图所示，Katalyst 系统大致分为四层，从上到下依次包括：</p><p></p><p>最上层的标准 API，为用户抽象不同的 QoS 级别，提供丰富的资源表达能力；中心层则负责统一调度、资源推荐以及构建服务画像等基础能力；单机层包括自研的数据监控体系，以及负责资源实时分配和动态调整的资源分配器；最底层是字节定制的内核，通过增强内核的 patch 和底层隔离机制解决在离线跑时单机性能问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f0/f02f9732fc6756c86abc306bcdc59f86.png" /></p><p></p><h4>3.2 抽象标准化：QoS Class</h4><p></p><p></p><p>Katalyst QoS 可以从宏观和微观两个视角进行解读。</p><p></p><p>宏观上，Katalyst 以 CPU 为主维度定义了标准的 QoS 级别；具体来说我们将 QoS 分为四类：独占型、共享型、回收型和为系统关键组件预留的系统型；</p><p></p><p>微观上，Katalyst 最终期望状态无论什么样的 workload，都能实现在相同节点上的并池运行，不需要通过硬切集群来隔离，实现更好的资源流量效率和资源利用效率。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a5b0bdb64f8f329dec9aeb9cea5d6a2.png" /></p><p></p><p>在 QoS 的基础上，Katalyst 同时也提供了丰富的扩展 Enhancement 来表达除 CPU 核心外其他的资源需求：</p><p></p><p>QoS Enhancement：扩展表达业务对于 NUMA / 网卡绑定、网卡带宽分配、IO Weight 等多维度的资源诉求；Pod Enhancement：扩展表达业务对于各类系统指标的敏感程度，比如 CPU 调度延迟对业务性能的影响；Node Enhancement：通过扩展原生的 TopologyPolicy 表示多个资源维度间微拓扑的组合诉求。</p><p></p><h4>3.3 管控同步化：QoS Resource Manager</h4><p></p><p></p><p>为在 Kubernetes 体系下实现同步管控的能力，我们有三种 hook 方式：CRI 层、OCI 层、Kubelet 层。最终 Katalyst 选择在 Kubelet 侧实现管控，即实现和原生的 Device Manager 同层级的 QoS Resource Manager，该方案的优势包括：</p><p></p><p>在 admit 阶段实现拦截，无需在后续步骤靠兜底措施来实现管控；与 Kubelet 进行元数据对接，将单机微观拓扑信息通过标准接口报告到节点 CRD，实现与调度器的对接；在此框架上，可以灵活实现可插拔的 plugin，满足定制化的管控需求。</p><p></p><p><img src="https://static001.geekbang.org/infoq/30/304d4a1935d9a16c90efb57daec859c0.png" /></p><p></p><h4>3.4 策略智能化：服务画像和资源预估</h4><p></p><p></p><p>通常，选择使用业务指标构建服务画像比较直观，例如服务 P99 延迟或者下游的错误率。但其也存在一些问题，比如相对系统指标而言，业务指标的获取通常更不容易；业务通常会集成多个的框架，他们生产的业务指标含义并不完全相同，如果强依赖这些指标，整个管控的实现就会变得非常复杂。</p><p></p><p>因此，我们希望最终的资源调控或服务画像是基于系统指标而非业务指标来实现，其中最关键的就是如何找到业务最关心的系统指标。</p><p></p><p>我们的做法是使用一套离线的 pipeline 去发现业务指标和系统指标之间的匹配。例如，对于图中服务来说，最核心的业务指标是 P99 调用延迟，通过分析发现与其相关度最高的系统指标是 CPU 调度延迟，我们会不断调整服务的资源供应量，尽可能地逼近它的目标 CPU 调度延迟。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fdbc626f43d1105b487d7b1efe445828.png" /></p><p></p><p>在服务画像的基础上，Katalyst 针对 CPU、内存、磁盘和网络等方面提供了丰富的隔离机制，必要时还对内核进行了定制以提供更强的性能要求。然而对于不同的业务场景和类型，这些手段并不一定直接适用，因此需要强调的是，隔离更多是一种手段而不是目的，我们在承接业务的过程中，需要根据具体的需求和场景来选择不同的隔离方案。</p><p></p><h4>3.5 运维自动化：多维度动态配置管理</h4><p></p><p></p><p>尽管我们希望所有的资源都在一个资源池系统下，但是对于在大规模生产环境中，我们不可能把所有节点都放在一个集群里。此外，一个集群中可能同时有 CPU 与 GPU 的机器，虽然可以共享控制面，但在数据面上需要一定的隔离。在节点级别，我们也经常需要修改节点维度配置以进行灰度验证，导致在同一节点上运行不同服务的 SLO 存在差异。</p><p></p><p>为解决这些问题，我们需要在业务部署时，考虑节点的不同配置对服务的影响。为此，Katalyst 针对标准交付提供了动态配置管理的能力，通过自动化的方法评估不同节点的性能和配置，并根据这些结果来选择最适合该服务的节点。</p><p></p><p></p><h2>Katalyst&nbsp;应用与案例分析</h2><p></p><p></p><p>在本章节，我们将结合字节内部的案例分享一些最佳实践。</p><p></p><h4>4.1 利用率效果</h4><p></p><p></p><p>从<a href="https://xie.infoq.cn/article/bc2bb6c1e95d4b5e21868eba8?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search"> Katalyst</a>" 实施效果上来说，基于字节内部业务的实践，我们在季度周期内，资源都可以保持在相对较高的状态；在单个集群中，每天的各个时间段内资源利用率也呈现出比较稳定的分布；同时，集群中大部分机器利用率也比较集中，我们的混合部署系统在所有节点上运行都比较稳定。</p><p></p><p><img src="https://static001.geekbang.org/infoq/84/84c1a8d4bd4ad7d8897d60466bc45619.png" /></p><p></p><p></p><h4>4.2 实践：离线无感接入</h4><p></p><p></p><p>在进入第三阶段后，我们需要对离线进行云原生化改造。改造方式主要有两种，一种是已经在 K8s 体系中的服务，我们将基于 Virtual Kubelet 的方式实现资源池的直接打通；另外一种 YARN 架构下的服务，如果直接基于 Kubernetes 体系对业务接入框架进行彻底的改造，这对于业务来说成本非常高，理论上会导致所有业务都滚动升级，这显然不是一个理想的状态。</p><p></p><p>为了解决这个问题，我们引用 Yodel 的胶水层，即业务接入仍然使用标准的 Yarn API，但在这个胶水层中，我们将与底层 K8s 语义对接，将用户对资源的请求抽象为像 Pod 或容器的描述。</p><p></p><p>这种方法使得我们在底层使用更成熟的 K8s 技术来管理资源，实现对离线的云原生化改造，同时又保证了业务的稳定性。</p><p></p><h4>4.3 实践：资源运营治理</h4><p></p><p></p><p>在混部过程中，我们需要对大数据和训练框架进行适配改造，做好各种重试、checkpoint 和分级，才能确保在我们将这些大数据和训练作业切到整个混部资源池之后，它们的使用体验不至于太差。</p><p></p><p>同时，在系统上我们需要具备完善的资源商品、业务分级、运营治理以及配额管理等方面的基础能力。如果运营做得不好，可能使得在某些高峰时段将利用率拉得很高，但在其他时段可能会出现较大的资源缺口，从而导致利用率无法达到预期。</p><p></p><p><img src="https://static001.geekbang.org/infoq/94/94a4c8c77b340339e587e653b4410ee7.png" /></p><p></p><h4>4.4 实践：极限资源效率提升</h4><p></p><p></p><p>在构建服务画像时，我们采用的是基于系统指标去做管控，但基于离线分析得到的静态系统指标无法实时跟上业务侧的变化，需要在一定时间周期内分析业务性能的变化来调整静态值。</p><p></p><p>为此，Katalyst 引入了模型来微调系统指标。例如，如果我们认为 CPU 调度延迟可能是 X 毫秒，过一段时间后，通过模型算出业务目标延迟可能是 Y 毫秒，我们就可以动态地调整该目标的值，以更好地评估业务性能。</p><p></p><p>以下图为例，完全使用静态的系统目标来进行调控，业务 P99 将处于剧烈波动状态，这意味着在非晚高峰时段，我们无法将业务资源使用压榨到更极致的状态，使其更接近业务在晚高峰时可承受的量；引入模型后，可以看到业务延迟会更加平稳，使得我们可以全天将业务的性能拉平到一个相对平稳的水平，获得资源的收益。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2b/2bfd257f197b41abf22d8320ba6ff236.png" /></p><p></p><p></p><h4>4.5 实践：解决单机问题</h4><p></p><p></p><p>在混部推进的过程中，我们会不断遇到在线和离线各种性能问题和微拓扑管理的诉求。例如，最初所有机器都是基于 <a href="https://www.infoq.cn/article/2NV2wqX8CKrAt0kpcgbz?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">cgroup V1 </a>"进行管控，然而由于 V1 的结构会使得系统需要遍历很深的目录树，消耗大量内核态 CPU。</p><p></p><p>为了解决该问题，我们在将整个集群中的节点切换到 cgroup V2 架构，使得我们能够更加高效地进行资源隔离和监控。对于推广搜等服务来说，为追求更加极致的性能，我们需要在 Socket/NUMA 级别实现更加复杂的亲和与反亲和策略等等，这些更加高阶的资源管理需求，在 Katalyst 中都可以更好地实现。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e8/e8508702627d2c779845323bf46a930f.png" /></p><p></p><h2>总结展望</h2><p></p><p></p><p>目前，Katalyst 已正式开源并发布 0.3.0 版本，后续将会持续投入更多精力进行迭代。社区将在资源隔离、流量画像、调度策略、弹性策略、异构设备管理等多方面进行能力建设和系统增强，欢迎大家关注、参与该项目并提供反馈意见。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/9VZpK1cefI2chOMcZlr6</id>
            <title>字节跳动开源 Kelemetry：面向 Kubernetes 控制面的全局追踪系统</title>
            <link>https://www.infoq.cn/article/9VZpK1cefI2chOMcZlr6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/9VZpK1cefI2chOMcZlr6</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 02:47:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Kelemetry, Kubernetes, 追踪系统, 可观测性
<br>
<br>
总结: Kelemetry是字节跳动开发的用于Kubernetes控制平面的追踪系统，它通过追踪单个Kubernetes对象的完整生命周期以及不同对象之间的相互影响，从全局视角串联起多个Kubernetes组件的行为。通过可视化K8s系统内的事件链路，Kelemetry使得Kubernetes系统更容易观测、更容易理解、更容易Debug。 </div>
                        <hr>
                    
                    <p>Kelemetry 是<a href="https://www.infoq.cn/article/AsgjeVRM8isLSzo7IXzh?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">字节跳动</a>"开发的用于 Kubernetes 控制平面的追踪系统，它从全局视角串联起多个 Kubernetes 组件的行为，追踪单个 <a href="https://mp.weixin.qq.com/s?__biz=MzI5ODQ2MzI3NQ%3D%3D&amp;chksm=eca7f1acdbd078baf17571575d642a09391401c945ddae152443ea51963b8eaca11b540c3364&amp;idx=1&amp;mid=2247502056&amp;scene=27&amp;sn=ef7a6deb9a63551cac10322b6b059aad&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">Kubernetes </a>"对象的完整生命周期以及不同对象之间的相互影响。</p><p></p><p>通过可视化 K8s 系统内的事件链路，它使得 Kubernetes 系统更容易观测、更容易理解、更容易 Debug。</p><p></p><p><img src="https://static001.geekbang.org/infoq/db/db4d1992cabf04d9d5b71732aa916335.png" /></p><p></p><h2>背景</h2><p></p><p></p><p>在传统的分布式追踪中，“追踪”通常对应于用户请求期间的内部调用。特别是当用户请求到达时，追踪会从根跨度开始，然后每个内部 <a href="https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA%3D%3D&amp;chksm=bdbec6368ac94f20a4bc7b7ca477b73dadb587d40cc4d30907e9a68f6c25d8dde1fe6a216337&amp;idx=2&amp;mid=2651012709&amp;scene=27&amp;sn=520d66662328a7b4dbb2c7a93e7f17ff&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">RPC</a>" 用会启动一个新的子跨度。由于父跨度的持续时间通常是其子跨度的超集，追踪可以直观地以树形或火焰图的形式观察，其中层次结构表示组件之间的依赖关系。</p><p></p><p>与传统的 RPC 系统相反，Kubernetes API 是异步和声明式的。为了执行操作，组件会更新 apiserver 上对象的规范（期望状态），然后其他组件会不断尝试自我纠正以达到期望的状态。</p><p></p><p>例如，当我们将 ReplicaSet 从 3 个副本扩展到 5 个副本时，我们会将 spec.replicas 字段更新为 5，rs controller 会观察到此更改，并不断创建新的 pod 对象，直到总数达到 5 个。当 kubelet 观察到其管理的节点创建了一个 pod 时，它会在其节点上生成与 pod 中的规范匹配的容器。</p><p></p><p>在此过程中，我们从未直接调用过 rs controller，rs controller 也从未直接调用过 kubelet。这意味着我们无法观察到组件之间的直接因果关系。如果在过程中删除了原始的 3 个 pod 中的一个，副本集控制器将与两个新的 pod 一起创建一个不同的 pod，我们无法将此创建与 ReplicaSet 的扩展或 pod 的删除关联起来。</p><p></p><p>因此，由于“追踪”或“跨度”的定义模糊不清，传统的基于跨度的分布式追踪模型在 Kubernetes 中几乎不适用。</p><p></p><p>过去，各个组件一直在实现自己的内部追踪，通常每个“reconcile”对应一个追踪（例如，kubelet 追踪只追踪处理单个 pod 创建/更新的同步操作）。然而，没有单一的追踪能够解释整个流程，这导致了可观察性的孤立岛，因为只有观察多个 reconcile 才能理解许多面向用户的行为；例如，扩展 ReplicaSet 的过程只能通过观察副本集控制器处理 ReplicaSet 更新或 pod 就绪更新的多个 reconcile 来推断。</p><p></p><p>为解决可观察性数据孤岛的问题，Kelemetry&nbsp;以组件无关、非侵入性的方式，收集并连接来自不同组件的信号，并以追踪的形式展示相关数据。</p><p></p><h2>设计</h2><p></p><p></p><h4>1.将对象作为跨度</h4><p></p><p></p><p>为了连接不同组件的可观察性数据，Kelemetry 采用了一种不同的方法，受 kspan 项目的启发，与将单个操作作为根跨度的尝试不同，这里为对象本身创建一个跨度，而每个在对象上发生的事件都是一个子跨度。此外，各个对象通过它们的拥有关系连接在一起，使得子对象的跨度成为父对象的子跨度。</p><p></p><p>基于此，我们得到了两个维度：树形层次结构表示对象层次结构和事件范围，而时间线表示事件顺序，通常与因果关系一致。</p><p></p><p>例如，当我们创建一个单 pod 部署时，deployment controller、rs controller 和 kubelet 之间的交互可以使用审计日志和事件的数据在单个追踪中显示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/b8/b8d64999b5f740434013fb8d12cb0551.png" /></p><p></p><p>追踪通常用于追踪持续几秒钟的短暂请求，所以追踪存储实现可能不支持具有长生命周期或包含太多跨度的追踪；包含过多跨度的追踪可能导致某些存储后端的性能问题。因此，我们通过将每个事件分到其所属的半小时时间段中，将每个追踪的持续时间限制为 30 分钟。例如，发生在 12:56 的事件将被分组到 12:30-13:00 的对象跨度中。</p><p></p><p>我们使用分布式 KV 存储来存储（集群、资源类型、命名空间、名称、字段、半小时时间戳）到相应对象创建的追踪/跨度 ID 的映射，以确保每个对象只创建一个追踪。</p><p></p><h4>2.审计日志收集</h4><p></p><p></p><p>Kelemetry 的主要数据源之一是 apiserver 的审计日志。审计日志提供了关于每个控制器操作的丰富信息，包括发起操作的客户端、涉及的对象、从接收请求到完成的准确持续时间等。在 Kubernetes 架构中，每个对象的更改会触发其相关的控制器进行协调，并导致后续对象的更改，因此观察与对象更改相关的审计日志有助于理解一系列事件中控制器之间的交互。</p><p></p><p>Kubernetes apiserver 的审计日志以两种不同的方式暴露：日志文件和&nbsp;webhook。一些云提供商实现了自己的审计日志收集方式，而在社区中配置审计日志收集的与厂商无关的方法进展甚微。为了简化自助提供的集群的部署过程，Kelemetry 提供了一个审计 webhook，用于接收原生的审计信息，也暴露了插件 API 以实现从特定厂商的消息队列中消费审计日志。</p><p></p><h4>3.Event 收集</h4><p></p><p></p><p>当 Kubernetes 控制器处理对象时，它们会发出与对象关联的“event”。当用户运行 kubectl describe 命令时，这些 event 会显示出来，通常提供了控制器处理过程的更友好的描述。例如，当调度器无法调度一个 pod 时，它会发出一个 FailToSchedulePod 事件，其中包含详细的消息：</p><p></p><p></p><blockquote>0/4022 nodes are available to run pod xxxxx: 1072 Insufficient memory, 1819 Insufficient cpu, 1930 node(s) didn't match node selector, 71 node(s) had taint {xxxxx}, that the pod didn't tolerate.</blockquote><p></p><p></p><p>由于 event 针对用于 kubectl describe 命令优化，它们并不保留每个原始事件，而是存储了最后一次记录事件的时间戳和次数。另一方面，Kelemetry 使用 Kubernetes 中的对象列表观察 API 检索事件，而该 API 仅公开 event 对象的最新版本。为了避免重复事件，Kelemetry 使用了几种启发式方法来“猜测”是否应将 event 报告为一个跨度：</p><p></p><p>持久化处理的最后一个 event 的时间戳，并在重启后忽略该时间戳之前的事件。虽然事件的接收顺序不一定有保证（由于客户端时钟偏差、控制器 — apiserver — etcd 往返的不一致延迟等原因），但这种延迟相对较小，可以消除由于控制器重启导致的大多数重复。验证 event 的 resourceVersion 是否发生了变化，避免由于重列导致的重复event。</p><p></p><h4>4.将对象状态与审计日志关联</h4><p></p><p></p><p>在研究审计日志进行故障排除时，我们最想知道的是“此请求改变了什么”，而不是“谁发起了此请求”，尤其是当各个组件的语义不清楚时。Kelemetry 运行一个控制器来监视对象的创建、更新和删除事件，并在接收到审计事件时将其与审计跨度关联起来。当 Kubernetes 对象被更新时，它的 resourceVersion 字段会更新为一个新的唯一值。这个值可以用来关联更新对应的审计日志。Kelemetry 把对象每个 resourceVersion 的 diff 和快照缓存在分布式 KV 存储中，以便稍后从审计消费者中链接，从而使每个审计日志跨度包含控制器更改的字段。</p><p></p><p>追踪 resourceVersion 还有助于识别控制器之间的 409 冲突。当客户端传递 UPDATE 请求的 resourceVersion 过旧，且其他请求是将 resourceVersion 更改时，就会发生冲突请求。Kelemetry 能够将具有相同旧资源版本的多个审计日志组合在一起，以显示与其后续冲突相关的审计请求作为相关的子跨度。</p><p>为了确保无缝可用性，该控制器使用多主选举机制，允许控制器的多个副本同时监视同一集群，以确保在控制器重新启动时不会丢失任何事件。</p><p></p><p><img src="https://static001.geekbang.org/infoq/26/26f1cf7e0f78030b03886254eb61bd8f.png" /></p><p></p><h4>5.前端追踪转换</h4><p></p><p></p><p>在传统的追踪中，跨度总是在同一个进程（通常是同一个函数）中开始和结束。因此，OTLP 等追踪协议不支持在跨度完成后对其进行修改。不幸的是，Kelemetry 不是这种情况，因为对象不是运行中的函数，并且没有专门用于启动或停止其跨度的进程。相反，Kelemetry 在创建后立即确定对象跨度，并将其他数据写入子跨度， 是以每个审计日志和事件都是一个子跨度而不是对象跨度上的日志。</p><p></p><p>然而，由于审计日志的结束时间/持续时间通常没有什么价值，因此追踪视图非常丑陋且空间效率低下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/f6/f6d756f06651a91611b69ac2358ba451.png" /></p><p></p><p>为了提高用户体验，Kelemetry 拦截在 Jaeger 查询前端和存储后端之间，将存储后端结果返回给查询前端之前，对存储后端结果执行自定义转换流水线。</p><p></p><p>Kelemetry 目前支持 4 种转换流水线：</p><p></p><p>tree：服务名/操作名等字段名简化后的原始 trace 树timeline：修剪所有嵌套的伪跨度，将所有事件跨度放在根跨度下，有效地提供审计日志tracing：非对象跨度被展平为相关对象的跨度日志</p><p></p><p><img src="https://static001.geekbang.org/infoq/5e/5ed7ff44abbbd184f84df33edd6b7051.png" /></p><p></p><p>分组：在追踪管道输出之上，为每个数据源（审计/事件）创建一个新的伪跨度。当多个组件将它们的跨度发送到 Kelemetry 时，组件所有者可以专注于自己组件的日志并轻松地交叉检查其他组件的日志。</p><p></p><p>用户可以在追踪搜索时通过设置“service name“来选择转换流水线。中间存储插件为每个追踪搜索结果生成一个新的“CacheID”，并将其与实际 TraceID 和转换管道一起存储到缓存 KV 中。当用户查看时，他们传递 CacheID，CacheID 由中间存储插件转换为实际 TraceID，并执行与 CacheID 关联的转换管道。</p><p></p><p><img src="https://static001.geekbang.org/infoq/49/4907b57d081885c707ca102ddb53d2ed.png" /></p><p></p><h4>6.突破时长限制</h4><p></p><p></p><p>如上所述，追踪不能无限增长，因为它可能会导致某些存储后端出现问题。相反，我们每 30 分钟开始一个新的追踪。这会导致用户体验混乱，因为在 12:28 开始滚动的部署追踪会在 12:30 突然终止，用户必须在 12:30 手动跳转到下一个追踪才能继续查看追踪。</p><p></p><p>为了避免这种认知开销，Kelemetry 存储插件在搜索追踪时识别具有相同对象标签的跨度，并将它们与相同的缓存 ID 以及用户指定的搜索时间范围一起存储。在渲染 span 时，所有相关的轨迹都合并在一起，具有相同对象标签的对象 span 被删除重复，它们的子对象被合并。轨迹搜索时间范围成为轨迹的剪切范围，将对象组的完整故事显示为单个轨迹。</p><p></p><h4>7.多集群支持</h4><p></p><p></p><p>可以部署 Kelemetry 来监视来自多个集群的事件。在字节跳动，Kelemetry 每天创建 80 亿个跨度（不包括伪跨度；使用多 raft 缓存后端而不是 etcd）。对象可以链接到来自不同集群的父对象，以启用对跨集群组件的追踪。</p><p></p><h2>未来增强</h2><p></p><p></p><h4>1.采用自定义追踪源</h4><p></p><p></p><p>为了真正连接 K8s 生态系统中的所有观测点，审计和事件并不足够全面。Kelemetry 将从现有组件收集追踪，并将其集成到 Kelemetry 追踪系统中，以提供对整个系统的统一和专业化视图。</p><p></p><h4>2.批量分析</h4><p></p><p></p><p>通过 Kelemetry 的聚合追踪，回答诸如“从部署升级到首次拉取镜像的进展需要多长时间”等问题变得更加容易，但我们仍然缺乏在大规模上聚合这些指标以提供整体性能洞察的能力。通过每隔半小时分析 Kelemetry 的追踪输出，我们可以识别一系列跨度中的模式，并将其关联为不同的场景。</p><p></p><h2>使用案例</h2><p></p><p></p><h4>1. replicaset controller 异常</h4><p></p><p></p><p>用户报告，一个 deployment 不断创建新的 Pod。我们可以通过 deployment 名称快速查找其 Kelemetry 追踪，分析 replicaset 与其创建的 Pod 之间的关系。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2c/2c97bb74029ca6a6da856f1ae10da9d1.png" /></p><p></p><p>从追踪可见几个关键点：</p><p>Replicaset-controller 发出&nbsp;SuccessfulCreate&nbsp;事件，表示 Pod 创建请求成功返回，并在 replicaset reconcile 中得到了 replicaset controller 的确认。没有&nbsp;replicaset&nbsp;状态更新事件，这意味着&nbsp;replicaset controller&nbsp;中的&nbsp;Pod reconcile&nbsp;未能更新&nbsp;replicaset&nbsp;状态或未观察到这些&nbsp;Pod。</p><p></p><p>此外，查看其中一个 Pod 的追踪：</p><p></p><p><img src="https://static001.geekbang.org/infoq/68/681b330669ecbef35076a4bd8ae49625.png" /></p><p></p><p>Replicaset controller 在 Pod 创建后再也没有与该 Pod 进行交互，甚至没有失败的更新请求。</p><p></p><p>因此，我们可以得出结论，replicaset controller 中的 Pod 缓存很可能与 apiserver 上的实际 Pod 存储不一致，我们应该考虑 pod informer 的性能或一致性问题。如果没有 Kelemetry，定位此问题将涉及查看多个 apiserver 实例的各个 Pod 的审计日志。</p><p></p><h4>2.浮动的 minReadySeconds</h4><p></p><p></p><p>用户发现 deployment 的滚动更新非常缓慢，从 14:00 到 18:00 花费了几个小时。如果不使用 Kelemetry，通过使用 kubectl 查找对象，发现 minReadySeconds 字段设置为 10，所以长时间的滚动更新时间是不符合预期的。kube-controller-manager 的日志显示，在一个小时后 Pod 才变为 Ready 状态</p><p></p><p><img src="https://static001.geekbang.org/infoq/7d/7d1c4dff024fe46343fe4da131bf0b5a.png" /></p><p></p><p>进一步查看 kube-controller-manager 的日志后发现，在某个时刻 minReadySeconds 的值为 3600。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8b/8b77d4ed7afdf84b068a5a25d633b157.png" /></p><p></p><p>如果使用 Kelemetry 进行调试，我们可以直接通过 deployment 名称查找追踪，并发现 federation 组件增加了 minReadySeconds 的值。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4a/4a80f8e4fb2b8e8abfec3b66384c46af.png" /></p><p></p><p>后来，deployment controller 将该值恢复为 10。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f2/f2df40bee17f5a6535ff8fc48c904b19.png" /></p><p></p><p>因此，我们可以得出结论，问题是由用户在滚动更新过程中临时注入的较大 minReadySeconds 值引起的。通过检视对象 diff ，可以轻松识别由非预期中间状态引起的问题。</p><p></p><h2>尝试 Kelemetry</h2><p></p><p></p><p>目前，Kelemetry 已在 GitHub 上开源：github.com/kubewharf/kelemetry</p><p></p><p>感兴趣的开发者可以按照 docs/QUICK_START.md 快速入门指南，亲自体验 Kelemetry 如何与组件进行交互。如果你不想设置一个集群，也可以查看从 GitHub CI 流水线构建的在线预览：kubewharf.io/kelemetry/trace-deployment/。</p><p></p><p>我们期待有更多朋友关注与加入社区，如果大家在试用过程中发现了一些问题，也欢迎大家提出 issue 给我们反馈！</p><p></p><p><img src="https://static001.geekbang.org/infoq/cd/cd9e76b489cb86e1f34493f5f83f2f4b.jpeg" /></p><p>扫描二维码，添加小助手</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/8WCa8vc3wjxuod6gXhxh</id>
            <title>字节跳动开源 Katalyst：在离线混部调度，成本优化升级</title>
            <link>https://www.infoq.cn/article/8WCa8vc3wjxuod6gXhxh</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/8WCa8vc3wjxuod6gXhxh</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 02:36:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 互联网应用, 资源使用情况, 潮汐特性, 削峰填谷
<br>
<br>
总结: 当下互联网应用的资源使用情况具有潮汐特性，为了节约成本和保证业务稳定性，可以通过削峰填谷的方式，将闲置资源出让给优先级低的服务，并在需要时及时归还。 </div>
                        <hr>
                    
                    <p>当下互联网应用以天为单位，在线业务的资源使用情况往往会随着访问数量的波动而变化，具备明显的潮汐特性。为了确保业务稳定性，业务方往往会参考高峰时段的资源使用情况来申请资源，但这部分资源在低峰时段容易被闲置。</p><p></p><p>如果可以把这些闲置资源暂时出让给优先级低的服务，当在线业务需要使用的时候及时将资源归还，形成在离线服务混部，就可以达到削峰填谷，节约成本的效果。</p><p></p><p></p><h2>字节跳动云原生混部实践</h2><p></p><p></p><p><a href="https://www.infoq.cn/article/AsgjeVRM8isLSzo7IXzh?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">字节跳动</a>"业务规模庞大、业务类型多元，其中涵盖了包括微服务、推广搜服务、机器学习与大数据、存储在内的多种业务类型。通常来说，不同业务类型对底层基础设施会有不同的资源管理诉求，传统的管理模式是基于业务线或者服务类型切分资源池，实现定制化需求。</p><p></p><p>但切分资源池的做法容易形成资源孤岛，无法实现资源层面的灵活拆借，不利于全局资源利用效率的提升和业务成本的优化，加重集群运维的负担。</p><p></p><p>此外，由于不同类型业务的 SLO 要求、资源潮汐特性存在互补，基础设施团队期望充分利用这些特性，通过调度和管控等手段去优化资源效率，实现资源池的融合统一，帮助业务团队获得更低的资源成本和更强的弹性能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/eacec4cca090bbb206becadfb88aeaac.png" /></p><p></p><p>为实现资源统一托管，字节跳动从 2016 年就开始基于<a href="https://mp.weixin.qq.com/s?__biz=MzI5ODQ2MzI3NQ%3D%3D&amp;chksm=eca7f1acdbd078baf17571575d642a09391401c945ddae152443ea51963b8eaca11b540c3364&amp;idx=1&amp;mid=2247502056&amp;scene=27&amp;sn=ef7a6deb9a63551cac10322b6b059aad&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect"> Kubernetes </a>"构建统一的基础设施。</p><p></p><p>到现阶段，字节内部已经基本完成全量微服务、推广搜服务以及大部分机器学习与大数据业务的云原生化改造。在此过程中，基础架构团队持续探索统一资源池下的资源优化手段，并逐渐形成了 “弹性伸缩” 和 “常态混部” 互相配合的资源池混部方案。</p><p></p><p>弹性伸缩：实现机器级别、<a href="https://mp.weixin.qq.com/s?__biz=MzIwMzY1OTU1NQ%3D%3D&amp;chksm=96ceb6bca1b93faa5de5fcc563ec907c7b14c920809f6ff563e1696cd3e857c6161776352915&amp;idx=2&amp;mid=2247497456&amp;scene=27&amp;sn=cac27bb9cfb119a5225371886a296147&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">Numa</a>" 级别的资源分时复用，结合业务指标和系统指标，共同指导业务实例的横向和纵向扩缩容策略，最终使得离线类服务以更加低廉的价格购买更多闲时资源，在线类服务以更加高昂的价格购买更多峰时资源，通过资源市场化运营的方式实现综合效率的提升。混合部署：提供资源超卖的能力，充分利用集群中 “已经售卖但未充分使用的资源” 部署更多低优业务，同时在系统侧完善 CPU、内存、磁盘、网络等多维度的资源隔离机制，并且智能预测、感知各类服务的负载变化，结合服务的分级机制，通过分钟级的指标感知和调控策略，保证服务的稳定性。</p><p>该方案在链路上基于 Kubernetes 和 Yarn 两套体系实现联合管控，在单机上同时运行 Kubernetes 和&nbsp;Yarn&nbsp;的管控组件，配合中心协调组件对两套系统可见的资源量进行分配。在联合管控系统之上，团队基于服务资源画像实现实时的资源预估，在保证各类服务 SLA 要求的前提下，实现更加灵活和动态的资源分配。</p><p></p><p>在该资源池混部方案落地实践的过程中，基础设施团队完成了资源并池可行性的验证，完成了混部基础能力的构建，并且在部分核心业务集群实现了整机天级利用率从&nbsp;23%&nbsp;到&nbsp;60%&nbsp;的提升。</p><p></p><p><img src="https://static001.geekbang.org/infoq/10/103c99cf7f723241327e2e0e28c69ca5.jpeg" /></p><p></p><p></p><h2>Katalyst：从内部验证到开源</h2><p></p><p></p><p>在经历内部抖音、今日头条等大规模潮汐流量业务验证后，字节跳动的云原生混部实践已日臻完善。</p><p></p><p>为了帮助更多人了解大规模资源混部实践的工作原理，方便更多开发者用户体验这种开箱即用、一键式部署的资源管控能力，研发团队决定回馈社区，采用 Kubernetes Native 的方式重构并增强了资源管控系统的实现，提炼出资源管控系统&nbsp;Katalyst&nbsp;并正式开源。</p><p></p><p>Katalyst 引申自单词 catalyst (音&nbsp;[ˈkætəlɪst])，本意为催化剂。首字母修改为 K，寓意该系统能够为所有运行在 Kubernetes 体系中的负载提供更加强劲的自动化资源管理能力。</p><p></p><h4>什么是 Katalyst</h4><p></p><p></p><p>Katalyst 脱胎于字节跳动混部技术实践，同时也从资源、管控、调度等多个维度对资源管控能力进行了扩展和补充。它的主要特点包括：</p><p></p><p>完全孵化于超大规模混部实践，并在字节服务云原生化的进程中同步接管资源管控链路，真正实现内外技术体系的复用搭载字节跳动内部的 Kubernetes 发行版 Enhanced Kubernetes&nbsp;同步开源，兼容性好，体验更多字节自研的核心功能系统基于插件化模式构建，用户可以在 Katalyst Framework 之上自定制各类调度、管控、策略、数据等模块插件提供一键式部署模版和详尽运维手册，降低外部用户的理解和接入使用成本</p><p></p><h4>Katalyst 如何实现资源抽象</h4><p></p><p></p><p>在资源层，Kubernetes 原生 QoS 分级无法满足大规模生产环境的要求，Katalyst 在此基础上进行了进一步的抽象。</p><p></p><p>Katalyst 以 CPU 为主维度，为应用提供了 system_core 系统核、dedicated_core 独占核、shared_core 共享核、reclaimed_core 回收核等多种不同等级，同时每种等级又辅助以多种 enhancement 机制（例如是否需要 numa node 绑定，是否需要网卡亲和或者带宽限制等），实现差异化的资源分配和管控策略。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f7/f71230f305f46d0df4e67c511a34d8ae.png" /></p><p></p><p>通过抽象资源模型，Katalyst 为用户提供了统一的资源入口，用户根据实际业务需求，将业务服务映射到对应的 QoS 和售卖模式上准确地表达自身的需求，最终实现从统一的资源池获取资源而不用关注底层资源池细节。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bda96be8e6b52a9ac1aca992f8b3de17.jpeg" /></p><p></p><p></p><h4>Katalyst&nbsp;的架构设计</h4><p></p><p></p><p>早期的混部架构存在几方面的问题：Kubernetes 和 Yarn 两套系统的联合管控虽然实现了在离线业务的常态混部，但是复杂的系统也使得维护成本变高。</p><p></p><p>另外这种架构也带来了额外的资源损耗，这些损耗一方面来自于联合管控模式下单机 Agent 组件资源占用，尤其在超大规模的集群中，这部分资源非常可观。此外，由于两套管控导致系统复杂度变高，系统交互过程中会产生多级资源漏斗，任何环节的异常都会导致资源丢失。</p><p></p><p>在 Katalyst 中，我们对整体的混部架构做了优化重构：</p><p></p><p><img src="https://static001.geekbang.org/infoq/55/5550deec101cf3a61bdafd496e1d274d.png" /></p><p></p><p>在管控层，我们将字节早期基于 Kubernetes 和 Yarn 两套体系的融合系统整合成一套基于 Kubernetes 的系统。</p><p></p><p>具体来说，我们在接入层同时保留了 Kubernetes 以及 Yarn 各自的 API 入口，底层系统的元数据管理和资源管控实现则统一收敛到基于 Kubernetes 的管控系统 Katalyst 上。</p><p></p><p>在调度层， Katalyst 在统一元数据的基础上实现了 “中心” 和 “单机” 互相协调的资源调度机制。</p><p></p><p>在单机调度侧：Katalyst 搭载 Enhanced Kubernetes 里的扩展模块 QoS Resource Manager (QRM) 能够实现可插件化的微拓扑亲和性分配，并通过自定 CRD 将微拓扑上报到中心打通调度流程；在服务运行过程中，Katalyst 会持续观察 Pod 运行时系统指标，集合业务 QoS 要求和业务指标反馈进行预估，决策出 Pod 在各个资源维度上的分配量，通过 QRM reconcile 机制实时下发到 CRI。上述过程中的资源预估模型和 QRM 实现，都可以通过插件化的方式定制，使得资源调控的策略更加匹配不同业务场景的诉求。</p><p></p><p>在中心调度侧：Katalyst 基于原生 Scheduler Framework 扩展了更加丰富的调度能力，在调度过程中同时考虑不同 QoS 业务在同一个集群中运行时资源层应该如何分配及协作，配合单机调度实现更加细粒度的调度语义要求；同时，中心调度还会结合业务容器运行时的实时数据和服务画像，在全集群范围内实现动态的 Rebalance 策略，降低集群空置率，提升业务稳定性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d10908b4dd2ac433f598eb11c15c9012.jpeg" /></p><p></p><p>通过收敛资源管控，Katalyst 有效减少了系统运行中的资源损耗。此外，在联合管控体系下，Katalyst 将旁路异步管控的实现切换成基于标准 CRI 接口的同步管控实现，避免因为各种异步引发的 race 或者冲突，从而引发服务性能抖动。</p><p></p><p>最后，在一套管控体系下，我们还能够充分享受 Kubernetes 面向 API 设计的优势，通过自定义 CRD 的方式去解耦内部系统，泛化管控策略，使得系统能够通过插件化的方式更加灵活地系统迭代，真正实现了内外同源。</p><p></p><p></p><h2>RoadMap</h2><p></p><p></p><p>Katalyst 作为一个资源管理系统，在离线混部是其核心应用场景之一。除了抽象上述核心概念之外，我们还为 Katalyst 提供和规划了丰富的 QoS 能力：</p><p></p><p>精细化的资源出让策略：Katalyst 支持基于静态启发式、无监督算法、QoS Aware 的多种资源预估策略，更准确的计算和预测节点可出让资源量，进一步提高资源利用率。多维度的资源隔离能力：基于 cgroup, rdt, iocost, tc 等能力，实现不同混部场景中对 cpu，内存，磁盘，网络等多种资源的有效隔离，保障在线业务的 QoS 不受影响。多层级的负载驱逐策略：支持基于多种指标，多层级的驱逐策略，在保障在线业务 QoS 的同时也尽可能提高离线业务的 QoS。</p><p>除了混部场景，Katalyst 也提供一些增强的资源管理能力：</p><p>资源弹性管理：提供灵活可扩展的 HPA/VPA 资源弹性策略，帮助用户提高部署率和资源利用率。微拓扑感知调度：感知节点上 CPU，内存，异构设备的微拓扑，基于更细粒度的微拓扑信息完成资源分配，满足高性能业务的 QoS 要求。</p><p></p><p>详细的功能规划请参考 &nbsp;roadmap                                       （https://github.com/kubewharf/katalyst-core/blob/main/docs/roadmap.md）。</p><p></p><p>虽然混部技术在字节内部已经经历了几次的技术迭代，但是一个通用、标准化的平台底座必然要经过各种场景的打磨，我们非常期待更多朋友加入到 Katalyst 开源社区中！</p><p></p><p>项目地址：github.com/kubewharf/katalyst-core</p><p></p><p><img src="https://static001.geekbang.org/infoq/c3/c31da82a63f8db5f4f010fbdca49637b.jpeg" /></p><p>扫描二维码加入群聊</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/LAvHdllNrSpdZlsAk9x4</id>
            <title>没必要非得固守纯向量数据库！专访亚马逊云科技数据库和迁移副总裁Jeff Carter</title>
            <link>https://www.infoq.cn/article/LAvHdllNrSpdZlsAk9x4</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/LAvHdllNrSpdZlsAk9x4</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 01:59:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 生成式AI, 向量数据库, 数据库服务, 数据库产品
<br>
<br>
总结: 亚马逊云科技在数据库产品上推出了多种服务，包括生成式AI和向量数据库。他们认为向量搜索是数据库的核心功能之一。他们的规划是提供多种数据库选项，以满足不同客户的需求。他们还注重改善客户体验和帮助客户构建自己的应用程序。 </div>
                        <hr>
                    
                    <p>采访 ｜ Kevin</p><p>编辑 ｜Tina、芳芳</p><p>&nbsp;</p><p>生成式AI时代的到来催生了向量数据库日益增长的需求和应用。亚马逊云科技也在多种数据库服务上实现向量搜索功能，并且他们也认为这是任何数据库都应当具备的一项核心功能。那亚马逊云科技在数据库产品上有什么样的规划、他们如何看待纯向量数据库需求？针对上述问题，近期在re:Invent现场，InfoQ采访了亚马逊云科技数据库和迁移副总裁Jeff Carter。</p><p>&nbsp;</p><p>Jeff Carter主要负责亚马逊云科技的关系数据库、非关系数据库和迁移方面的十几种服务。在加入亚马逊云科技之前，任职于Amazon.com网站，曾将亚马逊的原有Oracle分析环境迁移至亚马逊云科技，并将所有事务处理引擎从遗留技术栈中迁移至亚马逊云科技。Jeff 还担任过几年首席信息安全官，在加入亚马逊之前，曾在Tera Data工作过30年。</p><p>&nbsp;</p><p>以下为访谈实录，经过不改变原意的整理：</p><p>&nbsp;</p><p>InfoQ：针对当前技术趋势，亚马逊科技在数据库技术方面的规划是什么样的，包含了什么样的架构和关键产品？</p><p>&nbsp;</p><p>Jeff Carter：先从技术框架说起。在框架的最底层，我们拥有一套全面的数据库集合。在操作型数据库方面，我们之前提供15种不同服务，但到本周结束时服务数量会增加到17种。很多客户都问我为什么要有这么多服务。答案很简单，就是人确保客户在考虑使用亚马逊云科技时，能在商店中找到符合需求的充足数据库选项。</p><p>&nbsp;</p><p>所以我们一直在努力推出更多方案。除此之外，客户对于未来两到三年的发展肯定也设有愿景。有些愿景可能需要现有技术，也有一些可能依赖于新的技术。因此我们也会尝试提供能支撑未来需求的新技术。总之，必须密切关注客户的期望，而且这种期望是当下与未来的综合体。如我所说，到本周末我们将拥有17种不同的数据库服务，有些是关系数据库、也有些是非关系数据库。</p><p>&nbsp;</p><p>以非关系数据库为例，比如我们即将发布的新方案，我称之为操作型数据库。但实际上，它的应用更偏重于分析。还有Neptune，我们的老牌图数据库。除了Neptune之外，我们的图数据库阵容还会进一步扩展。而对于现有数据库，特别是关注操作型用例时，我们也将更多强调分析方面的应用。比方说，图领域中的分析。以社交图谱为例，对社交图谱的操作能显示当前用户有哪些联系人，并列出前十位，如果需要也可以列出几千位。Facebook就属于操作型案例。但有时候，大家可能希望查询谁在网络上的影响力最大，这往往就需要所谓全表扫描。不过毫无疑问，我们当然不希望把全表扫描当作操作型负载，事务数据库也不擅长执行这类操作。所以分析应该在内存内进行，这样速度会非常快。这是一种持久设计，而且可以基于我们已经在使用的相同技术，比如DynamoDB和MemoryDB，只是分析会在内存内实现。</p><p>&nbsp;</p><p>之后是核心层，本周之后我们将拥有17种不同的数据库。接下来最重要的就是如何集成数据，因为并不是每个人都想面对这么多不同的资产。所以我们希望在各种资产中建立起重要的共性，也就是零ETL。作为一项基本目标，现在我们已经围绕它建立起诸多案例，而我们真正关心的就是如何让数据无缝、顺畅地从事务引擎转移至数据仓库或者数据湖。而零ETL，就是实现这种无缝移动的关键。</p><p>&nbsp;</p><p>在我们执行插入、更新、删除等标准数据库操作时，数据其实就开始了流通和变化。数据要么进入RedShift，要么移动到使用端。接下来是把数据湖治理好。因此，我们最近才公布了Data Zone数据区。这样大家就能找到环境中的数据，我们也可以为数据创建权限组。用户可以为权限组指定成员，再把权限组分配给权限集。而且无论大家具体使用什么分析引擎，这些权限都能正确起效。&nbsp;</p><p>&nbsp;</p><p>最后是生成式AI，这里我要讨论两种生成式AI的应用形式。第一，我们要采取哪些措施来改善客户体验？第二，我们要如何帮助客户构建他们自己的应用程序？而改善客户体验的案例，就是我们本周发布的公告之一：Q。在本届re:Invent大会之前，亚马逊也有名叫Q的产品，就是QuickSight Q，主要通过自然语言处理建立仪表板和报告。这项功能现在仍然存在，但新的Amazon Q属于独立品牌，涵盖亚马逊云科技内部利用生成式AI增强客户体验的所有成果。我们这次着力宣传的一个例子就是：我们把所有用户文档同大语言模型相结合，这样用户就能随意用自然语言询问相关问题，Amazon Q则会根据文档信息给出建议和相应的详尽操作步骤。</p><p>&nbsp;</p><p>这样文档的交互性就更好了。不只是在搜索中使用生成式AI，我们还用Aurora在RDS领域做过类似的尝试。我们还开发了DevOps Guru for RDS，借此查看数据库中是否存在性能异常，并提前向客户发出提醒。我们还与负责GuardDuty的安全团队合作，随时监控那些指向您数据库的登录行为。</p><p>&nbsp;</p><p>一旦它发现异常，就会主动发出提醒。具体可能是有人在反复尝试密码，也可能是来自安全部门已知暗网或恶意IP地址的一次登录操作。哪怕只发生一次，它也能牢记在心。这就是我们利用生成式AI服务帮助客户的三个简单案例。我们还投入大量资金，希望帮助客户们取得更大的成功，而这方面成果就是Bedrock。在Bedrock之下，我们还推出了PartyRock等服务。如果大家还没试过，我强烈建议您马上体验。它非常简单也非常有趣，总的来说这就是一大堆不同大语言模型的集合。我们坚信至少就目前来看，还没有哪种单一模型能够证实世界，必须要借助多种不同的方法和模型，发挥它们各自擅长的专业知识。</p><p>&nbsp;</p><p>比如说一种模型可能擅长编辑图片，而另一种模型可能擅长编排音乐，第三种模型则擅长修改文本或者文字润色。它们各有自己的关注取向。因此，我们希望保证客户能轻松找到、并选择最适合自身需求的模型。我们当然支持多种模型，但不同用户对模型有不同的要求，毕竟大家的业务也是独一无二的。你可能需要添加本地数据，这个过程被称为检索增强生成，简称rag。</p><p>&nbsp;</p><p>使用时，大家可以指定共享驱动器，指定要启用的大模型，指定支持vss向量相似性搜索的数据库。数据库可以是Pincone向量数据库，也可以是Redis，或者我们支持的七种数据库中的任何一种。我们还在扩展，目前已经有七种不同数据库选项，包括OpenSearch、RDS Postgres、Aurora Postgres、MemoryDB等等，未来还会有更多。指定完成之后，点击开始，它就会读取文档，把文档拆分成块，用你选定的大语言模型将其转为向量、创建向量嵌入。之后则把它们放进支持vss的数据库，再把大模型和vss数据库组合起来，就能回答你提出的问题了。</p><p>&nbsp;</p><p>现在我们的模型已经掌握了这些来自数据存储的特定业务知识。在交互过程中，所有的知识都圆融一体，可供你随时选择并交付给客户。现在我们就能把大语言模型跟向量存储这套组合一并交给客户了。如果愿意，也可以只提供给内部员工。总之，大家可以随意挑选用户，指定他们能跟文档中的哪些部分进行交互。文档可以是任何形式，比如说网页或者PDF等等。总之我们提交一个文档，把它转换成向量。之后大模型就能识别这些数据，避免手动对程序进行处理。单靠Bedrock就能实现全面自动化。</p><p>&nbsp;</p><p>当然，本届re:Invent大会上还有很多其他案例。总之我们团队一直在与Bedrock团队密切合作，共同实现向量搜索功能，我们也认为这是任何数据库都应当具备的一项核心功能。</p><p>&nbsp;</p><p>InfoQ：我们要怎么判断哪种数据库最具性价比呢？</p><p>&nbsp;</p><p>Jeff Carter：我们可以在模型中设定多种参数，比如说召回率。整个所谓向量相似性搜索的过程，就是先提取数据、再立足多个维度创建数字，也就是说每个文档可能拥有20、30或者40个不同的维度。然后在这40个维度上，vss的作用就是在不同的维度间寻找最近邻。这就是我们想要向核心数据库中添加的功能，即快速执行vss查找的能力。这就是召回率，它是个介于0和1之间的数字。召回率越高，得出的答案质量越好，但也会消耗更多算力。你可以选择90%的召回率，也可以选择99%的召回率。根据选择召回率的不同，各种数据库的性价比也有差异。</p><p>&nbsp;</p><p>我觉得对于大多数用例，Aurora Postgres都表现出色，另外OpenSearch也很不错。但如果想要极高的召回率，那么作为最佳配伍，我觉得MemoryDB的性能表现最为极致。因为它会把所有数据都存放在内存内，而不必多次访问磁盘。</p><p>&nbsp;</p><p>InfoQ：亚马逊云科技今天公布了好几项关于零ETL的产品。我很好奇，这是不是代表随着越来越多的零ETL产品面世，不久的未来ETL就会彻底消失了？你怎么看这个问题？</p><p>&nbsp;</p><p>Jeff Carter：根据个人经验，我还是更关注消费者的感受这部分。ETL其实分为两层，其一就是从事务引擎中获取基础数据并放入数据环境，而零ETL其实实现的就是对这一层的自动化。而对基础数据进行业务层级转换以建立更高级别的业务组，即T的部分，则仍然要用到Glue或者第三方工具才能建立起更高级别的业务领域。从Amazon.com的角度来看，前一个级别的实例就是配送中心库存。核对我们配送中心里的每种产品还有多少库存，再把这些数据转移到数据湖中，这就是零ETL起效的部分。但要想把所有配送中心都整合起来，把全局数据显示在网站上，那就需要更多的T层，要用到Glue之类的工具。</p><p>&nbsp;</p><p>ETL通常是向数据仓库和数据湖读取和写入数据，但如果愿意，也可以使用Glue访问不同的数据库以获取信息。在亚马逊云科技中，当我们谈到数据仓库时，通常是指RedShift。而Glue能跟RedShift无缝对接。至于说数据湖，我们主要是指Lake Formation，还有EMR和Athena等其他几种项目。Redshift是一种作为数据仓库的并行列式数据库。</p><p>&nbsp;</p><p>那么未来，是不是人们会更多把数据传送到数据湖中？而不再大量使用列式数据库那样的数据仓库？我觉得这两种情况都会存在，具体取决于查询的大小、类型还有表的类型，不同的场景对应不同的方法。但我觉得从长远来看，我们目前正在研发、但还没有公布的很多技术应该能发挥创造性的作用，真正把这两种环境联系在一起。</p><p>&nbsp;</p><p>InfoQ：大家都知道，向量数据库领域的竞争非常激烈。在来这里之前，我跟中国技术社区的人们进行过很多交流，包括跟向量数据库相关的线上讨论。有些专家认为，搭配大语言模型的长期记忆组件不应该是单纯的向量数据库。所以作为亚马逊云科技的数据库和迁移服务负责人，你如何看待向量数据库的发展方向？未来几年又可能出现哪些潜在的挑战和机遇？</p><p>&nbsp;</p><p>Jeff Carter：首先，我们希望通过亚马逊云科技为客户提供更多选择。我之前用两家公司举了例子，当然还有很多其他案例。首先就是Pincone，他们作为纯向量数据库的代表。另外Redis Labs也有能支持vss的版本。只要客户愿意，我们非常乐意支持这些产品以供免费使用。</p><p>&nbsp;</p><p>他们对这些方案肯定都有自己的判断，而我们很高兴能支持他们的实际选择。但我一直觉得，没必要非得固守纯粹的向量数据库。正因为如此，我们才取其核心技术并融入其他方案，努力在不同技术之间做出权衡。这样客户就能做出最符合业务需求的明智选择。</p><p>&nbsp;</p><p>现在形势一直在快速变化，当下我们给出的答案未来都可能变成错误答案，比如6个月之后情况可能会大为不同。甚至未来3个月都可能出现变化。我相信所有企业都希望能用自己的业务信息来扩展基础模型。相信也会有企业愿意花几个月时间自行训练吧，只是成本会高得多。</p><p>&nbsp;</p><p>我觉得微调和检索增强生成（rag）的边界比较模糊，二者的适用范围也有交集。总之虽然大语言模型的表现令人惊叹，但还远称不上完美。</p><p>&nbsp;</p><p>InfoQ：我们都知道数据库技术已经诞生半个世纪了，在这样的老古董上搞创新也变得越来越艰难。那亚马逊云科技是如何在数据库技术领域不断创新的？近年来，您认为亚马逊在数据库领域取得的最重大、最显著的技术进步是什么？</p><p>&nbsp;</p><p>Jeff Carter：这是个好问题。首先，我们每年都会对所有产品进行创新，并投入大量时间跟客户和社区成员进行交流，了解客户在使用现有产品时遇到过哪些问题，并尝试做出改进。</p><p>&nbsp;</p><p>但也有很多比较长远的问题，例如如何适应未来几年可能出现的趋势、提前做好准备。我们目前开发的项目可能要到一年、两年或者三年之后发布，当然希望它们在亮相时仍然具有变革性。请原谅我无法透露目前的工作内容，但我想强调的是，我们会同时考虑短期和长期问题，考虑怎么把事情做好。就以生成式AI为例，我们已经意识到这是一项变革性的技术，而这样的技术可能每十年都会出现一次。我们内部已经在努力转向，讨论之前的哪些成果能与之对接，并且公开表态将积极向着这条路线推进。我们会始终保持旺盛的创新动力，并真正把心力投入到有希望的特定领域当中。</p><p>&nbsp;</p><p>所以像Bedrock、Titan还有PartyRock，这些都是我们能在相对较短的时间里拿出的现实成果。我们是一家专注于机器学习和AI的公司，我随随便便就能举出十几个在消费级业务领域应用机器学习的例子，比如利用机器学习改造搜索功能，借此在所有配送中心内建立起智能化的补货系统。这样的案例可以说数不胜数。</p><p>&nbsp;</p><p>而现在，我们又把目光投向了生成式AI，希望大家都能感受到我们严肃的态度。至于生成式AI方面的用例，我觉得不同的人可能会有不同的看法。生成式AI最神奇的能力就在于处理自然语言。是的，就像前面提到，它能阅读文档，再根据读取过的内容正确回答问题，相当于将语言承载的完整历史都纳入了模型自身。这真的太酷了。我相信肯定会有很多有趣的应用出现。</p><p>&nbsp;</p><p>InfoQ：2023年即将过去，如果用3个关键词来描述这一年来的数据库领域，你会选择哪3个？</p><p>&nbsp;</p><p>Jeff Carter：第一个词很简单，降本。第二个是生成式AI。第三个是集成或者说整合。过去18个月以来，人们一直在努力寻求能够降本增效的方法，亚马逊云科技只是其中之一。这也代表着这段时间以来的一大趋势。每一次交流，对方都会强调降本和AI，而且人们迫切需要更简单的实现方式。而要想实现二者，整合就是关键所在。</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/TavkpylwzkTM7924wE8y</id>
            <title>抖音直播电商背后的技术实践：画面更高清、网络更低延迟、玩法儿更多</title>
            <link>https://www.infoq.cn/article/TavkpylwzkTM7924wE8y</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/TavkpylwzkTM7924wE8y</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 04:02:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 双十一, 抖音电商, 直播电商, 火山引擎
<br>
<br>
总结: 2023年的双十一活动中，抖音电商通过与火山引擎合作，提供更高清、更沉浸、更互动的直播体验和新玩法，取得了巨大成功。通过火山引擎的自适应展示策略和内容自适应编码，直播画面更高清，商品细节更清晰可见。同时，通过降低直播延迟，消费者与商家的互动更加实时，提高了购买决策的效率。这些创新措施使得抖音电商成为双十一的赢家之一。 </div>
                        <hr>
                    
                    <p>2023 年的“双十一”收官，品牌商家在这场“年终大考”中，纷纷交出了满意的“答卷”。</p><p></p><p>根据抖音发布的《玩法阶段战报》显示，在 2023 年双 11 好物节期间，抖音官方的立减玩法让商品的总计曝光达 1,341 亿；抖音平台的新商家数量同比增长 91%；抖音商城的消费人数同比增长 111%。从双十一的直播数据来看，10 月 20 日至 11 月 11 日，抖音电商里的直播间累计直播时长达 5,827 万小时，挂购物车的短视频播放量达 1,697 亿次。</p><p></p><p>视频化时代，直播电商的兴起改变了消费者的习惯和品牌商家的商业模式。但对于抖音电商来说，如何在“<a href="https://www.infoq.cn/article/d2vXbzccCxM1Ni5VRpGT?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">双十一</a>"”这样的“流量大考”中，为消费者提供更高清、更沉浸、更互动的直播体验和新玩法，从而驱动品牌的业务增长？这是<a href="https://mp.weixin.qq.com/s?__biz=MzA4NTU2MTg3MQ%3D%3D&amp;chksm=84605d6bb317d47de854b51c767168481dd2859f0fe563f39c9a6c34ad082b846dfce4e14c11&amp;idx=3&amp;mid=2655189515&amp;scene=27&amp;sn=06a95fea7b2120b93c4bf3ffea3d3079&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">抖音电商</a>"首要解决的命题之一。</p><p></p><h2>一、直播画面更高清，让商品细节纤毫毕现</h2><p></p><p></p><p>非专业的直播设备让商品展示时的画面变得模糊，进而影响消费者的购买决策。抖音电商携手<a href="https://www.infoq.cn/article/VhJrF0rKTF1fDRGlTcdo?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">火山引擎</a>"打造了不同产品特性的定制化展示策略，通过自适应的方式做到场景化“满分”展示：</p><p></p><p>利用画质分析层对 ROI（Region Of Interest）、信号复杂度与基础画质进行特征分析；预测层接收特征，对场景进行分类，然后对画质 / 码率进行预测；通过决策算法，基于当前场景的目标画质、网络分析，决策出更优参数集合；最后将画质指标反馈给预测算法进行调优。</p><p></p><p><img src="https://static001.geekbang.org/infoq/93/93e56abe87d61c10d855ee4632cd9e46.png" /></p><p>图：火山引擎内容自适应策略</p><p></p><p>火山引擎的人眼感兴趣区域检测（ROI 检测），通过深度的机器学习来对视频内容进行分析，进而辅助 CAE 在编码阶段就进行更合理的码率分配，以此提高人眼感兴趣区域的编码质量，同时做到无损压缩。与传统的 ROI 检测能力相比，火山引擎通过数据集的优化和性能的不断打磨，做到了通过客户端使用便可与视频内容、视频种类解耦，目前已涵盖了直播间中包含人脸、商品等大规模常见的 ROI 区域。</p><p></p><p><img src="https://static001.geekbang.org/infoq/30/3048da71efd88d55faa66cb6a88ed9a1.png" /></p><p>图：火山引擎 ROI 检测与人眼兴趣区域高度重叠</p><p></p><p>在推流侧，火山引擎通过新一代自研 BVC 编码器，可实现内容自适应编码和感知视频编码。在电商场景下，受机型与系统编码能力的限制，会让推流侧出现时域上的画质波动（部分画面质量低）。这时，火山引擎新一代自研 BVC 编码器当会预测到低于既定的阈值，及时动态触发，调整码率与增强画质，实现高清画质相对恒定的直播体验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/08/0838b8edd3f43a81b8d097eb0c68e009.png" /></p><p>图：火山引擎视频时域画质</p><p></p><p>此外，在播放端，火山引擎以 SDR 转 HDR 算法，通过参考 SMPTE 相关标准并结合色调映射算法，将 SDR 视频源转换为 HDR10 标准格式的 HDR 视频，以提升画面的对比度、色彩丰富度，为消费者带来更好的直播视觉效果，帮助品牌商家全方位展示商品细节。</p><p></p><p><img src="https://static001.geekbang.org/infoq/65/65ba01b3e6e02ec4a82c6cd454cdd65c.png" /></p><p>图：火山引擎 SDR 转 HDR 算法</p><p></p><h2>二、直播体验更低延迟，让消费者与商家云端同频</h2><p></p><p></p><p>直播延迟的高低，影响着消费者购买决策的是与否。</p><p></p><p>直播延迟主要分布在生产端、流媒体服务、消费端三个环节：</p><p></p><p>在生产端，一般情况下，网络延迟平均在 20~30ms，编码延迟依赖编码参数设置而定，因此在整体延迟中占比较小；在流媒体服务上，在拉流转码的场景下，会额外产生 300ms~2S 的转码延迟，如果直接播放源流，则不会产生转码延迟；在消费端，网络延迟一般在 100ms~200ms，延迟主要由链路分发节点之间的传输延迟产生。</p><p></p><p><img src="https://static001.geekbang.org/infoq/46/46b15ebd3232560fdfeac2d2c653e2d0.png" /></p><p>图：火山引擎直播延迟分析</p><p></p><p>在抖音电商的直播场景中，评论、提问和解答，是消费者与品牌商家互动的主要方式，品牌商家是否及时反馈，对直播间的活跃度和成单转化起着关键性的作用。在抖音电商的直播实践过程中，基于火山引擎直播 SDK 的云端一体化解决方案，可以将 FLV 拉流延迟降低至 2-3s 内；RTM 超低延时拉流延时降低至 1s 内；同时也能做到面对网络不稳定时的灵敏响应、对抗弱网环境和提升带宽利用率。</p><p></p><h2>三、直播玩法更丰富，让电商业务获得新增长</h2><p></p><p></p><p>如今，消费者越来越倾向于能互动、有参与感的购物体验，直播电商平台的互动同质化已难以满足消费者的需求，导致电商平台的成单转化或成挑战。</p><p></p><p>以往直播间会采用「主播介绍 - 上链接 - 观众抢购」的模式，对于头部主播来说，可以有效提升商品流转效率，从而提升 GMV，但对于其他中腰部主播或官方品牌的直播间来说，丰富与消费者的互动玩法晚饭，让消费者更加深入地了解商品，才能提升消费者的购买动力。</p><p></p><p>抖音电商与火山引擎共同打造的互动直播方案，可以为品牌商家打造抖音同款的互动体验，支持嘉宾连麦、主播 PK 等丰富的直播形式，相较于传统直播的文本聊天和发弹幕的形式更具有互动性。从直播数据生产到直播数据的全链路数据驱动，帮助有效提升消费者粘性与业务营收。火山引擎互动直播方案目前已在抖音平台落地实践，并获得日均场次超 210 万、人均观看时长同比增长 15.4% 的成效。</p><p></p><p><img src="https://static001.geekbang.org/infoq/51/515a5cd3c6749ae03e5f74f8d4cc89ee.png" /></p><p>图：抖音电商直播画面</p><p></p><h2>四、写在最后</h2><p></p><p></p><p>从线下门店到线上购物再到直播电商，消费者的消费习惯不断变化，火山引擎视频云也将继续聚焦“消费体验”，持续升级迭代技术能力，助力品牌商家稳抓消费者心智、驱动业务新增量。未来，火山引擎将持续打造更安全、更稳定、更专业的云端直播服务，为消费者提供更低延迟、更高画质、更流畅的直播体验。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/30kQk4C6UOvlrZJxiLNM</id>
            <title>PayPal 合规平台研发总监朱琳确认出席 QCon 上海，分享图智能在 PayPal 的应用</title>
            <link>https://www.infoq.cn/article/30kQk4C6UOvlrZJxiLNM</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/30kQk4C6UOvlrZJxiLNM</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, 图智能在 PayPal 的应用, 合规调查, 交易网络
<br>
<br>
总结: PayPal 合规平台研发总监朱琳将在QCon全球软件开发大会上分享图智能在PayPal的应用，探讨如何利用图算法优化合规调查的流程，提高交易网络的调查效率，并解决大规模交易量下合规调查所面临的独特挑战。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1206&amp;utm_content=zhulin">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。PayPal 合规平台研发总监朱琳将发表题为《<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5675?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1206&amp;utm_content=zhulin">图智能在 PayPal 的应用</a>"》主题分享，探讨 PayPal 如何利用图算法优化合规调查的流程，提高交易网络的调查效率，并有效预防金融犯罪，以及如何成功解决大规模交易量下合规调查所面临的独特挑战。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/presentation/5675?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1206&amp;utm_content=zhulin">朱琳</a>"，2008 年硕士毕业于复旦大学计算机系。2018 年加入 PayPal 合规平台部门至今。专注以人工智能技术为合规调查提供创新、实用的解决方案，在反洗钱、违反用户使用协议检测和反金融犯罪等领域积累了丰富的实战经验。她在本次会议的演讲内容如下：</p><p></p><p>演讲：图智能在 PayPal 的应用</p><p></p><p>作为一家电子支付公司，PayPal 每天处理着大量的交易，为了防止金融犯罪（如欺诈、洗钱、恐怖融资等）、保证 PayPal 合法合规运行，合规调查发挥了至关重要的作用。然而，合规调查也面临着诸多挑战，包括交易网络的复杂性，犯罪模式的多样性等等。</p><p></p><p>在本次分享中，我们将介绍 PayPal 如何利用图算法优化合规调查的流程，提高交易网络的调查效率。</p><p></p><p>演讲提纲：</p><p></p><p>PayPal 合规调查的背景和挑战基于交易网络的账户合规调查图可视化算法及技术助力合规调查交易模式的自动检测和发现</p><p></p><p>听众收益点：</p><p></p><p>○ 了解 PayPal 如何通过图算法显著提高交易网络合规调查的效率，并有效预防金融犯罪</p><p>○ 了解 PayPal 如何成功解决大规模交易量下合规调查所面临的独特挑战</p><p></p><p>除上述演讲外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！ 现在购票，享 9 折优惠，立减￥680！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Kmuok7Y278ktUSZox2PS</id>
            <title>度小满杨青：传统 AI 与大模型融合，引领金融科技创新浪潮</title>
            <link>https://www.infoq.cn/article/Kmuok7Y278ktUSZox2PS</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Kmuok7Y278ktUSZox2PS</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 01:57:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 金融科技, 人工智能, 传统AI, 大模型
<br>
<br>
总结: 在金融科技领域，人工智能扮演着重要角色。传统AI在金融行业中的应用对业务增长至关重要。现在，传统AI技术与大模型结合，提高了金融服务效率和智能化水平，革命性改变了金融业的运营模式。 </div>
                        <hr>
                    
                    <p>在金融科技领域快速发展的当下，人工智能（AI）扮演着日益重要的角色。虽然生成式AI技术和大模型时代备受关注，但传统AI在金融行业中的应用依然至关重要，对业务增长的推动作用不可小觑。如今，我们正步入一个将传统AI技术与大模型紧密结合的新时代。这种融合不仅极大提高了金融服务的效率和智能化水平，同时也为金融业的运营模式带来了革命性的改变。</p><p></p><p>在近期举行的<a href="https://fcon.infoq.cn/2023/shanghai/">FCon全球金融科技大会</a>"上，度小满技术委员会执行主席，数据智能部总经理杨青发表了主题为《人工智能在金融行业中的创新应用》的演讲。他深入分析了传统AI在金融领域的持续影响力，同时探讨了在大模型时代下，如何有效结合传统AI与新兴技术，以应对未来挑战。</p><p></p><p>杨青指出：“人工智能正在步入一个新的发展阶段，金融智能化的时代已然到来。在这一过程中，传统人工智能，尤其是有监督机器学习，依旧展现出巨大潜力，它在推动金融业务增长方面发挥着不可替代的作用。”</p><p></p><p>同时，杨青强调：“生成型人工智能的能力范围在不断扩展，它正从局部应用转变为全面赋能，从而重塑整个金融行业并创造巨大价值。未来，人与机器的合作关系将经历深刻变革，AI将在重塑金融业乃至整个社会的生产关系中发挥关键作用。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/05/05a9d504e66f0ea03726261ba7edece4.jpeg" /></p><p></p><p>以下内容为演讲分享正文：</p><p></p><p>今天，我将分享人工智能在金融行业中的创新应用。尽管这一主题看似常见，但由于人工智能的快速发展，我们总有新的思考和发现。本次分享分为四个部分：</p><p>金融科技发展回顾：首先，我将快速梳理金融科技的发展历史，同时分析当前阶段我们面临的机遇与挑战。传统人工智能在业务增长中的作用：接下来，我将探讨传统人工智能，特别是有监督机器学习如何推动业务增长。生成式人工智能的探索与实践：鉴于任何关于人工智能的讨论都不可避免地涉及AIGC，我也将分享我们在生成式人工智能领域的最新探索和实践。未来展望：最后，我将提出对未来的一些思考和预测。</p><p></p><h4>人工智能与金融行业：正当其时，大有可为</h4><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/66/66efb35eb67d46a4e30d56dd1b6cc226.png" /></p><p></p><p>金融科技的发展经历了三个阶段，每个阶段都有着其独特的特点。</p><p></p><p>第一阶段为信息化阶段，技术主要作为金融业务的基础设施，为金融机构提供支撑和保障，并实现后台开发和系统建设等功能。这一阶段的关键词是“驱动后台”。</p><p></p><p>第二阶段为数字化阶段，金融机构开始将线下业务向线上迁移，这不仅提升了用户体验和便捷性，同时也加速了金融业务的数字化进程。在这一阶段，科技逐渐从后台向前台转移，成为金融业务的核心驱动力。这一阶段的关键词是“驱动前台”。</p><p></p><p>第三阶段为数智化阶段，随着人工智能技术的不断发展，金融业务逐渐与人工智能深度结合，渗透到业务的每一个环节。大数据、风控智能、获客、营销、投顾等领域也逐渐实现了技术与业务的融合和创新。这一阶段的主题是“技术与业务的融合和技术创新驱动业务变革”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d8/d84e2721661839bce1eb104acbf7950f.png" /></p><p></p><p>在当前阶段，数字化转型已进行一段时间，但我们仍面临许多机遇和挑战。</p><p></p><p>机遇方面，首先是政策层面的显著机遇。新一代人工智能发展规划、十四五数字经济发展规划，以及中央金融工作会议都鼓励科技金融的发展，特别是数智化以增强金融科技含量；其次是数据、算力和算法的共同进步。</p><p></p><p>在讨论数据层面时，我们可以从宏观和微观两个角度来观察。宏观层面上，整个数据市场的规模正在不断扩大，数据资源日益丰富，流通机制也变得更加明确和高效。具体来说，中国的数据市场规模已经呈现出爆炸式增长。截至2020年，中国大数据市场的规模已达到5000多亿，年化增长率超过20%。同时，共享流通机制也得到显著改善，例如去年年底，已有48家数据交易机构支持的共享调用次数超过5000亿次。此外，国家数据局的成立对构建完整的数据流通体系和激活数据元素生产具有重大意义。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c5/c5d768be978995b7acef6c265bf932d4.png" /></p><p></p><p>从微观层面来看，随着数字化转型的加速，金融领域的数据生产和利用正在迅速增长。比如，用户在使用产品过程中产生的行为数据，以及他们主动上传的信息，都为机器学习和人工智能提供了丰富的数据标签。以用户风险标签为例，这些由用户行为积累而来的标签，能够提高我们模型和算法的准确率。随着模型效果的提升，我们能够为用户提供更优质的服务，从而吸引更多用户，形成一个正向的飞轮效应。</p><p>&nbsp;</p><p>在算力方面，我们可以观察到中国在这一领域正在迎来一个新的发展阶段，得益于政策的支持和指导。近期，工业和信息化部联合发布了关于算力技术的设置和高质量发展的指导意见，标志着中国算力发展进入了一个新时期。</p><p></p><p>在过去五年中，我国的算力总规模平均年增长率接近30%，在处理复杂任务的速度、大规模并行计算能力以及解决复杂问题的能力方面均有显著提升。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/9d/9d5b69fa6043e53d46b55cf5da8d6448.png" /></p><p></p><p>在算法层面，首先，算法技术的不断更新和迭代显著提升了我们的原有业务效果，其中最显著的例子是大数据风控。最初，大数据风控主要依赖于人工规则，随后我们发展到使用逻辑回归模型。随着技术的进步，我们开始采用更复杂的模型，如XGBoost，甚至进一步发展到复杂的深度学习算法。这些算法的迭代每次都显著提升了效果，促进了业务增长；第二个方面是新技术和新场景的出现加速算法创新，这里有两个关键例子：一是因果推断的应用，这是一种新技术，它在金融场景中的应用可以大幅提升我们的经营效率；二是时序网络的应用。鉴于用户具有大量的时序行为信息，我们通过构建复杂的时序网络，有效预测用户未来的需求和流失情况，特别是在存量市场中，这极大地帮助提升了用户留存率。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fa/faa89e631614f675762860e8b755c5b4.png" /></p><p></p><p>第三个重要的机遇是基于深度学习的生成式人工智能的发展。在过去六年中，人工智能技术的发展经历了迅猛的变革，尤其是从2017年的Transformer模型到去年底ChatGPT产品的推出，生成式AI技术已经得到了广泛关注。这个领域的发展呈现出两个显著的趋势：一是模型技术的更新迭代速度越来越快，二是模型的多模态表达能力逐渐提高；大模型的能力不断增强，为大模型带来了更多的应用想象力。回归到金融行业本身，它具有三个显著特点。首先，金融是一个数据驱动的行业。</p><p></p><p>其次，金融领域存在较高的专业知识门槛，不同的子领域，如信贷、保险和理财，各自有着独特的特点和要求。最后，金融业务流程通常较为复杂，涉及大量的人工操作和决策过程。这些特性与生成式人工智能的核心能力——理解、记忆和规划等高度契合。因此，我们认为金融行业是生成式人工智能的理想应用场景之一。金融与生成式AI的结合不仅能够充分利用AI的这些能力，还有可能打开更多的创新空间和可能性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c9/c9f9a7b6a222df4a30fc32ee3acc4dec.png" /></p><p></p><p>除了迎接机遇，我们还必须应对许多挑战。首先，数据合规和隐私保护是金融领域中的重大问题，这是因为金融数据通常包含敏感信息，因此必须遵守相关法规和规定；其次，我们必须努力吸引、保留和培养具备金融和人工智能双重专业知识的人才，这是一项极具挑战性的任务，因为这些专业知识在不断变化和发展；另一个挑战是在技术变革下的人机协作，这要求人们学会如何与机器交互和协作，以提高工作效率和生产力。这涉及到个人技能的提升和组织文化的建设，未来的工作流程可能更多涉及人与机器的交流，这对组织来说是一个重要的适应过程。此外，组织变革也是一个挑战，随着智能体（Agent）的出现，协作关系和业务流程可能会发生巨大变革。</p><p></p><h4>开始的结束：厚积薄发，传统式人工智能驱动金融业务</h4><p></p><p>尽管最近&nbsp;AIGC&nbsp;非常火爆，但我认为在传统时代，传统的有监督机器学习仍然拥有巨大潜力。目前，传统式人工智能已经在金融的每一个主要业务环节中得到深入集成，包括人脸识别、客户服务、营销和获客等方面。</p><p>以信贷为例，我们面临获客、风控、经营三大核心业务难题，而人工智能技术为解决这些问题提供了有效的手段。</p><p>首先，智能获客的挑战在于如何更有效地获取目标客户并提升获客效率；其次，大数据风控的关键在于如何提升风险识别能力，从而帮助降低风险；最后，第三个挑战是在客户经营阶段如何更好地管理客户，提供个性化的信贷额度和利率。传统方法在处理这些问题时，可能导致成本高、效率低下，且效果难以保证。通过运用AI技术，我们能够有效地赋能上述业务流程，解决这些难题。那么如何运用不同的技术手段来解决一系列问题呢？</p><p></p><p><img src="https://static001.geekbang.org/infoq/4f/4f73506fa824e9aff7acc4962ad4857d.png" /></p><p></p><p>首先是NLP技术在大数据风控领域的应用。在日常运作中，我们处理大量文本信息，包括用户上传的合同、沟通对话以及征信报告中的文字。这些文本通常非结构化且信息稀疏，使得提取有价值的变量变得复杂。起初，我们通过人工统计词频来分析这些文本，如通过计数特定的风险词汇来评估风险。但这种方法对语义的深层理解有限。</p><p></p><p>例如，虽然能通过风险词汇识别潜在的不良用户，但无法准确洞察用户的真实意图或行为。</p><p>为了克服这一限制，我们引入了更先进的NLP技术，比如基于语义的模型。例如，词袋模型不仅考虑统计信息，还引入了语义量化，使我们能更有效地处理和理解长文本。此外，我们还引入了注意力机制，以学习长文本序列中的关键特征。最近，我们进一步发展了基于特定领域的预训练模型，这些模型专门针对风控领域的文本处理需求。与早期方法相比，这些模型在文本处理效果上取得了显著提升，大约提高了数倍。</p><p></p><p>其次是图机器学习在信贷领域的应用。对于金融企业而言，征信报告的重要性不言而喻，它包括了用户的借款行为、查询历史等关键信息。最初，风险评估主要依赖于从征信报告中人工提取特征，大概可能有十几维变量，使用逻辑回归进行建模。然而，这种传统方法未能充分利用报告中丰富的数据。</p><p></p><p><img src="https://static001.geekbang.org/infoq/26/266605ac31b4804a5dfd7ac4d2e91871.png" /></p><p></p><p></p><p>为了深入挖掘征信报告的潜力，我们进入了人工特征衍生阶段。在这一阶段，结合对征信报告和业务的深入理解，我们从原始数据中衍生出了几百到上千维的特征，使用XGBoost模型进行风险评估，从而提高了预测的精度和效率。</p><p>随后，我们进一步发展到机器自动衍生特征的阶段。这个阶段，我们利用自动化的特征衍生框架，能够从一份征信报告中提取出千万维度的特征。考虑到这些特征的庞大数量，我们采用了有监督的机器学习方法，自动筛选出最具有价值的特征，最终从一份征信报里构建出一个含约40万维的有效特征，大大提高了风险评估的准确性和效率。</p><p></p><p>最终，我们转向深度学习和图机器学习阶段，专注于征信报告中的文本信息解析和报告间的关系分析。我们将单个征信报告视作一个图结构，深入分析各种关系，如企业间的投资和流水关系，以及报告内部的动态变化。这一新阶段的引入，不仅提升了征信报告的识别能力，也为风险评估带来了革命性的突破。今年3月，我们的项目因其在人工智能领域的创新贡献，荣获吴文俊人工智能科学技术进步奖。</p><p></p><p>通过这一系列的技术创新和应用，我们展示了如何在不增加额外数据的前提下，通过算法的创新和优化，显著提升征信报告的解读效果，并为业务带来了巨大的经济效益。</p><p>&nbsp;</p><p>第三个案例是计算机视觉技术的应用，它极大地提高了我们处理复杂信息的效率。例如，处理小微企业一年的交易流水，这些流水可能包含超过100页的PDF文件。通过智能文档处理技术，我们能够实现对这些多媒体材料的秒级审批，将人工审核效率提高了70%以上，同时显著降低了人工成本。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/2b/2b6ff40ff2860c62fd039341e0358ec4.png" /></p><p></p><p>最后一个案例是因果推断技术在信贷领域的应用。通常情况下，因果推断被广泛应用于营销场景，比如通过模型决定发放什么样的优惠券以最大化收益。我们创新性地将因果推断技术应用于信贷领域，通过模型直接计算用户的贷款额度和利率。这种方法不仅提高了决策的准确性，也为业务增长注入了新动力。</p><p></p><p>首先解释一下因果推断。它是一种用于确定变量之间因果关系的技术。例如，在生活中我们可能会通过大数据样本分析运动与胆固醇之间的关系。初始数据可能显示运动量少的人胆固醇水平较低，这与我们的直觉相反。但当我们按年龄分组后，就会发现一个符合直觉的趋势：运动量少的人胆固醇水平反而更高。这种现象被称为辛普森悖论。</p><p></p><p>这在信贷领域存在类似的情况。例如，传统机器学习可能会错误地得出额度高风险低的结论，这显然是不符合实际的。在信贷领域，传统的额度分配方法通常基于用户画像、风险评级等静态因素，而未考虑到用户情况的动态变化。因果推断则能够识别这种风险迁移的动态变化，更精确地评估用户的综合情况。通过这种方法，模型直接决定的额度不仅更精准，也大幅提高了风险管理的效果。</p><p></p><h4>开始的开始：另辟蹊径，生成式人工智能重塑金融业务</h4><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/60/6095fe743c29534c719b2bc1161e7a8b.png" /></p><p></p><p>接下来，我将探讨生成式人工智能与金融业务结合的方式，尤其关注大模型技术的应用。</p><p>这些技术不仅具备理解、生成和记忆逻辑的能力，还在金融场景中实现了更深层次的抽象，包括个性化内容生成、交互增强、预测模拟和强自动化等多项功能。这些功能被看作是专业知识的一部分，能够作为高效的辅助工具，解决专业领域问题并提高工作效率。以下是一些实际应用的案例：</p><p></p><p>营销场景：在这一领域，AI通过大模型实现智能获客，提高营销效率。例如，它能实时生成针对不同用户画像的个性化营销文案，实现千人千面的策略。当用户点击广告并申请服务时，AI&nbsp;即时分析用户需求和意图，匹配适合的产品，大幅提升获客效率。服务场景：大模型显著增强了对话机器人的能力，使得人机交流更自然。这不仅提高了人工坐席的工作效率（约25%），还能在特定场景中直接提供用户服务，同时保持高质量服务和增强服务能力。运营方面：金融业务本质上是数据驱动的。传统的数据分析和报表统计，既耗时又依赖于个人能力。大模型逐步降低了分析门槛，实现了日常统计和分析决策的自动化，提升了企业的经营效率。代码生成：这方面在内部应用中表现出色，它提高了大家的工作效率。它不仅协助生成和补全代码，还能进行代码检测。事实上，采纳率超过40%，研发效益提升近20%。智能办公：金融行业涉及广泛的专业知识，对新员工来说是一大挑战。通过行业大模型的应用，可以有效解决这一问题。它就像是员工身后的一个金融智能助手，辅助决策并提供相关知识，显著提升工作效率。大模型在风控中的应用：目前大模型不能直接用于风险决策，但结合传统有监督机器学习的风控决策引擎，可以有效提高了处理非结构化信息的能力。这种结合使得智能风控更加实时和主动。</p><p></p><h4>未来展望：传统与新兴融合，人工与AI协作</h4><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/28/28662cb06c38d683fd76872a0c5a24fc.png" /></p><p></p><p>&nbsp;</p><p>首先，我更倾向于将传统&nbsp;AI&nbsp;视为业务流程的嵌入式模型。这些“嵌入式模型”可以视为业务的“肢体”，通过不断增强它们，我们能够推动整体业务的增长。相对地，生成型AI则像是一个“大脑”，拥有广泛的专业知识和高级功能，例如辅助绘图和编写代码。</p><p>&nbsp;</p><p>传统AI与生成型AI的结合，就如同肢体与大脑的协同，共同构成了我们金融业务的智能体。这个智能体具备规划和执行任务的能力，能够持续进行反馈和迭代。在金融领域，我们可以将复杂任务拆分为若干子任务，由人工和AI共同完成。这意味着未来的协作模式将超越传统的人与人互动，扩展到人与机器、机器与机器之间的合作。我认为，人工智能和人类的结合将深刻改变业务流程、组织架构，甚至是整个生产关系。</p><p>&nbsp;</p><p>回顾今天的演讲，我们首先指出，人工智能正在迎来新的发展阶段，金融智能化时代已经来临；其次，传统人工智能，特别是有监督机器学习，仍然拥有巨大的潜力，它能够持续推动金融业务的增长；第三，生成式人工智能正在逐渐扩展其能力范围，从局部应用走向全面赋能，重塑金融行业，创造出巨大的价值。最后，人与机器的合作关系将经历一次深刻的变革，AI将重塑金融业乃至全社会的生产关系。</p><p>&nbsp;</p><p>最后，介绍下度小满轩辕大模型，作为国内首个千亿级金融大模型，已在今年5月发布，并在后续几个月内陆续开源了其多个版本。它在国内知名榜单上排名第一，表现出色，尤其在金融知识问答领域的应用上。未来，我们计划在12月开源轩辕13B模型，并在1月发布一本总结我们在金融大模型实践经验的书籍。我们希望通过不断的开源工作，鼓励大家下载使用轩辕大模型，共同促进金融大模型行业的发展。谢谢大家。</p><p></p><p>活动推荐：</p><p>QCon 全球软件开发大会（上海站）即将在12月28-29日开始，届时将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。现在购票，享 9 折优惠，立减￥680！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/JFKgDD9292iZuigqEcdE</id>
            <title>腾讯安全推出大模型隐私保护脱敏技术</title>
            <link>https://www.infoq.cn/article/JFKgDD9292iZuigqEcdE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/JFKgDD9292iZuigqEcdE</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 10:49:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 隐私保护, HaS技术, 文本脱敏
<br>
<br>
总结: 腾讯安全玄武实验室发布了一项名为HaS的技术，该技术是业内首个支持信息还原的自由文本脱敏技术，能够帮助大模型用户在本地终端侧防范隐私数据泄露。该技术通过对用户上传的提示词进行隐私信息脱敏，并在大模型返回计算结果后进行恢复，兼顾了隐私安全和计算资源消耗。这项技术适用于典型的NLP任务场景，如机器翻译、文本摘要等，能够保护隐私实体不泄漏而不影响任务性能。 </div>
                        <hr>
                    
                    <p>大模型已经被广泛应用在各类场景，帮助人们进行报告摘要、文本翻译、数据分析、业务风控等各项工作，极大地提升了工作效率，但同时公众对于大模型带来的数据泄露的隐忧也从未停止。</p><p>&nbsp;</p><p>近日，腾讯安全玄武实验室披露了一项关于大模型隐私保护的安全脱敏与还原（Hide and Seek, HaS）技术，有望帮助大模型产品使用者从本地终端侧防范隐私数据泄露。</p><p>&nbsp;</p><p>HaS为业内首个支持信息还原的自由文本脱敏技术，通过对用户上传给大模型的prompt（提示词）进行隐私信息脱敏，并在大模型返回计算结果后进行恢复，该方案兼顾了隐私安全和计算资源消耗：脱敏与还原算法经过4bit量化后权重文件仅500MB，可在手机、 PC等终端上部署。</p><p>&nbsp;</p><p>这是业内首个公开发布的、能被大模型用户部署于终端侧的隐私保护脱敏技术。借助这一技术，用户可以从“源头”免除使用云端大模型带来的数据泄露方面的担忧。</p><p>&nbsp;</p><p>据悉，这个模型主要适用于典型的NLP任务场景，例如机器翻译、文本摘要，文本润色、阅读理解、文本分类、情感分析等，其主要的技术难点在于如何解决实体识别与替换、实体指代消解、多义词识别、自纠错鲁棒性还原、实体翻译等。此前，不少大模型提供方以及专业安全厂商均在积极进行相关的尝试，但目前尚未有理想的解决方案。</p><p>&nbsp;</p><p>“在大模型应用中提示词是一种自由文本，而针对自由文本的隐私保护问题，一种全密态的解决方案是通过安全多方计算（Multi-Party Computation, MPC）协议实现安全推理。然而，MPC协议在计算成本和通信量上的高需求严重限制了其在大模型应用中的落地。”腾讯安全玄武实验室高级安全工程师陈昱表示，“实际上，多数场景下用户并不需要保护整句的信息不泄漏，而仅需要保护其中的隐私实体不泄漏。”玄武实验室经过正则匹配、近义词向量、BERT NER+CR等方法的不断探索试错后，最终提出了这项技术。</p><p>&nbsp;</p><p>实验表明，HaS的数据脱敏处理并不会对任务造成影响。在使用模型进行隐私保护与直接调用大模型相比“分类任务”与“翻译任务”两个场景的性能对比，在使用560M生成式方案下，文本分类任务的性能不仅没有下降，反而增加了0.14%；在1.7B标签式方案下，机器翻译任务性能仅下降了0.85%。</p><p>&nbsp;</p><p>腾讯安全玄武实验室将上述研究发现以论文形式发布，更多技术细节可参考玄武官方博客（https://xlab.tencent.com/cn/2023/12/05/hide_and_seek/）。</p><p>&nbsp;</p><p>“一些企业或者个人用户开发者通过部署本地大模型来规避隐私数据泄露，但这要求一定的技术门槛，对于重度隐私需求的用户来说，通过本地安全模型/算法来实现数据保护，可能是更可行的办法。”玄武实验室正在逐步丰富这一模型的应用覆盖面，并完善其部署和交付方式，以供企业用户和个人用户在未来能够便捷使用。</p><p>&nbsp;</p><p>&nbsp;</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0PY4NyD9cPDe4pEQlRVa</id>
            <title>永不消逝的工匠：AI崛起与编码者涅槃</title>
            <link>https://www.infoq.cn/article/0PY4NyD9cPDe4pEQlRVa</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0PY4NyD9cPDe4pEQlRVa</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 07:18:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 父母, 孩子, 编程, GPT
<br>
<br>
总结: 编程作为一门新兴艺术和基本技艺，对孩子的读写能力和就业竞争力有益。然而，使用GPT等人工智能技术后，对编程的热情可能会减退。作者回忆起自己对编程的痴迷，并提到了李世石输给AlphaGo的故事。编程需要耐心和痴迷，而人工智能的出现可能改变了这一切。 </div>
                        <hr>
                    
                    <p>就像我的父母坚信我能读书写字一样，我理所当然地认为，我的孩子也一定能像我一样编程。编程是一门新兴艺术，也是一门基本技艺，而且正日益变得重要。熟练掌握编程是对孩子读写能力有益的补充，还能让他们保持就业竞争力。在我写下这些文字时，我的妻子怀着我们的第一个孩子，离预产期大约还有三周。我是一名专业的程序员，但等到孩子能够打字的时候，编程作为一项有价值的技能可能已经从这个世界上淡出了。</p><p>&nbsp;</p><p></p><h2>有了GPT，我失去了对编程的热情</h2><p></p><p>&nbsp;</p><p>我第一次冒出这种想法是在今年夏天的一个星期五的早晨，当时我正在开发一个小型的业余项目。几个月前，我和朋友Ben决定尝试完全让计算机自己制作一款类似“纽约时报”风格的填字游戏。2018年，我们曾借助软件制作过一款字谜游戏，我们做的事情很少——只是在某些地方根据需求做了少量改动。现在，我们尝试开发一个不需要人工干预的填字游戏生成程序。</p><p>&nbsp;</p><p>在过去，开发类似的项目既涉及硬件部分，也涉及软件部分，Ben更擅长硬件。我们曾经做过一款霓虹灯牌，当地铁接近我们公寓附近的站点时会发光。Ben负责制作玻璃灯管，并连接了变压器的电路板，我则编写了处理交通数据的代码。Ben也有过一些编程经历，但很短暂，而且已经过去了二十多年，所以主要的编程工作留给了我。不过，对于新的填字游戏项目，Ben引入了第三个“参与者”。他订阅了ChatGPT Plus，并使用GPT-4作为编程助手。</p><p>&nbsp;</p><p>然后，一些奇怪的事情发生了。我和Ben讨论了项目需要用到的一些程序。然后，很快地，Ben就弄好了。有一次，我们需要一个能够根据字典文件打印一百行随机内容的命令。我思考了一会儿没有思路，于是尝试使用谷歌搜索。我尝试了一些我能够找到的方法，而正当我开始写代码时，Ben把我们的想法告诉了GPT-4，并得到了完美可运行的代码。</p><p>&nbsp;</p><p>像这样的命令是出了名的繁琐，应该所有人都会去查，所以算不上是真正的编程。几天后，Ben说想要开发一款 可以对字典中的单词进行评分的iPhone 应用，但他不知道开发 iPhone 应用有多痛苦。我曾经尝试过几次，都半途而废。我觉得苹果的开发环境很难入门，不仅需要学习一门新的编程语言，还需要熟悉新的编辑和运行代码的程序。你还要学习各种“用户界面组件”和将它们组合在一起的方法，最后，你还需要弄清楚如何打包应用程序。要学习的东西堆积如山，似乎不值得这么去做。但是，第二天早上，我的收件箱里出现了一个完全符合Ben要求的应用程序。它可以完美地运行，还采用了非常可爱的设计。Ben说这是他花几个小时做出来的，GPT-4 承担了大部分繁琐的工作。</p><p>&nbsp;</p><p>如今，大多数人都有过使用AI的经历，但并非所有人都对它感到印象深刻。Ben最近才说：“直到我开始让它帮我写代码，我才真正对它肃然起敬。”我怀疑那些本来就生性多疑的人，那些看着ChatGPT输出生硬文字或虚假事实的人，他们仍然会低估正在发生的一切。</p><p>&nbsp;</p><p>原本需要费劲一生才能掌握的知识和技能，现在一口就能被吞下。对于我来说，编程一直是一个无穷无尽、丰富多彩的领域。现在，我发现自己想要为它写一篇挽歌。</p><p>&nbsp;</p><p>我想起了李世石。李世石曾是世界顶级围棋选手，也是韩国的民族英雄，但现在最出名的是他在2016年输给了一个叫作AlphaGo的计算机程序。当初，在开始比赛时李世石相信自己能够轻松击败人工智能。然而，在经过了长达数天的比赛之后，他输了。李世石在新闻发布会上说：“我为自己的无能感到抱歉。”三年后，他退役了。似乎有一个问题在困扰着李世石：这个让他费劲了大半生的东西会变成什么？</p><p>&nbsp;</p><p></p><h2>我曾经十分痴迷并且膜拜编程</h2><p></p><p>&nbsp;</p><p>我对计算机的痴迷始于六岁那一年。那是在九十年代早期的蒙特利尔，当时我和哥哥在玩《真人快打》游戏。他告诉了我一些“必杀技”——一些可怕而机智地击败对手的方法，但我们都不知道如何施展这些技能。他在MS-DOS终端拨号连接了一台FTP服务器，然后输入了一些晦涩的命令。很快，他打印出了一页代码——在游戏中使用的必杀技指令。我们回到地下室，开始互相爆头痛击。</p><p>&nbsp;</p><p>我曾经以为我的哥哥是一名黑客。和许多程序员一样，我也曾经梦想着入侵和控制远程系统，重点不是制造混乱，而是找到隐蔽的东西。1986年，Loyd Blankenship在《黑客宣言》一文中写道：“我的罪过就是太过好奇了。”1995年的电影《骇客天团》中有一个场景，Dade Murphy在地下俱乐部证明自己的实力：有人从背包里掏出一本本五颜六色的电脑书，Dade从封面上认出了每一本：绿色的关于国际Unix环境的书，红色的关于N.S.A.可信网络的书，还有一本封面上画着身着粉红衬衫的家伙在玩IBM个人电脑。Dade把他的专业知识用在了学校的喷水灭火系统上，还帮着清理油轮的压舱物——所有这些都是通过敲击键盘来完成的。这个故事告诉我们知识就是力量。</p><p>&nbsp;</p><p>但到底要怎样才能学会黑客技术呢？在我上五年级时，我们家在新泽西定居。在上高中时，我去了肖特山购物中心的Borders书店，买了Ivor Horton写的《Visual C++入门》（Beginning Visual C++）。它长达1200页——我的第一本魔法书。就像许多其他教程一样，它一开始很简单，然后，突然，它就不那么简单了。中世纪的学生把初学者遭遇学习失败的时刻称为“驴桥”（Bridge of Asses，或理解为“笨人难过的桥”）时刻。这个术语源于《欧几里得几何原本》第一卷的第五个命题，这是书中第一个难以理解的概念。那些成功跨过驴桥的人将继续掌握几何学，过不去的人只能望洋兴叹。《Visual C++入门》的第4.3节，关于“动态内存分配”，就是我的驴桥。可惜，我没有跨过去。</p><p>&nbsp;</p><p>但我并没有放弃这个课题，我还记得事情发生转变的那一刻。那是一次长途飞行，我带了一台笨重的笔记本电脑和一张装有Borland C++编译器的CD-ROM。编译器将你写的代码转换成机器可执行的机器码，我已经花了好几天时间才让这个编译器跑起来。按照惯例，每个编程初学者的第一个程序是“Hello, world.”。当我尝试运行我的版本时，只看到了无穷无尽的错误消息。解决了一个问题，又冒出了另一个。我读过《哈利·波特》系列，我感觉自己拥有了一把扫帚，只是还没有学会让它飞起来的咒语。我知道如果我能做到的话可能会发生什么，所以我努力坚持着。我从中悟到了一个道理，编程实际上并不是关于知识或技能，而是耐心，或者说是对这件事的痴迷。程序员是能够忍受无尽乏味的人。想象一下，在没有图片的情况下，用一种你很少说的语言，在电话里向一个傻瓜解释如何组装家具。再想象一下，你得到的唯一回应是，你提了一个荒谬的建议，而家具组装得越来越不像样。所以，当你终于完成了这件事，你会感到如释重负。我清楚地记得自己趴在飞机过道上，最后一次按下了回车键。然后，我坐了起来，这一次程序竟然做了我要它要做的事情。“Hello, world”出现在光标上方，仿佛一个智者醒来并向我介绍它自己。</p><p>&nbsp;</p><p>我们大多数人都不会成为《黑客》中所描述的那种黑客。所谓的“hack”，用程序员的话来说，就是修修补补——通过代码来表达创造力。我从未正式学习过编程，我只是在瞎折腾，让电脑做一些有用的或令人愉快的小事情。在我上大一时，也就是2006年，为了不错过Tiger Woods在大师赛中的排名情况，我写了一个程序，在pgatour.com上搜索排行榜信息，并在他打出小鸟球或柏忌球时给我发短信。后来，在英语课上读了《尤利西斯》之后，我写了一个程序，从书中随机抽取句子，计算它们的音节，并汇编俳句——这是一种比现在的聊天机器人更原始的语言反刍，但我认为，它已经能够写出真正的诗歌：</p><p>&nbsp;</p><p></p><blockquote>I’ll flay him aliveUncertainly he waitedHeavy of the past</blockquote><p></p><p>&nbsp;</p><p></p><h2>作为一名程序员，我曾经历过最好的时代</h2><p></p><p>&nbsp;</p><p>我开始认真对待编程。我主动提出为朋友的创业公司开发程序。我逐渐了解到，计算机世界是庞大的，就像是地质学中的沉积物层一样有序。从Web浏览器到晶体管，每个子领域或系统都是建立在其他更古老的子领域或系统之上，层层叠加但有条理。你挖得越深，就越是会感受到赛车手Jackie Stewart所说的那种“机械同理心”，即对机器优势和局限性的一种感知，知道如何让它发挥作用。</p><p>&nbsp;</p><p>在朋友的公司里，我感到我的机械同理心在变强。大二时，我和朋友一起看《危险边缘》，他建议我根据它制作一款游戏。我想了几个小时，然后非常失望地告诉他，这超出了我的能力范围。但是，这个想法在我上大三时再次出现，而此时我终于找到了解决办法。我现在对这台机器能做什么有了更好的认识。我花了接下来的14个小时去开发游戏。几周之后，玩这款游戏已经成为我朋友之间的常规活动。这段经历意义深远。我可以理解为什么人们愿意把他们的生命时光花到手工艺术上：没有什么比看着别人享受你做的东西更有趣的了。</p><p>&nbsp;</p><p>在这期间，我完全沉浸在一中“纸牌屋”式的状态中，并把学习放在了一边。我努力，但并不是在学习上。有一个晚上，我在地下室里用6台机器来并行运行一个程序。我把满是数字的打印件放在地板上，思考着一种路径规划算法。代价是，我经历了一场噩梦——参加期末考试却一无所知。2009年，在数十年来最严重的金融危机期间，我以2.9的G.P.A.（平均绩点）毕业了。</p><p>&nbsp;</p><p>但我还是很轻松地得到了我的第一份全职工作。我有程序员工作经验，也没有人问我的成绩如何。对于年轻的程序员来说，这是他们的繁荣时期。各家公司都在争夺顶尖的程序员。对有经验的程序员的抢夺如此激烈，以至于他们抱怨“招聘邮件满天飞”。大学计算机科学专业的热度开始爆炸性增长。（我的学位是经济学。）声称能在不到一年时间内将初学者变成高薪程序员的编程“训练营”开始出现。</p><p>&nbsp;</p><p>在我二十多岁时的一次面试中，公司的首席执行官问我觉得自己值得拿多少薪水。我说了一个让自己都感到尴尬的数字。他当场起草了一份合同，给出的薪水比我要求的高出百分之十。“软件工程师”的技能备受推崇。在我工作过的一家公司，有人因为使用 HipChat（Slack的前身）直接问我的一位同事问题而惹上了麻烦。他被告知“永远不要直接使用 HipChat 联系工程师”。我们太重要了，你们不能那样！</p><p>&nbsp;</p><p>这是一个利率接近于零、科技行业增长惊人的时代。一些规范已经成形，像谷歌这样的公司在告诉这个行业，程序员可以享受免费的浓缩咖啡和热食，世界一流的医疗保健和育儿假，现场健身房和自行车室，随意的着装，以及“20%的时间”，这意味着他们每周可以花一天时间做任何他们喜欢的事情。人们认为他们的技能是如此的重要，以至于产生了一种迷信。例如，估计编码任务可能需要多长时间被认为是愚蠢的，因为程序员可能随时翻开石头挖出一堆Bug。交付期限是一种诅咒，如果交付的压力太大，程序员只需要说出“精疲力竭”这个词就能额外获得几个月的时间。</p><p>&nbsp;</p><p>我从一开始我就感觉这一切并不太对劲。我们所做的事情真的那么珍贵吗？这种繁荣能持续多久？在我十几岁的时候，我做过一些网页设计。在当时，这项工作曾经很受欢迎，也很受人尊重。你可以用一个周末完成一个项目，并赚到数千美元。但随之而来的是Squarespace这样的工具，它可以让披萨店老板和自由艺术家只需点击几下鼠标就创建好自己的网站。对于专业程序员来说，一部分高薪、相对不费力的工作消失了。</p><p>&nbsp;</p><p>程序员社区对这些变化做出的反应是——你必须不断提升技能，去学习更难、更晦涩的东西。软件工程师，作为一个物种，喜欢自动化。不可避免地，他们当中最优秀的那部分人所构建的工具会使其他类型的工作过时。这种本能解释了为什么我们会得到这么好的照顾：代码有着巨大的杠杆作用。一款软件可能会影响数百万人的工作，甚至会取代程序员自己。我们应该把这些进步看作是涨潮，潮水会逐渐淹没我们的裸露的双脚，但只要我们持续地学习，就不会被潮水淹没。这是个明智的建议，除非遇上了海啸。</p><p>&nbsp;</p><p></p><h2>ChatGPT来了，它改变了我们的工作</h2><p></p><p>&nbsp;</p><p>当我们被允许在工作中使用人工智能聊天机器人来帮助编程时，我故意避免使用它们。我原以为我的同事们也会这样。但很快，我从他们的屏幕上看到人工智能聊天会话的颜色——那种问答风格的斑马条纹。一种常见的说法是，这些工具会让你更加高效，在某些情况下，它们可以帮你以快十倍的速度解决问题。</p><p>&nbsp;</p><p>我不确定我是否想要那样。我喜欢编程过程本身，也喜欢“自己是个有用的人”的感觉。我熟练使用的工具，比如我用来格式化和浏览代码的文本编辑器，可以同时满足这两个需求。它们加强了我对这门手艺的实践——虽然它们能让我更快地完成工作，但我仍然觉得自己功不可没。但人工智能，就像人们对它所描述的那样，似乎有所不同。它提供了很多帮助，我担心它会剥夺我既享受解决难题的乐趣又满足于成为解决问题的人的满足感。我可以无限提高生产力，但我所能展示的可能只有结果而已。</p><p>&nbsp;</p><p>大多数程序员的实际工作成果很少是令人感到兴奋的。事实上，它们往往平淡无奇。几个月前，我下班回家告诉妻子，我度过了一个非常愉快的一天，解决了一个特别有趣的问题。我正在开发一个生成表格的程序，有人想要添加一个跨多个列的标题——而我们的自定义布局引擎并不支持这个功能。这项工作很紧急：这些表格被用在重要的文件中。因此，我把自己关在房间里大半个下午。这里有许多需要解决的子问题：应该如何让布局引擎用户表达他们想要一个跨列的标题？它们的代码应该是什么样子的？还有一些琐碎的细节，如果忽略了就会出现Bug。例如，如果跨列标题其中的一列因为没有数据被删除了该怎么办？我知道这是美好的一天，因为我必须拿出笔和纸——我在绘制可能的场景并反复检查我的逻辑。</p><p>&nbsp;</p><p>然而，如果以鸟瞰的角度看那天发生的事情会怎样？一个表格得到了一个新的标题，很难想象还有什么比这更平凡的事情了。对于我来说，乐趣完全在于这个过程，而不在于结果。如果这个过程只需要进行三分钟的 ChatGPT 对话，那么这个过程会变成什么样子？作为程序员，我们的工作除了字面上的编码之外，还涉及许多其他事情，比如指导新人和设计系统，但编码一直是其根本所在。</p><p>&nbsp;</p><p>在我的整个职业生涯中，我一直因解决编程琐碎难题的能力而被雇主选中。然而，突然间，这种能力变得不那么重要了。</p><p>&nbsp;</p><p></p><h2>被生成式AI征服</h2><p></p><p>&nbsp;</p><p>我从Ben那里得知了很多信息，他一直告诉我他使用 GPT-4 取得了惊人的成果。事实证明，它不仅擅长处理琐碎的事情，还具备了资深工程师的素质：它可以从丰富的知识库中提出解决问题的方法。在一个项目中，Ben将一个小型扬声器和一个红色 LED 灯泡连接到查尔斯国王肖像上。他的想法是当用户在网站上输入文字时，扬声器会播放声音，灯光会用摩斯密码闪烁出消息。但为设备编写获取消息的程序困扰着Ben，这似乎需要专业的知识，不仅涉及他所使用的微控制器，还涉及存储消息的后端服务器技术 Firebase。Ben向我求助，我咕哝了几句。事实上，我并不确定他想要的是否可能实现。然后他问了 GPT-4，它告诉Ben，Firebase有一个功能可以让这个项目变得简单得多。就这样，这里还有一些代码，这些代码与微控制器是兼容的。</p><p>&nbsp;</p><p>尽管我害怕使用 GPT-4，也对为了使用它需要向 OpenAI 每月支付二十美元感到有些不适，但我还是开始通过Ben来探索它的能力。我们一起研究我们的填字游戏项目。我会说：“你为什么不试着用这个提示词？”他会把键盘递给我。我会说：“不，你来操作”。我们在一起逐渐摸索出了这个人工智能的能力。Ben在这方面的经验比我多，似乎更能高效地利用它。正如他后来所说的，他自己的神经网络已经开始与 GPT-4 的神经网络对齐了。我想说他已经获得了机械同理心。</p><p>&nbsp;</p><p>有一次，他做了一件让我感到特别惊讶的事情，他让这个人工智能开发了一款贪吃蛇游戏，就像旧诺基亚手机上的那种。在与 GPT-4 进行了一番简短的交流之后，他让它修改游戏，当你输掉比赛时，它会显示你偏离最有效的路线多远。机器人花了大约十秒钟完成了这个任务。坦率地说，我不确定自己能不能完成这项任务。</p><p>&nbsp;</p><p>国际象棋领域数十年来一直被人工智能所主宰，一位玩家唯一的希望就是与机器人搭档。这种半人半人工智能的团队（也就是所谓的半人马）可能仍然能够击败单打独斗的最优秀的人类和人工智能引擎。编程还没有发展到国际象棋那样的程度，但半人马已经出现了。目前看来，单打独斗的GPT-4是一个比我更糟糕的程序员，Ben更糟糕，但Ben和GPT-4结合起来就很危险了。</p><p>&nbsp;</p><p>很快，我屈服了。我开发了一个小型的搜索工具，希望突出显示用户查询与结果匹配的部分。我将用户查询分割成单词，这让事情变得复杂。我的耐心受到了考验，于是开始考虑使用 GPT-4。或许我可以花点时间写“提示词”，或者与人工智能聊一聊，而不是花一整个下午在编程上。</p><p>&nbsp;</p><p>1978年，计算机科学家Edsger W. Dijkstra在一篇题为《论“自然语言编程”的愚蠢》的文章中指出，如果你不是用C++或Python这样的正式编程语言来指导计算机，而是用你的母语，那么你就是在拒绝计算机的有效性。他写道，正式的编程语言是“一种非常有效的工具，可以排除各种各样无意义的废话，而这些是在使用我们的母语时几乎无法避免的”。Dijkstra的观点在编程界成为了一个共识。2014年，当这篇文章在 Reddit 上再次被传开时，一位评论者写道：“我不确定这两种情况哪一种更可怕：一个是‘这个想法是多么地显而易见’，一个是‘许多人仍然不知道它’”。</p><p>&nbsp;</p><p>当我开始使用GPT-4时，我能明白Dijkstra说的是什么。你不能只是简单地对人工智能说：“帮我解决这个问题。”或许这一天终会到来，但现在它更像是一种你必须学会演奏的乐器。你必须仔细地说明你想要什么，就像和一个初学者交流一样。在搜索突出显示问题上，我发现自己一次性要求GPT-4做太多的事情，结果它失败了，然后又重新开始。每一次，我的提示词都变得不那么雄心勃勃了。最后，我并没有直接和它谈论搜索或高亮显示的问题，我把这个问题分解成具体的、抽象的、明确的子问题，这些子问题加在一起，就能得到我想要的东西。</p><p>&nbsp;</p><p>在发现了人工智能的能力后，我的工作生活开始发生变化。GPT-4就像一把锤子，什么东西在我眼里都成了钉子。我终于明白为什么办公室的电脑屏幕上总是满是聊天框，Ben又是如何变得如此高效的。于是我敞开心扉，更频繁地使用它。</p><p>&nbsp;</p><p>我回到了填字游戏项目。我们的谜题生成器以一种难看的文本格式打印输出，比如像“s”、“c”、“a”、“r”、“”、“k”、“u”、“n”、“i”、“s”、“”、“a”、“r”、“e”、“a”这样字母行。我想将这样的输出转换成一个漂亮的网页，让用户能够探索网格中的单词，并能一目了然地显示得分信息。但我知道这项任务很棘手：每个字母都必须标记上它所属的单词，无论是横向的还是纵向的。这是一个很细致的问题，很可能会占用我一整个晚上的时间。</p><p>&nbsp;</p><p></p><h2>未来会是什么样的？</h2><p></p><p>&nbsp;</p><p>我的孩子快出生了，我的空闲时间不多了，于是我开始和 GPT-4 聊天。我们来来回回地交谈，有时候还得看一些代码才能理解它在做什么，但我几乎没有做过我曾经认为的那种编程式的思考。我没有考虑数字、模式或循环这些东西，也没有使用我的大脑来模拟计算机的活动。正如另一位程序员 Geoffrey Litt 在经历了类似的情况后所写的：“我从未让我的程序员大脑参与其中。”那么我都做了些什么？</p><p>&nbsp;</p><p>或许是因为感觉围棋游戏被永久性地贬低了，李世石选择退出这个游戏。我当初学习编程是因为我觉得计算机就像是一种魔法一样。计算机赋予了你力量，但你需要研究它的秘密——学习一种咒语，这需要一种特殊的思维方式。我觉得自己是被选中的。我埋头苦干，认真思考，积累晦涩的知识。然后，有一天，不需要思考和知识就可以达到许多相同的目的。从某种角度来看，这可能会让一个人的大部分工作看起来像是在浪费时间。</p><p>&nbsp;</p><p>每当我想到李世石，就会想到国际象棋。大约三十年前，当机器征服了这个游戏，人们担心再也没有理由去玩它了。然而，国际象棋从未像现在这样受欢迎过——人工智能使这个游戏变得更加活跃。我的一个朋友最近开始学习下国际象棋。他可以随时使用人工智能教练，这个教练可以给他提供正好符合他能力边界的棋局，并且在他输掉比赛后告诉他错在哪里。与此同时，国际象棋大师们在研究计算机的走法，就像在阅读神谕一样。学习国际象棋从未如此简单，研究它最深层的秘密也从未像现在这样令人兴奋。</p><p>&nbsp;</p><p>计算机科学尚未被征服。GPT-4 令人印象深刻，但普通人无法像程序员那样驾驭它。我仍然觉得我的职业是安全的。事实上，我感觉比以前更安全了。随着软件开发变得越来越容易，它会越来越普及，程序员将更多地转向设计、配置和维护。尽管我一直觉得编程中那些琐碎的部分最能让人投入其中、也最为重要，但这些并不是我特别擅长的。我没有通过很多大型科技公司的编程面试。我相对擅长的是知道什么是值得做的、用户喜欢什么，以及如何进行技术和人性层面的沟通。我的一位朋友把这个人工智能的时刻称为“平庸程序员的复仇时刻”。随着编码本身的重要性开始降低，也许软技能会大放异彩。</p><p>&nbsp;</p><p>这让我不确定我的孩子出生后该教他些什么。</p><p>&nbsp;</p><p>我甚至认为，当我的孩子长大成人，我们看待“程序员”的方式，就像我们现在回顾“计算机”发展历史一样。自己敲打C++或Python代码，最终可能看起来像在打孔卡上发出二进制指令一样荒谬。Dijkstra可能会感到震惊，让计算机做你想做的事情可能就像礼貌地提出请求那么简单。</p><p>&nbsp;</p><p>所以也许要教的不是技能，而是精神。我有时会想，如果我出生在另一个时代，可能会做些什么。在农业时代，编码者可能会研究水车和作物品种，在牛顿时代，可能会着迷于玻璃、染料和时间测量。最近我看了有关神经网络发展历史的口述资料，其中很多被访谈的人——出生于20世纪30年代前后的人——小时候曾经玩过收音机，这让我感到震惊。也许下一代人会在深夜里探究那些曾被他们的父母视为黑匣子的人工智能。我不应该担心编程时代的结束。黑客精神永存！</p><p>&nbsp;</p><p>【声明：本文由InfoQ翻译，未经许可禁止转载。】</p><p>&nbsp;</p><p>原文链接：<a href="https://www.newyorker.com/magazine/2023/11/20/a-coder-considers-the-waning-days-of-the-craft">https://www.newyorker.com/magazine/2023/11/20/a-coder-considers-the-waning-days-of-the-craft</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/FCley3cdJi5mbzCeaUE5</id>
            <title>引领人工智能革命的程序员、OpenAI的秘密武器Greg Brockman：五年前，我并不擅长机器学习</title>
            <link>https://www.infoq.cn/article/FCley3cdJi5mbzCeaUE5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/FCley3cdJi5mbzCeaUE5</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 06:47:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 编辑, Greg Brockman, OpenAI, 机器学习
<br>
<br>
总结: Greg Brockman是OpenAI的联合创始人兼首席技术官，他在OpenAI中担任总裁一职，负责推动产品的发展。他具备极强的技术能力和战略思维，能够协同团队进行软件效率的改进。虽然他没有接受过正规的机器学习教育，但通过自学，他成功转变为一名机器学习实践者。他在OpenAI的工作中取得了显著的成就，特别是在将复杂的游戏转换为机器学习环境方面。 </div>
                        <hr>
                    
                    <p>编辑 ｜ Tina</p><p>&nbsp;</p><p>Greg Brockman因担任 OpenAI 联合创始人兼首席技术官而闻名。</p><p>&nbsp;</p><p>Brockman曾于2008年在哈佛大学就读，后来转到麻省理工学院，但也没待多久，2010年，他退学加入金融科技初创公司 Stripe。随后，Brockman 在 Stripe 待了五年，最终成了公司 CTO，在这段时间里，Stripe 实现了爆炸式增长，成为科技行业中最有价值的创业公司之一。2015年他离开Stripe并创立了OpenAI。</p><p>&nbsp;</p><p>虽然 OpenAI 的 CEO Sam Altman 代表公司的公众形象，但 <a href="https://www.sohu.com/a/665266306_355029">Brockman 是公司的秘密武器</a>"：</p><p>Brockman 作为 OpenAI 的总裁，一直在推动 OpenAI 的产品前进。Altman 说： “如果没有他的参与，整个项目不可能以如此高的质量交付。”Quora 的 CEO Adam D’Angelo 则评论说，“Greg 是那个真正能使技术变成现实的人。”</p><p>&nbsp;</p><p>Brockman的日常职责与拥有该头衔的大多数科技领导者几乎没有任何相似之处——34 岁的 Brockman 没有直接下属，这使他免于常规的管理类繁琐事务，可以将大约 80% 的时间用于编程工作。</p><p>&nbsp;</p><p>“Greg 有极强的能力，能够看到每个技术问题的细节，对每个层次都了如指掌，同时也能非常有战略性地考虑每个部分需要如何协同工作。”Altman 说。</p><p>&nbsp;</p><p>Brockman是一位“流动人员”，在不同的团队之间游走，制定目标并推动团队在软件效率方面逐步改进。在达到特定节点时，他将项目交给管理者，并转向新项目。“我一直在不断地探索新领域。”Brockman说。</p><p>&nbsp;</p><p>在 OpenAI 的前几年，Brockman 和 Ilya Sutskever 做了大部分日常决策，Brockman 管理公司的软件工程师，而 Sutskever 则负责管理研究员。</p><p>&nbsp;</p><p>因此，Brockman 需要解决的一个难题是让研究员和工程师协同工作的问题，这让他必须需要了解人工智能相关的工作。而作为一个长期从事软件方面的工作，并且没有接受过正规的“人工智能”方面的本科教育的人来说，自学机器学习肯定具有挑战性。</p><p>&nbsp;</p><p>Brockman自称是“<a href="https://blog.gregbrockman.com/define-cto-openai">人工智能方面的新手</a>"”，在 OpenAI 工作的头三年里，虽然他一直梦想可以成长为一名机器学习专家，但是他的机器学习技术能力一直没有长进。但之后，他花了九个月的时间，终于完成了向机器学习实践者的转变。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/f8/f843d10d5b3805166ad5398ce9bd4837.jpeg" /></p><p></p><p>Greg Brockman过去的<a href="https://gregbrockman.com/">文章和演讲</a>"主要集中在软件方面。</p><p>&nbsp;</p><p>他曾写过一篇文章，来总结他的学习过程，在这篇“励志”文章中，Brockman指出，这很难，但并非不可能。在他看来，大多数优秀的程序员都会一些（或愿意学习）<a href="https://www.deeplearningbook.org/">数学</a>"，既然如此他们就也能做到这一点。他给我们的一个建议是：花时间进行试验、快速失败，然后继续根据现实世界的用例进行研究。下面让我们一起看看这篇文章：</p><p>&nbsp;</p><p></p><h2>Greg Brockman的自述：我是如何成为一名机器学习从业者的？</h2><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/87/87845356966bf0a7e356547253590c21.png" /></p><p></p><p>&nbsp;</p><p>在 2018 年假期期间学习机器学习</p><p></p><h2>早期</h2><p></p><p>&nbsp;</p><p>OpenAI 的一个基本原则是重视研究和工程——我们的目标是构建能够解决以前不可能完成的任务的工作系统（我们团队中有 25%的人主要使用软件技能，还有 25%主要使用机器学习技能，剩下 50%混合使用这两种技能）。所以从入职 OpenAI 的第一天开始，我的软件技能就一直很<a href="https://blog.gregbrockman.com/define-cto-openai#gym_1">受欢迎</a>"，这也致使我一直拖延学习机器学习技能。</p><p>&nbsp;</p><p>在帮助完成了<a href="https://openai.com/blog/openai-gym-beta/">OpenAI Gym</a>"项目之后，我被安排负责<a href="https://openai.com/blog/universe/">Universe</a>"项目。之后，我们团队又开始致力于<a href="https://openai.com/five/#timeline">DOTA</a>"项目——在开始机器学习之前，我们需要有人将游戏场景变成一个机器强化学习环境。</p><p>&nbsp;</p><p></p><h2>DOTA</h2><p></p><p>&nbsp;</p><p>在没有源代码的情况下将如此复杂的游戏转换成一个研究环境是一项非常有<a href="https://www.youtube.com/watch?v=UdIPveR__jw">挑战性</a>"的<a href="https://openai.com/blog/more-on-dota-2/#infrastructure">工作</a>"，每当我有所突破的时候，团队都会感到很兴奋。我知道如何打破游戏的 Lua 沙箱，在 GRPC 服务器上以编程的方式使用<a href="https://stackoverflow.com/a/426260">LD_PRELOAD</a>"控制游戏， 增量地将整个游戏状态转储到原 buf 中，&nbsp;并为我们可能想要使用的许多不同的代理配置构建一个对未来兼容的 Python 库和抽象层。</p><p>&nbsp;</p><p>但我感到当时有点盲目了。在<a href="https://blog.gregbrockman.com/figuring-out-the-cto-role-at-stripe">Stripe</a>"时，尽管我更倾向于基础设施解决方案，但我可以在堆栈的任何地方进行更改，因为我非常熟悉产品代码。在 DOTA 项目中，我不得不从软件的角度来思考所有的问题，这意味着有时我会碰到一些困难的问题，而这些问题本可以通过稍微不同的机器学习来避免。</p><p>&nbsp;</p><p>我想像我的同事 Jakub Pachocki 和 Szymon Sidor 那样，他们创造了驱动 DOTA 机器人的核心突破。他们并不认为 OpenAI 中的增强算法不能伸缩。为此他们编写了一个分布式强化学习框架，名为 Rapid，每两周左右就以指数形式进行扩展，运行的一直很顺畅。我希望能够做出一些关键的贡献，比如把软件和机器学习技能结合起来。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/63/6385c1c5c953b6a42d9a82a82cec0ee3.png" /></p><p></p><p>&nbsp;</p><p>左边是 Szymon，右边是 Jakub</p><p>&nbsp;</p><p>2017 年 7 月，我似乎有了机会。软件基础设施很稳定，我开始着手一个机器学习项目。 我的目标是利用行为克隆从人类训练数据中教授神经网络。但我并没有完全准备好，我感觉自己会像一个初学者。</p><p>&nbsp;</p><p>我一直为一些小的工作流细节感到沮丧，这些细节让我不确定自己是否取得了进展，比如不确定某个给定的实验使用了哪些代码，或者意识到我需要将上周的结果与我没有正确存档的结果进行比较。更糟糕的是，我不断地发现一些小错误，这些错误一直在破坏我的结果。</p><p>&nbsp;</p><p>我对自己的工作开始失去信心，但奇怪的是，别人对我很有信心。人们会安慰说从人类数据中克隆行为是多么多么困难。而我总会从自己身上找问题来说明是我自己的问题，这可能是我更愿意相信是我能力问题而非项目问题。</p><p>&nbsp;</p><p>当我的代码被使用到机器人项目中时，我觉得一切困难都是值得的，Jie Tang 将我的代码用在蠕变阻塞上，然后通过强化学习对其进行微调。但是之后他找到了不用我的代码就能得到更好结果的方法，这也意味着我的努力没有带来任何成果。</p><p>&nbsp;</p><p>之后我就再也没有在 DOTA 项目上尝试使用过机器学习。</p><p>&nbsp;</p><p></p><h2>超时</h2><p></p><p>&nbsp;</p><p>在 2018 年的国际比赛中，我们输掉了两场比赛，大多数人都认为我们已经<a href="https://twitter.com/polynoamial/status/1032988066967965696">竭尽所能</a>"。但从我们的度量标准中，我们知道我们离成功已经非常靠近了，最需要的是用更多的数据训练学习。这意味着，我们需要降低对时间的要求。2018 年 11 月，我觉得自己有了一个机会，可以用三个月的时间来赌一把。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/4f/4f8d5475a19a781d0e094af3ba08bd38.png" /></p><p></p><p>&nbsp;</p><p>队员们在国际比赛中输掉了第一场比赛后情绪高涨</p><p>&nbsp;</p><p>当我的目标确定了之后，我往往能够做的更好。我决定试着做一个聊天机器人。我开始自学我们为<a href="https://openai.com/blog/openai-fellows/">学员</a>"制定的课程，只选择与 NLP 相关的模块。例如，我编写并训练了一个 LSTM 语言模型，然后是一个基于转换的模型。我还研读了<a href="https://colah.github.io/posts/2015-09-Visual-Information/">信息论</a>"等主题的书，阅读了许多论文，每一行都细细研读，直到完全吸收。</p><p>&nbsp;</p><p>其实进展过程很缓慢，这也是之前就预想到的。这让我想起了我刚开始编程时的感受，我一直在想，要花多少年的时间才能获得一种掌控感。老实说，我并不相信自己会擅长机器学习。但我一直在努力，因为……说实话，因为我不想被限制在只理解我项目的一部分，我想把全貌看清楚。</p><p>&nbsp;</p><p>我的个人生活也是让我坚持下去的一个重要因素。我和一个人开始了一段感情，他让我觉得即使失败也没关系。我和她一起在解决机器学习的问题中度过了我们的第一个假期，但无论因为学习耽误了多少计划好的活动，她都能理解并陪着我。</p><p>&nbsp;</p><p>一个重要的关键点是克服我不敢使用 DOTA 的障碍：对其他人的机器学习代码进行实质性的修改。我对找到的聊天数据集进行了<a href="https://github.com/openai/finetune-transformer-lm">GPT-1</a>"微调，并做了一些小修改，添加了我自己的原始采样代码。但当我试图生成更长的消息时，它变得非常慢，之前的恐惧也逐渐变成了沮丧， 我实现了 GPU 缓存——这一改变影响了整个模型。</p><p>&nbsp;</p><p>我尝试了好多次，寻求各种帮助，因为它们超出了我头脑中所能容纳的复杂性。几天后，当它可以正常工作时，我才意识到我学到了一些以前我认为不可能的东西：我现在理解了整个模型是如何组合在一起的，包括一些小的风格细节，比如代码基如何优雅地处理 TensorFlow 变量范围。</p><p>&nbsp;</p><p></p><h2>改进</h2><p></p><p>&nbsp;</p><p>经过三个月的自学，我觉得自己已经准备好做一个真正的项目了。这也是我觉得我可以从 OpenAI 的许多专家那里受益的第一点，当 Jakub 和我的联合创始人 Ilya Sutskever 同意为我提供建议时，我很高兴。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/c5/c573d06b9682e7913e1285599b5d6294.png" /></p><p></p><p>&nbsp;</p><p>Llya 在我们公司外唱卡拉 OK</p><p>&nbsp;</p><p>我们开始得到非常令人兴奋的结果，Jakub 和 Szymon 全职加入了这个项目。每当我看到他们在我的机器学习代码库中做出 commit 时，我都感到很自豪。</p><p>&nbsp;</p><p>我开始觉得自己具备机器学习能力了，虽然我还没有完全掌握。但这反映在了我能激励自己专注于机器学习工作的时间上——我现在的编程时间大约是<a href="https://twitter.com/sama/status/792898456650076160?lang=en">我过去工作时间</a>"的 75%。</p><p>&nbsp;</p><p>这是第一次，我觉得我正在步入正轨。起初，我被似乎无穷无尽的机器学习新概念淹没了。在最初的六个月里，我意识到我可以在不断学习全新的基础知识的情况下取得进步。我仍然需要在许多技能上获得更多的经验，比如初始化一个网络或设置一个学习进度计划，但现在的工作感觉是渐进的，而不是潜意识中认为不可能。</p><p>&nbsp;</p><p>从我们的研究员和学者项目中，我知道拥有扎实的线性代数和概率基础的软件工程师只需几个月的自学就能成为机器学习工程师。但不知何故，我说服自己，让自己认为是一个例外，无法学习。但是我错了——即使是在 OpenAI 这样的公司，我也不能进行角色转换，因为我不愿意再次成为一个初学者。</p><p>&nbsp;</p><p>你可能也不是一个例外。如果你想成为一个深度学习的实践者，你一定可以。你需要给自己失败的机会。如果你从失败中学到足够多的东西，你就会成功——而且成功所花的时间可能比你想象的要少得多。</p><p>&nbsp;</p><p>在某种程度上来说，和优秀的专家一起工作对你会很有帮助。这是我非常幸运的一个地方。如果你是一个优秀的软件工程师，请记住，有一种方法可以让你跟优秀的人一起工作——来 OpenAI<a href="https://openai.com/jobs/">工作</a>"吧!</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://www.infoq.cn/article/xFSNjrv9MaNdsODqOITh">https://www.infoq.cn/article/xFSNjrv9MaNdsODqOITh</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/FzhKDk2oUSugP5JLj89l</id>
            <title>2023 深圳国际金融科技大赛决赛名单揭晓，12月16日深圳大学“决战”见！</title>
            <link>https://www.infoq.cn/article/FzhKDk2oUSugP5JLj89l</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/FzhKDk2oUSugP5JLj89l</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 06:09:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 金融与科技融合, 金融科技产业, 深圳国际金融科技大赛, 决赛阶段
<br>
<br>
总结: 金融与科技的融合为传统金融业带来了变革的动力。为了满足金融科技产业的需求，深圳市地方金融监督管理局、深圳市福田区人民政府、深圳市南山区人民政府联合举办了2023深圳国际金融科技大赛。大赛吸引了众多学生团队参与，经过初赛和复赛的角逐，最终有30支队伍进入决赛阶段。决赛将在深圳大学举行，参赛队伍将进行编程马拉松和现场答辩，最终决出各赛道的获胜队伍。决赛阶段还将颁发区块链数字证书，为获奖者提供便捷的查验与认证通道。这场大赛不仅是对技术的考验，也是对心态和意志的挑战。希望参赛者能够展现出最好的一面，冲刺最后的胜利。 </div>
                        <hr>
                    
                    <p>金融与科技相互融合，为传统金融业的变革注入了强大动能，为了满足金融科技产业技术创新及人才需求，更好地推动金融科技产业发展，在深圳市地方金融监督管理局、深圳市福田区人民政府、深圳市南山区人民政府战略指导下，由深圳大学、微众银行、深圳香蜜湖国际金融科技研究院等多方联合举办的“<a href="https://www.infoq.cn/zones/fintechathon/campus2023/">2023 深圳国际金融科技大赛（FinTechathon）——西丽湖金融科技大学生挑战赛</a>"”于 10 月 16 日正式开赛。</p><p></p><p>这是一场面向金融科技前沿技术领域的学生团队<a href="https://www.infoq.cn/news/9AYU96ZSPoCZ6kyClK94">竞赛活动</a>"，大赛初赛报名及作品提交阶段共历时 43 天，向大赛组委会提交报名信息的 1500 余名学生组成了近 300 支学生队伍报名参赛，最终提交了 180+ 份作品。期间，来自深圳大学、清华大学、武汉大学、西安电子科技大学、中科院、微众银行等数十家学企单位专家评委通过技术公开课、线上答疑会、复赛答辩会等方式为赛队提出了专业建议，并对参赛作品进行了专业的评价和打分。经过初赛与复赛的激烈角逐、多位专家评委的综合评估与讨论，人工智能、区块链、产品经理三个赛道入围决赛的 30 支赛队名单终于出炉！以下为具体晋级名单（以下排名不分先后）：</p><p></p><p>人工智能赛道</p><p><img src="https://static001.geekbang.org/infoq/fd/fd91d1857b0853f09ab96eca7838ace8.png" /></p><p></p><p>区块链赛道</p><p><img src="https://static001.geekbang.org/infoq/74/74baa4151ac1c4c0b84700850d0bb2c7.png" /></p><p></p><p>产品经理赛道</p><p><img src="https://static001.geekbang.org/infoq/0c/0ced5e1a309b0154f2cae837f71f8a83.png" /></p><p></p><p>截至目前，我们已经初步见识了参赛选手们的才华和实力，看到了大家夺奖的毅力和决心，然而决赛才是大家最终的竞技舞台。12 月 16 日，本届大赛决赛将正式在深圳大学拉开帷幕，区块链、人工智能赛道的入围赛队需要进行 36 小时编程马拉松，对<a href="https://www.infoq.cn/article/Yuid3uNuLHTe2aeliZbs">初赛</a>"提交的作品进行开发和完善，并参与现场路演答辩；产品经理赛道的入围赛队则需要通过现场答辩的方式来展示自己的项目，包括但不限于项目介绍、产品流程及功能说明等。</p><p></p><p>决赛阶段将产生三个赛道的一等奖、二等奖、三等奖赛队，获胜赛队除了能获得大赛奖杯、纸质获奖证书及独一无二的“区块链数字证书”外，还将瓜分大赛组委会准备的 69W+ 的赛事奖金。值得一提的是，本届大赛所颁发的区块链数字证书，为所有获奖者提供具有唯一标识的数字化获奖凭证。获奖选手可随时查看和下载，校方、招聘企业等也可以扫码快速验证证书真伪，达到可信验证、高效互通的效果。未来，区块链数字证书可进一步拓展到奖学金认证、技能培训认证等场景，为金融科技人才专业资格与资质提供便捷的查验与认证通道，助力构建金融科技人才培养可信体系。</p><p><img src="https://static001.geekbang.org/infoq/96/963443ea0765b2a88dc732d2721ccbd0.jpeg" /></p><p></p><p>据悉，自 2019 年第一届大赛落地，该赛事至今已成功举办 5 届并于去年完成了品牌升级，其通过 5 年时间的沉淀，从“新势力”成长为了具有广泛影响力的赛事标杆。每届大赛的举办都会在行业内引起一波浪潮，共计已有 5000+ 名学生报名参赛，参赛选手们通过比赛，不仅提高了自己的技术水平和知识储备，还锻炼了自己的创新能力和团队协作能力，为未来的职业发展打下了坚实的基础。</p><p></p><p>为了持续保持每届大赛作品的技术前沿性和大赛的公平性，本届大赛组委会又一次特别邀请了多位政、学、企界的专家和大咖担当学术顾问，为大赛提供智力支持，并为金融科技行业发掘更多优秀人才。</p><p></p><p>截至今日，距离决赛开幕仅有 11 天的时间，留给选手们准备的时间已经不多了。这场决赛不仅仅是对技术和知识的考验，更是一场对心态和意志的挑战。对于参赛者们来说，这将是一场展现自我、突破自我的机会。希望大家都能够在决赛场上展现出自己最好的一面，用专业和热情向最后的胜利发起冲刺！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/4kCbbGuk6si8fiy8hfqi</id>
            <title>免费下载！2023 全球金融科技大会 PPT 来啦</title>
            <link>https://www.infoq.cn/article/4kCbbGuk6si8fiy8hfqi</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/4kCbbGuk6si8fiy8hfqi</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 03:13:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: FCon 全球金融科技大会, 科技 + 金融, 创新力量, 金融行业大模型应用
<br>
<br>
总结: 首届 FCon 全球金融科技大会以「科技 + 金融，激发创新力量」为主题，汇聚了来自金融龙头企业的数百名技术高管，探讨了金融行业大模型应用等前沿议题，展示了金融数智化建设的最新实践。 </div>
                        <hr>
                    
                    <p>日前，在上海成功举办的<a href="https://www.infoq.cn/article/BNiefsWtdjeQmreGkaUI?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">首届 FCon 全球金融科技大会</a>"，以「科技 + 金融，激发创新力量」为主题，汇聚了来自金融龙头企业的数百名技术高管，掀起一场探讨新时代金融科技未来的高潮。</p><p></p><p>大会覆盖多个专题，既讨论了「金融行业大模型应用」、「创新的金融科技应用」、「数据要素流通与数据合规」等引领行业的前沿议题，也呈现了多个金融数智化建设的最干实践如「银行数智化建设」、「金融实时数据平台建设之路」、「金融安全与风险管控」等等。</p><p></p><p>当中主要亮点包括但不限于：</p><p>大模型为金融带来巨变，探讨大模型在金融行业的落地探索，金融行业因此面临的范式转换；新一代 HTAP 图技术、Web3 在金融行业的最新应用；数据是数字时代的黄金，但流通和合规是其关键。深入了解数据的流动与合规，探索隐私计算在智慧产业的实践应用；DevOps改变金融业态，加速创新与交付，了解大厂的创新实践；在AI时代，如何更高效地使用大模型、大数据等技术能力，让数据更好地服务于金融市场；揭秘金融实时数据平台的优化之道，深度探讨设计理念、技术挑战，解析未来趋势；围绕网络运行安全、用户个人信息保护、关键信息基础设施安全、网络信息安全等方面来展开讨论具体的技术手段；助力银行全面数智化建设，如何让数据资产充分共享、工具平台提升中台能力，AI应用实现数据化归因和自动优化......</p><p></p><p>经征得大会分享嘉宾同意，InfoQ数字化经纬为您奉上以下精彩演讲PPT！</p><p></p><p><img src="https://static001.infoq.cn/resource/image/96/a6/962290229f467a326fd761222f7290a6.png" /></p><p></p><p></p><p><img src="https://static001.infoq.cn/resource/image/36/48/3688c3b9d621db85517ed79817d93348.png" /></p><p></p><p>点击上方两张海报，扫码关注「InfoQ数字化经纬」公众号、并回复关键词即可获取对应的PPT。深度洞悉科技趋势，助您引领金融创新未来！</p><p></p><p>延展阅读：<a href="https://www.infoq.cn/theme/212">《FCon全球金融科技大会专题报道》</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/HzpHzgZF7ZhGZiYGvtl0</id>
            <title>中信银行：用 AI 搞定普惠型财富管理的高成本问题</title>
            <link>https://www.infoq.cn/article/HzpHzgZF7ZhGZiYGvtl0</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/HzpHzgZF7ZhGZiYGvtl0</guid>
            <pubDate></pubDate>
            <updated>Mon, 04 Dec 2023 07:44:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 客户关系, 普惠型财富管理, 数据和AI能力, 数字化转型
<br>
<br>
总结: 中信银行将财富管理定位于新零售的核心，通过对客户关系的深度经营和普惠型财富管理的推动，实现了对全量客户的服务。为了降低成本并提供更好的客户体验，中信银行借助数据和AI能力，实现了对普惠财富管理的赋能。通过数字化转型，中信银行解决了财富管理行业面临的挑战，实现了客户数字化、产品数字化、渠道数字化、营销数字化、管理数字化和组织数字化。 </div>
                        <hr>
                    
                    <p>随着客户需求日趋个性化、多元化，财富管理已经成为<a href="https://www.infoq.cn/article/Wutse91k66iV8ay6xcQu?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">零售银行竞争</a>"的主战场。为了抢占市场先机，中信银行将财富管理定位于新零售的核心，并在 2021 年设立了财富管理部，牵头推动全行的财富管理业务。</p><p></p><p>在中信银行看来，做好财富管理的关键在于对客户关系的深度经营。而打造普惠型的财富管理，则是中信银行构建深度客户关系，布局新零售的主要抓手。换言之，除了维护传统的中高端客户外，近几年来，中信银行也逐渐面向包括基础客户在内的全量客户提供服务。</p><p></p><p>然而，所谓“全量”的规模达到了亿级，对于银行而言，服务成本极高。如果采用传统人对人的交互和服务模式，不但人力不足、效率不高，并且客户体验也非常差。</p><p></p><p>为此，<a href="https://xie.infoq.cn/article/8a492b1d2ac354a441cbb0e38?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">中信银行</a>"借助数据和 AI 能力实现了对普惠财富管理的赋能。一方面，对于高度基于规则的服务，机器完全可以替代人工，甚至可以实现现更好的效果，从而覆盖更多的客户；另一方面，对于大众富裕客群 / 中小企业，基于技术和算法的智能投顾服务可以降低传统人工投顾的成本，提供“轻财富顾问服务”</p><p>在日前举办的 FCon 大会期间，中信银行财富管理部副处长黄河接受了 InfoQ 采访，分享了中信银行财富管理数字化转型的具体实践。</p><p></p><p>作为关键人，黄河参与了中信银行零售银行的数字化转型规划与推动，通过在客户旅程重塑、超渠道、个性化经营、数字化运营等领域推进产品化与工具化，助力了银行零售数字化能力的升级，在数字化营销和数字化管理领域应用非常深刻的洞察和丰富的经验。</p><p></p><p>以下是对话全文（经 InfoQ 进行不改变原意的编辑整理）：</p><p></p><h5>InfoQ：当前财富管理行业面临的主要挑战是什么？中信银行是如何识别和应对这些挑战的？</h5><p></p><p></p><p>黄河：财务管理目前正处于净值化时代，但市场上由于专业人才稀缺，供给缺陷难以满足客户的广泛需求。因此，我们目前正在打造普惠型的财富管理。除了维护中高端客户外，也希望面向全量客户提供支持。</p><p></p><p>我们认为，在客户服务和需求维护上，应该回归到客户在不同人生阶段的需求展开，比如投资理财、小额分散交易、消费金融和信贷需求以及财富管理等等。</p><p></p><p>具体做法上，中信银行以“三全五主”作为战略引领，以 AI+ 金融为实现路径。所谓“三全”指的是全客户服务、全产品供给和全渠道触达，“五主”旨在深化主结算、主投资、主融资、主服务、和主活动的客户关系。在 AI+ 金融方面，主要采用数据和 AI 自动化替代人工，实现客户下沉和效率提升。同时，通过智能化工具赋能一线服务提供者，从而降低成本。</p><p></p><h5>InfoQ：数字化策略在其中具体是如何赋能和落地的？</h5><p></p><p></p><p>黄河：为了支撑全客户、全产品、全渠道的精准适配，我们进行了体系化的工作，具体包括客户数字化、产品数字化、渠道数字化、营销数字化、管理数字化以及组织数字化。</p><p></p><p>第一，客户数字化解决的是客户洞察的问题的，比如，我们在系统建设上有标签工厂、商机系统以及客户线上行为分析。</p><p></p><p>第二，产品数字化的目的是将所有产品数据化，然后再通过算法去做客户 KYC（Know your customer）和 KYP (Know your product) 的精准适配。</p><p></p><p>第三，渠道数字化包括两个层面：首先，银行渠道众多，其中线上经营是主战场，远程在线渠道主要定位在断点承接和商机挖掘，线下网点渠道则专注于情感链接，提供专业服务；其次，如何实现全渠道协同，这个问题实际上要解决的是，在客户旅程中对应哪些节点应该采用线上轻打扰的方式触达，哪些节点应该采用线下强干预的方式触达，这要结合客户体验、转化效率和服务成本综合考虑和决定。</p><p></p><p>第四，客户数字化、产品数字化和渠道数字化三方面如何串联在一起，我们认为，这需要在营销层面去实现。也就是说，通过<a href="https://www.infoq.cn/article/fE084eOJxKOxJyD1Z7kB">营销数字化</a>"，打造精细化运营体系，解决客户千人千面千时的问题，即——在特定时间、通过何种方式交付给他们产品。</p><p></p><p>第五，管理数字化的核心是要确保所有动作都瞄准我们的“北极星指标”（即第一关键指标，是指当前阶段与业务 / 战略相关的绝对核心指标），在这个过程中，我们需要将最顶层的北极星指标向下拆解到获客、经营、销售、渠道管理等各个团队，不同渠道和团队需要背负不同的指标，但它们最终都会指向“北极星”。这样一来，每个团队的行为举措都会对应的过程指标进行监控，从而形成管理穿透。</p><p></p><p>第六，组织数字化聚焦的是业技融合，如何使得业务和技术在不同的目标下，组建成为一个“同一 KPI、不同 OKR”的灵活交付敏捷团队，这非常考验组织力。</p><p></p><h5>InfoQ：在推进数字化转型过程中，我们遇到过哪些挑战？</h5><p></p><p></p><p>黄河：首先是战略能力方面的挑战。数字化的投入巨大，在具体推行过程中，并非一帆风顺，可能会经历多个成长期，陷入多个瓶颈期。因此，战略的火力和定力都很重要，如此巨大且长周期的投入，如果没有一把手的支持显然是无法实现的。</p><p></p><p>其次，数字化转型与业务经营不能是“两张皮”，数字化带来业务模式创新，成为非线性增长的引擎。但数字化转型不仅仅是科技部门的事情，必须基于业务模式的转型和创新，从而释放业务价值。</p><p></p><p>再者，刚才提到组织数字化和业技融合是很难的，因为涉及两个部门、两个实体，大家的思维模式、经营目标、沟通语境都不一样，如果不是“双向奔赴”，最后很容易变成相互指责。</p><p></p><p>此外，数据基础非常重要，所有技术价值的发挥，都来源于数据全链路、全方位采集，这要求数据具有足够的准确性和及时性。</p><p></p><h5>InfoQ：数据基础的问题中信银行是如何解决的？</h5><p></p><p></p><p>黄河：<a href="https://www.infoq.cn/article/CZU0RmQ8IDLuCdgpcpOY">数据治理</a>"涉及面广、投入巨大且周期较长，为了能够阶段性地释放产能，我们强调的是“应用牵引”，确保客户属性、产品数据、埋点数据、以及及历史订单相关内容的数据完备性和准确性。</p><p></p><p>在中信银行，第一步是做客户数据治理，包括最底层的身份证、职业、学历等客户基础信息，需要在全行层面实现统一，我们将这些信息称为客户主数据；同时，还包括应用层面的客户画像标准化、标签体系建设等等，这些数据会直接支撑业务层面对客户的洞察。</p><p></p><p>第二步是做产品数据治理，最典型的是产品货架梳理。之前我们的产品分散在各个系统，比如理财可能在理财平台上，存款可能在核心系统上，并且随着场景建设的不断增多，还会有越来越多的权益、活动等新的产品形态出现，为了实现统一管理，首先必须建货架，例如金融产品有金融产品的货架，权益有权益的货架，活动有活动中心等等。</p><p></p><p>第三步，做营销层面的统一的内容治理，内容具体包含产品、活动、权益、资讯等等，治理的范围不仅仅包括数据，还包括图文展现的组件、素材等更多的非结构化数据形态。</p><p></p><p>第四步，渠道数据治理，我们也称为渠道统一，这实际上是数据治理中最难的一部分，甚至比业务系统中的结构化数据治理更难，它背后涉及底层组件、渠道之间的协同，以及线上埋点数据的变化等等。非常重要的是，要在前期制定好相应的规范，比如埋点的规范、命名的规范，以及跟活动之间全链路的打通。核心是从用户表、产品表和点击流三个方面入手。</p><p></p><h5>InfoQ：针对于客户关系管理和运营效率提升，数据和 AI 的价值如何体现？</h5><p></p><p></p><p>黄河：我们认为，<a href="https://www.infoq.cn/news/Ve2HG2YEZzThNcSEe7eC">数据和 AI</a>" 会带来商业模式的巨大变化。</p><p></p><p>过去，银行总分支行三者之间的关系是，由总行作为管理角色，对分行进行督导、考核、培训，通过总行推动分行、分行推动支行的模式来经营客户。这种客户经营模式带来的是业务的线性增长，劣势是难以摆脱地理区域限制。比如，银行想要业务增长，就必须开设网点、招聘更多人员。如今，随着人口红利的消退，这种线性增长越来越难以实现。</p><p></p><p>数据和 AI 技术的价值在于，它可以帮助银行开辟新的赛道。</p><p></p><p>比如，总行除了管理职能，还可以作为经营单元，依托数据、策略、平台，集约化地开展客户经营。其中，我们把 AUM（资产管理规模）在 5 万以下的基础客户交由总行直接经营，因为对于总行来说，可能只需要投入 7~10 名数据人员就可以经营管理超过 8000 万的客户，但这部分客户如果交给分行团队去维护，收入是覆盖不了成本的。</p><p></p><p>通过这种模式的转变，我们认为带来的一定是指数级增长，因为机构的数据、技术、人员投入的边际成本不断下降。这就是数字化的意义，它一定会带来业务模式的创新，带来新的增量空间。</p><p></p><h5>InfoQ：是否可以结合具体场景介绍一下，如何把数据和 AI 融入到整个商业模式创新中去？</h5><p></p><p></p><p>黄河：数据本身是一个基本盘， AI 解决的是核心的攻坚问题以及自动化的问题。</p><p></p><p>最典型的场景之一是，进行<a href="https://www.infoq.cn/article/GcdCeRTEHlT9tUYDN55A">人货场的匹配</a>"。比如，基于算法实现精准营销，并不断进行营销策略优化和自动化能力的提升；或者基于归因算法分析每次营销活动的成功原因等等。</p><p></p><p>第二个场景是机器人应用和人机交互的模式创新。比如，可以让机器人在 APP 上与客户进行交互，从而降低人员的成本投入。在机器人背后，涉及的是自然语言处理、ASR、TTS 技术等等，同时也包括今年业界热议的语言大模型。</p><p></p><p>举几个例子，中信银行目前利用 AI 已经可以实现资讯推荐、产品推荐和权益匹配。</p><p></p><p>比如，我们可以根据市场动态和客户需求、消费偏好等信息来为用户进行内容推送。根据数据统计，在中信银行 APP 的资讯板块，通过这种智能化、个性化的内容推荐，板块流量比去年同期增长了 36％，客户点击量增长了 37％；</p><p></p><p>产品推荐方面，通过千人千面的投放，比如，在用户使用 APP 的过程中进行弹框提醒，使得财富管理产品的平均曝光数比去年同期增长了 3 倍以上；</p><p></p><p>权益匹配方面，基于智能化分析，我们可以了解到用户的更多偏好，比如喜欢立减金还是咖啡券，然后对应地发放给用户，从而增加兑换率。具体而言，我们的权益兑换率比去年同期增加了 5%。</p><p></p><h5>InfoQ：显然，AI 还无法独立完成所有工作，那么人和机器应该如何分工和协作？</h5><p></p><p></p><p>黄河：这背后实际上是业务驱动和算法驱动如何分配的问题。</p><p></p><p>业务驱动是以 KPI（如营收）为导向的，而算法驱动实际上是以点击率为导向的。比如，客户更愿意点击某个产品内容，系统就会进行加强推送。但问题是，由算法推送带来的点击率和最终的营收目标会存在一定差距；然而，你也不应该过分强调营收，因为高费率产品可能会影响客户满意度，反向导致营收目标无法完成。</p><p></p><p>所以，关键问题是如何将业务驱动与算法驱动结合起来。在中信银行，我们的思路是回归到业务价值中，通过对推荐引擎进行设计，同时匹配相应的管理手段，在营收目标和客户体验之间实现权衡，去实现不同产品和不同条线间的流量分配，甚至在更细的产品品类和客户分群方面进行权重整合。</p><p></p><p>当然，其中的权重如何分配是无法一次性做到位的，必须在运营和监控过程中不断进行迭代，逐渐调整到一个最优化的状态或者不断逼近最优解。</p><p></p><h5>InfoQ：针对今年爆火的大模型，您有什么样的期待？</h5><p></p><p></p><p>黄河：我们希望通过<a href="https://www.infoq.cn/news/Xhlku65TOzhUtKR2yaSi">大模型</a>"能够解决更多客户经营的问题。</p><p></p><p>例如，在营销素材方面，现在营销素材的生产工作非常重，银行一般没有专门的人员，需要靠外包，但是外包人员的管理相对困难。而大模型在这方面是具有独特优势的，甚至 90% 的工作它都可以独立完成，人在其中的作用主要是对素材进行加工或审核。</p><p></p><p>另外，在人机交互方面，如何更好地、全方位地回答客户问题，带来更好的交互体验，大模型在背后也会发挥关键作用。但由于银行是一个强监管的行业，这方面的应用还依赖于专业领域大模型的进一步发展。</p><p></p><p>在中信银行，目前我们也在尝试通过大模型实现对内赋能，比如有部分数据需求就可以直接与大模型沟通，告诉它想要的数据类型，并进行加工。通过这样的方式，可以让数据管理和使用的门槛在一定程度上降低。</p><p></p><p>值得关注的是，数据体量和数据质量非常重要。虽然银行过去的数据治理基础比较好，但是银行产品的专业化程度更高，对安全合规的要求也更高，所以在具体落地应用的过程中，行稳是基本前提。</p><p></p><h5>InfoQ：为了进一步推进数字化相关工作，您下阶段最关注的课题或技术是什么？</h5><p></p><p></p><p>黄河：从前沿性技术方向来看，大模型毫无疑问是一个重要领域。</p><p></p><p>除此之外，归因分析也是我们非常关注的细分领域。过去大数据更强调相关关系而不是因果关系，但是这种分析方式更适用于营销场景而不是决策场景。我们认为，数字化最终影响的是决策环节，而决策涉及方方面面，仅有相关性是远远不够的。所以，归因分析会成为下一阶段的重点。</p><p></p><p></p><h4>内容推荐</h4><p></p><p>11月19日-20日在上海成功举办的首届 FCon 全球金融科技大会，以「科技 + 金融，激发创新力量」为主题，汇聚了来自金融龙头企业的数百名技术高管，掀起一场探讨新时代金融科技未来的高潮。经征得大会分享嘉宾同意，InfoQ 数字化经纬为您奉上精彩演讲 PPT！关注「InfoQ 数字化经纬」，回复「金融创新」即可获取 PPT，深度洞悉科技趋势，助您引领金融创新未来！</p><p></p><p><img src="https://static001.geekbang.org/infoq/0e/0e09b84945701548f14ab91a2c49ef51.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>