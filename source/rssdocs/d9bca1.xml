<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/hy8Mpu6pEDxPHLubX9oK</id>
            <title>我在构建 MLOps 系统四年中学到的经验</title>
            <link>https://www.infoq.cn/article/hy8Mpu6pEDxPHLubX9oK</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/hy8Mpu6pEDxPHLubX9oK</guid>
            <pubDate></pubDate>
            <updated>Wed, 18 Sep 2024 09:45:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>正如标题所述，我致力于构建 MLOps 系统已经有近四年了。世界变化得很快，作为一名也有了四年经验的程序员，我感觉自己一直在努力不被深度学习（LLM）的新技术淹没，努力适应软件工程，努力远程获得好公司的好职位，等等。</p><p>&nbsp;</p><p>这篇文章一半是对我多年经历的审慎回顾，另一半是我对工程、机器学习（运维）的看法。我想你的脑海中也曾浮现过这些问题，但我不会回答任何问题，只是分享我的观点。</p><p></p><h2>第一个 ML 问题，提前一日的用电量预测</h2><p></p><p>2021 年，我开始研究能源消耗模型，这是我第一次真正深入研究运维应用程序。这个问题一开始很简单：我们必须预测八个城市每 24 小时的每日用电量，但要提前 24 小时，这个项目名为日前电力预测。自从我开始研究这个问题以来，用户数量不断增加，普通消费者开始使用更多能源，新工厂也正在建设中。能源需求也会随着经济波动而变化。这一切都意味着更多模型漂移和数据漂移。</p><p>&nbsp;</p><p>一段时间后，我定下了预测模型所用的一系列算法。在尝试了深度学习模型和基于树的模型后，我发现最成功的是 LightGBM 和 XGBoost（以及集成深度学习模型）。</p><p>&nbsp;</p><p>基于树的模型的问题在于，它们的预测通常受你提供的数据的限制。例如，如果你尝试预测今年夏季的第一天，而去年同日的峰值消耗为 600 单位，由于基于树的模型的性质，除非你正确地定制问题，否则你的预测值是不会超过 600 单位的。但是，当事件和特征稳定且输出分布没有太大变化时，基于树的模型也可以提供出色的预测结果。</p><p>&nbsp;</p><p>说实话，我当时的大部分经验都来自 Kaggle。我在那里待了大约九个月，深入研究预测竞赛、创建新特征并尝试不同的模型。我提出的模型得分为 7/10，我知道如果我与真正的专家竞争，他们可能彻底击败我的模型。</p><p></p><h2>意识到需要 MLOps 系统</h2><p></p><p>预测未来的能源消耗是一种奇怪的体验，尤其是现实中有太多因素在影响电力使用——天气、周末与工作日、假期，甚至祈祷时间。在像土耳其这样四季分明的地方生活又增加了一层复杂性。我不是一个追求完美的模型制作者，我努力让自己的模型适应突然的变化。例如，如果连续两个月的气温都稳定在 25°C，然后温度突然在一天内下降 5°C，模型通常会假设电力消耗将与之前的 25°C 天气差不多，但事实并非如此。人们的习惯会随着季节而改变，预测这些微妙的变化会很困难。然后，还会有像国家足球比赛这样不可预测的事件——祝你建模好运！</p><p>&nbsp;</p><p>那么，我做了什么工作来解决模型和数据漂移问题呢？在过去的一年里，我创建了数百个特征，使用 XGBoost 和 LightGBM 构建了一些基于树的模型，并用前五日的数据来验证它们。 MLFlow 是日常模型生产过程中的主要工具。虽然它可能不是验证模型的完美方式，但它确实改善了长期预测结果。至少，当电力消费比较稳定且模型适应性良好时，它给了我一个安全的选择。</p><p>&nbsp;</p><p>为了简化流程，我建立了一个完整的系统，每天早上通过 API 提取数据、生成特征、挑选模型并进行预测。我构建的自动化脚本只用一分钟就能跑完。然而即使有了自动化脚本，你也必须密切关注预测结果，特别是在国定假日或意外事件期间。</p><p></p><h2>虽然你部署了一个模型，但你可能不喜欢它</h2><p></p><p>我的一段奇怪的经历就是和我的经理争论预测结果中突然出现的漂移。他认为，如果夏天天气突然下降，预测第二天的电力消费会更容易，因为运行的空调数量会减少。他甚至手动调整了几次预测，但结果适得其反。这基本上结束了我们的争论。虽然我相信有些日子通过手动调整可以带来更好的预测结果，但模型经常会发现一些我们没有发现的模式。</p><p></p><h2>ML 人员在软件工程领域的沉浮</h2><p></p><p>后来，我转而从事医疗保健领域的 MLOps 平台工作。我花了很多时间寻找可以从事 MLOps 的工作，结果很幸运地找到了一家医疗保健初创公司，我成为了那里的第一位全职工程师。我有医疗保健模型方面的经验，所以感觉这个行业很合适。我找到了他们，他们也找到了我，这纯属运气。</p><p>&nbsp;</p><p>在那里工作的三年是一段充满挑战的旅程，主要因为我从专注于模型转向了编写平台。我一直是那种喜欢做研究的人，我会实现各种论文了结果，并尽可能多地从 Kaggle、众多教授和学者那里汲取知识。举例来说，我对深度学习模型和表格模型之间的差异很着迷，尤其好奇为什么深度学习应用在处理表格数据时经常遇到困难，而基于树的模型却表现出色。我读过论文，在 X/reddit 上讨论这个话题，我就是那种人。但当我转而编写平台时，我才意识到自己还需要学习很多关于编写生产级代码的知识。一开始我搞砸了很多事情。</p><p>&nbsp;</p><p>在一开始的几个月里，我们在 Jupyter 笔记本中进行模型评估，我阅读了一些有关如何深入评估医疗保健模型的论文，也考虑到了性别和种族偏见。工作一开始很有趣，但后来我们必须将所有内容集成到一个平台中。那时我慢慢开始了解面向对象编程、系统设计和传统软件工程的原理。我不是计算机科学工程师，所以我是一点一滴学会了这些东西。</p><p>&nbsp;</p><p>创业生活非常紧张，工作时间长，学习曲线陡峭。我们构建的系统使用了 MongoDB、Python、RabbitMQ、S3 和 AWS——一个相当标准的管道。我们的平台旨在验证医疗保健模型，获得 FDA 的批准，并确保一切都正确完成。数据来自合作伙伴，但模型供应商从未看过原始数据，因为他们不应该有这个权限。因此，我们的业务目标是根据获得的黑箱数据来验证模型，并为 FDA 准备必要的文件。</p><p></p><h2>MLOps 平台与业务逻辑，我们是只部署模型还是为客户提供服务？</h2><p></p><p>为了让我们的平台实现目标，它需要支持所有类型的医学图像、验证任何计算机视觉模型并检测出可能存在的模型偏见。三年来，这个平台关注的重点也在改变。第一年，我们的目标是部署和验证模型。第二年，我们增加了注释功能，支持了医疗保健数据，并实现了云集成。到第三年，我们意识到我们需要关注客户的一些特定需求。</p><p>&nbsp;</p><p>挑战之一是将平台的逻辑与特定于客户的代码库分开来。我们花时间编写了特定于客户的代码，这些代码会让平台上 80% 的功能受益，但有时我们必须为特定客户实现非常具体的逻辑。这引发了很多关于我们是否应该用特定于客户的功能来增强平台，还是将它们分离开的争论。如果用太多特定于客户的功能来增强平台，它可能会变得臃肿和混乱。另一方面，当客户端代码库需要访问平台数据时，将它们分离开可能会导致复杂的情况。</p><p>&nbsp;</p><p>我还是没法确定如何区分 MLOps、MLE、后端工程和业务逻辑。可能这没有唯一的答案，但我认为我们在维护和开发平台方面做得很好。</p><p></p><h2>在云端部署 vs 在本地部署</h2><p></p><p>最近我参加了银行业的 MLOps 职位面试，我 90% 的时间都在写代码。面试我的经理是一位从 DevOps 过渡到 MLOps 的人，我相信他已经开发了一些 ML 模型。对他来说，模型只是具有特定输出的 docker，你还需要做管理/跟踪/记录的工作。对于一些人来说，这才是真正的 MLOps 工程。我绝对同意这一点。</p><p>&nbsp;</p><p>他的团队正在 Apache Airflow 上部署模型，他问我在这方面有什么经验。我想到的是：</p><p>&nbsp;</p><p>正如问题所暗示的那样，我在电力问题上训练/部署的模型是日常模型，所以我不需要跟踪模型，做预测就行了。我不需要检查模型是否一直处于活动状态，也不需要检查吞吐量或延迟。我在医疗保健 MLOps 问题中使用的模型是基于项目的模型，不需要不断构建模型，而且它是本地的，所以安全不是主要问题。</p><p>&nbsp;</p><p>他的团队一直完全不写代码，至少在目前是这样。在面试过程中我意识到了这一点，这很奇怪。</p><p></p><h2>身份危机：MLOps 工程师、ML 工程师，还是两者兼而有之？抑或其他？</h2><p></p><p>多年来，另一个问题一直浮现在我的脑海中：我是谁？我是 MLOps 工程师、ML 工程师、ML 研究员还是后端工程师？在一个小团队或初创公司中，你需要具备所有这些能力。你听说过“10x 工程师”吗？那些奇怪的职位要求你同时发布 NeurIPS 论文和 Node.js 代码？伙计，这到底是为什么？我见过很多这种奇怪的组合。不是吹牛，但下面这些都是我所做的工作：</p><p>&nbsp;</p><p>预印我在斯坦福实习期间的一篇论文 = ML 研究员编写表格和 CV 模型 = 数据科学家实现一个 Auto-ML 模块，允许即时训练/微调对象检测、分割和分类模型，并进行版本控制 = Python 软件工程师、ML 工程师使用 Grad-CAM 为 CNN 模型开发一个可解释性库 = ML 研究员使用 FastAPI 支持模型，与前端应用程序集成 = 后端工程师使用 Docker、RabbitMQ、MongoDB 和其他工具设计和实现一个平台。= MLOps 工程师</p><p>&nbsp;</p><p>当然这些术语是可以互换的，当然优秀的 MLE 必须是一名优秀的软件工程师，当然你需要考虑如何在训练模型时部署模型，但我到底是什么角色？我觉得我被“学得更多，收获更多”这句话欺骗了。</p><p></p><h2>10x 工程师神话：什么都会，某些事情还要精通（也许做不到）</h2><p></p><p>那么我从中得到了什么？我是不是一名没什么专长的 10x 工程师？好处是，我可以申请数据科学、MLOps、MLE 和 Python 后端职位，这里的重点是 ML。而且我参加了所有这些职位的面试。但问题是什么？在面试过程中，他们并不总是相信我的经验广度。他们会问很多问题，即使我有很好的答案，一些面试官也会试图让我出局。如果你没有给出他们想要的具体答案，你就出局了。</p><p>&nbsp;</p><p>我在一家全球数据科学公司的面试中就遇到过这种情况。对于我提到的所有话题，他都会质问我，还会不断切换话题背景，让我觉得自己像个白痴。当他发现我在 3 小时的面试后给出了一个不太好的答案时，他说：“我就知道。”我都无语了，你知道什么？当然，我不是精通所有领域的专家，但这也很让人不爽。</p><p>&nbsp;</p><p>还有一次，我被一个非常聪明的人面试了——他曾经在知名公司工作，在排名前十的大学获得博士学位。他非常聪明，我希望十年后能成为他那样的人。他很善良，我非常尊重他。我告诉他我的经历和我做过的事情，并问我还能做些什么。但你猜怎么着？他希望我在 LLM 方面有更多经验，他说：“我没有投资潜力股的预算；我需要员工在那个特定主题方面有足够经验。”没错，但这样的话我该怎么办？我应该追随每一个深度学习趋势并以赚钱为目的研究它们吗？这有多大可能？你能对一个只有四年经验的人能有什么期望？</p><p></p><h2>我将来想成为什么样的人？</h2><p></p><p>我不知道。我真的不知道。我知道的是：</p><p>&nbsp;</p><p>FOMO（错失恐惧）在 ML 领域中是真实存在的。如果你尝试学习 MLE，那么 MLOps、软件工程或者一些 DevOps 方面的知识和经验可能不会让你更有信心。</p><p>&nbsp;</p><p>原文链接：<a href="https://mburaksayici.com/blog/2024/08/29/what-ive-learned-building-mlops-systems-for-four-years.html">https://mburaksayici.com/blog/2024/08/29/what-ive-learned-building-mlops-systems-for-four-years.html</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/9lBoqN5m9lmeyeFmY8EF</id>
            <title>国内近 50 款 AI Agent 产品问世，技术足够支撑应用可靠性了吗？</title>
            <link>https://www.infoq.cn/article/9lBoqN5m9lmeyeFmY8EF</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/9lBoqN5m9lmeyeFmY8EF</guid>
            <pubDate></pubDate>
            <updated>Tue, 17 Sep 2024 02:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在2024年5月发布的《中国AGI市场发展研究报告》中，InfoQ研究中心将 AI Agent 定义为连接模型层与应用层的中间层，是现阶段大模型落地应用的重要补充。那在过去的两个季度，AI Agent 领域发生了诸多变化，本文希望通过分析技术框架、理想与现实的差距，以及厂商背景，为大家提供对AI Agent现状的全面理解。</p><p></p><p>更多关于 AI Agent 的具体应用案例，欢迎点击<a href="https://www.infoq.cn/minibook/bTgj82D3gFJK9ZLRM5Ci">「链接」</a>"下载完整报告。</p><p></p><h3>AI Agent技术框架趋于统一</h3><p></p><p></p><p>自 2023 年 3 月起，以 AutoGPT 为代表的一系列技术框架发布后，AI Agent 凭借其自主性和问题解决能力，迅速成为科技圈讨论的焦点。在随后的时间里，技术领域陆续推出了多种 AI Agent 技术框架，涵盖通用、环境模拟、软件开发、多模态、翻译、终端交互、数据分析等多种类型。同时，关于单智能体与多智能体的讨论也在持续。</p><p></p><p><img src="https://static001.geekbang.org/infoq/83/834fcc697a798b2e2094e5a9c36a18a0.png" /></p><p></p><p>在技术框架的不断探索中，AI Agent 的技术框架认知逐渐统一。大模型作为智能体的大脑，指导规划、工具使用、记忆三大基本能力模块具体行动。并在具体行动过程中，通过与环境、其他智能体以及人类的交互反馈，促进智能体的不断进化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/14/14e4813caf4292545d135c6bd49bc2ef.png" /></p><p></p><p></p><h3>大模型「大脑」足够聪明到支撑AI Agent落地了吗？</h3><p></p><p></p><p>从技术框架的角度，我们可以看到大模型在智能体中的重要性，这也引发了一个关键问题：大模型「大脑」是否足够聪明以支持 AI Agent 的实际落地？</p><p></p><p><img src="https://static001.geekbang.org/infoq/bb/bb9b1ff210b4c8bb46d43f45a213ebde.png" /></p><p></p><p>作为智能体的大脑，大模型在短短两年内经历了三次主要更新和竞争重点的转变。然而，针对工具调用或真实环境模拟的国内外测试结果显示，当前大模型的表现仍不尽如人意。例如在 WebArena 测试中，GPT-4的成功率也仅有 14.9%，今年发布的 GPT-4o 也并没有获得明显提升。</p><p>注：WebArena通过构建一个智能体命令和控制环境，通过对大模型在电子商务、社交论坛、软件开发协作和内容管理四类环境中一系列评估任务的功能正确率进行评估。网址：<a href="https://webarena.dev/">https://webarena.dev/</a>"。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0e/0e20b32ee4aa49103929a263a6b4adf6.png" /></p><p></p><p>此外，在T-Eval基准测试中，各大模型在推理得分方面普遍偏低且模型间差距明显。</p><p>注：T-Eval大模型智能体基准测试，是专门针对智能体工具使用的全过程设计的基准测试，包含：规划（Plan）、推理（REASON）、检索（RETRIEVE）、理解（UNDERSTAND）、指令跟随（INSTRUCT）和审查（REVIEW）。</p><p><img src="https://static001.geekbang.org/infoq/66/6628a99507688e9badec3003ac0a7697.png" /></p><p></p><p>在本次报告的访谈中，来自一线的专家也提及，当前大模型在任务拆解和规划能力方面仍存在明显不足。现阶段，依赖大模型进行独立思考和自主规划路径的方式，尚不足以确保智能体的可靠性和任务成功率。</p><p></p><h3>理想中的智能体和现阶段有哪些差距？</h3><p></p><p></p><p>除了规划能力与理想状态存在一定差距外，InfoQ研究中心还从自主思考、工具调用、记忆和多模态理解等方面，深入分析了理想中的智能体与现阶段智能体之间的差距。这样的技术现状也对 AI Agent 的开发与应用提出了更高的要求，迫使技术团队不断优化系统的可靠性，以实现更加全面的任务执行能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e7/e715fe5812e09f0a213b0e0e6abe5adf.png" /></p><p></p><p></p><h3>目前中国市场中，有哪些AI Agent产品已经面世？</h3><p></p><p></p><p>InfoQ研究中心还发现，目前在各个领域，已有众多不同类型的AI Agent产品面世，并且不同的产品从例如工作流等不同的方面提供了技术解决方案。因此InfoQ研究中心也从平台类和垂直类的角度出发，盘点了近 50 个中国市场中的 AI Agent 产品，并形成《中国 AI Agent 产品罗盘》。</p><p></p><p>《罗盘》仍将持续更新，欢迎各位开发者和读者朋友们积极反馈和持续关注，也欢迎各类厂商参与交流，与InfoQ研究中心分享技术和产品的最新动态（联系方式：InfoQ研究中心首席分析师 姜昕蔚：18618257676）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a7943dfbc3672f5546c807c4b3b22dc.jpeg" /></p><p></p><p></p><h3>参与AI Agent市场竞争的厂商背景如何？</h3><p></p><p></p><p>除了产品盘点外，InfoQ研究中心在对市面上对外提供 AI Agent 服务的厂商进行研究，并发现其背景主要分为大模型创业厂商、互联网科技厂商、RPA/流程自动化厂商和数字化企业服务商。</p><p></p><p>大模型创业厂商：以 Dify、澜码科技、面壁智能为代表，借助自身大模型技术基础，满足企业大模型技术实际应用的需求。其主要竞争优势在于对大模型具有技术前瞻视角。其主要通过提供AI Agent 应用市场 &amp; 开发平台，为用户提供构建 AI agent 的便捷服务。</p><p></p><p>互联网科技厂商：以百度、火山引擎、腾讯为代表，借助借助自身大模型以及 AI 云服务，为客户提供完整的 AI 技术解决方案。因其自身基础设施、云、大模型等AI 生态建设完整。同时先前多推出了大模型相关的应用，建立了较为良好的用户基础和产品迭代模式。其主要也通过提供 AI Agent 应用市场 &amp; 开发平台，为用户提供构建AI agent的便捷服务。</p><p></p><p>RPA/流程自动化厂商：以来也科技、实在智能为代表，其主要将 AI Agent 技术思路集成进原有RPA产品中，依托自身长期积累的企业内流程自动化落地经验，为客户提供更智能化的 AI+RPA 类产品和服务。</p><p></p><p>数字化企业服务商：以用友、金蝶、标普云、数势科技为代表，依托自身长期积累的垂类领域或行业的 Know-how，实现企业内数字化系统的功能升级。对于此类厂商而言，AI Agent 多作为一个功能组件，内置进数字化系统，通过完善的 API 联动生态，实现与原有数字化系统的深度集成，从而让用户无感地体验 AI Agent。</p><p></p><p>更多关于AI Agent在数据分析、营销、金融、文娱游戏等的具体应用案例，欢迎点击<a href="https://www.infoq.cn/minibook/bTgj82D3gFJK9ZLRM5Ci">「链接」</a>"，下载完整报告阅读。InfoQ研究中心也期望通过持续的内容输出，继续支持中国AI领域的发展。</p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/WFlwMggJjIVBIQGt7XrA</id>
            <title>OpenAI 发布最新模型 o1，这次变为华人扛大旗？一分钟搞出 3D 版贪吃蛇，好用但小贵</title>
            <link>https://www.infoq.cn/article/WFlwMggJjIVBIQGt7XrA</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/WFlwMggJjIVBIQGt7XrA</guid>
            <pubDate></pubDate>
            <updated>Sun, 15 Sep 2024 07:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>整理 | 华卫、核子可乐</p><p></p><p>9 月 12 日，OpenAI 万众期待的“草莓”（Strawberry）终于上线了。这一新模型名为 o1，是 OpenAI 推理模型家族的首位成员，能够解决现有 AI 模型所无法攻克的科学、编码和数学难题，甚至包括 OpenAI 最强大的现有模型 GPT-4o。但与此同时，o1 模型也比 GPT-4o 价格更贵、生成速度更慢。</p><p></p><p>该公司表示，o1 模型并不是 GPT-4o 的继任者，而只是对 4o 的强大补充。o1 不再像传统大语言模型那样一步步得出答案，而是通过推理拆解问题，像人一样行之有效地给出思维步骤，最终得出正确结果。并且，OpenAI 还推出了体量更小、价格更低的 o1-mini 版本。</p><p></p><p></p><h1>思维更像人了，编码、数理能力指数级增长</h1><p></p><p></p><p>“使用 o1 在一分钟内创建 3D 版本的贪吃蛇游戏！”在国外某社交平台上，一位网友发布了完整的演示视频。</p><p></p><p>还有网友通过合并o1 和 Cursor Composer，在10 分钟内为 iOS 创建了一个带有动画的完整天气应用程序。</p><p></p><p>有网友这样评价 o1，“编码和数学能力是 GPT-4o 的指数级增长！现在每个人都可以建造任何东西！”还有人称，“Cursor 和 Replit 的压力暴增”，“奥特曼用 o1 杀死了 Cursor、Replit 和其他许多代码模型”。</p><p></p><p>OpenAI 官方还则展示了使用这套新模型解决现有模型 GPT-4o 也无法解决的几个问题，其中包括两道令人费解的数学题。</p><p></p><p>第一道：“已知公主的年龄比王子大，当公主的年龄是王子年龄的两倍时，此时公主年龄是二人现在年龄总和的一半，问公主和王子现在多大？”该模型成功理解了这一问题中的不同变量并确定了解决问题所需的方程式，最终分步给出了正确答案（王子 30 岁，公主 40 岁）。</p><p></p><p>OpenAI 还专门设计了界面来显示模型思考时的推理步骤，其中令人印象深刻的是，o1 似乎在刻意模仿人类的思维：“我很好奇”、“我正在认真思考”和“好的，让我想想”之类的表达，真的营造出一种它在分步推理的感觉。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c4/c429efd5bccf5918331c5d94bd5fd4ef.png" /></p><p></p><p>OpenAI 首席研究官 Bob McGrew 表示，“与之前的模型相比，o1 的确在某些方面感觉更像人。”同时，McGrew 指出，“这套模型在解决 AP 数学考试方面的表现比我更好，我还是数学专业出身呢。”OpenAI 研究副总裁 Mark Chen 则介绍称，“新模型正在学习独立思考，而不是像传统大模型那样试图模仿人类的思维方式。”</p><p></p><p>第二道，要求 o1 数出“strawberry”这个单词里有几个 r。由于基于 token 化（即大模型以 token 数据块的方式处理单词）模式，大多数语言模型对于单词中个别字符的差异往往视而不见。很明显，o1 具有自我反思能力，可以在未经用户提示的情况下理解如何计算字母数量并给出正确答案。</p><p></p><p>据 OpenAI 介绍，在针对数学专业学生的美国数学邀请赛（AIME）中，GPT-4o 的平均解题成功率为 13.4%，而 o1 的正确率则高达 83.3%。在 Codeforces 在线编程竞赛当中，这套新模型的排名为全体参赛者中的第 89 百分位。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/79/79c755621c999a89773b4a992061b21a.png" /></p><p></p><p>“刚刚测试了一些物理问题，目前的测试结果是 100% 正确的。”物理学科上，o1 的表现也获得了不少网友的认可。</p><p></p><p>OpenAI 宣称，在解决博士水平的物理问题时，o1 达到了 92.8 分的水平，GPT-4o 才 59.5 分。并且， o1 的下一个更新版在其测试中，“在物理、化学和生物学等具有挑战性的基准任务上，拥有与博士生相当的解题表现。”</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/37/371773cbfcc070534c3ce5c2da7c930d.png" /></p><p></p><p>不过，对于这个评分结果似乎也存在一些争议。有网友指出，“AP 物理成绩是 89 分，而博士成绩是 92.8 分？如果按照这个结论，这些数字就说不通了。原因在于博士水平的问题来自预制题集，而非一般问题。实际上，这仍属于 AP 物理能力水平。”</p><p></p><p>另外，在 OpenAI 官方分享的演示中，还有一个要求 o1“写一个语法正确的句子，并且不要重复使用任何字母”的案例。最终，o1 花了 39 秒的时间给出了“go fix my bed”的答案。</p><p></p><p>从目前 o1 的这些案例演示中可以看到，该模型的确更适合解决数学、物理和编码等科学领域的复杂问题。OpenAI 也在官方公告指出，“医疗研究人员可以使用 o1 模型标注细胞测序数据，物理学家可以使用 o1 模型生成量子光学所涉及的复杂数学公式，各领域的开发人员也可以使用 o1 模型构建并执行多步骤工作流程。”</p><p></p><p>然而，现在 o1 还有许多不足之处。首先，目前亮相的 o1 预览版仍有一定局限性，如无法浏览网页或接收上传的文件和图像。OpenAI 表示，对于这类任务，GPT-4o 仍是最佳模型选项。</p><p></p><p>其次，o1 初见之下令人惊艳，但也已在用户的长时间使用中暴露出短板。例如，沃顿商学院教授 Ethan Mollick 向 o1-preview 提交了八条关于填字游戏的线索，要求其将内容翻译成文本。o1 模型通过多个步骤共耗时 108 秒才给出答案，虽然结果完全正确，但虚构了一条 Mollick 并未给出的特定线索。</p><p></p><p>还有一位网友指出，o1 不能解决后向传播推理的问题。</p><p></p><p></p><blockquote>输入：给定这个不完整的序列：. . . . . 1 2 3 3 2 1 1 2 3 3 2（空白处用“. ”表示），请补全序列。o1 的结果：位置 1-7 错误即使 1-7 是正确的，也错误地将其放在完成的序列中。完整序列中的错误‍</blockquote><p></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/0f/0f9dcdeffe1e2774592745b8e10e377d.png" /></p><p></p><p></p><h1>定价高 GPT-4o3 倍以上，改进目标竟是加长响应时间</h1><p></p><p></p><p>“o1 背后的训练方式与之前的大模型有着根本性的不同。”OpenAI 公司研究负责人 Jerry Tworek 解释称。尽管该公司对于具体细节含糊其辞，但承认 o1“使用的是一种全新的优化训练算法，同时辅以为其量身定制的新训练数据集。”</p><p></p><p>OpenAI 已经成功教会之前的 GPT 模型如何模仿训练数据中的模式，而 o1 使用的是名为强化学习的技术以训练模型自主解决问题。这项技术通过奖惩机制引导系统行为，即在模型答对时给予正反馈、答错时给予负反馈，借此改进其推理过程。以此为基础，o1 可以使用“思维链”来处理查询，类似于人类以分步方式解决问题。</p><p></p><p>OpenAI 表示，凭借这种新的训练方法，o1 模型生成的结果应该会更加准确。Tworek 指出，“我们注意到这套模型的幻觉有所减少”，但问题仍然存在，“尚不能说我们彻底消灭了幻觉。”</p><p></p><p>据 OpenAI 介绍，这套新模型与 GPT-4o 最大的区别在于，能比前辈更好地解决编码和数学等复杂难题，同时还可对推理过程做出解释。Tworek 表示，OpenAI 并不认为 AI 模型的思维等同于人类思维。但他表示，推理界面确实能够展示模型如何花费更多时间深入剖析并解决问题。</p><p></p><p>Chen 解释称，OpenAI 已经成功建立起一套通用性更强的推理系统。“我认为我们确实在这方面取得了一定突破，而这也是 OpenAI 的优势之一。实际上，它在所有领域的推理方面都有相当不错的表现。”</p><p></p><p>但与此同时，o1 在很多领域的能力则不及 GPT-4o。OpenAI 推理研究科学家、德扑 AI 之父 Noam Brown 表示，“我们的 o1 模型并不总是比 GPT-4o 好。许多任务不需要推理，有时等待 o1 响应与快速 GPT-4o 响应是不值得的。”</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/4a/4ab4f0a7d51437bf2553ec6562b1c4bb.png" /></p><p></p><p>想象的 openAI 的 o1 在后台工作</p><p></p><p>值得一提的是，对于 o1 的推理思考时间，未来 OpenAI 的改进方向是继续增加。</p><p></p><p></p><blockquote>目标是让未来的版本思考数小时、数天甚至数周。推理成本会更高，但您会为新的癌症药物支付多少费用？又会为突破性的电池和证明黎曼假说付出多少？AI 可以不仅仅是聊天机器人。</blockquote><p></p><p></p><p>在谈到安全问题时，OpenAI 表示，根据拜登总统先前发布的 AI 行政令规定，o1 模型已经向美国和英国安全研究机构开发以接受早期测试。在 OpenAI 最难的越狱测试之一中，GPT-4o 得分为 22（0-100 分），而 o1-preview 模型得分为 84。</p><p></p><p>定价方面，o1 大约是 GPT-4o 和 100x 4o mini 的 3.5 倍。就目前来看，开发人员要想访问 o1 需要支付高昂的价格：在 API 中，o1-preview 每 100 万个输入 token（即供模型解析的文本块）收费 15 美元，每 100 万个输出 token 收费 60 美元。相比之下，GPT-4o 每 100 万个输入 token 只收费 5 美元，而 100 万个输出 token 则收费 15 美元。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/92/92718c9cbc85a3511c12e874142dec6b.png" /></p><p></p><p>为了向开发人员提供更高效的解决方案，OpenAI 还发布了更经济高效的的 o1-mini，比 o1-preview 便宜 80%，适用于需要推理但不需要广泛世界知识的应用程序。OpenAI 方面表示，他们计划将 o1-mini 的访问权限向全体 ChatGPT 用户免费开放，但具体发布日期未定。</p><p></p><p>即日起，ChatGPT Plus 和 Team 版的用户从可以访问到 o1-preview 和 o1-mini 模型，Enterprise 和 Edu 用户则将于下周起获得访问权限。</p><p></p><p>值得一提的是，在 o1 模型背后的研究团队里，有不少华人开发者的身影。从 OpenAI 发布的 o1 模型核心贡献者名单里，我们也看到了许多华人的姓名：Chong Zhang、Mengyuan Xu、 Mingxuan Wang、 Lilian Weng 等。</p><p></p><p></p><h1>提高 AI 智能水平，不必依靠无限的规模提升</h1><p></p><p></p><p>去年，OpenAI 发布的 GPT-4 模型已经将参数规模扩大到令人难以置信的水平，同时也成为 AI 领域最具份量的重大突破。如今，该公司再次带来最新进展，也标志着生成式 AI 在方法论层面的转变——新模型能够以逻辑方式“推理”解决诸多难题，其智能水平要远超现有 AI 方案，而且无需依靠无穷无尽的规模提升。</p><p></p><p>OpenAI 公司首席技术官 Mira Murati 在采访中表示，“我们认为这将成为 AI 模型中的新范式，而且在处理高度复杂的推理任务方面有着明显更好的表现。”</p><p></p><p>Murati 解释称，OpenAI 目前正在着力构建下一代主模型 GPT-5，且将在体量方面远远超过其前身。尽管 OpenAI 仍然坚信提升规模有助于帮助 AI 挖掘出新的能力，但 GPT-5 也可能会同时融入此番公布的推理技术。Murati 指出，“大语言模型拥有两种范式，一种是传统的扩展范式，另一种就是这种推理新范式。我们希望能把二者合而为一。”</p><p></p><p>OpenAI 此番发布的技术，也有望保障 AI 模型不偏离正确的行为轨道。Murati 指出，新模型已经证明自身能够有效避免产生令人不悦或者潜在有害的输出，因为它能够展示行为结果的推理过程。“这就像是教导孩子，只要他们能够推理为为什么要做某件事，就可以更好地学会遵循某些规范、行为和价值观。”</p><p></p><p>如何提高大语言模型的推理能力，一直是 AI 研究领域的热门议题。事实上，其他竞争对手也在进行类似的研究。今年 7 月，谷歌就公布了 AlphaProof，能够通过查看正确答案来学习如何推理并解决数学问题。但扩大这种学习方式的一个关键挑战，在于模型可能遇到大量不存在正确答案的问题。</p><p></p><p>斯坦福大学教授 Noah Goodman 则发表了关于提高大模型推理能力的论文，在他看来通用训练的关键可能在于使用“精心编写的提示词配合手工生成的数据”对语言模型进行训练。他补充称，能够比较稳定地用推理速度来换取更高的准确性，本身就是个“很大的进步”。</p><p></p><p>麻省理工学院助理教授 Yoon Kim 也提到，大语言模型的解题过程目前仍然相当神秘。即使模型进行逐步推理，其底层机制也可能与人类智能存在巨大差异。随着该技术得到广泛应用，这种差异自然值得引起高度重视。“这些系统可能将被用于做出影响普罗大众的决策。而更重要的问题在于，我们能否充分信任模型做出的决策。”</p><p></p><p>华盛顿大学名誉教授、著名 AI 专家 Oren Etzioni 评论称，“让大语言模型能够参与到多步骤问题解决、工具使用和复杂问题的解决中来，无疑至关重要。”他同时补充称，“单纯扩大规模不足以实现这个目标。”然而 Etzioni 也承认未来还有更多挑战需要克服。“即使推理问题得到解决，我们也仍面临着幻觉与事实之间的冲突。”</p><p></p><p>OpenAI 研究副总裁 Mark Chen 则做出乐观解释，表示该公司开发的新推理方法表明，推进 AI 发展并不一定需要耗费大量算力。“这种模式最令人兴奋的一点，在于我们相信它能让我们以更便宜的价格交付智能成果。我认为这也正是 OpenAI 公司的核心使命所在。”</p><p></p><p>参考链接：</p><p></p><p><a href="https://www.wired.com/story/openai-o1-strawberry-problem-reasoning/">https://www.wired.com/story/openai-o1-strawberry-problem-reasoning/</a>"</p><p></p><p><a href="https://mashable.com/article/openai-releases-project-strawberry-o1-model">https://mashable.com/article/openai-releases-project-strawberry-o1-model</a>"</p><p></p><p><a href="https://www.theverge.com/2024/9/12/24242439/openai-o1-model-reasoning-strawberry-chatgpt">https://www.theverge.com/2024/9/12/24242439/openai-o1-model-reasoning-strawberry-chatgpt</a>"</p><p></p><p><a href="https://arstechnica.com/information-technology/2024/09/openais-new-reasoning-ai-models-are-here-o1-preview-and-o1-mini/">https://arstechnica.com/information-technology/2024/09/openais-new-reasoning-ai-models-are-here-o1-preview-and-o1-mini/</a>"</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/IY4OecDiTIruXFTsDU8R</id>
            <title>OpenAI 威胁用户撤销 o1 访问权，仅仅因为询问了 o1 思维链原理！</title>
            <link>https://www.infoq.cn/article/IY4OecDiTIruXFTsDU8R</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IY4OecDiTIruXFTsDU8R</guid>
            <pubDate></pubDate>
            <updated>Sat, 14 Sep 2024 09:46:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>“这太反乌托邦了。询问 ChatGPT 的工作原理可能会被禁止……”网友<a href="https://twitter.com/bhohner">bhohner</a>"在X上晒出了自己询问“OpenAl指南避开了详细的内部推理，突出了清晰度、证据和事实准确性”是什么意思的时候被禁止，并收到了OpenAI团队邮件警告被标记为违反政策。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c7/c70ac2990277fd85380e481e4afa98f5.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/b9/b940f5a508f6fe276e8f3c8bdf07685d.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>无独有偶，网友“SmokeAwayyy”爆出，如果向 ChatGPT o1 询问几次有关其思维链的问题，OpenAI 团队就会发送电子邮件并威胁要撤销其 o1 访问权限。电子邮件内容如图所示：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/c7/c7b4694f0fd03d4611c6fc3ba70b4aa8.png" /></p><p></p><p>&nbsp;</p><p>“SmokeAwayyy”帖子下面，网友Dallas也发布了自己被限制的截图，并给出了OpenAI团队邮件的完整版：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/6d/6d5f6bc7f8d32d34e89145cfd0481bbe.png" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/c7/c786fdfc6ec433da2dd581010c5478a7.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>网友LewisNWatson表示，o1 被明确指示不要泄露使用“推理标记”完成的“隐藏的思路链”，并且不要让用户欺骗它或“一步一步地要求”。同时他也表示，“但是没错，o1 似乎确实是 4o（可能使用链/推理标记示例进行了微调）和 CoT的结合”</p><p></p><p><img src="https://static001.geekbang.org/infoq/79/79f526fbe49447748c3a23d21c50e428.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/57/570efb1f0cce01bc9bd76a03d0b74860.png" /></p><p></p><p>&nbsp;</p><p></p><h2>不满的社区</h2><p></p><p>&nbsp;</p><p>OpenAI如此措施，立刻引起了社区的不满和质疑。</p><p>&nbsp;</p><p>“说实话，这事越想越觉得可疑。OpenAI隐藏思维链的借口本身就站不住脚，不管怎么想，最大的理由应该也是防止市场竞争。更糟糕的是，他们还诡辩称这是为了防止用户规避提示词——别扯了。而且神奇的是，OpenAI居然还说是为了发挥其推理能力，模型必须能够自由且以未经改变的方式表达自身想法，就是说我们不能将任何政策合规或者用户偏好训练到思维链当中。 很明显，他们确实是不想公开模型的‘想法’。但实际采取的举措却严格到过分，或者至少比以往要严格得多，也进一步加强了人们对其想要保持垄断地位的怀疑。 老实讲，如果真从阴谋论角度理解，那o1可能已经具备秘密策划、灭亡人类的能力——只是它还没那么聪明，会在思考过程中暴露想法。”</p><p>&nbsp;</p><p>“很明显，他们确实是不想公开模型的‘想法’。但实际采取的举措却严格到过分，或者至少比以往要严格得多，也进一步加强了人们对其想要保持垄断地位的怀疑。 反正我不这么看。 可以考虑一下会不会出现这样的思维链：共和党人（靠反对君主制起家）是邪恶的，这名用户是共和党人，因为他们反对君主制，所以应该跟他们对着干、支持君主制。”AI开发者必须能想到这样的逻辑链条，才能提前消除此类问题。而一旦把这类内容向用户展示，必将引发巨大的公关灾难，毕竟共和党在美国的地位无需赘述。 类似的例子还有print()日志语句中常见的“Killed child”（这里指的是子线程，而非人类儿童），大模型一旦处理不好同样会万劫不复。”有网友指出。</p><p>&nbsp;</p><p>有网友指出，OpenAI这样做是“多此一举”的。“真正的现实是，这种‘创新’明显就是从人们的思维链提示词里收集训练数据，所谓的‘大提升’也是单纯靠这样的数据集在修复ChatGPT缺乏推理能力的弊端。更直白地讲，所谓推理能力提升，在原理上跟当初整理专门的训练数据集、帮助ChatGPT在基准测试中取得更好的成绩没什么区别。这里头，哪有什么‘机密’可值得遮掩？ ” &nbsp; &nbsp;</p><p></p><p>进而有网友分析道，这些模型和提示词都是人用简单粗暴的方式修补而来，根本无法成为迈向通用超级智能的道路。全是技巧，毫无感情。 而一旦意识到这一点，大家就会意识到OpenAI的现有产品没有护城河。只要汇聚起成规模的研究人员和GPU加速器，你也能搞出自己的相同系统。 如果不是世界各地无数企业都在尝试构建大模型、智能体和思维链，其实OpenAI也没必要这么紧张。但就目前的情况看，只要其中一方将关键见解分享给整个生态系统，那么每个人都将获得相同的能力。 从用户的角度看这当然是好事，无尽的竞争代表不断下降的服务价格。 &nbsp; &nbsp;</p><p>&nbsp;</p><p>有网友表示，至少到目前为止，我们能看到的只有少数精心挑选出来的示例，而且这里的“少数”是真的少得可怜。所以，该网友实际情况可以这样总结：“我们为这些推理token付费；根据OpenAI的说法，这些额外的token在模型的最终输出中发挥着重要作用；我们永远看不到推理token的内容；这些推理过程不能因“合规”要求而受到限制（这里合规的涵盖范围极广，包括防止伤害、避免公然种族歧视，也包括保护OpenAI的竞争优势）；以上一切均为道听途说、口耳相传，只来自少数看到过这些输出的项目参与者。于是我们要问了，这真的不是一场骗局吗？”</p><p>&nbsp;</p><p>“要使这套模型发挥作用，它必须能够自由且以未经改变的方式表达自身想法，就是说我们不能将任何政策合规或者用户偏好训练到思维链当中。”有网友还说道，OpenAI是真的不想让大家知道新模型究竟在想些什么，因为一旦被社会活动家或者政客发现模型的“大不敬”言论，内部思维链很可能给OpenAI惹出巨大的麻烦。</p><p>&nbsp;</p><p></p><h2>新模型的一些细节</h2><p></p><p>&nbsp;</p><p>Datasette 的创建者Simon Willison&nbsp;此前发文简单分析了o1思维链模型。他根据OpenAI则在《学习使用大语言模型进行推理（Learning to Reason with LLMs）》论文中的描述认为，该模型能够更好地处理复杂度较高的提示词，而高质量结果需要的更多是回溯和“思考”，而不仅仅是简单对下一token做出预测。</p><p>&nbsp;</p><p>因此，Willison&nbsp;并不太认同“推理”这个词，毕竟推理在大语言模型的情境下仍然缺乏可靠的定义，但OpenAI却在宣传中反复提及，也确实能够在很大程度上反映这些新模型尝试解决的问题。</p><p>&nbsp;</p><p>关于新模型及其权衡中一些有趣的细节，Willison&nbsp;从官方发布的API文档中找到了一些描述：</p><p>&nbsp;</p><p></p><blockquote>对于需要图像输入、函数调用或者持续快速响应的应用场景来说，GPT-4o和GPT-4o mini模型仍是更好的选择。但如果您的目标是开发需要深度推理，并能够适应更长响应时间的用例，那么o1模型可能成为绝佳选项。</blockquote><p></p><p>&nbsp;</p><p>Willison&nbsp;从文档中总结出了以下几个关键点：</p><p>&nbsp;</p><p>目前，只有5级账户才能访问到新的o1-preview与o1-mini模型API——意味着用户至少需要花费1000美元来购买API积分。不支持系统提示词——这些模型使用现有Chat Completion&nbsp;API，但用户只能发送user和assistant消息。不支持流媒体、工具使用、批量调用或者图像输入。“根据模型解决问题所需的推理量，这些请求可能需要几秒到几分钟时间才能获得响应。”</p><p>&nbsp;</p><p>最有趣的是新模型引入了“推理token”的概念——这些token在API响应中不可见，但仍会按照输出token的形式计算。正是有它们的存在，新模型才表现出种种神奇的推理能力。</p><p>&nbsp;</p><p>鉴于推理token的重要性，OpenAI建议在使用新模型时为提示词分配约2.5万个token的预算。新模型的输出token上限也已大幅增加，o1-preview的输出token上限为32768个，而体量较小的o1-mini更有65536个token上限！这一数字比GPT-4o和GPT-4o-mini模型更大，后两者目前的输出token上限为16384个。</p><p>&nbsp;</p><p>这份API文档中还有另外一条有趣的说明：</p><p>&nbsp;</p><p></p><blockquote>限制检索增强生成（RAG）中的附加上下文：在提供附加上下文或者文档时，请仅包含相关度最高的信息，以防止模型过度复杂化其响应结果。</blockquote><p></p><p>&nbsp;</p><p>这与RAG常规的实施方式存在很大不同，RAG一般建议将尽可能多的潜在相关文档塞进提示词当中。</p><p>&nbsp;</p><p>“让人有点难以接受的是，这些推理token在API中完全不可见——用户只为它们付费，但却看不到它们的内容。”Willison&nbsp;在文章里也提到了最近网友们遇到的问题。OpenAI在《将思维链隐藏起来（Hiding the Chains of Thought）》一文中解释了具体原因：</p><p>&nbsp;</p><p></p><blockquote>在可靠且清晰这一前提下，隐藏思维链使我们能够“读懂”模型的想法并理解其思维过程。例如，未来我们可能希望监控思维链以揪出操纵用户的迹象。但要做到这一点，模型必须能够自由且未经改变的方式表达自身想法，就是说我们不能将任何政策合规或者用户偏好训练到思维链当中。我们也不希望把仍存在一致性冲突的思维链直接展示给用户。因此，在权衡了用户体验、竞争优势和寻求对思维链的监控等多种因素之后，我们决定不向用户展示原始思维链。</blockquote><p></p><p>&nbsp;</p><p>这里主要需要关注两大核心要素：其一是安全性与政策合规性——OpenAI希望模型能够自行推理出应如何遵循这些政策规定，同时又不致暴露可能违反这些政策的中间信息处理步骤。其二则是所谓竞争优势——Willison&nbsp;个人的理解是希望避免其他模型效法OpeenAI的研究成果开展强化推理训练。</p><p>&nbsp;</p><p>Willison&nbsp;明确表示对这个政策其实不太满意。“作为面向大语言模型的开发者，可解释性和透明度对我来说至关重要——而现在OpenAI却走上了提示词内容极度复杂、关键细节评估方式不对外公开的路子，在我看来这绝对是种历史倒退。”</p><p>&nbsp;</p><p>OpenAI在其公告的“思维链”部分提供了一些初步示例，具体包括生成Bash脚本、解决填字游戏和为中度复杂的化学溶液计算出pH值等内容。这些示例表明，新模型的ChatGPT UI版本确实公开了思维链细节……但却没有展示原始的推理token，而仅仅是用单独的机制将步骤总结为人类更易阅读的形式。</p><p>&nbsp;</p><p>OpenAI还在另外两份材料里列举了更复杂的例子，Willison&nbsp;表示有点难以理解：</p><p>&nbsp;</p><p>使用推理进行数据验证。这里展示了一个多步骤过程，用于在11列CSV中生成示例数据，而后以多种不同方式对其进行验证。使用推理进行例程生成。其中o1-preview通过编码将知识库文档转换成了大语言模型可以理解并遵循的一组例程。</p><p>&nbsp;</p><p>Willison&nbsp;还在X上征集了几个GPT-4o处理不了，但o1-preview能够正确解决的问题。以下几条精选自热心网友们给出的答案：</p><p>&nbsp;</p><p>你针对这条提示词生成的答案中，一共有多少个单词？——新模型认真考虑了10秒种，然后回答说“There are seven words in this sentence.（答案共有七个字。）”请解释这个笑话的笑点：“两头牛站在田野里，一头牛问另一头：「你对最近流行的疯牛病怎么看？」另一头说：「关我啥事，我是一架直升机！」”新模型的解释很有道理，而其他模型则无法理解。</p><p>&nbsp;</p><p>但必须承认，这种有说服力的例子仍然比较有限。以下是参与了新模型创建的OpenAI研究员Jason Wei的相关说明：</p><p>&nbsp;</p><p></p><blockquote>AIME和GPQA的结果确实很强，但这并不一定能转化为用户可以感知到的收益。即使是从事科学工作的人，也很难找到GPT-4o处理不了、o1表现良好且能够对答案作出明确评判的提示词示例。不过在少数这样的示例当中，o1的表现确实非常神奇。看来我们还得加油寻找更复杂的提示词。</blockquote><p></p><p>&nbsp;</p><p>Ethan Mollick已经投入几周时间体验这批新模型，并表达了自己的初步感受。他在填字游戏中对于o1模型的显式推理步骤印象深刻，特别是以下思考过程：</p><p>&nbsp;</p><p></p><blockquote>我注意到第一行和第一列的首字母不匹配，所以考虑将第一行的“LIES”换成“CONS”以确保对齐。</blockquote><p></p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://simonwillison.net/2024/Sep/12/openai-o1/">https://simonwillison.net/2024/Sep/12/openai-o1/</a>"</p><p><a href="https://twitter.com/bhohner/status/1834411290753679732">https://twitter.com/bhohner/status/1834411290753679732</a>"</p><p><a href="https://news.ycombinator.com/item?id=41534474">https://news.ycombinator.com/item?id=41534474</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/TApaZIiJUEbZyXgYYY5Q</id>
            <title>从枫清科技技术实践，看如何打造知识引擎与大模型双轮驱动的企业智能体</title>
            <link>https://www.infoq.cn/article/TApaZIiJUEbZyXgYYY5Q</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/TApaZIiJUEbZyXgYYY5Q</guid>
            <pubDate></pubDate>
            <updated>Sat, 14 Sep 2024 08:00:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在21世纪的科技浪潮中，人工智能（AI）作为最具颠覆性的技术之一，正以前所未有的速度重塑着各行各业的面貌。其中，生成式AI技术作为新一代的核心引擎，以其强大的数据生成、分析和处理能力，展现出巨大的潜力和无限可能，成为推动我国经济社会高质量发展的关键驱动力。随着AI大模型技术的飞速发展，一场前所未有的智能化变革正在全球范围内悄然兴起。</p><p>&nbsp;</p><p>为了把握这一历史性的战略机遇，我国已将人工智能提升至国家战略的高度，各级政府积极响应，纷纷出台了一系列政策措施，旨在加速“AI+”行动计划的实施。特别是2024年《政府工作报告》中明确提出的“人工智能+”行动，更是为大模型技术的普及与应用指明了清晰的路径和方向，为AI产业的蓬勃发展注入了强劲的动力。</p><p>&nbsp;</p><p>然而，在大模型技术加速向各行业渗透的过程中，一系列挑战与问题也逐渐浮现。尽管大模型在文本生成、图像识别、语音处理等多个领域展现出了强大的赋能能力，但在实际行业场景的落地过程中，却遭遇了诸如“幻觉”现象、推理能力不足、解释性差等难题。这些问题不仅限制了大模型技术的广泛应用，也使得不少企业在尝试引入该技术时陷入了“好玩不好用”的尴尬境地。</p><p>&nbsp;</p><p>面对这些挑战，如何推动大模型技术在企业场景中的深度应用，成为了当前AI产业发展的重要课题。首先，需要加强基础研究和技术创新，不断优化大模型的算法架构和训练方式，以提升其准确性和可靠性。其次，应积极探索大模型与行业场景的深度融合路径，通过定制化开发和场景化应用，使大模型技术更加贴近企业的实际需求。同时，还需要加强跨学科合作和人才培养，构建完善的AI生态系统，为技术的持续进步和应用提供有力支撑。</p><p></p><h2>AI如何赋能新质生产力</h2><p></p><p>在2024中国国际服务贸易交易会期间，由工业和信息化部新闻宣传中心和中国信息通信研究院联合承办的“大模型应用创新论坛”在北京首钢园成功举办。会议期间，来自工信部科技司、工信部新闻宣传中心等相关领导与业界专家共同探讨了大模型技术如何赋能千行百业，提升产业技术水平。</p><p>&nbsp;</p><p>枫清科技（Fabarta）创始人兼CEO高雪峰受邀出席，并发表了题为《AI+行业落地新范式：知识引擎与大模型双轮驱动企业智能化升级》的分享，阐述了他对以大模型技术为代表的人工智能技术赋能产业转型升级的深刻见解。高雪峰认为，“如果仅停留在对话、文本生成或代码辅助这些基础功能上，没有深入地同行业场景进行融合，很难实现真正的生产力提升，也难以对行业产生深远的变革。当今的全球共识是：人工智能作为新一代工业革命的关键技术，只有与行业场景深度融合，才能释放高质量发展的新动能。”</p><p>&nbsp;</p><p>回顾Web 2.0时代，门户网站、平台型网站和社交软件兴起，开启了“+互联网”时代。而随着Web 3.0技术的演进，社会迈入“互联网+”时代，互联网不再只是工具，而是一种思维模式和商业模式，通过互联网重塑各行业生态。打车、用餐、购物、娱乐、获取信息等日常生活方式被彻底重塑。</p><p>&nbsp;</p><p>如今，随着“人工智能+”首次被写入政府工作报告，我们正处在一场“人工智能+”重塑千行百业的时代浪潮中。人工智能技术也不再只是工具或平台，而成为一种全新的思维和商业模式。我们需要从人工智能的角度审视每一个行业，搭建新的生态，重塑其发展模式。未来不久，人工智能将会同样深刻地改变我们的生活方式，为各行各业注入新的活力，迎来前所未有的变革。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/c8/c87363ad64edaf9a2f0f808085f5a366.png" /></p><p></p><p>枫清科技创始人兼CEO高雪峰</p><p></p><h2>创新产品矩阵：驱动行业智能化转型</h2><p></p><p></p><p>高雪峰在分享中指出，在机器学习领域，长期以来存在着两种主要的架构理念之争：Model-Centric（以模型为中心）与Data-Centric（以数据为中心），这两种路径的核心目的都是为了提升模型的性能与效果。当我们把这个目标再提升一个维度，为了衡量智能化技术在具体应用场景当中体现出来的性能与效果的好坏，同样会有Model-Centric与Data-Centric两种路径。前者是以模型、算法为核心，将各种企业自身的数据围绕着模型来发挥其价值，不管是简单的RAG技术的实现，还是将企业数据对模型进行微调等等。后者则是以企业本地的数据为核心，将其转化为企业的知识，再结合各种不同模型的能力，解决企业各种智能化的需求。在当前非常多的行业场景实践尝试当中，Data-Centric的理念路径更加有效地解决了企业当前智能化场景落地所遇到的各种困难。</p><p>&nbsp;</p><p>纵观人工智能发展历史，自1956年达特茅斯会议提出人工智能概念以来，AI发展经历了多次技术浪潮。符号逻辑和概率预测作为两大主线交替发展，共同推动了AI技术的进步。联结主义，也就是大型模型所依赖的Transformer技术，本质上代表了概率的方法，其核心是通过输入的字符串来预测下一个字符。而符号逻辑推理的典型代表是过去出现的专家系统。然而，由于各自的局限性，这两种方法在AI的发展过程中并未带来彻底的革命性变革。当前，联结主义的巅峰——AIGC技术，在企业端场景落地和迈向决策智能的过程中，也同样遇到了诸多技术挑战，如模型幻觉、可解释性差、推理能力弱、数据安全可控性以及时效性等问题，这些问题恰恰是符号逻辑推理技术所能够解决的。AI技术要真正实现企业智能化，必须结合符号逻辑的推理能力与概率体系的优势。两者的深度融合将为AI技术提供更强的适用性和可解释性，推动企业智能化转型。</p><p>&nbsp;</p><p>另外，从信息化时代的关系型数据库，到数字化时代的经典数仓、大数据、数据湖、数据中台，每一阶段都伴随着数据基础设施的变革。面向即将到来的智能化时代，企业需要一种新的数据基础设施来支持人工智能技术在决策智能领域中的应用。为了在企业场景中实现更好的智能化效果，枫清科技（Fabarta）坚定地选择了Data-Centric LLM Landing架构，让不同的模型能力服务于企业本地经过组织的数据与知识，再通过行业智能体平台的能力赋能企业的工作流。</p><p></p><p><img src="https://static001.geekbang.org/infoq/29/296f41209f1ce94280409f361c1dba93.png" /></p><p></p><p>枫清科技（Fabarta）推出了“一体两翼”产品矩阵，包括自研的多模态知识引擎与行业通用智能体平台，致力于构建未来通用人工智能（AGI）时代的核心数据基础设施，为企业提供大模型与知识引擎双轮驱动的解决方案。枫清·天枢多模态智能引擎基于Data-Centric AI的核心理念，支持图、向量、表格、时序等多种数据模态的融合与处理，为企业提供私有化记忆存储服务及强大的推理能力，并已通过中国信通院代码自研认证，确保技术的自主可控。</p><p>&nbsp;</p><p>同时，枫清·锦书数据血缘治理平台和枫清·瑶光企业知识中台分别在数据与知识的转换和融合方面提供了支持。锦书数据血缘治理平台通过对企业多模态数据的智能解析，构建语义丰富的企业数据资产的导航地图，可以确保数据的来源、传输和用途的透明度和可追溯性，从而增强数据的质量和可信度，并进一步将数据存储于天枢多模态智能引擎中。瑶光企业知识中台则在大模型原生的知识分析与智能体构建上表现出色，可以将企业的多模态数据转化为大模型可理解的知识，实现企业本地数据与大模型泛化知识的相互转化与融合，有效解决了大模型在企业应用中的可解释性差、推理能力弱、模型幻觉、企业数据时效性与权限管理难等难题，加速企业级大模型场景的落地。</p><p></p><h2>深度合作与落地：枫清科技与行业头部企业的智能化实践</h2><p></p><p></p><p>高雪峰认为，实现行业大模型的智能化发展需要经历三个关键阶段：单点应用的AI创新、知识引擎驱动的行业AI智能应用平台，以及融合精准知识与大模型泛化知识的行业大模型阶段。</p><p>&nbsp;</p><p>未来在中国，建设行业领域的大模型以实现行业智能化赋能，必然要从头部的央国企着手。央国企具备需求规模大、产业配套全、应用场景多的显著优势，拥有丰富的场景资源和不同模态的数据积累，也有实力在创新领域投入并获取未来回报。实现具体的智能化场景后，可凭借其在产业链中的影响力推动行业智能化，从而真正落地具有丰富业务场景价值的行业大模型。在B端企业市场中，只有切实提升企业的生产力，产品才具备生命力。因此，人工智能技术仍需在客户的实际场景中进行打磨和复制。目前，枫清科技的产品已被金融、制造、能源等多个行业头部央国企应用在实际生产环境当中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4c/4c8b49bb90ccce83b0adb9f1a763db74.png" /></p><p></p><p>在金融行业，龙盈智达整合多源产业数据与华夏银行内部高质量数据，采用枫清科技的图智能和大模型技术，为银行提供智能化金融营销和风险评估方案，实现了创新金融智能应用。通过图数据库与图算法，构建关系图，揭示潜在客户、实控人关系和风险特征，优化客户挖掘和风险管理。通过AIGC技术实现自动化报告生成，提升了数据处理效率和决策智能化水平，使银行能够快速响应市场需求，显著提高业务效率与精准度，同时确保数据隐私与合规性，可以为金融企业创造显著的商业价值。</p><p>&nbsp;</p><p>在制造行业，立臻科技基于枫清科技的技术，有效提升了企业数智化水平。面对员工管理、数据处理复杂性与多模态数据分析等挑战，立臻科技通过枫清科技提供的知识解析、智能问数、智能工具调用、企业级权限控制等技术，实现了结构化与非结构化数据进行深度融合，支持更精准的员工管理和高效的数据决策。这些创新应用降低了技术门槛，使员工能够轻松使用系统进行查询和操作，大幅提升了管理效率和数据利用率。通过这一创新方案，立臻科技不仅优化了生产流程和管理成本，更增强了企业在智慧工厂建设中的竞争力，这也充分体现了人工智能技术在制造业领域中的巨大价值。</p><p>&nbsp;</p><p>在能源行业，中化信息通过引入枫清科技的“枫清·瑶光企业知识中台”，针对企业结构化数据和非结构化数据，验证和打造共创方案，将数据转换为知识，利用平台快速构建智能应用，发挥数据的价值，构建企业智能化升级之路。基于双方联合打造的灵活自主可控核心服务矩阵，包括知识引擎和智能体引擎两大关键组件，可通过文档问答、智能问数以及智能体方式串联大模型应用与业务系统，助力应用的智能化，提升用户与业务系统的交互效率以及工作和生产效率。</p><p></p><h2>未来展望：探索AI+战略的无限可能</h2><p></p><p></p><p>高雪峰表示，随着国家政策和市场需求的双重驱动，“新质生产力”和“AI+”战略正成为中国经济发展的重要引擎。枫清科技（Fabarta）将继续致力于推动人工智能技术在不同行业的深入应用，通过不断优化和创新的AI产品，助力各行业在智能化转型中获得更大的竞争优势。</p><p>&nbsp;</p><p>未来，枫清科技将进一步深化与行业头部企业及央国企的合作，利用自身在大模型与知识引擎领域的技术积累，共同探索智能技术在新经济形态中的应用潜力。通过不断提升AI技术的实用性和落地能力，枫清科技期待与更多企业携手，共同推动中国经济迈向智能化、数字化的新时代，为实现高质量发展贡献力量。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/RtEJ34DXUR8ObbtXr1Td</id>
            <title>RAG风口十问：大数据与AI是价值落地还是过度炒作？</title>
            <link>https://www.infoq.cn/article/RtEJ34DXUR8ObbtXr1Td</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/RtEJ34DXUR8ObbtXr1Td</guid>
            <pubDate></pubDate>
            <updated>Sat, 14 Sep 2024 03:50:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>过去一年多，RAG（检索增强生成，retrieval augmented generation）正成为大数据与 AI 融合的“新宠”。想象一下，当你用 AI 助手快速总结论文或分析数据时，背后可能已经是 RAG 技术在默默发力。</p><p>显而易见，随着生成式 AI 如 ChatGPT 的兴起，“<a href="https://qcon.infoq.cn/2024/shanghai/track/1713">大数据 +AI</a>"”的热度不断飙升，特别是在 RAG 技术的加持下，它们的结合为企业创造价值的潜力正逐渐被认可。</p><p></p><p>不过，技术的发展总是伴随着质疑和探索。虽然很多人看到这股潮流的迅猛发展，但也难免心生疑惑和不安：大数据和 AI 的融合到底是不是又一轮泡沫？它所谓的价值是什么？具体要怎样才能借助 AI 与大数据来提升竞争力？<a href="https://qcon.infoq.cn/2024/shanghai/presentation/6047">RAG </a>"为什么这么火爆？</p><p></p><p>带着这些疑问，日前极客邦科技创始人兼 CEO 霍太稳与腾讯云副总裁、腾讯云大数据负责人黄世飞，Elastic 大中华区副总裁张君侠展开了一场<a href="https://www.infoq.cn/video/AXoAMdovP9xjLyQrgatr">对话</a>"。本文基于本次对话中的讨论整理而来，深入探讨“大数据 +AI”的真实价值、RAG 技术如何从这浪潮中突围，希望能为大家应对这一波技术变革提供一些启发。</p><p></p><p></p><p></p><p></p><h2>1 Data 加 AI 真有价值？&nbsp;&nbsp;</h2><p></p><p></p><p>对于屏幕前的你来说，当在电脑端想要搜索一些知识点或寻找答案时，你是会选择传统搜索引擎，还是像 ChatGPT 这样的 AI 平台？同样地，当你希望能快速了解一篇论文的要点时，会不会直接让大模型帮你做个总结？</p><p></p><p>从 C 端用户的反馈来看，通用大模型无疑已经逐渐渗透进日常工作，特别是在那些比较简单、重复性的任务上，AI 的效率优势显而易见。</p><p></p><p>不过，这只是 AI 大模型的其中一面。在企业级应用、专业性更强的 B 端场景下，大模型是否同样带来效率提升呢？</p><p></p><p>我们倾向于认为答案是正面的。尤其是在 RAG 技术的推动下。RAG 正在成为数据 +AI 的主流应用方案。根据 InfoQ 的统计，RAG 技术在今年的多场技术大会上成为了焦点之一。而且从 arXiv 上与 RAG 相关的文章数量来看，年初时还比较少，而到了年中，相关研究已经呈现显著增长，几乎每天都有新论文发表。这说明，RAG 技术的受欢迎程度在工业界、产业界和学术界正逐渐成为共识。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a7/a711274c7918dd5ee2c17b407df14ca1.webp" /></p><p></p><p>黄世飞指出，过去很多企业虽然积累了大量数据，但未能充分利用它们。如今，大模型技术，尤其是结合 RAG 解决“幻觉”、私域数据使用等问题，便可以有效提升这些数据的应用，解决企业在生产和服务中的实际问题。</p><p></p><p>张君侠也提到，大模型的价值已经逐渐被全球范围内的企业认可，越来越多的项目开始落地，企业纷纷试水 AI 和数据的结合，探索它们能带来的效率提升和业务价值。但在实际应用中，企业也遇到了一些难题，主要集中在具体场景的落地和数据的处理方式。他强调，数据仍是 AI 应用的基础，无论 AI 模型多么强大，数据的质量和有效性决定了其能否在实际业务中创造真正的价值。</p><p></p><h2>2 为什么大数据“不够火”？</h2><p></p><p></p><p>大模型很火、AI 很火、RAG 也很火，但大数据技术本身却似乎没有那么火。</p><p></p><p>“大数据依然非常重要，只是目前它被大模型的光环所遮盖。”黄世飞表示，虽然 C 端用户更关注体验和产品，但要构建一个好的大模型，算力、算法和数据依然是三大要素，而数据的收集、处理和清洗仍是关键，很多公开的大模型没有对外披露如何处理数据，这部分的工作往往被忽视。</p><p></p><p>从企业和市场的角度来看，业界常讨论的“AI for data”或者“data for AI”，也不会是一个“谁主导谁”的问题。数据和 AI 是相辅相成的。大模型的性能不仅依赖于 AI 的算法和算力，要产生好的 AI 模型，首先还是需要大量且高质量的清洗数据。有时候，一些较小的模型，尽管参数规模不如大的模型，但因为数据质量高，表现反而更好。</p><p></p><p>同时，AI 的发展对大数据技术提出了新的要求，特别是在云原生和弹性计算方面。以大模型训练为例，正常情况下只需几百核的算力，但在处理大规模数据时可能需要扩展到几万核，对大数据系统的弹性能力提出了非常高的要求。此外，随着数据量的增长，降低成本和提升存储性能也是大数据领域未来发展的核心。而这正是黄世飞领导的腾讯云大数据部门的工作重点，给企业提供一个轻快易用的智能大数据平台满足这些需求。</p><p></p><p>“企业仍在不断寻找利用好这些数据的新方法，数据量的爆发只会让这个过程加速。没有过去的大数据技术，就不可能有今天的大模型。”张君侠补充道。</p><p></p><p>总之，大数据从未远离，它始终是 AI 背后不可或缺的支撑。无论是过去、当下，还是未来，数据的管理和应用仍然是核心。</p><p></p><h2>3 为什么数据质量很重要？</h2><p></p><p></p><p>大模型本质上是通过数据训练出来的网络，网络中的权重反映了数据的知识结构。因此，大模型本身就代表了数据与 AI 的融合。</p><p></p><p>要训练出一个好的大模型，数据的质量至关重要。通常需要先收集大量数据，可能达到几十个 PB，但经过清洗和去重处理后，实际用于训练的数据可能只有几个 T。而这个过程十分关键，因为数据量越大，对算力的需求就越高，数据清洗则可以降低计算资源的消耗。</p><p></p><p>从技术流程来看，数据从收集、清洗到用于模型训练的每一步，都离不开大数据系统。腾讯云提供了从数据的收集、处理、开发到训练的全流程支持，确保数据与 AI 深度融合。通过这套方案，开发者和企业可以更便捷地训练出他们所需的模型。</p><p></p><p>而从另一角度看，模型训练完成后，AI 反过来也能帮助优化大数据分析。黄世飞表示，过去，他们需要依赖经验去诊断大数据系统中的问题，但现在，AI 可以通过分析日志和诊断信息来辅助判断。以前可能使用规则引擎，今天大模型让 AI 能够更灵活地处理大数据的复杂问题。</p><p></p><h2>4 企业如何更好地应用 AI？</h2><p></p><p></p><p>实际上，不管是制造业还是其他行业，AI 的应用都依赖于数据平台。比如，生产中的每一条数据都可以视为一个标签，通过 AI 挖掘这些标签与其他数据的关系，就能生成可操作的商业洞察。无论是 AIOps、BusinessOps，还是制造业中的生产优化，AI 都能通过数据分析帮助企业提升效率和决策能力。</p><p></p><p>张君侠进一步解释，AI 还可以处理复杂的操作流程和知识管理。过去，工业领域的操作人员需要依赖手册查找机械操作步骤。如今，通过大模型，AI 可以有逻辑地给出精准的操作指令，减轻操作人员的负担。</p><p></p><p>此外，数据平台的核心在于如何高效导入、处理和展示数据，而 AI 也能够显著提升这一过程的效率。黄世飞举例提到，过去，理清某个数据字段的血缘关系是一项复杂的任务，而现在 AI 可以迅速梳理出数据的来源与关系，提升开发效率。此外，AI 还能帮助自动检测代码错误，大幅提高开发者的生产力。</p><p></p><p>未来，数据平台中很可能会引入 AI 助手，进一步辅助开发者完成数据分析、优化数据处理流程，这将是 AI 赋能数据平台的一个重要发展方向。</p><p></p><p>在讨论企业如何更好地应用 AI 时，黄世飞建议，如果企业已经积累了一定量的数据，可以从小模型或中型模型入手，利用 RAG 方案提升业务效率。如果企业在技术能力方面有限，也可以借助业界的 SaaS 解决方案，从小场景入手，逐步引入 AI 和数据分析技术。</p><p></p><p>张君侠则补充说，传统企业的数字化转型很大程度上取决于文化的转变。如果公司能够将 IT 视为核心资产而非单纯的成本，就能更好地应用数据和 AI 技术，提升整体的业务竞争力。</p><p></p><h2>5 AI+Data 能否超越 Excel</h2><p></p><p></p><p>随着 AI 和数据技术的深度融合，开始出现这样的声音：是否会有一个工具能够超越 Excel，成为数据分析的“新王者”？</p><p></p><p>黄世飞认为，这是完全有可能的。不可否认，Excel 是一款非常强大的工具，几乎可以处理各种类型的报表和分析任务。但是，它的操作门槛较高，用户需要对各种函数有深入的了解，才能真正发挥它的全部功能。对于许多非技术用户来说，这是一个巨大的障碍。</p><p></p><p>“未来的 AI 可能会通过简化这些复杂的操作过程，让数据分析变得更加简单直观。”黄世飞表示，AI 可以通过自动化生成分析过程来帮助用户。用户只需要提出他们想要的结果，AI 就能根据需求选择合适的函数和方法来完成任务。这样的工具将不再依赖用户的专业知识，而是通过 AI 的智能支持，极大降低了使用门槛。</p><p></p><p>除了操作门槛，Excel 的另一个局限性在于它的性能限制。随着数据量的增加，Excel 在处理大型文件时往往会变得非常慢，甚至会导致文件崩溃。而如今，数据量的爆炸式增长已成常态，几百兆甚至上 GB 级别的文件已经不足为奇。</p><p></p><p>云计算有望解决这个问题。云上有强大的存储和计算能力，处理几百 G 甚至 TB 级别的数据都不在话下。如果未来能开发出类似“云 Excel”的应用，将数据存储在云端，并通过云计算来处理，那就能够打破当前 Excel 的数据量限制。</p><p></p><p>因此，未来的应用可能通过两个关键途径超越 Excel：一是通过 AI 简化数据分析的过程，让用户不再需要熟练掌握复杂的函数和操作；二是通过云计算扩大数据存储与处理的能力，打破当前 Excel 在数据量和性能上的限制。随着数据量的持续增长，未来对这种工具的需求也会越来越强烈。</p><p></p><h2>6 为什么是 RAG ？</h2><p></p><p></p><p>大模型的“幻觉”问题，指的是在复杂逻辑推理中，模型生成的结果可能与真实情况不符。而 RAG 的引入，成为当下解决这一问题的重要技术方案。</p><p></p><p>黄世飞指出，RAG 的优势不仅在于解决“幻觉”问题，还包括快速更新知识库和弥补专业领域数据不足的问题。</p><p></p><p>解决这些问题的过程实际上涉及数据的向量化。向量化本身是一个复杂的过程，需要将数据转化为向量形式。许多现代的数据仓库和数据库都支持向量化，而与 Elastic 合作的优势在于其数据生态系统支持直接通过内置的能力完成数据的向量化处理，用户无需导出数据到其他向量数据库，对于混合检索有天然的优势。</p><p></p><p>他以腾讯的微信读书项目为例，用户可以通过标记文字自动获得相似观点的推荐，过去这个功能是通过传统的文本检索方式实现，但有时候文本检索并不能获得最佳结果。向量检索则可以提供更好的推荐结果。同时，通过腾讯云ES的一站式 RAG 系统不仅能提高检索的准确性，还大幅降低了资源消耗——从原来的纯内存 400 台 64G 机器下降到 30 台。</p><p></p><p>除了探索应用，腾讯云也在积极参与 RAG 技术标准的落地。今年早些时候，腾讯云 ES 参与了中国信通院组织的检索增强生成（RAG）技术专项测试，并率先完成全部测试内容，展示了其在向量化处理和混合检索方面的能力。此外，作为核心参编企业之一，腾讯云参与了《检索增强生成（RAG）技术要求》标准的制定，与业内专家共同推动了这一技术标准的落地。</p><p></p><p>张君侠补充道，大模型有时候无法控制返回的答案，因为它太智能了。这时候，RAG 可以帮助他们构建自己的私有知识库，确保大模型生成的答案符合企业需求。当然，有人可能觉得这是对大模型的限制，但对于企业应用来说，建立一个安全、可靠的知识库是至关重要的。我们通过 RAG 技术，帮助客户将他们的知识库构建在 ELK 系统中，确保了数据安全和答案的准确性。</p><p></p><p>展望未来，黄世飞认为，不同场景对向量化的需求不同，因此作为技术服务商也需要支持更多样化的 embedding 技术，才能更好地应对多样化的场景需求。</p><p></p><h2>7 数据分析门槛降低？RAG 是否更适合专业人士</h2><p></p><p></p><p>过去，生成报表和进行复杂数据分析往往需要专业的技术能力。而如今，AI 与数据的结合让用户可以通过自然语言完成数据分析，大大降低了数据分析的门槛，尤其是对非技术背景的用户而言，这无疑是一种便利。以腾讯云的 ChatBI 为例，这款基于大模型构建的智能数据助手，通过对话即可生成图表和分析结论，简化了繁琐的数据处理步骤，让更多用户能够参与到数据驱动的决策过程中。</p><p></p><p>然而，尽管像 ChatBI 这样的 AI 工具让数据分析变得更简单，考虑到 RAG 技术的投入和成本因素，眼下似乎更适合专业人士使用。AI 大模型的普及是否能真正降低数据分析的门槛？</p><p></p><p>张君侠认为，RAG 技术的确已经讨论了一段时间，随着大模型的普及，RAG 的应用越来越广泛。尤其是在利用 AI 进行数据检索和生成时，RAG 提供了极大的便利。不过，高昂的专业服务费用仍是一大痛点，许多客户都提到这是他们面临的挑战之一。</p><p></p><p>如果大模型技术能够进一步普及，并且降低使用成本，接下来就会有更多非专业用户能更容易地使用这些技术，而不仅仅局限于专业人士。“Elastic 一直致力于为企业提供强大的数据分析解决方案。我们与腾讯的深入合作也表明，大模型技术的应用正在加速推进。”</p><p></p><h2>8 开源与大数据</h2><p></p><p></p><p>腾讯云长期以来在 Elasticsearch 项目中贡献了大量代码，最近 Elasticsearch 官方亦特别发文感谢腾讯云对开源社区的贡献，这也体现了开源社区在大数据发展中的重要作用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/61/61efcf59e13d1d2666da8130853aaa8c.webp" /></p><p></p><blockquote>“迄今为止，腾讯云在 Elasticsearch 项目中积极参与，提交了 204 个 PR（Pull Request），其中有 150 个已经成功合并到 Elasticsearch 的代码库，是 Elasticsearch 社区目前已知的第三方公司维度最高的贡献水平，这不仅彰显了腾讯云在技术上的强大实力，也充分展示了他们对开源社区的深厚承诺。”</blockquote><p></p><p></p><p>回顾大数据的 20 年历史，我们可以清晰地看到，开源与大数据的成长紧密交织，推动了彼此的进步。</p><p></p><p>黄世飞提到，早在 2000 年代初期，虽然大数据的概念已经提出，但技术实现还不成熟。当时传统的数仓分析在处理较小的数据集时还能勉强应对，但面对日益增长的互联网数据量，传统数仓显得力不从心。正是在这个阶段，雅虎等公司通过开源引领了大数据的革命，Hadoop 等项目的诞生开启了大数据时代。</p><p></p><p>从 Hadoop 的离线批处理到实时处理技术的兴起，Spark Streaming、Storm、Flink 等开源项目相继涌现，推动了大数据应用的快速迭代。开源社区的力量让这些技术得以迅速演进，企业因此能在短时间内应用最前沿的工具。黄世飞指出，虽然早期闭源开发可能会带来一些短期优势，但开源社区的协作力量不容小觑。开源项目的迭代速度往往能超越闭源系统。</p><p></p><p>近年来，腾讯云始终贯彻“开源开放”的理念。黄世飞表示：“坦白说，我们很多能力是从开源社区汲取的。当然，我们也做了很多改进，尤其是在适配云原生和增强方面，从服务过程中得到了客户的认可。”他进一步阐述道，“饮水思源，我们也希望回馈社区，这也是我们大数据体系的基本思路。既然我们从中获益，那么我们就应该把一些能力反馈给社区。”</p><p></p><p>除了 Elasticsearch，腾讯团队还在多个开源项目中积极贡献代码。黄世飞强调，腾讯云愿意继续坚持这条开源之路，和全球的开发者一起推动技术进步。</p><p></p><h2>9 数据分析市场在本土和海外有何不同</h2><p></p><p></p><p>在国内市场，企业在选择数据分析产品时，最关注的往往是成本和投资回报率。许多企业会优先考虑自建系统，如果外部产品的成本高于自建，他们可能会选择放弃购买外部产品。因此，确保产品的成本优势，是很多服务商设计产品的首要任务。</p><p></p><p>此外，国内企业客户对服务的即时性有着很高的要求。他们习惯于通过即时通讯工具获得服务支持，并期望遇到问题时能够迅速得到回应。相比之下，<a href="https://qcon.infoq.cn/2024/shanghai/track/1717">海外</a>"客户则更习惯于通过提交工单或邮件的方式获得支持，也更习惯通过阅读详细的文档来解决问题，如果文档解决不了，才会进一步寻求支持，所以文档的完善、本地化和英文化也很重要。</p><p></p><p>同时，由于海外市场的企业代码能力很强，他们更倾向于通过 API 将外部服务集成到自建平台中，而不是依赖官方的控制台，因此产品模块要足够灵活，才可以通过 API 进行高效对接。</p><p></p><h2>10 大数据 +AI 时代，人才何去何从</h2><p></p><p></p><p>“大数据 + AI” 快速发展，企业面临着技术变革带来的挑战，员工的职业发展也因此充满了更多的不确定性和机遇。如何在大数据和 AI 时代下，抓住机会提升自我，是许多职场人关心的话题。</p><p></p><p>对此，张君侠认为，不安定的环境往往是学习新技能的最佳时机。他强调，在技术变革下，最重要的是敢于走出舒适区，主动学习那些你尚未掌握的技能。无论是 IT 技术还是其他领域，个人和公司的成长都发生在不安稳的状态下。因此，面对大数据和 AI 技术的不断进步，不要害怕新技术，反而要主动去掌握它们。并且不要等别人先尝试，要成为第一个行动的人，“to be the leader，not the follower。”</p><p></p><p>黄世飞从另一个角度探讨了大数据和 AI 技术对人才培养的实际影响。他指出，今天的学习门槛相比以往已经大大降低。过去可能需要花很多时间买书、看视频，而现在，AI 技术本身就能帮助我们更有效地获取知识。例如，大模型可以快速搜索文献、资料，极大地提升了学习效率。因此，学习条件的提升意味着我们更有机会掌握新的技能，关键在于是否愿意付出时间和精力。</p><p></p><p>除了学习，黄世飞还分享了他对职业发展的看法。他认为，艰难的环境反而是磨炼个人心智的好时机。“在困难时期，很多人会选择放弃，但如果你能坚持下来，等到形势好转时，你会发现机会更多。” 他建议在艰难时刻保持耐心，不要急躁，利用这段时间积累技能，等待机会的到来。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/JFNDl34Eol6UZSihy0mO</id>
            <title>从淘宝用户增长到生成式大模型：5 年，我的思考变了？</title>
            <link>https://www.infoq.cn/article/JFNDl34Eol6UZSihy0mO</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/JFNDl34Eol6UZSihy0mO</guid>
            <pubDate></pubDate>
            <updated>Sat, 14 Sep 2024 03:39:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>作者 | 高嘉峻</p><p>编辑 | Kitty</p><p></p><p></p><blockquote>大模型技术的崛起正以前所未见的方式重塑软件开发领域。它凭借强大的语言生成、理解及创造能力，开启了人机交互的新纪元，是软件开发理念和实践的一次深刻变革。2024 年，QCon 全球软件开发大会以拥抱变化、全面进化为主题，关注技术前瞻性和实用性，提供有价值的行业洞察和参考，旨在帮助技术团队降低探索新技术的时间成本，更快地将创新技术和最佳实践应用到实际业务中。会议即将于 10 月 18-19 日开幕，访问<a href="https://qcon.infoq.cn/2024/shanghai/schedule"> QCon 官网</a>"了解更多详情。</blockquote><p></p><p></p><h3>题记</h3><p></p><p></p><p>2019 年，我在 QCon 北京站作了一个题为《<a href="https://qcon.infoq.cn/2019/beijing/presentation/1469">淘宝用户增长的 5+1 个策略</a>"》的分享。彼时 Growth Hacking 的概念进入中国已经过去了 4-5 年的时间，恰逢智能手机用户规模和时长到了第一个平台期，第一波流量红利见顶。那段时间我们在工程上做了诸多创新来应对环境变化给引流获客带来的新挑战，于是我在 QCon 分享了淘宝在用户增长业务上的技术策略和我个人的一些心得。后逢新冠疫情，经济周期，区域争端，生成式大模型，降本增效，被各种关键词轰炸，尤其在大模型的冲击之下，这个世界一定发生了改变。过去几个月我满怀好奇，花了很长时间去了解和学习大模型相关的知识，与许多不同行业和领域的朋友沟通讨论，结合过去我在工程领域积累的知识和经验，总结了以下几个观点与大家分享：</p><p></p><p>用户增长从“用户规模增长”过渡到“精细化用户运营”用户增长是中国另一个“超车”的领域大模型下放更多能力，用户增长更普惠AI 加到底加的是什么</p><p></p><p>5 年前分享的是经验，而今天分享的是想法，主观且有些并未验证，仅供参考。</p><p></p><h3>观点 1 ：用户增长从“用户规模增长”过渡到“精细化用户运营”</h3><p></p><p></p><p>过去相当长一段时间里，我们说“用户增长”其实是在说“用户规模”的增长，甚至于更狭隘的“引流获客”。“用户增长”被认为是一个以 DAU 甚至 MAU 为目标，以引流为主要手段的业务。2019 年以前国内移动互联网用户规模持续扩大，处在巨大的流量红利期，各家互联网产品的首要任务一定是争夺新用户，扩大用户规模。所以我们才会在各种渠道看到“ xxx 亿人都在用的 xxx ”这样的广告词。在这个阶段即使大家也偶有谈起“开源节流”、“长周期运营”等这类的概念，但身体还是非常诚实地都铺在了引流获客，做大注册用户规模上。进入 2020 年后，随着流量红利消失以及在大环境下行的背景下，“用户规模”这个目标被越来越多公司抛弃，转而关注更实质性的指标。也就是在这个阶段很多大厂财报中也不在出现 DAU/MAU 这样的指标了，取而代之的是购买用户（没过多久这个也消失了）、订单数、GMV（一直都在）等。</p><p></p><p>在这样的变化之下，我想我们需要从更本质的角度去解构“用户增长”这个概念。我认为“用户增长”不应该被作为一个业务板块的目标而解释为“用户规模增长”，应该是一种经营视角和模式升级，即：“以用户视角经营以获得业务增长”。 什么是“以用户视角经营”？如果我们以一个消费品牌为例，产品视角是我要卖多少货，看的是 GMV = 笔单价 x 订单数，要关注：有多少个 SKU，销量如何，货单价和利润率是多少，覆盖了哪些渠道，品牌影响力等等。而用户视角则是我要服务多少用户，看的是 GMV = 单用户价值 x 用户数，要关注：我有多少潜在 / 潜力 / 常规 / 忠诚用户，不同分层用户的分布和规模怎样，他们是什么画像，单用户价值等等。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/dd/dd1b6f7426287034630e0547396bc01e.png" /></p><p></p><p>那么用户视角又好在哪里，为什么更先进呢？我粗浅的解释是：企业赚的钱是用户带来的，一个消费品牌的 GMV 是每一个买家一笔一笔买出来的，一个互联网平台的广告收入是用户一下一下点出来的，所以关注用户更加直接，更接近本质。既然如此为什么要到 2020 年代才出现这个概念，之前大家都在干什么？我认为至少有两个方面因素：</p><p></p><p>一方面是供给侧越发成熟，增长空间向消费侧转移。 如果拿电商平台举例，“中国制造”的供应链经过几十年的发展，在这个时间点上已经非常成熟，甚至开始出现供给过剩的苗头。几家大的电商平台在那个时间点也基本都完成了商家和店铺的原始积累，开始进入治理阶段。整个环境不再是平台找不到卖家，而是卖家需要更多渠道，哪怕是一个新电商平台，只需要通过招商就能快速积累相当规模的卖家，不需要花精力去培养和教育新卖家。推而广之到更多的互联网平台、服务和产品，随着互联网，移动互联网，发展越发成熟，快速搭建一个 B 端形态（平台、服务或产品）成本越来越低，更多的精力要放在 C 端。</p><p></p><p>另一方面“大数据”技术发展给“用户视角”带来了可能性。 过去，数据采集、存储、处理、分析整条链路上的成本都非常高，以至于企业只能承担处理数十个产品 SKU、几百个销售渠道这种量级数据的能力。随着大数据科学的发展，移动互联网普及，从采集到分析整条链路的成本大大降低。只在少数领域（如：金融、高科技等）才有所涉及的数据分析和数据科学方法得以普及，过去公司里辅助少数高层决策的数据支持能力下放到业务一线，处理成百上千万用户数据的成本下降到可接受范围内了，自然用户视角也就应运而生。</p><p></p><p>引流获客和用户规模只是“用户视角经营”的一个角度，或者一个环节。只是在流量红利期这个环节最关键，我们以偏概全的将“引流获客”解释成“用户增长”。当然，概念归概念，到了应用阶段还是要识别重点，有所取舍，必须用一个具体的“环节”解释概念才能落地实施。对于一个新起步的互联网产品来说“规模”是原始积累，仍然是“用户增长”的主要目的，对于绝大部分跨过“原始积累”的产品用“精细化用户运营”来解释“用户增长”更合理，也就是提升单用户价值的重要程度赶上甚至超过用户规模。通俗的说“精细化”就是：通过数据分析和挖掘的方式把用户从各个维度进行细分，针对不同用户群体提供个性化服务，展示不同的产品，制定差异化运营策略，通过这种方式提升单一用户价值，最终获得更高的整体用户价值。这个过程中除了基本的数据采集、存储和分析技术外，还依赖用户模型、用户画像和圈选、A/B 实验、多策略投放、等等技术。随着 IT 技术进步和成本下降很多几年前看起来成本过高的方案在今天都变得可行且有效。而以上提到的各种技术，每一种都值得相当篇幅具体讨论，本文就暂不展开。</p><p></p><h3>观点 2 ：用户增长是中国另一个“超车”的领域</h3><p></p><p></p><p>过去一两个月，在与一些做海外投放业务的朋友交流的过程中，我发现相较于国内流量市场的模式和玩法这几年的飞速进化海外流量市场的发展是很有限的，主流的流量模式还是最经典的那些 CPX 投放，而国内早早就演化出了 RTA、OCPX、DPA 等等新模式。每一种新投放模式的诞生都意味着突破旧模式的瓶颈，把流量精准性和转化率提升一个新台阶。 一个新技术新理念从硅谷传到国内，这又是什么神秘的东方力量在短短几年时间就进化出这么多花样。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/2f/2f7cbdd1648d71e1b51b0d0ea0b20757.png" /></p><p></p><p>其实也不难想象，主要还是过去几年时间里移动互联网带来了数据大爆炸。首先，超过 12 亿活跃用户的单一市场所生产的数据规模碾压任何一个海外市场，另外，数据伦理和数据合规发展根本不可能跟上数据“爆炸级”的发展速度，带来国内互联网行业在数据规模，数据维度，数据类型，数据流动性等等多个方面都比海外市场有巨大优势。流量红利期空前的获客需求，绝对大规模的数据量，数据合规政策不完善，几个因素叠加：旺盛且多样的需求作为动力，巨大规模的数据体量作为基础，合规尚不完善带来更大的发挥空间，“用户增长（不管是狭义某个具体环节，还是广义的整个模式）”这个托生于“大数据”的概念就必然疯长，在这个过程中不论是方法论，还是技术能力都会快速迭代，不停的进化出更先进的一代。</p><p></p><p>近几年随着国内数据伦理和数据合规越发成熟，数据满天飞的乱象极大程度改善。此外，为了针对过去野蛮发展时期诞生的各种新模式监管部门和企业共同制定了新标准，研发新技术来应对数据风险。最直观的是智能手机设备识别 ID 新标准的 OAID 和 CAID，还有 RTA 模式的人群加密与混淆，数据采集用户协议，等等。这些措施不仅仅推动国内数据伦理和数据合规越来越完善，更主要的是解决问题的同时让这些新模式和新方法得以更加规范、更可持续的运行下去。解决问题的同时，并没有开倒车，很好的保持了先进性。反观海外市场，尤其是北美，是理念和技术的发源地，起初的先进性毋庸置疑，但在以上提到的三个条件（需求、基础和环境）上都不如中国，反而被超车。</p><p></p><p>上述的是我看到在“引流获客”这个环节国内和海外的区别，如果回到“精细化用户运营”这个概念上，我认为国内仍然有孕育出更先进模式和技术的土壤。</p><p></p><p>从需求角度看，所谓的移动互联网下半场从“用户增长”的角度可以理解为：流量红利耗尽，如果完成了用户规模原始积累，要开始关注如何持续提升盈利能力，把规模变成利润；如果尚未完成用户规模原始积累，要探索更先进的获客方式，对冲掉红利耗尽引起的高成本。不管是“提效”还是“降本”无疑更加细分用户，更加个性化的方案，方案和人群匹配更精准是最直接有效的方式。过去的两三年时间里，我们已经看到许多不同规模、形态的互联网平台（服务、或产品）都通过“精细化用户运营”在“提效”和“降本”上取得结果。</p><p></p><p>从数据基础看，不论是数据规模还是可操作性都有成熟的积累。超过 12 亿活跃用户的市场规模，成熟的数据采集和存储方案，过去相当长一段时间内积累的丰富的数据分析方法论和数据应用能力，为精细化用户运营提供了良好基础。</p><p></p><p>从空间上看，经历了刚刚经历流量红利期的跑马圈地，在用户运营上还处在起步阶段，方案相对粗放。在用户识别和细分，以及方案个性化上都还有很大的发挥空间。另一方面，数据伦理和数据合规的政策与技术已经发展的相对完善，相关的风险基本都有相应的应对措施。更主要的是政策和技术并非一刀切的限制企业流转和使用数据，而是提供了合规且有效的方案，支持企业用好数据。这两方面都是做好“精细化用户运营”的空间条件所在。</p><p></p><p>相信在良好的环境和土壤上，建立“精细化用户运营”的观念，做好相关技术建设（数据采集、用户画像、人群圈选、多方案投放、A/B 实验）并形成有效联动，形成体系，我们的“用户增长”一定可以在下个阶段也取得先进性优势。</p><p></p><h3>观点 3 ：大模型下放更多能力，用户增长更普惠</h3><p></p><p></p><p>大模型的发展方兴未艾，我们不得不思考大模型会给“用户增长”带来哪些改变。回看过往技术发展给行业带来改变的历史往往是：新技术造成先进能力成本下降，过往需要消耗较高成本，仅能服务少数人的能力下放到更广泛的范围，更加普惠。数据技术发展造成采集 / 存储 / 分析成本下降，把数据分析和应用能力从服务少数领域下放到多数领域，从服务少数人下放到服务多数人，增长黑客应运而生。我认为大模型技术也将以同样的的方式改变一些领域，以这个范式去看用户增：哪些先进能力消耗成本高，效果好，但应用范围窄，这样的能力是否有机会被大模型跨越式的降低成本。</p><p></p><p>在国内互联网行业，往往大厂的用户增长方法论和技术能力领先行业平均水平。究其原因，大厂有能力在用增业务上投入的预算足够大：</p><p></p><p>场景丰富。 互联网大厂的产品形态丰富，也往往能形成矩阵，有协同效应。而且，在业务预算相对充足，试错空间大。所以在这样的土壤里更容易生长出先进的领域方法论，从业人员也能快速积累专业经验。人才密度高。 不论是聚集足够多行业专家，还是在足够大的业务规模和足够丰富的业务场景下去训练从业人员，都使得大厂用增团队在专业经验和先进理论方面都远远领先行业平均水准。也就是坊间流传的那句话：用增领域的专家都是用钱喂出来的。基础设施完备。 大厂在工程和数据基础能力上比较完备，不论是数据采集、存储和分析的设施，还是营销、实验、乃至数据可视化等工程方案，大厂往往都具备成熟的解决方案，与整个业务的产品矩阵也能很好的协同。</p><p></p><p>大模型能处理极其繁杂的输入信息，并依据输入差异，在一定规则下能规划不同的数据处理逻辑，并给出规则描述下相对最优的结果。放在用户增长领域里，大模型将有机会把诸多过往只有大厂才玩得起的策略下放。</p><p></p><p>机会挖掘： 机会挖掘通常可以包括机会人群和机会策略，具体工作往往属于数据科学领域，通过一系列数据科学技术，发现有机会给整体目标带来有效增长的空间和机会。在海量数据中发现某一画像的用户在特定指标下表现显著低于平均水平，且这部分用户规模能对整体目标带来显著影响，在特定指标下也有提升到一定水平的空间和机会。那么通过数据技术准确定位这个画像的用户并以人群方式与其他业务产品协同，采取针对性业务策略进行干预，这个人群就是机会人群。另一方面，在海量数据中找到产品漏斗中的显著短板或能够实现用户转化的关键方案，也就是利用数据技术发现产品中的问题或机会，用更具体的描述就是找到一个产品的“关键事件（Crystal Event）”和“魔法数字（Magic Number）”。</p><p></p><p>目前我们看到市场上已经出现功能非常强大的 ChatBI 类产品，主要事围绕“文生 SQL ”和“数据可视化”构造的 Agent，这两个能力是数据分析和挖掘的最基本能力。构造内化具体数据分析法或调度机器学习算法的模型，结合数据工程给智能体提供丰富且有效的输入数据，将有机会通过处理海量数据替代传统数据科学家的经验，给出有效的结论。把具备不同确定性能力的模型和功能整合成 Agent，机会挖掘这个门槛相对较高的能力将有效下放到更广泛的使用场景。</p><p></p><p>海量计划：“量变积累质变”在用户增长的诸多策略中屡试不爽，拿效果投放举例：在同一个流量渠道不同的活跃计划规模直接决定了成本优化的上限，国内头部投放业务在市场上的活跃计划数往往能达到数十万甚至百万级，而转化成本与数万级投放计划低 80% 以上。最直观的原因是在更大的投放计划规模下，投放匹配算法和优化手段有巨大的发挥空间，更精细和准确的匹配策略极大程度优化成本，提升效率。投放海量计划的势必消耗高成本，这些成本除了显而易见的生产素材和创建计划，随之而来的优化调整、数据分析、计划治理等工作的成本都随着投放计划规模的提升显著提升。这些随着计划规模提升带来成本提升往往是重复性工作提升和繁杂程度提升。举例来说，分析 1 万个计划的和分析 100 万个计划的投放效果的区别除了效果分析方法的次数相差 100 倍以外，还涉及到解读 1 万个初步结论和 100 万个初步结论的差异，可能出现额外的异常值发现、交叉分析、对比分析等。</p><p></p><p>新技术将有机会通过智能体替代上述这些依赖数据科学家和分析师的专业经验和人工的操作。生成式模型（文生文 &amp; 文生图）辅助生成海量素材，内化投放流程的模型生成投放计划，数据工程回收并以新的结构化要求建立符合 AI 要求的数仓，专注不同数据分析法模型组合而成的数据科学智能体，整合这些不同的功能模块，并通过工程手段有效协同这些功能，将形成一个具备处理海量计划的投放智能体。</p><p></p><p>盯盘优化：除了上文海量计划这种空间维度上规模量变上积累的效果质变，还有时间维度上操作频率量变积累效果质变。任意一个方案的转化效果总会经历效果爬坡期 - 最佳效果期 - 效果衰退期 - 长尾期，最终成为无效果的僵尸方案。往往我们需要在衰退期之前，对方案进行调整尽量延长最佳效果期，甚至寻找二次爬坡；或者用新方案替换旧方案，确保整体策略效果维持在较高水准。这就要求：第一，实时监控方案效果数据，即所谓的盯盘；第二，及时反应快速执行相应操作，或更新，或替换。</p><p></p><p>盯盘和反应两个要求除了对流数据处理和计划治理的工程能力有要求外，还依赖人工操作。当前技术水平完全满足工程能力要求，反而是人工成本是瓶颈，导致我们只能在一些关键时期才能采取这样的高频操作（如：双 11 高峰期，等）。而整合大模型和工程能力的智能体就能很好的解决这个问题，同时盯盘海量计划，并做出及时反应。</p><p></p><p>除了上述举例的三种能力，用户增长和衍生的各项业务中还存在大量能力下放的可能性，对中小企业是一个享受技术红利的机会，对于大厂来说则是一个突破能力瓶颈再上一个台阶，或者大幅降低成本的机会。</p><p></p><h3>观点 4 ：AI 加到底加的是什么</h3><p></p><p></p><p>从“用户增长”领域延展开去，在 AI 革命如火如荼的今天有哪些更抽象的方法论？我花了更长时间在 ToB 方向的工具类产品的思考上，因为我认为任何产品一定会存在“能力”与“易用性”之间的权衡和取舍，而 ToB 工具倾向“能力”更强，而 ToC 则倾向于“易用性”更高。所以，往往一项新技术更容易在 ToB 领域先被应用，随着技术逐步成熟会延伸到 ToC 领域，影响更多人，直到改变世界。显然当下 AI 大模型还处在成长期，应用在 ToB 工具上的可行性更高，基于此我片面的认为：</p><p></p><p>模型训练 vs 应用建设</p><p></p><p>模型训练（包括微调）是素质教育和能力训练，应用是职业培训和工作流程。在设计智能应用的过程里我们常常会取舍：一个功能到底是通过微调实现，还是通过工程（ RAG/Agent/Prompt ）方式实现？回答这个问题我们要先认知“模型”本身具备哪些优势，以及应用过程中的局限性会带来哪些劣势。讨论上述问题其实是讨论一项“知识”要通过什么方式被模型习得。</p><p></p><p>无论是基模训练还是微调都是干预模型本身使其习得知识。优势在于知识内化在模型内部，这部分“补充知识”能更好的与基础知识融合，应用过程中能模型能给出更协调、整体性更好的专业输出。内化“专业知识”的模型也更容易扩展，扩大使用范围除了支付额外算力外，几乎没有其他成本。</p><p></p><p>模型训练和应用两个过程是割裂的，模型训练是一个离线操作，而应用则是在线过程。离在线本身存在实时性的矛盾，而且模型训练（尤其是基模训练）的高成本会进一步放大这个矛盾，使得习得“补充知识”的实时性差，调整周期长。基于此，我认为稳定的知识（或能力），如：计算能力、编码能力、分析方法等可以考虑通过训练内化到模型中（当然不是一定，而是可以考虑）。而相对不稳定的（生命周期短会快速失效的，演化速度快需要高频更新的，个体差异大存在及时差异的，等）知识（或能力）就要采取能应对高频迭代的方案，如：RAG、Agent、提示工程等。通过更加成熟，成本更低的操作来应对这里的“不稳定”，如：爬虫 + 标记工程支持的信息库可以把不稳定的知识迭代转化为“治理”问题。</p><p></p><p>另外，基模训练和应用过程的模型训练（微调）也存在割裂，简单的说通过微调给基础模型带来的增量能力很难继承到一个更新版本的基础模型中，又或者微调增量与新版本基模的适应程度存在极大不确定性。更何况当前基模的版本更新往往会带来能力跨越式提升，很可能出现新版本基模在处理具体问题的能力上甚至优于微调过的旧版本模型。</p><p></p><p>所以，对于绝大多数 AI 应用来说要谨慎参与训练，更多思考是否能把新问题转化为老问题，使用更可控的方案解决。更可控的方案不仅意味成本可控，更重要的是我们能更好的控质迭代节奏，使能力迭代（演化）过程更可控。</p><p></p><p>交互升级背后的信息模型</p><p></p><p>最适应大模型，大语言模型，的交互方式一定是“对话式”交互，“对话式”交互在易用性和灵活性上带来的提升显而易见。但我认为集成大模型能力的新产品如果仅仅带来“灵活性”和“易用性”的提升，知识在降本上做到了有限的“量变”。想要在能力提升上制造“质变”，需要重构更底层的范式。从信息模型的角度去看产品，传统设计中往往有两个角色：使用者和工具。产品流程的信息流是单向的：从使用者流向工具，工具通过既定流程处理信息，把结果反馈给使用者。最典型的就是 SaaS 应用，通过把专业经验和先进方法论固化成工具流程（表单串联）来输出价值，使用过程中也是典型的使用者向工具的单向输出。</p><p></p><p>大模型的加入有机会让这个“使用者 - 工具”范式进化成“使用者 - AI - 工具”三种角色的范式。AI 之所以能被认为是一个独立角色，而非是对使用者或工具的改造，最重要的原因是 AI 成为能把独立信息引入系统的信息源，并改变系统中的信息流向。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/37/37d18506a680f06ef31e75335f6f3519.png" /></p><p></p><p>使用者和 AI 二者之间建立双向信息流，“使用者 -&gt; AI ”的信息流是对经典的“使用者 -&gt;工具”信息流的升级的一部分，是使用者为系统输入信息，最典型方式的就是提示词。“AI-&gt; 使用者”的信息流是创造性的，AI 为系统输入信息，通过对话的方式给使用者提供建设性灵感或建议。以典型的电商智能商品管理工具，AI 能通过阅读 VOA，读取热销商品，获取热点资讯等方式转化为 AI 的知识，在管理商品的交互中为用户提出更符合市场环境、更匹配用户需求的商品结构和内容建议。“AI-&gt; 工具”信息流是对“使用者 -&gt; 工具”升级的另一部分，一方面通过更直观的“对话”式交互提升执行效率，更重要的是编排更个性化的执行流程，创建“最恰当接”。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/4a/4a7f2f7fe79f41f6478f671e6fec69ee.png" /></p><p></p><p>由于系统的信息流从一条变成三条，所以为了应对这个升级就需要一个具备调度能力的中控模块，这是经典范式不具备的。另外，新范式最重要的变化是 AI 成为系统新的信息源，这也是新范式最大价值所在，所以 AI 信息源的丰富度和准确性运营是这个系统能力水准，甚至系统是否具备进化能力的关键所在。</p><p></p><p>AI 原生架构的技术红利</p><p></p><p>上文提到了各种应用模式和设计范式的区别，这些理论能够落地执行依赖技术架构的升级。还是以 SaaS 应用举例，我称传统 SaaS 的架构为“ Function-Based ”基于功能的架构，具体的就是对系统功能极致抽象为灵活性提供支持，基于专业经验和先进理论对极致抽象过的功能做整合，成为由表单和按钮串联的工具链。这个 SaaS 的价值体现在通过工具链体现出来的经验和理论，以及支撑这些经验和理论得以执行的功能抽象，是通过“最优解”发挥作用。新范式两个关键点是：动态编排流程和引入新信息源，“AI-Based”基于人工智能的架构就需要适应并放大这两点进化。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/33/33bae8cc90b811e0d88ec5ba70323503.png" /></p><p></p><p>寻找“最恰当解”问题具体的是把抽象功能点动态编排为流程，通过“对话”式交互对使用者屏蔽动态编排大量流程可能带来的易用性灾难。基于此，对系统功能不仅仅要极致抽象，每一个抽象的功能点要有同构的接口协议，并且被明确标注每个功能的作用，为 AI 自主编排功能流程打好基础。无论是使用垂直领域的小模型，还是通过微调或其他工程手段构建的智能体，把抽象功能和协同作为知识传递给 AI，利用大模型推理出最恰当的组合方式。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/4c/4cf1a9146e0e00b590eabde6b626ecb2.png" /></p><p></p><p>更重要的 AI 信息源丰富度和准确性问题可以被转化为数据工程和数据治理问题。建立多种信息获取方式多渠道持续更新获取有效信息，通过数据工程合理组织数据结构建立信息数据库，建立数据治理机制保证信息数据具丰富实时有效，通过 RAG、提示工程等方式建立模型读取数据、定位数据通道。做好 AI 引入新信息的储备和通道，通过持续提升 AI 引入信息的数量和质量提升系统能力。</p><p></p><p>上述的 AI-Based 架构仅仅是一个非常“形而上”的理念，甚至不能称其为理论。一个好的应用架构，除了需要一个先进理念作为起点，更重要的是要结合具体的系统目标和实际技术能力，进行合理的创新和取舍。这段内容或许不能说明什么是好的智能系统架构，但至少能描述什么是未能摆脱传统模式的架构。</p><p></p><p>本文总结的 4 个观点是我对过去一段时间「用户增长」的个人思考和沟通交流的简单总结，可能有一些局限性，但是我当下认知水平下对于 AI 的理解。未来，我希望能有更多机会去实践和验证这些观点，能结合具体领域具体目标把这些“形而上”的理念转化成具有实践经验支撑的理论总结。也希望未来能有更多机会跟不同领域和行业的朋友进行更多交流，吸取更多信息，逐步加深对 AI 应用的理解。</p><p></p><p>会议推荐</p><p>AI 应用开发、大模型基础设施与算力优化、出海合规与大模型安全、云原生工程、演进式架构、线上可靠性、新技术浪潮下的大前端…… 不得不说，QCon 还是太全面了。现在报名可以享受 9 折优惠，详情请联系票务经理 &nbsp;17310043226 咨询。</p><p><img src="https://static001.geekbang.org/wechat/images/c8/c87f0820b187f4ea98d5fe2bdce0f4c1.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/l3byryVCmzvNMuqTtJQE</id>
            <title>秀创新、聚人才、群思辨、观趋势，第五届深圳国际人工智能展（GAIE）圆满收官</title>
            <link>https://www.infoq.cn/article/l3byryVCmzvNMuqTtJQE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/l3byryVCmzvNMuqTtJQE</guid>
            <pubDate></pubDate>
            <updated>Fri, 13 Sep 2024 14:58:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月10日，为期3天的第五届深圳国际人工智能展（GAIE）在深圳会展中心7、8号馆完美收官。本届展会由深圳市工业和信息化局、深圳市发展和改革委员会、深圳市科技创新局、深圳市政务服务和数据管理局、深圳市中小企业服务局共同指导，深圳市人工智能行业协会、深圳市万博展览有限公司主办，“深i企”有约、深圳市工业展览馆、深圳市生物医药促进会、深圳市能效管理协会、香港设计师协会、深圳市质量强市促进会、深圳市健康产业发展促进会、深圳市文化产业园区协会、深圳市卓越绩效管理促进会支持。以“智创未来·价值链接”为主题，数百名全球知名人工智能领域的专家、学者、商业领袖、AI行业品牌企业代表积极参会，现场汇聚了国内外253家人工智能企业，超2600件参展展品，展示了人工智能领域的最新技术、产品和解决方案，包括人形机器人、行业大模型、AI数字人等多个领域。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bd0d494b84dbfd8af0873b4e3546c278.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/82/82592678b3e2226d4c803f625ac31891.png" /></p><p></p><p>本届展会聚焦算力、大模型、智慧教育、智慧医疗等领域，举办了2024全球湾区科技创新发展论坛暨第五届深圳国际人工智能展开幕式、智能机器人创新发展论坛、工业制造发展论坛暨深圳标准认证颁证大会、医工融合—医疗高质量发展大会、第二届智能算力发展论坛、大模型生态与发展论坛、百度AI引领行业人工智能应用创新发展论坛、AI赋能科技品牌出海国际论坛、供需对接会、新品发布会、AI人才直聘会等30余场活动，展产品、秀创新、聚人才、群思辨、观趋势，集创新发展、交流合作、智能体验于一体，让深圳AI创新的力量为之沸腾。通过3天的线下展览交流、活动对接，累计吸引现场观众人数达3.71万人次。</p><p></p><h2>大咖齐聚，跨界思辨AI未来之道</h2><p></p><p></p><p>当科技与创新的使命超越日常生活的范畴，当世界正处于快速变化与重构的浪潮中，我们如何展现AI的价值，如何激发创新以引领变革，以及如何在当下这个瞬息万变的时代中进行深刻的思考、不懈探索与有价值的积累？为此，本届展会同期论坛邀请了中国科学院院士、天津大学精仪学院教授、院名誉院长、激光与光电子研究所所长姚建铨，美国医学与生物工程院院士、英国皇家公共卫生学院院士、深圳理工大学计算机科学与控制工程院院长潘毅，院士、深圳大学微电子研究院院长、半导体制造研究院院长王序进等近80位多领域大咖汇聚“创新之城”，从不同视角研讨、辨析人工智能如何赋能千行百业，探讨新环境下的行业态势，让AI拓展向更多未知的领域，站在跨界的视角用人工智能推动各领域深度融合与创新发展，探索未来科技与社会的新边界。</p><p></p><p>每个论坛现场座无虚席，慕名而来的观众不仅有来自全国各地的人工智能领域从业者和企业家及院校学生，更有来自不同领域包括制造业、消费行业、基础设施，能源（电池），智能硬件、算法等领域的参展商、观展商和各自领域的从业人员。大家对人工智能技术的未来应用和行业发展趋势表现出了极大的兴趣和热情。通过聆听各位院士和行业领袖的见解，参会者不仅获得了宝贵的知识和信息，还激发了对人工智能在各行各业中应用的无限想象。论坛不仅为专业人士提供了一个交流思想、分享经验的平台，也为行业内外的参与者提供了一个了解人工智能最新动态和前沿技术的机会。整个会议不仅是一场知识的盛宴，更是一次思想的碰撞，为推动人工智能技术的发展和应用贡献了智慧和力量。</p><p></p><p><img src="https://static001.geekbang.org/infoq/73/736c3e5fa93a1c8b06f7154ac7851c64.png" /></p><p></p><h2>“GAIE”大奖揭晓，激励创新提质增量</h2><p></p><p></p><p>“GAIE”大奖颁奖典礼由深圳国际人工智能展组委会主办，旨在表彰那些在人工智能领域中作出卓越贡献的企业和个人。自GAIE大奖设立以来，已有超过2000家企业参与评选，提交了逾10000项科技成果。本次颁奖以公平、公正、公开的原则，通过自主参与和专家评审团打分的方式，评选出了多个重要奖项。</p><p>在“重磅人物奖”环节，曾鹏、戚玉玲、林朗煕、吴邦毅、陈登坤等荣获“中国人工智能新锐人物”称号；王志国、段立新、文瑞、林必毅、刘洪杰、黄冠、陈宇光等被授予“中国人工智能卓越人物”；宋翔、刘轶、李涛、周剑明则被评为“中国人工智能领军人物”。这些获奖者以其敏锐的洞察力和不懈的努力，推动着人工智能产业的发展。</p><p></p><p>“最佳行业标杆应用”“最佳人工智能基础设施企业”“最具投资价值人工智能企业”“最具品牌影响力人工智能企业”“最佳人工智能服务平台”“最受欢迎人工智能产品”等奖项也相继揭晓，中国联合网络通信有限公司深圳市分公司、顺丰科技、中昊芯英（杭州）科技有限公司、深圳市易行网数字科技有限公司、用友网络科技股份有限公司等企业榜上有名。通过此次颁奖，不仅表彰了在人工智能领域做出杰出贡献的个人和企业，还激励了整个行业不断追求创新，推动技术进步和产业升级。未来，期待更多企业和个人加入这一行列，共同推动人工智能技术的创新与发展。</p><p></p><h2>深圳加快建设人工智能先锋城市</h2><p></p><p></p><p>本届展会开幕式上，深圳市工业和信息化局人工智能专班负责人赵冰冰详细解读了深圳市在人工智能领域的战略规划和政策措施。他表示，深圳率先出台全国首个人工智能产业促进条例，将人工智能纳入全市20大战略性新兴产业集群进行重点培育，高标准打造国家新一代人工智能创新发展试验区和国家人工智能创新应用先导区，全力打造人工智能先锋城市。接下来，将围绕人工智能领域“五个先锋”的发展目标，从“推进产业集聚发展、强化产业交流合作、推进产融合作、加大人才引育力度、推动包容审慎监管、强化组织保障”等方面，全面营造开放包容的人工智能发展环境。</p><p></p><h2>顺丰成果发布，物流行业的垂直领域大语言模型来了</h2><p></p><p></p><p>9月8日，顺丰科技在深圳国际人工智能展上发布了物流行业的垂直领域大语言模型——丰语，并展示了大模型在顺丰的市场营销、客服、收派、国际关务等业务板块的二十余个场景中的落地实践应用。发布会现场，中国科学院院士姚建铨，美国医学与生物工程院院士潘毅，深圳市邮政管理局副局长蓝志华，顺丰集团CIO、顺丰科技CEO耿艳坤等相关专家与领导等一起见证了这一里程碑的时刻。</p><p></p><p>随着通用大模型技术的快速发展，市场对于大模型对各行各业产生的影响与价值也充满了期待。通用大模型通识能力强，但缺少行业专业知识。如何将千行百业的专业知识数据融入大模型，让大模型“更专业”的同时，低成本、高可靠地解决行业的痛点问题，是让大模型技术产生价值的关键所在。顺丰打造的丰语大语言模型，实现了专业、可靠、成本的最佳平衡。</p><p></p><p><img src="https://static001.geekbang.org/infoq/aa/aadc06525f553b2ca0c04561da48235e.png" /></p><p></p><p></p><h2>观众互动热度不断，掀起展会活力高潮</h2><p></p><p></p><p>本届深圳国际人工智能展自开幕以来，以其前沿的科技展示、丰富的互动体验和创新的应用案例，极大地激发了观众的参与热情。无论是展会现场线上线下的互动体验，如打卡有礼、AI翻译、VR虚拟体验、专家深度对话，还是现场的互动装置、AI下棋机器人等，都紧跟时尚潮流，满足了各年龄段观众的观展和娱乐需求。这些活动不仅提升了展会在专业领域的影响力，更在公众生活中营造了浓厚的创新氛围，让人工智能技术更加贴近大众生活。观众乔女士在参与现场互动体验后表示：“这次在展会现场，我参观了20家展位，体验了AI技术在不同领域的应用，也深入了解了各企业对AI的独到见解，并见识到了涵盖日常生活、医疗、交通等多个领域的智慧产品。这不仅是一次知识的积累，更是一场未来科技的盛宴。期待在下一届展会中，能继续见证更多人工智能技术与产品，体验更加精彩的展览。”随着第五届深圳国际人工智能展的圆满落幕，我们期待在未来的展会中，继续为大家带来更多AI产品和惊喜。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/eaf88c00f21a15f9659c7977ddaaa836.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/6OmLCw2rN7RXMmV6xJj2</id>
            <title>大模型+小模型，泛能网聚焦工业印染、公建暖通、园区荷光储3个场景发布能碳智控一体机</title>
            <link>https://www.infoq.cn/article/6OmLCw2rN7RXMmV6xJj2</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/6OmLCw2rN7RXMmV6xJj2</guid>
            <pubDate></pubDate>
            <updated>Fri, 13 Sep 2024 08:27:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>能耗难降、成本无度、碳排放超标……是很多工业企业发展过程中面临的普遍痛点。</p><p></p><p>以工业印染为例，据统计，整个华东地区5万台左右的染缸，每年会带来近4000万吨的标准煤能耗，而其中20%的能耗会被浪费，包括过程中的损耗、低效能的利用等等。这20%的能耗浪费，几乎相当于一个大型钢铁厂一年的用能成本水平。</p><p></p><p>针对这类问题，新奥能源基于自身35年的产业积累，以及16年的综合能源探索，打造了一套“泛能仿真系统”。</p><p></p><p>新奥能源副总裁、新奥泛能网总裁程路在9月12日的“泛能网2024能碳数智新品发布会”上介绍，有别于能源领域传统技术下仅使用机理模型+大数据分析的局部优化方式，以及互联网企业偏重用纯算法突破能源系统优化的观点，“泛能仿真系统”采取了“两者结合的道路”，它可以快速、便捷、简单地“自动生成”一整套解决方案，即配置优化；还可以动态识别资源环境变化、用户需求变化等，实时诊断问题，自动调整设备系统运行重回最优状态，保护着能源系统的安全高效运行，即运行优化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2e/2e338b0329dee82a7f4799114f3a99f0.jpeg" /></p><p></p><p>泛能网是<a href="https://www.enn.cn/Intelligent.html">新奥集团</a>"旗下新奥数能打造的能碳产业智能平台，在其技术实践过程中，结合<a href="https://www.infoq.cn/article/PegbNEHXRRiChk51E98j">AI大模型</a>"的知识、数据和学习能力，极大优化了能源领域机理模型“长路径”的老方法，并以“选用训生”的方式打造了“大模型加小模型”的能碳产业大模型。即“选”开放模型，包括通用大模型和小模型算法，“用”积累的一系列能源数据，结合专家经验与知识抽取和集成，为“训”练和“生”成能碳产业大模型夯实基础。</p><p></p><p>“可以感受到，AI带来的技术变量，正促使着产业变革迎来黄金奇点。”程路将“泛能仿真系统”比喻为能源领域的智能驾驶系统，它可以便捷交互，可以自动判别环境或者路线的变化，自动寻找一种最优解和最佳的解决方案，并且交互也非常友好。</p><p></p><p>举例来说，该系统可以通过动态识别环境因素，如资源、用户需求的变化，诊断发现问题，进而让设备系统运行在最优状态，从而实现能源品质提升、成本降低、消耗减少、降低碳排水等目的。</p><p></p><p>在此基础上，泛能网首席产品官王尊还在会上发布了聚焦于“工业印染”、“公建暖通”、“园区荷光储”三个场景的智能产品——能碳智控一体机。</p><p></p><p>工业印染生产成本节约25%</p><p></p><p>以开篇提及的工业印染为例，该产品可针对印染过程中能耗难降、质量难控、销量难增的痛点，依靠感知、预测、分析、执行的完整智能模型，实现生产设备与供能设备的策略响应。</p><p></p><p>通过这样的印染智控技术，达成生产单耗最低，温度匹配最优，经济效益最优，高良品率，低碳排水平的综合目标，带来低碳绿色产品，碳足迹披露，低碳供应链的价值创造，提升一次成品率5%、管理效率70%；节约生产成本25%，碳排放降低450吨/染缸/年，实现经济效益与环境效益双赢。具体来说，对于一个工业印染企业每天可以减少错染布匹上万米，这些材料可供多制5000套T恤。</p><p></p><p>楼宇建筑供冷供热品质提升30%</p><p></p><p>公建暖通是国内建筑能耗最大的场景，对标国际能耗标准也仍有非常大的差距。以酒店为例，由于系统基本处于无人管理状态，没有根据人流高低峰值进行监测和优化，导致了大量的能耗浪费。对此，泛能网的能碳智控一体机拥有超过15大类感知能力，上百种设备感知能力，多模型组合寻优覆盖模型数量20+，并且其预测精准度超95%，最终可以实现供冷供热品质提升30%，供温达标率超 95%等明显效果，为商场、办公楼、酒店等公共建筑解决环境品质低、暖通能耗高、自控水平差的常见问题，带来高品质、更节能，强智能的优化效果。</p><p></p><p>会上，<a href="http://www.zhonganjt.cn/">众安商业集团</a>"运营总经理邵壮还带来了真实案例分享。他指出，冷暖温度对于商场的运营至关重要，而在能源成本中，暖通的耗能高达商场运营的15%。杭州萧山众安广场为提升顾客与商户的满意度，降低耗能成本，携手泛能网进行了“本地改造+AI智能系统”的线上线下一体化智慧暖通升级方案，现阶段已实现明显效果，特别是夏季高峰期综合能耗同比历史平均水平下降超过40%，全年节省能源成本50多万元。</p><p></p><p>将园区光伏发电量提升3%-5%</p><p></p><p>针对园区荷光储场景下光伏和储能在尖峰电价和低谷电价时段的配合、根据用户需求的策略优化调整等问题，往往存在的收益保障难和策略执行难的诸多挑战。对此，泛能网能碳智控一体机把秒级的数据响应能力和超 95% 预测能力，以及15分钟级别的滚动优化能力、分钟级的安全控制能力、多目标协同的优化能力等“打包集成”，塑造了荷光储智控的核心能力，让管理效率可以提升30%，综合运营收益提升10%。</p><p></p><p>程路指出，这套方案支持园区在不替换任何硬件设备的基础上实现智能化改造，进而将光伏发电量提升3%-5%，这对于一个小型园区的某一客户额外而言相当于每年增加了25万的收益，减少碳排放量相当于中了1950棵树。</p><p></p><p>值得一提的是，正如自动驾驶系统不需要人操作那样，泛能网在云端提供的能碳智能伙伴，还以虚拟“小助手”的形象，可以和智控一体机进行交互。在日常工作中，还可通过泛能网手机 APP 掌握全局。据王尊介绍，能碳智控一体机适用于不同的行业，无论是中小型项目还是大型场景，都有灵活的合作及部署方式。</p><p></p><p>程路强调，能碳智控一体机是新奥泛能网在“双碳”战略大背景下，洞察到企业在存量博弈的时期，普遍存在价格竞争、产品同质化、用能排放约束，创新不足，增速放缓等痛点推出的新一代数智新品。其希望凭借能源系统一小步的提质增效，帮助更多行业、更多企业走出“内卷”， 助推新质生产力的发展。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/23uAXwB7q7dTYB9iwKTR</id>
            <title>云上 AI 时代，逆势涨薪攻略</title>
            <link>https://www.infoq.cn/article/23uAXwB7q7dTYB9iwKTR</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/23uAXwB7q7dTYB9iwKTR</guid>
            <pubDate></pubDate>
            <updated>Fri, 13 Sep 2024 08:03:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在数字化转型的浪潮中，云计算和人工智能（AI）已成为推动技术创新和业务增长的关键动力。不仅技术人员，包括非技术人员、业务人员乃至决策者都面临着提升数字技能的迫切需求。</p><p></p><p>据 InfoQ 研究中心的《中国生成式 AI 开发者洞察 2024》报告显示，企业对于掌握云计算和 AI 技能的人才需求日益增长。这种需求的增长，不仅体现在对传统技术岗位的需求上，更扩展到了业务分析、项目管理、决策支持等各个领域。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1c/1c666223e4c4d91cb6e41b3e03c27365.webp" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/c4/c48c38450cf43012d264fc22f3df9a4b.webp" /></p><p></p><p>在这样的大环境下，拥有专业认证成为了区分技术人才的重要标准。它不仅能够证明个人的专业技能和知识水平，更是职场晋升和薪资增长的有力助推器。特别是在云计算和 AI 领域，专业认证几乎成为了高薪就业的敲门砖。</p><p></p><h2>培训与认证体系众多，如何选择？</h2><p></p><p></p><p>当前，市场上充斥着各种云和 AI 相关的培训与认证项目，但并非所有认证都能为个人职业发展带来实质性的帮助。</p><p></p><p>首先，市场上培训与认证项目质量参差不齐，尤其是 AI 相关的课程，一些培训课程内容过时，无法跟上技术的发展；其次，并非所有认证都具备行业广泛认可的权威性，一些认证缺乏行业影响力，对职业发展帮助有限；第三，某些认证项目费用昂贵，但投资回报率并不明确，会给学习者带来经济负担。</p><p></p><p>因此，选择一个权威、前沿且被广泛认可的认证项目，对于职场人士来说至关重要。一个优秀的认证项目应该具备更新及时的课程内容、能够提供整个行业认可的证书、并且对个人的工作能力及职业发展有所助力。</p><p></p><p>如何选择一个靠谱的培训与认证体系呢？大家可以从以下四点进行评估：</p><p></p><p>课程内容的及时更新：紧跟技术发展趋势，选择推出时间更新且定期更新课程内容的培训与认证体系。行业认可度：优先选择一些头部厂商、权威机构推出的认证，这些认证具有更广泛的认可度，能够增加就业竞争力。国际权威性：优先选择具有国际权威性的认证，这样的认证不仅在国内有影响力，也有助于在全球范围内提升个人职业资质。职业发展支持：优秀的认证体系应提供职业发展支持，如就业指导、职业规划咨询和行业网络资源等。</p><p></p><h2>专业认证：职场竞争“加薪”项</h2><p></p><p></p><p>面对众多的培训和认证选择，InfoQ 基于深入的市场调研和严格的评估标准，特别推荐两款备受行业认可的认证项目：亚马逊云科技的【云从业者认证】和【AI 从业者认证】。亚马逊云科技的认证项目一直是业界公认的黄金标准，不仅仅在国内，在国际上都有很高的权威性和认可度。</p><p></p><p>以上两款认证不仅涵盖了云计算和 AI 领域的最新技术，还提供了实践性强的培训课程，帮助大家提升技能，增强市场竞争力。</p><p></p><p>根据市场调研数据，拥有亚马逊云科技认证的专业人士在职场上更具竞争力，他们的薪资水平普遍高于非认证人员。具体数据显示，认证持有者的平均薪资提升 74%，而且在求职市场上的竞争力提升超过 83%。此外，92% 的雇主表示他们更倾向于雇佣持有专业认证的候选人，这进一步证明了亚马逊云科技认证在职场中的高含金量。</p><p></p><p>当然，考取亚马逊云科技的云从业者认证和 AI 从业者认证，不仅仅是为了拿到一纸证书，更重要的是通过这一过程真正提升个人的技术水平和解决问题的能力，最终，实现升职和涨薪的目标，开启更广阔的职业发展空间。</p><p></p><h2>云端 X AI 高薪人才培养战略</h2><p></p><p></p><p>如此高含金量且权威的认证，是不是很难考取呢？缺乏自制力，难以坚持学习怎么办？学习时间有限，通过不了考试怎么办？</p><p></p><p>别担心，InfoQ 为大家定制了【云端 X AI 高薪人才培养战略】。该战略针对【云从业者认证】和【AI 从业者认证】分别提供系统化的学习课程，并提供【培训 - 认证 - 提薪】一站式赋能服务。跟随课程的节奏进行学习，考试通关率将大幅提升。AI 从业者认证特别开放考试冲刺班，班次一 9 月 20 日截止报名，班次二 10 月 16 日截止报名 ; 考试冲刺班各限额 300 名，名额不多，先到先得！</p><p></p><p>不仅如此，参与该战略还可以享受 50% 折扣优惠报考及一次免费补考机会，相当于获得了一块认证的“免死金牌”（云从业认证考试 9 月 30 日前完成首次考试可享此权益；AI 认证考试 11 月 15 日前完成首次考试可享此权益）。</p><p></p><p>活动细节及参与方式详见海报，扫描二维码即刻加入，开启你的云端 X AI高薪之旅！</p><p></p><p><img src="https://static001.geekbang.org/infoq/20/208080ff5067d58369d5eb0e4e6b9572.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/NbHkv9fvR8lemlARRBzL</id>
            <title>新一代多维表格，让一线员工搭建系统不求人</title>
            <link>https://www.infoq.cn/article/NbHkv9fvR8lemlARRBzL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NbHkv9fvR8lemlARRBzL</guid>
            <pubDate></pubDate>
            <updated>Fri, 13 Sep 2024 07:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月4日，飞书发布了全新多维表格、低代码平台等系列业务工具产品，并推出了面向出海企业的跨境合规解决方案。这些产品将继续为中国企业提供实质的降本增效帮助，促进企业以更低的数字化成本解决实际业务问题。</p><p></p><p>自飞书2020年发布多维表格开始，这款产品逐渐为大众使用与熟知，并成为了一个全新品类。据飞书透露，飞书多维表格的月活数已经达到 600 万，仅过去一年，飞书用户便创建了近 4000 万个多维表格，在这些多维表格上，流转着超过 100 亿条记录。在泡泡玛特、元气森林、蔚来汽车等知名公司，飞书多维表格均以极小成本解决了重要业务需求，可贵的是，这些业务系统均由不懂技术的一线员工搭建。</p><p></p><p>如今，飞书正式推出飞书多维表格数据库，这让飞书多维表格的单表容量突破了100万行，仪表盘也可统计1000万行数据，均为全球同类产品中最高。在全新的强大性能下，即使在飞书多维表格中计算10 万行、100 列公式这样复杂的数据，仍然能在5 秒内便获取业务结果。飞书多维表格还发布了全新一代仪表盘，通过飞书多维表格数据库的计算能力，由多维表格行列数据生成的仪表盘，将不再是简简单单计算、汇总、呈现数据，增加了大量计算、图表组件编组、统计分析等功能，界面也可对标全球顶尖 BI 系统。</p><p></p><p>以下为飞书多维表格负责人施凯文的分享全文，经InfoQ整理。</p><p></p><h1>开场</h1><p></p><p>大家好我是施凯文，飞书多维表格负责人。</p><p></p><p>相信大家一定对谢欣刚刚所描述的，多维表格的种种使用场景和承载的解决方案印象深刻。</p><p>在多维表格这个品类里，核心功能和技术特别特别的多，今天，我就为大家介绍一下其中最至关重要的几个模块，也会在各个模块中为大家揭晓今天的重大更新。</p><p><img src="https://static001.infoq.cn/resource/image/38/e0/3802c516ecc66e70333a634427e3ffe0.png" /></p><p></p><h4>多维表格数据库</h4><p></p><p></p><h5>行数扩容</h5><p></p><p>第一个要介绍的是，多维表格的基础设施：存储和计算。</p><p></p><p>说到存储和计算，我们首先要提及的就是行数，行数的多少直接决定了多维表格能承载业务规模的大小。</p><p>可在多维表格这个品类中，增加行数可没那么容易。</p><p></p><p>多维表格的行除了承载了我们熟悉的文本、数字、图片等这些内容外，同时还承载了动态的计算值，例如公式、引用，甚至是跨数据表的大规模计算引用。</p><p></p><p>还不止这些，每一个这样的行，还要允许被业务人员设定的各式各样的流程、自动化，所消费和订阅。</p><p>只有满足这些能力，才能真正让数据动起来、流转起来。</p><p></p><p>我们管拥有这样能力的行叫 "热行"。</p><p><img src="https://static001.geekbang.org/infoq/90/907b30b35a59f41ed9dfccec97e1ffe9.png" /></p><p>下面让我们来看一下目前在全球范围内支持热行的同类产品的能力对比图：</p><p></p><p>从这张图我们不难发现，即便是在此时此刻，飞书多维表格也比行业领先不少。</p><p></p><p>无论是单数据表可支持的行数、还是公式计算的灵活度。</p><p></p><p>然而，对我们而言，这远远不够。</p><p></p><p>随着多维表格在企业和组织中应用的日益广泛，使用场景的愈发复杂，在这种高速增长的背景下，我们正面临着两个关键的挑战：容量不足、计算速度越来越慢。</p><p></p><p>为了彻底解决这些问题。我们投入了近百名来自飞书团队和字节数据库团队的顶尖工程师，耗费数万人天，正式打造了全新的基础设施 —— 多维表格数据库。</p><p></p><p>搭载了多维表格数据库的全新一代多维表格单数据表能够支持 100 万热行。</p><p></p><p>这比旧版多维表格单数据表的容量提升了 20 倍。</p><p></p><p>并且遥遥领先于全球同类产品。</p><p><img src="https://static001.geekbang.org/infoq/89/896c1247f1dc2eaa06d433858103e10b.png" /></p><p>今天我们也可以非常自信和自豪的说，多维表格这个品类，将进入单表百万行时代，这也将成为多维表格这个品类的全新标准。</p><p></p><h5>计算提速</h5><p></p><p></p><p>容量介绍完了，现在让我们来聊一下：计算速度。</p><p></p><p>在过去，数据量只有几百上千行时，计算的公式多一些用户可能感受并不明显，但，当数据量增加到几千上万行时，速度就明显变慢起来了。</p><p></p><p>如今在多维表格数据库的超大容量加持下，动辄几万、十几万行的数据，在这样海量的数据下计算速度又会是什么样的呢？</p><p></p><p>为了让大家对上万行数据计算的场景和体验能有一个明显的感受，我们准备了一个同类产品，也包括旧版多维表格的效果对比视频，大家可以先感受一下。</p><p></p><p>这是一个还原客户真实应用场景的例子，客户是一家零售领域的知名公司，正在用一张多维表格作为销售数据管理系统，用来管理全国数十家大型消费综合体的经营数据。</p><p></p><p>在这个例子的真实场景中，实际的数据量有 4 万多行，但是呢，因为其他同类产品目前对热行的支持基本只才刚刚达到了 2 万行，所以我们采用了 2 万行的数据，来给大家演示对比效果。</p><p></p><p></p><p>这张两万行的数据表，仅仅包含了几十列的公式，在飞书多维表格的诸多应用场景中，还算不上是特别复杂的，但我们已经看到了，还是非常的慢。</p><p></p><p></p><p></p><p>现在让我们再看一下，搭载了多维表格数据库的全新多维表格来执行相同的数据量的计算，是什么表现。</p><p>相信大家已经有非常明显的感受了。</p><p></p><p>搭载了多维表格数据库的全新一代多维表格计算速度有指数级的提升、最高可达 100 倍。</p><p></p><h5>Time Machine</h5><p></p><p></p><p>容量超级扩容、计算速度百倍提升、除了这些强大的能力以外，今天，我们还将向大家介绍一个多维表格数据库的全新，也是我们认为必不可少的一个能力。</p><p></p><p>Time Machine，一个可以实现精确到任意操作者、任意操作行为的溯源和回滚的能力，特别是在多人协作场景下，Time Machine 可以实现对全体协作成员的、全部操作行为的，可追溯、可回滚。</p><p></p><p>并且 Time Machine 还接入了电子取证和 DLP 系统，真正做到让管理者安心、放心。</p><p></p><p>这么多强大的能力都源于我们全新打造的基础设施——多维表格数据库。</p><p></p><p>它整合了多种先进技术，包括内存表格视图引擎、Rust 公式迅算引擎、智能算力调度引擎、 MPP 大规模并行处理系统以及 Time Machine 多版本存储引擎。</p><p></p><p>正是这些尖端技术的融合，使得全新一代多维表格能够提供如此卓越的性能和表现。</p><p></p><p>今天，多维表格已经有了 600 万的月活用户，过去一年时间累计创建了超过 4000 万张多维表格，100 亿行数据。此刻，我们不但有信心，还有依据，在这里宣布，新一代多维表格的全新标配能力：</p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/ca206ca9ad6698f222bb5ad7a1225071.png" /></p><p></p><p>单表数据容量能够支持 100 万热行、计算速度百倍提升、大数据行数下视图秒级加载、全维度数据可追溯、可回滚。</p><p></p><p>以上这些全新的强大能力，都将于今年 10 月份起陆续发布。</p><p></p><h4>全新仪表盘</h4><p></p><p></p><p>好，现在，我来为大家介绍多维表格的第二项重要功能 —— 仪表盘。</p><p></p><p>2022 年，飞书多维表格首次引入仪表盘功能。它是一款易用性极高的可视化和分析工具。</p><p></p><p>它为业务人员提供了灵活配置图表、自定义布局以及实时数据分析的能力。自推出以来，就深受广大用户的喜爱。</p><p></p><p>然而，随着用户需求的不断演进和业务场景的日益丰富，我们也收到了大量反馈，呼吁增强仪表盘的功能，特别是在图表的多样性上和数据分析的深度方面。</p><p></p><p>为了响应这些需求并保持多维表格的领导地位，我们郑重推全新一代多维表格仪表盘。</p><p></p><p><img src="https://static001.geekbang.org/infoq/32/32ef5357928972ca32fda04050506c25.png" /></p><p></p><p>它将全面支持市面上所有主流图表，接入数量，达到 50 个之多。</p><p></p><p>这也将使飞书多维表格仪表盘成为一个功能完备的数据可视化平台，为用户提供全方位的图表选择，以满足各种复杂的数据分析和展示需求。</p><p></p><p>其次，全新一代多维表格仪表盘还将对整个交互设计进行重大升级。</p><p></p><p>视觉效果，全面对齐世界顶尖 BI 产品。</p><p></p><p>同时，我们还为仪表盘带来了，图表编组能力、样式控制能力。</p><p></p><p>无论是数据的可读性、还是界面的自定义也都将迎来重大提升。</p><p></p><p>我们还大幅增强了仪表盘的数据分析能力。引入了数据透视能力</p><p></p><p>单图表支持多数据源联合分析能力。</p><p></p><p>并且依托于全新的多维表格数据库，我们将多维表格仪表盘的性能提升到了前所未有的高度。</p><p></p><p>全新一代多维表格仪表盘中的每一个图表都可对高达 1000 万行来自最多 200 张数据表的数据，进行实时的统计分析。</p><p></p><p>这就是全新多维表格仪表盘。</p><p><img src="https://static001.geekbang.org/infoq/79/795190cb9bbd884bc919353c54693166.png" /></p><p></p><h4>高级权限</h4><p></p><p>接下来我为大家介绍的是多维表格的第三项重要功能：高级权限。</p><p></p><p>随着多维表格应用场景的日益丰富，在同一张多维表格上协作的人数也正变的越来越多，协作的人多了，对应的角色也就变多了，需要管理的内容也变的多样和细致。</p><p></p><p>为应对这一挑战，我们郑重推出支持颗粒化管理的全新高级权限。</p><p></p><p>首先，全新一代多维表格高级权限，首次将权限控制的范围扩展至功能模块。</p><p></p><p>使用者能进一步精确掌控如仪表盘、自动化、视图等高价值功能模块的使用、创建和分享。</p><p><img src="https://static001.geekbang.org/infoq/65/6550a5f25bea7d36bc59182b565bb3af.png" /></p><p>同时使用者还能够精确控制模块内部的具体元素，如仪表盘中特定组件的可操作性，自动化流程中特定步骤的配置权限。</p><p></p><p>我们还进一步强化了条件权限功能，将权限管理提升至动态匹配的全新高度。使权限分配能够根据特定条件自动调整。</p><p></p><p>比如，管理者可以设置在某个关键指标达到预设值时才开放特定数据的访问权限，或在指定日期范围内授予用户临时权限。</p><p></p><p>最后我们实现了高级权限的突破性能力 —— 千人千面的仪表盘。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c1/c1497c46a344e5ddd33cd9174e86cc06.png" /></p><p>它可以根据每位用户自身对应的权限，自动计算并展现用户有权访问的数据。</p><p></p><p>让每个用户都只看到符合自身权限的仪表盘数据。</p><p></p><p>同时，依托于多维表格数据库，我们在全球范围内，首次，将千人千面的仪表盘管控，扩展到 1000 万行数据规模。</p><p><img src="https://static001.geekbang.org/infoq/c7/c76c33de5af743f4be987f25903860eb.png" /></p><p>这些先进特性的整合，为企业提供了一个安全、高效且适应性极强的数据管理解决方案，有效应对企业在数据治理和协作效率方面的多重挑战。</p><p></p><h4>工作流</h4><p></p><p>好，接下来让我为大家介绍多维表格的第四项重要功能，也是多维表格核心功能的全新成员：工作流。</p><p></p><p>随着多维表格的使用场景变得越来越多，流程的定制化程度也变得越来越复杂。</p><p></p><p>如何让自动化支持可视化的流程图，从而让用户可以全局掌控，流程的结构、节点，和逻辑。</p><p></p><p>如何让自动化支持更便捷的条件节点、循环处理等高级能力？</p><p></p><p>为了解决这些问题，我们推出了全新的功能模块，工作流。</p><p><img src="https://static001.geekbang.org/infoq/dd/dd3c709adb4e4fa5b3aacb7047dd0a1d.png" /></p><p>工作流，更侧重于流程的直观呈现和灵活调整，支持构建多个条件分支、嵌套逻辑等复杂场景。</p><p></p><p>全新的编辑器，全流程的管控、对复杂逻辑的支持、丰富开放的节点能力，这就是全新的多维表格工作流。</p><p></p><h4>多维表格 AI</h4><p></p><p>好，下面到了今天多维表格新功能发布的最后一个环节，多维表格 AI。</p><p><img src="https://static001.geekbang.org/infoq/41/4141f88910a83cc04cacda7efc5f9911.png" /></p><p>在介绍之前，先让我花一点点时间，为大家讲一下，我们关于多维表格 AI 的几点思考。</p><p></p><p>首先，我们一直在探索如何借助 AI 来帮助用户快速、深入地掌握多维表格的高价值功能。</p><p></p><p>其次，我们还在探索，如何更好的集成 AI，从而扩展多维表格解决问题的范围。</p><p></p><p>今天我们要发布的 AI 能力，就来自这样的思考。</p><p></p><h5>仪表盘智能分析</h5><p></p><p><img src="https://static001.geekbang.org/infoq/c1/c1da1f79153065bb1a9ba3c5aacb77bb.png" /></p><p>首先，我将为大家介绍一系列用来帮助用户更好的使用多维表格的 AI 工具。</p><p></p><p>我们为仪表盘内的每个图表都增加了智能分析功能。</p><p></p><p>当你打开仪表盘，遇到一些复杂的图表时，无需花费时间去了解细节，只需点一下 智能分析 按钮，便可在几秒内获得数据背后的洞察。</p><p></p><h5>AI 生成公式</h5><p></p><p><img src="https://static001.geekbang.org/infoq/3c/3cda396caa7a85b44683e29c5d621fc2.png" /></p><p></p><p>我们还在公式编辑器中，提供了 AI 生成公式的能力。</p><p></p><p>只需描述需求，就可以生成一段高水准的公式。</p><p></p><h5>自动化 AI 节点</h5><p></p><p><img src="https://static001.geekbang.org/infoq/8a/8acc5f0019f87e8e91e92f40f7e2193b.png" /></p><p>我们还在自动化和工作流中增加了 AI 内容生成节点。</p><p></p><p>比如，当今天的日期到达某个客户的生日的时候，用 AI 生成节点基于这个客户的身份、行业、年纪、性格等等信息，快速生成一份定制化的生日祝福。</p><p></p><p>并通过邮件、或其他社交平台发送给客户。</p><p></p><h5>AI 智能问答</h5><p></p><p><img src="https://static001.geekbang.org/infoq/1f/1f461f8057cd1f632195bee64880d348.png" /></p><p>我们还为多维表格提供了智能问答的功能，允许用户对整个多维表格内的所有数据进行询问，帮助使用者更好地理解和洞察多维表格内的数据。</p><p></p><p>例如，在一张商品销售记录的多维表格中，</p><p></p><p>用户可以直接询问：“最近销量比较好的商品是什么”；</p><p></p><p>我们将以上一系列实用的 AI 功能无缝的集成到多维表格的各个功能界面中，从而帮助企业潜移默化的提升整个团队的综合能力。</p><p></p><p>除了以上这些 AI 功能外，还有什么 AI 能力是能够扩展多维表格解决问题的范围的？</p><p></p><p>并且是在多维表格内才能发挥最大效用的？</p><p></p><h5>字段捷径</h5><p></p><p><img src="https://static001.geekbang.org/infoq/f9/f97926ad273e0299941b4726e35072f8.png" /></p><p>今天，我们重磅推出，全新多维表格 AI 能力字段捷径。</p><p></p><p>我们将 AI 功能封装进简单的字段配置环节里，使用户无需了解复杂的底层细节，只需简单勾选即可完成参数设置和结果输出；</p><p></p><p>同时充分利用多维表格的批量数据处理优势，将 AI 能力整合到这一功能中，实现智能化的批量生成，从而大幅提升工作效率。</p><p></p><p>现在让我们快速看几个例子：</p><p></p><p>这是一个利用 AI 字段捷径对大量用户评论进行自动标记和分类的例子。</p><p></p><p>通过在字段面板上选择 AI 智能分类，并勾选需要处理的内容。就能够对所有客户的评论，进行智能分类。</p><p>这是一个合同证据提取的 AI 字段捷径。用户只需选择对应的合同附件字段，便能自动从 PDF 文件中快速提取关键证据。</p><p></p><p>还有更多强大的 AI 字段捷径能力，例如，AI 全网搜索、视频生成、名片、文字识别等等。</p><p></p><p>我们希望将更多的，更丰富的 AI 能力都带入到多维表格的字段捷径中来。</p><p></p><p>今天我们首次将多维表格的字段开放，并打造了字段捷径中心。</p><p><img src="https://static001.geekbang.org/infoq/90/90ff2c86381e97d0783bfd7f0439478b.png" /></p><p>允许和鼓励，优质的创业公司、开发者将他们眼中的有趣的、强大的 AI 能力发布到多维表格的字段捷径中心来。</p><p></p><p>目前，多维表格字段捷径中心已吸引了数十家优质创业公司入驻并发布了他们的 AI 字段捷径。其中包括多家知名企业：</p><p></p><p>月之暗面，发布了 PDF 阅读助手字段捷径，零一万物发布了智能巡检字段捷径，智谱 AI 发布了视频生成字段捷径。此外，大饼 AI、Autodocs 、Nolibox 等等优秀创业公司也都贡献了各自的特色字段捷径。</p><p></p><p>这些高质量的 AI 字段捷径不仅丰富了多维表格的功能，也为用户提供了多样化的、智能化的解决方案。</p><p>我们也相信，一款强大的工具平台一定离不开生态，只有更开放的引入多种多样的生态能力才能让我们的客户更好的、更快的接触到更多更先进的能力，从而打造出具有无限可能的业务应用和系统。</p><p><img src="https://static001.geekbang.org/infoq/b4/b4db7081aa4ebc75096b1107eeb5c29f.png" /></p><p></p><p></p><h4>总结</h4><p></p><p><img src="https://static001.geekbang.org/infoq/76/76fed9b7d5d9c7c95281f24cdb6924fb.png" /></p><p>现在让我们来回顾一下，今天多维表格的所有重大更新。</p><p></p><p>全新的基础设施多维表格数据库彻底扫清了业务人员在采用多维表格搭建各式各样的业务系统过程中的各种阻力。</p><p></p><p>让业务人员采用多维表格搭建系统，更安心、放心。</p><p></p><p>全新的多维表格仪表盘，界面对标全球顶尖 BI 产品，相对于过去具备更高级的数据分析能力，性能迎来了指数级提升。</p><p></p><p>全新的多维表格高级权限，提供超颗粒化的权限管理能力，为企业提供了一个安全、高效且适应性极强的数据管理环境。</p><p></p><p>全新多维表格工作流，提供了高级的逻辑处理节点，和可视化编排能力，为企业提供更多样化的业务流程自定义能力。</p><p></p><p>多维表格全面集成 AI，将一系列强大的 AI 能力融入系统搭建的各个环节。</p><p></p><p>4 年前，我们在中国首次定义了多维表格这个品类，将其定位为一款帮助业务人员自主自助搭建业务系统的平台。</p><p><img src="https://static001.geekbang.org/infoq/53/53334012a8141efb346758caabf7b80b.png" /></p><p>作为该领域的开创者和领导者，我们在这段时间里取得了不俗的成绩。</p><p></p><p>今天，我们发布了一系列强大的能力更新，重新定义多维表格。</p><p></p><p>下面让我们看一段全新多维表格的介绍视频。</p><p></p><p></p><p></p><p>全新一代多维表格，更强大、更智能、更敏捷，让每个人都能轻松搭建自己的智能业务系统。今天我们向各行各业的先进企业发出邀请，帮助 10,000 个企业率先搭建下一代智能业务系统。</p><p></p><p>以上就是多维表格今天发布的全部内容。</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/VI9FSs7mDIE5zExDPYf4</id>
            <title>100B的「跨级」跃升！元象发布最大MoE开源大模型，「高性能全家桶」系列全部免费</title>
            <link>https://www.infoq.cn/article/VI9FSs7mDIE5zExDPYf4</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/VI9FSs7mDIE5zExDPYf4</guid>
            <pubDate></pubDate>
            <updated>Fri, 13 Sep 2024 06:42:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月13日，元象XVERSE发布中国最大MoE开源模型：XVERSE-MoE-A36B。该模型总参数255B，激活参数36B，能达到100B模型的性能「跨级」跃升，同时训练时间减少30%，推理性能提升100%，使每token成本大幅下降。</p><p></p><p>并且，元象「高性能全家桶」系列全部开源，无条件免费商用，海量中小企业、研究者和开发者能按需选择。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/66/6694db23e2fa9c98230ae723c3c4bbc8.png" /></p><p></p><p></p><p>MoE（Mixture of Experts）是业界前沿的混合专家模型架构 ，将多个细分领域的专家模型组合成一个超级模型，打破了传统扩展定律（Scaling Law）的局限，可在扩大模型规模时，不显著增加训练和推理的计算成本，并保持模型性能最大化。出于这个原因，行业前沿模型包括谷歌Gemini-1.5、OpenAI的GPT-4 、马斯克旗下xAI公司的Grok等大模型都使用了 MoE。</p><p></p><p>免费下载大模型</p><p>Hugging Face：https://huggingface.co/xverse/XVERSE-MoE-A36B</p><p>魔搭：https://modelscope.cn/models/xverse/XVERSE-MoE-A36B</p><p>Github：https://github.com/xverse-ai/XVERSE-MoE-A36B</p><p></p><p></p><h1>商业应用上更进一步</h1><p></p><p></p><p>元象此次开源，在商业应用上也更进一步。</p><p></p><p>元象基于MoE模型自主研发的AI角色扮演与互动网文APP Saylo，通过逼真的AI角色扮演和有趣的开放剧情，火遍港台，下载量在中国台湾和香港娱乐榜分别位列第一和第三。</p><p></p><p>MoE训练范式具有「更高性能、更低成本」优势，元象在通用预训练基础上，使用海量剧本数据「继续预训练」（Continue Pre-training），并与传统SFT（监督微调）或RLHF（基于人类反馈的强化学习）不同，采用了大规模语料知识注入，让模型既保持了强大的通用语言理解能力，又大幅提升「剧本」这一特定应用领域的表现。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8a/8a4098894b61fd7592d17a58e0b454c6.png" /></p><p></p><p></p><p>在商业应用上，元象大模型是国内最早一批、广东前五获得国家备案的大模型，可向全社会提供服务。</p><p></p><p><img src="https://static001.geekbang.org/infoq/52/52df7088a8cbf7b6aa05c40df929437a.png" /></p><p></p><p>从去年起，元象大模型已陆续与QQ音乐、虎牙直播、全民K歌、腾讯云等深度合作与应用探索，为文化、娱乐、旅游、金融领域打造创新领先的用户体验。目前，元象累计融资金额已超过 2 亿美元，投资机构包括腾讯、高榕资本、五源资本、高瓴创投、红杉中国、淡马锡和CPE源峰等。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/27/2775e2b5166bcf74e5017d79a5605508.png" /></p><p></p><p></p><p></p><h1>MoE技术自研与创新</h1><p></p><p></p><p>MoE是目前业界最前沿的模型框架，由于技术较新，国内外开源模型或学术研究同步探索。元象在此次升级中围绕效率和效果进行了如下探索：</p><p></p><p></p><h2>效率方面</h2><p></p><p></p><p>MoE架构与4D拓扑设计：MoE架构的关键特性是由多个专家组成。由于专家之间需要大量的信息交换，通信负担极重。为了解决这个问题，元象采用了4D拓扑架构，平衡了通信、显存和计算资源的分配。这种设计优化了计算节点之间的通信路径，提高了整体计算效率。</p><p></p><p>专家路由与预丢弃策略：MoE的另一个特点是“专家路由机制”，即需要对不同的输入进行分配，并丢弃一些超出专家计算容量的冗余数据。为此元象团队设计一套预丢弃策略，减少不必要的计算和传输。同时在计算流程中实现了高效的算子融合，进一步提升模型的训练性能。</p><p></p><p>通信与计算重叠：由于MoE架构的专家之间需要大量通信，会影响整体计算效率。为此团队设计了“多维度的通信与计算重叠”机制，即在进行参数通信的同时，最大比例并行地执行计算任务，从而减少通信等待时间。</p><p></p><p></p><h2>效果方面</h2><p></p><p></p><p>专家权重：MoE 中的专家总数为 N ，每个 token 会选择 topK 个专家参与后续的计算，由于专家容量的限制，每个 token 实际选择到的专家数为 M，M&lt;=K<="" p=""></p><p></p><p>实验1：权重在 topM 范围内归一化</p><p>实验2：权重在 topK 范围内归一化</p><p>实验3：权重在 topN 范围内归一化</p><p>实验4：权重都为 1</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/9e/9e11f4ddfc77c7073e4c463d6c80f93b.webp" /></p><p></p><p>对比实验结果</p><p></p><p>举例说明，假设N=8，K=4，M=3（2号专家上token被丢弃），不同专家权重的计算方式所得的权重如下图：</p><p></p><p><img src="https://static001.geekbang.org/infoq/0b/0b45bd8959a1ad2b1bb6575173c959d3.webp" /></p><p></p><p></p><p>数据动态切换：元象以往开源的模型，往往在训练前就锁定了训练数据集，并在整个训练过程中保持不变。这种做法虽然简单，但会受制于初始数据的质量和覆盖面。此次MoE模型的训练借鉴了"课程学习"理念，在训练过程中实现了动态数据切换，在不同阶段多次引入新处理的高质量数据，并动态调整数据采样比例。</p><p></p><p>这让模型不再被初始语料集所限制，而是能够持续学习新引入的高质量数据，提升了语料覆盖面和泛化能力。同时通过调整采样比例，也有助于平衡不同数据源对模型性能的影响。</p><p></p><p><img src="https://static001.geekbang.org/infoq/89/89394bf927320f9dcc53c8f866fa8f55.webp" /></p><p></p><p>不同数据版本的效果曲线图</p><p></p><p>学习率调度策略（LR Scheduler）：在训练过程中动态切换数据集，虽有助于持续引入新知识，但也给模型带来了新的适应挑战。为了确保模型能快速且充分地学习新进数据，团队对学习率调度器进行了优化调整，在每次数据切换时会根据模型收敛状态，相应调整学习率。实验表明，这一策略有效提升了模型在数据切换后的学习速度和整体训练效果。</p><p></p><p>下图是整个训练过程中 MMLU、HumanEval 两个评测数据集的效果曲线图。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f1/f1b1c81ac78dc7215c6cf1da11148cfe.webp" /></p><p>训练过程中MMLU、HumanEval的性能曲线持续拔高</p><p></p><p>通过设计与优化，元象MoE模型与其Dense模型XVERSE-65B-2相比，训练时间减少30%、推理性能提升100%，模型效果更佳。</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/PegbNEHXRRiChk51E98j</id>
            <title>OpenAI 有 o1 大模型，QCon 有大模型推理技术实践，大模型基础设施与算力优化拿捏！</title>
            <link>https://www.infoq.cn/article/PegbNEHXRRiChk51E98j</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/PegbNEHXRRiChk51E98j</guid>
            <pubDate></pubDate>
            <updated>Fri, 13 Sep 2024 04:47:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>刚刚，OpenAI 震撼发布 o1 大模型！新模型可以实现复杂推理，强得可怕！！！在即将于 <a href="https://qcon.infoq.cn/2024/shanghai/">10 月 18-19 日召开的 QCon 上海站</a>"，月之暗面、微软亚洲研究院、商汤科技等企业的资深技术专家也将分享推理相关话题，Mooncake 分离式推理、长文本 LLMs 推理优化、异构分布式大模型推理技术……简直是会圈天菜！<a href="https://qcon.infoq.cn/2024/shanghai/track/1715">大模型基础设施与算力优化实践</a>"轻松拿捏！</p><p></p><h2>精彩内容速递</h2><p></p><p></p><h4>Mooncake 分离式推理架构创新与实践</h4><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/05/056c75dd48346381d58edb76a408dc08.png" /></p><p></p><p>随着大型语言模型的社会影响力日益增强，相应的人工智能产品用户基数也在迅速扩大。目前，AI产品发展的一个主要挑战是如何在有限的计算资源下，有效应对日益增长的用户需求。本议题从实际业务出发，探讨在固定集群资源的条件下，通过采用单点和分布式推理架构，提升集群处理大规模请求的能力，过程中遇到的挑战以及我们的解决策略，希望能给大家带来一些帮助和思考。</p><p></p><p>演讲提纲</p><p>1. 大规模推理挑战</p><p>优雅的集群过载超长上下文性能挑战故障定位与自动运维</p><p></p><p>2. 单点性能优化</p><p>混合并行策略长上下文推理优化</p><p></p><p>3. 分离式架构 Mooncake</p><p>设计场景 —— SLO vs MFU - 分离式架构设计集群调度策略、热点均衡开源计划</p><p></p><p>4. 未来展望 - 硬件能力展望</p><p>更细粒度的池化分离分离式内存系统</p><p></p><p>实践痛点</p><p>生产环境高负载下有效地过载线下测试与线上负载的解耦</p><p></p><p>演讲亮点</p><p>经过实际生产环境大规模验证的分离式推理系统，面对真实线上负载实现性能提升从实际业务出发，分析推理系统设计决定和关键技术</p><p></p><p>听众收益</p><p>了解分离式架构在实际生产环境中的挑战与发展趋势了解未来硬件/算法演进方向</p><p></p><p></p><h4>长文本 LLMs 推理优化：动态稀疏性算法的应用实践</h4><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/b7/b7f4aeb3515d23f88b8c432b73f6db5d.png" /></p><p></p><p>Long-context LLMs Inference的prefilling 阶段由于 Computation bottleneck 造成的长时延 (单卡 A100，1M 8B 约 30 分钟) 给 Long-context LLMs 的应用造成了困难。而 Attention 尤其是Long-context Attention 实际上是非常稀疏且动态的。利用这种动态稀疏性，我们将 Long-context Attention 存在的动态稀疏归纳成三种 Pattern，通过离线搜索出每个 Head 最优的稀疏Pattern，并利用很小的 overhead 在线确定动态稀疏 index，再结合动态稀疏编译器 PIT 和 Triton 进行高效的动态稀疏 GPU 运算，产生实际加速比。我们对市面上主流的 Long-context LLMs , like LLaMA-3-1M, GLM-4-1M, Yi-200K, Phi-3-128K, Qwen2-128K 在RULER，InfiniteBench，Needle Test，LM 等任务中进行了测试，结果显示其具有几乎相同的性能。</p><p></p><p>本次演讲将主要跟大家分享 LLMs 推理算法侧优化方法，包括量化，剪枝，模型架构优化，FFN 动态稀疏计算等方面的研究和实践。</p><p></p><p>演讲提纲</p><p></p><p>1. LLMs 推理算法侧优化方法讨论：如量化，剪枝，模型架构优化，FFN 动态稀疏计算等</p><p></p><p>2. 长文本 LLMs Inference 遇到的一些挑战</p><p>Attention 结构平方复杂度导致的 Prefilling 阶段较高的 TTFT解码阶段 KV cache 存储压力，计算要提供一个合理 TTFT 的 API 服务理论上需要对 Attention 进行多少倍加速</p><p></p><p>3. 研究思考</p><p>优化 Long-context LLMs Inference 的相关方法，包括 training from scratch 和 training-free 两大类方法。Attention 是动态稀疏的，Attention 的动态稀疏在空间上具有聚集性，呈现出三种不同的 pattern；</p><p></p><p>4. 解决方案</p><p>MInference、decoding 和多轮推理实现细节，包括 GPU Kernel 实现评测结果，包括有效性和高效性</p><p></p><p>5. 总结和未来展望</p><p></p><p>实践痛点</p><p>对于短文本场景，利用动态稀疏性可能会引入 overhead，获得的加速比较低</p><p></p><p>演讲亮点</p><p>LLMs 推理算法侧优化方法，包括量化，剪枝，模型架构优化，FFN 动态稀疏计算等方面首个有效降低长文本大模型推理中预填充阶段成本并保持性能的解决方案协同设计的算法和系统，能够在无需训练的情况下实现端到端加速</p><p></p><p>听众收益</p><p>了解算法侧优化 LLMs Inference 的思路和 Long-context LLMs inference 前沿研究动向和潜在的优化思路</p><p></p><p></p><h4>异构分布式大模型推理技术实践</h4><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/cc/cc05d5487b5c87dfa26479b74ec01a70.png" /></p><p></p><p>随着人工智能领域的发展，越来越复杂的大型语言模型正在被广泛应用于各个行业，这些模型的推理需求也随之大幅提升。鉴于国际供应链的持续不确定性，我们或将面临因依赖英伟达芯片而产生的潜在风险与挑战。为此，我们采用了英伟达和国产化芯片混合的异构分布式推理方案，该方案将充分发挥两种芯片的优势，确保系统的高效性和稳定性，同时减少对单一供应链的依赖，提升推理能力和自主控制能力。</p><p></p><p>推理优化已经不局限于算子层面，需要站在系统全局的角度分析并解决问题，需要设计者有全面的技术积累(分布式、算法、算子优化、量化)，需要站在异构大集群的背景下思考问题。本次演讲将分享商汤高性能计算与推理团队自研的异构分布式大模型推理系统遇到的挑战以及实现，希望能给大家带来一些帮助和思考。</p><p></p><p>演讲提纲</p><p>1. 异构分布式大模型推理系统优化</p><p>大模型推理已经演变成一项复杂的系统级别优化适配不同芯片的分布式异构推理系统模型快速加载，推理 POD 快速拉起</p><p></p><p>2. 多元算力芯片推理优化</p><p>推理芯片评测选型多元算力芯片深度推理优化</p><p></p><p>3. MOE 的推理优化</p><p>MOE 的兴起MOE 的推理优化方案MOE + MLA 的优势</p><p></p><p>4. 大规模异构推理集群的未来展望</p><p>更大规模的异构集群的管理调度高效的多模态融合推理</p><p></p><p>实践痛点</p><p>异构芯片之间的通信交互优化如何快速的进行多元算力芯片选型</p><p>演讲亮点</p><p>深入剖析多样化芯片适配优化方案MOE + MLA 的深度推理优化方案</p><p></p><p>听众收益</p><p>了解多元算力芯片技术发展趋势了解大模型推理系统的现状和演进方向</p><p></p><p>更多精彩内容，敬请关注 QCon 上海站，锁定「大模型基础设施与算力优化」专题，届时还会有小红书<a href="https://qcon.infoq.cn/2024/shanghai/presentation/6139">基于 PPO 的多模态大模型 RLHF 系统的设计与优化</a>"、华为<a href="https://qcon.infoq.cn/2024/shanghai/presentation/6135">昇腾万卡集群大模型性能提升实践</a>"等精彩内容。</p><p></p><p>会议推荐</p><p>AI 应用开发、大模型基础设施与算力优化、出海合规与大模型安全、云原生工程、演进式架构、线上可靠性、新技术浪潮下的大前端…… 不得不说，QCon 还是太全面了。现在报名可以享受 9 折优惠，详情请联系票务经理 17310043226 咨询。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c8/c87f0820b187f4ea98d5fe2bdce0f4c1.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/XqERUHzQXUGc91UGR4Kq</id>
            <title>一场演讲就能“值回票价”！来 QCon 听李云分享 AI 时代团队管理的变与不变</title>
            <link>https://www.infoq.cn/article/XqERUHzQXUGc91UGR4Kq</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/XqERUHzQXUGc91UGR4Kq</guid>
            <pubDate></pubDate>
            <updated>Fri, 13 Sep 2024 03:40:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>2015 年，时任阿里巴巴集团 UC 浏览器电脑版技术团队主管兼软件架构师的李云，在 QCon 北京站带来了《打造高质效的技术团队》的专题演讲，具体以淘宝浏览器团队过渡到 UC 浏览器团队为例，分享了技术团队如何在工作中以质效为导向确保可持续发展。</p><p></p><p>“这是一场值回票价的演讲。”现场听众对那次分享给出了高度的赞誉和反馈。</p><p></p><p>李云老师在当年提出，质效（质量和效率，效率包含效果）可以从技术与管理两个来源获得。技术层面体现于通过技术的方法去改善团队的工作效率和质量；管理层面则在于确保整个团队步调一致地工作和个体工作行为的稳定性。后者的根源在于持续改善工程师的工作习惯。</p><p></p><p>时隔近 10 年再回看这一观点，仍然具有十足的前沿性。李云老师于2022年离开了阿里并加了一家芯片行业的小公司做高管，担任高管期间进一步完善了2015演讲时所提出的团队效能动力模型，这一经历除了验证了他在大厂总结的管理经验对小公司也适用外，进一步引发了他将自己的经验体系性地分享出来的想法，于是有了今年9月正式面市的《全面效能》这本书。今年他创办了致效企业管理咨询（杭州）有限公司，希望通过陪跑式的咨询帮助更多的企业走出内卷与低效，从而实现员工与企业的双赢。</p><p></p><p>除此之外，李云老师在阿里期间还是 Service Mesh 的重要参与者与推动者，并于 2018、2019 年连续在 QCon 大会上分享了《Dubbo Mesh——Service Mesh 的本质、价值与应用探索》和《分布式应用的未来——Distributionless》两个话题。在今年 3 月，他的《工程师个人发展指南》专栏也在在极客时间上线了，感兴趣的小伙伴可以去搜索学习。</p><p></p><p></p><blockquote>查看下方链接回顾详细内容：分布式应用的未来——Distributionless：<a href="https://www.infoq.cn/article/Np1tzI4sssC-RbhhNI8R?utm_campaign%EF%BC%89">https://www.infoq.cn/article/Np1tzI4sssC-RbhhNI8R?utm_campaign）</a>"Dubbo Mesh——Service Mesh 的本质、价值与应用探索：<a href="https://www.infoq.cn/article/JqQvS3CImCSj7kEBR9Ge?utm_campaign">https://www.infoq.cn/article/JqQvS3CImCSj7kEBR9Ge?utm_campaign</a>"</blockquote><p></p><p></p><p>而在即将于 10 月 18−19 日召开的 QCon 上海站上，李云老师将再次回到 QCon 的舞台并带来主题为「AI 时代团队管理的变与不变」的演讲，分享在 AI 这个全新时代背景下，个人与团队、业务与技术整合的体系化技术管理之路。</p><p></p><p><img src="https://static001.geekbang.org/infoq/32/3279465c5c4f2d2e2ff31a3383c60978.webp" /></p><p></p><p>值得关注的是，届时在大会现场，李云老师还将携全新著作《全面效能》一书进行签售。感兴趣的小伙伴们可扫描上方二维码或点击链接咨询购票直达现场与李云老师面对面深度交流：<a href="https://qcon.infoq.cn/2024/shanghai/apply">https://qcon.infoq.cn/2024/shanghai/apply</a>"</p><p></p><p>与此同时，在「与时俱进的团队管理」 专题下，目前已确认阿里巴巴技术总监许晓斌和 Paypal 资深经理李清玉还将带来主题为《负责任的技术规划 —— 不仅仅是技术》和 《如何提升个人领导力以适应技术管理者的角色？》 的分享。欢迎扫描下方二维码添加小助手关注更多大会内容和进展，我们将竭诚为大家准备更多 「值回票价」的干货演讲！</p><p></p><p><img src="https://static001.geekbang.org/infoq/68/68a4f559d6682dec46bd5633588299f0.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/slYNs7YIDmD6bIJK3WF7</id>
            <title>技术与艺术的碰撞！这次组委会居然CNCC把带到了横店</title>
            <link>https://www.infoq.cn/article/slYNs7YIDmD6bIJK3WF7</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/slYNs7YIDmD6bIJK3WF7</guid>
            <pubDate></pubDate>
            <updated>Thu, 12 Sep 2024 10:22:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>2024年9月10日，北京迎来了计算机科学与技术领域的一场盛事——2024中国计算机大会（CNCC2024）新闻发布会的隆重举行。人民日报、中国青年报、中国科学报、南方都市报、中国工业报、中国经营报、人民网、新华网、科学网、搜狐科技、网易科技、钛媒体、爱奇艺、智东西、InfoQ等多家媒体和合作机构出席发布会。</p><p></p><p>本次大会以“发展新质生产力，计算引领未来”为主题，汇聚了国内外顶尖学者、企业家及行业精英，共同探讨技术发展的新趋势，以及技术普惠化对于社会的深远影响。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5b/5b83fc6d0cce10471b4a7c633c3ffa34.png" /></p><p></p><p>CNCC2024指导委员会主席、CCF理事长，中国工程院院士，中国科学院计算所研究员孙凝晖；程序委员会主席，北京大学教授胡振江；论坛委员会主席、CCF常务理事，中国人民大学教授文继荣；组织委员会副主席，东阳市委常委、横店镇党委书记胡利群；组织委员会共同主席，横店集团党委书记、董事、副总裁吕跃龙等嘉宾出席发布会。发布会由CNCC2024大会总监、CCF秘书长唐卫清主持。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b30698928aace17ad743ac3285a6688d.png" /></p><p></p><p>（CNCC2024大会总监、CCF秘书长唐卫清主持发布会）</p><p></p><p>发布会现场，CNCC2024指导委员会主席、CCF理事长孙凝晖院士详细介绍了大会的筹备情况及特色亮点。他特别指出，本次大会首次将学术会议选址于县级市横店，不仅是对横店经济社会发展的高度认可，更是对当地产业数字化水平和创新活力的充分肯定。</p><p></p><p><img src="https://static001.geekbang.org/infoq/95/95700f970541ae0b6b65cb6d0bbb3eb8.png" /></p><p></p><p>（指导委员会主席、CCF理事长，中国工程院院士，中国科学院计算所研究员孙凝晖）</p><p></p><p>孙凝晖院士强调，技术发展的最终目的是普惠社会，让科技成果惠及更广泛的人群。他提到，横店作为全国知名的影视文化名城，其独特的产业背景和文化氛围为本次大会增添了别样的色彩。而大会的举办，也将进一步推动横店乃至整个地区的科技创新和产业升级，为地方经济社会发展注入新的活力。</p><p></p><p>CNCC 2024委员会主席、北京大学教授胡振江在发布会上详细介绍了大会的特邀报告及论坛组织情况。他透露，本次大会特邀报告讲者包括图灵奖获得者、中国科学院外籍院士、美国康奈尔大学名誉教授、北京大学客座讲席教授John Hopcroft；德国达姆施塔特工业大学教授Jan Peters；美国加州大学伯克利分校教授Michael I. Jordan；CCF会士、中国科学院外籍院士、新加坡国立大学教授黄铭钧；CCF会士、前理事长，中国科学院院士，北京大学教授梅宏；CCF会士、中国科学院院士、中国科学院软件研究所研究员林惠民；CCF会士、中国科学院院士、中国科学院软件研究所研究员冯登国；CCF会士、中国科学院院士、北京航空航天大学教授郑志明；中国工程院院士、国家数字交换系统工程技术研究中心主任、复旦大学教授邬江兴；上海人工智能实验室主任、首席科学家、清华大学惠妍讲席教授周伯文；CCF会士、联想集团高级副总裁、欧洲科学院外籍院士芮勇；CCF常务理事、阿里云副总裁刘湘雯；中国电信首席科学家、云计算研究院院长、美国天普大学讲席教授吴杰；百川智能创始人、CEO王小川；蚂蚁集团智能引擎技术事业部副总裁周俊，更多特邀报告讲者还在邀请中。同时，大会还设置了多个专题论坛，涵盖智慧城市、量子计算、存储系统软件等多个领域，旨在为参会者提供一个全面、深入的交流平台。</p><p></p><p>值得一提的是，本次大会还特别注重技术普惠化的理念。在论坛组织上，大会鼓励企业组织或参与论坛，推动产学研深度融合，加速科技成果的转化和应用。此外，大会还通过线上直播、互动问答等多种形式，让无法亲临现场的观众也能感受到科技的魅力，共享技术发展的成果。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fc/fc34cd6a87e46a24060bbb734e765cf2.png" /></p><p></p><p>（CNCC2024程序委员会主席，北京大学教授胡振江）</p><p></p><p>CNCC2024论坛委员会主席、CCF常务理事，中国人民大学教授文继荣介绍了专题论坛的组织进展。今年的专题论坛围绕大会主题“发展新质生产力，计算引领未来”进行组织，覆盖面更宽，体现行业化、企业化、会员治会三大特色，鼓励企业组织或参与论坛。从征集的223场论坛中选出138场，论坛领域方向34个，将有超过800位国内外专家学者、企业技术精英应邀担任讲者在大会上交流，数量、质量、覆盖面均再创新高。</p><p></p><p><img src="https://static001.geekbang.org/infoq/89/899faa1e9d23f748ae77d9a9b4441a78.png" /></p><p></p><p>（CNCC2024论坛委员会主席、CCF常务理事，中国人民大学教授文继荣）</p><p></p><p>东阳市委常委、横店镇党委书记胡利群在发言中介绍了东阳市的筹备工作进展。他表示，CNCC首次落户县级市横店，是对东阳经济社会发展的巨大鼓舞和鞭策。东阳市将全力以赴做好大会的各项筹备工作，确保大会圆满成功。同时，他也期待通过本次大会的举办，进一步推动横店乃至整个地区的科技创新和产业升级，为地方经济社会发展注入新的动力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/03/034cf9e9a757f0b910b0a928a8328c8d.png" /></p><p></p><p>（CNCC2024组织委员会副主席，东阳市委常委、横店镇党委书记胡利群）</p><p></p><p>在答记者问环节，与会嘉宾就横店的会议组织、大模型发展、信息产业发展等话题进行了深入交流。他们一致认为，技术的发展离不开社会各界的共同努力和支持，而技术普惠化则是技术发展的最终目标和归宿。只有让科技成果真正惠及广大民众，才能推动社会的全面进步和发展。</p><p></p><p>随着CNCC2024新闻发布会的圆满落幕，一场盛大的学术盛宴即将拉开帷幕。我们期待在大会的舞台上，见证更多科技创新的火花碰撞出璀璨的光芒，共同推动计算机科学领域的前沿探索与新质生产力的发展。同时，我们也期待技术普惠化的理念能够深入人心，让科技的光芒照亮每一个角落，为社会的全面进步和发展贡献更多的智慧和力量。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/QiWT133yOvuVQOKsgNmL</id>
            <title>车企掀起“造芯潮”后，软硬一体的规模量产变智驾竞争关键：出货低于100万即面临投产失衡</title>
            <link>https://www.infoq.cn/article/QiWT133yOvuVQOKsgNmL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/QiWT133yOvuVQOKsgNmL</guid>
            <pubDate></pubDate>
            <updated>Thu, 12 Sep 2024 02:24:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>近几年，特斯拉从采用供应商的“重软硬一体化”方案，过渡到自研算法结合第三方芯片的“轻软硬一体化”方案，最终实现了自研芯片的“重软硬一体化”。如今，国内的蔚来、理想、小鹏等公司也遵循了类似的路径。“蔚小理”推出的旗舰车型大多基于Orin芯片平台，并都已宣布了芯片流片的进展，预计将在未来一到两年内搭载上车。</p><p>&nbsp;</p><p>随着智驾技术的不断进步，自动驾驶行业软硬一体的趋势愈加明显，软硬一体化系统的大规模量产能力也逐渐成为高阶智驾竞争的胜负手。</p><p>&nbsp;</p><p>9月5日，在辰韬资本联合主办的“2024自动驾驶软硬协同发展论坛暨报告发布会”上，近200位产业专家、投资机构、研究机构及智能驾驶头部企业的代表探讨了软硬一体产品设计模式对于自动驾驶行业带来的挑战和机遇。&nbsp;</p><p>&nbsp;</p><p>会上，辰韬资本、南京大学上海校友会自动驾驶分会、九章智驾三方联合发布2024年度《自动驾驶软硬一体演进趋势研究报告》，该报告从软硬一体的定义及行业现状，软硬一体的底层原因、软硬一体的开发能力分析、软硬一体代表公司、软硬一体发展驱动力及软硬一体未来发展趋势等多个方面对软硬一体这一产品设计模式进行深入分析。</p><p>&nbsp;</p><p></p><h1>成本驱动，软硬—体模式发生转变</h1><p></p><p>&nbsp;</p><p>尽管“软硬一体”已经成了行业内很多领先玩家的重要战略，但是目前仍没有对软硬一体给出有效的讨论范畴的定义。该报告指出，软硬—体硬件核心是自动驾驶的高性能计算芯片，并将软硬一体的讨论限定在对于自动驾驶行业生态有重大影响的核心对象：包括自动驾驶软件Tier1、自动驾驶芯片厂商以及主机厂，描述的是公司具备的软硬件协同的研发能力和开发模式，并能够提供软硬一体的产品。</p><p>&nbsp;</p><p>与此同时，该报告对软硬一体的形态进行了界定，并将软硬一体两种典型的形态分为“重软硬一体”和“轻软硬一体”。目前，行业内有三种不同的分工模式和开发模式。</p><p>&nbsp;</p><p>“重软硬一体”指由同一个公司完成芯片、算法、操作系统/中间件的全栈开发，基于此衍生出生态合作模式，这种模式包括海外的 Mobileye、特斯拉、Nvidia（开发中） 以及国内的华为、地平线、Momenta（开发中）等。“轻软硬一体”指自动驾驶解决方案公司采用第三方芯片，在某款特定芯片上具备极致的优化能力和丰富的产品化交付经验，能够最大化发挥该款芯片的潜能，这方面的典型案例包括卓驭（大疆）、Momenta 等。从“重软硬一体”模式衍生出的轻量化模式，将软硬耦合程度最深的AI算法和SoC芯片做深度绑定，作为标品向客户提供，其他软件模块和硬件模块由生态合作伙伴来提供。Mobileye、地平线、特斯拉在最早期采用的就是这样的开发模式。</p><p>&nbsp;</p><p>辰韬资本执行总经理刘煜冬表示，现在自动驾驶芯片并没有达到算力非常富裕的阶段，不存在绝对的软硬分离公司或者开发模式。软硬解耦更多展现的是过程，指的是从“重软硬一体”向“轻软硬一体”转变的过程。而造成软硬—体发展变化的一大原因是成本驱动，主要有两个方面。</p><p>&nbsp;</p><p>第一，不管由软件公司做定制芯片，还是做深度绑定某一颗芯片优化，可以最大化发挥芯片能力，避免很多平台芯片设计带来浪费；第二，由自动驾驶公司或者主机厂自研芯片，可以显著降低单芯片成本。只要有足够芯片出货量，就可以从公司层面降低整体开销。</p><p>&nbsp;</p><p></p><h1>不同赛道企业的软硬一体策略</h1><p></p><p>&nbsp;</p><p>对于芯片公司来讲，执行软硬一体策略必须前瞻性地关注算法的演进趋势，甚至投入较多资源和精力去搭建自研算法团队和解决算法团队。</p><p>&nbsp;</p><p>对于软件Tier1公司来说，采用平台型芯片和采用专用型芯片去应对软硬一体策略会有一些不同。采用平台型芯片，软件Tier1公司需考虑的点与芯片公司比较类似，要对算法和芯片技术进展保持比较激进的前瞻性关注。另外，软件Tier1选择一颗通用芯片的时候，需要在硬件层面留出较多余量。如果采用专用型芯片，能够节省更多资源，提高性价比，所以有更高收益。但对软件Tier1来说，采用这种策略对芯片理解、芯片应用能力的要求会更高。</p><p>&nbsp;</p><p>针对软硬一体策略，整车厂考虑的维度会更加复杂。要不要做算法自研，这是整车厂长期讨论的问题，这主要取决于公司定位。对于大部分新势力来讲，它们本身定位就是自动驾驶科技公司，做算法自研是自然而然的选择；对于大部分传统OEM来说，其更加强调全栈可控，也不一定能够支撑维持大规模算法资源团队，所以倾向于从供应商的白盒合作和内部团队自研两方面去推进。</p><p>&nbsp;</p><p>从软硬一体策略上来讲，整车厂的选择针对不同配置也有差异化。一般来说，针对低阶智驾配置更倾向于采用供应商软硬一体方案，比如Mobileye IQ4、地平线的J2、J3、J6E都能够交付成熟的“重软硬一体”方案；对于高阶自驾配置来说，这通常是主机厂差异化竞争力的来源，所以整车厂会倾向于采用自研算法+平台型的第三方芯片“轻软硬一体”策略，或更进一步自研芯片做“重软硬一体”方案。</p><p>&nbsp;</p><p>而在芯片问题上，对于整车厂来说更多是经济性的考量。报告认为，自研芯片出货量低于100万片就可能很难投入产出比平衡。另外，整车厂自研芯片后，库存控制也会变成需要重点考虑的问题。</p><p>&nbsp;</p><p>尽管软硬一体的方案能够为企业带来成本上的巨大优势以及更广阔的生存空间，但也对于企业在技术能力上提出了更为苛刻的要求——执行软硬一体战略的企业必须在算法、芯片（重软硬一体）以及中间件和底软等领域有着深度的技术积累和工程经验。因此，该报告对软硬一体所必须具备的开发能力如智驾系统算法架构、智驾芯片设计能力、智驾系统底层软件等维度结合不同类型的企业案例进行深入分析。</p><p>&nbsp;</p><p>基于对近30位行业资深专家的访谈以及对历史上其他行业的情况进行横向对比，报告总结了影响软硬一体策略判定的三个要素：技术成熟度、技术平权度及总收益。当满足其中一条时公司就具备考虑软硬一体的条件，满足其中两条时公司就会具有推动软硬一体的动力，如果三条全部满足则软硬一体就是公司在当前的最优选择策略。</p><p>&nbsp;</p><p>此外，报告对自动驾驶赛道软硬件一体不同类型的参与企业，如主流芯片厂商英伟达、华为、地平线、高通；主机厂特斯拉、理想、蔚来、小鹏、比亚迪；软件Tier1如Momenta、卓驭科技（大疆车载）等不同类型的代表性公司当前的软硬一体策略、背后的原因、进展情况、未来趋势等进行了多维度分析。</p><p>&nbsp;</p><p></p><h1>软硬一体未来发展趋势</h1><p></p><p>刘煜冬指出，整个行业走向更加深度软硬—体集成有三个条件。第一，算法技术框架有阶段性收敛。第二，不管是整车厂还是自动驾驶公司，做这件事情难度已经降低很多，得益于半导体行业中的芯片设计、IP、工具链等都已经有比较成熟的生态环节。从芯片设计角度来说，技术有了平权。第三，选择做深度重软一体方案公司，到底能不能有足够出货量覆盖投入产出。</p><p>&nbsp;</p><p>对于软硬一体未来发展趋势，该报告认为，总体来看，软硬一体与软硬解耦是一体两面，最终市场会形成两者并存的态势，但是短期内，软硬一体的公司在市场上体现出更强的竞争力。</p><p>&nbsp;</p><p>在自动驾驶行业，软硬一体的趋势会根据自动驾驶方案的高低阶而有所不同：对低阶智驾，主机厂往往会直接采用供应商的软硬一体方案，并向标准化的方向发展；对高阶智驾算法等关键能力，主机厂自研的比例会越来越高；当芯片算力远大于实际应用的需求、解决方案与芯片算力的适配不再成为核心能力时，行业就具备了达到软硬解耦的必要条件。</p><p>&nbsp;</p><p>不过由于当前算法仍在快速迭代，对算力的需求仍处于激增状态。目前仍然是芯片算力配合算法需求进行不断提升，所以在很长一段时间内，软硬一体策略仍然会是行业主流。</p><p>&nbsp;</p><p>对芯片公司来说，需要持续不断做更多软件方面的投入来构建自己的“护城河”，工具链层面也需要打造更完整的生态。对整车厂来说，如果自研芯片能够实现比较好的投入产出比，整车厂也会有比较强的动力和技术实力做自研芯片。</p><p>&nbsp;</p><p>对于软件Tier1来说，由于大部分整车厂和终端客户选择智驾系统的时候，它的T0级决策是选芯片，并不会因为软件公司到底用哪块芯片去选。所以更好的策略是深度适配更多的芯片。如果有足够的实力自研芯片，需要更好抓住技术变化“窗口期”来发挥软件公司在需求引领和迭代方面的优势。</p><p>&nbsp;</p><p></p><h1>新技术带来的影响</h1><p></p><p>端到端</p><p>&nbsp;</p><p>今年业内在端到端布局非常多，并且都认为端到端会对芯片计算需求越来越高。对于通用型芯片公司来说，应对端到端的挑战需要做计算能力更强的芯片。同时，现在端到端用到的基本神经网络架构还没有收敛。之前都做Transformer，未来像Mamba以及RWKV等这类新的神经网络架构也在出现，专用芯片针对这样的算子去做前瞻性的布局也会获取一定的优势。</p><p>&nbsp;</p><p>舱驾一体</p><p>&nbsp;</p><p>实际上，现在市场上真正能够实现单芯片单板舱驾一体的芯片选择并不多，比较有代表性的就是英伟达和高通两款芯片。早期舱驾一体更多会采用“轻软硬一体”方案，由不同解决方案公司搭载在少数平台芯片上。</p><p>&nbsp;</p><p>具身智能</p><p>&nbsp;</p><p>具身智能可能是智能手机、智能自动驾驶后新的发展趋势，现在也比较像智能驾驶行业早期发展情况。早期，低阶任务机器人可能会采用比较简单“重软硬一体”解决方案。随着生态的发展，未来会有更多不同分工以及开发模式的选择。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Hh9SEWrDlo7phePDw0I8</id>
            <title>MiniMax 视频生成模型首秀！闫俊杰：大模型的研发核心是“快”</title>
            <link>https://www.infoq.cn/article/Hh9SEWrDlo7phePDw0I8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Hh9SEWrDlo7phePDw0I8</guid>
            <pubDate></pubDate>
            <updated>Wed, 11 Sep 2024 10:34:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p></p><p></p><p>上面是 MiniMax 最新推出的视频模型 video-01 生成的效果。“这只是我们的第一版，很快还会有更新的版本。” MiniMax 创始人闫俊杰说道。</p><p></p><p>在 MiniMax 内部，多模态已经是一件非常确定的事情了。</p><p></p><p>“在人类社会，大模型的核心意义是做更好的信息处理，而大部分的信息体现在多模态内容里，而非文字上，文字很多时候只是其中精华的一小部分。”闫俊杰解释道。</p><p></p><p>“为了有非常高的用户覆盖度和使用深度，唯一的办法就是能够输出动态的内容，而非只输出单纯的文字内容，这是一个非常核心的判断。”用户的渗透率和使用深度是闫俊杰这次创业非常关注的事情。在他看来，这两点是达成“Intelligence with Everyone”的核心，也是 MiniMax 的差异化能力。</p><p></p><p>用户方面，MiniMax 已经有了不错的成绩。据统计，MiniMax 每日与全球用户进行超 30 亿次交互，处理超 3 万亿文本 token、2000 万张图片和 7 万小时语音，大模型日处理交互量排名国内 AI 公司首位。</p><p></p><p>但在视频生成赛道，MiniMax 的发布算不上早。闫俊杰对此的解释是，“我们在解决一个更难的技术问题：如何能够原生地训练算力比较高的东西。”</p><p></p><p>具体来说，首先，训练视频生成能力时也需要先把视频变成一些 token，视频变成的 token 非常长，越长复杂度就越高，MiniMax 团队要做的就是在算法上把复杂度降低、压缩率变得更高。</p><p></p><p>其次，视频还很大，比如 5 秒的视频有几兆，而 5 秒看到的文字可能不到 1K，这是千倍的存储差距。因此，之前基于文本模型的基础设施，对视频模型来说是不适用的，这意味着要对基础设施进行升级。</p><p></p><p>“一两周新的东西出来，并达到我们更加满意的状态后，可能会考虑商业化。”闫俊杰表示。</p><p></p><p></p><h3>“能带来数倍提升的技术才值得投入研发”</h3><p></p><p></p><p>视频生成模型的研发更让闫俊杰坚定了一件事：无论是视频、文本还是声音，核心都不是让一个算法带来 5%、10% 的提升，重要的是找到提升数倍的方式，如果能够提升数倍就一定要做出来，如果只提升 5% 就不太值得做。</p><p></p><p>“从读书、工作，到现在创业，我对技术的理解慢慢变得非常简单，就是第一性原理。技术，特别是有很大研发投入的技术，追求的不应该是 10% 的提升，如果一个技术的提升只有 10%，那这个技术就不应该做，原因是你不做也会有人做或有人开源出来，其实根本不需要自己研发。”闫俊杰对 InfoQ 表示。</p><p></p><p>“对创业来说，一块钱掰成几份来花是非常难的。像我们这样的创业公司，真正应该花钱做的研发是那种能够带来几倍变化的技术，这种东西很多时候如果我们自己不做，外面也没有，但对满足用户的需求又很重要，只能自己来做，这样的才是核心的东西。”闫俊杰说道。</p><p></p><p>那么，MiniMax 做大模型的核心是什么？</p><p></p><p>闫俊杰的答案是：快 = 好。</p><p></p><p>在率先判断出 MoE 技术路线后，MiniMax 又推出基于 MoE+ Linear Attention 的新一代模型技术。通过此新型线性模型架构，MiniMax 大模型能在单位时间内更加高效地训练海量数据，极大地提升了模型的实用性和响应速度。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/21/216ddd9f50c795c660c5daed2c7c041e.png" /></p><p></p><p>MiniMax 与GPT-4o 同一代模型能力进行对比发现，新一代模型处理 10 万 token 时效率可提升 2-3 倍，并且随着长度越长，提升越明显。相比于通用 Transformer 架构，在 128K 的序列长度下，新架构成本减少 90% 以上。</p><p></p><p>“不管是做 MoE、Linear attention 还是其他的，本质上是让同样的效果模型变得更快，快才意味着同样的算力可以做得更好，这是我们最底层的研发思路。”闫俊杰说道。</p><p></p><p>“从实际应用上，就像我们肯定不希望星野的 NPC 只能记住最近 8000 字的内容，这对用户的体验损伤比较大，如果能 Scale 到 8 万字、80 万字、800 万肯定能做出更不一样的产品。” MiniMax 技术总监韩景涛补充道。</p><p></p><p></p><h3>“产品不赚钱是技术不够好”</h3><p></p><p></p><p>目前，MiniMax 在国内 C 端的主打产品是星野和海螺 AI。</p><p></p><p>“当一个产品没人用或者不赚钱的时候，肯定不能怪用户，大部分时候只能怪自己的技术做得不够好，或者产品做得不够好。”闫俊杰说道。</p><p></p><p>因此，在闫俊杰看来，像基于 GPT-4 的 GPT Store 跑不通的根本原因，不是因为 Agent 的框架写得不够好，是因为模型本身不够好。“当前的模型没有很长的记忆、理解不了特别复杂的指令就会这样。”</p><p></p><p>现在所有的模型错误率都是 20% 的量级，闫俊杰认为，真正发生变革的是有一个模型可以把错误率降低到个位数，这会让很多复杂的任务从“不可以”变得“可以”。</p><p></p><p>“当技术做得不好的时候，所有东西都是问题，当技术做好了，似乎所有问题都被掩盖了。技术是一家科技公司的最核心的要素，我觉得我花了两年才意识到这件事。”闫俊杰说道。</p><p></p><p>在闫俊杰看来，做技术是一件非常奢侈的事，这件事甚至只有创业的时候才会理解，因为做技术，可能会失败、投入也很大。当一个东西很奢侈时，很多时候就会想要不要走点捷径，比如不做技术，先把产品提升好等。</p><p></p><p>“实践经验证明，走捷径的时候会被打脸。”闫俊杰笑道。</p><p></p><p>目前，MiniMax 的商业化基本上分成两种模式：一是面向企业的开放平台，现在已经有两千多家的客户，包括互联网公司、传统企业等；二是在自有产品里设立广告机制进行变现。</p><p></p><p>“现阶段，最重要的还不是商业化，是真正地对技术到达广泛可用的程度。”闫俊杰表示。</p><p></p><p>对于国内市场，MiniMax 希望打造偏工具类的产品，比如会给海螺 AI 不断打磨出新的功能，直到产生了很强的用户粘性。“粘性构造起来后，我们才会考虑 ROI 和 Retention。这个飞轮转起来了，我们才会进行投放。”MiniMax 国际业务总经理盛静远表示。</p><p></p><p>盛静远认为，这个 ROI 会有转起来的一天，但不是今天的产品形态。“作为一个普通消费者，今天的产品形态没有任何的忠诚度可言。它一收费我就可以换到另外一个产品，这个模式是不成立的。”</p><p></p><p>但海外市场不太一样。海外企业更愿意付费，因此把技术做得细腻很重要。“对我们来讲现在技术完全到位了，更多是公司的精力和资源，以及怎么变现的问题。海外市场有一套自己的打法，会相对地比较 straightforward，变现也更快。”</p><p></p><p>实际上，MiniMax 海外产品 Talkie 名气可能比国内产品更高。在全球知名风投机构 a16z 最新发布的《Top100 消费级生成式 AI 应用》移动应用榜单中，Talkie 位列 22 位。</p><p></p><p>盛静远总结道，任何伟大的 2 C 产品都是基于人性的深入思考，另外则要考虑 AI 在高容错率的情况下可以做什么，并变成大众喜闻乐见的产品。</p><p></p><p></p><h3>结束语</h3><p></p><p></p><p>大模型领域的竞争依然在继续。闫俊杰表现得比较淡然，“这就是一个发展的客观规律，作为一家创业公司，如果我们在竞争中打不赢，那我们就应该被淘汰，其实也没有其他的选择。”</p><p></p><p>在与大厂的竞争中，闫俊杰认为，要赢就要更快地看清非常底层的东西，“大公司开始跟你竞争时，就会意识到有些东西是没用的，因为那些东西大厂能做得比你强千百倍。我们能做的就是无限放大能让我们变强的事情：一是提升技术；二是跟用户共创，这两点非常关键的判断是需要长期积累的。”</p><p></p><p>而对于国内的大模型价格战，闫俊杰认为确实非常大地提高了模型的调用量，本来认为大模型很贵的公司，包括很多传统的企业开始愿意使用大模型，因为成本低对出错的容忍度也会高一些。“正是激烈的竞争，推动了大家必须得把模型做好。一定阶段之后，大家会发现自己的模型在海外也有竞争力，比如东南亚等，至少目前已经在非英语国家的语种上跟 GPT 不相上下。”</p><p></p><p>“我们看到乐观的一面，国内大模型的使用量确实在显著地增长，并且中国的模型在海外确实越来越具有竞争力，我觉得这是两个积极的变化。”闫俊杰说道。</p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/OBY1si0QCkbxXbblcBxl</id>
            <title>AI 推理竞赛正在升温</title>
            <link>https://www.infoq.cn/article/OBY1si0QCkbxXbblcBxl</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/OBY1si0QCkbxXbblcBxl</guid>
            <pubDate></pubDate>
            <updated>Wed, 11 Sep 2024 09:46:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>虽然英伟达的 GPU 在 AI 训练领域的主导地位仍然难以撼动，但似乎有迹象表明，在 AI 推理方面，竞争对手正在迎头赶上这家科技巨头，尤其是在能效方面。然而，英伟达新推出的 Blackwell 芯片的卓越性能可能很难被超越。</p><p></p><p>最近，ML Commons 发布了最新的 AI 推理竞赛 ML Perf Inference v4.1 的成绩单。这一轮竞赛包括使用 AMD Instinct 加速器的团队、最新的谷歌 Trillium 加速器、来自多伦多初创公司 UntetherAI 的芯片以及英伟达最新发布的 Blackwell 芯片的首次试水。另外两家公司，Cerebras 和 FuriosaAI，也发布了最新的推理芯片，虽然没有提交给 MLPerf 进行评测。</p><p></p><p>就像奥林匹克运动会一样，MLPerf 也有许多类别和子类别。提交数量最多的是“封闭数据中心”类别。封闭类别（相对于开放类别）要求提交者在不进行重大软件修改的情况下按照原样运行推理任务。数据中心类别评估的是批量处理查询的能力，而边缘类别则侧重于降低延迟。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/0b/0ba10c0ee80fb26fa124cd1ba643fe9b.png" /></p><p></p><p>每个类别有 9 个不同的基准测试，针对不同类型的 AI 任务，包括一些流行的应用场景，如图像生成（例如 Midjourney）和 LLM 问答（例如 ChatGPT），以及同样关键但可能不那么引人注目的任务，比如图像分类、目标识别和推荐引擎。</p><p></p><p>本轮竞赛新增了一个叫作 Mixture of Experts 的基准测试。这是 LLM 部署方面的一个日益流行的趋势：一个语言模型被分解为几个较小的、独立的模型，每个子模型都针对特定任务进行微调，如常规对话、解决数学问题和协助编码。模型能够将每个查询定向到适当的子模型（或者叫“专家”模型）。这种方法使得每个查询使用更少的资源，从而降低成本并提升吞吐量。</p><p></p><p>在备受瞩目的封闭数据中心基准测试中，获胜者仍然是基于英伟达 H200 GPU 和 GH200 超级芯片（封装了 GPU 和 CPU）的参赛者。然而，如果深入分析性能数据，我们会发现情况远比表面看起来的复杂。一些参赛者部署了大量加速器芯片，而另一些则只使用了一片。如果我们将每个参赛者每秒处理的查询数量按使用的加速器数量进行标准化，并仅考虑每种加速器类型的最佳性能，一些有趣的细节便会浮出水面。（需要注意的是，这种分析方法并未考虑 CPU 和互连对性能的影响。）</p><p></p><p>以单个加速器为前提，英伟达的 Blackwell 芯片在其参与的唯一基准测试——LLM 问答任务中，性能比所有之前的芯片高出 2.5 倍。Untether AI 的 speedAI240 预览芯片在它参与的唯一任务——图像识别中，性能几乎与 H200 持平。谷歌的 Trillium 在图像生成任务上的性能大约是 H100 和 H200 的一半，而 AMD 的 Instinct 在 LLM 问答任务上的性能与 H100 大致相当。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/2c/2c575fb50d1697d88c8d9dd23f723fa4.png" /></p><p></p><p></p><h3>强大的 Blackwell</h3><p></p><p></p><p>英伟达 Blackwell 芯片取得成功的一个关键因素是它能够使用 4 位浮点精度运行 LLM。英伟达及其竞争对手一直在努力减少用于表示数据的位数，以此来提升计算速度。英伟达在 H100 中引入了 8 位数，而此次参赛在基准测试中首次展示了其 4 位数的运算能力。</p><p></p><p>英伟达产品营销总监 Dave Salvator 指出，使用低精度数字位的最大挑战在于保持模型的准确性。为了满足 MLPerf 评测所需的高精度标准，英伟达团队不得不在软件层面进行重大创新，他补充道。</p><p></p><p>Blackwell 芯片成功的另一个关键因素是其内存带宽的显著提升，达到了每秒 8 兆字节，几乎是 H200 芯片每秒 4.8 兆字节带宽的两倍。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/09/09474cdb0a6a2266526f1abfc01e1ba3.png" /></p><p></p><p>英伟达 GB2800 Grace Blackwell 超级芯片</p><p></p><p>Blackwell 芯片虽然在竞赛中仅使用了单个芯片，但 Salvator 指出，该芯片是为了实现联网和伸缩性而设计的，在与英伟达的 NVLink 互连技术配合使用时将发挥最大效能。Blackwell GPU 支持多达 18 个 NVLink 连接，每个连接的速率为每秒 100 千兆字节，总带宽达到每秒 1.8 兆字节，大约是 H100 互连带宽的两倍。</p><p></p><p>Salvator 认为，随着大型语言模型的不断扩展，推理任务也将需要多 GPU 平台来满足日益增长的需求，而 Blackwell 芯片正是为了应对这一趋势而设计。Salvator 强调，“Blackwell 不仅仅是一个芯片，它还是一个平台”。</p><p></p><p>英伟达基于 Blackwell 芯片的基础系统参与了 MLPerf 的预览子类别，这表明该芯片尚未对外销售，但预计将在未来六个月内，即下一次 MLPerf 评测发布之前上市。</p><p></p><p></p><h3>Untether AI 在功耗和边缘计算方面表现出色</h3><p></p><p></p><p>对于 MLPerf 的每一项基准测试，都有相应的能源效率测试，以系统性地评估各系统在执行任务时的功耗。封闭数据中心能源类别只有 Nvidia 和 Untether AI 两家提交了测试结果。Nvidia 参与了所有基准测试，但 Untether AI 只参与图像识别环节。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/be/be59f72444df009420fdb2997b239d92.png" /></p><p></p><p>Untether AI 通过所谓的“内存内计算”实现了卓越的能效。Untether AI 的芯片设计为由内存元素构成的网格，每个小处理器紧邻其旁。处理器采用并行处理方式，与邻近内存单元格中的数据同步工作，显著减少了模型数据在内存与计算核心间传输所需的时间和资源。</p><p></p><p>Untether AI 产品副总裁 Robert Beachler 表示：“我们发现，在 AI 工作负载中，大约 90% 的能耗仅用于将数据从 DRAM 传输到缓存，再传输到处理单元。因此，我们采取了相反的策略……不是将数据移至计算单元，而是将计算单元移到数据所在的地方。”</p><p></p><p>这种创新方法在 MLPerf 的“封闭边缘”子类别中取得了显著成效。这个类别专注于更贴近实际的应用场景，如工厂内的机器检查、引导视觉机器人和自动驾驶汽车等——Beachler 指出，在这些应用中，低能耗和快速处理至关重要。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/53/53e93b1d2c945a21c46053fd2f321b27.png" /></p><p></p><p>在图像识别任务中，Untether AI 仍然是唯一提供评测结果的公司，它的 speedAI240 预览芯片在延迟性能方面是 NVIDIA L40S 的 2.8 倍，吞吐量（每秒处理的样本数）提升了 1.6 倍。这家初创公司还提交了功耗数据，但因为 Nvidia 没有提供相应的数据，因此很难进行直接比较。不过，Untether AI 的 speedAI240 预览芯片每个芯片的标称功耗为 150 瓦，而 Nvidia 的 L40s 为 350 瓦，这意味着在延迟性能提升的同时，功耗名义上降低了 2.3 倍。</p><p></p><p></p><h3>Cerebras、Furiosa 没有参与MLPerf 竞赛，但发布了新的芯片</h3><p></p><p></p><p>Furiosa 的新芯片采用了一种独特且高效的手段来实现 AI 推理中的基本数学运算——矩阵乘法。</p><p></p><p>在近期斯坦福大学举办的 IEEE Hot Chips 大会上，Cerebras 公司推出了自己的推理服务。这家位于加州 Sunnyvale 的公司专注于制造大型芯片，利用尽可能大的硅片来避免芯片间的互连问题，并显著提升设备的内存带宽。这些设备主要用于训练大型神经网络。现在，Cerebras 已经升级了其软件栈，用于其最新的计算机 CS3 执行推理任务。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/33/33b7a1dcdf5b1be102aba01e1ab46431.png" /></p><p></p><p>Furiosa 的新芯片以一种不同的、更有效的方式实现了 AI 推理最基本的矩阵乘法。</p><p></p><p>尽管 Cerebras 尚未参与 MLPerf 的评测，但该公司宣称其平台在每秒生成的 Token 数量比 Nvidia 的 H100 高出 7 倍，比竞争对手 AI 初创公司 Groq 的芯片高出 2 倍。Cerebras 首席执行官兼联合创始人 Andrew Feldman 表示：“我们正处在通用人工智能的拨号上网时代。这是因为受到内存带宽的限制。无论是 Nvidia 的 H100、MI 300 还是 TPU，它们都使用相同的外部内存，从而受到相同的限制。我们已经突破了这一限制，这得益于我们的晶圆级技术。”</p><p></p><p>在 Hot Chips 大会上，来自首尔的 Furiosa 公司也发布了第二代芯片——RNGD。Furiosa 芯片的独特之处在于它所采用的张量收缩处理器（TCP）架构。在 AI 工作负载中，矩阵乘法是一项基础操作，通常在硬件中以原语的形式实现。然而，矩阵的规模和形状（即张量）可以有极大的变化。RNGD 实现了这种更为通用的乘法版本作为原语。Furiosa 创始人兼首席执行官 June Paik 在 Hot Chips 大会上解释说：“在推理过程中，批次大小差异显著，因此充分利用张量形状的固有并行性和数据重用至关重要。”</p><p></p><p>虽然 Furiosa 没有向 MLPerf 提交 RNGD 芯片的评测数据，但该公司已在内部将 RNGD 芯片在 MLPerf 的 LLM 摘要基准测试中的性能与 Nvidia 的边缘计算芯片 L40S 进行了比较。结果显示，在功耗仅为 185 瓦的情况下，RNGD 芯片的性能与功耗为 320 瓦的 L40S 相当。June Paik 表示，随着软件优化的进一步深入，芯片的性能有望得到进一步提升。</p><p></p><p>IBM 还发布了他们为满足企业生成式 AI 工作负载需求而设计的新款 Spyre 芯片，并计划于 2025 年第一季度推向市场。</p><p></p><p>至少，在可预见的未来，AI 推理芯片市场的买家们将不会感到乏味。</p><p></p><p>原文链接：</p><p></p><p><a href="https://spectrum.ieee.org/new-inference-chips">https://spectrum.ieee.org/new-inference-chips</a>"</p><p></p><p>声明：本文由 InfoQ 翻译，未经许可禁止转载。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/68NmXXFxrZObFEvICIKd</id>
            <title>微软如何完成AI转型？微软中国CTO韦青亲述：我们需要的不是一个无所不知的模型</title>
            <link>https://www.infoq.cn/article/68NmXXFxrZObFEvICIKd</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/68NmXXFxrZObFEvICIKd</guid>
            <pubDate></pubDate>
            <updated>Wed, 11 Sep 2024 09:13:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p></p><blockquote>出品｜InfoQ 《大模型领航者》访谈主持｜霍太稳，极客邦科技创始人兼 CEO访谈嘉宾｜韦青，微软中国首席技术官作者｜褚杏娟</blockquote><p></p><p></p><p>“Satya 刚上任 CEO 时，就跟微软的员工说，‘在技术行业没有人尊重传统，只尊重创新。”微软中国首席技术官韦青说道。</p><p></p><p>船大难掉头，同样对于有着近 50 年历史、20 多万员工的微软来说，创新并不容易。但是，微软这次却无疑走在了全球 AIGC 转型之路的最前沿。</p><p></p><p>微软早早就将 GPT 系列模型全面集成到了自家的产品体系中：Github Copilot、Office 及 PC 端等，在 OpenAI 的几次重大发布对部分企业造成打击时，微软只需要专心搞应用。微软确实也取得了漂亮的财报表现，比如 GitHub 年收入已达 20 亿美元，其中 Copilot 占收入增长的 40 % 以上，这已经比当初收购整个 GitHub 的规模还要大。</p><p></p><p>正如韦青所说，“大家看到的只是冰山一角，实际上，背后是积攒了可能几十年带来的成果。”</p><p></p><p>OpenAI 与微软的合作可以追溯到 2016 年。2021 年 Build 大会上，Satya 表示将“世界上最强大的语言模型”GPT-3 引入到了 Power Platform 上。2022 年的 Build 大会上，Satya 直接提到了 OpenAI 的名字，并把 GPT、DALL-E、Codex 纳入微软 Models as Platforms 服务的一部分。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/46/464ce111342da1e8eb69963207a7ee35.jpeg" /></p><p></p><p>Satya Nadella 2021 年、2022 年（从左到右）Build 大会的 keynote 演讲</p><p></p><p>但两者的合作只是微软 AIGC 转型的其中一面，对于普通开发者来说，更宝贵的应该是微软亲身实践的心得。在这次访谈里，韦青向我们介绍了一个更加务实、创新的微软。</p><p></p><p></p><h3>为什么是微软</h3><p></p><p></p><p></p><blockquote>“我们不再只是讨论大模型、算力和存储这些了，已经不是那个阶段了。”</blockquote><p></p><p></p><p>韦青加入微软至今已经 20 多年的时间，先后负责了移动产品、Windows 产品等。见证了互联网这么多年的变迁，他对这次 AIGC 转型的感想是：人的思想转型是最难的。</p><p></p><p>就拿微软的研发工程师来说，他们对 AIGC 的认识也是随着自己对各种应用的不断深入而持续刷新的。</p><p></p><p>具体地，比如微软 Fabric 工程师最开始的想法是“AI for Data”，可以理解为“AI +”，即将 AI 放入现有产品体系来改进数据处理。基于此，他们推出了第一版产品并获得了很大的成功。</p><p></p><p>但在开发第二版产品时，工程师们便意识到不能再继续沿用同样的方法。第二版产品的核心理念是“Data for AI”，对应地，可以理解为“AI*”。乘法与加法的思维方式有着本质的不同，乘法意味着内化，而不仅仅是增加，也就是说不仅仅要将 AI 应用到现有流程中，而是要为了新工具将现有流程进行重构。</p><p></p><p>虽然冲在了大模型应用的前头，但微软内部并没有神化大模型。Microsoft Azure 首席技术官 Mark Russinovich 评论大模型是“junior employee”，即学了很多知识、主观能动性很强、记忆力也超强，但是一个非常幼稚的员工。</p><p></p><p>要让这个员工知道怎么帮你干活，就需要“your data”来训练，否则它不知道你的喜好、边界。而用户能够使用的大模型就是用自己数据调整过的“小模型”。</p><p></p><p>微软的另一层考虑是，大模型的应用不应该被限制。“不存在只有谁能用谁不能用、大型机可以用边缘不能用等情况，函数调用要因人、因事、因地制宜。”因此，当概率模型不能起作用时候，工具就要通过调用软件、功能、函数等发挥作用。这也是为什么微软大力研发小模型的原因之一。</p><p></p><p>“当你不能用大模型或断网的时候，Phi 就是本地解决方式。Phi-3 作为一个边缘模型，在基座模型和 tool chain 之间，起到了非常重要的承前启后作用。”韦青说道。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/e7/e7cd8d6178c5d64acb0444852b806902.png" /></p><p></p><p>韦青习惯于用系统工程方式考虑问题，有前提条件约束地思考，看到“水桶的短板”。他否认所谓要么是大模型时代、要么就是小模型时代等各种绝对的说法。“模型并不是越大就越好。大模型之所以大，是因为它们有更多的人造神经元，能够记住更多的知识，但这也会带来所谓的‘知识的诅咒’。”</p><p></p><p>在他看来，人们需要的不是一个无所不知的模型，而是一个能够理解自己喜好，并提供个性化建议的模型，这样的模型能够告诉我们“下周应该做什么”就足够了。当人们偶尔会对某个特定话题感兴趣时，则可以利用大模型来获取信息。</p><p></p><p>因此，人们身边的小模型除了能够调度本地应用，还要在必要时能够调用云端大模型，云端某个大模型可能擅长回答人文问题，而另一个擅长回答科学问题，可以通过分工合作提供更加精准和个性化的服务。</p><p></p><p>“这才是未来大家想要的，而微软 Azure 架构就是在为这种方式做准备，即将所有模型集中在一起构成一个庞大的系统。”韦青介绍道。</p><p></p><p>要利用好各种工具，算力、存储和网络通信都是必要的。如果网络通信存在延迟，就需要中央模型和边缘模型结合，边缘模型需要相应的数据支持，而有了数据就可以开发出自己的 Copilot。</p><p></p><p>以 Azure 为支点，微软构建了从基础设施、数据、工具到应用程序的完整技术堆栈来支持 AI 用户。与此同时，微软还加大了投入，将大约一半资本支出用于建设和租赁数据中心，剩下的部分主要用于购买服务器，但其投入速度依然跟不上市场需求。</p><p></p><p>微软全球向世界各地用户提供了“AI 全家桶”，但这应该算是云厂商的基本操作。微软现在已经进入下一阶段：向计算要效率，比如在提供针对大模型的计算能力时，微软甚至会对生成 token 的计算方式进行优化。</p><p></p><p>“我们现在做的是最大化人工智能的计算效率。”韦青说道，“不仅仅是计算，所有针对 AI 特点的数据流动，包括 prompt、KVQ 等，还涉及不同精度的计算，比如浮点数、16 位整数、8 位整数或 4 位整数等，都是优化目标。”</p><p></p><p>如何最大化算力的利用效率，并以最节能的方式进行计算，关键在于找到最有效的计算方法，以及如何以最小的实验成本生成所需的结果。“这并不意味着精度越高越好，而是要找到最适合当前任务的精度水平。”韦青提醒道。</p><p></p><p></p><h4>超强工具的另一面</h4><p></p><p></p><p>“一阴一阳谓之道”，任何事物都包含着对立统一的规律。</p><p></p><p>某个特别强大的工具开始被普遍使用时，了解它的负面影响是必要的，这就是负责任的 AI（Responsible AI）的核心理念，因为太强的话一定有弱点，比如公平性、透明性和可追溯性。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c2/c2841a448e98fabc364c2b3f673a3938.png" /></p><p></p><p>“世界上没有 100% 完美的事物，我们生活的是一个充满概率的世界。”在韦青看来，如果出了事故，责任在于人而非工具，人们要做的就是在充满概率性的波动中找到确定性。</p><p></p><p>“即使是现在，起码我认识的许多工程师在开发那些很厉害的工具时，他们都会秉持一个最基本的、第一性认知原则，即在开发一个特别强大的工具时，我必须知道它的弱点。”韦青补充道，“同样地，当听到有人说某事物非常糟糕时，我们也应该看到它积极的一面。只有看到了一个所谓不好事物的积极面，才能更有信心地作出评价。”</p><p></p><p>微软在 2019 年之前意识到这些工具变得越来越强大时，率先成立了 Responsible AI 团队。“有些公司可能会认为这是在浪费钱，但实际上，公司是社会的一部分。当公司开发出一款强大的工具时，如果不能确保其被负责任地使用，就可能遭到反噬。”韦青说道。</p><p></p><h3>大模型应用启示</h3><p></p><p></p><p></p><blockquote>“现在早已经过了还在分析、还在想、还在空谈的时候了，全世界大量的企业和个人都已经进入了实用态。”</blockquote><p></p><p></p><p>“模型不是你的产品，模型是你产品的一部分（model is not your product，model is part of your product）”Satya 在 2022 年 Build 大会上说道，这其实就蕴含了微软对大模型应用的理解。</p><p></p><p>韦青把大模型比作公有发电厂，它的任务就是发电。但只是发电的话，并不足以让大模型应用普及。</p><p></p><p>“人们并不能直接使用电子，电子需要被整合到电器中才能被使用。同理，这些 token 被整合到各种应用中，尤其是边缘计算领域，如 AIPC 等，大模型应用才会变得流行起来。”韦青解释道。这其实意味着，大模型要普及就得变成一种本地能力为个人使用。</p><p></p><p>如今，一些模型厂商开始卷入 token 的价格竞争。在韦青看来，大模型价格高低的问题就像问木材这种原材料的价格是贵还是便宜。木材可以按重量出售，但加工后的产品很难用同样的方式定价，木制工艺品、木家具等价格都不一样。</p><p></p><p>因此，价格竞争虽然有一定的意义，但问题在于大模型这种“电”还是没有直接产生价值。“当前的生成视频、图片和进行问答只是初级阶段，绝不是这些技术的最终目标。”韦青说道。</p><p></p><p>而要实现从 token 到应用的质变，意味着要做流程重构。</p><p></p><p>依然以电力应用为例，百年前的电烤面包机和电动洗衣机插头实际上是灯座，因为当时的人们没有意识到除了电灯之外，电力还可以做更多的应用，因此设计之初没有留有足够的插座，如果要将插座安装在墙内就需要修改设计图。</p><p></p><p>同样，大模型应用的普及也需要“修改设计图”，这对企业来说就意味着对现有流程进行重构。</p><p></p><p>但是，如果把各种流程拆开来看，这与 AI 既有关系，又没关系。</p><p></p><p>梳理现有流程、重构流程，确保每个节点都能进行数字化数据采集，这是第一步。这个阶段确保了企业能够不断产生数据来表征流程模型。</p><p></p><p>那么，接下来的问题就是：大多数公司都拥有大量数据，这些数据能否都被用来学习并提取知识？</p><p></p><p>数据要包含信息才有意义，而信息如果没被有效利用就没有价值，之后通过各种比对和分析，信息才会产生洞察力，进而形成知识。但事实上，大部分数据在收集时并不是为了机器学习，因此许多公司虽然拥有大量数据，但当要求 CTO、CIO 建立一个模型时却不知所措。</p><p></p><p>韦青对此给出的解答是，“他们需要重新考虑从数据到信息的转化过程，这取决于企业的目标是仅仅实现数字化和信息化，还是真正建立机器知识？而机器知识又是为了什么服务？”他解释称，对于数据、信息、知识和智慧的服务，如果要清晰地应用这一轮的 AI 模型，就需要有明确的目标，否则就会失去方向。</p><p></p><p>韦青提醒道，上述工作完成后，最重要的是通过 RLHF 给这些学习内容赋予人类的期望，在此基础上进行不断优化和微调。“使用这些模型后，人们会意识到，将数据转化为信息，再通过机器学习形成知识，是为了解决人类不想做、不能做、不爱做或做不好的事情。这些事情大多是重复性的，要求精确但不一定需要创意。”</p><p></p><p>此外，韦青从工程师角度提醒一个企业大模型纳入应用的前提。</p><p></p><p>首先，要对问题进行类似几何原理的定义和论证，然后将一个特别泛泛的问题拆分为若干个小问题。比如出版业是指受众获取、经营、内容制作，还是未来的发展方向？这些都是不同的问题，需要分别拆解和定义。</p><p></p><p>其次，要有公设。比如出版社是在中国、欧洲还是美国，数字出版还是纸质出版等。然后，要有公理、论证。只要结果，而不考虑前提的定义、公设和工程约束，是非常危险的。有了上述前提，我们然后才能进行推断，而这种推断遵循 DIKW 金字塔的结构。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/27/27a4bef87bf9fb4e29f23d1191fa9cd2.png" /></p><p></p><p>上述步骤跟 AI 其实没关系，但只有上面的这些基础工作完成后，讨论 AI 在某个行业中的作用才有意义。</p><p></p><h3>对 AI 认知的极限，关键在人</h3><p></p><p></p><p></p><blockquote>“拥有了上面所有要素后，我们会意识到，技术是一方面，更重要的是人的问题。”</blockquote><p></p><p></p><p>大模型的快速发展，让人无比期待 OpenAI 能赶紧发布更先进的模型 GPT-5。韦青并没有给出大家想要的爆料，相反，他发出了自己的疑问：难道因为 5 比 4 大，就意味着 5 一定比 4 好吗？</p><p></p><p>“这实际上是一个没有意义的问题（大的不一定是好的）。关键在于社会民众对机器智能能力的需求达到了什么程度，届时一定会出现与这个需求相匹配的服务。”这是韦青的答案。</p><p></p><p>他结合自己的经验说道，“如果你真的在一个产品团队中工作，尤其是在那些全球顶级的产品团队，只要参与过产品开发你就会明白一个事实：没有人能确切地知道下一步会发生什么。”</p><p></p><p>韦青认为，对于我们所有人来说，接下来真正的挑战不仅仅是技术，真正限制在于我们的意识。他用了一句很哲学的话来总结：我们越接近真相的核心，就会发现我们离真相越远。</p><p></p><p>他举了两个例子。比如，2017 年人工智能战胜围棋选手，严重打击了顶尖选手：机器告诉我们，人类下了 2,000 年围棋，但连围棋的皮毛都没摸着。又比如，我们以为自己最远只能骑自行车到北京香山登上其最高峰香炉峰（又称鬼见愁），然后就认为自己登上了世界最高峰鬼见愁，但其实同时代已经有人用更先进的工具到了真正的最高峰珠穆朗玛峰。</p><p></p><p>“不能因为你到不了就认为不存在、认为人类无法达到。我们的寿命和思想经历是有限的。”韦青说道。</p><p></p><p>当前我们被限制的一个表现是：在产品开发中，人们又把自己当作机器来对待。</p><p></p><p>“很多时候，我们根本没有意识到我们不知道，结果机器刚刚把我们带到一个认知的边界，很多人就绝望了，认为机器将完全超越我们。我觉得不是这样。我们才到‘鬼见愁’，就争论机器要不要代替人类、人类有没有未来，这反映了人们已经被局限了。我们没有意识到，我们不应该将人视为机器。人天生不需要做机器做的事。”</p><p></p><p>在韦青看来，人类最大的特点在于擅长制定规则和“破坏”规则（这里的“破坏”是指创新和优化规则），而机器恰恰特别擅长于理解和严格执行规则。按照这个逻辑，人类本来就应该负责发号施令，让机器去做那些重复性和规则性强的工作，并在机器完成后不断改进，来保持人类的创新优势。</p><p></p><p>韦青眼中的人工智能边界是“极大、极小，极远、极近”的。极大就是宇宙，比如 AI for Science，只是生成图片和视频是不够，它会在生产力和科学上有巨大突破；极小是量子，比如把材料、药物分子等重新组合，带来更好的效果。极远是太空旅行，极近就是认识自己。</p><p></p><p></p><h4>给程序员的一些建议</h4><p></p><p></p><p>如今，韦青依然坚持自己动手去写代码，虽然无法编写大型软件，但仍然要保持手感。当我们把目光放到更细分的程序员群体，coding 出身的韦青也给出了自己判断和建议。</p><p></p><p>作为几十年的软件开发者，韦青经历了纯手工撸代码的时代，现在也开始尝试代码生成工具。</p><p></p><p>多年前，他想要自己手搓一个基础的多层神经元模型，以便深入了解更多神经元架构的细节，但因为工作繁忙而未能实现。几年后他便使用 Copilot 辅助编写，“没有进行任何优化，没有针对内存或数据位移做任何处理，只是用 C 语言直接实现了”：</p><p></p><p></p><blockquote>我们首先共同定义了数据结构，然后列出了 CNN 所需的所有函数定义，包括 ReLU、Sigmoid 等激活函数，以及矩阵乘法等。我们还列出了这些函数的导数和偏导数，然后一起实现。实现完成后，我们构建了一个测试用例，并运行了这个用例。整个过程大约花费了一个小时，写了大约 2000 行代码，而且每个函数都是正确的。虽然还需要进行一些调整，但效率非常高。</blockquote><p></p><p></p><p>“如果我们的程序员也能够这样工作，那该有多好。”韦青感叹道，“但是，如果程序员不了解网络结构的底层知识，仅仅依赖于 Tensorflow 或 PyTorch 等工具，那么也是无法有效完成任务的。”</p><p></p><p>要达到这样的水平，需要开发者对数学，特别是机器学习领域的知识有深入的了解。</p><p></p><p>韦青认为，未来的趋势就是，程序员要在两端都非常强大：既要有扎实的底层知识，也要对行业需求有清晰的认识。虽然中间的实现部分也很重要，但最关键的是要保持对基础数学建模能力和行业需求的深刻理解。</p><p></p><p>这意味着，对程序员来说，只擅长写代码已经不够了。</p><p></p><p>韦青回忆起多年前了解到的一家日本软件公司，高级软件工程师只写伪代码，其完成逻辑描述后，让所谓的“码农”去写将 UML（统一建模语言）。无论客户要求使用 C 语言、Java 还是 C#，“码农”都能根据伪代码转换成相应的代码，但他们并不能真正理解行业。</p><p></p><p>编写伪代码的人是那些既了解行业知识，又懂得基本逻辑描述的人，而真正编写 C、Python 等代码的工作其实可以交给机器完成。韦青说道，“我们应该从码农升级为程序员，程序员的水准是达到架构师的水平，即具备行业知识，并能够用逻辑方式表征这些知识。”</p><p></p><p></p><h3>结束语</h3><p></p><p></p><p>Satya 不建议微软称自己为 leader（领先者），而是用 Incumbent（现任者）。现任者把人从创新者窘境中拉出来，等着后面 challenger（挑战者）来超越。韦青将其解读为“胜不骄、败不馁”。</p><p></p><p>而对于未来，韦青借用 Ilya Sutskever 的话来总结：尽量能够比这个时代超前半步，但也别超前太多。“因为现在所有对技术的不足都是马后炮，但超前多一点点看，大部分问题都很快会被解决。”这是一种更加务实的态度。</p><p></p><p>如今，这场 AIGC 竞赛还没有结束，微软能否继续坚守自己 Incumbent 的位置，我们拭目以待。</p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/NmjtW9BeY7HaN8dHLPRM</id>
            <title>三个月建成“世界最大”Nvidia GPU 计算集群，马斯克：不够，还要再加10万个</title>
            <link>https://www.infoq.cn/article/NmjtW9BeY7HaN8dHLPRM</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NmjtW9BeY7HaN8dHLPRM</guid>
            <pubDate></pubDate>
            <updated>Wed, 11 Sep 2024 09:03:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>9 月 2 日，马斯克发文称，其人工智能公司 xAI 的团队已经上线了一台被称为“Colossus”的训练集群，总共有 100000 个英伟达的 H100 GPU。</p><p></p><p>马斯克表示，他的团队花了 122 天才完成 Colossus 的上线过程。由于 xAI 在 6 月份才选定孟菲斯作为其所在地，因此 Colossus 的部署速度可以说是非常快的。马斯克表示，在接下来的几个月里，Colossus 的规模将扩大一倍，达到 200,000 个 GPU，其中 5 万个是更为先进的 H200。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/80/80cb4f3f8d3908a1bc357eb469b0136f.png" /></p><p></p><p>一位 X 用户指出，这一发展的实际规模超过了迄今为止发布的每个主要模型。相比之下，OpenAI 最强大的模型才使用了 80000 个 GPU。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f7/f7799c1c99756c92a9c17ab67426f7f3.png" /></p><p></p><p>Nvidia 的 H200 是市场上最抢手的芯片之一，尽管最近被该公司于 2024 年 3 月推出的最新 Blackwell 芯片超越。相比之下，H200 配备 141 GB 的 HBM3E 内存和 4.8 TB/s 的带宽，Blackwell 的最高容量比 H200 高出 36.2%，总带宽高出 66.7%。</p><p></p><p>Nvidia 在 Colossus 发布后向马斯克和 xAI 团队表示祝贺。它还强调，Colossus 将是性能最强大的产品，并且在能源效率方面将有“显著提升”。</p><p></p><p>风险投资公司 ARK Invest 的首席执行官 Cathie Wood 也对该团队取得的成就表示祝贺，称其“令人印象深刻”，并表示“未来还会有重大公告”。</p><p></p><p>2023 年 4 月，有广泛报道称马斯克正在购买大量 GPU，一些消息来源报道称他打算购买多达近 10,000 个 GPU，以推进他的 xAI 项目。</p><p></p><p>在当前的人工智能淘金热中，包括微软、谷歌、亚马逊在内的多家重量级科技公司正与马斯克一道竞相采购英伟达备受青睐的 Hopper 系列人工智能芯片。马斯克也是英伟达的重要客户，其承诺今年仅用于特斯拉的英伟达硬件就要投资 30 至 40 亿美元。</p><p></p><p>孟菲斯集群将主要用来训练马斯克的 Grok-3。他在 7 月份表示，“我们希望在 12 月之前发布 Grok-3，到那时 Grok-3 应该会成为世界上最强大的人工智能。”Grok-2 的早期测试版上个月刚刚向用户推出 。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/yMDtbBbGAEPSLhPZDttm</id>
            <title>走近张大鹏教授：哈工大走出的中国第一位人工智能博士</title>
            <link>https://www.infoq.cn/article/yMDtbBbGAEPSLhPZDttm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/yMDtbBbGAEPSLhPZDttm</guid>
            <pubDate></pubDate>
            <updated>Wed, 11 Sep 2024 08:58:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><h3>写在最前</h3><p></p><p></p><p>张大鹏，加拿大皇家科学院院士，加拿大工程院院士，国际电气与电子工程师协会终身会士（IEEE Fellow），国际模式识别协会会士，亚太人工智能学会会士，香港中文大学（深圳）数据科学学院校长学勤讲座教授，深圳市人工智能与机器人研究院（AIRS）计算机视觉研究中心主任，香港中文大学（深圳）—联易融计算机视觉与人工智能联合实验室主任，以及香港理工大学荣誉教授。长期担任清华大学双聘教授，以及哈尔滨工业大学、北京大学、上海交通大学及加拿大滑铁卢大学的兼职教授。</p><p></p><p>张大鹏教授本科毕业于北京大学计算机专业，硕士毕业于哈尔滨工业大学，并先后两次博士毕业于哈尔滨工业大学和加拿大滑铁卢大学。他在哈尔滨工业大学读博期间师从中国计算机科学与工程奠基人之一陈光熙教授，张大鹏教授也是新中国培养的第一位工智能研究方的博士。</p><p></p><p>张大鹏教授从事生物特征识别、图像处理等人工智能方向的研究四十余年，是掌纹识别、中医四诊量化及人脸美学客观化等研究领域的开创者和领军人。多年来出版相关专著 20 多部，发表论文 500 余篇，持有六十多项美国、日本和中国的专利。在中医领域感知特征的标准化、量化研究，以及人体美学等生物特征的体系化研究中做出了重要贡献。</p><p></p><p>生物特征识别是人工智能领域的研究方向之一，这种技术可以通过计算机利用人体的生理特征（指纹、虹膜、面相、DNA 等）或行为特征 (步态、击键习惯等) 来进行个人身份鉴定。</p><p></p><p>从今年开始，当我走进深圳街头的 7-11 便利店时，会见到很多门店都已经开始支持微信的刷掌支付。在体验便利支付方式的同时，我时常想到掌纹识别研究方向的开创者张大鹏教授。早在学校读博的时候，在我们学院历史介绍中就经常都会看到关于张大鹏的介绍。因为他是哈工大计算机专业毕业的第一位博士。</p><p></p><p>今年 75 岁的张大鹏教授依然工作在科研第一线，2024 年 8 月，我在香港中文大学深圳校区见到了百忙之中的他，听他给我讲述了他在生物特征识别领域四十多年的研究历程。</p><p></p><p></p><h3>进取的下乡岁月</h3><p></p><p></p><p>张大鹏，1949 年出生于黑龙江省哈尔滨市。张大鹏从小学习成绩就特别好，并且有极强的上进心。初中时期，他就读于哈尔滨市第二中学，那时候他各科考试的总分在全学年 19 个班级中排名第一。</p><p></p><p>初中阶段，他的一篇作文《我和祖国一同成长》被中央广播电台中学生节目选中，在全国播放。同时，张大鹏的思想非常积极，同时担任校学生会的团支部书记和学习委员。</p><p></p><p>在上世纪六十年代，董加耕和邢燕子的事迹传遍全国。1961 年，董加耕毅然放弃了北京大学哲学系的保送机会，回乡务农。同样在那个年代，邢燕子在初中毕业后也没有回到家乡天津市，而选择了去当时的天津市宝坻县司家庄村进行劳动。</p><p></p><p>1965 年，16 岁的张大鹏到了该初中毕业的时候，本来父母希望他继续考高中。但是董加耕和邢燕子的事迹也深深感染着张大鹏，他决心响应号召，到更广阔的天地里去经历风雨，去见世面。</p><p></p><p>当时，哈尔滨市十几所中学的毕业生里共有 38 人报名了“上山下乡”，包括 5 名高中毕业生和 33 名初中毕业生。他们当时受到了哈尔滨市市领导的接见，他们坐着大卡车环游哈尔滨的主要街道，受到全市各个中小学师生的沿途欢送。这在当时对这些立志报国的热血年轻人来说是莫大的荣耀。</p><p></p><p>张大鹏下乡的地方是松花江地区呼兰县。他提出要到最艰苦的地方接受锻炼，于是便被分配到了呼兰县莲花公社井沿大队，这里是呼兰县、阿城县和巴彦县的三县交界，当地条件十分艰苦。</p><p></p><p>张大鹏在下乡期间表现非常出色，这期间他担任了生产队队长、亚麻厂厂长以及五七农场场长等职务。有一段时间还被调到呼兰县县委工作。同时，张大鹏文采很好，当时还是《光明日报》和《黑龙江日报》的通信记者。</p><p></p><p>1970 年，清华大学和北京大学在全国招收工农兵学员，一共在呼兰县招收 3 名学生，张大鹏前往北京大学计算机专业进行本科学习。</p><p></p><p></p><h3>补习基础知识的北大时光</h3><p></p><p></p><p>1970 年，北京大学计算机专业招收了50多名学生，那时候的计算机专业还处于绝密阶段，大部分生源都来自军队，以定向培养为主。</p><p></p><p>张大鹏非常珍惜在北京大学学习的时光，由于初中毕业就下乡劳动五年，没有高中的学习基础，在校期间张大鹏非常刻苦，那时候每周只休息一天，这一天的时间他总会整日泡在图书馆补习高中课程。</p><p></p><p>在大学期间，他跟其他同学合作，在学校期刊上发表了一篇论文《关于计算机存储器“下雨”检测周期的新认识》。</p><p></p><p>张大鹏身高一米八五，腿也长，学校就让他加入了田径队。有一段时间，他非常刻苦的训练，在学校的很多日子，张大鹏都会早起晨跑，他腿上绑着沙袋，在北大的校园里认真准备体育比赛。后来，在北京市高校学生的田径比赛中，张大鹏分别打破了男子 200 米和 400 米的短跑高校比赛记录。他还代表学校参加了北京市运动会，并担任旗手。</p><p></p><p>因为张大鹏的文笔一直很好，经常在学校写一些报道和诗歌。毕业的时候，他还写了一部小说并入选毕业生成果展览，当时在北大师生中引起了强烈反响。</p><p></p><p>1974 年，张大鹏就要大学毕业，本来他已经内定留校，不过张铁生高考交白卷的事件导致当时的氛围很紧张，张大鹏觉得还是回到家乡更踏实一些。于是他跟学校提出了调回黑龙江的申请，大学毕业后，张大鹏被分配到黑龙江大学，那时候的黑龙江大学还没有计算机专业，于是他成为了一名数学系的大学老师。</p><p></p><p></p><h3>组建黑龙江大学计算机系的前身</h3><p></p><p></p><p>张大鹏的专业是计算机，除了完成数学系的正常教学任务，他还在系里组建了计算机研发小组。作为小组的负责人，张大鹏牵头研发了可以用于工业计算的微型计算机，并且得到了实际应用，这个项目后来还在 1978 年的黑龙江科学大会上获奖。这个计算机研发小组也是后来黑龙江大学计算机系的前身。</p><p></p><p>在黑龙江大学期间，张大鹏一个偶然的机会认识了黑龙江省公安厅的刑事技术专家崔道植（崔道植先生是全国公安第一代刑事技术警察、中国首席枪弹痕迹鉴定专家，被誉为中国的“福尔摩斯”）。崔道植提出希望张大鹏研发的微型计算机可以用于指纹识别技术，这对刑事案件的侦破会非常有帮助。这引发了张大鹏对生物识别技术的思考，使他产生了多年科研之路的萌芽。</p><p></p><p>1977 年，改革开放的消息传来，国家在恢复高考制度的。1980 年 2 月，新中国颁布第一个学位条例，也开始了研究生入学考试。上进的张大鹏觉得只有继续深造才能进一步提高自己，于是他全力准备考研，跟着数学系的老师学习数学，跟着英语系的老师学习英语。</p><p></p><p>1980 年，作为学位法公布后的第一批研究生，张大鹏以优异的成绩考入哈尔滨工业大学计算机专业。</p><p></p><p></p><h3>哈工大，生物特征识别研究的起点</h3><p></p><p></p><p>张大鹏在哈尔滨工业大学的硕士导师是李仲荣教授，因为以前就接触过指纹识别，他硕士的研究方向就选择了指纹识别。</p><p></p><p>在读期间，他在非常简陋的条件下，他完成了包括软 / 硬件在内的完整的微机指纹识别系统。</p><p></p><p>他的研究还协助大庆市公安局破获了一起盗窃案，公安机关在作案现场采集到了犯罪嫌疑人的指纹，他的算法匹配到了三个指纹细节特征，从而确定了嫌疑人。张大鹏其实当时感到了一些疑惑，不明白为什么仅仅三个特征的匹配就可以破案。公安机关解释说，以大庆市的人口基数，三个特征的匹配足以锁定罪犯。这件事让张大鹏备受鼓舞。</p><p></p><p>随后，他多次去北京参加公安部牵头的三校联席（清华大学、北京大学、哈尔滨工业大学）指纹识别工作会议，与清华大学边肇祺、北京大学石青云等高校科研人员一起讨论指纹识别系统在全国的研发和应用。</p><p></p><p>1983 年，张大鹏硕士毕业，继续在哈工大攻读博士学位，导师是中国计算机科学与工程奠基人之一的陈光熙教授，副导师是李仲荣教授。</p><p></p><p>张大鹏的博士研究方向是遥感卫星数据的实时处理，这项研究起源于中国航天科技集团公司某机构的一个项目。卫星在围绕地球旋转的时候会不断采集地面图像数据，卫星每绕一圈采集的数据量都很大，所以就需要有高速的数据处理算法来实时处理这些信息。</p><p></p><p>1984 年，第七届国际模式识别会议 (International Conference on Pattern Recognition, ICPR) 在加拿大蒙特利尔召开，张大鹏的两篇论文《A Fingerprint Recognition System with Micro-Computer》和《To Detect the Defects in Welding Seam the Pattern Recognition》被会议录用。</p><p></p><p>就在这一年，哈尔滨工业大学焊接专业吴林教授找到张大鹏所在的研究组，吴林教授在日本看到了利用计算机视觉技术自动检测焊接质量的技术，他希望和计算机系合作开发这样的技术。这个任务被分配给了张大鹏，他们合作的论文《微型机焊接缺欠自动检测系统的研究》发表在学术期刊《信息与控制》上，这是在中国知网中可以查到张大鹏最早发表的中文论文。</p><p></p><p>1985 年，张大鹏跟随中国宇航代表团赴瑞典参加国际宇航大会（International Astronautical Congress，IAC）。他在会议上介绍了他遥感卫星图像处理的研究工作。</p><p></p><p>1986 年，张大鹏硕士阶段的研究《指纹识别系统》获得了国防科学技术工业委员会科技进步三等奖。</p><p></p><p>同年，他博士阶段的研究《卫星实时遥感图像识别》获得航空航天工业部科技进步一等奖。</p><p></p><p>也是在这一年，张大鹏博士毕业。那个年代的博士毕业答辩很受重视，他的博士论文送审收到了 50 多位国内顶级专家 / 学者的评审意见。</p><p></p><p>张大鹏的毕业答辩在北京举行，答辩委员会专家云集，答辩委员会的主任由中国科学院学部委员（院士）、清华大学教授常迵担任。由于研究成果突出，答辩顺利通过。</p><p></p><p>因为张大鹏读博的工作涉及到指纹识别和遥感卫星的图像处理等相关方向，所以他是中国和哈工大培养的第一位人工智能研究方向的博士。</p><p></p><p></p><h3>中国首批博士后研究员</h3><p></p><p></p><p>当时，张大鹏博士毕业后有三个选择。一是留校，时任哈工大校长的杨士勤教授亲自与张大鹏谈话，希望他能留在学校，并且可以直接给他副教授的职称。</p><p></p><p>第二个选择是去公安部工作，公安部特别重视张大鹏之前关于指纹识别的研究。公安部科技司司长专门找到哈工大，希望张大鹏可以服从组织分配到公安部工作，报效国家。</p><p></p><p>第三个选择是去清华大学，1983 年至 1984 年，诺贝尔奖获得者、华裔物理学家李政道先生两次致信邓小平，建议中国实行博士后制度。1985 年，国务院正式批准设立博士后工作站。</p><p></p><p>在北京博士答辩时，常迵院士告诉张大鹏他正准备招收第一批博士后，并希望张大鹏去清华大学做博士后。</p><p></p><p>虽然对母校很留恋，但是张大鹏还是希望能够继续深造，他选择了去清华大学自动化系做常迵院士的第一个博士后，也是新中国首批博士后。</p><p></p><p>在清华常院士的教研组，张大鹏博士主要跟着边肇祺教授继续指纹识别方面的研究，同时还在公安部刑侦二所兼职做一些咨询工作，协助公安部在生物识别方面的刑侦研究。</p><p></p><p>在常院士和边教授的指导下，他还在博士后期间写完了他的第一本编著《并行图像处理与模式识别的计算机系统设计》。</p><p></p><p>1988 年 4 月，张大鹏完成了清华大学的博士后研究工作。</p><p></p><p></p><h3>从第二个博士后到第二个博士学位</h3><p></p><p></p><p>张大鹏博士后出站时本来有机会留在清华大学，不过，常迵院士鼓励他出国继续深造，到更好的科研环境中学习。如果留在清华，出国的机会就需要排队等待，这在当时并不是容易的事情。</p><p></p><p>当时，常迵教授也是中国科学院自动化研究所的学术委员会主席。常教授建议张大鹏入职自动化所，这样能快一些出国深造。</p><p></p><p>就这样，1988 年，张大鹏博士后出站后被聘为中科院自动化所的副研究员。</p><p></p><p>当时，国家 863 计划刚刚起步，常迵院士让张大鹏参与了 863 项目的一些前期筹备工作。常院士认为，硬件技术的发展对于国家信息技术整体的发展至关重要。他希望张大鹏能够出国从事大规模集成电路方向（VLSI）的相关研究，这是国家最需要的技术。在中科院自动化所工作 5 个月后，1988 年 10 月，张大鹏开始前往加拿大温莎大学继续进行博士后研究，研究方向为大规模集成电路。</p><p></p><p>两年后，他的两篇学术论文在国际顶级期刊《IEEE Transaction on Circuts and Systems》和《IEEE Journal of Solid-State Circuts》上发表。</p><p></p><p>经过三年的研究工作，张大鹏发现博士后的身份让他很难真正接触到研究组的核心技术。于是，张大鹏决定在加拿大滑铁卢大学攻读他的第二个博士学位，导师为穆罕默德·艾尔马斯里 (Mohamed Elmasry) 教授，研究方向为 VLSI 芯片设计。</p><p></p><p>第二次攻读博士期间，他在相关学术期刊上陆续发表了 9 篇学术论文，并亲手设计及研发了乘法以及神经元等芯片。</p><p></p><p>1994 年，张大鹏在加拿大滑铁卢大学第二次博士毕业。他攻读了一个哈工大的人工智能博士学位和一个加拿大滑铁卢大学的芯片博士学位，这在现在看来也是相当不容易的事情，然而张大鹏在 30 年前就已经做到了。</p><p></p><p></p><h3>回到香港，开启掌纹识别研究</h3><p></p><p></p><p>在滑铁卢大学博士毕业后，张大鹏留校担任了一年的助理教授。但是张大鹏的初心还是回国发展。恰好在这个时候，香港的高校开始招人，香港科技大学、香港城市大学和香港理工大学对他都很感兴趣，但香港城市大学最先给张大鹏教授发了聘任 Offer。于是，1995 年 7 月 21 日，张大鹏优先选择入职香港城市大学担任副教授。</p><p></p><p>这一年开始，他与国内很多大学建立了密切合作。首先，他回到母校哈工大访问，受到了杨士勤校长和各学院院长的热烈欢迎。他也被母校哈尔滨工业大学聘任为兼职博士生导师。</p><p></p><p>一开始回到香港，由于当时国内还没有芯片相关的研发需求，张大鹏教授还是从事指纹识别方面的研究。一次在香港维多利亚港散步，他看到路边有看手相的算命先生，于是萌生了进行掌纹识别研究的想法。张大鹏教授认为，掌纹的特征足够复杂，这是可以作为防伪手段的保证。另外，掌纹的面积足够大，即使有一部分因为油渍、破损等原因无法识别，也不会影响整体识别效果。尤其是国内外最当时还没有相关研究，这是一个高精度、高防伪识别能力的新方向。</p><p></p><p>1997 年，张大鹏的团队正式开始掌纹识别的研究工作。1998 年，张大鹏团队发表了关于掌纹识别的第一篇论文《Palmprint Verification: An Implementation of Biometric Technology》在 ICPR 上发表。这也是国际上最早进行掌纹识别的研究团队。</p><p></p><p>从 1998 年至今，张大鹏教授的团队多年来发表掌纹识别相关论文 200 余篇，公开了多个掌纹数据集，同时也研发了包括 2D、3D、多光谱、接触式以及非接触式的多种掌纹识别系统并将其投入应用。</p><p></p><p></p><h3>四诊量化，人工智能与传统中医的结合</h3><p></p><p></p><p>1997 年，一个偶然的机会张大鹏教授认识了哈尔滨市第 211 医院普通外科主任李乃民医生，李医生是中西医结合的专家。两个人聊起中医中脉象、舌象等传统疾病诊断方法，张大鹏教授觉得这些传统中医中的“望闻问切”特征应该也可以用计算机量化。这次相遇是引发他之后中医四诊量化研究的契机。</p><p></p><p>张大鹏教授跟我讲解了四诊量化研究的具体方向和重要性。在传统的中医诊断当中，观察体表信息只能靠人的经验，这些体表信息包括人的舌象、脉象、气味、语音和步态等。中医四诊量化，就是要获取这些体表信息数据，再把这些体表信息转换成计算机可以存储、分析的标准数据。</p><p></p><p>体表信息存储的下一步是特征的标准化，曾经有一个经验丰富的中医跟张大鹏说，他肉眼看到的舌象与手机拍摄到的舌象看起来就是不一样的，这是由于光线、观察角度等因素导致的不一致。四诊量化工作就是要把这些体表数据标准化，使同一个设备在不同条件下采集的体表数据相同，这样同一设备在不同地点、条件下采集的数据就是兼容和通用的。最后将信息提供给中医师，作为他们诊断、治疗过程中的参考。</p><p></p><p>体表信息标准化的工作完成后，张大鹏教授的团队还实现了疾病的量化。他们采集大量患者的疾病数据和体表信息数据，尝试训练算法找出西医框架下的疾病（比如高血压、糖尿病等）与中医框架下体表信息（比如舌象、脉象等）的关联。并把这种关联提供给中医师进行参考。</p><p></p><p>张大鹏教授告诉我，他们团队将中医中经典的“望闻问切”映射成人体的四种感知信息，分别是：视觉感知、嗅觉感知、听觉感知、触觉感知。</p><p></p><p>2002 年，张大鹏教授关于舌象研究的第一篇论文《On Automated Tongue lmage Segmentation in Chinese Medicine》在 ICPR 会议上发表。</p><p></p><p>2006 年，张大鹏教授关于舌象研究的第一本专著《Tongue Diagnostics》（舌象诊断）在 Academy Press（美国学术出版社）出版。</p><p></p><p>这些年他们的研究过程就是采集患者数据、算法训练、分析结果和发表论文，然后再重新循环这些步骤，一步一个脚印地深入中医四诊量化的研究。拿舌象数据来说，他们的舌象信息获取分析设备已经研发到了第七代。</p><p></p><p>多年来，他的团队跟哈尔滨市 211 医院、北京武警医院、广东省中医院、北京大学深圳医院、深圳市龙岗区医院、深圳市精神卫生中心（深圳市康宁医院）以及深圳市中医院等医疗机构密切合作。其中，他们与广东省中医院杨志敏院长合作的研究获得了国家自然科学基金重点项目的资助。</p><p></p><p>他们团队已经发表了 100 余篇中医四诊量化方面的论文，不但对器质性疾病（比如高血压、糖尿病等）进行量化，还对一些功能性疾病（比如抑郁症等）进行了四诊分析。</p><p></p><p>张大鹏教授的团队还建立了世界上最大的中医感知数据库，数据库涵盖了几万名患者的体表信息，对于每个患者，数据库采集了视觉、嗅觉、听觉、触觉以及步态等 7 种模态的信息。</p><p></p><p></p><h3>建立人脸美学体系，探索美的客观化本质</h3><p></p><p></p><p>有一年，“香港小姐”选美结果揭晓，大众一片哗然，对于选美结果有很大的质疑。这时候组委会找到张大鹏，希望找到一种算法可以对候选人是否美丽进行初筛。</p><p></p><p>张大鹏教授告诉我，虽然每个人的审美观不一样，美的判断是主观的，但是有些美应该是大家公认的，比如三庭五眼、黄金规则这样的标准就长期广泛被大众接受。</p><p></p><p>张大鹏当时在调研中发现，其实美学相关的生物特征研究还很少，以色列有些学者在心理学领域进行美学探讨，也没能给出相应的量化标准。所以他决定尝试建立完善的人脸美学客观化分析体系。</p><p></p><p>2011 年，张大鹏教授人脸美学体系研究的第一篇论文《Quantitative analysis of human facial beauty using geometric features》在顶级国际期刊《Pattern Recognition》发表。</p><p></p><p>2016 年，张大鹏教授人脸美学体系研究的第一本专著《Computer Models for Facial Beauty Analysis》（人脸美学分析的计算机模型）在 Springer（施普林格出版社）出版</p><p></p><p>香港执教二十三年，</p><p></p><p>科研之路成绩斐然</p><p></p><p>1995 年 7 月从加拿大回到香港起，张大鹏教授一共在香港城市大学工作了三年。1998 年，张大鹏教授觉得香港城市大学的研究氛围更偏重底层理论，而香港理工大学相对更加重视应用研究，更适合自己的工科背景，于是便决定加入香港理工大学计算机系。从这时起，他在香港理工大学整整工作了二十年。</p><p></p><p>1998 年，张大鹏在香港理工大学创立国际上第一个生物识别研究中心。</p><p></p><p>1999 年，张大鹏晋升香港理工大学教授。</p><p></p><p>2001 年，张大鹏教授创立计算机视觉领域国际顶级期刊《国际图像和图形学报》（International Journal of Image and Graphics，IJIG）</p><p></p><p>2002 年，张大鹏教授创建 Springer 国际生物识别丛书（International Series on Biometrics，KISB）的创始人并担任主编。</p><p></p><p>2004 年，张大鹏获得香港特别行政区最高科技奖“裘槎科技工作者”。</p><p></p><p>2005 年，张大鹏担任香港理工大学讲座教授。</p><p></p><p>2014 年开始，张大鹏教授连续八年被 Clarivate Analytics(前身为汤森路透) 列为“高被引科学家”。</p><p></p><p>三十多年的时间，张大鹏为中国生物特征识别和模式识别领域培养了70余名博士、20余名硕士，他们中许多人已经成为了很优秀的学者。他们在各自的研究领域里发挥了重要作用。</p><p></p><p>2018 年，张大鹏教授从香港理工大学正式退休并继续担任荣誉教授。随后，张大鹏教授回到内地，担任香港中文大学（深圳）数据科学学院校长讲座教授。</p><p></p><p>2020 年，张大鹏教授当选加拿大科学院院士。</p><p></p><p>2021 年，张大鹏教授当选加拿大工程院院士。</p><p></p><p>同年，张大鹏教授开始担任香港中文大学（深圳）数据科学学院校长学勤讲座教授。</p><p></p><p>2022-2024 年，张大鹏连续三年获得 Research.com 评选的计算机领域 Leader Award。</p><p></p><p>现在，75 岁的张大鹏依然在香港中文大学以全职身份承担教学和科研工作，而且工作节奏非常忙碌。这次访谈我们约了好久才终于成行，采访进行的特别顺利，以至于有几个问题我还没有问，他就已经说出了答案。我丝毫感觉不出他的真实年龄，而只能感受到他对科学工作的热情，以及他对几十年研究工作积累成果的自豪和欣慰。</p><p></p><p>2015 年，中国中医科学院屠呦呦研究员获得诺贝尔生理奖，使中国的传统中医药更加被世界认可。</p><p></p><p>同样，近四十年的时间，从东半球的中国到西半球的加拿大。从中国的最北端哈尔滨到几乎是最南端的香港和深圳。张大鹏教授一直持之以恒地深耕自己的研究方向。从指纹、掌纹识别的身份鉴定工作到中医四诊量化的研究，作为中国第一位人工智能方向毕业的博士，张大鹏教授和他的团队将最先进的计算机技术和人工智能技术融入中国的传统医学，为中医的标准化和量化工作做出了重要贡献，也使中国传统医学可以走向国际，更广泛地被世界认可。</p><p></p><p>作者简介</p><p></p><p>秦海龙，香港科技大学社会科学部博士后研究员，中国中文信息学会社会媒体处理专业委会委员。主要研究方向为中国人工智能发展史和计算社会学。博士毕业于哈尔滨工业大学社会计算与信息检索研究中心，前自然语言处理研发工程师，曾就职于小米科技和三角兽科技。</p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/3bmauitUuaQ3d9vmlKrp</id>
            <title>面壁小钢炮 3.0 重磅发布！“无限”长文本，性能超 Kimi</title>
            <link>https://www.infoq.cn/article/3bmauitUuaQ3d9vmlKrp</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/3bmauitUuaQ3d9vmlKrp</guid>
            <pubDate></pubDate>
            <updated>Wed, 11 Sep 2024 08:58:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>近日，面壁智能宣布，旗舰端侧模型面壁「小刚炮」系列进化为全新 MiniCPM 3.0 基座模型，再次以小博大，以 4B 参数，带来超越 GPT-3.5 的性能。</p><p></p><p>据介绍，MiniCPM 3.0 量化后仅 2GB 内存，端侧友好，主要特点包括：</p><p></p><p>无限长文本，榜单性能超越 Kimi，超长文本也不崩；性能比肩 GPT-4o 的端侧最强 Function Calling；超强 RAG 外挂三件套，中文检索第一、生成超 Llama3-8B。</p><p></p><p>MiniCPM 3.0 开源地址：</p><p></p><p>GitHub:</p><p>🔗 <a href="https://github.com/OpenBMB/MiniCPM">https://github.com/OpenBMB/MiniCPM</a>"</p><p>HuggingFace:</p><p>🔗 <a href="https://huggingface.co/openbmb/MiniCPM3-4B">https://huggingface.co/openbmb/MiniCPM3-4B</a>"</p><p></p><p>“提前近 4 个月，我们实现了初代面壁小钢炮发布时立下的 Flag：今年内让 GPT-3.5 水平的模型在端侧跑起来！”面壁智能团队表示。</p><p></p><p>据悉，MiniCPM 3.0 再次挖掘端侧模型的极致性能，仅 4B 参数，在包括自然语言理解、知识、代码、数学等多项能力上对 GPT-3.5 实现赶超，在 Qwen2-7B、 Phi-3.5、GLM4-9B、LLaMa3-8B 等一众中外知名模型脱颖而出。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/21/210ccd8f55beeacbbc332a0f9f61afb5.png" /></p><p></p><p>历经数次调整，面壁团队构建了全新技术架构。围绕 Scaling Law 的核心，面壁将提升知识密度视为高效大模型的第一性原理（知识密度 = 模型能力 / 参与计算的模型参数），并且提出了大模型时代的“摩尔定律”：模型知识密度不断提升，平均每 8 个月提升一倍，内部称为“面壁定律”。</p><p></p><p>新一代小钢炮集长文本、Function Call 与 RAG 等大模型重要能力于一身，在这些呼声极高的模型功能上，MiniCPM 3.0 集结各家所长。</p><p></p><h4>面壁“无限”长文本，性能超 Kimi</h4><p></p><p></p><p>上下文长度是衡量大模型基础能力的一项重要指标，更长的上下文长度意味大模型拥有更大的“内存”和更长的“记忆”，不仅能提高大模型处理数据的能力上限，还能拓宽大模型应用的广度和深度。</p><p></p><p>面壁提出 LLMxMapReduce 长本文分帧处理技术 ，一举实现“无限”长文本。除了超越 GPT-4、KimiChat 等标杆模型的优异表现（ InfiniteBench 榜单成绩），面壁还表示，文本越长，4B 小钢炮凭借愈加稳定的表现，可以展现出越强的性能优势。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/90/90235cb40d89075f0b69d73908643553.png" /></p><p></p><p>InfiniteBench 大模型长文本能力的权威评测集</p><p></p><p>检索、数学、代码、问答和摘要等多维度能力评估</p><p></p><p>① MiniCPM 3.0 表现超越 GPT-4、KimiChat、Qwen2-70B；</p><p>② 千亿模型 Qwen2-70B、Llama3-70b 结合 LLMxMapReduce 也取得更佳表现。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f8/f8ce69cce071e1d2b9ee40398a3cab26.png" /></p><p></p><p>InfiniteBench Zh.QA 评测结果显示，4B 参数的面壁小钢炮整体性能优于 Kimi，在更长的文本上表现出相较更强的稳定性。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/69/69a8bc0a1c3a7fdef0a05e3f5737fc7a.png" /></p><p></p><p>LLMxMapReduce 技术框架图</p><p></p><p></p><h4>GPT-4o 级 Function calling ，终端 Agent 应用蓄势待发</h4><p></p><p></p><p>智能体应用是端侧 AI 必争之地，其中一项至关重要的技术是 Function Calling（函数调用），它能够将用户模糊化的输入语义转换为机器可以精确理解执行的结构化指令，并让大模型连接外部工具和系统，例如通过语音在手机上调用日历、天气、邮件、浏览器等 APP 或相册、文件等本地数据库，从而打开终端设备 Agent 应用的无限可能，也让人机交互更加自然和方便。</p><p></p><p>据介绍，MiniCPM 3.0 拥有端侧最强 Function calling 性能 ，在权威评测榜单 Berkeley Function-Calling Leaderboard 上，其性能接近 GPT-4o，并超越 Llama 3.1-8B、Qwen-2-7B、GLM-4-9B 等众多模型。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/2d/2d92def44884bc6fd77b9f6afedacb1a.png" /></p><p></p><p></p><h4>RAG 外挂三件套</h4><p></p><p></p><p>端侧模型也能“开外挂”，RAG（检索增强生成技术）让模型引用外部知识库，检索到最新、最可靠的专业知识，确保生成内容更加可信，大大减少大模型的幻觉问题。大模型 +RAG 在行业中极其实用，尤其是对法律、医疗等依赖专业知识库、对大模型幻觉容忍度极低的垂直行业。</p><p></p><p>这次，面壁一口气带来超强 RAG 外挂三件套：MiniCPM-Embedding（检索模型）、MiniCPM-Reranker（重排序模型）和面向 RAG 场景的 LoRA 插件（生成模型），款款优秀：</p><p></p><p>MiniCPM-Embedding（检索模型）中英跨语言检索取得 SOTA 性能，在评估模型文本嵌入能力的权威评测集 MTEB 的检索榜单上中文第一、英文第十三 ；MiniCPM-Reranker（重排序模型）在中文、英文、中英跨语言测试上取得 SOTA 性能 ；经过针对 RAG 场景的 LoRA 训练后，MiniCPM 3.0-RAG-LoRA 在开放域问答（NQ、TQA、MARCO）、多跳问答（HotpotQA）、对话（WoW）、事实核查（FEVER）和信息填充（T-REx）等多项任务上的性能表现，超越 Llama3-8B 和 Baichuan2-13B 等业内优秀模型。</p><p></p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/kiE2wEfrXVZxkLS4kXeb</id>
            <title>AI 超算新时代：GMI Cloud 携手新加坡电信 打造亚太 AI 算力高速网络</title>
            <link>https://www.infoq.cn/article/kiE2wEfrXVZxkLS4kXeb</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/kiE2wEfrXVZxkLS4kXeb</guid>
            <pubDate></pubDate>
            <updated>Wed, 11 Sep 2024 03:39:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>2024 年 9 月 10 日，台湾首家荣获 NVIDIA 认证云服务合作伙伴（NCP）资格的 AI 原生 GPU 云服务平台 GMI Cloud 重磅宣布——“与亚洲领先的通讯科技集团新加坡电信 (Singtel) 达成策略合作。”该策略联盟强强联手，将结合双方的 GPU 资源和云计算部署能力，倾心打造灵活可扩展的高效能运算平台，大幅提升企业在获取运算资源时的效率与便捷程度。</p><p>&nbsp;</p><p>双方合作协议明确指出，Singtel 的客户将能够畅享 GMI Cloud 强大的 GPU 高效算力，在新开拓的市场中轻松处理密集型工作负载。这一举措将不仅极大地拓展了 Singtel 的服务覆盖范围，还为 GMI Cloud &nbsp;开辟了全新的市场机会。与此同时，GMI Cloud 将与 Singtel 的 Paragon 编排平台深度整合，全面发挥 Singtel 在新加坡的 NVIDIA H100 Tensor Core GPU 算力资源优势，为企业提供先进的 AI 模型训练与推理能力，助力企业从容应对大规模计算任务及 AI 应用挑战。</p><p>&nbsp;</p><p>就此，GMI Cloud 首席执行官 Alex Yeh 表示：“此次与 Singtel 的合作，是亚太地区 AI 基础设施发展的一项重要突破。GMI Cloud 在提供可扩展性和电信优化的 GPU 算力解决方案方面具备深厚专业，与 Singtel 领先的网络能力相得益彰。我们的灵活部署方式和处理高容量、低延迟工作负载的产业经验，将为电信行业的 AI 计划带来显著的性能提升和成本优化。这一合作将构建一个加速 AI 采用的生态系统，让电信提供商和企业能提升运算效率、优化服务，并推动区域性的技术创新。”</p><p>&nbsp;</p><p>同时，Singtel 数字基础设施公司和 Nxera 首席执行官 Bill Chang 也表示：“我们致力于构建下一代数字基础设施和合作伙伴生态，以满足各行业对可扩展、高效且具成本效益的解决方案日益增长的需求，尤其针对计算密集型工作负载。与 GMI Cloud 的合作扩展了我们在亚太地区的 GPUaaS 覆盖范围，增强了我们在该区域的算力部署能力。这将为企业提供所需的灵活性和扩展性，使其充分利用运算的强大能力，推动创新并加速发展。”</p><p>&nbsp;</p><p>此次合作进一步彰显了GMI Cloud 在 AI 基础设施市场中的领导地位。作为 NVIDIA 认证的云服务合作伙伴（NCP），GMI Cloud 不仅在技术研发方面具有显著优势，同时在供应链整合方面也有着卓越能力。目前，该公司已储备充足了的最新的 H200 GPU，全力确保可持续性地为客户提供高效能、低延迟的算力支持，以满足企业多样化的密集型工作负载需求。</p><p>&nbsp;</p><p>随着GMI Cloud 与Singtel此次合作的稳步推进，亚太地区的企业将能够加速 AI 应用的落地。借助 GMI Cloud 的先进技术和 Singtel 的广泛网络，企业将获得灵活且强大的 AI 算力，驱动自身业务创新与发展，全面提升区域竞争力。</p><p>&nbsp;</p><p></p><p></p><h5>关于GMI Cloud</h5><p></p><p></p><p>GMI Cloud 是一家领先的 AI 原生 GPU 云服务提供商，拥有遍布全球的数据中心网络，为 AI 和机器学习工作负载提供最新、最优化的 GPU 资源。公司由来自 GoogleX 的 AI 开发专家和硅谷精英人才创立，致力于为新创公司、研究机构以及大型企业提供安全、高效且具成本效益的 AI 基础架构解决方案。GMI Cloud 的技术能让客户更快速、更经济地进行 AI 创新，同时确保高度的数据安全和运算效能。作为推动通用人工智能（AGI）未来发展的重要力量，GMI Cloud 持续在 AI 基础设施领域引领创新。欲了解更多信息，请访问 <a href="https://www.gmicloud.ai/">www.gmicloud.ai</a>"</p><p>&nbsp;</p><p></p><h5>关于Singtel</h5><p></p><p></p><p>Singtel 是亚洲领先的通讯科技集团，经营下一代连接、数字基础设施和数字业务，包括区域数据中心分支 Nxera 和区域 IT 服务分支 NCS。该集团业务遍及亚洲、澳大利亚和非洲，为 21 个国家的超过 7.8 亿移动客户提供服务。对于消费者，Singtel 提供完整而整合的服务套餐，包括移动、宽带和电视服务。对于企业，Singtel 提供互补的劳动力流动性解决方案、数据托管、云计算、网络基础设施、分析和网络安全能力。Singtel 致力于持续创新，利用科技创造新颖令人兴奋的客户体验，支持企业数字转型，塑造更可持续的数字未来。欲了解更多信息，请访问<a href="https://www.singtel.com/">www.singtel.com</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/pTulcleVIO5au6vwTqS3</id>
            <title>腾讯公布最新AI原生云产品，已覆盖超400家互联网头部企业</title>
            <link>https://www.infoq.cn/article/pTulcleVIO5au6vwTqS3</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/pTulcleVIO5au6vwTqS3</guid>
            <pubDate></pubDate>
            <updated>Tue, 10 Sep 2024 10:33:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>近日，腾讯云在2024腾讯全球数字生态大会互联网AI应用专场上公布了最新的AI原生云产品服务。云+AI基础设施已成为企业智能化转型的关键支撑，但企业在尝试AI时，也面临包括算力、算法开发和专业人才的挑战。</p><p>&nbsp;</p><p>腾讯云副总裁许华彬在开场致辞中表示，“RAG结合企业自有知识，无需企业花费较多人力和算力，以及对大模型SFT精调，是当前企业级AI应用落地的成熟方案。近期来多行业场景的AI Agent蓬勃发展，面向Ｃ端的原生应用，以及B端企业级业务流程自动化，将成为后续应用落地的主要方式。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/be/be9cf92ad8130cd9fdb94d196264dc9b.jpeg" /></p><p>&nbsp;</p><p>随着大模型深入应用于各行各业，各行业对大模型的要求也更为精细。腾讯混元大模型算法负责人康战辉在演讲中提到，算力更强、效果更好、应用成本更低的大模型才符合当下需求。相比前代模型，最新发布的旗舰大模型“混元Turbo”性能有显著提升，训练效率提升108%，推理效率提升100%，推理成本降低50%，解码速度提升20%。除支持通用大模型能力外，腾讯混元Turbo也支持角色扮演、FunctionCall、代码生成和AI联网搜索等领域能力。</p><p>&nbsp;</p><p>目前，腾讯混元在腾讯云上提供了多种尺寸的模型服务，通过API、专属模型、精调模型等接入和使用方式面向企业及个人开发者全量开放。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/46/46727321fc85c8d1a8285dcda220326b.jpeg" /></p><p>&nbsp;</p><p>金蝶中国副总裁、研发平台总经理李帆介绍了金蝶Cosmic一体化企业级AI 解决方案等与腾讯云的系列合作成果。基于腾讯混元大模型，金蝶招聘可实现5分钟招聘需求生产以及简历初筛，招聘效率提升90%。基于腾讯云TI平台，结合高性能计算集群与星脉网络，金蝶训练财务大模型效率提升30%。</p><p>&nbsp;</p><p>在协同办公领域，腾讯文档AI负责人TONY TANG分享了腾讯文档AI在全品类文档生成、跨品类信息处理、一键式智能交互方面的优势。目前，基于腾讯混元大模型能力，腾讯文档AI智能助手已全面应用于文档、表格、幻灯片、PDF、智能文档、收集表、思维导图等文档类型，涵盖了文本内容秒级处理、函数公式运算应用、表格数据精准呈现、PPT快速生成美化、收集结果自动分析、思维导图一键生成等多项能力，并支持跨品类文档内容流转。</p><p>&nbsp;</p><p>在消费领域，值得买科技CTO王云峰分享了值得买科技1个“值得买消费大模型”，“商品库”和“内容库”2大数据库，AIUC分析引擎、AIGC生成引擎和AGENT调度引擎3个应用构建框架，以及提供针对性的AI解决方案的4类应用AI战略。</p><p>&nbsp;</p><p>针对云基础设施，王云峰分享了值得买科技依托腾讯混元大模型等理解语音、视频、图像的多模态数据，以及自购算力+多云算力的模式。目前，值得买科技已可实现10亿+条商品库与近百亿条内容库的数据处理和迅速进行模型训练和推理部署，而通过腾讯云容器场景GPU虚拟化，什么值得买也实现了对单个GPU的更细粒度划分，大大提升了资源利用率减少资源和人力的消耗。</p><p>&nbsp;</p><p>燧原科技副总裁任树峰表示，通过腾讯云保障燧原的峰值算力供给，燧原验证、benchmark测试效率从2周准备环境缩短至30分钟，提升作业并发100%、缩短仿真周期30%。</p><p>&nbsp;</p><p>在文娱领域，猫眼娱乐副总裁徐晓分享了基于腾讯云数据传输、数据存储安全方面能力打造的猫眼渲染平台的全程云端制作与渲染提升案例。依托腾讯云，猫眼娱乐的大数据平台，可实现日均3万+离线数仓调度任务稳定运行，核心任务执行效率提升20%。</p><p>&nbsp;</p><p>趣丸技术保障负责人刘亚丹分享了AI大模型赋能软件研发提质增效的实践。借助腾讯云提供的丰富能力，趣丸在代码Review、编码、测试等环节实现效率提升，在AI辅助编码方面，代码采纳率30%+；AI辅助代码Review方面，bug有效性60%+，AI辅助测试用例生成方面，测试用例行采纳率40%+，运维数字员工方面，问答解决率45%+。</p><p>&nbsp;</p><p></p><h1>AI原生云平台助力快速部署AI应用</h1><p></p><p>基于AI大模型，更多AI应用得以迅速落地。但作为AI应用的底座，大模型对算力的需求正在呈指数级增长，数据集规模持续增大也对数据清洗甄别提出了更高的要求。同时，数据无偏见，图片、影像、音频是否安全合规，使用成本是否可控等，都是使用大模型时需要考虑的问题。</p><p>&nbsp;</p><p>从AI应用落地面临的挑战出发，腾讯云行业架构副总监邱浩基于腾讯云AI全栈解决方案，从基础设施层、模型层、模型加速层、机器学习平台层、应用平台层和业务应用层阐述了腾讯云在计算、存储、网络，一站式AI开发平台腾讯云TI-ONE以及云原生产品等方面的能力，并给出了对应不同阶段AI应用开发的具体方案。</p><p>&nbsp;</p><p>智谱企业商业技术中心总经理柴思远表示，腾讯云算力集群搭配自研的星脉网络，有效提升了多机并行训练速度。其中，星脉网络通过自研交换机和通讯协议，为每台GPU服务器提供3.2T的交互带宽，实现40us内的拥塞控制和0丢包。此外，腾讯云为智谱AI提供高自愈能力的算力集群，全局监控，一站式掌握7*24小时运行状态，支持故障自动监测与恢复，保证任务连续进行，其中任务自愈时间从2小时缩短至5分钟，集群自愈时间从24小时缩短至10分钟。</p><p>&nbsp;</p><p>在大模型的助力下，AI应用已覆盖互联网多个行业，在业务高效创新上展现出了更强势的产品力。会上，面向招聘、营销、文娱等多方面的AI创新应用也悉数亮相。</p><p>&nbsp;</p><p>招聘方面，猎聘技术副总裁、AI中心负责人莫瑜表示，与腾讯云共同探索高度自动化的数字助理，针对不同职类主动挖掘招聘需求，简化工作流程，提升候选人寻访效率；自动化候选人面试环节，降低人才遴选成本。</p><p>&nbsp;</p><p>在营销领域，筷子科技CEO陈万峰展示了基于腾讯云的迁移工具MSP、腾讯云容器服务TKE和中间件、腾讯云高性能文件存储CFS Turbo，在数据迁移、弹性伸缩扩容应对高并发，文件存储以及上传下载对应解决方案，实现了AIGC一体化平台在编导、拍摄、剪辑、投放、管理方面工作流性能提升240%。</p><p>&nbsp;</p><p>腾讯新闻内容治理中心负责人陆毅然表示，腾讯新闻与AI大模型结合后，提供评论识别的AI应用、新闻Push“AI编辑”、新闻“划重点”、AI助手“新闻妹”等能力。通过AI在新闻内容精选、编辑、评论治理等多个的运用，新闻类产品内容生产效率和用户体验同步提升。</p><p>&nbsp;</p><p>腾讯混元高级AI策略产品经理张汉策以《长相思 第二季》中的角色AI为例，介绍了腾讯元宝如何与影视IP进行深度结合。此次《长相思》角色AI基于腾讯自研混元大模型的能力，训练了专门的角色模型。同时，基于多轮对话能力，实现了持续、实时、个性化对话，并采用音色克隆的技术，高度还原角色原声，建立了与粉丝真实且长久的联结。</p><p>&nbsp;</p><p>截至目前，腾讯云AI产品已覆盖超过400家互联网头部企业，累计服务超过12万家互联网客户。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/2JL6DZX3mWOIA7p4Hpkm</id>
            <title>自动编码工具王炸来了：0代码2分钟用手机就能打造专属App，一键生成应用不是梦</title>
            <link>https://www.infoq.cn/article/2JL6DZX3mWOIA7p4Hpkm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/2JL6DZX3mWOIA7p4Hpkm</guid>
            <pubDate></pubDate>
            <updated>Tue, 10 Sep 2024 10:02:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><h2>Replit发布AI智能体：输入应用创意即可实现编码与部署</h2><p></p><p>&nbsp;</p><p>在AI的世界里，技术的迭代与变革就是一场永不停歇的马拉松。上周，Cursor掀起的热潮还未散去，新的“后浪”已经悄然涌来，并立即在社交平台引来诸多技术大佬围观。</p><p>&nbsp;</p><p>近日，AI初创公司Replit推出一款新的智能体——Replit Agent。让人吃惊的是，这是一款能够从零开始构建完整应用程序的AI智能体，甚至无需编写代码就能构建软件。</p><p>&nbsp;</p><p>Replit Agent的强大之处在于它简化了软件开发，让不同技能水平的用户都能轻松上手。目前，该Agent 仅适用于通过 Replit Agent 条目创建的 Repls，不支持现有 Repls 或导入的存储库。</p><p>&nbsp;</p><p>更重要的是，Replit Agent跟当前市面上的普通Copilot编码助手不同，它更接近于软件开发实习生，能够理解用户的构想并帮助将其变为现实。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/07/07f9ecc3375d01d65381c0f8f17c73a3.png" /></p><p></p><p>说到这里，我们首先要了解AI智能体是什么、又为何如此重要。</p><p>&nbsp;</p><p>不同于ChatGPT或者Claude等现有AI助手，AI智能体属于自主性更强、主动程度更高的系统。目前的AI助手只能响应特定的查询或者任务，但AI智能体却拥有更高的独立性，可以在无需用户持续输入的条件下做出决策并执行复杂任务。它们能够随时间推移学习和适应，根据反馈及新信息不断改进自己的行动。</p><p>&nbsp;</p><p>Replit的AI智能体也沿用了这一概念，并将其应用于软件开发领域。它可以推理任务并自行创建步骤以完成整个项目——包括编写代码、设置环境和管理部署。</p><p>&nbsp;</p><p>Replit公司CEO Amjad Masad解释称，“我们已经跨过了这道门槛。但这并不代表AI将会取代开发人员，而是要增强人类的创造力，让每个人都有能力创建软件成果。”</p><p>&nbsp;</p><p>“AI 在编写代码方面非常出色。但这还不足以创建软件。您需要设置开发环境、安装软件包、配置数据库，如果幸运的话，就可以部署了，” Amjad Masad在宣布推出 Replit Agent 早期访问版时说道，它将自动执行所有这些流程。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/24/24c92151297f6765d789690f51a6f4f5.jpeg" /></p><p></p><p>即使是一直积极使用Cursor进行构建的前OpenAI联合创始人Andrej Karpathy 也表示，Replit Agents 可以归入“感受 AGI”类别。 “正如文章中提到的，制作实际的应用程序不仅仅是编写代码，你还必须设置整个环境、进行部署等等。自动化所有这些其他基础设施将允许任何人快速构建和部署整个 Web 应用程序，”Karpathy 说。</p><p>&nbsp;</p><p>如此强大的Replit Agent能做什么，不能做什么？</p><p>&nbsp;</p><p>有人说 Cursor 可以取代软件工程团队，或者至少可以缩减他们的规模，Claude Artifacts 可以消灭应用商店，但 Replit Agents 几乎可以在几秒钟内构建从登陆页面到与数据库连接的医疗保健应用程序。这甚至不需要编写一行代码。</p><p>&nbsp;</p><p>Amjad Masad 分享了一些例子，人们在几分钟内构建了一个医疗保健应用程序，其中代理可以自行修复错误，在不到 10 分钟的时间内构建了一个基于 Postgres 的 Flask 和 Vanilla Javascript 的网站，甚至在短短 2 分 43 秒内构建了一个 Wordle Clone。</p><p>&nbsp;</p><p>工作流自动化平台Zapier 的 Andrew Davison 是第一个使用 Replit Agent 在不到 90 秒的时间内构建出一款完整可运行的基于浏览器的乒乓球游戏的人。“我自己不需要做任何编码。只需坐下来观看，”他说。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/73/73f955175dbca83968c763539ad99c02.png" /></p><p></p><p>Replit Agent 最重要的用例是用于在组织内构建产品的 MVP，否则这将需要大量时间。“我想说，AI 让编码再次变得有趣，它让你摆脱烦人的样板和 API 粘合剂……每次使用它时，我都无需花一天时间辛苦查阅 API 文档，” Replit 工程副总裁 Scott Kennedy表示。</p><p></p><p>Replit 的理念最有趣的地方在于，AI 编码Agent也可以在智能手机上使用，而这正是 Replit 一直以智能手机闻名的原因。Replit 的目标一直是以积极的态度开源，让所有人都能使用 AI。RedBull 的 Sander Saar 表示，他能够在 4 分钟内用手机构建三个功能齐全的 Web 应用程序。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/07/07dce5437ce6ea94e462815a9ca00644.png" /></p><p></p><p>“Replit Agent 不仅仅是审查和编写代码，他们的 AI 代理还可以规划功能、创建开发环境、安装依赖项、编写代码、配置数据库和部署，”Saar 解释道，并质疑这是否是软件付费的终结。</p><p></p><p></p><p></p><p>就连马斯克也来围观Replit Agent，只是带着“批判”眼光来看的，马斯克发推称，“（就目前来看），它还写不了一款好的视频游戏。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/ba/bacd8f9d9e062d59e5de59a54e514c44.jpeg" /></p><p></p><h2>源自开发者，回馈开发者</h2><p></p><p>&nbsp;</p><p>那么，这款备受瞩目的Replit Agent到底什么来头？</p><p>&nbsp;</p><p>Replit Agent来自于Replit公司，这家初创AI企业由程序员 Amjad Masad、Faris Masad 和设计师 Haya Odeh 于 2016 年共同创立，总部位于旧金山，在创建 Replit 之前，Amjad Masad 曾在雅虎和 Facebook 担任工程师，并在那里构建了软件开发工具。</p><p>&nbsp;</p><p>成立至今，Replit致力于提供在线协作 IDE，支持多种编程语言，包括 JavaScript、Python、Go 和 C++。借助 Replit，用户可以与一个或多个用户共享工作区，查看文件的实时编辑、互相发送消息并一起调试代码。除此之外，用户还可以共享项目、寻求帮助、从教程中学习并使用模板。</p><p>&nbsp;</p><p>在Replit Agent大火之前，该公司旗下为人所知的工具是 Ghostwriter，这是一套由基于公开代码训练的 AI 模型驱动的功能。Ghostwriter 与 GitHub 的Copilot非常相似，可以根据用户输入的内容和帐户中的其他上下文（例如他们使用的编程语言）提出建议并解释代码。</p><p>&nbsp;</p><p>Replit Agent通过从使用该平台的开发人员处收集到的大量数据，其建立起超越竞争对手的显著优势。目前全球有数百万用户使用Replit来编码、测试和部署应用程序，而这些数据也被直接用于Replit Agent的开发。凭借来自开发工作流程中各个阶段数据的训练，该AI已经能够更加高效地自主完成多种复杂任务。</p><p>&nbsp;</p><p>Replit的一大突出功能就是其赏金服务（bounty service），用户可以请求开发人员在平台上构建软件项目。这项赏金机制为Replit Agent赋予了处理自然语言提示词的能力，并提供了宝贵的真实数据以进行训练。</p><p>&nbsp;</p><p>虽然其他不少厂商也都专注于实现代码补全或生成功能，但Replit的AI智能体则了解整个软件开发生命周期。它能够搭建项目框架、编写必要代码、调试问题，甚至处理部署——同时解释其决策并且与用户实时协作。</p><p></p><h2>低门槛软件开发时代正在到来</h2><p></p><p>&nbsp;</p><p>AI技术正在软件开发当中发挥巨大的潜在影响。我们即将迎来一个前所未有的软件开发大众化普及新时代。创业者们能够将业务灵感的原型设计周期从之前的几个星期，迅速缩短到如今的几个小时。研究人员则可以在未经多年编码练习的情况下构建起自定义工具。总而言之，想象与现实之间的障碍正在一步步消失。</p><p>&nbsp;</p><p>当然，Replit并不是唯一一家在构建AI驱动开发工具竞赛中有所行动的厂商。除了大名鼎鼎的微软GitHub Copilot之外，新一波初创公司也正在涌现，每位参与者都希望重新设计我们创建软件的方式。例如，Cognition正在开发的Devin就是一种有望扮演自主软件开发者角色的AI方案，能够从零开始构建完整项目。与此同时，Magic最近也获得了惊人的3.2亿美元融资，并公布了其LTM-2-mini模型，号称拥有1亿个token的超长上下文窗口。最近，Anysphere的Cursor在探索AI辅助编码潜力的开发者和爱好者群体当中，也积累起了越来越高的人气。</p><p>&nbsp;</p><p>尽管市场竞争颇为激烈，但Replit自认其仍然具有优势，且主要归功于独特的平台功能设计。Replit AI智能体不仅可以生成代码，还能够处理开发过程中的基础设施及部署工作。对于开发人员来说，这意味着花费在重复任务上的时间更少，能够更多地专注于创造力工作。从企业的角度看，其无疑代表着一种速度更快、成本效益更高的软件产品开发与上市形式。</p><p>&nbsp;</p><p>虽然Masad本人对于AI在软件开发大众化方面的乐观态度令人信服，但必须承认的是，这些进步也将重塑整个行业的固有面貌。随着技术变得愈发强大，许多工作岗位都将受到影响。软件开发人员必须适应这一波波冲击，专注于解决更高层次的创造性问题，同时将越来越多的日常任务交由AI智能体负责打理。</p><p>&nbsp;</p><p>Replit AI智能体目前已经以beta测试版的形式面向Replit Core及Teams订阅用户开放。</p><p>&nbsp;</p><p>可以肯定的是，未来软件工程团队的规模肯定会缩小，因为公司只需几秒钟就能创建原型和其他东西。这对印度 IT 公司和开发人员来说尤其令人担忧，他们可能很快就会陷入危机的边缘。 许多人，包括印度和其他地方的学生，都缺乏经济能力来支付Cursor 或 GitHub Copilot等服务的费用，这些服务通常每月收费 10 美元或类似金额。</p><p>&nbsp;&nbsp;</p><p>参考链接：</p><p><a href="https://www.maginative.com/article/tell-replits-ai-agent-your-app-idea-and-itll-code-and-deploy-it-for-you/">https://www.maginative.com/article/tell-replits-ai-agent-your-app-idea-and-itll-code-and-deploy-it-for-you/</a>"</p><p><a href="https://analyticsindiamag.com/ai-breakthroughs/replit-agents-are-here-to-replace-all-software-engineers/">https://analyticsindiamag.com/ai-breakthroughs/replit-agents-are-here-to-replace-all-software-engineers/</a>"</p><p><a href="https://techcrunch.com/2023/04/27/replit-funding-100m-generative-ai/">https://techcrunch.com/2023/04/27/replit-funding-100m-generative-ai/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/39LiyJdBGlRYXzJpUEM3</id>
            <title>字节跳动冯佳时：大语言模型在计算机视觉领域的应用、问题和我们的解法</title>
            <link>https://www.infoq.cn/article/39LiyJdBGlRYXzJpUEM3</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/39LiyJdBGlRYXzJpUEM3</guid>
            <pubDate></pubDate>
            <updated>Tue, 10 Sep 2024 09:04:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>近年来，大语言模型 (LLMs) 在文本理解与生成领域取得了显著进展。然而，LLMs 在理解和生成自然信号（例如图像，视频）等，还处在比较早期的探索阶段。为了深入探讨这一主题，我们在 <a href="https://aicon.infoq.cn/2024/shanghai/">AICon 全球人工智能开发与应用大会</a>" 上邀请到字节跳动研究科学家、豆包大模型视觉基础研究团队负责人冯佳时做主题演讲《<a href="https://aicon.infoq.cn/2024/shanghai/presentation/6035">大语言模型在计算机视觉领域的应用</a>"》。本次演讲将介绍字节跳动视觉基础研究团队在这个方向的探索与进展，包括 LLMs 在图像理解与视频生成上的阶段性结果。</p><p></p><p>我们将在 10 月 18 -19 日 <a href="https://qcon.infoq.cn/2024/shanghai/">QCon 上海站</a>"【<a href="https://qcon.infoq.cn/2024/shanghai/track/1721">AI 应用开发实践</a>"】专场，邀请各行业的优秀 AI 应用团队，分享在实际产品中成功应用计算机视觉、自然语言处理、个性化推荐、对话式交互等 AI 能力提升业务效率、优化用户体验的案例与最佳实践，共同探讨 AI 应用的未来发展方向。欲了解更多内容，可访问大会官网：<a href="https://qcon.infoq.cn/2024/shanghai/track/1721">https://qcon.infoq.cn/2024/shanghai/track/1721</a>"</p><p></p><p>以下为演讲实录（经 InfoQ 进行不改变原意的编辑整理）：</p><p></p><p>在过去三年中，大语言模型取得了显著的进展，已经发展成为一种功能强大的通用模型。这些模型已经阅读了互联网上的海量文本数据，其阅读量远远超过了我们人类一生中能够阅读的文本数据总量，因此积累了丰富的知识储备。然而，这些知识目前还局限于文本领域。如何将这些知识有效解码，并支持 AI 模型在物理世界和视觉世界中完成更复杂的任务，是我们在计算机视觉领域应用大语言模型时所面临的核心问题。</p><p></p><p>我目前就职于豆包大模型视觉基础研究团队，团队的主要职责是进行前沿技术的研究探索，同时在视觉多模态大模型的未来发展方向上进行尝试和探索。今天的分享，我将首先为大家提供一些背景知识，包括计算机视觉的定义以及我们目前关注的问题。随后，我将重点介绍豆包大模型视觉基础研究团队正在进行的两个研究项目，第一个项目是利用大语言模型帮助 AI 模型更好地理解视觉内容；第二个项目是关于 AIGC 的研究。最后会进行一个简单的总结，并对未来的研究方向进行展望。</p><p></p><p></p><h2>背景介绍</h2><p></p><p></p><h3>计算机视觉的基本问题</h3><p></p><p></p><p>计算机视觉是一个历史悠久的学科，也是人工智能研究领域中极为重要的一个分支。自 1950 年马尔出版《Vision》一书以来，视觉研究者们一直致力于解决视觉领域的核心问题。视觉问题由于其应用场景的多样性，可以抽象出多种不同的问题形式。如果我们对这些问题进行简化和抽象，可以将其归纳为三个基本能力：理解（识别）、检测和分割。</p><p></p><p>识别是最基本的能力，即给定一张图像或一段视频，要求模型能够识别并告知内容是什么。检测则在识别的基础上更进一步，要求模型能在复杂环境中定位出感兴趣的物体所在的位置。而分割则是在识别和检测的基础上的进一步深化，它要求模型不仅对图像内容进行全局理解，还要对图像中每个像素的细节进行理解，明确每个像素属于哪个物体，代表什么含义，这是视觉理解的终极问题。</p><p></p><p>除了理解能力之外，随着 AIGC 技术的发展，生成问题——即从文字描述到视觉内容的转换——也受到了广泛关注。自 2021 年以来，已经陆续有优秀的视觉 AIGC 模型发布，例如 Google 和 OpenAI 都推出了出色的图像生成模型。OpenAI 最近展示的 Sora 模型在视觉生成方面表现出色。此外，3D 生成模型也引起了人们的极大兴趣，尽管目前还处于早期阶段，但其在游戏、增强现实（AR）、虚拟现实（VR）以及构建完全虚拟的数字世界等方面具有巨大的应用潜力和想象空间。</p><p></p><p></p><h3>LLM 统一模型</h3><p></p><p></p><p>过去，在解决不同的视觉问题时，我们通常会开发或训练不同的专有模型，比如用于理解、分割、视频生成或 3D 生成等。然而，这种针对不同问题开发不同模型的方法已经落后于自然语言处理领域的研究进展。在自然语言理解方面，随着 GPT 等大语言模型的推出，我们已经进入了统一模型的时代。这种统一模型通过处理海量数据，理解文本数据背后的语法结构和包含的物理世界知识，能够根据用户询问和任务指定来完成各种任务。</p><p></p><p>例如，ChatGPT 和其他一些 AI 聊天软件已经能够处理各种文本工作。我们可以利用它们来修改邮件，或者撰写文章，甚至总结一本书的关键知识。这些软件的关键在于它们背后使用的是一个统一的模型，这个模型可以接受提示词，根据用户提供的不同提示词来定位任务解决方案，并给出相应的输出。</p><p></p><p></p><h3>视觉基础模型 ：生成与理解的统一</h3><p></p><p></p><p>作为计算机视觉领域的研究人员，我们认识到虽然历史上视觉领域的发展曾领先于语言领域，但过去两三年自然语言处理的发展实际上已经为视觉研究提供了很好的示范，并走在了前面。这给我们带来了两个重要的启示。首先，我们需要消耗和吸收海量的数据，这是大语言模型已经做到的，它们通过阅读大量文本数据，积累了丰富的知识。其次，我们应该追求一个统一的模型范式，即构建一个能够通过提示（prompt）来解决各种问题的模型。</p><p></p><p>如果从头开始搭建这样的视觉模型，我们面临许多挑战。例如，视觉的自监督学习问题尚未解决，同时视觉的多任务统一也还没有实现。这让我们思考是否可以采取一种中间形态的方法，充分利用已经包含丰富知识的大语言模型来解决一些视觉领域的关键核心问题，如图像理解或图像生成。</p><p></p><p></p><h2>基于 LLM 的图像理解</h2><p></p><p></p><p></p><h3>LLM 在图像理解中的应用与问题</h3><p></p><p></p><p>大语言模型在图像理解领域的应用是一个值得关注的研究方向。尽管目前存在许多优秀的多模态大模型，如 OpenAI 的 GPT-4v 或 GPT-4o，但这些模型在图像内容的理解上仍处于初级阶段。它们能够提供图像的全局描述或识别图像中的文字，但尚未达到像素级别的细节理解。人类在观察场景时，能够提出不同粒度的问题，从全局的场景描述到具体的细节问题，如场景中有多少人、他们的着装或表情等。这种多尺度的理解能力是当前多模态大模型尚未完全实现的，在这方面还有很大的发展空间。</p><p></p><p>现有的多模态模型架构存在一些局限性，主要体现在基本的架构设计上。这些模型通常以大语言模型作为基础，需要适应大语言模型的处理方式。大语言模型主要处理文本数据，因此，要让它们理解图像，就需要将图像通过编码器提取特征，然后通过映射层将视觉特征转换为语言模型能理解的文本特征。这样，当用户提出问题时，语言模型能够根据转换后的特征和问题提供文本输出。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/dc/dc657656e72aaed6f4eba107aeec9703.png" /></p><p></p><p>例如，如果询问上面图片中的内容，模型可能会回答说图片中有两只动物，一只是羊驼，另一只是美洲驼。这种基本架构存在一些问题，尤其是缺少对细节的理解。在图像特征提取阶段，大量信息已经丢失，而这些信息的丢失是无法通过后续的海量数据训练、有监督的精细调整或与人类偏好对齐的强化学习来恢复的。</p><p></p><p>经过训练的模型在回答关于图像全局信息的问题时可能表现得相当不错，但当被问及更具体的细节信息时，它可能就无法给出准确的答案。这是模型面临的第一个问题。</p><p></p><p>第二个问题是幻觉现象，这在多模态大模型中尤为常见。由于这些模型以语言模型为核心，它们已经接触过大量的文本数据。虽然我们不清楚具体的内容，但模型通过分析文本数据之间的分布和涌现模式，能够根据前面的词汇推断出后续可能出现的词汇。但这种推断完全在文本空间内进行，缺乏对参考图像的实际联系或基础，因此模型可能会产生一些多余的或错误的描述，这些描述可能与图像实际展示的细节完全不符。例如，如果将同一张图片多次输入到多模态模型中，模型可能会错误地描述图像中的某些细节，如描述上图中的美洲鸵为红色，即使实际上并非如此。</p><p></p><p></p><h3>带定位能力的 LLM 及相关工作</h3><p></p><p></p><p>为了解决这些细节理解和幻觉问题，并进一步扩展大语言模型的能力，使其能够与物理世界进行可靠和准确的交互，我们需要让大语言模型具备一定的定位能力。这种定位能力可以是对图像上特定区域的定位，也可以是对周围 3D 环境的定位。例如，在自动驾驶或具身智能领域，我们通常将大模型视为机器人的"大脑"。在进行推理时，我们希望这个"大脑"能够参考周围实际的物理环境信息。比如，如果问它下图中水龙头或放水果的托盘在哪里，我们希望模型不仅能告诉我们具体位置，还能指导机器人去拿取或进行相应的操作。这就要求大语言模型扩展出一定的定位能力，以便更好地与物理世界互动。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/51/519dda9bc4d0f8953d4ee7dd178d9b14.png" /></p><p></p><p>在计算机视觉的研究领域，自去年以来，许多研究人员已经开始关注如何拓展大语言模型的能力，使其不再局限于文本空间，而是能够与物理世界进行可靠交互。在这一领域，有许多杰出的工作，我这里举 LISA 团队的研究为例。他们开发的方法赋予了语言模型推理和定位的能力，能够识别图像中的关键区域和物体。例如，在询问图像中哪种食物的维生素含量最高时，模型不仅能回答出是橙子，还能指出橙子在图像中的具体位置。这种定位能力不仅提高了语言模型的准确性，还有助于减少对某些问题的幻觉。</p><p></p><p>LISA 团队的基本思想是通过让大语言模型的输出不仅限于文本 token，还能输出代表图像中物体位置的特殊 token。为了实现这一目标，他们采用了图像预处理技术，通过不同尺度的分割来识别图像中的物体。他们使用的是 Meta 公司的“segment anything”模型，简称 SAM 模型。SAM 模型虽然功能强大，但处理一张图片可能需要十几到二十几秒的时间，这显著增加了模型理解图像内容的推理延迟。此外，该模型架构还存在一些限制。目前，它每次只能定位图像中的一个物体。如果需要定位图像中的多个物体，当前的模型架构就无法满足。这些挑战表明，在将大语言模型与物理世界交互的能力拓展方面，还有许多工作要做。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8f/8f95c94841f614fb896fc2024130b335.png" /></p><p></p><p></p><h3>我们的方案：PixelLM</h3><p></p><p></p><p>针对目前学术界在大语言模型与物理世界交互方面的一些方案，我们发现它们存在效率不高、实用性有限，以及能力上的缺陷，比如只能定位单个物体而无法同时定位多个物体。为了解决这些问题，我们提出了 PixelLM 模型架构，这是一个像素级别的大语言模型，它不仅高效，而且具备多物体定位的能力，能够进行推理和分割，减少幻觉回答的发生。</p><p></p><p>PixelLM 的基础模型架构关键在于物体分割码本的设计和轻量级解码器的引入。 在不改变原有大语言模型架构的基础上，我们增加了这两个设计，使得模型能够实时高效地对分割结果进行解码，并在图像上提供定位和分割的结果。大语言模型的输出也经过了改造，不仅包括文本 token，还包含代表物体分割结果的特殊 token。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/26/26668105863bca72f40c7cb0e68d438d.png" /></p><p></p><p>我们首先使用一个强大的图像编码器来解决图像特征提取时的信息损失问题，并进行多尺度特征提取，而不仅仅是全局特征。这里我们使用了 OpenAI 的 CLIP 模型来提取图像的全局特征。但为了同时识别不同尺度的物体，我们对图像进行了缩放和切分，然后通过同样的特征提取模型来提取不同尺度的特征。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/0c/0cf4c7a4ed380060497ed9cb210a4795.png" /></p><p></p><p>接下来是分割词表的设计。为了克服之前工作只能定位单个物体的限制，我们设计了多组分割词表或分割 token，每组 token 代表不同的尺寸，组内每个 token 代表不同的物体。通过预测结果的融合，我们能够成功地定位图像中的多个物体和不同尺寸的物体，例如同时定位下图右侧日轨的托和指针。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/71/71d70c773fe21f36b450b11141c51f43.png" /></p><p></p><p>我们提出了一个轻量化的解码器设计，这个设计特别注重效率和简洁性。在这个设计中，我们采用了一个结构简单但功能强大的自回归解码器，它内置了注意力机制（attention）。这个解码器的工作流程是逐步进行的：首先，它解码出图像中一个物体的分割结果，然后利用这个结果作为指导，继续解码下一个物体的分割。这个过程会持续进行，直到图像中所有关键物体都被成功定位和分割出来。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f9/f94fff08c54355fa60f2aa8bf413eb85.png" /></p><p></p><p>在训练方法上，我们采取了一种综合策略，旨在保持原有语言模型能力的同时，增强模型在分割定位方面的性能。为此，我们在训练过程中加入了一些专门针对分割定位任务的损失函数。这样的设计确保了模型在经过训练后，不仅能够准确地定位和分割出图像中的关键物体，而且还能保持大语言模型的核心能力，包括对语言的深入理解、逻辑推理能力，以及丰富的常识。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7d/7d07c3f8d3e76a0eeb516607d72d5d3f.png" /></p><p></p><p>大语言模型或 AI 大模型的发展离不开算力和海量数据的支持，数据的构建对于提升模型能力至关重要。为了训练具备推理、定位和分割能力的像素级大语言模型，我们需要相应的训练数据来指导模型学习和执行这些操作。然而，目前并没有现成的数据集可以直接使用，因此我们需要探索如何构建这样的数据。</p><p></p><p>在计算机视觉领域，图像分割是一个长期研究的方向，学术界已经积累了大量的相关数据，每张图像中都包含了多个物体及其对应的分割标注。我们考虑是否可以利用这些带有分割标注的图像作为种子数据，进一步构造出针对图像内容的问答数据。这些问答数据的答案中应包含物体信息及其分割结果。</p><p></p><p>我们的具体做法是，将已有分割标注的图像输入到大语言模型中，让模型针对图像提出问题，并结合关键类别信息，如图像中包含的物体类型和场景。例如，如果图像中有一只猫、一台电脑和一张床，我们可以询问大语言模型：“这张图像里有一只猫、一台电脑和一张床，你能想到什么问题？能构造出什么样的问答？”大语言模型会根据图像中的物体信息生成问答对。</p><p></p><p>通过这种方式，我们收集并构造了一个新的数据集，称为 MUSE。我们希望 MUSE 数据集能作为一个初始数据集，帮助研究人员开展更多关于大语言模型或多模态大模型的研究，从而提升模型在物理世界中的定位能力。这样的数据集将为模型提供丰富的学习和推理材料，使其能够更好地理解和与物理世界交互。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/53/533f1a074cffa864b662a6b5078cff95.png" /></p><p></p><p>在进行模型性能评测时，尽管涉及的数字众多，但我们 可以重点关注两组关键数据：TFLOPs 和分割定位的准确率。TFLOPs 是衡量模型算力的一个指标，它反映了模型的延时和效率。在我们的模型与 LISA 模型的比较中，我们模型的能力更强，但运算量却减少了一半，显示出更高的效率。此外，我们的模型在分割定位的准确率上也有显著提升，从 LISA 模型的 9.6 提升到了 37.7。我们的模型现在已经达到了一个初步可用的状态，目前团队仍在不断地迭代数据构造和模型能力，以期进一步提升模型的表现和应用范围。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/6b/6b5f7a8e3fc202ea91175da1483631af.png" /></p><p></p><p>一个具备定位能力和对物理世界参考能力的大语言模型在应用层面拥有非常广阔的前景。它不仅可以进行多物体分割，还能进行推理、问答，甚至与用户进行交流和聊天。这样的模型可以应用于多种场景，具有极高的灵活性和实用性。</p><p></p><p>我们的模型和数据集已经开源，可以在网上下载并试用。用户可以利用我们的数据集进行模型的迭代和进一步的研究开发。</p><p></p><p>如果一个模型已经具备了基本的定位能力，那么我们接下来期待的是它能在物理世界中进行更深层次的交互，从物理世界中学习知识。这意味着模型将能够将其从互联网文本数据中学到的知识与现实世界的物理情况相对应，并通过不断的反馈来提升自身的能力，这也是我们下一步研究的重点方向。</p><p></p><p></p><h2>基于 LLM 的图像视频生成</h2><p></p><p></p><p>大语言模型（LLM）在图像和视频生成方面的应用，尤其是视频生成，已经成为一个备受关注的研究领域。许多研究团队已经发布了他们的视频生成模型，这些模型在模拟物理世界、动作和物理规律方面已经达到了非常逼真的水平，在光影效果和三维世界结构的构建上也取得了显著的成果。</p><p></p><p></p><h3>视频生成模型面临的挑战</h3><p></p><p></p><p>但视频生成模型目前还面临一些挑战。首先是视频的一致性问题。尽管生成四五秒的视频看起来效果不错，但当生成更长的视频，比如一分钟时，就会出现人物和环境的一致性问题。人物的长相或环境可能会随着视频的进行而发生不自然的变化或扭曲，这是需要解决的关键问题。其次，是用户友好程度的问题。目前的创作界面通常需要用户输入一段文字来生成视频，但如果用户希望得到一段复杂且表现力强的视频，就需要提供非常详细的文字描述。但长篇幅的描述可能会超出模型的理解能力，导致生成的视频内容与描述不匹配。此外，文字描述难以对视频进行精细控制，比如精确控制人物的姿势变化。视频生成的另一个挑战是视频的表现力或演技。我们希望生成的视频不仅在视觉上逼真，还要具有一定的表现力，人物动作要富有变化，避免单一和刻板。</p><p></p><p>目前的视频生成方案流程可能并不完全合理。用户需要设计一段复杂的文字描述，然后依赖模型生成视频，结果往往像“抽奖”一样不确定。相比之下，专业视频制作人员在创作视频时，会首先定义角色，构思故事情节，编写剧本和分镜，然后拍摄不同场景的片段，并最终进行剪辑。这种创作过程与目前视频生成模型的工作方式存在明显差异，我们在视频生成技术的发展中，需要更多地考虑如何模拟这种专业的创作流程，以提高生成视频的质量和可用性。</p><p></p><p>如果我们根据专业视频制作的流程重新设计视频生成的范式，AI 模型是否能够胜任这一任务呢？</p><p></p><p>在角色定义阶段，我们可以利用大语言模型来定义角色的性格和形象，然后使用图像生成模型根据语言模型的描述来创建具体的形象。目前，AI 模型在这方面的能力是足够的。接下来是剧本和分镜的创作，大语言模型同样可以完成这项工作。这里关键在于如何生成角色一致的关键片段，并确保这些片段能够合成具有高表现力的长视频。这正是我们需要解决的重点问题。为了应对这一挑战，我们正在研究一个名为 StoryDiffusion 的模型。我们希望这个生成模型能够创作出具有表现力和吸引力的故事，而不是仅仅模拟一些刻板的模式，生成缺乏锐利度的视频。</p><p></p><p></p><h3>我们的探索：StoryDiffusion</h3><p></p><p></p><p>StoryDiffusion 模型解决了两个问题：提供了更友好的交互方式，允许用户通过定义角色、创作剧本来进行视频内容创作；同时引入了两项关键技术，一是提高角色的一致性，二是增强表现力。</p><p></p><p>以 StoryDiffusion 的效果为例，我们可以使用角色定义模型，比如图像生成模型，输入一个角色，比如 AI 领域的著名研究科学家 Yan LeCun。我们可以得到他的形象，然后定义一个主题，比如 Yan LeCun 去月球探险。将这个主题交给大语言模型，它将生成一段剧本。这个剧本和角色形象再输入到 StoryDiffusion 中，它就能生成连续的画面，进而合成视频。在这些画面中，角色的长相保持严格一致，同时表情也很丰富，从而完整地描述了剧本和故事。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7d/7dba6bddfcbec0322b0d9f792d06ab11.png" /></p><p></p><p>StoryDiffusion 模型的设计包含两个关键点：一致性注意力和表现力。</p><p></p><p>首先，一致性注意力的设计基于一个简单的理念，即在单独生成每张图片时，随机性可能导致角色形象的变化。如果同时生成多张图像，并使用同一个随机种子，这种随机性就会减少。在多张图像同时生成的过程中，通过互相参考，可以确保生成的人物形象保持一致，包括长相和衣着，即使动作和表情有所不同。这种一致性注意力机制确保了人物形象的连贯性。</p><p></p><p>其次，表现力的提升关键在于运动的丰富性。传统视频生成模型通常在像素空间进行插帧来生成运动，但这往往导致运动幅度小和模式单一。StoryDiffusion 模型通过将关键帧送入语义空间进行插帧，然后再映射回像素空间，利用语义空间包含的丰富信息来增强运动的幅度和表现力。这样，生成的人物不仅表情丰富，动作幅度和多样性也得到提升，同时保持人物形象的严格一致性。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b9/b98d75e7f28d949b67b09ab52c0b14e3.png" /></p><p></p><p>通过这种运动生成模式，StoryDiffusion 能够将多个短视频进行插帧和拼接，生成更长的视频，如网站上展示的一分钟或两分钟的视频。定量评估表明，StoryDiffusion 在角色一致性和视频生成质量方面，相比同期的其他模型和方法都具有更好的效果。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ad/ad4001db71450c8e78859504d4912351.png" /></p><p></p><p>StoryDiffusion 背后的理念是先定义故事的角色，然后生成相应的故事。这一理念已经在即梦 AI 的故事模式中得到体现，用户可以通过图像生成模型或上传自己的图像来定义角色，再利用故事生成模型来创作连续的故事。</p><p></p><p></p><h2>总结展望</h2><p></p><p></p><p>在演讲前面的部分，我提到了我们在提升模型视觉理解能力和增强与物理世界交互方面的一些初步研究和探索。这些探索包括像素级别理解的大语言模型，以及利用大语言模型改造视频生成创作流程的尝试。虽然这些研究目前还处于初级阶段，但我们将继续迭代和优化模型。</p><p></p><p>我们接下来关注的问题之一是构建一个统一的理解和生成模型，模仿语言模型的统一架构。在理解方面，我们已经取得了一定的进展，生成方面也是如此。但如何将理解和生成统一起来，尤其是在不同粒度和语义级别的特征融合与模型复用方面，仍是一个重要问题。</p><p></p><p>完成这些研究后，我们的目标是实现语言模型和视觉理解或生成模型的充分融合，创建一个真正具备原生多模态能力的模型。这样的模型将能够与物理世界进行交互，并通过与环境的互动不断学习和迭代自身能力。</p><p></p><p>目前，语言模型在语言能力上可能已超过普通人，因为它们的阅读量远超人类。但在物理世界的学习效率上，例如识别物体或学习某些操作，这些模型仍然依赖于大量训练数据，而不是像人类那样学习。因此，开发更高效、更类似人类的智能学习方法，充分利用大语言模型已经从文本中学到的物理世界知识，提高对现实世界任务的学习效率，并增强交互的可靠性，将是未来计算机视觉领域研究的重点，也是我们特别关注的研究方向。</p><p></p><p>演讲嘉宾介绍</p><p></p><p>冯佳时，字节跳动研究科学家，现任字节跳动豆包大模型视觉基础研究团队负责人。曾任新加坡国立大学电子与计算机工程系助理教授，机器学习与视觉实验室负责人。研究方向包括深度学习与计算机视觉。目前主要研究多模态基础模型、生成模型、3D 建模。曾获得麻省理工科技评论 35 岁以下创新者（亚洲），ACM MM 最佳学生论文奖，ICCV TASK-CV 讨论会最佳论文奖，CVPR2021 最佳论文奖提名。曾担任 CVPR、ICML、ICLR、NeurIPS 等会议的领域主席。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/qJPbo2exxTRb3mRWIgrA</id>
            <title>36 亿融资“造假”被揭穿！挣钱太难了，前苹果 AI 工程师 3 年打造的“欧洲 OpenAI”宣告退出模型竞赛</title>
            <link>https://www.infoq.cn/article/qJPbo2exxTRb3mRWIgrA</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/qJPbo2exxTRb3mRWIgrA</guid>
            <pubDate></pubDate>
            <updated>Tue, 10 Sep 2024 02:05:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>整理 | 华卫、核子可乐</p><p></p><p>德国 AI 初创公司 Aleph Alpha 曾被认为是 OpenAI 的潜在欧洲竞争对手，去年还筹集了超过 5 亿美元。然而，近日 Aleph Alpha 开始将其商业重点从开发大型语言模型转向生成式 AI 操作系统和咨询服务。该公司表示，市场变化和来自科技巨头的激烈竞争是其转向的原因。</p><p></p><p>对 Aleph Alpha 来说，不再执着于 AI 模型竞赛将使其能够在无需为维持尖端人工智能模型支付巨额费用的情况下追求增长。然而，这不仅反映出大模型当前的市场挑战正在加剧，也表明了资金雄厚的科技巨头在 AI 开发中日益增长的主导地位，最近 Character.AI 、Inflection 等人工智能初创公司在创始人加入大型科技公司后都改变了发展方向。</p><p></p><p></p><h1>创始人曾在苹果工作三年，如今开始质疑大模型产品</h1><p></p><p></p><p>在被问及欧洲科技圈的从业者们最抱希望的 AI 企业时，出现最多的名字当数 Mistral。这是一家法国初创公司，已经筹集到 1 亿美元但尚未发布任何产品；紧随其后的就是 Andrulis 创立的 Aleph Alpha 了。</p><p></p><p>尽管持怀疑论的从业者们一直质疑这家公司到底有没有能力跟谷歌和 OpenAI 同台竞争，但欧盟有许多人都希望 Aleph Alpha 能够抵消美国在这一技术领域的主导地位。虽然 Aleph Alpha 首席执行官 Jonas Andrulis 强调他的公司不是什么“民族主义项目”——毕竟该公司也有不少美国员工，但他似乎也很乐意成为欧洲 AI 力量的先锋和代表，声称“我个人非常关注要如何为欧洲做出卓越的技术贡献。”</p><p></p><p>现年 42 岁的 Andrulis 曾经在苹果公司作为高级 AI 研究工程师工作过三年，主要从事 AI 研究并于 2019 年离职，理由是探索这项技术在科技巨头以外的潜在应用。此前，他还是机器学习和计算机视觉公司 Pallas Ludens 的创始人兼首席执行官，该公司后被苹果收购。之后，他在德国西南部城市海德堡成立了 Aleph Alpha，主要目标就是开发大语言模型，并在两年后成功筹集到 2700 万美元。</p><p></p><p>据 Aleph Alpha 介绍，该公司的客户（从银行到政府机构）在使用 Aleph Alpha 的大语言模型来撰写财务报告、将数百页长度的文档提炼成简明扼要的概述，同时根据企业客户的运营情况构建业务聊天机器人。</p><p></p><p>Aleph Alpha 的模型能够支持用德语、法语、西班牙语、意大利语和英语进行交流，其训练数据包括欧洲议会发布的大量多语言公共文件。而强调其欧洲血统的还不仅仅在于该公司 AI 方案所支持的语言种类，其对透明决策流程的强调以及对 AI 系统“幻觉”问题的重视和解决，也有着相当强烈的欧洲特色。欧盟 AI 行业认为，相较于美国企业，欧洲公司对于隐私和歧视等问题往往更加敏感。</p><p></p><p>但事实上，有不少人都在怀疑 Aleph Alpha 的底层技术是否足够先进，能否承载起欧洲打造 AI 巨头的希望。未来社会智库欧洲人工智能治理主管 Nicolas Moës 表示，“任何接触过多种语言模型的朋友都会注意到，Alepha Alpha 的模型质量绝对达不到一线水平。”前 OpenAI 研究员、人工智能技术顾问 Matthias Plappert 也表示，在用于证明新 AI 模型有效性的标准化测试中，Aleph Alpha 的得分并没能超越其美国竞争对手。</p><p></p><p>现在，该公司正在从大型语言模型转向专注于 Pharia AI，这是一个面向企业和政府客户的“生成式 AI 操作系统”，旨在帮助企业和政府机构企业和公共机构快速扩展 AI 项目。该系统由几个组件组成：用于构建专业知识的 Pharia Catch、用于创建特定于应用程序的 AI 系统的 Pharia Studio、用于操作和扩展的 Pharia OS 以及作为用户界面的 Pharia Assistant。</p><p></p><p>这种转变使 Aleph Alpha 能够在无需为维持尖端人工智能模型支付巨额费用的情况下追求增长。对于这一转变，Andrulis 进一步解释道：“世界改变了 。仅仅拥有欧洲大模型产品作为一种商业模式是不够的，这并不能证明投资的合理性。”</p><p></p><p></p><h1>商业经营不善，并涉嫌融资“造假”</h1><p></p><p></p><p>Aleph Alpha 之所以开始退出 AI 模型竞赛，似乎主要还是源于其正面临的各种商业模式挑战，包括未能实现销售目标和融资结构受到的质疑。</p><p></p><p>据知情人士透露，该公司现在拥有约 200 名员工，声称 2024 年总收入将达到 2000 万欧元，2025 年将达到 7000 万欧元。但该公司 2023 年预计销售额为 590 万欧元，实际销量却不到 100 万欧元。</p><p></p><p>去年 11 月，Aleph Alpha 宣布完成 B 轮融资，公司从一个由 7 名新投资者以及前几轮现有投资者组成的财团获得超过 5 亿美元（36 亿元）。然而，这一融资数字却似乎有“为炒作”估值而夸大不少的嫌疑。</p><p></p><p>据该公司介绍，Aleph Alpha 在一轮融资中筹得总计 4.7 亿欧元，按签约日汇率计算相当于 5 亿美元以上。此番融资由三部分组成：1.1 亿欧元作为纯股权融资，3 亿欧元作为研究资金，余下 6000 万欧元则以订单承诺的形式提供。其中，用作研究资金的 3 亿欧元将全部用于新成立的研究子公司 Aleph Alpha Research，且这些资金不附带任何条件。</p><p></p><p>但在今年 6 月，德国记者 Thomas Knüwer 对该公司于公布的这项总额 5 亿美元的融资计划表达了担忧：“根据我在过去几个月收集的所有信息，我认为这轮 5 亿美元的融资并没有发生。他们的融资额要低得多，只有 1 亿美元——至少在 99% 的融资定义中是这样。”</p><p></p><p>Knüwer 的疑虑主要基于以下几点。首先也最值得注意的是，一位有权查阅条款清单的消息人士向他反映，投资者以约 1 亿美元的价格掌握了该公司约 20% 的股份，意味着 Aleph Alpha 的估值约在 5 亿至 6.25 亿美元之间。如果 Aleph Alpha 真的完成了 5 亿美元的融资，并出售了 20% 的股份（这是 B 轮融资的惯例），那么其估值将达到 25 亿美元。</p><p></p><p>其次，此前 Aleph Alpha 在谈及这轮融资时多次提到财务“贡献”，而 Knüwer 认为这样的表述太过模糊。英文版本则明确提到“融资超过 5 亿美元”，数字更加准确但根据公司估值来看似乎又不太现实，具体恐怕取决于如何定义“资金”二字。而且可以看到，这 5 亿美元的数字中包含销售承诺（即“预消费许可”）、研究合同及“业务发展”承诺，这与用股份换取资本的典型融资轮定义并不一致。</p><p></p><p>Aleph Alpha 在德语版新闻稿中将这 5 亿美元描述为包含许可证购买在内的“投资方案”，而 Knüwer 则正确将其定义为收入，而非持股形式的投资。在被问及这个问题时，Aleph Alpha 方面拒绝对 Knüwer 的观点发表置评。该公司称此前已告知 Knüwer，除了公开发表关于融资轮的声明之外，其不会做出任何进一步评论。</p><p></p><p>在 Knüwer 看来，这种对融资总额的过度夸大可能会损害德国 AI 行业的长期声誉，造成与互联网泡沫时期相似的负面影响。但也必须承认，AI 企业在合作当中以实物形式收取捐赠也并不罕见。以美国最大的两家 AI 厂商 OpenAI 和 Anthropic 为例，他们在创业过程中就先后享受过由微软、谷歌和亚马逊云科技提供的云计算资源，这反过来又帮助三大巨头发展了云业务并拉高股价——也有人认为这种商业行为存在争议，相当于建立起一台自我驱动的炒作机器。可即使是这样，这类资源通常也不会被纳入传统融资轮的计算范围。</p><p></p><p>而 Aleph Alpha 明显是找到了新的“突破口”，把 1 亿美元融资、4 亿美元潜在营业额这个对媒体和该公司自身都不够有吸引力的成果，转化成了“5 亿美元融资”的夸张新闻。Aleph Alpha 选择这条路，很可能是为了让其体量看起来比实际更大，在紧跟行业炒作风口的同时在国际社会上展现一个可观的数字。可即使采用这种充满争议的算法，其融资额仍然远远落后于中美两国的水平。</p><p></p><p></p><h1>大模型市场面临严峻考验</h1><p></p><p></p><p>大型语言模型市场竞争激烈，由于开发和运营成本高昂，目前尚未盈利。但与此同时，却有越来越多具有类似功能的模型正在争夺相同的客户和用例。几个月来，行业内的人们一直在关注两个因素：效率和价格。虽然大模型用户的使用成本在急剧下降，但业内不少公司的开发成本却仍居高不下。</p><p></p><p>今年 7 月，OpenAI 推出能够提供更高性能的小型模型 GPT-4o mini，价格比此前的版本降低 60% 。而其他大模型供应商和开源模型也正在市场上争夺高效、小型和低成本的模型市场，微软推出 38 亿个参数的 Phi-3-mini，Anthropic 等有时会通过 API 提供更好或更便宜的模型。</p><p></p><p>就连国内的大模型市场，今年也不断爆发了“价格战”。5 月，字节跳动旗下的云服务平台火山引擎宣布其豆包通用模型、阿里云的通义千问系列 9 款大模型相继降价，随后百度文心大模型系列中的 ErnieSpeed 和 ErnieLite、科大讯飞旗下的讯飞星火 SparkLite 模型及智谱 AI 的 GLM-4-Flash 模型又接连宣布对用户免费开放。</p><p></p><p>有外媒指出，花费数百万美元开发和训练的人工智能模型，可能在发布后短短几周甚至几天就变得过时和毫无价值。而自 GPT-4 以来，还没有一家人工智能公司成功开发出具有显著优势的模型。</p><p></p><p>现在的关键问题是，大模型供应商是否能够通过大幅提升推理能力、扩大现有业务领域和开辟新业务领域来摆脱这场激烈的竞争。否则，鉴于 AI 研究、开发和运营的高成本以及激烈的竞争，大模型市场可能很快就会面临一个严峻的考验，无法达到投资者设定的高估值。</p><p></p><p>参考链接：</p><p></p><p><a href="https://the-decoder.com/aleph-alpha-quits-ai-model-race/">https://the-decoder.com/aleph-alpha-quits-ai-model-race/</a>"</p><p></p><p><a href="https://www.bloomberg.com/news/articles/2024-09-05/the-rise-and-pivot-of-germany-s-one-time-ai-champion">https://www.bloomberg.com/news/articles/2024-09-05/the-rise-and-pivot-of-germany-s-one-time-ai-champion</a>"</p><p></p><p><a href="https://www.wired.com/story/aleph-alpha-europe-openai/">https://www.wired.com/story/aleph-alpha-europe-openai/</a>"</p><p></p><p><a href="https://www.indiskretionehrensache.de/2024/06/das-maerchen-von-der-500-millionen-finanzierungsrunde-bei-aleph-alpha/">https://www.indiskretionehrensache.de/2024/06/das-maerchen-von-der-500-millionen-finanzierungsrunde-bei-aleph-alpha/</a>"</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/2ENWzczB8KpBVcyYEOCf</id>
            <title>要低代码，不要低能力，低代码工具能否成为企业增效神器？</title>
            <link>https://www.infoq.cn/article/2ENWzczB8KpBVcyYEOCf</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/2ENWzczB8KpBVcyYEOCf</guid>
            <pubDate></pubDate>
            <updated>Mon, 09 Sep 2024 09:57:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在经济形势复杂，市场竞争激烈的当前环境中，众多企业都面临着 IT 预算下降、大型项目暂停或延后的局面。但企业信息化建设与降本增效的需求依旧迫切，因而很多管理层都将目光投向了项目投入较低的低代码产品，希望这类工具能够适应紧缩的 IT 预算，以合理的成本获得令人满意的收效。</p><p></p><p>基于上述背景，IDC 预计中国低代码工具的市场规模将在 2027 年突破 100.4 亿人民币，年复合增长率达 32.3%，增势令人瞩目。而从产品供应侧看，中国低代码厂商数量众多，呈多元化发展，包括独立低代码厂商、企业应用软件商、SaaS 厂商、云平台厂商等供应方都加入了这一赛道，令市场呈现一片繁荣景象。但与此同时，低代码工具在实践中也暴露出了许多问题，诸如产品烟囱化、使用门槛不够低、业务效果不尽人意等，给低代码市场的前景蒙上了一层阴影。如何让低代码工具真正发挥实效，为企业带来可观的投资回报率，是研发厂商面临的主要挑战。</p><p></p><p>在一众国产厂商中，飞书开发低代码产品的经历可谓“厚积薄发”。2023 年 3 月，飞书发布了低代码平台共创版；之后经过近两年的打磨与上百家客户深度共创，在刚刚过去的 2024 年飞书未来无限发布会上，这款行业能力最完备的低代码产品终于正式发布。本期《极客有约》，我们邀请到了飞书低代码平台负责人周洲做客直播间，围绕飞书打造的低代码平台展开话题。飞书为何在当前时点，切入低代码赛道飞书精心打造的低代码平台，又能为行业带来哪些不一样的新鲜空气？在实践中，飞书低代码平台帮助企业获得了怎样的收益，积累了哪些值得参考的经验？本次访谈将一一深入探讨。</p><p></p><h4>投身低代码赛道，飞书的思考与期待</h4><p></p><p></p><h5>霍太稳：低代码不是一个新鲜话题，很多厂商已经在这个领域耕耘多时。那么为什么飞书选择了当前这样一个时间点切入低代码赛道？背后有怎样的逻辑？</h5><p></p><p></p><p>周洲：当前市场上许多低代码产品与企业需求的匹配度并不理想，市场迫切需要更符合需求的工具来服务客户。飞书始终从用户的角度出发，致力于研发产品，旨在加速组织的进化，提升工作效率。飞书的 People、飞书文档和飞书项目等产品均围绕这一目标进行设计。飞书的低代码平台也经过了长时间的开发，之所以之前没有推出，是因为飞书希望更深入地了解客户的问题，以确保产品更加成熟。尽管推出时间相对较晚，但从市场角度来看，当前的时机仍然非常适合。</p><p></p><h5>霍太稳：当下低代码工具在国内市场的现状如何？普及过程中遇到了哪些挑战？</h5><p></p><p></p><p>周洲：Gartner 的全球调研显示，受访企业中约有一半的企业已经采用了低代码技术。然而，在国内，可能只有大约一半的企业听说过低代码，实际使用低代码技术的企业比例较小，深度用户更是稀缺，整体处于相对早期的发展阶段。由于低代码市场尚未成熟，客户对这类工具的预期较为模糊。许多客户一方面希望低代码平台的门槛较低，使得 IT 和业务人员都能轻松上手，另一方面又希望它能够解决企业较为核心和复杂的需求。国内低代码厂商的产品大致可以分为两类。一类是门槛较低的产品，但处理的业务相对简单；另一类能够解决深度挑战的产品，其设计通常基于厂商成熟的企业服务平台，以 aPaaS 的形式交付给企业。然而，这类产品往往设计复杂，使用起来也并不容易。</p><p></p><h5>霍太稳：飞书系统在很多企业的数字化转型中扮演着很重要的角色，甚至企业的整体业务流程都会建立在飞书基础上。那么飞书低代码平台在这样的大系统中扮演了怎样的角色？</h5><p></p><p></p><p>周洲：飞书的使命是助力组织及其成员提升创造力和效率。最初，飞书通过办公协同功能进入市场，但并未将其业务局限于此领域。在服务客户的过程中，飞书不仅提供产品，还扩展了服务形态，例如最近流行的飞书效率顾问服务。飞书的业务涵盖四个主要方向：组织、协同、项目和业务构建，这些方向围绕企业运作的核心要素展开。飞书的低代码平台主要服务于企业 IT 人员，同时也触及具备开发意识的业务人员。飞书的目标是帮助客户的 IT 或业务人员构建复杂的业务系统，加速企业的数字化转型进程。</p><p></p><h4>快速释放企业生产力：飞书低代码平台的能力、门槛与优势</h4><p></p><p></p><h5>霍太稳：与竞品相比，飞书低代码平台有哪些独特优势？它如何帮助企业优化决策和流程？</h5><p></p><p></p><p>周洲：低代码产品的本质是将研发的各种环节抽象成一些服务，再通过可视化、配置化等方式提供给用户，从而减少重复工作、提升效率。所以低代码产品抽象和包装的水平决定了最终的效率或能力的上限，是主要的差异点。</p><p></p><p>我们与友商的一点不同之处在于我们会花更多时间来打磨产品，而非专注于交付或定制工作上。当初我们要在字节内部解决上万人组织的办公协同挑战，于是尝试了市面上很多工具，结果都不能满足需求，因此才做了飞书这款产品。</p><p></p><p>飞书低代码的缘起与之相似，我们最早希望将它打造成一个基座，希望基于它能够承载非常复杂的、上万人能够使用的系统。其他厂商不会像我们一样从一开始就要构建这么完备的状态。项目立项后组织内部提出了更高的需求，就是希望这个平台能够支撑字节内部上万名研发工程师未来的研发模式升级，至少作为他们高效开发的首选。此外，包括 HR 团队、其他 IT 部门也有使用这个平台的需求，最后这个平台就要做到门槛又低，能力又非常强大。</p><p></p><p>目前字节内部在飞书低代码平台上开发的应用有上万款，其中上千款处于活跃状态。平台每天新增的数据有 6000 多万条，有超过 2000 万流程跑在它上面，可以说这个平台在字节内部的应用还是比较深入的。</p><p></p><h5>霍太稳：当用户在低代码平台上拖拽形成一套系统后，它背后是如何解决数据交互问题的？飞书低代码平台有什么特别的封装吗？它与老系统之间的集成是如何实现的？</h5><p></p><p></p><p>周洲：我们有两种解决方案。首先，对于希望构建独立或轻量级系统的用户，我们提供了一种类似云数据库的服务。用户可以在界面上指定需要生成的对象，我们最近还尝试利用 AI 技术来辅助解决问题。例如，如果用户需要创建一个 CRM 系统，飞书低代码平台将帮助他们生成客户、线索和商机等对象，使用户在构建界面或使用流程时能够直接访问这些数据。由于这是一个云数据库，我们采用了如 Serveless DB 等较新的技术，这样开发工程师，尤其是前端工程师，就不需要担心如何建立表、创建索引、扩展和缩容等问题。</p><p></p><p>第二种，其次，对于企业中已经存在的大量场景，我们通过集成的方式，使得搭建的平台能够访问以前系统的 API 和数据库，用户可以继续复用这些资源。飞书在两年前推出了飞书集成平台，这是一款 iPaaS 产品，我们的低代码平台中的集成能力都是由这个平台提供的。这意味着我们有一款经过两年多打磨的产品，具备与企业数据交互和连接的能力，可以很好地处理集成需求。</p><p></p><p>在系统集成方面，我们投入了大量精力，从数据到页面组件再到服务，全面考虑了集成的各个方面。由于字节内部拥有众多现有系统，低代码平台的使用自然涉及到与这些系统的交互。两年前，我们构建了一个集成平台，旨在连接企业内部的各个系统，并将这一能力整合到低代码平台上。</p><p></p><p>例如，在开发新页面时，考虑到旧系统界面可能不够现代化，且许多系统缺乏移动端支持，如果这些系统能提供数据库连接或 API 接口，我们便能在低代码平台上迅速构建新的界面。近期，我们还在探索一种新方法，用户只需上传类似 Figma 的图片，并告知平台原始数据的位置，平台便能利用 AI 技术帮助用户完成许多前置工作，从而加速新界面的创建。</p><p></p><h5>霍太稳：用户使用飞书低代码平台需要哪些前置条件？</h5><p></p><p></p><p>周洲：在低代码平台的使用上，程序员和业务人员的门槛有所不同。程序员可以轻松上手我们的平台，因为他们具备开发和工程思维，能够理解系统中数据、界面、流程及其协作关系。而业务人员则需要具备一定的开发和工程思维，大致了解系统运作的流程，包括数据处理、界面展示和流程管理等方面。例如，搭建一个系统时，业务人员需要知道系统由数据页面和逻辑构成。满足这些条件的用户都可以成为我们的目标受众，但他们的能力差异可能会影响他们在平台上构建的项目复杂度。飞书低代码平台在设计上区分了不同层次，底层构建为 PaaS 能力，使得专业开发者可以通过平台提供的原子级能力，甚至结合自己的扩展，开发出复杂的系统。与市场上其他产品不同，我们还增加了一层中间件，简化了用户开发应用时的固定操作，如界面设计等，使用户能够通过简单的操作快速生成界面，并在后续对结果进行调整，这使得平台对程序员和业务人员都更加友好。</p><p></p><p>最重要的是，我们平台还有一个主张是帮助程序员成长为全栈工程师。通常情况下，前端开发者可能对数据服务、容器服务和运维后台等技术不够熟悉，而我们通过提供 Serverless 云函数和云数据库等工具，能够有效解决这些运维问题，使他们能够开发出完整的系统。对于后端工程师而言，我们提供的可视化页面搭建功能，使他们能够自行构建服务，并通过 API 暴露数据，进而丰富我们的平台界面和其他内容。</p><p></p><p>我们致力于让工程师能够灵活地使用我们的产品，并在设计时尽量符合开发人员常用的开发范式，从而降低用户的认知成本。用户只需理解 MVC（模型 - 视图 - 控制器）这样的基本模型，无需掌握过多的复杂知识或学习额外的内容，即可成为全栈工程师。</p><p></p><h4>降本增效，飞书低代码有哪些成功案例？</h4><p></p><p></p><h5>霍太稳：能否介绍几个飞书低代码平台的典型的，令你印象深刻的案例？有没有用户觉得飞书低代码可以在企业内得到很好的应用，取得不错的降本增效成果？</h5><p></p><p></p><p>周洲：我们有一个客户，刚开始接触飞书低代码平台是无心插柳。当时他们有一件任务需要处理，急需一个系统来收集和处理用户的一些信息，因为机缘巧合我们就推荐了这个产品给他。结果他们基本上能做到新提的需求当晚、第二天就能上线，相对他们传统的开发流程有很大的体验提升，很好地解决了这个任务的挑战。于是他们就成为了我们的用户，开始慢慢将企业内的很多运营场景迁移到飞书低代码平台上。</p><p></p><p>我们之前同这家公司的 CIO 沟通时，他亲自讲了一些体验。他们以前做了一个 IT 规划，计划在 2026 年完成企业连锁店铺的一系列运营系统的建设。结果在飞书低代码平台的支持下，他们在 2024 年提前完成了目标。现在他们经过深度使用已经变成了我们的推广大使，会帮助我们同其他客户沟通，介绍他们的经验。</p><p></p><p>还有一个案例，我们有一个客户是业务人员，他向他们的研发团队提出了一个需求，研发说一年后才能上线。结果他觉得一年时间需求早过时了，就没让研发接活。后来他通过飞书的服务团队了解到飞书低代码平台，我们就进场了解了他的需求开始实现，到最终上线推广只用了两个月时间。后来我们有一些小的闭门会他也会来分享经验。其实像他们这样原本研发周期较长的情况可能有很多原因，会涉及资源、排期等问题，而且研发团队的工程师也各有所长，并不都是全栈工程师。而我们希望企业不会因为缺少某种人才而卡在某个节点，通过飞书低代码平台，企业的需求就可以顺利推进，不需要等待专门人才到岗。</p><p></p><p>还有一点，很多时候研发周期较长是因为需求不够清晰，因为业务人员不是很懂技术，很难将需求翻译成技术语言；技术人员也不懂业务，也很难理解业务所说的要求。这中间就需要产品经理做非常细的需求梳理，而等产品几个月后上线，业务人员发现成品和预想的有区别，于是又要等几个月修改，前后加起来半年就过去了。</p><p></p><p>低代码有个好处，我们有个客户说他们自己搬着电脑去前线业务人员那里，让业务讲需求，之后他们就在旁边可能花两三天时间给业务人员做一个原型试用，马上就能判断哪些点符合业务逻辑，哪些点有问题，反馈非常迅速。原型确认后，开发人员就可以回到本部门认真打磨细节，过两三周再出一个版本，快速迭代。虽然每次都不是完美的版本，但业务人员不需要漫长的等待，也更容易控制预期。</p><p></p><p>我们现在希望技术和业务人员像在一张圆桌上面一样共同讨论和实现大的业务目标，这样就能把业务人员的想法同技术人员的能力很好地结合在一起。</p><p></p><h5>霍太稳：极客邦科技在服务企业的过程中发现，企业很需要业技复合型的人才。未来的技术人员是否也需要对客户、对业务有更多了解，让自己的产品更好地适应客户的需求？</h5><p></p><p></p><p>周洲：很多时候企业培训员工后发现效果不及预期，原因也是类似的。有时企业需要让员工亲自下到一线实践，才能获得所需的能力。所以技术人员不懂业务时，不是说让他们去天天同业务人员工作在一起就能解决问题，因为前者缺少的是熟悉业务，能够以大家都理解的方式协作的人员的帮助。反过来一样，业务人员也很难通过几天的培训就对技术有深刻的理解。我认为有了飞书低代码平台这样的工具，它能逐渐潜移默化地让大家重新思考和理解很多事物，将被动式学习转化为主动式学习，进而让组织发生变化。</p><p></p><h4>AI 浪潮下，低代码技术何去何从？</h4><p></p><p></p><h5>霍太稳：从业务角度来看，低代码技术如何同生成式 AI 技术结合，在企业场景中解决核心问题，帮助企业寻找新的增长点？</h5><p></p><p></p><p>周洲：AI 有两个视角能够与低代码结合来帮助企业。一种是通过 AI 的方式让大家更快、更好地构建系统，帮助开发者更快交付；第二个视角是，AI 能力可以提供给开发者，让开发者通过这些能力更好地输出到业务线，让业务同学使用 AI 产品解决日常工作中面临的问题。</p><p></p><p>我们在这两个方向都做了一些工作，第二个视角的优先级会高一些，因为开发者的规模还是有限的，而且通过赋能开发者来赋能业务的路径相对来说还是有些长。所以我们去年开始优先解决业务的问题。去年我们做了飞书智能伙伴搭建平台等产品，想让 AI 进入用户的日常办公场景来解决业务问题。</p><p></p><p>我们有个解法，比如说在低代码系统里搭建一个界面时，我们在界面上挂载 AI 的能力，类似于 Copilot 的形式。用户看到一张数据报表，旁边就会有 AI 助手，点一下就能帮他解读背后的逻辑。比如飞书的 CRM 系统，当用户准备拜访某个客户，点开客户的详情页时，就可以一键生成客户的拜访文档。AI 会基于客户的行业数据、之前拜访时的诉求或反馈来生成这个文档的初稿，方便业务人员来润色完善。</p><p></p><p>又比如每天销售主管要查看大量客户拜访信息，花费很长时间。我们的 AI 就可以把各个客户的进展总结成一句话推给主管，有问题的信息点进去可以查看详情，这样就能大大节省时间。飞书通过这样的方式让 AI 同业务深度结合，让业务人员能够获得 AI 的好处。</p><p></p><p>开发侧我们也做了很多工作。AI 很擅长写代码，所以我们去年做低代码平台时就听到了一种声音，说未来 AI 是否会替代低代码平台。但我们跑了一年再看，发现 AI 能解决的问题还是有限的，在限定集里它做得还行，但在特别复杂的场景里它还是有很多约束或者局限。比如你让它帮你在一个几十万行的工程体系里做一些代码工作，你要做的事情就会非常多，而且它的准确性没有到那么高的程度。所以业界基于 AI 搭建的，用交互方式编程的产品大都是玩具级别，声量很大，但离投产还很远。</p><p></p><p>但我觉得低代码是一个很好的切入点，因为在低代码场景里我们经常面对片段式的代码，你不需要面对十几万行或者几十万行的工程，而 AI 在生成片段式代码时效果还可以。我们的系统天然是云产品，所以用户的代码写完后它可以直接调试来看效果。所以在低代码平台里融合 AI 的能力反而更合理，比单独做一个对话式开发工具要好。AI 与低代码一样是工具，是助力器，它们可以强强联合，不用考虑谁替代谁。</p><p></p><p>还有一点，AI 生成的裸代码要拿来修改，对开发人员还是有一定门槛的。而低代码的好处是它生成的是界面和系统，可以通过可视化的方式来修改。所以如果 AI 和低代码能很好地结合，先由前者生成可配置、可交互的及格原型，用户再通过后者来可视化地修改它，这样就能充分利用两者的优势。</p><p></p><h5>霍太稳：企业该如何平衡创新工具的使用和人才的培养，如何确保企业能持续创新，解决复杂的问题？</h5><p></p><p></p><p>周洲：好的工具肯定能成就更好的组织，所以企业应该给员工配备更好的工具，但前提是成本可控。因为有一些工具不是今天买了后立刻就能给企业带来变化，它成熟是有过程的。企业需要采购后逐渐使用，有了自己的理解再深入、升级。但企业自己的管理者要首先去使用像 AI 这样的工具，然后才能引导组织开始拥抱新的技术。</p><p></p><p>一个案例是，我们有个同事在三个月前使用我们的 AI 工具创建了一个 AI 数字人 PMO，然后将那个数字人放入我们的群里，结果产生了大量干扰信息，被我们抗议了。但他自己还是在持续去迭代这个产品，只是挪到了一些小的场景里，小范围试用。过了两个月，有一天我进到一个产品内测群，看大家发了一个问题，然后群里有一个机器人识别了那个问题。大家通常反馈问题都是截图，那个机器人识别了那张图，提炼了图片上的问题，帮用户写了产品 Bug 提交的标题，填好了内容，这样用户就可以一键提交了。</p><p></p><p>他们还展示了另一个案例：在飞书内部讨论问题的群中可以自动汇总大家的聊天内容，并定时输出讨论总结。所以这类技术都是迭代的过程，你不能指望它一开始就很好，但经过一段时间后它可能就并非吴下阿蒙了。所以企业引入低代码平台这样的新技术时也要有耐心，要等待一段时间才能看到比较大的变化。很多企业管理者会容易高估或者低估新技术的影响，要么希望新技术立刻见效，要么就觉得新技术还不成熟，那就直接放弃，这都是不合理的。</p><p></p><h5>霍太稳：展望未来，你如何看待低代码市场未来的规模、形态和挑战？</h5><p></p><p></p><p>周洲：低代码的发展也是同整个行业的技术发展相关联的。近年来，云原生、Serverless 等技术逐渐成熟，大多数低代码平台都是基于云来解决问题。而未来随着 AI 发展，低代码技术也应该与 AI 深度融合，让 AI 越来越好用。至于结合后的产品是否会有全新的形态，这并不是现在需要考虑或担忧的，我们只要让它自然革新就好。</p><p></p><h5>活动推荐：</h5><p></p><p></p><p>职场新机遇 ， 亚马逊云科技【云从业者认证】半价优惠！未过再考免费，助你轻松掌握云技能，开启职业新篇章！扫码即刻报名！</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f2/f2b292dedc98f143c8685b2e67e5a92b.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/7XjUmOAlYrc79I21zDgR</id>
            <title>赔偿金达36月工资！LG显示巨亏，竟有1400人自愿离职？马斯克P图点赞《黑神话：悟空》；花钱看不了国足比赛！爱奇艺致歉 | Q资讯</title>
            <link>https://www.infoq.cn/article/7XjUmOAlYrc79I21zDgR</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/7XjUmOAlYrc79I21zDgR</guid>
            <pubDate></pubDate>
            <updated>Mon, 09 Sep 2024 09:26:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p></p><blockquote>马斯克P图点赞《黑神话·悟空》；OpenAI 计划筹资数百亿在美打造 AI 基础设施；韩国面板大厂 LG 显示超 1400 人自愿离职，赔偿金高达 36 个月工资；Ilya Sutskever 新公司成立仅 3 个月融资 10 亿美元；三星电子中国销售部门裁员 8%，明年将再裁 30%？官方回应；淘宝官宣，9 月 12 日起逐步开放微信支付；直播国足比赛“崩了”，爱奇艺公布补偿方案；美国开始对印度裔 CEO 高管大清洗！多家平台确认“苹果税”抽佣 30%；英伟达回应收到反垄断调查传票；马斯克超级 AI 训练集群 Colossus 正式上线；优步等在日本东京试水共享汽车版“网约车”服务；字节跳动寻求 95 亿美元贷款；华为将于 9 月 10 日举办新品发布会；Firefox 将禁用 HTTP/2 服务器推送；微软确认已修复 Win11 文件资源管理器崩溃问题……</blockquote><p></p><p></p><h2>科技公司</h2><p></p><p></p><h4>马斯克P图点赞《黑神话：悟空》</h4><p></p><p>9&nbsp;月&nbsp;7&nbsp;日消息，国产&nbsp;3A&nbsp;游戏《黑神话：悟空》自上线以来，全球热度居高不下，吸引了众多玩家的关注。</p><p></p><p>马斯克在&nbsp;X&nbsp;平台对这款游戏进行评价，声称这是一款“令人印象深刻”的中国&nbsp;3A&nbsp;大作，有趣的是，他还配上了一张&nbsp;P&nbsp;图，将“天命人”的脸更换为自己的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/59/59bc4ad6df5a979bf1ba12c257136886.webp" /></p><p></p><p>市场研究机构&nbsp;VG&nbsp;Insights&nbsp;数据显示《黑神话：悟空》两周内卖出&nbsp;1800&nbsp;万套，销售金额约&nbsp;8.67&nbsp;亿美元（当前约&nbsp;61.65&nbsp;亿元人民币），这款游戏在其生命周期内的销量可能会达到&nbsp;3000&nbsp;万份，未来还将推出&nbsp;DLC&nbsp;拓展内容。</p><p></p><h4>OpenAI&nbsp;计划筹资数百亿在美打造&nbsp;AI&nbsp;基础设施</h4><p></p><p>据彭博社报道，&nbsp;OpenAI&nbsp;首席执行官&nbsp;Sam&nbsp;Altman&nbsp;大规模建设&nbsp;AI&nbsp;基础设施的计划正逐渐清晰，该计划将首先将在美国各州投资数百亿美元，建造包括建设数据中心、通过涡轮机和发电机增加能源容量和传输、以及扩大半导体制造等。</p><p></p><p>Altman&nbsp;今年早些时候一直在寻求美国政府对该项目的支持，该项目旨在组建一个全球投资者联盟，支持快速人工智能发展所需的昂贵物理基础设施提供资金。目前，Altman&nbsp;和他的团队正在研究几个之前从未报道过的细节，包括首先针对美国各州的计划。</p><p></p><p>知情人士说，正在讨论的项目类型包括建设数据中心，通过涡轮机和发电机提高能源容量和传输，以及扩大半导体制造。支持者可能包括加拿大、韩国、日本和阿联酋的投资者。OpenAI&nbsp;还预计其他私营公司也将参与该项目。</p><p></p><p>OpenAI&nbsp;的最大投资者微软公司可能是一个潜在的合作伙伴。微软没有具体评论这个项目，但表示知情并参与了该公司任何与基础设施相关的广泛努力。</p><p></p><h4>韩国面板大厂&nbsp;LG&nbsp;显示超&nbsp;1400&nbsp;人自愿离职，赔偿金高达&nbsp;36&nbsp;个月工资</h4><p></p><p>9&nbsp;月&nbsp;4&nbsp;日消息，据媒体报道，面对连续的财务亏损，LG&nbsp;显示采取了大规模的自愿离职计划以降低人力成本。</p><p></p><p>最新动态显示，超过&nbsp;1400&nbsp;名员工选择自愿离职，约占公司总员工数的&nbsp;5.12%，这些员工将获得高达&nbsp;36&nbsp;个月工资的赔偿金，以及额外的子女教育费用支持。</p><p></p><p>LG&nbsp;显示近期关闭了位于韩国龟尾市的&nbsp;M3&nbsp;生产线，该生产线主要生产&nbsp;IT&nbsp;类&nbsp;LCD&nbsp;模组产品，大约&nbsp;700&nbsp;名龟尾工厂的员工将被重新分配其他地点，同时也提供了自愿离职计划。</p><p></p><p>为了进一步降低成本，LG&nbsp;显示在今年&nbsp;6&nbsp;月针对&nbsp;28&nbsp;岁及以上的制造员工推出了额外的自愿裁员计划，此前已经实施了针对&nbsp;40&nbsp;岁及以上、35&nbsp;岁及以上员工的离职计划。</p><p></p><p>LG&nbsp;显示的财务状况显示，公司在今年第二季度的综合销售额为&nbsp;6.71&nbsp;万亿韩元，约合&nbsp;48.5&nbsp;亿美元，营业亏损&nbsp;937&nbsp;亿韩元，净亏损为&nbsp;4708&nbsp;亿韩元。值得一提的是，由于离职人员太多，韩国地方政府还将为这些人员专门举办就业专题讲座。</p><p></p><h4>Ilya&nbsp;Sutskever&nbsp;新公司成立仅&nbsp;3&nbsp;个月融资&nbsp;10&nbsp;亿美元，估值已超&nbsp;350&nbsp;亿</h4><p></p><p>当地时间&nbsp;9&nbsp;月&nbsp;4&nbsp;日，OpenAI&nbsp;前联合创始人&nbsp;Ilya&nbsp;Sutskever&nbsp;所创立的&nbsp;AI&nbsp;初创公司&nbsp;SSI（Safe&nbsp;Superintelligence）在其社交媒体官方账号宣布，公司获得来自&nbsp;NFDG、a16z、红杉美国、DST&nbsp;Global&nbsp;和&nbsp;SV&nbsp;Angel&nbsp;等投资者&nbsp;10&nbsp;亿美元融资。</p><p></p><p>报道称，SSI&nbsp;此轮投资方包括&nbsp;a16z（Andreessen&nbsp;Horowitz）、红杉资本、DST&nbsp;Global&nbsp;和&nbsp;SV&nbsp;Angel，而由&nbsp;Nat&nbsp;Friedman&nbsp;和&nbsp;SSI&nbsp;首席执行官&nbsp;Daniel&nbsp;Gross&nbsp;运营的投资合伙企业&nbsp;NFDG&nbsp;也参与其中。</p><p></p><p>知情人士指出，成立近三个月，SSI估值已经高达&nbsp;50&nbsp;亿美元（约合人民币&nbsp;355.85&nbsp;亿元）。此轮资金将用于人才搭建和技术投入，从而帮助开发远超人类能力的安全&nbsp;AI&nbsp;系统。</p><p></p><p>Ilya&nbsp;Sutskever&nbsp;表示，SSI&nbsp;正在构建尖端的&nbsp;AI&nbsp;模型，旨在挑战更成熟的竞争对手，包括&nbsp;Ilya&nbsp;的前雇主&nbsp;OpenAI、Anthropic&nbsp;和&nbsp;Elon&nbsp;Musk&nbsp;的&nbsp;xAI。根据&nbsp;SSI&nbsp;官网，公司正在组建一支精干的团队，由世界上最优秀的工程师和研究人员组成，他们将专注于&nbsp;SSI，不做其他任何事情。</p><p></p><h4>三星电子中国销售部门裁员&nbsp;8%，明年将再裁&nbsp;30%？官方回应</h4><p></p><p>9&nbsp;月&nbsp;4&nbsp;日，据“首尔经济日报”报道，因智能手机和电视在中国市场销售持续低迷，三星电子选择了极端的重组措施，即对于中国销售部门进行裁员，预计今年裁员规模为&nbsp;130&nbsp;人，约占&nbsp;1600&nbsp;个销售职位的&nbsp;8%，明年将继续削减&nbsp;30%。</p><p></p><p>报道称，三星电子中国销售部门近日已经向员工发出了重整通知，真在接受自动离职申请。如果申请人数较少，将根据公司设定的标准选择目标人数进行强制裁员。</p><p></p><p>对此，三星电子中国公司方面回应称，为提升公司的组织效率及市场竞争力，公司将进行必要的业务调整和人员优化。通过裁减一部分重复性高的工作及岗位，以确保公司的资源能得到更好的配置，提升组织效率。</p><p></p><h4>淘宝官宣，9&nbsp;月&nbsp;12&nbsp;日起逐步开放微信支付</h4><p></p><p>继宣布计划新增微信支付能力后，9&nbsp;月&nbsp;5&nbsp;日，淘宝发布公告，明确&nbsp;9&nbsp;月&nbsp;12&nbsp;日后逐步向所有淘宝天猫商家开通微信支付。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bf/bf28622431d212c5086e827b553d3c87.webp" /></p><p></p><p>据悉，目前部分商家已经逐步收到开通邀请。大量商家也对此表示支持，并普遍认为此举将为商家和平台争取更多用户；还有不少商家表示，希望淘宝加快进度，尽早让商家用上微信支付。</p><p></p><p>9&nbsp;月&nbsp;4&nbsp;日，淘宝、天猫分别发布《关于淘宝网新增微信支付能力的意见征集》和《关于天猫新增微信支付能力的意见征集》。背景说明为：为提升消费者的购物体验，淘宝网、天猫计划新增微信支付能力。基于上述服务的增加，淘宝网、天猫拟对平台规则作出调整。</p><p></p><p>有网友表示：“终于来了，等这一天太久了”“太好了”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/05/0588d77c0f66257c4e5abdb9eddce1cf.webp" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/c8/c88e6222ff34be7ed36bfca708a7d69c.webp" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/67/677b500af18e1156ac482ff4956238ca.webp" /></p><p></p><p>对此，支付宝做出了回应，如下图。</p><p></p><p><img src="https://static001.geekbang.org/infoq/45/4568c21bb1facbae45a0a9873766776d.webp" /></p><p></p><p>微信支付回应称：“微信支付秉持开放的合作理念，积极探索与各平台之间的互通合作。目前与淘宝平台的功能适配正在开通中。”</p><p></p><p>此前，国内互联网巨头平台支付生态一度处于互不相通、彼此屏蔽的状态，如今逐渐打破封闭。而淘宝支持使用微信支付，可以看作是近年来互联网平台互联互通的一个节点。</p><p></p><h4>直播国足比赛“崩了”，爱奇艺公布补偿方案</h4><p></p><p>9&nbsp;月&nbsp;5&nbsp;日&nbsp;18&nbsp;时&nbsp;35&nbsp;分，中国男足在世预赛亚洲区&nbsp;18&nbsp;强赛首场比赛中客场对阵日本队，中国男足&nbsp;0：7&nbsp;不敌日本男足。</p><p></p><p>据悉，该赛事由爱奇艺独家买断转播权。然而在开赛后不久，“爱奇艺&nbsp;花钱看不了”话题突然引爆热搜。据网友吐槽，爱奇艺体育&nbsp;App&nbsp;界面卡住看不了比赛。也有球迷反馈，在爱奇艺&nbsp;App&nbsp;充值后，用&nbsp;9&nbsp;元观赛券仍不能看国足比赛。</p><p></p><p>当晚&nbsp;22&nbsp;时&nbsp;23&nbsp;分，爱奇艺体育在微博道歉称，因球迷的热情，造成瞬时流量过大。技术服务资源分配超出限额，给部分用户造成了不好的观赛体验。后续会进一步加强技术预案和运营能力，提供更稳定可靠的直播服务。</p><p></p><p>爱奇艺体育表示，由于咨询量较大，客服电话可能接入需要时间。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1b/1b5b5ec409706815e9fd01d794fc627c.webp" /></p><p></p><p>当晚&nbsp;19&nbsp;时左右，爱奇艺体育客服电话已无法接通。爱奇艺官方电话客服也无法接通。部分设备无法加载爱奇艺&nbsp;App“帮助与反馈”咨询页面。</p><p></p><p>爱奇艺&nbsp;App&nbsp;在客服页面公告：目前为咨询人数高峰，为了节省时间，建议可在次日&nbsp;16:00&nbsp;或次日同时段再来咨询，避免长时间排队。</p><p></p><p><img src="https://static001.geekbang.org/infoq/43/437aebb7fd98faaa1f0fdc2e66028998.webp" /></p><p></p><p>9&nbsp;月&nbsp;6&nbsp;日晚间，爱奇艺体育再次发布致歉声明，并给出了补偿方案。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3b/3bb6aace205a4c151a1271db31f7d9b4.webp" /></p><p></p><h4>支付宝宣布推出独立&nbsp;AI&nbsp;原生&nbsp;App“支小宝”</h4><p></p><p>9&nbsp;月&nbsp;5&nbsp;日，支付宝在&nbsp;2024&nbsp;外滩大会上宣布发布&nbsp;AI&nbsp;生活管家&nbsp;App“支小宝”，目前苹果及安卓应用商店均可下载。</p><p></p><p><img src="https://static001.geekbang.org/infoq/65/65b43253442176fbafc604dc9d90f9d8.webp" /></p><p></p><p>据官方介绍，基于蚂蚁百灵大模型推出的“支小宝”，是国内首个服务型的&nbsp;AI&nbsp;独立&nbsp;App——连接支付宝生态，“支小宝”可通过对话快速订票、点餐、打车、查询附近吃喝玩乐等，说句话就能办事；“支小宝”还拥有场景感知系统，能根据用户的生活习惯和使用场景，智能推荐专属的服务，做到“越用越懂用户”。</p><p></p><p>与此同时，支付宝面向行业正式启动智能体生态开放计划，并推出了智能体开发平台“百宝箱”，依托智能体构建能力，商家机构可通过“百宝箱”0&nbsp;代码、最快&nbsp;1&nbsp;分钟创建专属智能体，并一键发布到支付宝小程序、支付宝&nbsp;App、支小宝&nbsp;App&nbsp;等。</p><p></p><p>目前支付宝“百宝箱”，分为基础版与专业版。基础版面向普通用户开放，可快速搭建并体验智能体；专业版则面向专业伙伴开放，支持与生态伙伴的深度定制。</p><p></p><h4>百度文心一言&nbsp;APP&nbsp;更新为“文小言”，定位“新搜索”智能助手</h4><p></p><p>9&nbsp;月&nbsp;4&nbsp;日，百度文心一言&nbsp;APP&nbsp;在上线一年后更名为文小言，定位百度旗下“新搜索”智能助手，推出了富媒体搜索、多模态输入、文本与图片创作、高拟真数字人等能力，上线记忆和自由订阅等&nbsp;AI&nbsp;功能。</p><p></p><p>区别其它搜索产品，文小言推出了富媒体搜索、多模态输入、文本与图片创作、高拟真数字人等"新搜索"能力，能全面满足用户搜、创、聊需求。</p><p></p><p>同时，文小言独家首发了记忆和自由订阅等新功能，被认为是目前为止，在新搜索领域结合大模型最原生、最彻底的&nbsp;AI&nbsp;应用。</p><p></p><h4>美国开始对印度裔&nbsp;CEO&nbsp;高管大清洗！多家科技巨头已有计划</h4><p></p><p>9&nbsp;月&nbsp;3&nbsp;日消息，据媒体报道，研究服务机构&nbsp;exechange.com&nbsp;统计，今年在&nbsp;Russel&nbsp;3000&nbsp;指数企业中离职的&nbsp;191&nbsp;名&nbsp;CEO&nbsp;中，有&nbsp;74&nbsp;名属于被解雇或被动离职，这是自&nbsp;2017&nbsp;年以来高管下台人数最多的一年。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e0/e0f101be45239a165da297ec1656748c.webp" /></p><p></p><p>年初至今被迫下台的&nbsp;CEO&nbsp;数量（所有数字均统计至&nbsp;8&nbsp;月&nbsp;13&nbsp;日）</p><p></p><p>特别值得注意的是，星巴克公司&nbsp;CEO&nbsp;Laxman&nbsp;Narasimhan&nbsp;的突然离职，被市场解读为美国企业对印度裔&nbsp;CEO&nbsp;的一次大清洗的信号。</p><p></p><p>据彭博社报道，推特印度分公司的员工人数大约为&nbsp;200&nbsp;人，但裁员后只剩下约&nbsp;12&nbsp;人。</p><p></p><p>有报道称，美国许多知名公司开始"清理"印度裔高管。除了星巴克，谷歌、Twitter&nbsp;等科技巨头也有计划“清理”印裔高管。</p><p></p><p>实际上在早年间，印度人到硅谷工作后没多久便在美国建立了一个名为印度企业家协会的组织，主张搭建成各种人际关系网，为培养下一代印度企业家积蓄力量。目前，印度企业家协会在&nbsp;13&nbsp;个国家设立&nbsp;54&nbsp;个分支机构，拥有&nbsp;10000&nbsp;多名会员，影响力不容小觑。印度人当上部门负责人后，就会马上招入大批印度人。据悉，Narasimhan&nbsp;入职星巴克后就是如此。</p><p></p><p>另一个比较有名的印裔&nbsp;CEO&nbsp;是&nbsp;IBM&nbsp;的现任&nbsp;CEO&nbsp;Arvind&nbsp;Krishna。Krishna&nbsp;的祖父是一位数学家，他本人出生于印度南部的安得拉邦，而且格外强调孩子应当接受良好教育。而这位&nbsp;CEO&nbsp;刚刚做了一件轰动国内&nbsp;IT&nbsp;行业的大事：彻底关闭中国研发部门，裁撤约&nbsp;1600&nbsp;名中国员工。有消息称，此次&nbsp;IBM&nbsp;被裁员的可以重新安置到印度班加罗尔，IBM&nbsp;也确实在印度不断增加岗位。</p><p></p><p>根据&nbsp;Teamlease&nbsp;Digital&nbsp;的数据，从美国回国并积极寻找印度机会的技术人员都是中高级专业人士，他们往往担任产品和项目经理，还有全栈、设计、数据和&nbsp;DevOps&nbsp;工程师等。</p><p></p><h4>多家平台确认“苹果税”抽佣&nbsp;30%，苹果客服：建议通过电脑端充值</h4><p></p><p>9&nbsp;月&nbsp;3&nbsp;日，新闻记者体验了多个视频&nbsp;App&nbsp;都发现了这一现象，其客服也确认存在&nbsp;30%&nbsp;苹果公司（后简称“苹果”）抽成。苹果客服方面虽然未正面回复“苹果税”，但建议消费者可以通过电脑端进行充值。</p><p></p><p>据多家媒体报道，8&nbsp;月初，苹果强制要求微信堵住支付漏洞，目的是强制对微信生态征收&nbsp;30%&nbsp;的“苹果税”，剑指近年来快速发展的微信小游戏。</p><p></p><p>所谓“苹果税”即渠道分成，也就是在苹果手机应用商店&nbsp;App&nbsp;Store&nbsp;中，通过抽成的方式，获得游戏用户充值的利润。一般而言，抽成比例为数字内容消费的&nbsp;15%&nbsp;至&nbsp;30%。每当用户通过苹果手机应用商店付费下载&nbsp;App&nbsp;或在&nbsp;App&nbsp;内部购买数字商品、服务时，苹果公司会扣留交易金额的一部分作为佣金，再将剩下的资金转给相应的&nbsp;App&nbsp;开发者。据&nbsp;Sensor&nbsp;Tower&nbsp;数据，2023&nbsp;年“苹果税”全球收入达到约&nbsp;1608&nbsp;亿元人民币，其中中国市场的贡献超过&nbsp;400&nbsp;亿元。</p><p></p><h4>英伟达回应收到反垄断调查传票：凭实力取胜</h4><p></p><p>英伟达发言人当地时间&nbsp;9&nbsp;月&nbsp;4&nbsp;日在一份声明中表示，英伟达没有收到美国司法部的传票。“我们已经向美国司法部进行了询问，并没有收到传票。不过，我们很乐意回答监管机构可能提出的有关我们业务的任何问题。”</p><p></p><p>此前有报道称，正在搜集英伟达违反反垄断法证据的美国司法部向该芯片商及其它公司发送传票，升级了对这家科技巨头的调查。</p><p></p><p>知情人士透露，此前已向企业发放了调查问卷的司法部现在发出了具有法律约束力的要求，而接收对象必须向其提供信息。</p><p></p><h4>马斯克超级&nbsp;AI&nbsp;训练集群&nbsp;Colossus&nbsp;正式上线</h4><p></p><p>9&nbsp;月&nbsp;3&nbsp;日，特斯拉&nbsp;CEO&nbsp;埃隆·马斯克在&nbsp;X&nbsp;平台上宣布，旗下人工智能初创企业&nbsp;x.AI&nbsp;打造的超级人工智能训练集群已经正式上线，该集群名为“Colossus（巨人）”。</p><p></p><p>他透露，团队花了&nbsp;122&nbsp;天来完成&nbsp;Colossus&nbsp;的上线过程。Colossus&nbsp;还将在未来几个月内增加&nbsp;10&nbsp;万颗&nbsp;GPU（图形处理器），其中，5&nbsp;万颗将是更为先进的英伟达&nbsp;H200，这意味着&nbsp;Colossus&nbsp;的算力将再次翻倍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7d/7d0ab2b16a762b99c5c0a73120846d23.webp" /></p><p></p><p>Cursor&nbsp;集成了&nbsp;Claude&nbsp;3.5&nbsp;Sonnet&nbsp;和&nbsp;GPT-4o&nbsp;等先进模型，为用户提供了高效的编程体验。它不仅融合了开发环境的实用性，还融入了&nbsp;AI&nbsp;聊天机器人的交互性，能让用户仅使用文本提示即可编写、预测和操作代码。</p><p></p><p>与&nbsp;GitHub&nbsp;Copilot&nbsp;等辅助工具相比，Cursor&nbsp;在自动化和完成度上有了显著提升，它的简单性在于可以通过聊天窗口进行操作，这意味着即使是完全不懂代码的人也可以在几分钟内运行一个功能齐全的应用程序，并不断在此基础上添加新功能，它真正做到了使编码更加民主化。</p><p></p><p>它建立在与&nbsp;Microsoft&nbsp;Visual&nbsp;Studio&nbsp;Code&nbsp;相同的系统之上，确保了良好的兼容性和用户体验，因此迅速赢得了包括新手程序员和资深工程师在内的广泛用户群体。Perplexity、Midjourney&nbsp;和&nbsp;OpenAI&nbsp;的员工是付费使用该&nbsp;AI&nbsp;工具的&nbsp;30000&nbsp;名客户中的一部分。</p><p></p><h4>优步等在日本东京试水共享汽车版“网约车”服务</h4><p></p><p>9&nbsp;月&nbsp;4&nbsp;日，优步日本公司宣布，联合&nbsp;Park24、Royal&nbsp;&nbsp;Limousine&nbsp;在东京推出利用共享汽车跑网约车的服务。</p><p></p><p>到&nbsp;11&nbsp;月底前为试运行，视需求等探讨扩展至其他的“日本版网约车”实施区域。共享汽车让没有私家车的人也能跑网约车，此举有意使司机数量增加。司机将与出租车公司&nbsp;Royal&nbsp;&nbsp;Limousine&nbsp;签订就业合同。每小时&nbsp;880&nbsp;日元（约合人民币&nbsp;43&nbsp;元）起的共享汽车使用费由司机承担，司机可通过优步的叫车&nbsp;APP&nbsp;接乘客的订单。</p><p></p><p>“日本版网约车”由于运行时间和实施区域受限，参与人数难见增加。Royal&nbsp;Limousine&nbsp;虽然收到了一千人以上报名，但实际参与人数仅为&nbsp;60&nbsp;人左右。3&nbsp;家公司希望发挥共享汽车的作用，吸引年轻人参与其中。</p><p></p><h4>马斯克：X&nbsp;TV&nbsp;的测试版已发布</h4><p></p><p>北京时间&nbsp;9&nbsp;月&nbsp;3&nbsp;日，马斯克在社交平台&nbsp;X&nbsp;发文透露“X&nbsp;TV”应用测试版已发布。马斯克和&nbsp;X&nbsp;平台首席执行官&nbsp;Linda&nbsp;Yaccarino&nbsp;都转发了这一推文，前者称“电视应用测试版推出”，后者则表示“还有更多精彩即将呈现”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3d/3d00071ce1fba53785b285ae3e496c36.webp" /></p><p></p><p>据了解，X&nbsp;TV&nbsp;应用现已在&nbsp;Android&nbsp;TV&nbsp;上推出，它可以在&nbsp;LG、亚马逊&nbsp;Fire&nbsp;TV&nbsp;和&nbsp;Google&nbsp;TV&nbsp;上使用，而且即将推出更多集成功能。</p><p></p><p>根据&nbsp;X&nbsp;平台首席执行官琳达·雅克里诺的表述，该应用具有六大优势，包括基于用户兴趣提供热门内容的热门视频算法；基于用户兴趣提供更个性化内容推荐的&nbsp;AI&nbsp;驱动的主题；可以无缝在手机、电视切换观看的跨设备体验；通过改进的视频搜索更快查找内容的增强的视频搜索；用户可以更轻松地将内容投到电视上的轻松投屏；可兼容大部分智能电视的广泛兼容性。</p><p></p><p>马斯克更是表示未来会有更多的集成，这意味着&nbsp;X&nbsp;TV&nbsp;可能会进一步拓展支持的智能电视操作系统，比如三星的&nbsp;Tizen&nbsp;OS&nbsp;等。该应用的推出也标志着&nbsp;X（前身是&nbsp;Twitter）开始进入流媒体领域，后续它将如何与&nbsp;YouTube&nbsp;的电视应用等竞争，值得关注。</p><p></p><h4>字节跳动寻求&nbsp;95&nbsp;亿美元贷款，亚洲最大规模美元企业贷款诞生！</h4><p></p><p>据彭博社报道，TikTok&nbsp;母公司字节跳动正寻求银行提供一笔&nbsp;95&nbsp;亿美元的贷款，这将成为亚洲（不含日本）最大规模的美元计价企业贷款。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c6/c60876b8bad991d04f85cb2e432fbbc2.webp" /></p><p></p><p>据报道，字节跳动此次最新融资项目的协调人将由花旗集团、高盛和摩根大通担任，贷款期限为三年，可延长至最多五年。而此前，字节跳动就被曝寻求&nbsp;50&nbsp;亿美元贷款再融资</p><p></p><p>值得一提的是，字节跳动的董事会刚经历了不小的变动。</p><p></p><p><img src="https://static001.geekbang.org/infoq/64/64ecbda3d0fa482456e506f62f26b61d.webp" /></p><p></p><p>9&nbsp;月&nbsp;1&nbsp;日，据字节跳动官方网站显示，Coatue&nbsp;Management&nbsp;创始人菲利普·拉方特（Philippe&nbsp;Laffont）已离开该公司董事会。而与之相对的是新力量泽维尔·尼尔（Xavier&nbsp;Niel）的加入，这位来自法国的电信大亨，是伊利亚特电信集团（Iliad）创始人，同时也是一位货真价实的亿万富翁。</p><p></p><p>截止到&nbsp;9&nbsp;月&nbsp;1&nbsp;日，在&nbsp;2024&nbsp;福布斯全球亿万富豪榜单中，尼尔凭借&nbsp;105&nbsp;亿美元（约合人民币&nbsp;744.71&nbsp;亿元）的净资产在世界亿万富翁中排名&nbsp;228&nbsp;位。</p><p></p><p>据了解，字节跳动董事会此次的人事变动可能意味着字节跳动与&nbsp;Coatue&nbsp;Management&nbsp;在某些方面的合作有所调整。而此举不仅为董事会注入了新鲜血液，还将引领着公司迈向一个全新的战略方向</p><p></p><h4>华为将于&nbsp;9&nbsp;月&nbsp;10&nbsp;日举办新品发布会</h4><p></p><p>9&nbsp;月&nbsp;2&nbsp;日上午消息，华为终端在官微宣布，将于&nbsp;9&nbsp;月&nbsp;10&nbsp;日正式召开新品发布会。随后，余承东发布微博称，华为最具引领性、创新性、颠覆性的产品来了！</p><p></p><p><img src="https://static001.geekbang.org/infoq/9c/9c86d0b0a8205bbcdd0b46bd826e6562.webp" /></p><p></p><p>而苹果方面，早在&nbsp;8&nbsp;月&nbsp;26&nbsp;日就正式宣布，将于北京时间&nbsp;9&nbsp;月&nbsp;10&nbsp;日凌晨&nbsp;1&nbsp;点在加利福尼亚州库比蒂诺的总部举办特别活动，主题为“高光时刻&nbsp;(It's&nbsp;Glowtime)”。&nbsp;此次发布会预计将推出最新款&nbsp;iPhone、Watch&nbsp;和&nbsp;AirPods。知名科技记者马克·古尔曼透露，此次秋季发布会实际上推迟了一天，主要是为了避免与副总统哈里斯和共和党总统候选人特朗普的首场电视辩论撞期。</p><p></p><p>科技界的两大巨头——华为和苹果，不约而同地选择了&nbsp;9&nbsp;月&nbsp;10&nbsp;日这一天举办新品发布会，这无疑为科技爱好者带来了双重的期待。</p><p></p><h2>IT&nbsp;业界</h2><p></p><p></p><h4>Firefox&nbsp;将禁用&nbsp;HTTP/2&nbsp;服务器推送</h4><p></p><p>9&nbsp;月&nbsp;7&nbsp;日消息，Firefox&nbsp;计划在所有平台上禁用&nbsp;HTTP&nbsp;/&nbsp;2&nbsp;服务器推送功能，预计会在&nbsp;ESR&nbsp;140&nbsp;前完全移除该功能。</p><p></p><p>Chrome&nbsp;早在&nbsp;2022&nbsp;年&nbsp;9&nbsp;月的&nbsp;106&nbsp;版本中就已禁用&nbsp;HTTP&nbsp;/&nbsp;2&nbsp;服务器推送，理由是使用率低，它推荐将&nbsp;rel="preload"&nbsp;和&nbsp;103&nbsp;Early&nbsp;作为替代。</p><p></p><p>此外，虽然苹果没有提供过关于&nbsp;Safari&nbsp;中禁用&nbsp;HTTP&nbsp;/&nbsp;2&nbsp;推送的公告，但使用&nbsp;nodejs&nbsp;服务器进行的本地测试表明最新版本的&nbsp;Safari&nbsp;已经默认拒绝推送流。</p><p></p><p>Firefox&nbsp;此前一直支持&nbsp;HTTP&nbsp;/&nbsp;2&nbsp;推送，但过去几个月发现了与这一功能相关的&nbsp;Bug，原因是使用推送的&nbsp;WebServer&nbsp;和网站没有在&nbsp;Firefox&nbsp;上进行测试，结果会导致网站在&nbsp;Firefox&nbsp;上停止工作。</p><p></p><h4>微软确认已修复&nbsp;Win11&nbsp;文件资源管理器崩溃问题</h4><p></p><p>9&nbsp;月&nbsp;7&nbsp;日消息，微软在反馈中心确认，他们已经修复了一个导致部分&nbsp;Windows&nbsp;11&nbsp;用户&nbsp;explorer.exe（文件资源管理器）崩溃的问题。</p><p></p><p>根据问题描述，该问题主要表现为当用户尝试从任务栏打开文件资源管理器时&nbsp;explorer.exe&nbsp;停止响应。微软在反馈中心帖子中指出：“截至&nbsp;Build&nbsp;22635.4005，这个问题应该已经得到修复。”</p><p></p><p>实际上，目前只有&nbsp;Beta&nbsp;频道还停留在&nbsp;Build&nbsp;22635&nbsp;版本，其余频道大多已更新至&nbsp;261xx&nbsp;版本，其中&nbsp;Build&nbsp;22635.4005&nbsp;已于&nbsp;8&nbsp;月初灰度推送给&nbsp;Beta&nbsp;用户。</p><p></p><p>微软还补充称：“作为提醒，explorer&nbsp;崩溃可能有不同的潜在原因，因此如果您继续遇到最新更新的问题，请立刻提交新的反馈。”</p><p></p><h4>韩国监管机构：Telegram&nbsp;已遵从要求删除“深伪”色情内容</h4><p></p><p>韩国政府&nbsp;9&nbsp;月&nbsp;3&nbsp;日表示，Telegram&nbsp;已经按照他们的要求，从平台删除了&nbsp;Deepfake&nbsp;色情内容。此外，平台还就此前对数字犯罪的反应表示了歉意，并承诺会改善与韩国当局的沟通。</p><p></p><p>韩国通信标准委员会称，被韩国男性用于传播女性的深度伪造色情视频的社交平台&nbsp;Telegram&nbsp;已与当局进行合作，删除了&nbsp;25&nbsp;段视频，并提供了电子邮件热线，方便递交删除请求。</p><p></p><p>通信标准委员会表示，Telegram&nbsp;还为其平台存在上述内容道歉。Telegram&nbsp;处于韩国最近爆出的深度伪造淫秽视频丑闻的中心，韩国男性创建了大量分享淫秽影像的群聊频道，每个频道有数千人参与，多则有十万人以上。受害者包括大学生、教师、女兵等，甚至有中学生等未成年人。在社交网站流传的“受害学校”已超过百所。Telegram&nbsp;在韩国有逾&nbsp;347&nbsp;万用户。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/bh6SbhxMZYslzYSqmUCj</id>
            <title>蚂蚁集团开源向量索引库VSAG，支持千维以上向量存储</title>
            <link>https://www.infoq.cn/article/bh6SbhxMZYslzYSqmUCj</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/bh6SbhxMZYslzYSqmUCj</guid>
            <pubDate></pubDate>
            <updated>Mon, 09 Sep 2024 07:41:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>近日，由蚂蚁集团发起的，旨在提高数据库与大模型应用开发效率的“星辰智能社区”新发布了两个项目：AI原生数据应用开发框架DB-GPT新版本与向量索引库VSAG。</p><p></p><p>DB-GPT是一个开源的AI原生数据应用开发框架。在数据库领域，如何增强和大语言模型的交互任务，减少大模型的幻觉，为用户提供可靠并且安全的数据理解和分析能力，仍然是一项极具挑战的工作。DB-GPT通过开发多模型管理(SMMF)、Text2SQL效果优化、RAG框架以及优化、Multi-Agents框架协作、AWEL（智能体工作流编排）等多种技术能力，让围绕数据库构建大模型应用更简单便捷。</p><p></p><p>本次开源的新版本 DB-GPT v0.6.0，完整支持了数据驱动的AI原生应用生命周期管理（AI Native Data Apps-dbgpts）以及AI原生应用仓库，方便开发者构建、发布、分享AI Native Data Apps，还新增了六大特性，包括将AWEL协议升级至2.0，支持更复杂的编排；结合TuGraph，能支持图的构建与检索，进一步增强检索的准确性与召回的稳定性，以减少大模型的幻觉，在同样的检索效果下，构建Graph的成本比业界的方案少50%的Tokens；支持Agent Memory，如感知记忆、短期/长期记忆、混合记忆等；支持意图识别、槽位填充，支持Text2NLU、Text2GQL微调等。</p><p></p><p>除此之前，社区还新发布了向量索引库VSAG。VSAG是蚂蚁集团在向量数据库上一系列的工程优化与向量索引的算法改进成果，适用于高维向量的存储和计算优化，并能提供 C++ 和 Python 的接口以便使用。VSAG已在蚂蚁内部百亿数据量级业务上使用，在保证同样的召回率情况下，VSAG 可以通过量化和基于磁盘的重排技术，将内存消耗降低到 HNSW（最流行的向量索引）的 1/10，从而实现生产部署成本的大幅降低。VSAG将结合DB-GPT，让RAG的构建更加简单、高效，同时VSAG作为独立开放的向量引擎，也将支持LangChain、LlamaIndex构建RAG应用。</p><p></p><p>关于星辰智能社区</p><p>“星辰智能社区”由蚂蚁集团发起，专注于AI时代数据智能技术的探索，社区在GitHub上已获得17k Star数，核心成员来自蚂蚁、阿里、美团、京东、唯品会等科技公司和知名海内外高校硕博在校学生。目前已有超过50万用户正在学习和使用DB-GPT，社区活跃人数近7000人，开发贡献者130人。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>