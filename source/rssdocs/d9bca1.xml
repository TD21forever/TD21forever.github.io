<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/tqPIOFDHlVbpELk4jrYb</id>
            <title>别找啦！AIGC+金融场景的绝佳案例都在这</title>
            <link>https://www.infoq.cn/article/tqPIOFDHlVbpELk4jrYb</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/tqPIOFDHlVbpELk4jrYb</guid>
            <pubDate></pubDate>
            <updated>Thu, 18 Jul 2024 10:19:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 金融行业, 大模型应用, AIGC, 金融科技大会
<br>
<br>
总结: 金融行业因其专业知识密集、数据驱动、业务流程复杂性等特点成为大模型应用的理想领域。AIGC在金融行业落地并取得初步成果，涉及风控、营销、运营等领域。金融机构在大模型时代下不得不采用大模型，以提高风险感知、风控决策能力。金融科技大会将展示金融数智化实践的案例，为行业提供更多参考。 </div>
                        <hr>
                    
                    <p>金融行业被视为大模型应用的理想领域。从内因看，是因为金融本身具有专业知识密集、数据驱动、业务流程复杂性三个显著特点，而三大特点恰好与大模型理解能力、记忆能力、逻辑推理等优势高度吻合。从外因看，在政策驱动和市场热度的双重助力下，对于每一家金融机构来说，不采用大模型几乎是不可能的。</p><p></p><p>那么，经过一年多的探索，AIGC在金融行业落地情况如何了？哪些场景刚开始探索，哪些场景已经取得初步成果？在8月16日-17日即将于上海举办的FCon全球金融科技大会上，InfoQ搜罗了10+来自银行、保险、证券和金融科技等不同行业的AIGC+金融场景的绝佳案例，覆盖风控、营销、运营、研发等领域，希望为金融数智化实践提供更多参考。以下为部分议题介绍，更多重磅议题仍在实时更新中，欢迎前往大会官网进一步了解：<a href="https://fcon.infoq.cn/2024/shanghai/">https://fcon.infoq.cn/2024/shanghai/</a>"</p><p></p><h2>风控还是大模型“禁区”吗？</h2><p></p><p></p><p>数字化风控是金融行业的基石，安全与效率始终是其核心追求。在AIGC技术的浪潮中，逼真的AI生成内容对安全审核提出了前所未有的挑战；同时，金融数据的海量积累也对风控的智能化和效率提出了更高的要求。为应对这些挑战，度小满搭建了攻防对抗框架，不断迭代优化伪造检测系统，保障金融交易的安全性。此外，其还通过文档智能技术方案，自动提取和解析金融文档中的关键信息，极大提升了数智化处理的效率。</p><p></p><p>在「前沿金融科技探索与应用」专题论坛，度小满金融数据智能部计算机视觉方向负责人万阳春将分享《<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6030">计算机视觉技术在金融数字化风控中应用</a>"》。</p><p></p><p>聚焦反欺诈领域，随着消费金融行业的快速发展，个人和团伙欺诈行为日益猖獗。近年随着技术进步特别是AI技术的广泛应用，欺诈攻击手段呈现线上化、多样化和专业化趋势，传统反欺诈手段应对乏力，给金融机构和消费者带来了巨大的风险挑战。因此，构建一个适应当下的新型反欺诈技术体系成为当务之急。</p><p></p><p>中邮消费金融科技发展部AI算法专家陈盛福同样将在该专题下带来《<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6066">消费金融风控新防线：智能反欺诈技术体系全解析</a>"》的议题分享。通过介绍当前消费金融场景中的欺诈攻击现状，结合智能反欺诈旅程和实际落地经验全面剖析全流程解决方案，特别针对反欺诈涉及到的AI技术体系展开深入讲解，并展望在AIGC和大模型时代背景下的未来反欺诈新方向，探索针对新型攻击的提前布局，以魔法打败魔法，为消费金融领域筑牢新防线。</p><p></p><p>此外，在金融科技的浪潮中，账户风险管理也一直是金融机构关注的焦点。传统的人工驱动流程在处理复杂的欺诈案件时，不仅耗时且容易出错。随着大模型技术的兴起，越来越多的金融机构正在试图通过智能化手段，提高风险感知和风控决策的能力，从而降低人工失误率，提升运营效率。</p><p></p><p>在「金融数字化管理和运营实践」专题论坛，平安壹钱包大数据研发部算法负责人王永合将深入探讨如何利用大模型技术，<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6031">实现账户风险管理的数字化转型</a>"，以及这一转型如何为金融机构带来实质性的价值。</p><p></p><p>可以看到，随着应用的日渐深入，金融机构对于技术开始从摸索转变为“要效益”、“要闭环”。在「金融大模型应用实践和效益闭环」专题论坛，新希望金融科技风险科学部AI中心总经理王小东将在演讲《<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6011">大模型下的多模态智能风控落地实践</a>"》中介绍新希望金融科技AI团队利用视觉大模型AI风控、语音大模型AI风控、音视频+AI交互式智能风控等技术解决大模型浪潮下的各种新型信息伪造和欺诈攻击手段的技术算法解决方案和落地效果，并介绍在OCR、活体检测、视频双录环节的应用案例。</p><p></p><p>据了解，该方案已在600多家银行应用。通过大模型、交互式视频AI风控等实现了生成式大模型引发的新型金融反欺诈检测与识别以及破局之道，为金融反欺诈提供了一种新的解决方案。</p><p></p><h2>营销是大模型见效最快的场景吗？</h2><p></p><p></p><p>从用户角度来看，AIGC带来更智能、更便捷的体验。智能客服能够理解更复杂的语言，提供更精准的答案；个性化推荐系统可以根据用户喜好和需求，提供更有针对性的金融产品和服务；数字人可以全天候在线，提供更亲切、更人性化的服务。</p><p></p><p>对于银行来说，AIGC是一个能够赋能业务、提升效率的强大工具。AIGC可以帮助银行更精准地进行营销，通过分析用户数据，向不同群体推送个性化的金融产品信息，提高营销转化率。此外，AIGC还可以协助银行进行风险控制，识别潜在风险，帮助银行做出更明智的决策。</p><p></p><p>与此同时，AIGC还能为银行带来全新的业务模式。例如，数字人直播可以为用户提供更生动的金融知识讲解，更直观地展示金融产品，提升用户参与度和满意度。</p><p></p><p>围绕以上多个维度，在「数据资产化运营与数据智能应用」专题论坛，广发银行信用卡中心商业智能负责人徐小磊将<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6012">通过实际案例展示AIGC如何为金融科技带来变革</a>"。</p><p></p><p>针对整个体系化的银行运营和营销体系，富滇银行数字金融中心副主任李涛将在「金融数字化营销实践」专题论坛中分享《<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6048">数智化时代商业银行运营营销的“坑”与“路”</a>"》，从几个发人深省的“灵魂拷问”出发，如银行公私域运营模仿互联网电商可持续吗、北极星指标是个坑吗、而全的指标标签体系真的能赋能银行数字化营销吗等等，介绍富滇银行自身的答案和解法以及在这一过程中的人工智能应用实践。</p><p></p><p>与此同时，在「金融数字化管理和运营实践」专题论坛，度小满数据智能经营模型负责人李东晨还会进一步聚焦运营场景，分享《<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6005">基于因果推断的智能经营模型体系</a>"》，帮助听众了解信贷领域的经营模型框架体系，理清从预测到决策因果推断技术如何更好地支撑企业决策优化问题，以及从营销到盈利因果推断如何支撑所有资源有限情况下的最优求解问题。</p><p></p><h2>大模型是研发人员的福还是“祸”？</h2><p></p><p></p><p>大模型如何服务于研发生产力，同时做到普惠化，一是AI的基础设施，二是着重于能够云化落地的业务，三是结合AI给企业带来切实的降本增效。</p><p></p><p>AI代码助手，如GitHub&nbsp;Copilot、&nbsp;CodeX等，已成为现代软件开发中不可或缺的一环，它们极大地加速了代码编写的进程，提升了工作效率。然而，伴随而来的是对代码质量、开发流程乃至开发者角色的深刻挑战。特别是在金融这一数据密集型行业，对代码精准性、数据合规性的要求严苛至极。如何让AI模型在金融行业的研发领域得以切实有效应用，真正助力研发人员提升效能，而非仅成为初级开发者的辅助工具或高级开发者的互动玩具，是我们亟需解决的问题。</p><p></p><p>在「金融研发效能提升路径与实践」专题论坛，<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6056">众安银行高级架构师汤杰</a>"将从架构设计、算法工程化融合、团队协作策略、工具选型与整合等多个层面，深入探讨在AI助手日益普及的背景下，如何构建一套既提升开发效率又保障代码质量的软件开发生态。同时，基于众安国际丰富的实践经验与分析反思，他还将分享对AI助手在软件开发中角色定位的前瞻思考，以及对AI辅助编程未来发展趋势的展望。</p><p></p><h2>怎么让AI为你打工？</h2><p></p><p></p><p>「智能体」被视为是AIGC规模化应用的第一入口。而随着大模型与智能体技术的快速发展，多智能体协同模式在在解决复杂金融问题方面展现出巨大的潜力。在实际的业务发展过程中，蚂蚁集团通过使用多智能体协同范式，克服了众多技术落地难点取得阶段成果。在「金融大模型应用实践和效益闭环」专题论坛上，蚂蚁财富投研支小助技术负责人纪韩将深入探讨<a href="https://fcon.infoq.cn/2024/shanghai/presentation/5996">多智能体协同范式在金融产业中的技术应用</a>"并分享经产业验证的优秀真实案例。</p><p></p><p>成本是眼下要解决的另一大难题。在大模型时代背景下，“精益地迭代”或成为推动技术进步的关键。如何更好地构造知识驱动引擎，助力企业构建专家智能体建设，实现知识的高效转化和应用——成为很多企业正在攻克的关键问题。文因互联董事长、创始人鲍捷博士将在「前沿金融科技探索与应用」专题论坛分享如何《<a href="https://fcon.infoq.cn/2024/shanghai/presentation/5944">精益地打造金融专家智能体</a>"》。在业务分析领域，以“财务反粉饰”为场景示例，讨论如何结合专家知识管理系统进行有效的财务反粉饰，同时分析在这一场景下大模型能够发挥的作用及其面临的挑战。</p><p></p><p>可以看到，尤其是在知识密集和作业密集型场景，大模型越有的放矢。嘉银科技在这两个领域进行了深入的探索和实践，例如ToB主流AI产品、职能单元助手、智能作业辅助等业务，最终实现了效益闭环与专家已知解和算法暴力求解的平衡。在「金融大模型应用实践和效益闭环」专题论坛中，<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6033">嘉银科技技术中心人工智能经理姜睿思</a>"将详细介绍具体的大模型落地过程，技术和方法论层面的实践经验。</p><p></p><p>此外，中关村科金资深AI产品总监曹阳也将介绍《<a href="https://fcon.infoq.cn/2024/shanghai/presentation/5993">基于知识助手的金融大模型应用实践</a>"》，帮助金融从业者理解并应对大模型应用中的成本问题，包括如何进行模型选型、评估投入产出等；深入探讨金融大模型的安全与合规问题，了解有效的数据保护和风险管理策略；同时，通过案例了解如何评估哪些场景适合作为金融大模型应用的切入点。</p><p></p><p>更多AIGC场景应用案例还在上新中，本届大会由中国信通院铸基计划作为官方合作机构，除了以上嘉宾之外，还有来自工银科技、北京银行、平安银行、中信银行、平安证券、蚂蚁集团等金融机构及金融科技公司的资深专家将现身说法分享其在金融科技应用实践中的经验与深入洞察。大会火热报名中，7&nbsp;月&nbsp;31&nbsp;日前可以享受&nbsp;9&nbsp;折优惠，单张门票节省&nbsp;480&nbsp;元（原价&nbsp;4800&nbsp;元），详情可点击链接或扫码联系票务人员咨询：<a href="https://fcon.infoq.cn/2024/shanghai/">https://fcon.infoq.cn/2024/shanghai/</a>"</p><p></p><p><img src="https://static001.geekbang.org/infoq/31/31ff5488cc076e04976f66fd5d9869c7.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/PSyuBLvTIBXb8w8OzLLx</id>
            <title>真·智能体峰会：MSRA、腾讯、网易、MILA 齐聚一堂 ｜AICon</title>
            <link>https://www.infoq.cn/article/PSyuBLvTIBXb8w8OzLLx</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/PSyuBLvTIBXb8w8OzLLx</guid>
            <pubDate></pubDate>
            <updated>Thu, 18 Jul 2024 07:09:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI Agent, 机器学习, 智能体, 安全性
<br>
<br>
总结: AI Agent 是通过机器学习和人工智能技术实现自主感知环境、做出决策并执行相关动作的智能体。随着应用场景日益广泛，AI Agent 面临着更深入理解人类社会行为、解决安全性和隐私保护问题以及提升决策过程透明度和可解释性的挑战。在未来发展中，需要关注智能体的构建、感知、认知和行动能力的提升，以及多智能体技术在不同领域的探索应用。 </div>
                        <hr>
                    
                    <p>AI Agent 正迅速成为大模型非常重要的应用方向，这些智能实体通过先进的机器学习和人工智能技术，能够自主感知环境、做出决策并执行相关动作。AI Agent 的应用场景日益广泛，包括但不限于数字员工、具身智能、个性化推荐等。</p><p></p><p>然而，这一技术的发展并非一帆风顺。AI Agent 需要更深入地理解人类社会行为，包括语言、情感以及复杂的社会互动，以更好地适应多样化的应用场景。同时，随着其在各行各业的广泛应用，安全性和隐私保护问题变得尤为关键，确保数据安全和用户隐私是企业必须优先考虑的问题。此外，AI Agent 的决策过程的透明度和可解释性也是提升用户信任、推动技术进步的重要方面。</p><p></p><p>为了深入探讨这些挑战，并探索智能体技术的未来发展，我们在8 月 18 日 -19 日的 AICon 全球人工智能开发与应用大会（上海站），精心策划了【AI Agent 技术突破与应用】论坛，专题出品人是 DeepWisdom（MetaGPT）创始人兼 CEO 吴承霖，他拥有十亿级用户的大规模 AI 落地经验；同时也是开源多智能体框架 MetaGPT 作者；NeurIPS AutoDL / NeurIPS AutoWSL / KDDCup OGB-LSC 等竞赛世界冠军；也曾获福布斯 30U30 等荣誉。</p><p></p><p>我们荣幸地邀请到了以下几位在智能体领域有着深刻见解和丰富经验的专家学者，他们将为我们带来一系列精彩的议题，共同探讨智能体的现在与未来。</p><p></p><p></p><h5>精彩议题一：</h5><p></p><p></p><p>如果你想了解构建智能体中需要考虑哪些组件？当下的智能体构建还存在哪些问题？以及智能体的未来发展会是什么样？那么，微软亚洲研究院高级研究员宋恺涛的主题分享《The Future is Here, A Deep Dive into Autonomous Agent》值得听听。他将从 AI 智能体的崛起入手开始分享，着重分析如何构建、评估以及轻量化 AI 智能体，当然他也会分享如何构建自我进化的 AI 智能体。有专家反馈说，这个技术国内和国际上都是顶尖的，错过可惜！</p><p></p><p></p><h5>精彩话题二：</h5><p></p><p></p><p>如果你想了解如何系统性增强 LLM Agent 的感知、认知、和行动，以提升其在不同任务中的应用效果，让你的智能体更加智能，那蒙特利尔大学 &amp;MILA 研究所助理教授刘邦的演讲不可或缺。</p><p></p><p>刘邦将会深入分析和对比不同环境和任务对 LLM Agent 感知、行动能力及认知推理的独特要求，并探讨如何通过技术创新解决这些挑战。</p><p></p><p></p><h5>精彩话题三：</h5><p></p><p></p><p>如果你是游戏圈的从业者，那你应该听过前段时间比较火热的永劫无间的 AI 队友，这个 AI 队友不仅能听懂玩家的话 (语音信息识别)、观察战场局势 (战局信息输入)、了解地图和英雄技能 (游戏机制学，甚至还借助诸多高手的大数据学会了高端操作。真是惊呆了许多参与游戏的人，那么这样的游戏 Agent 是如何构建的呢？</p><p></p><p>我们为你邀请到了网易伏羲语言智能组负责人张荣升， 他将以《可实时语音交流的游戏队友 AI Agent 创新应用》 为主题，为你展开分享。他将重点介绍如何基于易生诸相多模态 AI 技术，实现《永劫无间手游》中的可实时语音交流的游戏 AI 队友，并打造丝滑的多模态游戏交互体验。</p><p></p><p>通过他的分享，你可以了解到《永劫无间手游》中 AI 队友的实现方式，包括实时语音交流和多模态交互技术，以及了解如何通过 AI 技术提升游戏玩家的交互体验。</p><p></p><h5>精彩话题四：</h5><p></p><p></p><p>无独有偶，创新类型的 Agent 应用， 腾讯也有，我们荣幸邀请到了腾讯 PCG 大模型中台 Agent 技术负责人陈浩蓝，他将为你以《多智能体技术在开放剧情扮演玩法中的探索》 为主题展开分享。</p><p></p><p>他将为我们深入介绍开放剧情扮演玩法，从其基本概念出发，探讨与传统剧情扮演的不同之处以及它对玩家的吸引力。接着，他会概述当前在剧情生成和角色扮演领域的主要研究工作，包括关键技术框架和方法论。</p><p></p><p>随后，他将详细讨论使用多 Agent 技术生成开放且精彩剧情时遇到的技术挑战，以及在单场剧情中进行角色扮演时的难点，分享目前的研究进展和探索方向。</p><p></p><p>通过他的分享，你可以了解到开放剧情扮演玩法的任务背景和相关研究、以及了解基于大语言模型和多智能体技术解决相关问题的难点与探索进展。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7f/7f2f88d59994b6e527f276578f9e3b8b.jpeg" /></p><p></p><p></p><p>活动推荐：</p><p></p><p>InfoQ 将于 8 月 18 日至 19 日在上海举办 AICon 全球人工智能开发与应用大会，汇聚顶尖企业专家，深入端侧 AI、大模型训练、安全实践、RAG 应用、多模态创新等前沿话题。现在大会已开始正式报名，详情可联系票务经理 13269078023 咨询。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3c/3c696f9be4a5aac17ed8d957c5df7621.jpeg" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/CLrzPOWW6DuxEgIJr7hX</id>
            <title>大模型如何重塑企业知识管理？丨对话AI原生《云智实验室》</title>
            <link>https://www.infoq.cn/article/CLrzPOWW6DuxEgIJr7hX</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/CLrzPOWW6DuxEgIJr7hX</guid>
            <pubDate></pubDate>
            <updated>Thu, 18 Jul 2024 01:39:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 知识管理, 大模型时代, 企业价值, 甄知
<br>
<br>
总结: 本文讨论了知识管理在企业中的重要性，随着大模型时代的到来，企业知识管理面临新的机遇和挑战。甄知作为一站式知识管理平台，通过大模型技术重塑知识管理流程，提升企业效率，解决知识管理领域的难题，帮助企业构建知识体系，实现知识的显性化和智能化应用。 </div>
                        <hr>
                    
                    <p>知识管理是企业持续发展和创新的核心动力之一，长期以来却面临着效率低下的挑战。大模型时代的到来，为企业知识管理带来了新的机遇，如何通过大模型重塑企业知识管理全流程进而提升效率？如何拓宽知识的边界，挖掘知识的内在价值？大模型重构的知识管理又能为企业带来哪些价值？带着这些问题，在对话AI原生《云智实验室》栏目中，南网科研院知识管理专家林正平、百度智能云知识管理产品线总经理宋勋超与InfoQ编辑展开了一次深度探讨。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/eb/eb0615ae51f0c56a73578c4b14faa7d1.png" /></p><p>点击链接收看：《大模型如何重塑企业知识管理？》</p><p>https://www.infoq.cn/video/QkMRVDNg4MKJ9eyS6Y2E</p><p></p><h3>以下为本期栏目精华内容</h3><p></p><p></p><p>InfoQ：企业知识管理是什么？对于企业来说有何价值？</p><p></p><p>宋勋超：知识管理自上世纪60年代在国外逐渐兴起，1998年以后，国内的企业开始探索知识管理的应用。</p><p>知识管理发展主要分为三个阶段：</p><p></p><p>第一个阶段是基础数字化的阶段。在这个阶段的知识管理平台，更多的是将企业散落的各种数据进行归总和汇集，但没有对知识去进行深度的利用，初期的知识管理主要是文档系统。</p><p></p><p>第二个阶段是知识的信息化阶段。在这个阶段，企业将收集的个人知识和系统知识进行粗粒度的加工和整理，进而形成了知识门户或者是企业搜索。</p><p></p><p>第三个阶段是大模型时代的知识管理，在大模型的加持下，知识管理的全流程效率都有了显著的提升。大模型强大的理解能力，使得知识管理平台以往很难去解决的知识加工和应用问题都迎刃而解。</p><p></p><p>在第三阶段，知识管理可以更加深入到业务，使得知识管理在企业内更加具象化为一个可以永远传承知识的老师傅，为企业员工提供持续的知识供给，带给企业的价值可以更好地去被衡量。</p><p></p><p>InfoQ：甄知与传统知识管理平台有何区别？</p><p></p><p>宋勋超：甄知是国内首个大模型全面重构的一站式知识管理平台，可以全面重塑知识管理流程。其实知识管理领域一直有四个问题是非常难解决的，也是制约着知识管理发展的瓶颈：</p><p></p><p>• 知识源分散</p><p>• 知识获取难度高</p><p>• 知识更新慢</p><p>• 基于知识的应用薄</p><p></p><p>有了大模型，实际上知识管理进入了新的阶段。之前的这四个问题如果在小模型时代去逐一地去解决，需要耗费非常高的成本，每一个模型都需要去对它进行大量的数据标注和精调。而在现在的大模型时代，我们可以不用任何工程化的手段就能够解决上述问题，成本极低。</p><p></p><p>另外，我认为大模型能给知识管理带来的最大的变化，其实就是Agent技术。一个智能体它要去执行复杂的任务，其不仅要理解任务本身，更多的是形成知识驱动的解决范式，所以我也认为知识管理是大模型成熟应用的第一站。</p><p></p><p>InfoQ：南网科研院在知识管理上遇到了哪些难题？</p><p></p><p>林正平：南网科研院过去标准知识管理主要还是针对文档级别，过去一线的生产人员只能通过纸质的标准或者在标准平台上通过标准名进行信息检索，怎么样去驱使所有人用统一的标准去完成相关工作，是我们在整个企业运作过程中急迫要去解决的一个问题。这是标准知识管理面临的最大的问题。</p><p></p><p>InfoQ：甄知如何解决南网科研院在知识管理上的难题？</p><p></p><p>宋勋超：甄知和南网科研院已经合作了数个年头了，我们最初要解决的问题就是知识的显性化问题，很多企业也会有类似的需求——企业里有很多经验，这些经验都存在于老专家的脑子里，你们能不能给我提供一个平台或系统，从而把老专家脑子里的知识“萃取”出来？</p><p></p><p>这个愿景对应的需求就是“能够将各种规则，各种隐性的数据引入知识管理系统”。针对这样的原始的需求，甄知知识管理平台提供了多种形式的数据接入方式。比如API方式的接入，手动的上传以及推拉拽等等一系列的知识同步机制，企业能够将结构化、或者是大量存在的非结构化的数据，甚至是数据类的知识，快速地去接入到系统里。</p><p></p><p>2024年甄知还有一个比较重要的产品升级：平台对接了非常多的企业级的知识源，比如说confluence、wiki、各类企业网盘，甚至包括飞书、钉钉文档、企业微信文档，这些在企业内广泛存在的企业级的知识源，在甄知的平台上能够去快速地、无缝地去集成和接入，极大程度地提升整个知识接入的效率。</p><p></p><p>更为重要的是甄知平台提供AI驱动的协同编辑和智能化的写作工具组件，使得企业很多在日常工作里面能够去随写随记的这一部分知识，都能够实现即创作、即沉淀，效率提升极大，且管理成本较低。</p><p></p><p>InfoQ：甄知怎么帮助企业构建知识体系？</p><p></p><p>宋勋超：面向于大模型的应用，我们需要把原始知识、原始数据整理成大模型能够理解的形态。这也是知识管理传统的系统耗费人力最多的地方之一。甄知的知识管理平台提供三个比较核心的技术能力，第一是多模态解析的能力，第二是知识结构的解析能力，第三是知识要素的加工能力。</p><p></p><p>多模态解析的能力是指现在越来越多的企业，它广泛的数据存在于非结构化的文档里面，这些非结构化的文档里包含各种复杂的表、图以及大段的文字。以南网科研院为例，几万篇的国标、行标、企标，都是以PDF文档的形式存在。甄知借助大模型的OCR视觉理解能力，能够非常便捷、高效、快速地把企业内存量的多模态文档解析成章、条、目、段落，甚至是图表，这对于后面的知识加工成功与否非常关键。</p><p></p><p>另外，面向于企业级的搜索和RAG，甄知提供了可配置化的段落切分的能力，可以去配置固定的长度。最为重要的是甄知能够用智能化的手段非常精确的识别语义化单元，从而使我们的RAG和搜索能够达到一个非常精准化的程度。借助大模型，刚刚所说的这一系列加工过程，都能以非常高效的方式，实现大规模的生产，企业只需要去配备一些必要的知识运营人员，对大模型生产的结果进行审核、校验和入库，相比于传统的方式，至少提升了2-3倍的工作效率。</p><p></p><p>InfoQ：南网科研院是怎么搭建知识体系的？</p><p></p><p>林正平：南网科研院其实非常关注科技创新这个领域。我们有海量的，像论文、标准、专利还有项目成果等等相关的一些知识数据，以往我们的项目数据怎么分到技术体系树里面去是一个非常大的难题。过去我们更多是根据一些规则，就是专家去梳理一些文本规则来对这些资源进行分类。整个效率相对较低，另外它的准确率也不够高，需要大量的人工去审核。</p><p></p><p>现在有了大模型，还有相关的智能化技术加成后，在知识分类上我们会根据业务需求组建知识体系框架，通过模型来自动打标签，实现海量知识的快速分类，便于对这些知识进行分析还有查找，目前我们的效率提升还是比较明显。</p><p></p><p>另外就是在知识关联关系的挖掘上，以往我们的知识都是单兵作战，像我们有一个论文，我只能看到这个论文它自身的一些内容，在知识关联上很少能够挖掘。像标准也是，标准之间它有引用关系，以往我只能看到某一个标准，但是我不知道这个标准跟它相关的一些作业指导书有哪些，跟它相关的技术规范书有哪些？在知识的利用上还不是非常的深入，对于整个知识的关联发现也非常难去完成。有了智能化技术的加成，平台可以更好地去挖掘知识之间的关联关系，最终来构建知识图谱，支撑标准文、论文相关知识的推理以及问答等应用，这些对我们的帮助都是非常的大。</p><p></p><p>InfoQ：甄知通过大模型重构知识体系的技术突破点是什么？</p><p></p><p>宋勋超：对于知识组织而言，我们的知识都来源于各个孤立的业务系统，它本身并不具备体系化和组织化。本质上来讲，知识组织和体系的建立，就是非常复杂的一个多维分类体系的建立。</p><p></p><p>传统的知识管理时代基本上都是靠人，基于大模型非常强大的语义理解能力，我们能够通过简单prompt，可以做到Zero-shot和Few-shot这样的分类，可以让运营人员很少参与到这个过程，就能够去实现复杂分类体系的建设。过去知识图谱的技术，它从关系的挖掘、消歧再到鉴编，这一系列的环节依赖于非常多的小模型。现在有了大模型，实际上在一些应用场景里面已经验证基本上不太需要专业的数据标注，基于大模型本身的理解能力，结合知识图谱平台构建的机制，就能够在初始的准确率达到80%以上。</p><p></p><p>在大模型加持下，甄知在知识组织与知识关联方面展现出强大能力。目前，我们在应用层面已成功实现了基于知识关联的门户构建，以及个性化推荐等一系列功能，真正做到了“千人千面”。事实上，这些能力的实现都得益于大模型的引入，它使我们能够以更低的成本，对企业知识进行系统化的组织和管理。</p><p></p><p>InfoQ：甄知如何提升南网科研院的知识生产效率？</p><p></p><p>林正平：在有大模型之前，我们跟百度合作开展标准问答相关的一些研究，以往我们是采用知识图谱的方式，我还记得是在两三年前，我们组织了大量的专家对我们标准的指标体系进行了深入的梳理，总共是梳理了十几级的体系，最终通过知识抽取、消歧融合等等相关的一系列的工作，才构建了我们试点设备的标准知识图谱，最终才能支持标准的知识问答。</p><p></p><p>那有了大模型之后，现在其实我们需要专家参与的工作量已经大大的减少，目前基本上我们只需要给一些简单的问答，通过大模型去训练，就可以完成问答模型的构建以及问答应用的开发。这对我们知识管理人员来说是一个极大的效率提升，大大的解放了我们的生产力。我们现在构建了标准的问答，还有语义检索相关的服务工具，对标准知识的查阅效率已经提升50%以上。</p><p></p><p>目前，大模型还能辅助员工去编写标准相关的大纲、正文并可以撰写科研机构的科技报告的综述、背景等等的相关的一些内容，我们的编写效率也是提升了两倍以上，对企业的知识生产运行效率来说提升还是非常明显的。</p><p></p><p>InfoQ：甄知如何帮助企业用好知识？</p><p></p><p>宋勋超：知识要能够驱动创新，它必然是要和场景结合的。刚刚我们说知识管理在南方电网就好像一个老专家。但是这个老专家他其实应该是有角色的，面向于企业的高层管理人员，他可能更加关注企业的宏观经营数据，面向于企业的中层管理人员，他可能更需要去了解项目的进度；面向于技术人员，他可能更关心的是技术方案。企业里面的每一个关键角色对于知识的诉求都是不一样的。所以我们认为如果要在一个企业内广泛地实现知识的创新，就必须要将知识发散，也就是要把知识应用到业务系统里面，形成真正的业务和场景化的知识助理。</p><p></p><p>甄知实际上为企业提供了三方面的能力，以助力其更好地利用知识。首先是集中化的RAG能力，甄知打造了一个企业级RAG平台，确保企业中的每位员工在知识管理系统中提问时，都能真正获得有价值的答案。</p><p></p><p>其次，甄知的推荐系统能够通过分析企业内部多元异构的员工画像，实现千人千面的个性化推荐，以及面向业务场景的即时推荐。甄知的知识管理平台已经不仅仅是一个知识门户或知识检索系统，它已经升级为一个企业知识化Agent构建的平台。在这个平台上，我们基于构建的知识体系，能够打造出知识化的差旅助理、报销助理、销售助理，甚至HR助理等一系列由知识驱动的Agent助理。我认为，只有当搜索、推荐和Agent服务这三个层面在同一个平台上实现出色的平台化整合时，我们企业内部的应用场景才会变得非常多元化，企业的创新源泉才会源源不断。</p><p></p><p>最后，甄知覆盖了企业的知识生产、加工及应用全流程，最终能够将企业员工、知识应用与知识生产形成一个闭环与反馈机制。因此，在使用过程中，我们不断收集企业对于知识管理产品应用效果的反馈，这使我们能够不断提升知识生产与加工的效率。</p><p></p><p>InfoQ：南网科研院如何用好知识，赋能每一位员工？</p><p></p><p>林正平：南网科研院一直在推进从人找知识升级为知识找人。南方电网整个企业岗位非常多，对效率的要求以及安全的要求也非常的高，我们开展知识管理的理念主要是从用户、场景和知识三维一体的角度来进行开展，比如说我们的科研人员关注怎样快速去获取相关领域的高质量的报告、核心的论文、最新的研究成果以及历史相关的项目经验等信息。而我们的一线生产人员则关更加关注缺陷处理的方法以及作业的规范，每个岗位对于知识的需求都有存在一定的差异。</p><p></p><p>以往，我们主要通过检索的方式来获取知识。然而，在人工智能时代，我们更需要从用户的画像和使用场景出发，为用户推荐有针对性、高质量的知识，以提升知识的获取效率。通过知识管理，我们希望总结并沉淀最佳实践，同时做好经验的传承。我们的知识管理平台的使命就是支撑知识的汇聚、存储和利用，甚至将知识直接嵌入到业务场景中去，无需人工干预，从而更好地支撑企业的创新和高质量发展。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/kjEuszalPjjd6Zepefm3</id>
            <title>15年功臣、英伟达首席科学家在股价巅峰期黯然辞职：对不起自己拿的丰厚报酬？</title>
            <link>https://www.infoq.cn/article/kjEuszalPjjd6Zepefm3</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/kjEuszalPjjd6Zepefm3</guid>
            <pubDate></pubDate>
            <updated>Wed, 17 Jul 2024 09:37:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Jacopo Pantaleoni, 数据转换为图像, AI革命, GPU
<br>
<br>
总结: Jacopo Pantaleoni是一位专长将数据转换为图像的科学家，曾在英伟达担任首席科学家，为图形处理单元开发了旗舰产品。他的工作对数字图像世界、电子游戏、电影艺术以及生物信息学都产生了重大影响。然而，他选择离开英伟达，批评AI技术在广告经济中的应用，认为这可能破坏人们的生计。他认为人工智能代表计算能力革命的最高峰，但也警示AI技术可能被用于降低人力成本，最终危害人们的生计。 </div>
                        <hr>
                    
                    <p>很多朋友可能没听说过Jacopo Pantaleoni的名字，但或多或少应该见证过他的工作成果。Pantaleoni的专长是将数据转换为图像，作为首席科学家在英伟达供职期间为其开发了旗舰产品的图形处理单元。他的工作帮助塑造了超现实的数字图像世界，进而构成了从电子游戏到电影艺术、再到作为DNA测序研究核心的生物信息学的一切。除此之外，他的贡献对于AI革命同样意义重大，支撑起OpenAI及其他从业企业开启了这个前所未有的时代。</p><p>&nbsp;</p><p>但去年的时候Pantaleoni选择离开英伟达，当时正值OpenAI主导的AI热潮将这家图形芯片巨头推向科技界的顶峰。辞职的原因，他表示自己“需要一段时间来反思，必须承认我对社会的间接贡献大多数并不太正向。”</p><p>&nbsp;</p><p>此后他不仅直言不讳地批评AI技术没用对地方，还批评了英伟达在这场浪潮当中扮演的角色。在他看来，AI技术不应该用到广告经济上，更不应该破坏人们的生计。</p><p>&nbsp;</p><p></p><h2>在英伟达高光时期独自离开的首席科学家</h2><p></p><p>&nbsp;</p><p>虽然最近15年Jacopo Pantaleoni在为英伟达工作，但实际上他在高性能计算领域已经拥有25年的工作经验，并曾两次获得高性能图形学的“时间考验（Test of Time）”奖，该奖项主要是表彰那些对计算机图形学产生广泛且持久影响的研究。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/68/683333447d6e39b691c09ea46f857a14.jpeg" /></p><p></p><p>&nbsp;</p><p>他为实时光线追踪创建的基础算法——这一技术是当前许多元宇宙和数字人类项目的核心——已经在革命性地改变今天的游戏行业。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/32/323ddf9ab42b1f4f46762c751ee1e105.jpeg" /></p><p></p><p>&nbsp;</p><p>这些渲染技术能够帮助我们从不存在任何视觉元素的三维数据集模型生成极为逼真的图像。身为一名计算机科学家，Pantaleoni帮助开发了詹姆斯·卡梅隆里程碑之作《阿凡达》的图形系统，并为《黑客帝国》背后的技术公司之一Mental Images编写了程序。</p><p>&nbsp;</p><p>同样的技术在电子游戏中也有所体现，这也是英伟达等厂商在90年代末到21世纪初得以崛起的原因所在。英伟达开始生产硬件，将这种计算机渲染技术带给普罗大众，最直接的作用就是运行电子游戏。早在2010年，他和黄仁勋等人就意识到，计算机图形所需要的计算能力与机器学习/人工智能等任务需要的算力是一回事。这正是黄仁勋出色才能的体现，而Pantaleoni则努力提高英伟达硬件的可编程性，帮助越来越多的人能够使用和支配这些算力，可以说Pantaleoni是 GPU 大规模并行高性能计算领域的早期贡献者。</p><p>&nbsp;</p><p>发展到现在，从人工智能热潮中获益最大的公司莫过于英伟达，黄仁勋曾说过该公司处于一场新“工业革命”的核心。训练大规模的大型语言模型并运行它们需要巨大的计算能力。英伟达的GPU已经是人工智能行业的黄金标准，亚马逊、谷歌、微软等云计算巨头和人工智能初创公司都在争相购买。</p><p>&nbsp;</p><p>自 2023年初以来，英伟达股价上涨了约785%，仅去年一年净利润同比大涨581%。今年6月，英伟达短暂地成为全球最有价值的公司。现在英伟达的市值已突破3万亿美元，相当于该公司2018年8月约1500亿美元市值的20倍。著名投资人詹姆斯·安德森更是大胆预测，在未来十年内，其市值有望达到惊人的50万亿美元，这一数字将超越当前标准普尔500指数所有公司的总市值。</p><p>&nbsp;</p><p>喜人的业绩之下，让押注英伟达股票的投资者们，分食了这场科技造富盛筵。就像一位投资者说的：“这种机会是十年难遇的，在我看来，英伟达一年之内，帮很多投资者赚到了原本用很多年才能赚到的钱。”</p><p>&nbsp;</p><p>作为英伟达的首席科学家，Pantaleoni也持有英伟达公司的股份，这些股份让他虽工作内容小众但收入丰厚，“哪怕是在股价上涨之前，就已经对我们的生活方式产生影响。”他们这群图形专家，“已经成为掌握全球大多数财富的那1%群体中的一员。”</p><p>&nbsp;</p><p>但他选择了在英伟达的上升期离开了这家他工作了十五年的公司，“我意识到，我对于这个社会的贡献，无论是直接还是间接的，其范围都比我想象中要大得多。我需要一段时间来反思、特别是以批判性的方式审视这一点。而且必须承认，我对社会的间接贡献大多数并不太正向。”</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>没想到自己的成果被用来摧毁普通人的生计</h2><p></p><p>&nbsp;</p><p>Pantaleoni毕生致力于提升计算和模拟技术，在他看来，人工智能从根本上代表着这一进程的最高峰，是一种可以融入和改进我们计算模型的全新且功能强大的工具。作为一种工具，人工智能可以在疾病诊断、基因组学、药物发现和气候模拟等领域发挥巨大作用。</p><p>&nbsp;</p><p>“我认为，将人工智能革命视为冰山一角更为恰当，其背后是更广泛、更深刻的计算能力革命。” Pantaleoni表示，“自从计算机发明以来，我们的计算能力一直在呈指数级增长。70 多年来，其速度每 18-24 个月就会翻一番。”</p><p>&nbsp;</p><p>虽然AI对像癌症检测这样的任务自动化确实非常有用，但目前极少数超大型公司在资本、基础设施和人力资源方面占据圧倒的优势地位，他们没有选择用这些技术造福人类，而是投放到了“注意力经济”和可能摧毁人们生计的地方：</p><p>&nbsp;</p><p></p><blockquote>“对人工智能的这种运用与所谓的注意力经济密切相关，注意力经济是机器学习算法和服务创造的新型市场，旨在抓住并保持用户注意力。这些产品通过不断向我们的大脑灌输大量程序生成和传播的 (错误) 信息，并限制我们集中注意力和花费时间将信息提炼成知识的能力，从而导致更广泛、更严重的认知衰弱。随着假新闻和阴谋论的疯狂传播，这一进程正日益将民主置于危险境地。&nbsp;然而，这场革命的另一个需要最大关注的方面是人工智能如何被应用于降低人力成本。生成式人工智能已经部分地实现了诸如客户服务、数字插画和计算机编程等领域的自动化。但真正危险的是，它将扩展到更敏感的领域，例如教育（例如通过数字人）和安全（例如自主武器）。虽然这些发展使商业运营成本降低，但它们最终也可能破坏人们的生计。&nbsp;此外，在更基本的层面上，我们必须记住，历史上自动化一直被广泛用作权力和权力集中的工具。我们已经看到，人工智能的进步正在推动科技领域一小部分超大型公司的资本、基础设施和人力资源的集中。”</blockquote><p></p><p>&nbsp;</p><p>自己创造的技术最终却是以这样的方式在发挥作用，这让他感到很意外，“我一生都致力于用技术手段将数学变成艺术，并将计算变成科学研究的工具——例如DNA测序。所以看到这项技术最终主要被用于广告宣传和注意力经济，这是有点令人失望。”</p><p>&nbsp;</p><p>“AI造成的风险是已知的。Sam Altman之类的人物提出的所谓灭世威胁，其实是想要以混淆视听的方式转移人们对最紧迫问题的注意力。毕竟他们往往是既得利益者，把水搅浑更有好处。”目前的主要风险在于权力的过度集中，进而导致工作岗位流失。另一个主要问题是，计算技术正在对整个人类社会施加愈发规模化的认知弱化压力。</p><p>&nbsp;</p><p>“我们将大部分注意力放在专门设计的算法上，而这些算法存在的意义就是娱乐我们的视觉皮层——这其实就是我一生中贡献最大的领域。我主要从事的就是视觉计算。视觉计算的力量实际上也正是英伟达等企业能够取得成功的原因。”</p><p>&nbsp;</p><p>Pantaleoni意识到不对劲的时候，大概是2014年到2015年那会，他开始对自己工作成果所产生的影响有了一种奇怪的感觉。“我见证了巨大的转变，看到谷歌和亚马逊这类厂商成为高算力硬件的大客户，并且在为完全不同的多种用途开发大规模并行计算能力。他们开始大规模使用机器学习技术，而目的却是为了靠广告宣传和吸引注意力赚钱。”</p><p>&nbsp;</p><p>“我认为这其实也就是谷歌获得成功的秘诀。”他们在本质上是一家机器学习公司，也是第一批从计算机设备的可扩展性中获得巨大利益的企业。以往一切制造公司都面临着收益递减的困扰，而他们是第一批克服了这种困境的厂商，在实质上完成了收益递增的转型。简单来说，如果我们有一家传统制造企业，那想要扩张就必须得向更多人力雇员支付更多工资。这就要求我们采购更多上游产品和原材料。但与之截然不同，谷歌这样企业的原材料却是完全免费的，单纯只是信息。他们要做的，就是投入越来越多的计算能力来处理这些素材。</p><p>&nbsp;</p><p>但作为英伟达的一员，实际上很难为此做出改变，英伟达虽然没有为注意力经济直接做出贡献，但却是支撑注意力经济正常运转的引擎。像英伟达这样的厂商没办法说“我们拒绝为这样的市场提供支持”。</p><p>&nbsp;</p><p>Pantaleoni之所以选择离开，就是意识到从企业内部改变结果根本就没有可行性。他认为从根本上打破这种对社会的负面影响的唯一方式，就是投身到监管事业中去。“我正尝试转型成为公共专家。我为监管机构提供咨询，也曾受邀在会议上发言。我不知道自己还要不要在技术这条路上走下去——也许会，但我必须找到真正符合自己对社会产生积极影响这个愿望的切入点。”</p><p>&nbsp;</p><p>比如说跟医学相关的项目，都是在尝试以积极、而非剥削的方式依靠技术来解决医疗难题。或者开发出更底层的技术——这类技术不会在公众当中引发高度关注，而是更加边缘化。或者是其他一些跟当今这些AI助手定位完全相反的技术，比如有助于激励孩子们参与社交、或者更加重视学习，乐于利用自己的智力来解决更多问题等等。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://nymag.com/intelligencer/article/why-i-quit-nvidia-at-the-start-of-the-ai-boom.html">https://nymag.com/intelligencer/article/why-i-quit-nvidia-at-the-start-of-the-ai-boom.html</a>"</p><p><a href="https://medium.com/authority-magazine/scientist-and-writer-jacopo-pantaleoni-on-the-future-of-artificial-intelligence-a15d7e5b447d">https://medium.com/authority-magazine/scientist-and-writer-jacopo-pantaleoni-on-the-future-of-artificial-intelligence-a15d7e5b447d</a>"</p><p><a href="https://cybernews.com/editorial/democracy-in-danger-artificial-intelligence-supercomputers/">https://cybernews.com/editorial/democracy-in-danger-artificial-intelligence-supercomputers/</a>"</p><p><a href="https://www.researchgate.net/profile/Jacopo-Pantaleoni">https://www.researchgate.net/profile/Jacopo-Pantaleoni</a>"</p><p><a href="https://x.com/jpantaleoni">https://x.com/jpantaleoni</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/rIqr6fSs2TwBaOyEAUs2</id>
            <title>场景融合与 ROI 考量：金融大模型落地的两大困境有解吗？</title>
            <link>https://www.infoq.cn/article/rIqr6fSs2TwBaOyEAUs2</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/rIqr6fSs2TwBaOyEAUs2</guid>
            <pubDate></pubDate>
            <updated>Wed, 17 Jul 2024 09:06:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 金融行业, 大模型应用, 技术落地, 业务场景
<br>
<br>
总结: 金融行业作为大模型应用的理想领域，具有专业知识密集、数据驱动、业务流程复杂性等特点，大模型技术在金融领域的落地应用面临着技术与业务场景结合、成本投入回报、合规安全需求等挑战。在金融领域，大模型技术已经开始在核心业务中深化应用，带来实际效益提升，但金融机构在技术落地过程中仍然保持激进与保守的态度。 </div>
                        <hr>
                    
                    <p>金融行业被视为大模型应用的理想领域，从内因看，是因为金融本身具有专业知识密集、数据驱动、业务流程复杂性三个显著特点，而三大特点恰好与大模型理解能力、记忆能力、逻辑推理等优势高度吻合。从外因看，在政策驱动和市场热度的双重助力下，对于每一家金融机构来说，不采用大模型几乎是不可能的。</p><p></p><p>但是，在技术具体落地过程中仍然有很多阻力和困境。比如，技术能不能与业务场景紧密结合从而给企业带来实际的效益提升，巨大的成本投入带来的 ROI 是否划算，大模型技术底层能力是否足够满足对合规安全有着严苛要求的金融业务需求等等。</p><p></p><p>在日前的<a href="https://www.infoq.cn/video/YMGQmvSmA3ZTjDFVh13K"> InfoQ《超级连麦. 数智大脑》xFCon 直播</a>"中，我们邀请到了度小满金融技术委员会执行主席 / 数据智能应用部总经理杨青，以及文因互联董事长、创始人 / 中国中文信息学会语言与计算专委会金融知识图谱工作组鲍捷博士围绕以上话题进行了深入的探讨。</p><p></p><p></p><blockquote>在 8 月 16-17 日将于上海举办的<a href="https://fcon.infoq.cn/2024/shanghai/">&nbsp;FCon 全球金融科技大会</a>"上，杨青老师将担任大会联席主席，与组委会共同品控大会内容质量，并在大会 Keynote 分享其在大模型领域的最新探索与实践。此外，鲍捷老师也将在「前沿金融科技探索与应用」专题论坛上，深入分享如何《<a href="https://fcon.infoq.cn/2024/shanghai/presentation/5944">精益地打造金融专家智能体</a>"》 。大会更多演讲议题已上线，点击链接可查看目前的专题安排：<a href="https://fcon.infoq.cn/2024/shanghai/">https://fcon.infoq.cn/2024/shanghai/</a>"</blockquote><p></p><p></p><p>以下内容根据对话整理，篇幅有删减：</p><p></p><h3>落地现状：不能跟场景紧密结合的技术没有出路</h3><p></p><p></p><h5>InfoQ：在去年的 FCon 大会上，杨青老师表示，未来 AI 将在重塑金融业乃至整个社会的生产关系中发挥关键作用。是否可以分享一下，从去年到现在，金融行业在大模型领域做了哪些新的探索？有什么新的实践成果和变化？</h5><p></p><p></p><p>杨青： 金融行业之所以是大模型应用的理想领域，是因为金融本身具有三个显著特点：专业知识密集、数据驱动、业务流程复杂。这些特点与大模型的强项——理解能力、记忆能力、逻辑推理——高度吻合。因此，大模型的应用能够极大地推动金融行业的生产力和生产关系的变革。</p><p></p><p>回顾过去一年，金融行业对大模型的探索已经从广泛的尝试转向了对核心业务的深化。去年，我们见证了一场“百模大战”，众多金融企业积极参与，探索大模型的潜在价值。经过一年的实践，大模型已经在多个业务场景中落地生根。例如，一些金融企业在人工智能大会上发布了自己的产品，展示了大模型在企业内部的实际应用。面向消费者的金融产品，如蚂蚁的余额宝、同花顺等，在理财、投研、投顾等方面，通过大模型技术的应用，不仅替代了部分人工工作，还为用户提供了更优的体验和效果。</p><p></p><p>我认为，过去一年最重要的变化是大模型从探索阶段走向了实际落地，这种转变为用户带来了实实在在的效益。随着大模型技术的普及和企业内部员工对其的掌握，大模型将在更多业务流程中得到应用，进一步提升我们的生产力。</p><p></p><h5>InfoQ：事实上，传统金融机构由于业务的特殊性，对于新技术的引入一直是“既激进又保守”，请问鲍老师，从您的角度来看，现阶段国内金融领域的大模型落地进展处于什么阶段？</h5><p></p><p></p><p>鲍捷博士： 去年，我们大家都处于学习阶段，因为缺乏必要的硬件资源，比如显卡，所以基本上大家都在学习和准备。那时候有个笑话说，去年真正赚到钱的只有卖显卡的和卖课程的。但到了去年年底，随着预算的到位，今年我们开始看到各种场景下大模型的具体落地实施。</p><p></p><p>这个落地过程，正如你所说，是“既激进又保守”。之所以说激进，是因为大模型已经成为国家战略的一部分，对于每一家金融机构来说，不采用大模型几乎是不可能的。即便他们的基础条件还不够成熟，为了避免落后，大家都会尝试引入一些大模型相关的技术。但在实际落地过程中，大家又表现得非常保守，因为按照技术发展的一般规律，新技术通常会先在领先的机构中尝试，然后逐步渗透到其他机构，这个过程可能需要 3 到 5 年的时间，从头部客户到腰部客户，再到长尾客户。</p><p></p><p>但这次大模型的落地有所不同，它受到了市场的疯狂炒作，所以几乎每家机构在应用尚未完全成熟的情况下都不得不尝试引入大模型。在这种背景下，大家在引入过程中自然会采取保守的策略。那么，什么策略是一定不会错的呢？那就是先提升算力。所以我们最近看到了许多大额的算力订单，有的上千万，有的上亿。但这些大单背后的实际应用却相对较小。</p><p></p><p>我最近走访了许多客户，包括金融客户和制造业客户。我问他们，作为国内行业领先的企业，愿意为国内头部大模型厂家的基础模型软件落地投入多少资金？结果发现，即使是 100 万、50 万他们都不愿意投入。从这个角度来看，他们的态度是非常保守的。但同时，也有大规模的应用正在发生，这些应用一定是基于业务需求的。</p><p></p><p>从激进的角度来看，大家肯定会尝试引入大模型。但从保守的角度来看，大模型无论是开源的还是闭源的，如果没有很强的业务属性，是不可能拿到百万以上的订单的。比如我们最近在做反洗钱的应用，如果我只是做一个纯粹的知识库应用，那又有什么竞争力呢？核心在于，你是否熟悉反洗钱的业务规则、法规解析和建模，以及常规的反洗钱套路。只有将这些纯业务性的东西与大模型技术深刻结合，才有可能实现落地。</p><p></p><p>我们最近统计了一下，从去年大模型开始到现在，我们已经有数十个大模型落地案例了，聚焦在两个行业里，一个是金融，一个是航空。这些案例都是基于强业务驱动的，不是那种只卖显卡、卖算力的，而是真正在业务场景中发挥作用。所以从这个角度来说，大家说今年是大模型应用的元年，我相信这是对的。在金融领域是这样，在其他领域也是如此，只有紧密结合应用和领域场景，才能有广阔的发展空间。</p><p></p><h5>InfoQ：从目前来看，大模型技术应用主要集中在哪些金融业务场景？</h5><p></p><p></p><p>鲍捷博士：底层的核心是构建各类知识库，包括法规知识库、投研知识库等。这些知识库能够对金融文档，尤其是 PDF 文档，以及各种信披材料、说明书和市场文档进行解析和搜索。相对而言，更复杂的应用是各类核查，如法务核查、财务核查、合同核查和银行流水核查。这些核查在大模型出现之前就已存在，但大模型显著提升了核查的泛化能力。以往的系统相对固定，数据模式和 schema 需要事先定义。而大模型提供了即时的、实时的数据生成和业务规则更新能力，这是以往难以实现的。因此，上半年在这一领域取得了显著的发展。</p><p></p><p>另一个发展迅速的领域是写作协作场景。如今，券商和银行的每个部门都有写作需求。实际上，自 2016 年以来，我们已经开发了大量的机器自动化写作应用。许多人可能没有意识到，监管机构发出的问询函底稿都是机器生成的。四五年前，我们与中国头部券商合作，当时还没有使用大模型技术，投行底稿的 80% 以上内容都是机器生成的。最近，我们与另一家头部投行合作，复制了相同的过程，但与四年前相比，人工消耗减少了 90%。这是大模型技术为整个行业带来的生产能力的巨大提升。</p><p></p><p>当然，还有许多其他应用场景，如客户问答场景中的问答机器人，包括客服、投研助手或内部运维管理助手。在大模型出现之前，数字员工、远程银行等已经存在。有了大模型之后，这些应用变得更加丰富和多样化。</p><p></p><p>目前大模型的应用主要集中在内部提效方面。 例如，在内部 IT 部门，大模型可以发挥重要作用，帮助他们编写代码、SQL 查询，以及进行更好的商业智能（BI）分析。客服领域由于监管机构的严格要求，目前大家普遍持谨慎态度，不敢轻易使用自动化工具。</p><p></p><p>杨青： 大模型的能力在提升内部员工效率 方面展现出了巨大的潜力。目前，大模型最快的应用之一就是帮助员工更有效地获取和理解私有数据和知识库中的信息。通过大模型，员工可以更快地访问所需知识，减少理解知识的时间，从而提升工作效率。</p><p></p><p>此外，大模型还可以作为员工的专业助手，例如在编写代码或进行数据分析时提供帮助。对于不太熟悉的领域，大模型能够协助解决问题，成为数据分析的小助理。在理财投顾等复杂业务领域，大模型也在不断尝试突破，提供逻辑和业务上的支持。</p><p></p><p>从我过去一年的观察来看，初级应用已经在许多企业中落地，尤其是在提升员工绩效和生产力方面，已经取得了显著的变化。企业根据自身的实际需求，不断优化和提升大模型的使用情况。然而，要让大模型更好地帮助企业提升效益，还有很多工作要做。一方面，大模型的底层能力需要不断地提升；另一方面，在业务流程中如何更好地嵌入大模型也是一个关键因素。大模型在未来还有很大的潜力等待我们去探索和开发。</p><p></p><h5>InfoQ：鲍老师提到文因互联目前服务的客户主要分为两大类：航空业和金融业。这两个行业之间存在显著的差异，我们在提供服务过程中会面临哪些不同的挑战？或者说，我们提供的产品具有较好的通用性能够同时满足这两个行业的特定需求？</h5><p></p><p></p><p>鲍捷博士： 自 2015 年以来，我们在金融领域深耕了七八年时间，专注于这一领域，不断沉淀经验。我们发现，工具层面需求基本相似，比如阅读、编写和查询文档，只是每个领域都有其特殊性。例如，在金融领域，我们需要阅读信息材料；而在航空领域，则需要阅读维修手册、飞行员手册和标准操作程序列表。不同领域中存在相似的系统和问题。</p><p></p><p>在航空领域，例如，有飞行品质控制系统，飞机每秒可以产生 3000 个到 20000 个数据点，这些数据点需要根据业务规则进行分析。例如，飞机着陆时如果加速度超过 1.8g，就可能造成过载，损害起落架。而在金融领域，我们每天都在处理类似的合规问题，即某个指标超过阈值时的应对策略。无论是航空还是金融，本质上都涉及到数据的变化和语义理解。在金融领域，我们进行指标对齐，而在航空领域，则需要进行数据译码，我们也在这个过程中开发了中国首个国产飞机数据译码器。</p><p></p><p>这些过程无论是数据理解还是业务知识建模，本质上都是知识库管理系统的一部分。不同行业的适配核心在于：一是理解数据的语义，二是将行业知识快速转化为可计算的规则。 大模型在这里发挥了核心作用，尤其是在业务规则建模方面。以前，这需要昂贵的业务分析师、产品经理、算法工程师和应用工程师共同完成，并且必须在设计时就固定下来。在新业务规则的实时添加方面存在很大困难。而大模型可以帮助我们将底层业务系统变成一个可以接受自然语言描述的系统，业务分析师可以实时地将业务规则以自然语言的形式添加到生产系统中，实现了热插拔和系统演化，将原本静态的业务分析系统转变为动态的，这大大提升了我们的跨领域服务能力。</p><p></p><h3>挑战与出路：“死抠”成本和 ROI</h3><p></p><p></p><h5>InfoQ：从去年到现在 AI 大模型在金融行业的应用实践探索主要还是集中在非核心业务场景，要进入核心业务场景，目前面临的主要障碍是什么？要充分发挥大模型的潜力，还需要解决哪些问题呢？</h5><p></p><p></p><p>杨青：大模型的应用潜力受到技术底层能力和使用者理解的双重影响。 首先，大模型在底层核心技术上可能还不足以应对复杂的决策问题。以风控为例，风控的核心在于风险判断和决策，这通常属于传统人工智能的监督学习范畴。如果使用大模型进行风险评估，可能会遇到幻觉问题和缺乏可解释性，这直接限制了大模型在核心业务流程中的应用。</p><p></p><p>其次，对大模型的理解和应用还不够深入。目前，可能只有少数人真正掌握了大模型的使用技巧。大多数人对大模型的了解有限，特别是在企业内部，可能只有少数专业人士真正了解大模型。这限制了大模型的推广和应用范围，阻碍了其在更多场景中的尝试。</p><p></p><p>虽然大模型在处理复杂角色方面存在局限，但它可以与其他技术结合，如通过 Agent 技术提升大模型的能力，或使用 RAG 技术解决幻觉问题。此外，通过不同的提问和输入，可以挖掘大模型的潜在能力。未来，随着开源和闭源模型的不断强化，技术将不断升级，为大模型的应用提供更强大的支持。同时，随着大模型概念的普及，越来越多的人将逐步了解、接受并学会使用大模型，许多问题也将在实践中得到解决。</p><p></p><p>鲍捷博士： 新技术的出现和普及总是伴随着挑战和问题，大模型也不例外。未来几年，大模型可能不再流行，新的技术将取而代之。这是一个普遍现象，技术发展总是伴随着业务、技术需求的双轮驱动。然而，在当前阶段，业务需求是推动技术应用的主要动力，因为如果仅依靠技术驱动，最终可能只会是硬件销售（如显卡）。</p><p></p><p>目前一个核心问题是商业模式。我最近拜访了许多大模型公司，我们自身也是应用大模型的公司，但我发现，即使是行业内的专家，也难以给出一个令人满意的大模型商业模式的答案。至少在国内，基础大模型的商业模式似乎尚未找到。</p><p></p><p>应用大模型的落地核心在于成本控制。我们必须从用户的角度出发，考虑成本问题。例如，目前显卡的价格成本非常高，这并不是所有客户都能承受的，尤其是腰部和长尾客户。</p><p></p><p>另一个问题是投资回报率（ROI）。目前，许多应用无法准确计算 ROI，即客户投入资金后能获得多少回报。此外，还有总体拥有成本（TCO），这不仅包括算力和基础模型的成本，还包括运营成本、推理成本、人力成本。因为如果大模型没有进行业务上的深度定制，其数据处理的准确度通常不会超过 70%，这在 ToB 应用中是不可接受的，尤其是在金融领域。</p><p></p><p>目前市场上的卡点在于，许多人过于关注算法、算力和数据，而忽视了业务细节。基础大模型厂商可能还在尝试早期的商业模式，但其报价可能与市场现实脱节，导致他们不得不与大型硬件厂商合作，一起走单。这是当前市场上最真实的卡点。</p><p></p><p>解决大模型应用问题的理想途径是通过众多的“草根应用”来实现，让这些技术在成千上万甚至上百万的小应用中得到实践，从而积累人才和应用经验。然而，现实情况并非如此。</p><p></p><p>由于政策、投资和采购等多方面的原因，当前市场并不丰富，缺乏草根层面的创新和应用。 这导致了整个市场过于集中力量办大事，而应用的广度和深度都不够充分，也没有给予足够的时间来让这些应用自然成长和成熟。在这种背景下，一些技术和解决方案被迫快速上马，而不是经过充分的测试和优化。这种做法可能会造成问题，因为它没有允许市场通过广泛的实践来发现和解决应用中的卡点。</p><p></p><h5>InfoQ：在 FCon 大会上，鲍老师将带来 《精益地打造金融专家智能体》 的议题分享，那么这样的金融专家智能体主要应用于哪些金融业务场景呢？</h5><p></p><p></p><p>鲍捷博士： 从具体场景来看，大模型的应用不仅限于工具层面，还深入到各种业务场景中。例如，我们最近在帮助某银行构建托管系统，该系统涉及到大量的对账、指令分拣和合同比对等工作。虽然在大模型出现之前，这些工作也在进行，但现在我们可以以更低的成本完成这些任务。目前有许多 Agent 平台，通过这些平台的流程自动化，可以大幅提升工作效率。我们还与某个证券交易所合作，帮助他们开发公告的自动化处理系统。这项工作我们从 2017 年就开始了，现在我们用大模型重新开发，完全基于提示工程，不需要编写一行代码。对于核心公告类型，我们能够达到 87% 的准确度，并且通过后处理进一步提高准确度。</p><p></p><p>我们计算了成本，发现实现这种智能体构造的成本与 2017 年相比下降了 100 倍，即使与 2021 年相比也下降了 10 倍。这表明大模型确实为这一领域带来了巨大的生产力提升。这种技术开发过程的平民化也是一个显著进步。2017 年，我们的团队由海归博士领导，团队成员至少是 985 高校的硕士。而现在，一些优秀学校的实习生就能够参与这项工作，这是工具链成熟带来的巨大飞跃。</p><p></p><p>尽管如此，业务系统的难度并没有减轻，业务知识的建模依然是一个高度复杂的过程。例如，在银行内部的智能化运营中，包括监管报送等复杂系统，如 1104 报表系统，涉及数百种不同表格和上千个具体表格，这些表格之间存在复杂的关联关系，识别和自动化处理这些表格是一个高度复杂和业务化的任务。以前没有大模型时，将这些复杂的业务逻辑转换为新的表格需要编写大量的 Java 代码。有了大模型之后，也可以简化 90% 以上的工作。</p><p></p><p>此外，还有各种投研底层工具的构建，如研报检索系统、摘要系统，以及底层资产的信息披露材料的自动化生成。</p><p></p><p>总的来说，大模型的写作应用可以归纳为三种类型的写作系统：从底稿到新底稿的生成、从数据到新底稿的生成，以及从思路到新底稿的生成。这些场景在投研、券商合规、发行等银行评论报告生成中都有广泛应用。金融领域中现在已经看到的智能体可能有上百种不同的细分场景，可以使用统一的平台来处理。</p><p></p><p>打造金融智能的关键在于能否以比传统方法更低的成本实现，这最终关系到能否盈利。 商业模式能否通过提高效率来实现盈利，比如相比过去提高 10 倍的效率或相比同行提高两倍的效率。因此，我们的核心任务是降低成本，提高每个细分环节的效率，使用技术提升每个场景的效率。这不仅仅是算法和算力的问题，还包括用户界面的构造，用户界面可以极大地提高提示工程师的效率，这些最终都可以转化为经济效益。通过快速迭代和精益分解，以实现这一目标。</p><p></p><h5>InfoQ：金融行业文档的内容比较固定，文因也在这个领域做了很久，切换到大模型的时候还要不要等待所谓“智慧涌现”？</h5><p></p><p></p><p>鲍捷博士： 这是一个工程问题，而非科学问题。现在，即使是刚刚毕业的年轻人，通过使用简单的提示工程，也能够使系统达到 60% 到 70% 的效果，如果运气好，甚至可以达到 80%。因此，这件事情的门槛已经大大降低。包括以前那些特别复杂的表格处理，现在通过多模态能力，即使不理解 PDF 底层引擎的解析方式，也能够快速实现 70% 到 80% 的效果。</p><p></p><p>真正的挑战在于如何将系统的效果提高到最后的 10% 到 20%，尤其是在金融领域交付专业级文档体系时。例如，我们与交易所合作时，他们要求的是"四个九"即 99.99% 的准确率，这不是任何普通系统都能做到的。为了达到这样的高标准，可能需要在底层进行一些微调，微调之上还需要大量的预处理和后处理工作，尤其是预处理，这涉及对各种不同文档结构的细粒度理解。这里面没有所谓的智能涌现，而是需要进行大量枯燥的工程工作，包括数据的处理和清洗。</p><p></p><p>我们具体来看两个问题：首先是提示词的编写。假设针对一份招股书，招股书中可能有 94 个章节，涉及 2400 多个小类的数据点，那么你是使用一种提示词，还是使用 2400 种提示词？2400 种提示词的管理是一个挑战，可能需要对不同类型的数据进行相应的归类。如何进行归类，这就涉及到业务属性的问题。其次，提取出的数据如果不加以控制，可能有 30% 以上是编造的。在金融领域，这是绝对不允许的。那么，如何发现数据是编造的呢？这就需要进行幻觉检测和控制，需要数据溯源的技巧。所有这些工作都是在大模型之外进行的。</p><p></p><p>所以对于工程师而言，需要把大量的业务知识和专业技能注入到系统中，从而将一个只能达到 60% 到 70% 效果的系统提升到 99.99% 的准确率。这是一个需要深入理解和精心设计的工程挑战。</p><p></p><h5>InfoQ：通用大模型在面对大多数 TOB 场景问题时有哪些局限性？落地金融专家智能的挑战又是什么？</h5><p></p><p></p><p>鲍捷博士： 所谓的通用大模型底座，并不意味着它本身具有通用性，而是它具有成为通用基础的潜力。从这个意义上说，通用大模型底座在任何特定领域的初始表现都不够完美。它的优势在于，通过技术手段的叠加，可以使其适配并服务于不同的业务系统。</p><p></p><p>当前行业面临的一个关键问题是如何降低适配的成本。业界对此有不同的看法：一些人认为仅通过提示工程即可，一些人认为需要进行微调，还有一些人认为下一代模型出现后微调将不再有效。这些观点不一定谁对谁错，因为实际情况取决于具体的应用场景。例如，在进行微调时，真正的成本并不仅仅是算力，而是微调所需的数据。获取这些数据才是真正的挑战。你需要设计微调的数据集，考虑数据量的大小，是 100 万条还是 10 万条，以及这些数据是否具有代表性。微调之后，需要评估准确度是否提升，是否减少了幻觉（错误的推断），以及是否需要相应的测试集来验证微调的效果，而测试集的构建本身也是一个挑战。这些因素才是微调过程中真正的成本所在。随着模型版本的迭代，底座模型可能会不断更换，但微调的数据可能成为你最宝贵的资产。因此，关键在于如何平衡通用性、模型的演进性以及成本，这是一个非常复杂的工程过程。</p><p></p><p>多年前，我曾在 InfoQ 上提出，人工智能的核心在于工程，我反对仅从算法角度来看待人工智能系统。有些投资人可能会认为人工智能应用应该是算法的创新，而不是工程上的创新。他们可能不太看重在一线真正从事工程工作的人员，但我认为，所有这些美妙的成果都是工程带来的，而不是抽象的科学。</p><p></p><h5>InfoQ：为什么您会提出“精益迭代”这样的理念？企业又具体如何实现“精益地迭代”？</h5><p></p><p></p><p>鲍捷博士： 比如创建一个基于大模型的写作系统，人们可能会首先考虑拥有一个尽可能强大的基座模型。例如，如果可以使用 72B 参数的模型，就不会选择 14B 参数的模型。但这种做法往往并不实际。你需要考虑实际的硬件需求，如显卡的数量和性能。显卡不仅运行时噪音大，而且发热量大，这就要求客户拥有适当的机房设施。并非所有客户都有这样的条件。如果客户仅使用 4090 显卡就能解决问题，他们何必要非使用 A100 显卡呢？在设计系统时，必须为客户考虑这些实际问题，包括显存大小、是否采用 4 比特量化版本或 F16 浮点版本，以及这些选择对效果的影响。例如，效率可能降低 3 个百分点，客户是否接受？此外，还需要考虑客户使用场景的环境温度，以及他们是否有空调设备等工程问题。</p><p></p><p>当客户需要一个写作模板时，他们会询问是否需要自己配置模板。如果需要 40 个模板，客户可能会觉得太繁琐，因为他们可能没有人员能够配置这些模板。这时，你需要考虑如何以低成本自动化生成模板，以及如何填充模板所需的数据。数据的来源和准确性如何？如果客户要求 99.99% 的准确度，但系统只能达到 96%，这就需要额外的数据校验和核查，这将带来成本。在整个过程中，你都需要死抠每一个细节，以降低成本。例如，如果数据校验需要投入成本，你需要告诉客户可能出错的数据在哪里，以便他们进行核查。如果数据不满足要求，客户可能需要一个置信度系统，以大幅降低人工校验成本。</p><p></p><p>杨青： 我们需要认识到大模型是一个明确的未来趋势，而要真正跟上这一时代的步伐，首要的问题是开始实际使用大模型。如果人们不去使用，仅仅停留在了解概念或听新闻的层面，他们可能永远不会真正理解大模型的潜力和价值。</p><p></p><p>我的建议是，想要在大模型时代取得进展，首先要做的是开始使用大模型。其次，关于成本问题，我认为这并不是眼前的核心问题。关键在于大模型能否解决实际问题，以及这些问题的 ROI 是什么。目前来看，对于很多探索大模型的企业来说，现有开源模型的能力往往无法满足核心业务需求，只能做一些辅助的事情，因此会觉得它们的 ROI 会比较高。</p><p></p><p>但当你拥有合适的能力时，成本就不再是问题，没有能力时，成本永远会是问题，因为无法解决很多实际的问题以及带来实际的收益。所以，核心是如何提升大模型的能力，以便为你带来更多价值。我的看法是，随着 AI 能力的不断提升，我们不断走向通用人工智能的道路，能实际解决的问题将会越来越多，能够突破的成本边界也会越来越高。</p><p></p><h5>InfoQ：度小满金融数据智能部计算机视觉方向负责人万阳春老师也将在大会上介绍 《计算机视觉技术在金融数字化风控中应用》，杨青老师可以围绕这个议题浅浅给大家做个剧透吗？</h5><p></p><p></p><p>杨青： 我<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6030">简单介绍一下我们即将讨论的内容</a>"。我们主要关注两个方面：首先是大模型如何在现有的视觉技术中带来新的增益。在传统的人工智能应用中，OCR（光学字符识别）技术非常常见，尤其是在企业数字化转型过程中，多媒体信息的数字化尤为重要。金融行业中，票据、流水等图文和文本信息的数字化处理的流程较复杂，且每遇到新的数据类型都需要单独开发，而大模型的应用可以实现端到端的解决方案，大幅提升效率和效果。</p><p></p><p>第二方面是随着大模型技术的发展，出现了文生图、文生视频等技术，这些技术带来了深度伪造信息的问题。在金融行业，伪造信息技术可能被用来绕过生物认证流程，比如在借贷、理财等场景中，这是一个较大的安全隐患。我们将分享我们在识别深度伪造信息方面的经验和一些实际落地的做法。</p><p></p><h3>机器与机器、人与人、人与机器的未来协作模式</h3><p></p><p></p><h5>InfoQ：我们观察到现阶段越来越多的企业实际上在将传统人工智能技术与大模型相结合，以满足业务场景的需求。那么，传统 AI 技术与大模型各自的优势是什么，以及两者之间如何有效地将它们的优势结合起来，以更好地服务于金融业务场景？</h5><p></p><p></p><p>杨青： 传统人工智能（AI），之前提到的更多是有监督机器学习，其优势在于帮助企业提升决策效果。例如，大数据风控，这些年得到了快速发展，应用了许多前沿的 AI 技术来增强风控能力，这可以视为传统 AI 的一个应用实例。前面提到的 OCR 技术也是一个传统 AI 的例子，根据不同的场景需要开发相应的模型。</p><p></p><p>而生成式 AI，现在通常指的是大模型，其优势主要在于其理解、记忆和逻辑方面的能力。对企业来说，这意味着不需要定制众多模型，而是希望利用通用的人工智能的能力覆盖多种场景。两种技术的基本区别是：一种是根据具体业务目标单独开发模型，另一种是实现通用 AGI 来解决各种问题，甚至可以把他当成一个人来使用，当然，后者还在路上。</p><p></p><p>从大模型技术不断发展的过程中可以看出，一方面，大模型的出现开拓了一些新的应用场景，这些场景以前更多依赖人工操作，现在可以通过技术提升效率甚至不断的替代人工，相当于一种新质生产力。另一方面，大模型也在逐步替代一些传统场景，比如 OCR 技术，通过多模态可以有效地解决 OCR 方面的问题。</p><p></p><p>目前来看，传统 AI 和生成式 AI 正处于一个相互结合的阶段，各自发挥优势解决不同的问题。随着大模型和通用 AGI 的发展，未来可能会趋于融合，大模型能解决的问题会越来越多，最终大模型可以是像一个人一样，传统 AI 更多像一个特定的工具，大模型可以使用传统 AI 这种工具弥补自身能力的缺陷，以能够更好的完成更多任务。但这确实需要一些核心底层能力的突破，可能还需要一定的时间周期。</p><p></p><p>鲍捷博士： 技术的发展是一个继承而非取代的过程。因此，我们不能简单地说小模型和大模型哪个更优或更劣。例如，正则表达式已经存在了 50 年，并且预计在未来 50 年仍将继续存在。而大模型只有大约 5 年的历史，但我们无法保证它在未来 50 年后仍然流行。</p><p></p><p>在很多场景中，如果传统机器学习已经能够很好地解决问题，那么就没有必要使用大模型。认为所有问题都应该用大模型来解决是一个错误的想法。 我们应该从这个思路出发，更清晰地理解技术的应用的演化脉络：从小数据起步，逐步发展到大数据系统，再到大知识系统。</p><p></p><p>大模型系统本质上是大规模的知识库。 它标志着我们从数据时代进入了知识时代。今天所谓的大模型，可能在未来五到十年后看来并不算大。核心问题在于我们如何从数据时代过渡到知识时代。数据本质上是表格化的、二维的结构，而知识则包括了更多复杂结构，如树形、网状结构和复杂的语义结构，我们称之为本体。传统知识库系统的构建非常复杂，因为它依赖于逻辑系统。而大模型系统则将这些高成本的逻辑系统转变为基于数据驱动的系统，能够进行相对简单的知识建模。</p><p></p><p>当前的大模型系统存在严重的逻辑缺陷，例如在进行基本的四则运算时可能会出现问题。这是因为它们是基于预测下一个 token 的系统，依赖于概率，如果不引入特定的机制如 LangChain 或 Agent，它们无法处理递归结构生成的问题。而大多数科学语言和数学语言都是递归结构。如果未来五到十年内出现一种新的系统，能够让大模型处理递归结构，那么我们可能会发现今天的大模型系统并不如我们想象的那么强大。它处理的数据其实非常简单。技术的本质不在于规模大小，而在于处理的语义结构是否足够丰富。 从这个角度来看，不同的应用和不同的侧面需要不同的模型来配合，我们不必纠结于是否使用大模型。</p><p></p><h5>InfoQ：进一步来看，在大模型时代，除了机器与机器的协作，人与人、人与机器的协作模式也将面临巨大的变化，作为金融机构和技术从业者，为了应对这种变化现在如何做好准备？</h5><p></p><p></p><p>鲍捷博士： 我们的一些预想与实际情况并不完全相符。最初，我们设想人类从事创造性工作，而机器处理管理性任务，但现在情况似乎颠倒了。我特别关注的一个未来趋势是可穿戴设备的发展。虽然现在我们看到了机器人和其他智能设备，但人机交互界面的自然化可能是大模型技术带来的最深远的影响。</p><p></p><p>设想未来，像项链、耳环或耳机这样的日常配饰可能就是大模型的终端设备。当你走进家中，墙壁本身可能就是一个大模型终端，集成了传感器和新型显示设备，甚至成为建筑材料的一部分。未来的显示设备可能与大模型直接相连，当你进入车辆时，车联网也可能直接接入大模型。我们对智能设备的接触已经从大型机器演变到小型机、个人电脑、手机，再到现在的可穿戴设备和智能手表。未来，这些设备可能会更无缝地集成到我们的生活中，生活中的一切可能都将自动化，我们的生命将被记录和存储。</p><p></p><p>这种全面的智能化可能带来科幻般的变化，例如《全面记忆》一书中描述的全方位记忆能力，这可能会记录我们一生的每个细节。这将对我们人类的运行机制和社会组织产生深远影响，无论是积极的还是消极的。</p><p></p><p>从 ToB 的角度来看，大模型技术将极大地转变我们的生产力。过去三四十年里，尽管计算机设备变得更快，但办公室生产力并没有发生根本性的变化。大模型技术可能真正实现办公室生产的自动化，即大模型驱动的办公自动化。我相信这种自动化将带来经济上的巨变，称之为“第四产业”——不再是物质的生产与分发，而是信息的生产与分发。目前，第四产业在经济总量中占比约为 30%，但在未来几十年内，可能会增长到 80%。这是人工智能可能带给我们的未来。</p><p></p><p>杨青： 目前，虚拟人和虚拟主播已经给人们带来了深刻的感触，这让我们对未来具身智能机器人的发展充满期待。未来的发展形态是比较清晰的：从大模型技术结合具身智能，逐步走向规模化的通用人工智能（AGI），最终可能实现类似人类的机器人。</p><p></p><p>想象一下，未来有许多机器人成为你的伙伴、助手或同事，这时你需要考虑如何与它们共生和协作。我认为这是未来的一种终极状态。在这个过程中，对每个人来说，如我之前提到的，需要去实际使用和了解大模型技术。很多人可能对大模型很好奇，但他们可能并没有真正使用过它。</p><p></p><p>所以我的建议仍然是，要去多使用大模型，把它应用在你的日常生活和工作中，并逐步理解和 掌握它。这样，最终你才能与机器人更有效地沟通和协作。</p><p></p><h4>活动推荐</h4><p></p><p>8 月 16-17 日，FCon 全球金融科技大会将在上海举办。本届大会由中国信通院铸基计划作为官方合作机构，来自工银科技、北京银行、平安银行、广发银行、中信银行、度小满、蚂蚁集团等金融机构及金融科技公司的资深专家将现身说法分享其在金融科技应用实践中的经验与深入洞察。大会火热报名中，7 月 31 日前可以享受 9 折优惠，单张门票节省 480 元（原价 4800 元），详情可联系票务经理 17310043226 咨询。</p><p></p><p><img src="https://static001.geekbang.org/infoq/31/31ff5488cc076e04976f66fd5d9869c7.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/7cZHespRRGbfQwOGyZSc</id>
            <title>打破系统孤岛，建立端到端流程，飞书项目再迎新升级</title>
            <link>https://www.infoq.cn/article/7cZHespRRGbfQwOGyZSc</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/7cZHespRRGbfQwOGyZSc</guid>
            <pubDate></pubDate>
            <updated>Wed, 17 Jul 2024 08:52:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 飞书项目, IPD产品解决方案, 流程驱动新增长, 企业价值提升
<br>
<br>
总结: 飞书项目在上海举办了新产品发布会，发布了针对制造企业的IPD产品解决方案，旨在提升企业价值，通过流程驱动实现新增长。 </div>
                        <hr>
                    
                    <p>7月17日，飞书项目在上海举办了主题为“流程驱动新增长”的新产品发布会，正式发布针对制造企业的“飞书项目IPD产品解决方案”，并分享了对流程引擎、视图、基础平台和产品体验等多项平台能力进行的升级。</p><p>&nbsp;</p><p>“飞书从最初作为协同办公软件，为企业提供新一代的工作方式；到后来通过飞书People打通了企业的人和事，成为为企业提供先进理念的组织管理工具；到今天，我们希望通过飞书项目将飞书的价值进一步提升，助力企业的业务流程，通过专业的项目管理工具为客户创造更多价值。”飞书CEO谢欣在会上提到。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b2/b23324956b9d75f656fc6000f73dcc76.png" /></p><p></p><p>图注：飞书CEO谢欣在活动现场分享</p><p></p><p>据悉，飞书项目起源于字节跳动的研发实践，专为现代企业的复杂项目管理需求而设计。自2021年正式对外商业化以来，其“可视化、标准化、高可配置”的工作流引擎等产品亮点，吸引了包含小米、安克创新、蔚来汽车、理想汽车、小鹏汽车、ubras、公牛、联影医疗等超过1000家不同行业的头部企业。</p><p></p><h2>沉淀两年，飞书项目IPD产品解决方案正式发布</h2><p></p><p>经过两年的持续研发和大量投入，飞书产品副总裁洪涛在现场正式发布了飞书项目IPD产品解决方案。据悉，该解决方案可以通过建立系统的研发管理体系，实现以规则的确定来应对结果的不确定。同时，面对复杂项目，可以打破系统孤岛，建立端到端流程，帮助用户掌控全局进展。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f3/f354ef6b7e1148c20c4a373618949bc3.png" /></p><p></p><p>图注：飞书产品副总裁洪涛现场发布飞书项目IPD产品解决方案</p><p></p><p>洪涛在现场详细分享了包括计划管理、评审管理及流程优化在内的三大IPD解决方案功能亮点。“计划管理中全景泳道图就像给业务拍了一张流程CT图，项目进展、关键里程碑的状态在泳道图中一目了然，让关键风险直接暴露，加速问题解决”。评审集成化则能够通过组织级评审要素库的建设等方式，让IPD评审持续沉淀，成为组织固化的高价值资产。针对过去研发管理传统工具固化，难以快速进行流程迭代的问题，飞书项目在IPD解决方案中加入了度量、资源库等方式，让持续洞察问题、灵活优化流程成为可能。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/9e/9e81157e0e696e69ce5d21d475e2feb7.png" /></p><p></p><p>图注：全景泳道图产品示意</p><p></p><p>目前，飞书项目IPD产品解决方案已经在锐捷网络、新大陆、玛丽黛佳、SKG等多家企业实现了共创落地。作为最早与飞书项目进行IPD行业专版共创的客户，锐捷网络如今已基于飞书项目IPD专版构建起了完整的一体化平台。“IT技术已从IT部门的专有能力转变为组织的通用能力，未来组织中的每个人都能随时使用数字化工具来解决自身问题，这是一场效率的变革。”锐捷网络信息资源部总经理张洪丹在现场进行了基于飞书项目IPD的实践分享。</p><p></p><h2>打造更灵活、更开放的工作流引擎</h2><p></p><p></p><p>“今年一年内的时间里，飞书项目就上线了638个功能点或产品优化，平均每天1.75个，每一天，飞书项目都变的更好。”IPD行业专版解决方案外，洪涛还在现场分享了飞书项目在流程引擎、视图、基础平台和产品体验等平台功能上的全面升级。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f2/f27cdd85f90cd32e779e769ff741fc1b.png" /></p><p></p><p>&nbsp;图注：飞书项目工作流程引擎产品示意</p><p></p><p>工作流引擎一直是飞书项目最被客户点赞的能力。在本次升级中，新版本提升了飞书项目工作流引擎的开放性与灵活性，让更多业务场景可以被纳入流程里，让流程看得见、易执行、可进化。另一方面，通过树形、全景等全新视图的增加，让复杂战略拆解落地的过程变得清晰可控。</p><p>&nbsp;</p><p>“项目推动离不开组织各角色在平台中的高效协同，飞书项目基础平台同样带来了一系列非常实用的功能更新。”洪涛介绍称，面对越来越强的客户出海场景需求，飞书项目上线了针对跨国协同团队的“协作时区”、“自定义多语言”能力；面对上下游跨组织场景，企业互联能力的加持让不同组织的人实现一个协同空间的完美配合；而面对信息安全问题，飞书项目通过更精细的权限管理设置，更好保障了关键项目信息的安全。</p><p>&nbsp;</p><p>除了复杂项目管理能力的迭代，在用户体验上飞书项目也作出了提升，比如最新升级的综合搜索能力让信息准确率提升了40%，帮助员工更快找到想要的项目信息；在项目新建中新增的简洁模式让建单效率提升了35%；而通过持续优化产品性能，新版的飞书项目关键页面访问速度则提升了140%。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5a/5a66c713a9b774d0bf5a5e2fbb734958.png" /></p><p>&nbsp;</p><p>图注：飞书项目最新升级的综合搜索能力界面</p><p>&nbsp;</p><p>在现有的平台能力升级外，谢欣还在现场分享了飞书基于AI的项目管理创新实践。据介绍，飞书项目管理团队利用飞书智能伙伴创建平台搭建了一个AI PMO，能够帮助PMO们通过问答快速了解产品能力问题，通过整合推送资源投入进展把握产研资源投入情况，还可以在日常工作群聊中主动识别To&nbsp;Do，协助研发在飞书项目中创建工单等。“数字化是智能化的基础。未来，在大模型能力的支持下，通过飞书项目积累的流程数据，将为大模型在项目管理场景的落地提供有效的数据基础。”谢欣表示。</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/P2oGNMSXP9VOLEphy6b6</id>
            <title>大模型在融合通信产品中的应用实践</title>
            <link>https://www.infoq.cn/article/P2oGNMSXP9VOLEphy6b6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/P2oGNMSXP9VOLEphy6b6</guid>
            <pubDate></pubDate>
            <updated>Wed, 17 Jul 2024 07:39:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: IM, RTC 技术, LLM 技术, PaaS 平台
<br>
<br>
总结: IM 和 RTC 技术作为融合通信的关键技术，在社交、客服、协同办公等场景中得到充分利用；在大模型快速发展和日渐成熟的背景下促使行业也发生了很大的变化，特别是 LLM 技术的第一个现象级应用 ChatGPT 就是以会话作为唯一交互形态；如何结合 LLM 和通信 PaaS 平台加速场景化落地成为一个关键议题。在网易云信中，我们主要做通信，它是一个通道。但这个通道的应用场景通常在社交领域，这是一个相对高风险的场景。在这个过程中，我们不仅要做好内容安全管控，还要将通道过程中产生的数据价值与 AI 结合，充分发挥其潜力。 </div>
                        <hr>
                    
                    <p>IM 和 RTC 技术作为融合通信的关键技术，在社交、客服、协同办公等诸多场景中得到充分利用；在大模型快速发展和日渐成熟的背景下促使行业也发生了很大的变化，特别是 LLM 技术的第一个现象级应用 ChatGPT 就是以会话作为唯一交互形态；如何结合 LLM 和通信 PaaS 平台加速场景化落地成为一个关键议题。</p><p></p><p>网易也在做大模型技术应用探索，也在内部的多个业务线有落地实践。在 ArchSummit 架构师峰会上，来自网易云信首席架构师，产品部负责人周梁伟就介绍了大模型在产品上的一些应用。以下是演讲整理。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/59/59634846a2702c6ed20d2f2e80b8da35.png" /></p><p></p><p>我们的业务叫网易数智，早在 2015 年左右就开始涉足 ToB 市场。业内的一些云公司采用的是高举高打的策略，从底层架构开始搭建。而网易则结合自身业务进行内部基础设施建设。在服务自身业务的过程中，我们逐步完善这些基础设施，将其场景化，并最终包装成对外提供的 ToB 服务。</p><p></p><p>今年年初，网易数智进行了业务变革，结合大模型和 AIGC 等业界前沿技术能力，整合了更多业务，增加了智慧和 AI 的元素。那么，目前网易数智包含哪些业务呢？首先是网易易盾，主要负责内容安全。网易有大量内容型产品，如网易新闻和网易云音乐，这些产品中存在大量的内容审核需求。在网易易盾中，我们将传统的机器审核与大模型结合，推出了 AI 内容检测、智能风控、实名认证以及安全架构等相关能力，对外提供网易易盾安全产品。</p><p></p><p>第二块业务是我负责的网易云信。自 2015 年起，网易云信开始对外商业化。其起源可以追溯到网易泡泡，这是一款即时通信软件。在微信时代，我们与电信合作推出了网易翼信，达到了亿级用户规模。在 C 端市场逐步积累了大量融合通信经验后，我们发现即时通信（IM）和音视频通信在各种场景中都是非常刚需的能力，但要建设好这些能力难度很大。因此，我们将其包装为网易云信，对外提供即时通信、实时音视频、直播、点播以及与运营商结合的短信等业务。</p><p></p><p>今年，我们的业务整合后，还融合了部分轻舟微服务和轻舟中间件，这些是面向开发者的软件服务。网易云商则结合了网易严选和考拉等电商平台，推出了客服平台。数帆和 CodeWave 也在其中。随着大模型的发展趋势加深，各产品都整合了相关功能。举个例子，AI 内容检测从过去的基于机器规则匹配，转变为结合 AI 模型识别大量 UGC 内容，带来了更多挑战。在自动问答生成过程中，如何做好内容风控也是一个关键点。我们结合 AI 的合规要求，推出了一些新产品。</p><p></p><p>在网易云信中，我们主要做通信，它是一个通道。但这个通道的应用场景通常在社交领域，这是一个相对高风险的场景。在这个过程中，我们不仅要做好内容安全管控，还要将通道过程中产生的数据价值与 AI 结合，充分发挥其潜力。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/5f/5f270991fed6e40ff5d3a39556174054.png" /></p><p></p><p>这是网易云信的 PaaS 架构。我们从网络层开始构建了一个全球实时通信网络。这个网络支撑了目前三大主要业务：RTC 音视频通话的实时网络、IM 即时通信和直播流媒体的分发网络。这个基础网络是我们 PaaS 平台的底层服务，尽管不直接提供给客户，但它是我们 PaaS 产品的核心基建。</p><p></p><p>在 PaaS 产品层，我们提供的服务包括 IM 即时通信、信令、直播、点播、短信和音视频等。这些能力是开发者可以直接使用的。除了我们自身提供的能力之外，我们还与第三方厂商合作，特别是在安全方面，我们有网易易盾这样的能力，也有第三方厂商的生态支持。</p><p></p><p>在大模型应用场景上，由于 PaaS 平台的广泛应用，我们的客户可能会使用我们的能力来实现娱乐社交、教育、协同办公等场景。由于这些场景多样，很难通过单一模型覆盖所有需求，因此我们更多地起到桥梁和生态连接器的作用，与各类公有云和私有模型部署、训练实施等厂商合作。</p><p></p><p>此外，我们还与视频处理厂商合作，提供如美颜、变声等能力，以及与社交行业中的游戏厂商合作。这些都是我们的生态合作伙伴。今年，我们业务整合后提出了数字化方向，主要面向企业场景，包括内部办公和内外营销沟通。</p><p></p><p>许多企业提出不能使用公有云，包括大模型，因此我们自 2017 年起就提供私有化部署服务，解决客户在数据安全方面的顾虑。我们还结合实施和集成类生态合作伙伴，共同提供这些服务。</p><p></p><p>我们的 PaaS 平台支持客户使用多种融合通信场景，包括单聊、群聊、弹幕聊天室、音视频等。这些场景在娱乐社交、教育、医疗和金融等领域有广泛应用。例如，在医疗场景中，本地医院医生可以与远程专家在线阅片，产生大量音视频互动和 IM 内容互动。</p><p></p><p>在金融场景中，我们支持虚拟营业厅和柜员，基于这些底层能力构建上层业务场景。协同办公是另一个重要场景，虽然市面上有钉钉和飞书等 all-in-one 产品，但很多客户出于合规或个性化需求，不能使用 SaaS 产品。因此，他们可以基于我们的融合通信能力构建自己的 OA 办公平台、企业培训平台、视频会议平台和客服系统。</p><p></p><p>通过 8 年的持续产品迭代，我们已经积累了大约百万开发者，发送了超过 2 万亿条消息。这显示了我们在数据体量上的巨大优势。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/d9/d960ac133d2314d36063eca577b714d8.png" /></p><p></p><p>回到大模型结合的问题上，我们的通信系统本身是一个连接器和消息分发网络。在结合大模型的过程中，我们发现客户有几个关键痛点。</p><p></p><p>首先，如何将 AI 能力融入用户交互层面。例如，在群聊或客服互动中，如何将 AI 能力整合到点对点或群组沟通中。</p><p></p><p>其次，在群聊或客服互动过程中，已经产生了大量上下文数据。这些数据在与客户沟通时，可以被提取和利用，进行基于上下文的持续问答。这些数据对于模型训练非常重要。</p><p></p><p>第三，也是最关键的，许多客户内部已有 AI 团队在训练大模型，或使用供应商提供的大模型实时和训练服务。如何与这些 AI 能力结合，是一个重要问题。因此，我们更多地作为一方、二方和三方之间的大模型生态连接器，促进这些资源的整合。</p><p></p><p>最后，许多客户更倾向于本地化部署。这主要出于两个原因：安全性和业务迭代的持续性。本地化部署的大模型成为企业的大脑，包含所有数据信息，能够支持更多业务和不同场景的数据整合和利用。这是客户最关心的问题。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7e/7e3a03722d7221df72d3261910baf370.png" /></p><p></p><p>从业务场景的角度出发，我们的方案主要涉及以下几个层面。首先，从最基础的层面看，AI 技术已经广泛应用于各种业务场景。例如，通过 AI 进行关键词搜索、提取相关信息、语言翻译，甚至是自动语音识别（ASR），这些都与传统方法有很大不同。此外，AI 还可以应用于论文索引、代码生成和优化，为程序员提升效率。</p><p></p><p>今天我们重点讨论两个与 IM 和大模型结合的场景。第一个场景是客服，包括售前咨询和营销类客户服务。第二个场景是娱乐社交，包括虚拟人物和 AI 机器人社交语聊。在这些场景之上，AI 技术可以广泛应用于不同行业。例如，在电商领域，客户通常在购买商品后需要与客服沟通处理售后问题。虽然这个场景的交互时间较短，但在其他场景中，客服的交互时间可能更长。</p><p></p><p>以我们自身为例，网易云信面向开发者的产品在售前咨询过程中往往持续半个月到一个月。开发者在集成 SDK 或 API 时，会产生大量的问答，并且这些问答可能有前后关联。长流程客服场景中，对历史信息的提炼和关联尤为重要。</p><p></p><p>类似地，在线教育和办公协同中也存在客服场景。比如在办公过程中，HR 和 IT 服务涉及的问答频率很高，例如社保策略或薪资问题。在这些情况下，通过 AI 客服来提供持续服务显得尤为重要。通过将 AI 客服与 OA 办公平台结合，可以有效解决这些问题。此外，AI 技术在游戏和电商等领域也有广泛应用。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/5e/5ecc50e7f1a56a55645f031f74809a5f.png" /></p><p></p><p>首先，谈到客服场景，最关键的问题是什么？目前市场上有大量的第三方客服服务平台，例如我们网易提供的客服平台“网易七鱼”。早期的客服主要依赖人工，人工客服需要持续回答问题并进行管理。然而，随着业务发展，客服人员的规模和培训成本不断增加，导致运营成本大幅上升。</p><p></p><p>为降低成本，许多企业转向使用机器人客服来替代人工客服。目前，客服流程通常包括机器人客服、人工客服和工单处理三个阶段。这种模式虽然降低了运营成本，但也带来了新的问题，例如知识库的维护成本。客服需要回答的问题和产品知识不断迭代，传统的解决方案是构建一个知识库，需专门的机器人训练师从非结构化数据中提取产品相关知识点和标准问答，进行语义训练和回答。</p><p></p><p>然而，这种方法存在不足，尤其是面对业务的不断变化和知识的持续迭代。大模型通过自然语言理解、关键信息提取及持续训练能力，可以更好地解决这一问题。其次是用户意图的把握。用户具有不同的背景和表达习惯，同一问题的表述可能千差万别。过去主要依赖自然语言理解（NLP）技术，但在用户意图的准确把握上，大模型具备更强的处理长文本和历史信息的能力，带来了显著变化。</p><p></p><p>第三个问题是大量历史信息的查阅。在电商场景中，客服问题通常与当前订单相关，只需通过业务系统集成调取订单信息即可解决。然而，在技术平台类客服或其他复杂场景中，客服需要基于大量历史数据。这些历史数据通常存储在 IM 平台或 RTC 平台中，通过大模型可以更有效地进行处理和利用。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/79/798d50cf4eb880d0822dfd8abd9d286a.png" /></p><p></p><p>通过结合 AI 技术，客服效率得到了显著提升，主要体现在以下几个方面：</p><p></p><p>首先，AI 可以更精准地理解用户意图，从而提高机器人客服的问答质量。尽管仍需要人工客服，但 AI 的应用可以减少人工客服的工作量。在人工客服的过程中，AI 可以提供两方面的帮助：一是为客服人员提供用户的上下文信息和建议答案，客服人员只需进行人工校准即可；二是收集和利用人工客服提供的精准散点化回答，这些回答具有重要价值，可以被引入知识库进行二次训练，从而优化后续的自动化回答。</p><p></p><p>通过 AI 的群体客服能力，以上问题得到了有效解决，有助于构建更高效的自动化知识库。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/69/69021879d5ae295b6a5cdc9bc82ef3be.png" /></p><p></p><p>构建知识库的过程中，有两个重要的数据源。传统模式，企业知识库通常来自业务系统或企业内部文档，如 PDF、Word 文件，或 HR 发布的规章制度网页等。这些分散的信息需要被结构化处理，过去的方法是手动理解和提取知识点，再整合入知识库。结合大模型后，只需将文档输入系统，由大模型自动理解和消化。</p><p></p><p>第二个重要的数据源是用户沟通过程中的历史记录，这在常会话或多人客服场景中特别重要。例如，新能源汽车的客服场景显著不同于传统汽车。通过直销模式，新能源汽车企业提供更优质的服务，从客户进店到下订单、提车、使用过程中出现的事故、保险理赔和售后维护，全过程信息都通过客服群汇总。在这种场景中，如何将用户过去一月甚至一年内的反馈结合起来，解决当前的问题，成为一个重要课题。通过将 IM 中的大量历史数据与大模型结合，再加上标准的企业知识库，可以大幅提升客服的回答质量。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/57/57b19d5f5264d113224e122e73a782ee.png" /></p><p></p><p>这是一个精准识别提问的示例，就是在数字人客服过程中，过去可能主要就是通过关键词提取，那现在可能基于关键词，在企业知识库的矢量向量库里去做一些关键内容的提取，之后再根据场景化定义的那些 promote 列提示词，然后提供更好的回答。它具备根据上下文不断地追溯的能力做生成式回答。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/31/3128fa8de8f79bafe705c61830612894.png" /></p><p></p><p>这是人工服务持续提效的事宜，客服在回答用户问题的过程中，可能产生了很多散点的问题，或者会给出一些建议性答案，通过 IM 里面的上层 UI 工具和底层数据能力之间打通，可以快速地给到一些提示。同时客服人员可以对当前会话的一些相关信息打标签，之后这个数据可以回溯再去做二次训练。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/4d/4d5e1ede2447d65291f04f99e1397768.png" /></p><p></p><p>另一个场景叫群客服，在群客服场景里要解答问题，如何快速提取上下文？它可以通过 @ AI 机器人，让它帮我把上下文提取出来，或者给我一个建议。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b3/b320d44e746588215f0d7654e03a95ff.png" /></p><p></p><p>第二个场景主要涉及营销。过去的营销方式常常在用户访问网站时弹出一个客服页面，询问用户问题并提供快速链接。然而，这种方式的精准度相对较低，用户往往需要多次点击才能找到准确的信息。通过结合大模型，可以显著缩短推荐链路。</p><p></p><p>大模型的优势在于能够利用用户在产品中的历史数据，分析其特征和偏好。例如，用户的年龄、地区、职业标签（如职场女性）等信息。将这些分析结果与企业知识库中的产品推荐信息相结合，可以为客户提供更精准的推荐。同时，在客服过程中，也能提供快速触达和促成交易的链接，提高营销效果。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/4b/4bd455c4045cc312016aa962e956b6d4.png" /></p><p></p><p>第二个场景主要偏向娱乐社交和游戏。当前，新产品推出的最大挑战之一是如何快速积累用户，或需要高昂成本来制作大量的用户生成内容（PGC）。在这种情况下，如何快速启动变得尤为重要。例如，小说、漫画、动漫等创作需要大量专业知识和持续的内容积累。借助大模型，可以通过不同的标签和提示生成拟人化、个性化的人物角色，应用于社交场景。近年来，市场上已经推出了许多类似的 C 端产品，这些产品可能会逐渐替代真人社交，成为虚拟人社交的趋势。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ed/ed9916afbad17e414408a8660c37652e.png" /></p><p></p><p>在社交场景中，即使是真人用户的一对一交流，面对大量问题时也可能应接不暇。在这种情况下，可以利用用户的固定知识信息和表达习惯，通过大模型生成一个类似 AI 的角色，即数字人分身，来代为回答问题。这种方式不仅可以在单聊和群聊中增强互动，还能提高交流频率和交互效率。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7e/7eea020b1ec97c5b1c44b6cf7dc928d8.png" /></p><p></p><p>基于上述场景，在融合通信或 IM 通信领域中与大模型结合的主要难点包括：</p><p></p><p>首先，训练过程非常复杂。目前市场上有许多大模型，包括公有云和私有化的大模型。要进行有效的二次训练或结合自身业务数据进行训练，面临两大顾虑：一是担心业务数据投入训练后可能带来的数据泄露风险；二是对大模型和垂直领域的大模型了解不足，难以选择最合适的模型。解决训练难题成为一个重要课题。</p><p></p><p>其次是选型问题。客服场景内有许多垂直领域需求。例如，我们是一个技术开发平台，客服场景中需要的大模型应对代码理解能力更强，因为客户的问题多与代码相关，答案也是代码。因此，需要选择一个对代码理解更好的模型。而在电商、财务或法规等领域，则需要选择对相应领域理解更好的模型。因此，未来将存在大量不同领域的垂直模型。</p><p></p><p>第三是数据整合问题。不论是构建上下文关联数据库、生成提示词，还是进行二次训练，都需要结合大量业务数据。在 IM 中，通信过程中已产生大量业务数据，这些数据对企业来说非常有价值，可以作为模型训练的重要数据来源。作为一个 PaaS 平台，我们的开放性很好，可以通过 API、SDK、Webhook 等形式实现数据互通与开放，从而持续增强模型的训练效果。</p><p></p><p>最后是业务融合问题。我们的产品以 UI Kit、SDK 或 API 形式对外提供，可以与各种业务场景进行融合。这种灵活性和开放性使得数据和业务功能能够无缝集成，提高了整体系统的智能化和自动化程度。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/54/54dd542cb731539f17fa5d65bc6609fd.png" /></p><p></p><p>这是大概的架构图。首先，在底层支持的资源和服务包括本地部署的算力平台和向量数据库存储服务，以及即时通信和内容合规的云服务。我们支持公有云和私有化部署模式。</p><p></p><p>其上，我们为客户提供了一些经过调研的大模型建议，可以是云上运行的模型，也可以是私有化部署或经过二次训练的模型，实现与我们平台的互通。</p><p></p><p>第三部分涉及模型训练过程中可能需要大量第三方标准数据，如图表、日期和气象数据等，这些数据通过插件形式引入到互动过程中。我们在 UI Kit 等层面实现数据的互通。向量数据库作为关键基础资源，实现数据上下文存储。</p><p></p><p>最顶层提供给用户的是 SDK 和 API，同时还提供了控制台等界面化操作能力。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/d6/d628f1a2fc380c66640422e21d9c6ab3.png" /></p><p></p><p>基于这种方案，我们作为一个 PaaS 平台，服务了大量的 IM 场景客户。</p><p></p><p>在这些场景中，我们积累了开发者的使用习惯和业务理解。基于这些理解，我们设计了更优的交互和 API 与开发者系统的集成设计。另一方面，这些场景产生了大量数据。虽然这些数据对于平台本身的价值不高，因为它们主要属于客户的垂直业务场景，但对客户来说，结合他们的大模型进行二次训练是非常有价值的。</p><p></p><p>因此，我们的数据可以帮助客户通过与业务系统集成，获得更好的回答。这些数据仅在用户与模型进行互动时才能发挥作用，这些过程基于通信中的数据安全和脱敏关系。另外，由于我们服务的客户场景多样，客户可以积累跨不同垂直领域的经验。例如，我们为电商公司提供的解决方案可能在低成本下被其他电商公司复用，从而企业可以以更低的成本实现类似场景的落地。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ec/ec0362629dd957aafb7d2ec07efb2187.png" /></p><p></p><p>最后一个部分涉及基础能力的互通，这是传统架构中的一个对话过程。一旦与云信打通，我们便能够通过 API 开放平台，让客户提取数据并调用 API 接口快速响应。这种打通可以显著加速整个开发过程。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c0/c0824422a255352ae25a36e7c2874e41.png" /></p><p></p><p>这解决了当前场景下两个问题：强化 AI 的场景响应效果和语料积累。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/05/05a175a7d6bf134c3381ad52247db9d2.png" /></p><p></p><p>重点在于通信过程中，用户在我们平台产生的数据如何结合向量数据库，进而适应我们业务垂直的大模型进行训练，实现更佳的响应效果。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/73/739ccd478a72e24a2491a05f34deea9e.png" /></p><p></p><p>简而言之，作为一个 PaaS 平台，我们提供 UI Kit、SDK 和 API 能力。UI Kit 旨在帮助客户和开发者在降低开发成本的同时，提供各种 AI 交互入口，例如工具栏和虚拟账号，以及在群体中的机器人交互。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/01/01961743bfcb9ad3a28f3e9dfa2b1fef.png" /></p><p></p><p>除了 UI Kit 的开放能力，我们还提供控制台和简便的集成互动能力，如数字人模板库。该模板库主要基于场景定义和基础数据训练，虽然通用但不包含企业私有数据，可以进行二次定制。</p><p></p><p>未来，我们将开放生态合作伙伴在模板库内的生态系统，支持模型能力服务实施和行业场景训练模板上传，为更多开发者提供使用和定制的机会。</p><p></p><p>客户选择模型后，可以快速连接并上传私有数据集进行二次训练和调整。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/59/59accde94a4005d6652047bb5465d7c2.png" /></p><p></p><p>最后，我们的交付策略将满足客户对数据和模型安全性的高要求。我们支持全面私有化部署或混合云部署模式，以适应不同的需求和场景。</p><p></p><p>总结来说，作为 PaaS 平台，我们通过深入理解客户需求和通信领域的核心优势，结合大模型能力，与生态合作伙伴合作，落地解决方案。</p><p></p><p>【会议推荐】</p><p></p><p>AICon 全球人工智能开发与应用大会，为资深工程师、产品经理、数据分析师等专业人群搭建深度交流平台。汇聚 AI 和大模型超全落地场景与最佳实践，期望帮助与会者在大模型时代把握先机，实现技术与业务的双重飞跃。在主题演讲环节，我们已经邀请到了「蔚来创始人 李斌」，分享基于蔚来汽车 10 年来创新创业过程中的思考和实践，聚焦 SmartEV 和 AI 结合的关键问题和解决之道。大会火热报名中，7 月 31 日前可以享受 9 折优惠，单张门票节省 480 元（原价 4800 元），详情可联系票务经理 13269078023 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/29/2992999a64ad4ad07df3c7cc4977bf6f.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/u64NGww7q7zTeeJEDXaZ</id>
            <title>快手AIGC原创神话短剧上线，首次曝光可灵技术如何降低制作成本</title>
            <link>https://www.infoq.cn/article/u64NGww7q7zTeeJEDXaZ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/u64NGww7q7zTeeJEDXaZ</guid>
            <pubDate></pubDate>
            <updated>Wed, 17 Jul 2024 01:48:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 快手, AIGC, 可灵, AI影视
<br>
<br>
总结: 快手首部AIGC"原创奇幻微短剧《山海奇镜之劈波斩浪》上线，以AI制作的奇幻微短剧为观众构建了一个光怪陆离的上古神话世界，制作周期短、成本低，AI技术提高了奇幻短剧创作效率，快手推出了“星芒短剧×可灵大模型”创作者孵化计划，未来将不断升级技术以满足用户和创作者需求。 </div>
                        <hr>
                    
                    <p>近日，快手首部<a href="https://www.infoq.cn/minibook/HfXFv4RaAEyPUFk5HAfJ">AIGC</a>"原创奇幻微短剧《山海奇镜之劈波斩浪》上线。在线下看片会现场，该剧主创团队首次揭秘了快手自研视频生成大模型可灵对该剧进行的深度技术支持。</p><p>&nbsp;</p><p>作为一部画面完全由AI制作的奇幻微短剧，《山海奇镜之劈波斩浪》以中国古代经典《山海经》为灵感来源，为观众构建了一个光怪陆离的上古神话世界。该剧讲述了在上古神话世界，一名勇敢的少年向传说中的神兽发起挑战，拯救世界的热血故事。</p><p>&nbsp;</p><p>该剧导演陈坤（@闲人一坤）在看片会上分享了整个创作过程。陈坤表示，过去同样体量和质量的短剧，无论是以实拍还是动画制作的形式，制作周期最少也需要3-6个月，而《山海奇镜之劈波斩浪》仅制作而言只花费了2个月的时间，这在过去是“不可思议”的。</p><p>&nbsp;</p><p>他认为AI影视在发展初期，一定要选择传统影视的痛点来进行创作。在传统影视当中，奇幻和科幻两大赛道受制于CG特效的高成本和长周期，虽然有着坚实的观众需求基础，但发展受限。而AI的加入，让奇幻短剧的整个创作过程变得十分高效。</p><p>&nbsp;</p><p>“过去我们做传统影视，外拍需要100多人的制作团队，包括演员、摄影、灯光、音响、剧务、服化道等等。现在制作流程因AI的存在而完全升级了，全片没有一个画面是真人拍摄，服化道也可以通过AI来进行生成，极大地提升了创作效率，而且大大降低了制作成本。”</p><p>&nbsp;</p><p>关于可灵AI为该剧提供的技术支持，快手视觉生成与互动中心负责人万鹏飞介绍到，“可灵能够呈现出复杂的物理规律效果，例如筷子夹面、以及塞进嘴巴、咀嚼等过程，甚至包括面部动作都能够进行呈现。同时，可灵的视频大模型能够比较好地呈现出运动画面，可以很好地组合各种天马行空的想法。”而陈坤也在现场表示了对可灵的功能齐全度、运动幅度、运动合理性以及物理规律识别程度的认可。</p><p>&nbsp;</p><p>万鹏飞表示，可灵自发布并开放给创作者使用后，收到了很多创作者的反馈和建议，有助于后续版本的升级。</p><p>&nbsp;</p><p>快手文娱业务负责人陈弋弋则代表该剧出品方快手短剧，在活动现场分享了快手对于“微短剧+AI”的洞察和思考。目前快手平台上的短剧用户非常多，每天约有超2.7亿用户在观看短剧，播放量过亿的短剧有300多部，有超10万创作者在进行短剧相关的内容创作。从数据上看，星芒短剧内容处于供不应求的情况，而通过AIGC技术可以大幅度提高短剧创作的效率。</p><p>&nbsp;</p><p>陈弋弋还介绍，快手推出了“星芒短剧×可灵大模型”创作者孵化计划，将为创作者提供前期资金支持和激励，帮助短剧创作者创作更加优质的内容，未来还将对孵化计划迭代，带来更大的空间和更大的激励。</p><p>&nbsp;</p><p>针对现场关于“快手是否会在AI与娱乐内容上有更多类型化尝试”的提问，陈弋弋更是坦言“一切皆有可能”。在她眼中，快手是一个跟随用户和创作者需求发展的平台，未来随着用户消费内容、创作者反馈的变化，快手的技术会不断升级。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/4GqybUYo1vXoM1e9pE8v</id>
            <title>特朗普80后竞选搭档支持AI开源言论，赢得科技圈好评！网友：如果想让开发人员支持你，就坚持</title>
            <link>https://www.infoq.cn/article/4GqybUYo1vXoM1e9pE8v</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/4GqybUYo1vXoM1e9pE8v</guid>
            <pubDate></pubDate>
            <updated>Tue, 16 Jul 2024 07:34:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 特朗普, J.D. Vance, 开源人工智能, 美国总统大选
<br>
<br>
总结: 美国前总统特朗普宣布J.D. Vance将作为其副总统候选人参加2024年的总统大选，Vance是保守派MAGA理念的支持者，尤其在外交政策、贸易和移民方面是特朗普政策的捍卫者。Vance支持开源人工智能，认为解决方案是开源。他在参议院关于人工智能加速器的听证会上表示担忧，认为监管可能有利于政府而非消费者。Vance与硅谷关系密切，拥有多年风投经验，商业领域职业生涯比政治生涯时间长。 </div>
                        <hr>
                    
                    <p></p><p>北京时间7月16日凌晨，在险些被暗杀的几天后，美国前总统唐纳德·特朗普宣布，俄亥俄州联邦参议员J.D. Vance将作为其副总统候选人参加2024年的总统大选。</p><p>&nbsp;</p><p>Vance 今年39岁，如果在今年11月和特朗普一起赢得大选，他将成为美国历史上第三年轻的副总统。据介绍，万斯是“保守派MAGA理念”坚定支持者（MAGA，即Make&nbsp;America Great Again），尤其是在外交政策、贸易和移民方面，都是特朗普长期政治政策“让美国再次伟大”最有力的捍卫者之一。</p><p>&nbsp;</p><p>Vance 是美国典型的草根阶层起家的政治家，虽然他的一些政治观点颇受争议，但Vance 发表的支持开源人工智能的言论却受到了很多科技界人士的称赞。</p><p>&nbsp;</p><p></p><h2>Vance：解决方案是开源</h2><p></p><p>&nbsp;</p><p>在特朗普宣布Vance 参选的消息后，Vance 一条3月份的帖子再次被网友们翻了出来。这个帖子里，Vance 明确了自己支持人工智能开源的态度：</p><p>&nbsp;</p><p></p><blockquote>毫无疑问，人工智能存在风险。最大的风险之一就是，一群疯狂的党派人士利用人工智能将左翼偏见渗透到信息经济的每一部分。Gemini 无法产生准确的历史，ChatGPT 宣扬种族灭绝的概念。解决方案是开源。</blockquote><p></p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/f2/f2de784a226b2b23e96e75567badb031.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>值得注意的是，Vance 上周刚刚在美国参议院商务、科学和交通委员会关于“保护美国人隐私和人工智能加速器的必要性”的听证会上作证。据<a href="https://www.techpolicy.press/transcript-senate-hearing-on-protecting-americans-privacy-and-the-ai-accelerant/">TechPolicy.Press</a>"消息，Vance 在听证会上表示：</p><p>&nbsp;</p><p></p><blockquote>“我认为，很多时候，首席执行官，尤其是那些已经在人工智能领域占据优势地位的大型科技公司的首席执行官，会谈论这项新技术的可怕安全隐患，以及国会需要如何尽快站出来进行监管。我不禁担心，如果我们在现任政府的压力下采取行动，那么这将有利于现任政府，而不是美国消费者。&nbsp;”</blockquote><p></p><p>&nbsp;</p><p>开源人工智能模型提供商 Abacus AI 首席执行官 Bindu Reddy 激动地发文称，她认为“Vance 做到了！”并补充说，“解决方案当然是开源！”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/92/92ead4cf667c5106bbab66624acc2181.png" /></p><p></p><p>&nbsp;</p><p>自称是<a href="https://www.lesswrong.com/posts/2ss6gomAJdqjwdSCy/what-s-the-deal-with-effective-accelerationism-e-acc">有效加速主义者</a>"的 Tetsuo 也发布了一句引自 Vance “名言”的帖子：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/35/35ce0fabf75f85b97df3d962a37dc10a.png" /></p><p></p><p>&nbsp;</p><p>位于华盛顿特区的亲开源非营利组织&nbsp;Future Manifesto执行董事 Brian Chau在 X 上发布了 Vance 证词中的一段引言：</p><p>&nbsp;</p><p></p><p><img src="https://uploader.shimo.im/f/fSfEkyFeTlFag9Tt.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjExMTUyNTYsImZpbGVHVUlEIjoiOTAzMEpXbldZZUNEclZrdyIsImlhdCI6MTcyMTExNDk1NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.RaEEjSvLOuGRsLKedpatoF9mk7hOzoEdfD-mRFsLbmE" /></p><p></p><p>&nbsp;</p><p>有资深开发人士评论道，就凭Vance “The solution is open source” 这句话，硅谷很多人就会选特朗普。而有些人，一边剽窃开源，一边诋毁开源。</p><p>&nbsp;</p><p>现在还有网友纷纷跑去Vance帖子下面留言：“如果你想让开发人员支持你，请坚持支持开源。”</p><p>&nbsp;</p><p>当然，也有网友提出疑问：“我如何相信你当选后会继续支持开源？”</p><p>&nbsp;</p><p>实际上，Vance 本人与硅谷关系密切。他拥有多年的风投经验，直到2022年当选参议员后才告别该行业。可以说，近40岁的Vance，其商业领域的职业生涯比其政治生涯时间还长。</p><p>&nbsp;</p><p>2013年从耶鲁法学院毕业之后，Vance搬往旧金山并在那里担任Mithril Capital基金负责人。该基金由前 PayPal 首席执行官、长期为共和党提供大量资金的Peter Thiel 与 Ajay Royan共同创立。2016年，Thiel本人公开积极参与政治，并对首次参选美国总统的特朗普表示支持，同时资助Vance冲击参议院。但时间来到2024年，Thiel已明确表示不会在本轮大选中向任何共和党人捐款。</p><p>&nbsp;</p><p>2017年，Vance离开 Mithril，并加入了由前美国在线CEO Steve Case于华盛顿特区成立的Revolution公司担任董事总经理。供职期间，Vance 帮助 Case 发布了Rise of the Rest基金战略，此项战略专注于投资美国主要科技中心以外的初创企业。</p><p>&nbsp;</p><p>根据彭博社消息，很多创业界的人都为Vance 欢欣鼓舞，可见相对于大型科技公司，Vance 更加支持小型科创企业。</p><p>&nbsp;</p><p>同样是在Revolution任职时间，这位新晋共和党副总统候选人领导了与密歇根州Aatmunn公司的交易——后者是一家开发软件及可穿戴设备，以保障建筑工人在职安全的初创企业。他还支持并担任肯塔基州室内农业初创公司AppHarvest的董事会成员，这家公司于2021年通过SPAC上市，但2023年申请了破产保护。</p><p>&nbsp;</p><p>然而，Vance并没有在Revolution的第二只Rise of the Rest种子基金中发挥积极作用，该基金于2019年以1.5亿美元的融资成绩宣布关闭。在此期间，Vance主要专注于为自己的公司——位于辛辛那提的Narya Capital筹集资金。</p><p>&nbsp;</p><p>到2020年初，这家新公司从Thiel、Marc Andreessen和Eric Schmidt等有限合伙人手中筹得9300万美元，但未能达到1.25亿美元融资目标。据Axios消息，后来以共和党总统候选人身份加入、被认为是特朗普副总统有力竞争者之一的生物科技企业家Vivek Ramaswamy，也是Narya基金的有限合伙人之一。</p><p>&nbsp;</p><p>与Rise of the Rest一样，Narya基金主要专注于支持美国服务不足地区的初创企业。虽然尚不清楚Steve Case是否出手支持了Narya的第一只基金，但这位 AOL创始人已经公开与Vance的政治观点及职业生涯划清界线。</p><p>&nbsp;</p><p>Steve Case在2022年9月接受采访时曾表示，“自从他去年宣布竞选参议员以来，我就再没有和他交谈过，也没有支持他的竞选活动。”</p><p>&nbsp;</p><p>在赢得参议院竞选之后，Vance退出了Narya基金的运营工作。该公司目前由Narya联合创始人Colin Greenspon负责管理，他曾是Rise of the Rest种子基金的合伙人，同时曾任Mithril前董事总经理。根据2022年底提交给美国证券交易委员会的监管文件，Narya公司正在为第二只基金募集资金，目标同样是1.25亿美元。</p><p>&nbsp;</p><p></p><h2>国内外反对开源的CEO，各有说法</h2><p></p><p>&nbsp;</p><p>人工智能领域的开源问题一直是一个热门话题，Vance的这条推特再次引发了热议。国内外不少科技公司领导者都有不支持开源的，但大家理由各不相同。</p><p>&nbsp;</p><p>在最近的2024 世界人工智能大会期间，百度创始人、董事长兼首席执行官李彦宏就旗帜鲜明地反对开源：“开源是一种智商税。”</p><p>&nbsp;</p><p>“当你理性地去想，大模型能够带来什么价值，以什么样的成本带来价值的时候，就会发现，你永远应该选择闭源模型。今天无论是 ChatGPT、还是文心一言等闭源模型，一定比开源模型更强大、推理成本更低。”李彦宏说道。</p><p>&nbsp;</p><p>李彦宏认为，闭源模型不是一个模型，它是一系列的模型，可以根据使用场景平衡使用者的需求，比如要多好的效果、要多快的推理速度、要多低的成本。模型有非常多的变种，可以根据用户的需求，让他来做选择。</p><p>&nbsp;</p><p>他认为，开源模型打不过闭源模型。闭源模型还有一个开源模型不具备的优势，就是规模更小的模型都是从最大、最 powerful 的模型裁剪出来的，裁剪出来的这些更小规模的模型，仍然比那些同样规模的开源模型要效果更好。“To B 的机会仍然在闭源不在开源。”</p><p>&nbsp;</p><p>据媒体报道，12日，百度talk推送全体员工，李彦宏在邮件中再次公开diss了开源大模型。</p><p>&nbsp;</p><p>他表示，“外界有相当一批公司是想靠在开源上套个壳、做点客户化的东西去赚钱，所以你只要说开源不好，都会有一大堆人跳出来跟你理论说开源为什么好。但我是站在整个产业发展的角度真正讲道理、去看趋势——过去几百年历次产业革命所印证的，就是闭源的效率更高。这是毫无疑问的，没有人可以argue这一点，他们argue的全都是一些边边角角的别的东西，安全不安全可控不可控，所以我觉得有必要不断地澄清这个观点。”</p><p>&nbsp;</p><p>前谷歌CEO Eric Schmidt 也反对开源，他的主要考量是“安全”。</p><p>&nbsp;</p><p>Schmidt 多次在公共场合表了对开源安全的担忧，他以虚假信息和deepfakes深度伪造为例，表示这几乎就是无解的问题。原因是代码能够以几乎免费的方式快速生成虚假信息，而且无论好人还是坏人都可以轻松访问。做这种事不需要任何成本，而且生成的图像质量已经非常非常高了。虽然也有一些方法在尝试进行监管，但魔鬼一旦被召唤了出来，就不再受到创造者的控制。</p><p>&nbsp;</p><p>他在与外媒Noema Magazine对话时提到，只有那些向AI领域投入数千亿美元的西方领先企业，才会在能力攀升的过程中受到严格监管。</p><p>&nbsp;</p><p>“在开源和开放权重模型当中，源代码和模型权重（即用于确定不同关联强度的数值）都会向公众公开。这些信息会被立即传遍世界，包括传到中国、传到俄罗斯等。”Schmidt 说道。</p><p>&nbsp;</p><p>他还补充道，“我最近刚刚造访过中国，那里的几乎所有AI工作都是以西方世界的开源模型为起点，而后被不断放大。”</p><p>&nbsp;</p><p>对于中国AI技术的发展，Schmidt 认为中国落后美国大约两年。“相对芯片，我更担心开源方案的扩散。我相信中方也抱有同样的担忧，比如开源成果可能会被滥用以对付中美两国官方。”Schmidt 表示。</p><p>&nbsp;</p><p>所以，Schmidt 认为，首先需要确保开源模型能通过所谓“基于人类反馈的强化学习（RLHF）”来保障自身安全。这种强化学习经过微调，因此其中的护栏不会被恶意人士“撤销”。而且只要开源模型真正实现了安全，再要把它变得危险也将相当困难。</p><p>&nbsp;</p><p>Schmidt 还表示，50年前的物理学家必须拥有建造而且价格昂贵的回旋加速器，但在软件领域从来就没有出现过这样的高门槛情况，因为软件产品属于资本廉价型、而非资本密集型。但AI模型的出现改变了一切：其极高的训练强度对硬件提出了愈发复杂和精密的要求，而这也将对应着巨大的经济变化。</p><p>&nbsp;</p><p>对此各类机构正在以自己的方式解决这个问题。微软和谷歌等财力雄厚的公司正计划投入数十亿美元，毕竟他们手中有的是现金。Schmidt认为，大企业的资金会源源不断注入到研究中来，这是件好事，也是长久以来的创新来源。但其他机构，特别是大学，永远也负担不起这样的成本。他们没有能力投资硬件，自然也就做不出高度依赖硬件的创新成果。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://x.com/JDVance1/status/1764471399823847525">https://x.com/JDVance1/status/1764471399823847525</a>"</p><p><a href="https://techcrunch.com/2024/07/15/trumps-vp-candidate-j-d-vance-has-long-ties-to-silicon-valley-and-was-a-vc-himself/">https://techcrunch.com/2024/07/15/trumps-vp-candidate-j-d-vance-has-long-ties-to-silicon-valley-and-was-a-vc-himself/</a>"</p><p><a href="https://venturebeat.com/ai/trump-v-p-pick-j-d-vance-praised-for-comments-seemingly-in-support-of-open-source-ai/">https://venturebeat.com/ai/trump-v-p-pick-j-d-vance-praised-for-comments-seemingly-in-support-of-open-source-ai/</a>"</p><p><a href="https://www.noemamag.com/mapping-ais-rapid-advance/">https://www.noemamag.com/mapping-ais-rapid-advance/</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/g5rNfy9oXgaDmN8s6d1z</id>
            <title>AI 时代，网关更能打了？</title>
            <link>https://www.infoq.cn/article/g5rNfy9oXgaDmN8s6d1z</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/g5rNfy9oXgaDmN8s6d1z</guid>
            <pubDate></pubDate>
            <updated>Tue, 16 Jul 2024 07:29:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 网关, AI 时代, LLM 应用, Envoy
<br>
<br>
总结: 网关在网络通信中扮演着多重角色，包括数据转发、协议转化、负载均衡、访问控制和身份验证、安全防护、内容审核等。随着互联网进入AI时代，LLM应用对网关的需求也发生了变化，需要更强大的功能和性能。传统的Nginx网关难以应对新需求，因此基于Envoy的新一代开源网关应运而生，如Higress。在AI时代的互联网中，网关配置的热更新成为一个重要问题，Envoy等新型网关通过更合理的抽象和更新方式解决了传统网关的瓶颈问题。 </div>
                        <hr>
                    
                    <p></p><p>网关在网络通信中扮演着诸多角色，包括数据转发、协议转化、负载均衡、访问控制和身份验证、安全防护、内容审核，以及服务和 API 颗粒度的管控等，因此常见的网关种类有流量网关、安全网关、微服务网关、API 网关等。在不同语义下，网关的命名也会有所不同，例如 K8s 体系下，有 ingress 网关，在 Sping 体系下，有 Spring Cloud Gateway。但不论如何命名，网关的管控内容几乎都离不开流量、服务、安全和 API 这 4 个维度，只是功能侧重不同、所遵循的协议有差异。</p><p></p><p>另外，随着互联网从 Web 2.0 迈进到 AI 时代，用户和互联网的交互方式，AI 时代下互联网的内容生产流程都发生了显著的转变，这对基础设施（Infra）提出了新的诉求，也带来了新的机遇。Infra 包含的内容非常丰富，本文仅从网关层面分享笔者的所见所感所悟。</p><p></p><p>我们先来看一些 AI 时代出现的新场景和新需求：</p><p></p><p>相比传统 Web 应用，LLM 应用的内容生成时间更长，对话连续性对用户体验至关重要，如果避免后端插件更新导致的服务中断？相比传统 Web 应用，LLM 应用在服务端处理单个请求的资源消耗会大幅超过客户端，来自客户端的攻击成本更低，后端的资源开销更大，如何加固后端架构稳定性？很多 AGI 企业都会通过免费调用策略吸引用户，如何防止黑灰产爬取免费调用量封装成收费 API 所造成的资损？不同于传统 Web 应用基于信息的匹配关系，LLM 应用生成的内容则是基于人工智能推理，如果保障生产内容的合规和安全？当接入多个大模型 API 时，如何屏蔽不同模型厂商 API 的调用差异，降低适配成本？</p><p></p><p>在支持大量 LLM 客户的过程中，我们也看到了一些行业发展趋势，借本文分享给大家：</p><p></p><p>互联网内容的生产机制将从 UGC（User Generate Content） 转变为 AIGC（Artificial Intelligence Generate Content），互联网流量增长，除了要考虑传统的 SEO，还需要考虑 AI 抓取下的 SEO。目前处于 AI 时代的 Web 1.0 阶段，基于静态内容生成，可以预见，AI 时代的 Web 2.0 不久会到来，基于理解互联网内容来识别页面中提供的“可操作能力”，来完成复杂任务，真正的 Web 3.0 也将由 AI 来实现。API 是 LLM 应用的一等公民，并引入了更多流量，催生企业新的生命力和想象空间。LLM 应用对网关的需求超越了传统的流量管控功能，承载了更大的 AI 工程化使命。</p><p></p><p></p><h3>AI 场景下的新场景和新需求</h3><p></p><p></p><p>相比传统 Web 应用，LLM 应用在网关层的流量有以下三大特征：</p><p></p><p>长连接。由 AI 场景常见的 Websocket 和 SSE 协议决定，长连接的比例很高，要求网关更新配置操作对长连接无影响，不影响业务。高延时。LLM 推理的响应延时比普通应用要高出很多，使得 AI 应用面向恶意攻击很脆弱，容易被构造慢请求进行异步并发攻击，攻击者的成本低，但服务端的开销很高。大带宽。 结合 LLM 上下文来回传输，以及高延时的特性，AI 场景对带宽的消耗远超普通应用，网关如果没有实现较好的流式处理能力和内存回收机制，容易导致内存快速上涨。</p><p></p><p>传统 Web 应用中普遍使用的 Nginx 网关难以应对以上新需求，例如变更配置需要 Reload，导致连接断开，不具备安全防护能力等。因此国内外均出现了大量基于 Envoy 为内核的新一代开源网关，本文将以笔者维护的 Higress (<a href="https://github.com/alibaba/higress">https://github.com/alibaba/higress</a>") 为例展开描述。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/58/58b26b83a34f1504747119780ff6900d.png" /></p><p></p><p>Higress 已经为通义千问 APP、灵积平台 (通义千问 API 服务)、人工智能平台 PAI 提供 AI 业务下的网关流量接入，以及多个头部 AGI 独角兽提供 API 网关。这篇文章详细介绍了 Higress AI 网关的能力：《Higress 发布 v1.4，开放 AI 网关能力，增强云原生能力》</p><p></p><p></p><h4>如何实现网关配置的热更新</h4><p></p><p></p><p>互联网从 Web 1.0 演进到 Web 2.0 的时代，互联网从静态内容为主，变为动态更新的 UGC 内容为主，大量用户开始高频使用互联网。用户使用形态，以及网站内容形态的改变，催生了大量技术的变革。例如 HTTP 1.0 到 HTTP 1.1 协议的升级，解决了连接复用的问题。又例如以 Nginx 为代表的基于异步非阻塞的事件驱动架构的网关诞生，解决了 C10K 问题。</p><p></p><p>到了 AI 时代的互联网，LLM 驱动的对话式场景，大量采用 SSE/WebSocket/gRPC 等长连接协议来维持会话。网关除了要解决并发连接问题，还需要解决配置变更导致连接断开的问题。配置变更时的连接断开，不但导致用户会话断开，影响体验，在高并发场景下，断开后的并发重连风暴很有可能将网关和后端业务同时打挂。</p><p></p><p>而类似 Nginx 这样 Web 2.0 时代诞生的网关，并不能解决此问题，Nginx 的整体配置文件发生任意变更，都需要重启 Worker 进程，会同时导致客户端连接（Downstream）和服务端连接（Upstream）断开：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/37/377a410446cf2da8b0cfd849eedba811.png" /></p><p></p><p>笔者参与维护的 Higress 开源网关，使用 Envoy 作为数据面，来解决这一问题。Envoy 站在 Nginx 等网关的肩膀上，对网关配置做了更合理的抽象。例如将处理客户端连接（Downstream）的监听器（Listener）配置发现定义为 LDS（Listener Discovery Service），将处理后端服务连接（Upstream）的服务集群（Cluster）配置发现定义为 CDS（Cluster Discovery Service）。LDS 和 CDS 可以独立更新，从而 Listener 连接池参数更新不会断开 Upstream 连接，Cluster 连接池参数更新变了不会断开 Downstream 连接。</p><p></p><p>对于跟连接无关的配置，又做了进一步抽象，例如路由配置发现定义为 RDS（Route Discovery Service），TLS 证书 / 密钥配置发现定义为 SDS（Secret Discovery Service），都可以独立更新，那么无论是路由变更，还是 HTTPS 证书变更，都不会导致任何连接断开：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7a/7ab2e14a56bf06a7790b10a325ec9c51.png" /></p><p></p><p></p><h4>如何在网关层做好安全和流量防护</h4><p></p><p></p><p>当前的 AI 技术，尤其是 LLM 正处于快速发展阶段。虽然模型压缩、知识蒸馏等技术正被广泛应用以提高效率，但 LLM 应用的资源消耗仍然显著高于传统 Web 应用。</p><p></p><p>针对传统的 Web 应用，服务端处理单个请求的资源消耗通常不会大幅超过客户端，因此对客户端来说，发起分布式拒绝服务（DDoS）攻击的成本相对较高。然而在 LLM 应用的场景中，客户端通过发送长文本或提出复杂的推理问题，可以轻易增加服务端的负载，而自身资源消耗甚微。这种情况突显了在 LLM 应用中部署强大的网关安全防护策略的重要性。传统网关，通常具备两类限流能力：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/e3/e33666a1c041612c6b750fb84ca599dd.png" /></p><p></p><p>Higress 不仅支持这些传统限流能力，例如每秒、每分、每小时和每天的请求次数限制（QPS/QPM/QPH/QPD），还引入了对令牌数量的细粒度管理，采用每分钟、每小时和每日令牌数（TPM/TPH/TPD）作为衡量指标，除了 QPS，还支持面向 Token 吞吐的限流防护。</p><p></p><p>“令牌”（Token）在这里作为一个衡量单位，更准确地量化了 LLM 处理的数据量。对 LLM 应用而言，以令牌而非传统请求次数来计量使用情况，更能贴切地反映资源消耗和成本开支。同时也支持多种限流统计维度，包括：API、IP、Cookie、请求 Header、URL 参数和基于 Bearer Token 认证的调用方。</p><p></p><p>AI 场景下，后端保护式限流尤其重要，很多 AGI 厂商都有免费的 Web 应用吸引用户流量，而一些黑灰产可能会爬取页面调用封装成收费 API 来提供给用户实现牟利。这种情况下就可以使用 Higress 的 IP、Cookie 等维度的保护式限流进行防护。</p><p></p><p>此外，当大模型未经过适当的过滤和监控就生成回应时，它们可能产生包含有害语言、误导信息、歧视性言论甚至是违反法律法规的内容。正是因为这种潜在的风险，大模型中的内容安全就显得异常重要。在 Higress 中，通过简单的配置即可对接阿里云内容安全服务，为大模型问答的合规性保驾护航。</p><p></p><p></p><h4>如何应对大带宽和高延时的流量特征</h4><p></p><p></p><p>除了能针对 Token 进行限流，基于 Token 的完整可观测能力，也是 AI Infra 中不可或缺的，例如提供日志、指标、告警等可观测能力。下方展示的限流、观测能力，都依赖对 HTTP 请求 / 响应 Body 的解析处理。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/92/9259de05e4df96b7edc726dfed74ee25.png" /></p><p></p><p>传统网关，如 Nginx/Openresty，以及基于此实现的 Kong/APISIX 等在 Lua 脚本中处理 Body 时，要求必须对请求 / 响应开启缓存。而基于 Envoy 的开源网关，例如 Higress，其插件扩展机制是基于 Wasm 实现的，能够支持对 Body 的流式处理，以处理请求 Body 为例：</p><p></p><p><code lang="null">func onHttpRequestBody(ctx wrapper.HttpContext, config Config, chunk []byte, isLastChunk bool, log wrapper.Log) []byte {    log.Infof("receive request body chunk:%s, isLastChunk:%v", chunk, isLastChunk)    return chunk}
</code></p><p></p><p>在 AI 场景下，因为大带宽 / 高延时的流量特征，网关是否对请求 / 响应进行真正的流式处理，影响是巨大的。</p><p></p><p>首先，LLM 场景下如果网关没有实现流式响应，将严重影响用户受到首个响应的时间，其速度影响能从秒级变到分钟级，严重影响用户体验。其次，是对资源开销的影响。以 Higress 的一个开源用户 Sealos 举例（旗下有 FastGPT 等 AI 相关平台产品），在使用 Nginx 时因为开启了请求 / 响应缓存，在 AI 业务应用被高并发访问时，网关资源水位占用处于崩溃边缘。迁移到 Higress 之后，网关只需很少资源。因为 Higress 提供了完整的流式转发能力，而且提供的插件扩展机制也可以流失处理请求 / 响应，在大带宽场景下，所需的内存占用极低。内存虽然相比 GPU 很廉价，但内存控制不当导致 OOM，导致业务宕机，损失不可估量。下图是常态流量下，Sealos 切换前后网关使用资源的对比：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7e/7e5b2b02182c681c6d8f3407e63de6e7.png" /></p><p></p><p></p><h4>如何提升海量域名、海量理由规则下的多租能力</h4><p></p><p></p><p>在 AI 场景下，Envoy 的热更新能力备受青睐，Higress 的一些 AI 平台场景的用户，在一开始也选用了基于 Envoy 的网关，例如 Contour、Gloo、Istio gateway 等。但大都会遇到两个问题：</p><p></p><p>给每个用户 or 每个模型分配一个域名，数量级达到一万规模时，新建路由的生效速度至少要 1 分钟；对多个租户域名使用同一本泛域名证书，开启 HTTP2 时，浏览器访问会遇到 404 问题。</p><p></p><p>对于第一个问题，其根本原因在于路由规则下发方式不够精细，社区开发者对此进行过分析。与此相比，Higress 可以在域名级别进行分片加载，即使达到一万个域名，新增路由的生效时间也只需三秒。此外，Higress 支持按需加载机制，即只有在接收到特定域名的请求时才加载该域名下的路由配置。在配置了大量域名的环境下，这种策略只加载活跃的路由配置，显著减少了网关的内存使用。</p><p></p><p>关于第二个问题，浏览器在 HTTP2 环境中会尽量复用连接。两个请求的域名不同，但解析到的 IP 地址和使用的证书是相同时，连接复用会导致 Host 请求头与建立连接时的 SNI 不匹配，进而在 Envoy 场景下产生 404 错误。多数基于 Envoy 的解决方案是返回 421 状态码，提示浏览器断开连接并重新发起请求，但这个解决方案在浏览器兼容性上存在问题。于是，Higress 借鉴了 Nginx 的办法，使 SNI 的查找（TLS 层）与 Host 头部的查找（HTTP 层）分离，允许它们不匹配，从而能正确地路由配置（在要求客户端证书验证的场景例外）。</p><p></p><p>Higress 支撑海量域名的能力，也是众多 MaaS/SaaS 服务用于实现多租的关键。比如智算服务 PAI- 灵骏平台在近期将网关从同样基于 Envoy 实现的 Contour 迁移到了 Higress 之后，新增路由生效的时间从分钟级变为秒级，同时整体消耗的云资源也大幅下降。</p><p></p><p></p><h3>AI 场景下，网关比我们想象中更能打</h3><p></p><p></p><p>传统 Web 应用，网关扮演的基础角色是流量管理。但在 AI 场景下，网关正承载着更大的 AI 工程化使命，分别体现着 MaaS/AGI 接入层、应用接入层、和企业内部各类系统对接等。</p><p></p><p></p><h4>MaaS/AGI 接入层</h4><p></p><p></p><p>整体架构如下，网关对接入层进行流量管理，除此之外还具备满足负载均衡和流量灰度和观测的能力。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7e/7e68956a812953995aa8c60da9614a87.png" /></p><p></p><p>负载均衡：</p><p></p><p>由于 AI 场景下，网关的后端通常是模型服务本身，对网关的负载均衡能力提出了特殊要求。由于 LLM 场景具有高延时，且不同请求差异大的特点，传统的 Round Robin 负载均衡策略无法正确平衡负载。Higress 目前采用基于最小请求数的均衡策略，将请求优先转发给当前处理中请求最少的后端节点。针对模型服务负载均衡的挑战，Higress 计划在未来通过调用一个低延时的小参数模型进行旁路预测，以估计每个后端节点的实时负载，从而尽量将请求发送给负载最低的后端节点。</p><p></p><p>流量灰度和观测：</p><p></p><p>AGI 厂商高度依赖 A/B 测试和服务灰度能力来进行模型迭代。作为流量入口，AI 网关需要在流量灰度和观测方面发挥关键作用，包括灰度打标以及入口流量延时和成功率等指标的监测。Higress 凭借其在云原生微服务网关领域的经验，已经积累了强大的能力来满足这些需求。</p><p></p><p></p><h4>AI 应用层</h4><p></p><p></p><p>整体架构如下：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ac/acde0335f6e9e3ae5033ab567e1e21e0.jpeg" /></p><p></p><p>跟随 GPT4 等模型的爆火，涌现了大量的优秀的 AI SaaS 应用，例如：</p><p></p><p>makelogo.ai：AI 生成产品 LogoMyMap.ai：AI 辅助规划 IdeaGamma：AI 生成 PPTPodwise：AI 辅助查看播客</p><p></p><p>许多 AI 应用开发者，尤其是独立开发者，通常不会自己部署模型服务，而是直接利用模型厂商提供的强大 API 来实现创意应用。值得注意的是，许多开发者来自国内。然而，由于底层技术依赖于 OpenAI 等海外 LLM 厂商，这些技术可能不符合国内法规。为了避免潜在的麻烦，这些开发者往往选择将产品推向国际市场，而不是面向国内用户。</p><p></p><p>随着国内大模型技术逐渐赶上 OpenAI 等厂商，并且国内 API 在价格上具有竞争优势，越来越多的 AI 应用预计会选择使用国内厂商的 API 来实现相关功能。这将对网关提出特定需求：</p><p></p><p>通过网关的统一协议，屏蔽不同模型厂商 API 的调用差异，降低适配成本。对涉黄涉政等敏感内容进行屏蔽和过滤，更好地符合国内法规要求。切换模型后的 A/B 测试以及效果观察和对比，包括延迟、成本、用户使用频率等因素。</p><p></p><p>Higress 目前支持的 LLM Provider 有：通义千问、OpenAI/Azure OpenAI、月之暗面、百川智能、零一万物、智谱 AI、阶跃星辰、文心一言、腾讯混元、DeepSeek、Anthropic Claude、Groq、MiniMax、Ollama 等，借助 Higress 活跃的开源开发者社区，支持的类型还在持续增加中。</p><p></p><p></p><h4>企业内部 AI 网关</h4><p></p><p></p><p>整体架构如下：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/59/59fadb676e0259ff2aad044afb48d4f5.png" /></p><p></p><p>大量 AGI 厂商在闭源和开源模型能力方面展开竞争，而受益者主要是企业用户。企业在选择模型时需要在性能和成本之间做 trade-off。面对众多模型，尤其是在厂商提供的 API 不一致时，企业需要一个统一的网关来屏蔽模型协议的差异，从而提供一个统一接口，便于企业内部系统的对接和使用。在这种场景下，网关的架构类似于 ESB（企业服务总线）的架构，即所有内部 AI 流量都通过网关进行统一治理。这样的架构带来了以下好处：</p><p></p><p>成本分摊计算：借助网关的观测能力，可以审计企业内部不同业务部门的 Token 消耗量，用于成本分摊并发现不合理的成本。提高稳定性：基于网关提供的多模型对接能力，当主用模型调用失败时，可以自动切换至备用模型，保障依赖 AI 能力的业务稳定性。降低调用成本：在一些固定业务流程中，LLM 接口的输入输出相似性较高时，可以基于向量相似性进行缓存召回，从而降低企业的 AI 调用成本。认证和限流：通过对企业内员工的 API 调用进行限量控制，管理整体成本。内容安全：实现统一的内容安全管理，禁止发送敏感数据，防止企业敏感数据泄漏。</p><p></p><p>这种架构下，网关不再只是接入层的流量网关，而是要处理来自所有依赖 AI 能力的业务模块的访问流量。网关更能打了。</p><p></p><p></p><h3>畅想 AI 时代的互联网发展</h3><p></p><p></p><p>笔者发现在 AI 火了之后，大家已经很少提 Web 3.0 的概念了。而且很有趣的一个事是，CDN 和网络防护提供商 CloudFlare，已经将控制台内的一级入口 Web 3.0 换成了 AI，并集成了 Workers AI 和 AI Gateway 这两款产品。而笔者觉得，真正的 Web 3.0 也许将由 AI 带来。</p><p></p><p>就像 Web 1.0 到 Web 2.0 的演进，用户的交互方式和互联网的内容形式发生了彻头彻尾的改变，我们其实已经身处在类似的改变之中。例如，笔者的常用搜索工具从 Google 换到了 Perplexity。做互联网流量增长，除了要考虑传统的 SEO，还需要考虑 AI 抓取下的 SEO，下面来自 Perplexity 对这一问题的回答：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/9a/9abc239430c6fdbedf7ec85ca0549f80.png" /></p><p></p><p>到并不是说 Perplexity 未来一定会替代 Google，但这种改变其实反应了一种趋势：</p><p></p><p>从用户角度看，用户从主动参与互联网转变为通过 AI 帮助参与。从内容角度看，不仅需要服务于真实用户，还要同时服务于 AI。</p><p></p><p>Perplexity 这样的工具还只是基于静态内容，可以类比为 AI 时代的 Web 1.0。可以预见，AI 时代的 Web 2.0 会是：</p><p></p><p>电商场景下，在用户浏览商品时，AI 将充当导购，根据商品信息与用户对话，并在用户确认后自动下单；出行场景下：AI 将根据用户的出行目标地点自动安排旅行计划，了解用户喜好，预订沿途餐厅和酒店；OA 场景下：用户需要操作资源时，AI 将自动提交审批申请，查询审批状态，并在获批后完成资源操作。</p><p></p><p>在这种模式下，AI 需要理解互联网内容，并识别页面中提供的“可操作能力”，从而代替人类执行操作。苹果宣布将在 iOS 18 中 大幅提升 Siri 的能力，未来 Siri 将能够访问应用程序的各种功能，这也需要应用程序为 AI 提供“可操作能力”的声明。HTML 也有相关社区提案，让 AI 可以更方便地识别页面中的可执行任务，明确其输入和输出定义。</p><p></p><p>未来的互联网内容，无论是 APP 还是 HTML 场景，都将面向 AI 进行改变。核心在于让 AI 知道如何操作页面内容，从而帮助用户完成复杂任务。为 AI 提供的“可操作能力”声明，实际上就是 API 声明。当前，大量互联网应用，尤其是 ToC 应用，API 仅在内部开发过程中使用，最高频使用 API 的可能是前端或 BFF 层的开发人员。在国内，由于研发成本普遍低于国外，不会为了降低前后端对接成本，而去优化 API 设计，开发过程中往往忽略了 API 的重要性。因此，虽然在海外 API 管理产品的市场竞争已经是一片红海，但在国内 API 管理以及 API First 的理念并不普及。</p><p></p><p>随着 AI 操作互联网场景的不断增加，API 将成为 LLM 应用的一等公民，API 管理的重要性将愈发明显。类似于 Perplexity 在抓取页面内容时使用清晰的标题、小标题和列表以便 AI 更容易理解和提取内容；定义清晰的 API、明确的输入输出参数，以及 API 的版本管理，将变得至关重要。</p><p></p><p>对网关来说，应回归本质，在 AI 的加持下，帮助用户做好 API 的设计、管理将成为核心能力。而通过合理设计的 API，网关也可以更深入地了解所处理流量的业务含义，从而实现更智能化的流量治理。</p><p></p><p>内容推荐</p><p></p><p>AIGC技术正以惊人的速度重塑着创新的边界，InfoQ 首期《大模型领航者AIGC实践案例集锦》电子书，深度对话30位国内顶尖大模型专家，洞悉大模型技术前沿与未来趋势，精选10余个行业一线实践案例，全面展示大模型在多个垂直行业的应用成果，同时，揭秘全球热门大模型效果，为创业者、开发者提供决策支持和选型参考。关注「AI前线」，回复「领航者」免费获取电子书。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/18/18566cb9b5575c02ffb89182a3582b73.jpeg" /></p><p></p><p>活动推荐</p><p></p><p>AICon 全球人工智能开发与应用大会，为资深工程师、产品经理、数据分析师等专业人群搭建深度交流平台。聚焦大模型训练与推理、AI Agent、RAG 技术、多模态等前沿议题，汇聚 AI 和大模型超全落地场景与最佳实践，期望帮助与会者在大模型时代把握先机，实现技术与业务的双重飞跃。</p><p></p><p>在主题演讲环节，我们已经邀请到了「蔚来创始人 李斌」，分享基于蔚来汽车 10 年来创新创业过程中的思考和实践，聚焦 SmartEV 和 AI 结合的关键问题和解决之道。大会火热报名中，7 月 31 日前可以享受 9 折优惠，单张门票节省 480 元（原价 4800 元），详情可联系票务经理 13269078023 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/65/6573657a90550f91dc3658ad05122b02.other" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/dd52FPTw9XqEjkmY5ZRj</id>
            <title>大厂期权归属前遭暴力裁员，80 余万期权泡汤；去哪儿宣布每周两天“不坐班”；萝卜快跑是人类远程代驾？客服：无人操控 | Q资讯</title>
            <link>https://www.infoq.cn/article/dd52FPTw9XqEjkmY5ZRj</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/dd52FPTw9XqEjkmY5ZRj</guid>
            <pubDate></pubDate>
            <updated>Mon, 15 Jul 2024 07:31:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 苹果, AI服务, 期权, 裁员
<br>
<br>
总结: 苹果公司计划增加iPhone 16系列机型的出货量，依靠AI服务提振需求；大厂员工因裁员导致期权泡汤，引发法律纠纷。萝卜快跑无人驾驶服务被曝有真人干预，百度设立无人驾驶实验基地应对问题。 </div>
                        <hr>
                    
                    <p></p><blockquote>苹果大动作！AI 服务或为关键驱动力；大厂期权归属前遭暴力裁员，80余万期权泡汤；“萝卜快跑”有真人干预？腾讯被爆调薪！微软和苹果双双放弃 OpenAI 董事会观察员席位；谷歌开放“暗网报告”功能；中国区员工只能用 iPhone？微软回应；抖音宣布推出抖音 VR 直播；去哪儿网正式推行“3+2”混合办公模式；马斯克叫停与甲骨文的 100 亿美元谈判；“WPS 崩了”：三周内第二次；RockYou2024 文件泄露，数百万用户信息暴露；二季度 PC 出货量增长 3%；开源代码编辑器 Zed 发布原生 Linux 版本；Java 之父 James Gosling 宣布退休……</blockquote><p></p><p></p><h2>科技公司</h2><p></p><p></p><h4>苹果大动作！iPhone&nbsp;16出货预期大增，AI服务或为关键驱动力</h4><p></p><p>彭博社7&nbsp;月&nbsp;11&nbsp;日报道，表示苹果公司已经通知其供应商和合作伙伴，**2024&nbsp;年&nbsp;iPhone&nbsp;16&nbsp;系列机型的出货量目标要比&nbsp;iPhone&nbsp;15&nbsp;系列（8100&nbsp;万台）增长&nbsp;10%，至少要超过&nbsp;9000&nbsp;万台，**以期借助人工智能（AI）服务带来的潜在需求扫除公司2023年遭遇的阴霾。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7c/7cedef26a96016811c9384717d707bcf.png" /></p><p></p><p>这印证了本周早些时候产业链人士的说法。知情人士表示，苹果告诉供应商和合作伙伴，其新款iPhone的出货量将较前几款增加约10%。与之相比，在2023年下半年，iPhone&nbsp;15的出货量约为8100万部。</p><p></p><p>根据知情人士的说法，苹果已经变得越来越有信心，这家科技巨头可能认为，其推出的个人智能化系统“Apple&nbsp;Intelligence”（苹果智能）中的一些功能将有助于提振iPhone&nbsp;16上市时的需求。</p><p></p><p>上月，苹果在年度全球开发者大会（WWDC）上披露了公司在AI方面新的进展，包括与OpenAI构建合作伙伴关系，推出能够优先置顶推送、总结文本、生成图片的套件，更强大的Siri等。</p><p></p><h4>大厂期权归属前遭暴力裁员，80&nbsp;余万期权泡汤</h4><p></p><p>近日，综合凤凰网和澎湃新闻消息，在得物兢兢业业工作两年后，前员工徐凯决定和老东家“对簿公堂”。</p><p></p><p>一年前，徐凯多次与公司沟通取得期权再离职未果后，到上海市仲裁委员会处申请恢复与得物的劳动关系，后被予以支持。2024&nbsp;年&nbsp;7&nbsp;月，因不服上海市仲裁委员会裁定的结果，得物继续上诉，再度将前员工诉于法庭之上。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ed/ed62eadbc21f8bfc6749ed84519e39aa.webp" /></p><p></p><p>据徐凯表述，其在&nbsp;2021&nbsp;年&nbsp;9&nbsp;月加入该公司，任职为前端技术专家，税前薪资为月薪&nbsp;5&nbsp;万元左右，同时其握有部分期权。徐凯在工作期间，一直保持高效且敬业的态度，未曾出现过任何显著的工作失误。然而，在&nbsp;2023&nbsp;年&nbsp;7&nbsp;月，公司却以“未按时提交周报”和“工作时间分配不当”等理由，对他进行了绩效评估，最终给出了最低等级&nbsp;C&nbsp;的评价。紧接着，该企业启动了裁员流程，意图解除与徐凯的劳动合同。</p><p></p><p>尤为令人关注的是，此时距离徐凯手中期权的归属期满仅剩一个多月。他持有的近&nbsp;2000&nbsp;股期权，按原计划将在&nbsp;2023&nbsp;年&nbsp;9&nbsp;月行权大约&nbsp;1000&nbsp;股，价值高达&nbsp;80&nbsp;余万元。然而，裁员决定一旦生效，这些期权将自动失效，导致徐凯的潜在收益瞬间蒸发。</p><p></p><p>徐凯还表示，他在该公司工作期间常年面临着“10106”的局面，晚上&nbsp;10&nbsp;点下班是常态，且技术员工会被计算总工时，工时靠后的人就面临着被淘汰的风险。“我一开始是带团队的，风险就很高”。得物相关负责人针对以上情况回复凤凰网表示称，“该员工曾因&nbsp;3&nbsp;次绩效考核不合格（2022&nbsp;年&nbsp;Q3&nbsp;季度、2023&nbsp;年&nbsp;Q1&nbsp;季度、2023&nbsp;年&nbsp;Q2&nbsp;季度），于去年&nbsp;7&nbsp;月已经沟通解除劳动合同”。此外，得物方面还表示，“得物公司业务健康发展，欢迎优秀人才加入，感谢关注。”</p><p></p><p>另外，2023&nbsp;年&nbsp;7&nbsp;月，京东司法拍卖网披露的消息显示，“上海市中山南路&nbsp;566&nbsp;弄”一处房屋以&nbsp;1.58&nbsp;亿元成交，较起拍价&nbsp;1.25&nbsp;亿元溢价&nbsp;26.4%。根据竞拍结果显示，该套房屋由杨冰拍下。据澎湃新闻报道，该名自然人杨冰为得物创始人兼&nbsp;CEO。</p><p></p><h4>“萝卜快跑”被曝有真人干预？</h4><p></p><p>最近有网图流传，揭示了关于“萝卜快跑”无人驾驶服务的新视角——其背后竟隐藏着真人远程代驾的运作模式。图片聚焦于萝卜快跑的智控中心，清晰可见有专业人员坐在配备模拟方向盘的监控屏幕前，精准操控着车辆。</p><p></p><p><img src="https://static001.geekbang.org/infoq/45/4512c73dde1c4c5097579a4a8129212b.webp" /></p><p></p><p>据媒体报道，百度设立的无人驾驶实验基地，其核心功能之一便是应对无人驾驶车辆可能遭遇的棘手问题，通过云端安全员的远程介入，确保车辆能够安全脱困。</p><p></p><p>鉴于极端驾驶场景并非常态，云端驾驶员相比随车安全员展现出更高的效率与灵活性。他们能以“一对多”的模式，同时服务于多辆无人驾驶车辆，极大地提升了资源利用率与响应速度。知情人士进一步透露，在无人驾驶网约车的运营体系中，后台座舱内的一名安全员能够高效监控并管理&nbsp;3&nbsp;至&nbsp;5&nbsp;台车辆，这些安全员多具备丰富的驾驶经验，来自网约车司机、公交车司机等职业背景。</p><p></p><p>此外，随着&nbsp;2023&nbsp;年&nbsp;11&nbsp;月交通运输部办公厅发布的《自动驾驶汽车运输安全服务指南&nbsp;(试行)》的正式实施，明确了在特定区域内运营的完全自动驾驶出租车，可采用远程安全员模式，并规定了远程安全员与车辆之间不得低于&nbsp;1:3&nbsp;的人车比，为无人驾驶行业的规范化发展提供了政策指引。</p><p></p><h4>腾讯被爆调薪！年底十三薪分摊到月薪</h4><p></p><p>7月10日，腾讯内部向全员发布邮件称，将调整内部的薪酬福利政策，一是将年底十三薪分摊到月薪上；二是将现有的易居租房补贴融入月薪。</p><p></p><p><img src="https://static001.geekbang.org/infoq/03/03fb1bdb72e4e972cc75fd34872b2917.png" /></p><p></p><p>图片来源于网络</p><p></p><p>腾讯表示，过去两年，外部环境的变化让不少员工对于即时稳定的现金流有了更高诉求。经过慎重评估后，决定今年除正常进行年度薪酬回顾外，还将对薪酬结构做出两个调整：一是进行全公司薪酬结构的调整，把服务奖融入月薪；二是将现有的易居租房补贴融入月薪。</p><p></p><p>腾讯邮件中称，这两个举措旨在帮助大家在更高、更稳定的月收入基础上更安心地安排工作与生活。相关调整于2024年7月1日起生效，8月5日的发薪中开始体现。</p><p></p><p>腾讯表示，自成立之初便在年终为员工提供额外的十三薪，以此作为对员工一年辛勤工作的认可。随着公司规模的扩大，腾讯还引入了与业绩挂钩的奖金制度，形成了包括服务奖、绩效奖和特别奖在内的年终奖金体系，其中服务奖主要取决于员工的月工资和全年的工作时长，实质上是固定工资的补充部分。</p><p></p><p>此次将以年度薪酬回顾后的月薪为标准，把服务奖平均分摊融入到12个月固定工资中，以提升员工的月度现金流，未来年终奖也将回归到纯粹的业绩激励。</p><p></p><p>相关知情人士表示，对员工而言，这次将十三薪和租房补贴融入到月薪之后，每月员工到手的收入将增加，也比较利好生活压力比较大的应届生和有房贷的员工。</p><p></p><h4>微软和苹果双双放弃OpenAI董事会观察员席位</h4><p></p><p>据金融时报7月10日报道，在全球监管机构对大型科技公司投资AI初创企业的审查日益加剧之际，美国两大科技巨头微软和苹果都放弃了在OpenAI董事会的观察员席位——微软宣布立即退出OpenAI董事会的观察员席位，苹果也不会担任此类职务。这就意味着，OpenAI&nbsp;董事会不再设立无投票权的观察员席位。</p><p></p><p>鉴于全球监管部门审查科技巨头投资&nbsp;AI&nbsp;初创企业的活动越来越严格，微软已放弃了其在&nbsp;OpenAI&nbsp;董事会观察员的席位，而苹果将不会担任类似的职位。</p><p></p><p>微软已向&nbsp;OpenAI&nbsp;投资了&nbsp;130&nbsp;亿美元，微软在致&nbsp;OpenAI&nbsp;的一封信中表示，退出&nbsp;OpenAI&nbsp;董事会的席位“立即生效”。</p><p></p><p>据一位直接了解此事的人士透露，作为将&nbsp;ChatGPT&nbsp;整合到苹果设备的交易的一部分，外界原本预计苹果也将在&nbsp;OpenAI&nbsp;董事会中担任观察员角色，但现在它不会这么做。苹果拒绝置评。</p><p></p><h4>谷歌开放“暗网报告”功能：网罗安全事件、通知用户信息泄露</h4><p></p><p>据消息，谷歌公司7月10日宣布将于本月底向所有谷歌账号用户开放“暗网报告”功能，帮助用户更快了解网络上发生的个人数据泄露事件。 “暗网报告”功能此前仅限于购买&nbsp;Google&nbsp;One&nbsp;订阅的功能，主要监控常规网络方式无法访问的网络部分，除了排查个人信息是否已经泄露之外，还可以搜索相关漏洞信息。</p><p></p><p>谷歌在公告中表示&nbsp;Google&nbsp;One&nbsp;本月底将不再提供“暗网报告”功能，用户登录账号后可以免费访问。</p><p></p><p><img src="https://static001.geekbang.org/infoq/61/61bcd4c8ceef1f69f566f497ec161cce.png" /></p><p></p><p>谷歌用户登录账号之后，可以打开“关于你的结果”页面，查找近期信息泄露事件中是否包含你的个人信息。“暗网报告”功能目前已经在全球&nbsp;46&nbsp;个国家和地区上线，可以检查与你的姓名、电子邮件、地址、用户名和密码相关的数据，如果在数据泄露事件中发现用户个人信息，就会第一时间通知用户。</p><p></p><h4>&nbsp;中国区员工只能用 iPhone？微软回应</h4><p></p><p>近日，有媒体报道称，微软已要求中国员工不能使用安卓手机，时间是从今年9月份开始。报道中提到，**微软已告知其中国员工，从今年9月份开始，他们只能在工作中使用iPhone，**此举实际上将安卓设备排除在了工作场所之外。</p><p></p><p>根据业内说法，该措施是微软全球“安全未来计划”（Secure&nbsp;Future&nbsp;Initiative）的一部分，将影响中国的数百名员工，旨在确保所有员工使用微软身份验证器Microsoft&nbsp;Authenticator（微软开发的一款应用）和Identity&nbsp;Pass等应用程序。</p><p></p><p>7月9日晚间，微软发言人就此事回应媒体记者称：**“Microsoft&nbsp;Authenticator和Identity&nbsp;Pass应用程序已正式在Apple&nbsp;Store（软件商店）和Google&nbsp;Play&nbsp;Store上架。我们希望为员工提供访问这些必要应用程序的途径，由于本地区无法使用Google移动服务，我们即向员工提供了例如iOS设备的选择。”**对此，不少网友称，若是能配发工作机就没问题。但若强制要求员工自行购买，则“不能接受”。</p><p></p><h4>抖音宣布推出抖音VR直播</h4><p></p><p>7月10日消息，抖音集团正式宣布推出抖音VR直播功能，用户现在可以在Apple&nbsp;Vision&nbsp;Pro设备上下载并体验这一创新功能。据了解，该软件支持180°、360°全景直播，即使不在现场，用户也能有身临其境的感觉，实时沉浸式看直播。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ee/ee5daa14e98e80bfd9ac36944b276734.png" /></p><p></p><p>**据介绍，抖音VR直播支持小范围6DoF&nbsp;3D直播，可进行180°或360°全景直播，用户可多屏切换、发送3D礼物。**Vision&nbsp;Pro是苹果首款头显设备，定位是MR（混合现实）设备。</p><p></p><p>MR技术结合AR和VR，通过加强虚拟对象与现实世界的交互，实现混合世界的全新体验。目前XR设备所带来的功能价值较为有限，内容生态成为产品竞争的关键因素。兼具软硬件生态的苹果入局MR，有望通过其市场影响力吸引顶尖内容制作者建立良好开发生态，同时依靠品牌影响力能够在更低的用户教育、触达成本下实现优质内容供给—平台破圈引流—消费者需求响应的良性生态。</p><p></p><h4>去哪儿网正式推行“3+2”混合办公模式，员工每周有2天可自主选择办公地点</h4><p></p><p>7&nbsp;月&nbsp;9&nbsp;日消息，去哪儿&nbsp;CEO&nbsp;陈刚发全员信宣布，从&nbsp;7&nbsp;月&nbsp;15&nbsp;日（下周一）起，每周三、周五，员工可以灵活选择办公地点。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fdfa591cf4dead0631f4449bc121d902.png" /></p><p></p><p>陈刚特意强调，员工按规定混合办公，“无需任何申请审批”。</p><p></p><p>据了解，混合办公的适用人员范围以入职&nbsp;6&nbsp;个月以上的标准工时正式员工为主。去年&nbsp;10&nbsp;月，去哪儿开始了为期&nbsp;9&nbsp;个月的混合办公试验。回收数据显示，员工对混合办公的各个维度反馈正面&nbsp;——&nbsp;超过九成的员工认为混合办公后幸福感有明显提升，员工主动离职率在混合办公后下降了三成。</p><p></p><p>去哪儿&nbsp;COO（首席运营官）刘连春表示，混合办公没有让公司业绩变坏，并且显著提升了员工的幸福度。那这件事情公司何乐而不为呢？他强调混合办公不会影响员工的绩效和晋升。</p><p></p><h4>马斯克叫停与甲骨文的100亿美元谈判，拟自建“超算工厂”</h4><p></p><p>当地时间7月9日，据The&nbsp;Information报道，马斯克旗下的人工智能（AI）初创公司xAI已与甲骨文终止扩大一项现有协议的谈判，这笔交易的潜在价值高达100亿美元。根据该协议，xAI将从甲骨文租用英伟达的AI芯片搭建超级计算机。</p><p></p><p>马斯克当天在X平台上回应，xAI将自行建造超级计算机，这样能保证更快速地完成，从而赶上竞争对手。目前，该公司正使用戴尔和超微电脑提供的英伟达芯片，在美国田纳西州孟菲斯建立AI数据中心。</p><p></p><p><img src="https://static001.geekbang.org/infoq/18/18523a4710256617e83c83e40520693e.png" /></p><p></p><p>此次谈判自&nbsp;5&nbsp;月份以来一直在进行，经过一个多月的商谈后依然以失败告终。据悉，交易告吹导致甲骨文股价暴跌，周二股价下跌&nbsp;3%，收于&nbsp;140.68&nbsp;美元。此次下跌结束了甲骨文连续七天的上涨势头，并引发了投资者对该公司在竞争激烈的云计算市场中能否获得并维持大规模合同的担忧。</p><p></p><p>虽然新交易失败，但甲骨文和&nbsp;xAI&nbsp;将继续在基础设施需求方面进行合作。xAI&nbsp;与甲骨文签订的在&nbsp;Oracle&nbsp;Gen2&nbsp;Cloud&nbsp;中训练&nbsp;AI&nbsp;模型的现有合同仍不受影响，这表明两家公司之间的关系并未完全断绝。</p><p></p><p>更多阅读：《<a href="https://mp.weixin.qq.com/s/mPuhRGW8AeLgntE1QLfSyw">xAI&nbsp;和&nbsp;Oracle&nbsp;间&nbsp;100&nbsp;亿美元的生意谈崩了！有钱也租不到芯片的马斯克要自建超级计算中心，就不信“钞”能力还会失效？</a>"》</p><p></p><h4>半年报扭亏！手机市场回暖&nbsp;欧菲光逐渐走出“苹果阴影”</h4><p></p><p>近日，欧菲光（002456）发布的公告引起了广泛关注。被苹果“抛弃”的欧菲光，在经过三年的业绩低迷后，终于重新进入稳定的收入状态。7月9日晚间，该公司发布的公告显示，预计今年上半年实现3600万元–4500万元的盈利，这也是欧菲光时隔三年首次实现半年报盈利。</p><p></p><p>结合此前的公告和业内人士分析内容，欧菲光盈利主要得益于华为这个大客户重新占领手机市场，以及欧菲光在智能汽车领域的增长，且该公司本身的业务结构也在向着更健康的方向调整。</p><p></p><p>值得注意的是，虽然欧菲光对于今年下半年智能手机和智能汽车市场的预期比较乐观，但该公司仍然非常依赖智能手机市场。业内人士给出建议，欧菲光应该深挖技术“护身河”，持续走多元化的路子。</p><p></p><h4>“WPS&nbsp;崩了”：三周内第二次，官方回应“服务已恢复正常”</h4><p></p><p>7&nbsp;月&nbsp;8&nbsp;日早，WPS&nbsp;崩了冲上微博热搜。这次的崩溃对于在工作中使用&nbsp;WPS&nbsp;的小伙伴影响比较严重。</p><p></p><p>不少网友反馈，自己扫码登不上&nbsp;WPS，验证码也收不到，尝试了多种办法都不行，也有网友反馈存在无法登录、云端文件无法打开，还有网友遇到了数据无法保存等场景。</p><p></p><p>针对这一情况，WPS&nbsp;客服团队迅速响应，称&nbsp;7&nbsp;月&nbsp;8&nbsp;日早上全国范围内的云服务都出现了故障，已接到了用户反馈。目前在紧急排查修复中，部分用户已恢复，其余也在逐步恢复中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/91/91d5786c52fe946f49b60445db223327.jpeg" /></p><p></p><p>值得一提的是，不久前，6月28日下午，多名用户反映WPS金山文档无法正常打开疑似应用崩溃，“WPS崩了”话题也登上了热搜。</p><p></p><p>有网友表示，之前碰到了很多次&nbsp;WPS&nbsp;崩溃的情况，打开&nbsp;WPS&nbsp;Office&nbsp;时遭遇了启动失败的错误，而且多次重启都未能成功。必须要卸载，重新安装新版本才能解决。</p><p></p><h2>IT&nbsp;业界</h2><p></p><p></p><h4>重大数据泄露事件：RockYou2024文件泄露，数百万用户信息暴露</h4><p></p><p>7月11日消息，据网络安全公司Cybernews近日披露，一个名为"RockYou2024"的文件在暗网论坛上被公开。文件中包含了9948575739条明文密码，这一数字几乎涵盖了全球网民的真实密码集合，网络安全专家认为这是有史以来最大的密码泄露事件。</p><p></p><p>Cybernews的研究人员通过泄露密码检查器的数据交叉对照后发现，这些密码来自新旧数据泄露的混合。也就是说，RockYou2024主要还是以往密码泄露事件的汇编，据估计包含了来自总计4000个巨大被盗凭证数据库的条目，时间跨度至少达二十年之久。值得注意的是，新文件中包含了早先的RockYou2021，其中含有84亿个密码，也就是说RockYou2024在2021的基础上新增约15亿个密码，时间范围从2021年至2024年。</p><p></p><p>Cybernews警告称，一旦RockYou2024与其他泄露数据库联手，比如用户的邮箱地址和其他敏感信息，那么数据泄露、金融诈骗、身份盗窃……一连串的灾难性后果将接踵而至。</p><p></p><h4>二季度&nbsp;PC&nbsp;出货量增长&nbsp;3%，中国市场继续低迷</h4><p></p><p>IDC&nbsp;公布的数据显示，二季度&nbsp;PC&nbsp;出货量比去年同期增加&nbsp;3%，在连续七个季度下滑之后连续两个季度保持了增长。</p><p></p><p><img src="https://static001.geekbang.org/infoq/28/28a1eda40eadb601e123c16d0947101d.png" /></p><p></p><p>但中国市场持续低迷阻碍了&nbsp;PC&nbsp;市场的复苏。二季度&nbsp;PC&nbsp;出货量&nbsp;6490&nbsp;万台，排除中国市场的&nbsp;PC&nbsp;出货量同比增长逾&nbsp;5%。</p><p></p><p>**IDC全球设备跟踪器集团副总裁Ryan&nbsp;Reith表示，**毫无疑问，PC市场和其他技术市场一样，由于成熟度和逆风因素，在短期内面临挑战。然而，连续两个季度的增长，加上围绕AIPC的大量市场炒作，再加上一个虽不够吸引人但可以说更重要的商用市场换机周期，似乎正是PC市场所需要的。热点显然是围绕AI的，但是，non-AIPC的购买产生的影响更大，使这个成熟的市场显示出积极的迹象。</p><p></p><p>近几个月来，大多数行业参与者都制定了AIPC的初步战略，主要关注组件方面和商用市场的潜力。尽管IDC认为，商用市场在PC行业的AI领域短期内具有最大的上升空间，但消费市场的故事尚未完全被讲述。人们都在期待苹果在今年晚些时候通过预期的产品发布来推动这一信息，但不应忽视的是，高通、英特尔和AMD都可能在消费者和商业AI&nbsp;PC领域制造声势。</p><p></p><h4>开源代码编辑器&nbsp;Zed&nbsp;发布原生&nbsp;Linux&nbsp;版本</h4><p></p><p>7月10日，Zed官方昨日发布了0.143.6版本，并正式支持Linux。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b5/b5c124710c52aa6893c39cfc4c0c1a67.png" /></p><p></p><p>据介绍，Linux&nbsp;上的&nbsp;Zed&nbsp;正在使用&nbsp;Vulkan&nbsp;API&nbsp;进行&nbsp;GPU&nbsp;加速。它同时支持&nbsp;Wayland&nbsp;和&nbsp;X11&nbsp;会话。到目前为止，Zed&nbsp;团队的开发重心主要集中在&nbsp;Ubuntu&nbsp;下的测试。</p><p></p><p>Zed&nbsp;是一款支持多人协作的代码编辑器，由&nbsp;Atom&nbsp;编辑器原作者主导，其底层采用&nbsp;Rust&nbsp;编写、默认支持&nbsp;Rust，还自带了&nbsp;rust-analyzer，主打&nbsp;“高性能”——作者表示希望将&nbsp;Zed&nbsp;打造为世界上最好的文本编辑器。</p><p></p><h4>Java&nbsp;之父&nbsp;James&nbsp;Gosling&nbsp;宣布退休</h4><p></p><p>近日，Java&nbsp;语言之父&nbsp;James&nbsp;Gosling&nbsp;在领英上发文宣布自己即将退休。他在博文中写道：“我现在终于退休了。干了这么多年软件工程师，是时候享受生活了。尽管曾经经历过疫情肆虐、业界萧条，但在亚马逊的过去七年里，我过得非常愉快。我还有很多未尽事宜要去完成，我将满怀期待开启新征程”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/09/0927617083be1d4e9ab9f0fcdac47b1d.jpeg" /></p><p></p><p>Java&nbsp;的故事始于&nbsp;1991&nbsp;年，当时&nbsp;Sun&nbsp;Microsystems&nbsp;试图将其在计算机工作站市场的领先地位扩展到新兴且发展迅速的个人电子产品市场。几乎没有人预料到&nbsp;Sun&nbsp;即将创建的编程语言会使计算大众化，激发了一个全球范围的社区，并成为了一个由语言、运行时平台、SDK、开源项目以及许多工具组成的持久软件开发生态系统的平台。</p><p></p><p>经过&nbsp;James&nbsp;Gosling&nbsp;领导团队数年秘密开发后，Sun&nbsp;于&nbsp;1995&nbsp;年发布了具有里程碑意义的“一次编写，随处运行”&nbsp;的&nbsp;Java&nbsp;平台，并将重点从最初的交互式电视系统设计转到了新兴的万维网应用程序上。在本世纪初，Java&nbsp;就已经开始为从智能卡到太空飞行器的一切制作动画了。如今，数以百万计的开发人员在使用&nbsp;Java&nbsp;编程，它至今仍然是工业界最受欢迎和使用最多的语言。</p><p></p><h4>微软宣布90天内将结束Win11部分版本服务</h4><p></p><p>7&nbsp;月&nbsp;9&nbsp;日，微软&nbsp;Windows&nbsp;11&nbsp;即将迎来其三岁生日，这意味着初始版本&nbsp;21H2&nbsp;和第一个功能更新版本&nbsp;22H2&nbsp;将很快将失去支持，参考微软近日发布的通知，微软警告这两个版本仅剩&nbsp;90&nbsp;天生命周期支持，之后&nbsp;Windows&nbsp;11&nbsp;21H2&nbsp;和&nbsp;22H2&nbsp;无法再获得安全更新。</p><p></p><p><img src="https://static001.geekbang.org/infoq/25/255e6ea92204e463a8c09fb030950741.png" /></p><p></p><p>2024&nbsp;年&nbsp;6&nbsp;月&nbsp;Windows&nbsp;各版本占有率&nbsp;&nbsp;来源：statcounter</p><p></p><p>事实上，目前Windows&nbsp;11&nbsp;21H2已经不再支持普通消费者，唯一仍在更新的版本是企业版、教育版和物联网企业版。</p><p></p><p>微软于2021年10月5日发布了&nbsp;Windows&nbsp;11，这是在&nbsp;Windows&nbsp;10&nbsp;推出大约六年后发布的。Windows&nbsp;11&nbsp;的第一个版本是&nbsp;21H2，该版本在2023年10月对普通用户结束支持。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/TYf58GAHpWIX5XFMBcP3</id>
            <title>国产芯片大厂三年干到 70 亿市值，却一次性裁员 50 %？员工曝 CTO 不懂技术！</title>
            <link>https://www.infoq.cn/article/TYf58GAHpWIX5XFMBcP3</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/TYf58GAHpWIX5XFMBcP3</guid>
            <pubDate></pubDate>
            <updated>Mon, 15 Jul 2024 06:45:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 芯华章, 裁员, EDA, 市场困境
<br>
<br>
总结: 中国国产EDA大厂芯华章开始大规模裁员，引发关注。裁员比例高达50%，公司否认裁员比例，称为谣言。裁员引发了对公司战略收缩和质疑的猜测。芯华章曾是估值70亿的独角兽，但裁员已进行两波。管理层和并购被指责，显示国产EDA厂商面临严峻市场困境。 </div>
                        <hr>
                    
                    <p>整理&nbsp;|&nbsp;华卫</p><p>&nbsp;</p><p>7月8日，某职场社交平台有网友爆料称，国产EDA大厂芯华章开始大规模裁员，裁员比例高达50%，并且第一批裁员已经谈话完毕。另有认证为“芯华章科技股份有限公司员工”的网友还补充称，“不止50%，软件部裁员接近60%。留下来的人更加惶恐。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/0c/0cbb1de6bc2e8f2a221f71f89cde5459.jpeg" /></p><p></p><p>&nbsp;</p><p>也有不少人关心芯华章此次裁员的赔偿方案，“百分之五十的裁员比例，赔偿总和可不是小数目。”</p><p>&nbsp;</p><p>消息传出后，有芯华章内部人士向媒体表示，“公司确实有在战略收缩，但是人员优化比例有限，裁员50%的说法是谣言。如果真像传闻那样一下子裁员50%，那公司根本就没法正常运转了。”</p><p>&nbsp;</p><p>但依然有不少对于芯华章此次战略收缩的深层猜测，其中有两方面的解释：一是公司业务方面，“管理层决策失误，技术路线步子迈大了，客户也没搞定”；二是市值套现的质疑，“如今财务造假严打，IPO收紧，就原形毕露了”。</p><p>&nbsp;</p><p>注：EDA全称Electronic&nbsp;Design&nbsp;Automation，意为电子设计自动化，是用于辅助完成超大规模集成电路芯片设计、制造、封装、测试整个流程的计算机软件，完整的集成电路设计和制造流程均需要对应的EDA工具作为支撑，因而有“芯片之母”的称号。</p><p>&nbsp;</p><p></p><h1>三年估值70亿，裁员已进行两波</h1><p></p><p>芯华章由前新思科技中国区副总经理王礼宾于2020年3月创立，当时正值国内积极倡导国产EDA工具发展，希望打破国外厂商垄断局面。</p><p>&nbsp;</p><p>刚成立一年不久，芯华章便已完成5轮融资，累计融资金额超12亿元。到成立三年时，芯华章已完成8轮投资，每轮融资均数亿元。在去年&nbsp;3&nbsp;月获得中信科&nbsp;5G&nbsp;基金的战略投资后，芯华章晋升为估值&nbsp;70&nbsp;亿元的独角兽。</p><p>&nbsp;</p><p>据了解，芯华章主要聚焦芯片EDA数字验证领域，打造从芯片到系统的验证解决方案，提供完整的验证EDA工具链服务。2021年，芯华章率先发布支持国产服务器架构的数字仿真器穹鼎GalaxSim，去年7月又推出新一代高速仿真器GalaxSim&nbsp;Turbo。</p><p>&nbsp;</p><p>关于这次的裁员，据一位认证信息为“芯华章科技股份有限公司员工”的网友介绍，他是芯华章GalaxSim部门的成员，其一再表示，“不知道还能苟多久，下次估计就是整个项目组了，毕竟没剩多少人了。”</p><p>&nbsp;</p><p>值得注意的是，就在今年4月，芯华章宣布其核心EDA软件产品已完成华为鲲鹏平台的移植工作。基于鲲鹏处理器等国产架构，芯华章逻辑仿真器GalaxSim、形式化验证工具GalaxFV，都能有效利用鲲鹏的高性能集群去提高编译与运算，显著提高了系统级芯片仿真验证效率。其中，GalaxSim在多个客户测试用例上已经取得了2-3倍的仿真性能提升，大幅降低了仿真回归测试的时间。</p><p>&nbsp;</p><p>然而，此次的“战略收缩”已不是芯华章第一次进行裁员。有知情人士透露，“第一波是去年12月，有人被裁了至今没找到满意的工作，这次人数来的更猛烈了，想谈个好价格就更难了…”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/da/da1a0b256294d25e94bf3d11fa1d19fd.jpeg" /></p><p></p><p>&nbsp;</p><p></p><h1>“CTO不懂技术”，并购成转折点？</h1><p></p><p>&nbsp;</p><p>“从头到尾都是资本运作的公司，一个工程师注册个公司2年市值60亿，为的就是IPO套现。如今财务造假严打，IPO收紧，就原形毕露了。”对于芯华章此次的大规模裁员消息，一位国金证券的投资理财顾问发表了这样的看法。</p><p>&nbsp;</p><p>另一位认证为“芯华章科技股份有限公司员工”的网友则把矛头指向了芯华章的管理层，“CTO酒量很好，人也仗义，可就是不太懂技术。”“CTO不懂技术这还是第一次听。”一位认证为“新思科技员工”的网友评价道。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/ca76c051463eedc67f8d14f7b272d8d1.jpeg" /></p><p></p><p>&nbsp;</p><p>2022年9月，芯华章收购高性能仿真软件企业“瞬曜电子”，并进行核心技术整合，并购金额没有披露，同时任瞬曜电子创始人傅勇为公司首席技术官（CTO）。</p><p>&nbsp;</p><p>公开资料显示，傅勇曾担任新思科技资深技术总监，主管亚太地区数字验证产品事业部的技术战略与客户支持。2021年，傅勇在离开新思科技中国一年后，创立了瞬曜电子，专注于数字芯片的前端验证领域，还发布了瞬系列RTL高速仿真器（ShunSim）。更早之前，傅勇毕业于清华大学电子工程系，获得了学士和硕士学位，毕业后任职于三星电子、楷登电子（Cadence）和新思科技，在EDA行业工作达25年。</p><p>&nbsp;</p><p>据了解，当时芯华章是对瞬曜电子的知识产权、产品等核心资产进行的收购。并购是EDA企业扩张的常见手段，借助技术与资本的双重力量，在扩宽产品系列的同时还消除了潜在竞争对手。</p><p>&nbsp;</p><p>不少国外的EDA巨头都通过大量并购优秀EDA点工具厂商实现了EDA全流程覆盖，Cadence通过收购Verilog、Silicon&nbsp;Perspective，解决芯片性能验证问题，将1C布局工具和S1分析工具收入囊；而Synopsys在收购Avanti后，成为了EDA史上首家可以提供顶级前后端完整1C设计方案的EDA工具商。</p><p>&nbsp;</p><p>但芯华章对瞬曜电子进行收购后，走向却似乎有所不同。有业内人士这样评价，“花了好大代价并购个寂寞，回头来看，转折点no1。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/a4/a499c83171ff5c4f92a1b01f6bd8fb85.jpeg" /></p><p></p><p>&nbsp;</p><p></p><h1>国产EDA厂商的严峻市场困境</h1><p></p><p>&nbsp;</p><p>去年9月18日，在国内EDA开放合作创新组织举办的首届IDAS设计自动化产业峰会“数字逻辑设计与验证领域”专题分论坛上，华为海思半导体平台验证部部长傅晓对芯华章的演讲者抛出了一系列问题：“芯华章目前生产了多少机框？实际上有多少机框被客户采用？又有多少FPGA被成功导入？”</p><p>&nbsp;</p><p>当时，傅晓强调，学术圈或者企业圈沟通有个基本原则，就叫实事求是，中国要把EDA干成，不能吹，不吹才能成事。引发关注和热议后，傅晓在朋友圈内向芯华章致歉，并对芯华章表明了认可。</p><p>&nbsp;</p><p>但这一风波，也侧面反映出国产EDA厂商的发展困境。EDA市场规模有限，头部的Cadence、新思科技、Siemens&nbsp;EDA等厂商经过多年发展，市场地位稳固。相比之下，国产EDA厂商所面临的竞争环境惨烈。</p><p>&nbsp;</p><p>根据赛迪智库统计，2020&nbsp;年国际三大&nbsp;EDA&nbsp;巨头新思科技、铿腾电子和西门子&nbsp;EDA&nbsp;在国内市场占据明显的头部优势，合计占领约&nbsp;80%的市场份额；国产&nbsp;EDA&nbsp;厂商华大九天市占率约&nbsp;6%，处于国内市场第四位。</p><p>&nbsp;</p><p>中国半导体行业协会预测，到2025年中国的EDA市场规模将达到184.9亿元人民币（约合25亿美元），届时将占全球EDA市场的18.1%。但考虑到该市场的大部分份额仍被国外EDA厂商所占据，留给国产EDA厂商的市场空间依然十分有限。国内头部EDA&nbsp;厂商华大九天4月发布的财报显示，其2023年全年营收也只有10.1亿元人民币。</p><p>&nbsp;</p><p>因而，对于还未上市的国产EDA企业来说，未来可能会遭遇更为严峻的融资环境与显著提升的上市门槛，“战略收缩”或是会其应对市场的必要出路之一了。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://maimai.cn/n/content/global-topic?circle_type=9&amp;topic_id=F8W8dHiR">https://maimai.cn/n/content/global-topic?circle_type=9&amp;topic_id=F8W8dHiR</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/YzYmsaitrTJZ1ysPfalk</id>
            <title>TaD+RAG- 缓解大模型“幻觉”的组合新疗法</title>
            <link>https://www.infoq.cn/article/YzYmsaitrTJZ1ysPfalk</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/YzYmsaitrTJZ1ysPfalk</guid>
            <pubDate></pubDate>
            <updated>Mon, 15 Jul 2024 06:18:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: TaD, RAG, LLM, 幻觉
<br>
<br>
总结: 本文介绍了京东联合清华大学提出的任务感知解码技术（TaD）和业内解决LLM幻觉问题的最有效系统性方案——检索增强生成技术（RAG）。大语言模型（LLM）在人类对话互动方面表现出色，但幻觉问题成为其落地应用的制约和瓶颈。幻觉问题主要来源于数据、训练和推理过程，针对这些问题提出了一些缓解策略，其中RAG通过引入信息检索过程，增强LLM的生成过程，提高准确性和鲁棒性，降低幻觉。 </div>
                        <hr>
                    
                    <p></p><p></p><p>TaD：任务感知解码技术（Task-aware Decoding，简称 TaD），京东联合清华大学针对大语言模型幻觉问题提出的一项技术，成果收录于 IJCAI2024。</p><p>RAG：检索增强生成技术（Retrieval-augmented Generation，简称 RAG），是业内解决 LLM 幻觉问题最有效的系统性方案。</p><p></p><h1>1. 背景介绍</h1><p></p><p></p><p>近来，以 ChatGPT 为代表的生成式大语言模型（Large Language Model，简称 LLM）掀起了新一轮 AI 热潮，并迅速席卷了整个社会的方方面面。得益于前所未有的模型规模、训练数据，以及引入人类反馈的训练新范式，LLM 在一定程度上具备对人类意图的理解和甄别能力，可实现生动逼真的类人对话互动，其回答的准确率、逻辑性、流畅度都已经无限接近人类水平。此外，LLM 还出现了神奇的“智能涌现”现象，其产生的强大的逻辑推理、智能规划等能力，已逐步应用到智能助理、辅助创作、科研启发等领域。京东在诸多核心业务如 AI 搜索、智能客服、智能导购、创意声称、推荐/广告、风控等场景下，均对 LLM 的落地应用进行了深入探索。这一举措提升了业务效率，增强了用户体验。</p><p></p><p>尽管具备惊艳的类人对话能力，大语言模型的另外一面——不准确性，却逐渐成为其大规模落地的制约和瓶颈。通俗地讲，LLM 生成不准确、误导性或无意义的信息被称为“幻觉”，也就是常说的“胡说八道”。当然也有学者，比如 OpenAI 的 CEO Sam Altman，将 LLM 产生的“幻觉”视为“非凡的创造力”。但是在大多数场景下，模型提供正确回答的能力至关重要，因此幻觉常常被认为是一种缺陷；尤其是在一些对输出内容准确性要求较高的场景下，比如医疗诊断、法律咨询、工业制造、售后客服等，幻觉问题导致的后果往往是灾难性的。</p><p></p><p>本文主要探索针对 LLM 幻觉问题的解决方案。</p><p></p><h1>2. 相关调研</h1><p></p><p></p><p>众所周知，大语言模型的本质依然是语言模型（Language Model，简称 LM），该模型可通过计算句子概率建模自然语言概率分布。具体而言，LM 基于统计对大量语料进行分析，按顺序预测下一个特定字/词的概率。LLM 的主要功能是根据输入文本生成连贯且上下文恰当的回复，即生成与人类语言和写作的模式结构极为一致的文本。注意到，LLM 并不擅长真正理解或传递事实信息。故而其幻觉不可彻底消除。亚利桑那州立大学教授 Subbarao Kambhampati 认为：LLM 所生成的全都是幻觉，只是有时幻觉碰巧和你的现实一致而已。新加坡国立大学计算学院的 Ziwei Xu 和 Sanjay Jain 等也认为 LLM 的幻觉无法完全消除[1]。</p><p></p><p>虽然幻觉问题无法彻底消除，但依然可以进行优化和缓解，业内也有不少相关的探索。有研究[2]总结了 LLM 产生幻觉的三大来源：数据、训练和推理，并给出了对应的缓解策略。</p><p></p><h4>2.1 数据引入的幻觉</h4><p></p><p></p><p>“病从口入”，训练数据是 LLM 的粮食，数据缺陷是使其致幻的一大原因。数据缺陷既包括数据错误、缺失、片面、过期等，也包括由于领域数据不足所导致的模型所捕获的事实知识利用率较低等问题。以下是针对训练数据类幻觉的一些技术方案：</p><p></p><p>数据清洗</p><p></p><p>针对数据相关的幻觉，最直接的方法就是收集更多高质量的事实数据，并进行数据清理。训练数据量越大、质量越高，最终训练得到的 LLM 出现幻觉的可能性就可能越小[3]。但是，训练数据总有一定的覆盖范围和时间边界，不可避免地形成知识边界，单纯从训练数据角度解决幻觉问题，并不是一个高性价比的方案。</p><p>针对“知识边界”问题，有两种主流方案：一种是知识编辑，即直接编辑模型参数弥合知识鸿沟。另一种是检索增强生成（Retrieval-augmented Generation，简称 RAG），保持模型参数不变，引入第三方独立的知识库。</p><p></p><p>知识编辑</p><p></p><p>知识编辑有两种方法：1）编辑模型参数的方法可以细粒度地调整模型的效果，但难以实现知识间泛化能力，且不合理的模型编辑可能会导致模型产生有害或不适当的输出[4]；2）外部干预的方法（不编辑模型参数）对大模型通用能力影响较小，但需要引入一个单独的模块，且需要额外的资源训练这个模块。</p><p>如何保持原始 LLM 能力不受影响的前提下，实现知识的有效更新，是 LLM 研究中的重要挑战[2]。鉴于知识编辑技术会给用户带来潜在风险，无论学术界还是业界都建议使用包含明确知识的方法，比如 RAG。</p><p></p><p>检索增强生成（RAG）</p><p></p><p>RAG 引入信息检索过程，通过第三方数据库中检索相关信息来增强 LLM 的生成过程，从而提高准确性和鲁棒性，降低幻觉。由于接入外部实时动态数据，RAG 在理论上没有知识边界的限制，且无需频繁进行 LLM 的训练，故已经成为 LLM 行业落地最佳实践方案。下图 1 为 RAG 的一个标准实现方案[11]，用户的 Query 首先会经由信息检索模块处理并召回相关文档；随后 RAG 方法将 Prompt、用户 query 和召回文档一起输入 LLM，最终由 LLM 生成最终的答案。</p><p></p><p>图 1. RAG 架构图</p><p><img src="https://static001.geekbang.org/infoq/ea/ea7b73f20dc4c921d356287164b7f45a.png" /></p><p>​</p><p>﻿﻿</p><p>RAG 借助信息检索，引入第三方事实知识，大大缓解了单纯依靠 LLM 生成答案而产生的幻觉，但由 LLM 生成的最终输出仍然有较大概率产生幻觉。因此，缓解 LLM 本身的幻觉，对整个 RAG 意义重大。</p><p></p><h4>2.2 模型训练引入的幻觉</h4><p></p><p></p><p>LLM 的整个训练过程，都可能会引入幻觉。首先，LLM 通常是 transformer 结构的单向语言模型，通过自回归的方式建模目标，天然存在单向表示不足、注意力缺陷[6]、曝光偏差[7]等问题；其次，在文本对齐阶段，无论是监督微调（SFT）还是人类反馈的强化学习（RLHF），都有可能出现有标注数据超出 LLM 知识边界、或者与 LLM 内在知识不一致的问题；这一系列对齐问题很可能放大 LLM 本身的幻觉风险[8]。</p><p></p><p>对于训练过程引入的幻觉，可以通过优化模型结构、注意力机制、训练目标、改进偏好模型等一系列手段进行缓解。但这些技术都缺乏通用性，难以在现有的 LLM 上进行迁移，实用性不高。</p><p></p><h4>2.3 推理过程引入的幻觉</h4><p></p><p></p><p>推理过程引入的幻觉，一方面源自于解码策略的抽样随机性，它与幻觉风险的增加呈正相关，尤其是采样温度升高导致低频 token 被采样的概率提升，进一步加剧了幻觉风险[9]。另一方面，注意力缺陷如上下文注意力不足、Softmax 瓶颈导致的不完美解码都会引入幻觉风险。</p><p></p><p>层对比解码（DoLa）</p><p></p><p>针对推理过程解码策略存在的缺陷，一项具有代表性且较为有效的解决方案是层对比解码（Decoding by Contrasting Layers, 简称 DoLa）[9]。模型可解释性研究发现，在基于 Transformer 的语言模型中，下层 transformer 编码“低级”信息（词性、语法），而上层中包含更加“高级”的信息（事实知识）[10]。DoLa 主要通过强调较上层中的知识相对于下层中的知识的“进步”，减少语言模型的幻觉。具体地，DoLa 通过计算上层与下层之间的 logits 差，获得输出下一个词的概率。这种对比解码方法可放大 LLM 中的事实知识，从而减少幻觉。</p><p></p><p>图 2. DoLa 示意图</p><p><img src="https://static001.geekbang.org/infoq/3c/3cbec3d739fb3141d66fed54dd85980f.png" /></p><p>​</p><p></p><p>上图 2 是 DoLa 的一个简单直观的示例。“Seattle”在所有层上都保持着很高的概率，可能仅仅因为它是一个从语法角度上讲比较合理的答案。当上层通过层对比解码注入更多的事实知识后，正确答案“Olympia”的概率会增加。可见，层对比解码（DoLa）技术可以揭示真正的答案，更好地解码出 LLM 中的事实知识，而无需检索外部知识或进行额外微调。此外，DoLa 还有动态层选择策略，保证最上层和中间层的输出差别尽可能大。</p><p></p><p>可见，DoLa 的核心思想是淡化下层语言/语法知识，尽可能放大事实性知识，但这可能导致生成内容存在语法问题；在实验中还发现 DoLa 会倾向于生成重复的句子，尤其是长上下文推理场景。此外，DoLa 不适用有监督微调，限制了 LLM 的微调优化 。</p><p></p><h1>3. 技术突破</h1><p></p><p></p><p>通过以上分析，RAG 无疑是治疗 LLM 幻觉的一副妙方，它如同 LLM 的一个强大的外挂，让其在处理事实性问题时如虎添翼。但 RAG 的最终输出仍然由 LLM 生成，缓解 LLM 本身的幻觉也极为重要，而目前业内针对 LLM 本身幻觉的技术方案存在成本高、实用落地难、易引入潜在风险等问题。</p><p></p><p>鉴于此，京东零售联合清华大学进行相关探索，提出任务感知解码（Task-aware Decoding，简称 TaD）技术[12]（成果收录于 IJCAI2024），可即插即用地应用到任何 LLM 上，通过对比有监督微调前后的输出，缓解 LLM 本身的幻觉。该方法通用性强，在多种不同 LLM 结构、微调方法、下游任务和数据集上均有效，具有广泛的适用场景。</p><p></p><p>任务感知解码（TaD）技术</p><p></p><p>关于 LLM 知识获取机制的一些研究表明，LLM 的输出并不能总是准确反映它们所拥有的知识，即使一个模型输出错误，它仍然可能拥有正确的知识[13]。此项工作主要探索 LLM 在保留预训练学到的公共知识的同时，如何更好地利用微调过程中习得的下游任务特定领域知识，进而提升其在具体任务中的效果，缓解 LLM 幻觉。</p><p></p><p>TaD 的基本原理如图 3 所示。微调前 LLM 和微调后 LLM 的输出词均为“engage”，但深入探究不难发现其相应的预测概率分布发生了明显的改变，这反映了 LLM 在微调期间试图将其固有知识尽可能地适应下游任务的特定领域知识。具体而言，经过微调，更加符合用户输入要求（“专业的”）的词“catalyze”的预测概率明显增加，而更通用的反映预训练过程习得的知识却不能更好满足下游任务用户需求的词“engage”的预测概率有所降低。TaD 巧妙利用微调后 LLM 与微调前 LLM 的输出概率分布的差异来构建知识向量，得到更贴切的输出词“catalyze”，进而增强 LLM 的输出质量，使其更符合下游任务偏好，改善幻觉。</p><p></p><p>图 3. TaD 原理图</p><p><img src="https://static001.geekbang.org/infoq/af/af2930c906a97052acb628e6d5017497.png" /></p><p></p><p>知识向量</p><p></p><p>为了直观理解 LLM 在微调阶段学习到的特定领域知识，我们引入知识向量的概念，具体如图 4 所示。微调前 LLM 的输出条件概率分布为 pθ，微调后 LLM 的输出条件概率分布为 pϕ。知识向量反应了微调前后 LLM 输出词的条件概率分布变化，也代表着 LLM 的能力从公共知识到下游特定领域知识的适应。基于 TaD 技术构建的知识向量可强化 LLM 微调过程中习得的领域特定知识，进一步改善 LLM 幻觉。</p><p></p><p>图 4. 知识向量</p><p><img src="https://static001.geekbang.org/infoq/c2/c237f091d02fda038009d5b19c39d4ae.png" /></p><p>﻿﻿</p><p>特别地，当微调数据较少时，LLM 的输出条件概率分布远远达不到最终训练目标。在此情形下，TaD 技术增强后的知识向量可以加强知识对下游任务的适应，在训练数据稀缺场景下带来更显著的效果提升。</p><p></p><p>实验结果</p><p></p><p>1）针对不同的 LLM，采用 LoRA、AdapterP 等方式、在不同的任务上进行微调，实验结果如下表 1 和表 2 所示。注意到，TaD 技术均取得了明显的正向效果提升。</p><p></p><p>表 1. Multiple Choices 和 CBQA 任务结果</p><p><img src="https://static001.geekbang.org/infoq/72/7204c1175cee2590cce2caa1f45905f3.png" /></p><p></p><p>表 2. 更具挑战性的推理任务结果</p><p><img src="https://static001.geekbang.org/infoq/d0/d04a0fc99171af777eae622bac9fa3ba.png" /></p><p>​﻿﻿</p><p>2）相比较其他对比解码技术，TaD 技术在绝大部分场景下效果占优，具体如表 3 所示。需要特别强调的一点是，其他技术可能会导致 LLM 效果下降，TaD 未表现上述风险。</p><p></p><p>表 3. 不同对比解码技术结果</p><p><img src="https://static001.geekbang.org/infoq/95/95281634c1b6ffd33e4095ac45d095d8.png" /></p><p>​﻿﻿</p><p>3）针对不同比例的训练样本进行实验，发现一个非常有趣的结果：训练样本越少，TaD 技术带来的收益越大，具体如表 4 所示。因此，即使在有限的训练数据下，TaD 技术也可以将 LLM 引导到正确的方向。由此可见，TaD 技术能够在一定程度上突破训练数据有限情形下 LLM 的效果限制。</p><p></p><p>表 4. 不同数据比例下的结果</p><p><img src="https://static001.geekbang.org/infoq/43/43edb0aa6a8d4c66f408d9e7fec712b9.png" /></p><p>​﻿﻿</p><p>可见，TaD 可以即插即用，适用于不同 LLM、不同微调方法、不同下游任务，突破了训练数据有限的瓶颈，是一项实用且易用的改善 LLM 自身幻觉的技术。</p><p></p><h1>4. 落地案例</h1><p></p><p></p><p>自从以 ChatGPT 为代表的 LLM 诞生之后，针对其应用的探索一直如火如荼，然而其幻觉已然成为限制落地的最大缺陷。综上分析，目前检索增强生成（RAG）+低幻觉的 LLM 是缓解 LLM 幻觉的最佳组合疗法。在京东通用知识问答系统的构建中，我们通过 TaD 技术实现低幻觉的 LLM，系统层面基于 RAG 注入自有事实性知识，具体方案如图 5 所示，最大程度缓解了 LLM 的生成幻觉 。</p><p></p><p>图 5. TaD+RAG 的知识问答系统</p><p><img src="https://static001.geekbang.org/infoq/ef/ef35c2a47c39ab1461fa6248279813fe.png" /></p><p>​</p><p>目前知识问答系统已经接入京东 6000+业务场景，为用户提供准确、高效、便捷的知识性问答，大大节省了运营、运维等人力开销。</p><p></p><h1>5. 思考与展望</h1><p></p><p></p><p>如果 LLM 依然按照语言模型的模式发展，那么其幻觉就无法彻底消除。目前业内还没有一种超脱语言模型范畴，且可以高效完成自然语言相关的任务新的模型结构。因此，缓解 LLM 的生成幻觉，仍然是未来一段时期的探索路径。以下是我们在系统、知识、LLM 三个层面的一些简单的思考，希望能够抛砖引玉。</p><p></p><p>系统层面——RAG+Agent+More 的复杂系统</p><p></p><p>RAG 技术确实在一些常见的自然语言处理任务中发挥出色的作用，尤其是针对简单问题和小型文档集。但是遇到一些复杂的问题和大型文档集时，RAG 技术就显得力不从心。近期有一些研究认为 RAG+Agent 才是未来的趋势[14]，Agent 能够辅助理解并规划复杂的任务。我们认为可能未来的系统可能不仅仅局限于 Agent 和 RAG，可能还要需要多种多样的内外工具调用、长短期记忆模块、自我学习模块......</p><p></p><p>知识层面——与 LLM 深度融合的注入方式</p><p></p><p>任何一个深度模型都会存在知识边界的问题，LLM 也不例外。RAG 通过检索的方式召回外部知识，以 Prompt 的形式送入 LLM 进行最终的理解和生成，一定程度上缓解 LLM 知识边界问题。但是这种知识注入的方式和 LLM 生成的过程是相对割裂的。即便已经召回了正确的知识，LLM 也可能因为本身知识边界问题生成错误的回答。因此探索如何实现外部知识和 LLM 推理的深度融合，或许是未来的一个重要的课题。</p><p></p><p>LLM 层面——低幻觉 LLM</p><p></p><p>LLM 本身的幻觉是问题的根本和瓶颈，我们认为随着 LLM 更广泛的应用，类似 TaD 可缓解 LLM 本身幻觉的探索一定会成为业内的更大的研究热点。</p><p></p><h1>6. 结语</h1><p></p><p></p><p>缓解 LLM 幻觉一定是个复杂的系统问题，我们可以综合不同的技术方案、从多个层级协同去降低 LLM 的幻觉。虽然现有方案无法保证从根本上解决幻觉，但随着不断探索，我们坚信业内终将找到限制 LLM 幻觉的更有效的方案，也期待届时 LLM 相关应用的再次爆发式增长。</p><p></p><p>京东零售一直走在 AI 技术探索的前沿，随着公司在 AI 领域的不断投入和持续深耕，我们相信京东必将产出更多先进实用的技术成果，为行业乃至整个社会带来深远持久的影响。</p><p>﻿</p><p>【参考文献】</p><p>[1] Hallucination is Inevitable: An Innate Limitation of Large Language Models</p><p>[2] A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions</p><p>[3] Unveiling the Causes of LLM Hallucination and Overcoming LLM Hallucination</p><p>[4] Editing Large Language Models: Problems, Methods, and Opportunities</p><p>[5] ACL 2023 Tutorial: Retrieval-based Language Models and Applications</p><p>[6] Theoretical Limitations of Self-Attention in Neural Sequence Models</p><p>[7] Sequence level training with recurrent neural networks.</p><p>[8] Discovering language model behaviors with model-written evaluations</p><p>[9] Dola: Decoding by contrasting layers improves factuality in large language models</p><p>[10] Bert rediscovers the classical nlp pipeline</p><p>[11] Retrieval-Augmented Generation for Large Language Models: A Survey</p><p>[12] TaD: A Plug-and-Play Task-Aware Decoding Method toBetter Adapt LLM on Downstream Tasks</p><p>[13] Inference-time intervention: Eliciting truthful answers from a language model</p><p>[14] Beyond RAG: Building Advanced Context-Augmented LLM Applications</p><p>​</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/kUAMTF00sAdNhsvulr7H</id>
            <title>首个专为半导体行业设计的开源大模型 SemiKong 问世</title>
            <link>https://www.infoq.cn/article/kUAMTF00sAdNhsvulr7H</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/kUAMTF00sAdNhsvulr7H</guid>
            <pubDate></pubDate>
            <updated>Mon, 15 Jul 2024 01:37:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Aitomatic, SemiKong, AI Alliance, 半导体行业
<br>
<br>
总结: Aitomatic推出了SemiKong，这是世界上第一个专为半导体行业设计的开源AI大型语言模型。SemiKong旨在解决半导体行业的挑战，利用领域知识和专业知识进行训练，有望降低半导体生产成本，推动行业创新。通过AI联盟的支持，SemiKong有望重新定义半导体制造业，为行业带来巨大的飞跃。 </div>
                        <hr>
                    
                    <p></p><p>7 月 10 日，国外初创公司 Aitomatic 宣布推出 SemiKong。这是世界上第一个专为半导体行业设计的开源 AI 大型语言模型（LLM）。它旨在通过将特定领域的知识纳入模型来解决半导体行业面临的一些挑战，例如有关半导体器件和工艺的物理和化学问题。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/bc/bc89c3afa90c31f5c1ef0a8aee82dcc4.png" /></p><p></p><p>SemiKong 由人工智能联盟（AI Alliance）成员合作研发。AI 联盟成立于 2023 年，致力于构建、支持和倡导整个 AI 技术领域的开放式创新，包括软件、数据和模型、安全、安保和信任、工具、评估、硬件、教育、开放科学和宣传。</p><p></p><p>SemiKong 基于联盟成员 Meta 开源的 Llama3 模型，利用了包括 Tokyo Electron 在内的领先半导体公司和 FPT Software 等 AI 专家的专业知识。IBM 研究院 AI 开放创新负责人 Anthony Annunziata 强调，“SemiKong DRAFT v0.6 的诞生表明，汇集不同的专业知识能推动半导体制造等关键行业的重大进步。”</p><p></p><p>SemiKong 的训练过程主要分为 3 个主要阶段：预训练领域知识——自我微调（指令数据集）——合并和量化。从放出的代码权重，可以看出 SemiKong 有 8B 的参数。它在准确性、相关性和对半导体工艺的理解方面表现出了显著的进步。</p><p></p><p>Aitomatic 表示，即使是其较小版本，在特定领域的应用中也常常超越较大的通用模型，从而有可能加速整个半导体价值链的创新并降低成本。并且，它也为那些打造适合自身的专有模型的芯片公司提供了一个有价值的基座。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/16/16ef131c1717c78c97feb525ac103847.png" /></p><p></p><p>随着 SemiKong 降低半导体生产成本，消费者可以在未来几年内以更低的价格看到功能更强大的智能手机、笔记本电脑和智能家居设备。SemiKong 于 2024 年 7 月 9 日起在 HuggingFace 和 GitHub 上提供下载。下一个更强大的版本计划于 2024 年 12 月推出，预计 2024 年 9 月将推出首批特定工艺型号。</p><p></p><p>开源地址：<a href="https://github.com/aitomatic/semikong">https://github.com/aitomatic/semikong</a>"</p><p></p><p>SemiKong 项目的领导者， Aitomatic 首席执行官 Christopher Nguyen 表示：“SemiKong 将重新定义半导体制造业。这种开放式创新模式由人工智能联盟提供支持，利用集体专业知识应对行业特定挑战。在 Aitomatic，我们正在使用 SemiKong 创建领域特定 AI 智能体，以前所未有的效率解决复杂的制造问题。”</p><p></p><p>Tokyo Electron 高级专家、半导体行业模型的早期提出者 Daisuke Oku 补充道：“SemiKong 是半导体开源 AI 的一个令人激动的开始。Aitomatic 的创新方法有可能为我们的行业带来巨大的飞跃。”</p><p></p><p>参考链接：</p><p></p><p><a href="https://www.prnewswire.com/news-releases/aitomatic-unveils-semikong-ai-breakthrough-set-to-reshape-500b-semiconductor-industry-302194215.html">https://www.prnewswire.com/news-releases/aitomatic-unveils-semikong-ai-breakthrough-set-to-reshape-500b-semiconductor-industry-302194215.html</a>"</p><p></p><p><a href="https://www.semikong.ai/">https://www.semikong.ai/</a>"</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/tJtBpzK2zf8KepMuoYkL</id>
            <title>“萝卜快跑”被曝是真人在屏幕前操作；阿里云宣布与月之暗面“联姻”；去哪儿宣布每周两天自选办公地 ｜AI 周报</title>
            <link>https://www.infoq.cn/article/tJtBpzK2zf8KepMuoYkL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/tJtBpzK2zf8KepMuoYkL</guid>
            <pubDate></pubDate>
            <updated>Mon, 15 Jul 2024 01:25:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 萝卜快跑, 无人驾驶, 安全问题, 人为干预
<br>
<br>
总结: 百度旗下自动驾驶出行服务平台“萝卜快跑”在全国多个城市开展无人自动驾驶服务，订单量激增，但面临安全与技术等问题，甚至有人为干预的传闻。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>网友称萝卜快跑其实有驾驶员人为干预；去哪儿员工每周两天可自主选择办公地点；阿里云宣布与月之暗面“联姻”；OpenAI 绝密项目「草莓」首次曝光；AMD 收购欧洲最大私人 AI 实验室 Silo AI。</blockquote><p></p><p></p><p></p><h2>行业热点</h2><p></p><p></p><p></p><h5>萝卜快跑订单疯涨，无人驾驶时代真的来了？</h5><p></p><p></p><p>7 月 10 日，百度旗下自动驾驶出行服务平台“萝卜快跑”登上微博热搜榜首。据了解，萝卜快跑已在全国 11 个城市开放载人测试运营服务，在武汉、重庆、深圳、上海、北京等城市开展全无人自动驾驶出行服务与测试。</p><p></p><p>媒体报道称，随着百台无人车的投入运营，“萝卜快跑”在武汉市全无人订单量也迎来了爆发式增长，单日单车峰值超 20 单。数据显示，萝卜快跑 APP 用户满意度评价达 4.9 分，其中 5 分满分好评占比高达 94.19%。此外，网络传言称，萝卜快跑已在武汉投放 1000 辆无人车，进而引发对网约车司机、出租车司机就业市场的深刻担忧。</p><p></p><p>目前，萝卜快跑还面临着安全与技术等方面的问题。有武汉网友 7 月 7 日下午在短视频平台发布视频，称百度旗下的自动驾驶出行服务平台“萝卜快跑”无人驾驶出租车在武汉街头与行人相撞，视频中可以看到一个行人躺在出租车前，交警正在现场，事故造成了部分车辆拥堵。这件事故引出了一个亟待解决的问题：无人驾驶车应该如何定责。目前，我国还没有建立一套完善的全国性法规，只有地方规定。此外，在长江网武汉市民留言板上，有关于“萝卜快跑”的留言，目前已累计达到 324 条。留言板多个内容显示，萝卜快跑 Robotaxi 在道路上运营期间，出现车辆在绿灯状态下停滞不前、红灯时冲入路口中央、转弯时卡顿不动等情况，并引发交通拥堵现象，对市民出行造成了一定影响。</p><p></p><p>针对此类现象，武汉经开区管委会回应称：“确认涉诉车辆为自动驾驶测试车，正在调试中。开发区交通大队将定期与公司负责人沟通和反映问题，确保道路安全。”另外，近日还有网友在社交平台称，无人驾驶的萝卜快跑其实有驾驶员人为干预。网传图片显示，在萝卜快跑汽车机器人智控中心，有真人坐在带方向盘的屏幕前操作。百度方面截至发稿没有回应。</p><p></p><p>据网信永川公众号 2023 年 7 月发布的内容，位于永川区大数据产业园的百度无人驾驶实验基地内，有云代驾安全员在 5G 云代驾舱进行远程实时控制，通过高带宽、低时延的 5G 网络，从屏幕组上观察汽车周围 360°状况，并利用方向盘、档把、脚踏板等控制器驾驶无人车辆。5G 云代驾的意义在于，在无人车没有安全员的情况下，当无人车出现解决不了的问题时，云端安全员可以帮助其远程脱困。</p><p></p><p></p><h5>三星爆发大规模罢工，韩媒：半导体部门员工是罢工主力</h5><p></p><p></p><p>据报道，韩国三星电子旗下最大工会“全国三星电子工会”于 8 日上午开始在京畿道华城市三星电子华城工厂正门前举行罢工，计划持续 3 天。该工会会员总数为 3 万人，约占三星电子员工总数（12.5 万人）的 24%。据悉，在 8 日的罢工中，工会推算有 4000 至 5000 人参与，三星公司和警方则估计有 3000 人参加。</p><p></p><p>韩媒称，这是三星电子成立 55 年来首次爆发大规模罢工。此前在 6 月初，工会部分成员曾利用休年假的形式罢工 1 天。工会此次提出的主要诉求有：全体工会成员薪酬上调、改变奖金标准、公司履行带薪休假承诺，以及对因罢工导致的工资损失进行补偿等。工会主席在接受采访时还表示，公司不透明的奖金计算方式，导致员工对自身利益的不确定性增加；若公司在 10 日前未拿出解决方案，工会将于 15 日起进行第二阶段的罢工。</p><p></p><p>韩国 SBS 电视台称，半导体部门的员工是此次罢工的主力。三星公司称半导体生产线的运行没有受到重大影响，但《东亚日报》报道称，即使许多生产线实现了自动化，操作这些生产线的重要人员也很难替换。半导体生产线一旦停止运转，恢复生产需要耗费大量时间和成本。</p><p></p><p></p><h5>腾讯全员邮件宣布调薪：员工月工资增加 3200 元等</h5><p></p><p></p><p>7 月 10 日，腾讯内部向全员发布邮件称，将调整内部的薪酬福利政策，对薪酬结构做出调整。</p><p></p><p>校招生的房补从每月 4000 元调整为按 15 个月发放，并将其纳入月薪基数中。调整后，员工每月基本工资增加 3200 元，多出来的三个月将在年终奖一起发放。根据资料，腾讯公司给校招生提供的房补标准为每月 4000 元（北上广深地区为 2000 元），三年共计 14.4 万元。员工服务奖（13 薪）从年底发放调整为平摊到 12 个月，并加入月薪基数中。腾讯邮件中称，这两个举措旨在帮助大家在更高、更稳定的月收入基础上更安心地安排工作与生活。相关调整于 2024 年 7 月 1 日起生效，8 月 5 日的发薪中开始体现。</p><p></p><p></p><h5>大模型人才紧缺，字节跳动加速争夺全球高校顶尖技术人才</h5><p></p><p></p><p>近日，字节跳动“筋斗云人才计划”启动。该计划是字节跳动面向优秀校园技术人才推出的专项招聘，意图在全球范围内，吸引和招募有志于用技术创造突破性价值的顶尖学生。</p><p></p><p>据悉，本次招聘涵盖 AI 应用、搜索、推荐、广告、AI for Science、AI Safety、机器人、隐私与安全、硬件、视频架构、工程架构等技术领域。招聘的目标群体是 2024 年 9 月 -2025 年 8 月毕业的博士群体，重点针对有亮眼学术成果、拥有顶会顶刊论文或专利的学术达人；有丰富的大赛经历，在国际知名竞赛中取得优异成绩的竞赛达人；或有极强的实践能力，参与过重大项目，擅长解决难题的实战达人。</p><p></p><p>在大模型相关技术人才招聘上，字节跳动是国内互联网大厂最积极的公司。据一位大模型行业人士透露，今年字节跳动的 AI 人才招聘规模最大。脉脉高聘人才智库数据印证了这一信息：今年上半年，字节跳动位列新发人工智能岗位最多的企业。从招聘指数上看，字节跳动以 9.53 位居第一，大幅领先于小红书（7.96）、蚂蚁集团（5.84）、美团（4.86）、腾讯（2.48）等互联网大厂。</p><p></p><p></p><h5>去哪儿员工每周三、周五可自主选择办公地点</h5><p></p><p></p><p>7 月 9 日，去哪儿 CEO 陈刚发全员信宣布，从 7 月 15 日起，每周三、周五，员工可以灵活选择办公地点。陈刚在信中强调，员工按规定混合办公，“无需任何申请审批”。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/09/090608dcb8e70b99b8a1aedee53a7e7f.jpeg" /></p><p></p><p>据了解，混合办公的适用人员范围以入职 6 个月以上的标准工时正式员工为主。</p><p></p><p>去年 10 月，去哪儿开始了为期 9 个月的混合办公试验。回收数据显示，员工对混合办公的各个维度反馈正面 —— 超过九成的员工认为混合办公后幸福感有明显提升，员工主动离职率在混合办公后下降了三成。</p><p></p><p>去哪儿 COO（首席运营官）刘连春表示，“混合办公没有让公司业绩变坏，并且显著提升了员工的幸福度。那这件事情公司何乐而不为呢？”</p><p></p><p></p><h5>微软要求中国区员工必须使用 iPhone？微软回应</h5><p></p><p></p><p>7 月 9 日消息，据媒体报道，微软中国员工已被告知，登录公司系统时必须使用 iPhone 进行身份验证。从 9 月起，将禁止使用 Android 智能手机作为多因素身份验证设备。</p><p></p><p>据介绍，此举属于微软全球安全未来计划的一部分，将影响中国大陆的数百名员工，旨在确保所有员工都使用微软 Authenticator 密码管理器和 Identity Pass 身份验证应用。另有消息显示，由于部分中国安卓设备不支持谷歌服务，微软将为受影响员工提供 iPhone15 作为工作手机。</p><p></p><p>一位微软发言人回应表示：「Microsoft Authenticator 和 Identity Pass 应用程序已正式在 Apple Store 和 Google Play Store 上架。我们希望为员工提供访问这些必要应用程序的途径，由于本地区无法使用 Google 移动服务，我们即向员工提供了例如 iOS 设备的选择。」</p><p></p><p>对此，不少网友称，若是能配发工作机就没问题。但若强制要求员工自行购买，则“不能接受”。</p><p></p><p></p><h5>阿里云宣布与月之暗面“联姻”：帮 Kimi 技术突破</h5><p></p><p></p><p>7 月 8 日，阿里云官宣两位新“代言人”——月之暗面科技有限公司创始人杨植麟和智联招聘集团总裁张月佳。</p><p></p><p>这是月之暗面首次公开与阿里云的合作情况。信息显示，阿里云的算力和大模型服务平台，助力月之暗面提升模型推理效率，加速 Kimi 智能助手实现技术突破。此外，智联招聘集团的大模型应用，也基于阿里云实现快速部署和上线支持。</p><p></p><p></p><h5>AMD 重砸 6.65 亿美元收购欧洲最大私人 AI 实验室 Silo AI</h5><p></p><p></p><p>AMD 宣布以价值约 6.65 亿美元的全先进交易价值收购欧洲最大的私人 AI 实验室 Silo AI。该收购案预计在 2024 年下半年完成。</p><p></p><p>收购完成后，Silo AI 首席执行官兼联合创始人 Peter Sarlin 将继续领导 Silo AI 团队，向 AMD 高级副总裁 Vamsi Boppana 汇报工作。</p><p></p><p>据了解，Silo AI 总部位于芬兰赫尔辛基，业务遍及欧洲和北美，专注于端到端 AI 驱动解决方案，帮助客户快速轻松地将 AI 集成到其产品、服务和运营中。他们的工作涉及不同的市场，客户包括安联、飞利浦、劳斯莱斯和联合利华。除了 SiloGen 模型平台外，Silo AI 还在 AMD 平台上创建了最先进的开源多语言 LLM，例如 Poro 和 Viking。</p><p></p><p>AMD 在新闻稿中表示，此次收购代表该公司基于开放标准并与全球 AI 生态系统建立强有力的合作伙伴关系，并提供端到端 AI 解决方案的战略又迈出了重要一步。Silo AI 团队由世界一流的 AI 科学家和工程师组成，他们拥有丰富的经验，为云、嵌入式和终端计算市场的领先企业开发量身定制的 AI 模型、平台和解决方案。</p><p></p><p></p><h5>多方监管增压，微软放弃参与 OpenAI 董事会</h5><p></p><p></p><p>7 月 10 日，据媒体报道，随着欧美监管机构加强对人工智能市场的反垄断审查，微软公司决定放弃在美国开放人工智能研究中心 (OpenAI) 董事会中的观察员席位。</p><p></p><p>微软 9 日致函 OpenAI 说明上述决定，并解释称，OpenAI 自去年发生董事会人事震荡以来，经营管理已有改善，因此不再需要微软参与。微软选择放弃观察员席位，决定“立即生效”。</p><p></p><p>据报道，去年 OpenAI 首席执行官萨姆·奥尔特曼“离职又复职”风波过后，微软在 OpenAI 董事会获任无投票权观察员。据此前报道，微软支持并短暂聘用过奥尔特曼。</p><p></p><p></p><h5>小红书被曝获 DST 投资，估值 170 亿美元</h5><p></p><p></p><p>7 月 11 日消息，小红书获得了风险投资公司 DST Global 的支持。三位知情人士透露，小红书在最近几周进行了股份出售，公司估值达到 170 亿美元。</p><p></p><p>DST Global 曾投资过 Facebook，并与红杉中国一起参与了小红书这一轮投资，红杉中国增加了其现有股份。此外高瓴资本、博裕资本和中信资本也进行了跟投。</p><p></p><p>此前有消息称，小红书在 2023 年首次实现盈利。据四位知情人士透露，小红书去年净利润达 5 亿美元，营收达 37 亿美元。</p><p></p><p></p><h2>大模型一周大事</h2><p></p><p></p><p></p><h4>大模型发布</h4><p></p><p></p><p></p><h5>OpenAI 绝密项目「草莓」首次曝光，内部人士曾称其可能威胁人类</h5><p></p><p></p><p>7 月 13 日，据外媒报道，OpenAI 内部正在一个代号为「草莓（Strawberry）」的项目中开发一种新的人工智能模型。该项目的细节此前从未被报道过，而 OpenAI 正努力证明其提供的各类模型能够提供高级推理能力。</p><p></p><p>当被问及上述所说的草莓技术时，OpenAI 的发言人在一份声明中表示：“我们希望自身 AI 模型能够像我们（人类）一样看待和理解世界。持续研究新的 AI 能力是业界的常见做法，大家都相信这些系统的推理能力会随着时间的推移而提高。”</p><p></p><p>尽管发言人并未直接回应有关“草莓”项目的问题，但媒体报道指出，该项目之前被称为 Q*，而 Q*正是去年导致 OpenAI CEO 被意外解雇的重要导火索。</p><p></p><p>OpenAI 的内部人士曾向董事会发出警告，称 Q* 的重大发现可能对全人类构成威胁。</p><p></p><p>媒体推测，Q* 可能具备 GPT-4 所缺乏的基础数学能力，这可能意味着它具有与人类智能相媲美的推理能力。而这可能标志着 OpenAI 在实现其 AGI 目标方面迈出了重要一步。</p><p></p><p>蚂蚁集团开源 EchoMimic：支持为人像照片对口型、生成肖像动画视频</p><p></p><p>近日，蚂蚁集团推出了一项开源项目——EchoMimic，这款 AI 工具能够根据声音内容，为照片中的人物创建逼真的口型同步动画。</p><p></p><p>EchoMimic 具备较高的稳定性和自然度，通过融合音频和面部标志点（面部关键特征和结构，通常位于眼、鼻、嘴等位置）的特征，可生成更符合真实面部运动和表情变化的视频。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f9/f920b20ecea13d28d4a7d6483eec24e1.png" /></p><p></p><p>EchoMimic 的技术核心在于其能够精确捕捉音频信号和面部特征之间的关联，并以此为基础生成动画。在训练过程中，EchoMimic 采用了先进的数据融合技术，确保了音频和面部特征的有效整合，从而提高了动画的稳定性和自然度。</p><p></p><p>经过与多个公共数据集和自收集数据集中的替代算法进行的全面比较，EchoMimic 在定量和定性评估方面均展现出卓越的性能。这一点在 EchoMimic 项目页面上的可视化效果中得到了充分体现。</p><p></p><p></p><h5>腾讯开源 web 端地图组件库 tlbs-map</h5><p></p><p></p><p>7 月 11 日，腾讯开源了其基于腾讯位置服务 JavaScript API 封装的地图组件库 —— tlbs-map，用于在网页上绘制地图，并在地图上绘制点、线、面、热力图等效果。它支持 Vue2、Vue3、React 等主流技术栈，可以帮助开发者降低地图开发的成本。</p><p></p><p>据官方介绍，tlbs-map 封装腾讯地图 API 为响应式组件，无需关心复杂的地图 API，只需要操作数据即可；同时，组件提供地图和图层实例，用户可编写自定义组件或直接调用地图 API 满足定制化需求。</p><p></p><p>为了方便开发者使用，tlbs-map 还提供了详尽的组件使用文档和示例代码，可以帮助开发者轻松上手，快速开发。</p><p></p><p></p><h5>智谱 AI 开源推出视频理解模型 CogVLM2-Video</h5><p></p><p></p><p>7 月 12 日，智谱 AI 提出了一种基于视觉模型的自动时间定位数据构建方法，生成了 3 万条与时间相关的视频问答数据。基于这个新数据集和现有的开放领域问答数据，引入了多帧视频图像和时间戳作为编码器输入，训练了一种新的视频理解模型—CogVLM2-Video。</p><p></p><p>智谱 AI 表示，目前视频理解的主流方法使模型失去了时间感知能力，无法准确地将视频帧与精确的时间戳关联起来。因此，模型缺乏时间定位、时间戳检测和总结关键时刻的能力。为了解决这些问题，团队提出了 CogVLM2-Video，这是基于 CogVLM2 图像理解模型的扩展视频模型。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/fd/fd0403bf707372f114caf13973410f00.jpeg" /></p><p></p><p>该模型不仅在开放域问答中实现了先进的性能，还能感知视频中的时间戳信息，从而实现时间定位和相关问答。</p><p></p><p>具体来说，这种方法就是从输入视频片段中提取帧，并为其注释时间戳信息，使后续的语言模型能够准确知道每一帧在原视频中对应的确切时间。</p><p></p><p></p><h5>几分钟生成四维内容，还能控制运动效果：北大、密歇根提出 DG4D</h5><p></p><p></p><p>近期，商汤科技 - 南洋理工大学联合 AI 研究中心 S-Lab ，上海人工智能实验室，北京大学与密歇根大学联合提出 DreamGaussian4D（DG4D），通过结合空间变换的显式建模与静态 3D Gaussian Splatting（GS）技术实现高效四维内容生成。</p><p></p><p>据悉，四维内容生成近来取得了显著进展，但是现有方法存在优化时间长、运动控制能力差、细节质量低等问题。DG4D 提出了一个包含两个主要模块的整体框架：1）图像到 4D GS ；团队使用 DreamGaussianHD 生成静态 3D GS，接着基于 HexPlane 生成基于高斯形变的动态生成；2）视频到视频纹理细化 ；团队通过细化生成 UV 空间纹理映射，并通过使用预训练的图像到视频扩散模型增强其时间一致性。</p><p></p><p>值得注意的是，DG4D 将四维内容生成的优化时间从几小时缩短到几分钟，允许视觉上控制生成的三维运动，并支持生成可以在三维引擎中真实渲染的动画网格模型。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8f/8f66fd24fd90c1c7d1201310546cfa29.gif" /></p><p></p><p></p><h4>企业应用</h4><p></p><p></p><p>7 月 10 日，谷歌宣布将于本月底向所有谷歌账号用户开放「暗网报告」功能，旨在帮助用户快速了解网络上发生的个人数据泄露事件，并提供相关漏洞信息的搜索服务。7 月 10 日，阿里推出专为科研人员、高校教师和学生、职场人士研发的大模型应用产品心流，其产品定位为用户的 AI 搜索助手，提供智能搜索、知识问答、智能阅读、辅助创作等能力。7 月 10 日，夸克宣布升级“超级搜索框”，推出以 AI 搜索为中心的一站式 AI 服务，为用户提供从检索、创作、总结，到编辑、存储、分享的一体化信息服务价值。7 月 11 日，三星表示将在今年推出基于自己人工智能（AI）模型的升级版语音助手 Bixby，据悉，这次 Bixby 的升级是三星在其设备套件上推广人工智能功能的一部分。7 月 12 日，粉笔发布了基于首个专注于职教行业的垂域大模型 AI 产品——粉笔 AI 老师 “粉笔头”，旨在让 AI 帮助老师化身“高效能人士”，向学员提供更有针对性的服务。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/IpgZP8CO4h8EwlI710fv</id>
            <title>大模型产品琳琅满目，企业应该如何选择？</title>
            <link>https://www.infoq.cn/article/IpgZP8CO4h8EwlI710fv</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IpgZP8CO4h8EwlI710fv</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jul 2024 08:54:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI, 大模型, 企业, C 端
<br>
<br>
总结: AI 和大模型在企业应用中的重要性日益凸显，企业需要考虑如何有效融入大模型到业务中，尤其在面向C端应用时更应注重用户体验。与此同时，企业在选择大模型时需要深入理解应用场景，并逐步推广。在大模型的商业化和投资方面，企业需要考虑未来的可替代性，以做出合理的选择。 </div>
                        <hr>
                    
                    <p></p><p>AI 和大模型方兴未艾，我们每天都在看到和尝试不同版本、不同品牌的大模型产品，它们的能力各不相同。无论是个人还是企业，都在思考如何尽早地参与进来到大模型的浪潮当中来。</p><p></p><p>目前，一些先锋企业已经将 AI 和大模型融入到他们的日常业务和产品中，并取得了不错的效果。但更多企业仍处于观望或迷茫状态。在有限的预算内，企业要怎样进行 AI 和大模型的商业化或投资？该选择怎样的大模型融入业务？带着这些问题，InfoQ《极客有约》特别邀请了广东 CIO 联盟会长、前海尔集团 CIO 李洋老师和北京中关村科金公司 CTO 李智伟老师，与 InfoQ 社区编辑张凯峰一同探讨企业如何在众多大产业和大模型产品中做出合理的选择。对话内容部分亮点如下：</p><p></p><p>● 企业开发大模型应用时，应该更多地考虑用户体验；</p><p></p><p>● 企业需要对应用场景有深入理解，并从试点开始逐步推广；</p><p></p><p>● 对于面向 C 端的应用，“+AI”是个不错的选择；</p><p></p><p>● 企业在进行系统建设时，必须考虑到未来的可替代性。</p><p></p><p>以下为访谈实录，经编辑。完整视频参看：</p><p></p><p><a href="https://www.infoq.cn/video/0KjL5Et9SFyJ6Yrryyqf">https://www.infoq.cn/video/0KjL5Et9SFyJ6Yrryyqf</a>"</p><p></p><p></p><blockquote>在 8 月 18-19 日即将举办的 AICon 上海站，我们设置了「大模型数据集构建及评测技术落地」专题，本专题将深入探讨大模型的需求分析与数据收集、数据清洗与增强、模型评测与优化，以及技术落地与维护等关键方向。目前大会 9 折购票优惠中，感兴趣的同学请锁定大会官网：<a href="https://aicon.infoq.cn/2024/shanghai/track">https://aicon.infoq.cn/2024/shanghai/track</a>"</blockquote><p></p><p></p><p></p><h2>To C 与 To B 场景和市场需求差异</h2><p></p><p></p><p>张凯峰：在 To C 和 To B 场景中，使用 AI 和大模型有什么区别？或者当企业负责人考虑将 AI 引入自己的企业时，通常会考虑哪些方面？</p><p></p><p>李智伟 ：通过查看最近 ChatGPT 一周的数据，可以发现其用户活跃数量超过了一亿。虽然这一数字很高，但大部分用户使用的都是 To C 的一些应用程序。国内的一些应用程序用户数量也很多，从用户教育角度来看，进展比较快。C 端应用程序的发展将会非常迅速。目前广泛使用的 agent 平台或 model builder 平台，都面向企业提供服务，提供公共云服务或者私有化服务。国内大部分公有云上的托管服务都由个人或小微企业进行使用。国内比较好的头部企业，其训练的模型约有 1.3 万个。</p><p></p><p>同时，通过查看今年 1 到 6 月份的公开招标网数据，我们可以看到大企业招标的情况。今年上半年到 6 月中旬，整个公开招标数量约为 234 件。其中，60% 以上的项目来自央国企。预计今年大企业客户对大模型的商业化使用将加速。</p><p></p><p>从 C 端来看，很多客户已经理解了大部分内容，但 B 端的进展仍然处于早期阶段。</p><p></p><p>李洋：从目前的趋势来看，人工智能是一个新质生产力，是工业革命的一部分。从国际上来看，人工智能的浪潮也比以往席卷得更快。其原因在于，它是由 C 端发起的。目前可以感知到的是，要把科技的生产力提高民众的感知度。对于企业来说，可能包括员工、客户以及上下游。C 端这种蜂拥而来的趋势就奠定了这次人工智能浪潮会高于前几次的基础。</p><p></p><p>对于 To B，我认为应该是未来科技革命所产生的生产力要兑现的一个非常重要的路径。目前可以看到的很多一二级市场的投资，对 C 端的投资还在逐渐增长。但如果缺乏一定的杀手级应用，甚至没有持续的宣传和科技元素的不断注入，这种热度很快就会退去。</p><p></p><p>对于 OpenAI 或国内许多做大模型的企业，无论是自研还是开源，要在 C 端实现商业变现都很难。因此，在 C 端巩固之后，随着大模型的成熟，To B 的发展应该会逐渐加速，但未来是否一定会发展成大模型还有待观察。</p><p></p><p>张凯峰：可见，C 端市场和 B 端市场所面临的情况、消费习惯以及背后的经济投入都是完全不同的。对于企业来说，AI 和大模型的应用还处于初级阶段。是否是因为满足 C 端服务更容易，但企业侧复杂的业务需求和市场竞争等因素，导致企业在接纳 AI 和大模型方面比 C 端更困难？这将具体表现在哪些方面呢？</p><p></p><p>李智伟：我认为这个问题与大模型的能力有关。大语言模型的技术能力可能更多地体现在知识的理解和生成方面更加成熟，因此在构建 C 端应用的场景中，它的融入是比较快速的。</p><p></p><p>对于知识的获取，大模型本身也采用了推荐的方式来提供服务。这种方式的技术更加成熟，能够更好地与用户交互。许多类似的 APP 或个人助手都提供了知识获取的功能。Perplexity AI 和国内的一些创新者正在开发类似搜索的应用，并且致力于提升 C 端用户的体验。当它面向 B 端企业渗透时，我们需要考虑其商业化能力。目前，商业能力主要集中于互联网、教育、金融和政企服务等行业。这些行业有一个共同点，即服务于大量 C 端用户。</p><p></p><p>面向个人用户的体验一定会延伸到企业端的员工使用中。在当下这一波浪潮中，当企业开发大模型应用时，应该更多地考虑用户体验。这也是我们一直致力于做的事情，因为我们是一家传统的营销和客户服务类产品公司，我们基本上都在开发交互类产品。</p><p></p><p>我们之所以非常重视大模型技术，是因为我们认为这项技术实际上是对整个交互体验的颠覆性升级，这是一个未来的巨大机会市场。因此，我们基本上也是将 C 端的体验产品能力应用于 B 端，以实现更深入的发展。</p><p></p><p>李洋：总的来说，C 端和 B 端的触点不太一样。以我个人为例，作为一个纯粹的 C 端用户，我对新科技产品的需求更多关注于体验感、科技感以及方便易用等方面。这也包括了一些家居、生活和工作的便利性，这些方面可以归为 B2C 类别。而从 B 端的角度来看，我们可以将大模型的应用或 AI 战略应用于整个企业中。特别是在后疫情时代，我们需要降低成本、提高效率，甚至创新商业模式，寻找新的利润增长点。因此，在企业内部，对于大模型或新技术的使用，其想法、构建和步骤可能会有所不同。</p><p></p><p></p><h2>大模型的选择与匹配</h2><p></p><p></p><p>张凯峰：通常我们会采用哪些方法来帮助企业识别在某些业务或方向上可以开始选择大模型，并与自己的业务需求相匹配。您们是否听说过一些成功或有待改进的例子，以及它们是如何操作的？</p><p></p><p>李智伟：我想先分享一下我们在做企业应用时遇到的问题。在过去一年多的时间里，很多企业的决策者都在问我们如何选择。实际上，我们并没有给出明确的答案。我认为这是一个逐步认知和迭代的过程，与企业构建的业务场景密切相关。</p><p></p><p>但是这次的情况有所不同。在传统 IT 系统中，我们通常以功能性为主导，根据客户需求构建系统。但是现在，由于整个 IT 企业架构的变化，核心变成了一个模型，我们需要将之与客户的业务深度融合，这无疑是一个很大的挑战。目前我们面临的冲突点是，很多企业出于自身发展或国家要求，会积极与厂商合作，很多场景都会进入。但是因为我们不熟悉所有场景，用户有时会受到限制。</p><p></p><p>企业需要做几个部分的工作。如果我们将这看作是一个流程，我可以提供一些具体的建议。例如，通常我们会先梳理功能性需求，但现在做法不同了。我会告诉客户，首先要进行认知对齐。如何让不同的人对大模型的认知保持一致呢？首先，参与项目的人员需要具备大模型的基本原理和能力；其次，客户方也需要有懂得大模型应用的专业人士。</p><p></p><p>其次，我们需要选择一些小的场景作为试点，以便快速响应市场变化。对于供应商来说，他们需要拥有敏捷的工具链和 demo 系统来帮助他们进行试点。去年，我们建立了一个工具链平台，并在官网上开通了线上 demo 系统。客户可以在了解系统之前先进行试用，确保能够接受它的外观和效果。试点是一个双方共创的过程，试点结束后，需要进行效果评估，然后双方再对产品进行规划并分段实施，最后总结反馈。大模型应用更注重端到端效果的优化。企业需要对应用场景有深入理解，并从试点开始逐步推广。从这个角度看，市场上可供选择的选项并不多，对于企业来说，虽然我们正在努力加速商业化，但更合理的是要看到其中的节奏。</p><p></p><p>李洋：在企业中使用 AI 和大模型的切入点比较多。建议企业在做这方面时，先确认需求，再定义相应的工具。在我的数字化转型工作中，我把它分为延产供销服务、运营风控等方面。不同行业的侧重点会有所不同。</p><p></p><p>当然，不仅限于 AI，还有区块链、云计算、大数据等技术，它们与我们所说的业务数字化层面和流程有关，以及我们所说的痛点或难点，哪些可以使用大模型或 AI 来解决？从目前的情况来看，我非常同意李总的观点。现在的问题可能是，由于大模型的火爆程度超出了某些企业的承受范围，导致我们的应用目标本末倒置。我们不应该拿着锤子去找钉子，而是应该根据钉子的特点选择合适的工具，包括大模型。</p><p></p><p>今天我还与一些企业进行了交流，他们认为，传统的机器学习、简单的规则和深度学习的神经网络也可以解决问题，不一定非要使用高量级的大模型，特别是那些对算力和数据要求很高、成本也很高的模型。因此，我认为我们应该从业务数字化和智能化的方向出发，进行全面规划，然后逐一比较，看看哪些问题可以使用人工智能工具来解决。在使用人工智能工具时也必须考虑成本、效率和效益等核心指标。</p><p></p><p>李智伟：对于这个话题，市场上有不同的提法，如“+AI”或“AI+”。“+AI”是指在现有能力的基础上添加相应的能力，而“AI+”则指下一代能力系统。我认为，对于面向 C 端的应用，加入 AI 是个不错的选择。对于 B 端应用而言，企业通常需要考虑如何重新利用现有的 IT 资产，使用 AI 原生技术的成本很高。</p><p></p><p>对于初创公司来说，重新构建企业架构的成本是难以承受的。因此，我们更倾向于鼓励企业采用 AI 技术来增强自身能力，这也是一种很好的 IT 演进思路。</p><p></p><p>张凯峰：除了自己训练大模型，还有一种可能就是用一个相对成熟的模型来训练一个自己垂直领域的小模型，供企业内部使用。这是企业在做大模型时需要考虑的选择方向之一。还有其他的方向吗？</p><p></p><p>李洋 ：现在大模型的应用模式一般分为以下几种：</p><p></p><p>● 提示词工程：使用大模型不需要重新训练或者构建数据集，但由于大模型自身的泛化能力和通用能力，企业可以通过提示词来进行引导，从而使得模型生成解决方案、文案等。</p><p></p><p>● RAG，可作为大模型的补充。作为外挂，在检索或提问过程中可以将数据融合到模型中，并生成相应结果。</p><p></p><p>● 微调，或称精调：企业可在确保质量过关的情况下，使用小部分数据，挑选出自己的模型，并将专业知识和私有数据融入其中。</p><p></p><p>● 预训练：如金融行业中的一个不普遍的领域，为了训练这种行业大模型，企业需要将购买或开源的大模型中的数据重新进行训练，使其获得具有金融行业或其他专业领域知识的行业模型。基本上可以参考以上几种方式来使用大模型。在封装开源模型的过程中，可能需要采用一些综合应用的方法。如在前期使用一些提示工程，在后期添加微调。</p><p></p><p>李智伟：对于企业来说，是否需要大模型，以及大模型的数量多少问题，需要看具体场景。在书写公文或者分析金融报告时，可以使用一个模型，无论是 prompt 还是 FT。而对于更加专业的领域而言，可能需要使用 FT，并为每个任务提供精标数据进行训练。对于整个企业来说，必须采用多模型。在小模型时代，我们在构建基于模型的软件和系统架构时，就已经采用了多模型组合的方式。而现在，更加明确的是，从扩展架构的角度来看，应该采用大模型、小模型和 RAG 的组合方式。</p><p></p><p>企业不能只依赖一个模型。现在的模型架构是大模型负责调度编排，小模型负责完成特定任务，任务完成后，我们需要把所有输出汇总并呈现给最终用户使用。目前来看，RAG 增强技术也不需要模型了，大部分只需要做 prompt。小模型的获取方式有两种：利用原有 IT 资产中的小模型，即资产再利用；另一种是在基于大模型训练后，通过剪枝和蒸馏等技术将神经网络缩小，得到小模型。去年，很多企业都认为一个模型可以解决所有问题，甚至花费数千万购买大模型。但现在人们已经转换了思想。例如我们在去年一直为一家零售客户制作电销大模型，由于我们公司在过去十几年中一直从事客服工作，所以我们使用小模型实现智能拨打电话，而大模型出现后，我们认为大模型对我们的业务更加有利，进行了替换。</p><p></p><p>大模型的应答效果和对话效果都比小模型要好，但是在效率方面存在问题。当时我们的做法是加大量的 GPU，提高并发性。但是现在看来性价比极低。从整个 IT 构建来看，只能先解决准确率问题。但是需要考虑到，长此以往，性价比是支撑不住的。我们需要考虑小型化的问题，比如通过模型的裁剪或者蒸馏来实现小型化。我们甚至需要将原本使用的小模型加入到中间过程中。例如在完成某项特殊任务时，小模型效果比较好，可以使用小模型来尽可能地减少大模型进行交互判断处理的工作量。随着时间的推移，我们的整体成本和算力需求正在逐步下降。</p><p></p><p>相比之前使用纯小模型，目前我们整体的外呼发起率可以提升到 30%，大屏通话也能增长 50%，这都是大模型带来的好处。此外，使用大模型与使用传统的人工呼叫相比，也有利于降低客户不满意度。</p><p></p><p>张凯峰：刚刚提到的模型更换问题，可以再展开一下吗？比如，在什么情况下企业需要为当前的投资考虑未来替换的可能性，以及在替换之前需要做好哪些准备工作？</p><p></p><p>李智伟：关于这个话题，实际上受到两个因素的影响。</p><p></p><p>第一个方面是，早前在大模型 GPT3.5 发布那时候，一些开源模型也随之出现，但我们当时使用下来发现这些开源模型其实效果达不到预期。为了达到我们想要的效果，还是需要重新做 SFT，当时的想法是所有的模型都是需要做训练的。但是随着基础模型的发展，特别是 ChatGPT 4o 的发布，现在国内的开源模型已经可以满足基础的需求。在过去的半年中，市场上训模型这件事其实慢慢变少了，基础模型可以直接用于一些常规场景，甚至进行信息的获取和整理。</p><p></p><p>第二个方面，目前国内虽然有约 200 家大模型和各种模型的生产公司，但都没有成熟的商业模式。市场上既存在开源模型，也有闭源模型。大多数大公司都倾向于闭源，但实际上这些做闭源的公司都希望通过消耗更多的云算力来盈利，这比销售单个模型更加有利可图。但这也带来了一个问题，如果一个企业的供应商消失了，其发动机怎样进行维修呢？</p><p></p><p>因此，企业在进行系统建设时，必须考虑到未来的可替代性。这意味着企业必须要在前期就考虑这个问题。在建构整个架构时，需要考虑到两个方面：</p><p></p><p>首先，我们需要一套供应链，以便在未来能够更换其他供应商。</p><p></p><p>还需要重点考虑一个问题。例如，去年我们开始训练大模型，并且为企业提供了许多输出。但是年初开始，当我们意识到这个问题后，就投入了大量的研发精力来开发模型部署工具链。它可以帮助企业监控和运营多个模型，甚至可以轻松地替换、上线和下线部署托管。一套标准化的能力体系可以对未来产生最大价值。这可以保证基础模型趋向标准化。企业可以在任何时间选择最适合自己的资产，还可以使用更加强大的模型来替换现有最低版本。</p><p></p><p>第二种情况是，企业针对自己的行业知识进行训练，这要求企业具备快速部署和完成的能力。此外，企业未来的混合部署需要一定的工具链支持。</p><p></p><p>张凯峰：在考虑采购国内的大模型时，无论是大厂的模型还是自行预训练的模型，哪些非功能性方向是我们需要仔细考虑的呢？</p><p></p><p>李智伟：除了要考虑性价比之外，企业还需要考虑准确性、鲁棒性和稳定性等因素。在选择模型时，企业需要考虑其应用场景。例如，在线系统需要更高的时效性，因此可能需要混合性模型部署。这是第一个。</p><p></p><p>第二个是，由于模型的泛化可能会带来负面影响，比如幻觉问题，因此在严肃场景之下，必须进行针对性数据增强训练。然而，增强训练会带来一个问题，随着模型参数的增加，其准确性和严肃性会提高，但效率也会降低。因此，需要采取一些措施，例如进行裁剪或蒸馏，以提高性能。但是在非严肃场景下，需要的是模型的泛化性，对性能指标的要求会降低。</p><p></p><p>近期我还非常关注模型的安全和合规，目前大模型在安全合规方面还有待提升。企业需要考虑到个人隐私保护问题。有多少企业的原始数据经过了严格的隐私清洗和认证呢？</p><p></p><p>另一个是多模态大模型问题。目前，多模态对数据的使用更加深入。此前，大语言模型更多关注到的是文本类理解使用。但涉及到多模态，就要考虑对于视觉、音频和视频的理解和使用，在这个过程中，数据安全是极其重要的。需要企业完成两个任务：第一，采购大模型时，需要考虑其合规性和安全性；第二，大模型使用必须经过备案，并接受审查。</p><p></p><p>李洋：人工智能领域，特别是现在的大模型和未来的发展方向，可能会像云计算一样。随着大数据等新兴技术的发展，网络安全合规方面也会有相应的审查标准。例如，如果大模型的服务提供商类似于我们的公有云提供商，作为租户，调用大模型时，租户与平台之间的责任共担和举证是非常重要的。此外，在选择大模型时，还需要考虑运营团队的能力。</p><p></p><p>大模型需要具备底座和二次开发及优化能力，但建立大团队不太现实，因此需要依赖服务商提供的人工智能，包括架构师和科学家的能力。人工智能的发展具有不确定性，可能会出现幻觉、误导或暴力等后果。我们需要考虑到的是，如何训练、采购和使用大模型，以及如何对其进行完善实施和调整。平台提供方仍在不断改进中，因此需要一个团队的支持。</p><p></p><p></p><h2>企业盈利新机遇</h2><p></p><p></p><p>张凯峰：除了模型制造商之外，那些从事大模型应用的企业，他们的盈利方向和模式可能会在哪里呢？可以结合自己的经验和故事来分享一下。</p><p></p><p>李智伟：在 B2B 领域，现在还处于早期阶段，各家还没有实现盈利的商业模式。</p><p></p><p>这次技术革命将带来许多新机会。我们以前在做内容审核时，会使用小模型。例如，做内容审核需要积累大量数据并训练专家。如果想审核某个涉政类的内容，就需要储备很长时间。但是，如果使用多模态大模型，就可以快速进入审核市场。这是对以前技术的一种弯道超车的机会。</p><p></p><p>在 B2B 领域盈利，对于企业来说，是一个非常多元化的机遇。但在中国市场，能否盈利仍然存在很大的不确定性。这种多元化的机会也带来了一些好处，比如以前从未涉足的新市场会带来新的机遇。</p><p></p><p>多模态模型和大语言模型在这一过程中都已经被开源甚至公开化了。因此企业可以更容易地进入这个市场并积累财富。</p><p></p><p>李洋：首先，AI 作为一种科技手段，只要能起到促进作用，它一定会促进原本产业的发展。比如在抖音、微信运营或公众号运营过程中，如果能在原有产业中嵌入 AI 元素，或者通过大模型实现促进，那么就可以实现盈利。但另一方面，提供大模型的公司只要向 B 端或 C 端提供相应的应用程序，就能盈利。例如，Stable Diffusion 平台或 OpenAI 开发的类似 GPT 的大模型应用程序或产品，只要被腾讯、抖音等公司采用，就可以利用大模型盈利。但是还有一个问题，在数字化和智能化的过程中，能否创造出新的盈利模式。例如华为、四大咨询公司（麦肯锡等）提供的咨询服务，如果企业能将这些咨询服务整合成一个大模型，并利用大数据分析技术，那么实施周期和效率可能会比传统方式更快。此外，还需要解决数据脱敏和隐私保护等问题。</p><p></p><p>企业应该将这种科技手段与所有产业结合起来。从第一次工业革命到第四次工业革命，我们一直在追求生产力的提升，但我们仍然需要抓住自己的主业。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/g65xuAxOEIUIWc5Ey5h7</id>
            <title>Karpathy又整活儿了！一天训练出GPT-2、成本还骤降100倍，网友：dream老黄把价格再打下来</title>
            <link>https://www.infoq.cn/article/g65xuAxOEIUIWc5Ey5h7</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/g65xuAxOEIUIWc5Ey5h7</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jul 2024 07:52:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, GPT-2, Andrej Karpathy, llm.c
<br>
<br>
总结: 本文介绍了OpenAI创始成员、前研究科学家Andrej Karpathy最近在llm.c中重现GPT-2的过程。Karpathy使用了15.58B参数的完整版本的GPT-2，并通过改进计算、软件和数据等方面，成功在24小时内以672美元的成本对该模型进行了重现。文章还提到了Karpathy的职业经历和他开发的llm.c，以及对GPT-2训练成本和硬件利用率的估算。最后，文章对比了Karpathy复刻的GPT-2与19年版本的GPT-2的输出结果，发现新模型的输出质量与GPT-2相当。 </div>
                        <hr>
                    
                    <p>OpenAI 创始成员、前研究科学家Andrej Karpathy 最近尝试在llm.c中重现了GPT-2。这里的GPT-2是15.58B参数的完整版本，最初亮相于OpenAI 2019年2月14日发布的博文《Better Language Models and their Implications》当中。</p><p></p><p>“2019年时，GPT-2 的训练工作还是一个涉及整个团队、需要规模化投入的项目。但如今5年过去，随着计算（H100 GPU）、软件（CUDA\cuBLAS、cuDNN、FlashAttention）和数据（例如FineWeb-Edu数据集）等层面的改进，我们已经能够在24个小时之内凭借单个八H100节点成功对该模型进行重现，且总成本仅为672美元。”Karpathy 说道。</p><p></p><p>Karpathy 在2017年离职后进入特斯拉担任AI 高级总监，但在2023年再次回到OpenAI组建团队，并推出了 ChatGPT。一年后，Karpathy离开了OpenAI，并出于教育意义开发了llm.c。llm.c 是简单、纯 C/CUDA 的 LLM（总计约5000行代码），无需使用涉及Python解释器或者高复杂度深度学习库（例如PyTorch/JAX、huggingface/transformers 等）的典型训练技术栈。</p><p></p><p>在Karpathy 公布了这一结果后，有网友问到当时训练 GPT-2 的成本，Karpathy 回答道：</p><p></p><p>这些信息从未公开过，但我估计成本要高得多。按乘数倍率来算，数据方面可能要高了 3 倍，硬件利用率方面高 2 倍。2019 年的计算集群可能使用的是 V100 (~100 fp16 TFLOPS)，现在可能换成了 H100 (~1,000)，这样算下来性能大概提高了 10 倍。所以成本方面非常粗略地估计，可能要高出 100 倍，也就是大约 100,000 美元左右？</p><p></p><p>对此有网友评价道，“随着英伟达对 AI 工作负载加速硬件开发的不断深入，我预计未来几年内，这款硬件的成本可能只有几十美元，并且训练时间只需几个小时。”</p><p></p><p>至于具体效果，Karpathy 与19年的GPT-2版本做了对比。同样用的当时博文介绍里的提示词“In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.” 结果新模型的输出结果相当连贯，质量也大致与GPT-2相当。</p><p></p><p>两个模型生成的文字较长，有兴趣的朋友可以点击查看：http://llmc.s3-us-west-2.amazonaws.com/html/gpt2_vs_llmc30kedu.html</p><p></p><p>下面我们来看下Karpathy的复刻过程。</p><p></p><p></p><h3>复现过程</h3><p></p><p></p><p>首先，Karpathy强调，使用llm.c训练GPT-2非常简单，因为它是用C/CUDA编写的，所以全程不涉及minconda、Python、PyTorch等。只需要一台八H100 GPU的设备即可。</p><p></p><p>“总之，不必担心，llm.c在算力要求方面非常灵活，哪怕只有一张GPU，大家也仍然可以训练出自己的GPT-2——只不过需要等待8天，而不是像我这样的1天。而如果您拥有16张GPU（例如使用新的Lambda 1 Click Clusters），还可以开展多节点训练，前后只需要等待12个小时。”Karpathy说道。</p><p></p><p>在节点启动之后，下面来看看GPT-2训练的完整说明，不用担心，Karpathy表示保证一分钟以内开始执行：</p><p></p><p><code lang="text"># install cudnn so we can use FlashAttention and run fast (optional)
# https://developer.nvidia.com/cudnn-downloads
# for me, CUDA 12 (run `nvcc --version`) running on Linux x86_64 Ubuntu 22.04
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt-get update
sudo apt-get -y install libcudnn9-dev-cuda-12

# "install" cudnn-frontend to ~/
git clone https://github.com/NVIDIA/cudnn-frontend.git

# install MPI (optional, if you intend to use multiple GPUs)
# (you might also have to install NVIDIA NCCL if it doesn't come with your setup)
sudo apt -y install openmpi-bin openmpi-doc libopenmpi-dev

# download and enter llm.c repo
git clone https://github.com/karpathy/llm.c.git
cd llm.c

# download the "starter pack" (~1GB download)
# contains GPT2-124M weights (used in tests), tokenizer, eval data .bin s
./dev/download_starter_pack.sh

# download the training dataset (FineWeb-Edu 100B token) .bin data shards
# note: this is a total of 1001 data shards. If you only want to test things
# out and don't want to do an actual run, feel free to append the number of
# training shards to download (e.g. for just 10 shards: ./edu_fineweb.sh 10)
# the full dataset is ~200GB, we can store it here in dev/data directory.
cd dev/data
./edu_fineweb.sh

# compile (~1 min 1st time for cuDNN mostly, few sec from then on)
cd ../../
make train_gpt2cu USE_CUDNN=1

# and train! (wait 24 hours here)
mpirun -np 8 ./train_gpt2cu \
-i "dev/data/edu_fineweb100B/edu_fineweb_train_*.bin" \
-j "dev/data/edu_fineweb100B/edu_fineweb_val_*.bin" \
-o "log_gpt2_1558M" \
-v 250 -s 300000 -g 384 \
-h 1 \
-b 16 -t 1024 \
-d 1048576 \
-r 0 \
-z 1 \
-c 0.1 \
-k "cosine" \
-l 0.0006 \
-q 0.1 \
-u 700 \
-n 2000 \
-x 32000 \
-ge 1 \
-y 1 \
-e "d48"</code></p><p></p><p>稍后会对参数做具体解释。接下来开始优化：</p><p><code lang="text">num_parameters: 1557686400 =&gt; bytes: 3115372800
allocated 2971 MiB for model parameters
batch_size B=16 * seq_len T=1024 * num_processes=8 and total_batch_size=1048576
=&gt; setting grad_accum_steps=8
created directory: log_gpt2_1558M
allocating 40409 MiB for activations
val loss 11.129390
allocating 2971 MiB for parameter gradients
allocating 742 MiB for AdamW optimizer state m
allocating 742 MiB for AdamW optimizer state v
allocating 742 MiB for master copy of params
step    1/32000 | loss 11.133732 (+nanz)| norm 52.9732 (+nanz)| lr 8.57e-07 | 3056.36 ms | 42.6% bf16 MFU | 343080 tok/s
step    2/32000 | loss 10.539388 (+nanz)| norm 43.5996 (+nanz)| lr 1.71e-06 | 2747.19 ms | 47.4% bf16 MFU | 381690 tok/s
step    3/32000 | loss 9.894109 (+nanz)| norm 23.2229 (+nanz)| lr 2.57e-06 | 2753.25 ms | 47.3% bf16 MFU | 381259 tok/s
step    4/32000 | loss 9.566241 (+nanz)| norm 28.4920 (+nanz)| lr 3.43e-06 | 2741.47 ms | 47.5% bf16 MFU | 381690 tok/s
step    5/32000 | loss 9.482848 (+nanz)| norm 23.7817 (+nanz)| lr 4.29e-06 | 2752.07 ms | 47.3% bf16 MFU | 381507 tok/s
step    6/32000 | loss 9.332832 (+nanz)| norm 15.9113 (+nanz)| lr 5.14e-06 | 2751.01 ms | 47.3% bf16 MFU | 381431 tok/s
step    7/32000 | loss 9.165650 (+nanz)| norm 10.5941 (+nanz)| lr 6.00e-06 | 2753.03 ms | 47.3% bf16 MFU | 381327 tok/s
step    8/32000 | loss 9.132234 (+nanz)| norm 16.2733 (+nanz)| lr 6.86e-06 | 2748.91 ms | 47.3% bf16 MFU | 381348 tok/s
step    9/32000 | loss 9.097384 (+nanz)| norm 12.1342 (+nanz)| lr 7.71e-06 | 2748.73 ms | 47.3% bf16 MFU | 381367 tok/s
step   10/32000 | loss 9.072879 (+nanz)| norm 10.5923 (+nanz)| lr 8.57e-06 | 2749.40 ms | 47.3% bf16 MFU | 381369 tok/s
...</code></p><p></p><p>可以看到，每个步骤大约需要2.75秒，而其中总共涉及3.2万个步骤，所以现在需要等待约24个小时。</p><p></p><p>在每一步中，训练作业的运行都会占用约100万个FineWeb-EDU token（数据内容来自互联网上的教育网页），并对模型的15.58亿个权重进行更新，使其能够更好地预测序列中将要出现的下一个token，到最后将总计处理3.2万 x 1048576 = 33.6 B个token。随着预测下一token的能力越来越强，loss也会随之下降。</p><p></p><p>接下来的工作是归一化（将数值范围控制在0.1至1之间），学习率也在前几个步骤中逐渐升温。从结果来看，这套模型的flops利用率（MFU）约为50%，可说是相当高效了。</p><p></p><p>现在唯一要做的，就是等待24小时让其完成，之后可以使用dev/vislog.ipynb jupyter notebook对main.log日志文件进行可视化。为此，大家需要安装Mython和matplotlib。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/9e/9e045d301d587cd855b073928d7b03d6.png" /></p><p></p><p></p><h4>评估</h4><p></p><p></p><p>如左图，正在跟踪FineWeb-EDU验证数据的loss。如果大家只运行OpenAI发布的GPT-2并在此基础上评估其loss，得到的就是红色水平线（loss 2.83）。而Karpathy模型的运行结果快速将其超越，步长约为5000。</p><p></p><p>当然，这样的比较并不公平，毕竟GPT-2是在从未发布的WebText数据集上训练而成，因此素材内容可能存在很大的分布差异。比方说，如果在LR 1e-4下对OpenAI模型进行1000步微调，loss就会迅速下降至划线（loss 2.61），代表其正在快速适应新的统计数据。</p><p></p><p>但就个人而言，Karpathy 认为loss验证更多只是一种健全性检查，要实际比较模型性能，还是得借助更靠谱的第三方评估。</p><p></p><p>这时候就要请出HellaSwag评估了，这也是目前市面上表现良好、流畅、普及度高、常被引用且能够提供早期信号的评估方案之一。其中提供的都是简单的常识性场景，大模型必须据此做出正确的内容延展。</p><p></p><p>Karpathy 在右侧窗格中评估了HellaSwag，并发现在约25K步左右与GPT-2模型的性能发生交叉（早于GPT-2，据估计GPT-2的训练数据集共有约1000亿个token。但这可能与数据质量的提高有关，之前Karpathy在124M训练期间也观察到了类似的现象）。绿线为同等参数规模的GPT-3模型，其模型架构与GPT-2几乎相同、仅存在细微差别（上下文长度从1024增长至2048），同时是针对3000亿token进行了训练（相当于我们此次实验训练token量的10倍左右）。</p><p></p><p>必须承认，HellaSwag也不能算是完美的单点比较选项，毕竟它测试的主要是简单的英语和常识，并不涉及多语言、数学或者代码内容。也许是因为WebText数据集在这几个方面拥有更高的比重，所以才把模型规模推到了这样的水平，但Karpathy团队并不确定，毕竟OpenAI从未对此做出过解释。</p><p></p><p>Karpathy指出，一般来讲，在GPT-2等低能力模型上很难获得良好的评估结果，毕竟这类模型无法理解多项选择题；而且其样本质量不够高，无法正常完成涉及标准数学或者代码的评估测试。</p><p></p><p></p><h4>参数指南</h4><p></p><p></p><p>现在让我们仔细看看我们在训练中传递的参数。OpenAI发布的GPT-2虽然包含模型权重，但相关细节却很少；GPT-3版本并未开放权重，但相关细节较多。因此在多数情况下，我们只能参考GPT-3论文中提及的超参数，毕竟GPT-2论文几乎没有提到这方面信息：</p><p>&nbsp;</p><p>-i -j&nbsp;用于训练和验证分割标记文件，需要提前使用&nbsp;edu_fineweb.sh进行下载。-o&nbsp;是写入日志和检查点的输出目录。-v 250&nbsp;要求每250步执行评估并记录验证loss。-s 300000&nbsp;要求每30万步采样部分token。因为总步数不足30万，所以这其实是一种关闭采样的灵活方式，实际只会在最后采样一次。-g 384&nbsp;将最后需要采样的token数设置为384。-h 1&nbsp;要求评估HellaSwag准确性。-b 16&nbsp;将微批次大小设置为16。如果内存不足，请降低此值，例如依次尝试8、4、2、1。-t 1024将最大序列长度设置为1024，与原版GPT-2保持一致。-d 1048576&nbsp;要求总批次大小为2的20次方，与GPT-3论文中的超参数设置相同。代码将确保满足所需的总批次大小，并计算优化所需的梯度累积“内循环”步骤。例如，之前提到Karpathy拥有8张GPU，每张GPU执行16 x 1024个token，因此每个微步（即一次向前向后）对应8 x 16 x 1024 = 131072个otken，因此代码计算梯度累积步数应该为8以满足每步所需的1M批次大小。即每向前+向后8次，而后进行一次更新。-r 0&nbsp;将重新计算设置为0。重新计算是一种在计算与内存之间求取平衡的方法。如果设为-r 1，则代表在反向过程中重新计算前向传递的一部分（GeLU）。就是说Karpathy不必须通过对其缓存来节约内存，但需要付出更高的算力成本。因此如果内存不足，请尝试设置-r 1或者-r 2（同时重新计算layernorms）。-z 1&nbsp;在多个GPU上启用ZeRO-1（即优化器状态分片）。如果使用多于1张GPU进行训练，则应当选择这样的设置，且基本上应始终将其保持为开启状态。但在单GPU上，此设置没有实际效果。-c 0.1&nbsp;将权重衰减设置为0.1。只有（2D）权重的衰减与GPT-2完全相同，具体数值来自GPT-3论文。-k "cosine"&nbsp;设置余弦学习率计划，这里姑且直接使用默认值。-l 0.0006&nbsp;将最大学习率设置为6e-4。根据GPT-3论文的解释，Karpathy这个大小的模型应当使用2e-4，但这里Karpathy将其增加至三倍，似乎训练速度更快且没有引发任何问题。这项参与未经认真调整。-q 0.1代表在训练过程中，将学习率衰减至最大LR的10%，取值参考自GPT-3论文。-u 700&nbsp;表示将在前700次迭代中将学习率从0提升至最大，总批次大小为0.5M时对应3.5亿个token，取值同样来自GPT-3论文。-n 2000&nbsp;要求每2000步保存一次模型检查点。-x 32000&nbsp;要求总共32K步。之所以选择这个数字是因为其好读好记，而且正好对应24个小时。-ge 1&nbsp;为CublasLt设置最近合并的gelu重新计算设置（可选）。-y 1用于将“恢复”标记设置为开启。如果训练因任何原因而崩溃或者挂起，则可按下CTRL+C并重新运行此命令，其将尝试恢复优化。Llm.c具备按bit确定性，因此大家将获得与崩溃之前完全相同的结果。-e "d48"&nbsp;要求从头开始初始化深度为48的GPT-2模型。</p><p></p><h4>内存指南</h4><p></p><p></p><p>大多数朋友面临的最大限制，可能就是自己的GPU内存达不到80 GB。Karpathy表示，“没关系，只要有耐心，之前提到的这些任务也都能顺利运行完成，只是速度会稍慢一些。”</p><p></p><p>但如果模型实在太大，又该如何处理？Karpathy表示，最重要的是调整微批次大小-b，尝试将其缩小并保持在合适的水平。例如16 -&gt; 8 -&gt; 4 -&gt; 2 -&gt; 1。以此为基础，尝试使用重新计算设置-r，即0（最快，但占用的内存最大）、1（速度慢得多，但可以节约大量内存）或者2（稍慢一些，但内存节约量较少）。</p><p></p><p>下一步优化思路则是禁用fp32中的主权重，这里可怜请用 -w 0（默认值为1）来执行此操作。Karpathy并没有为参数维护fp32副本，因为根据经验，之前的几次运行都没有问题，可能是因为使用了随机舍入。</p><p></p><p>“但如果大家在亲自尝试时遇到了问题（其实可能性极低），也可以使用-t减少最大序列长度，将默认值从1024下调至512、256等。但这意味着缩小了其最大注意力范围，所以模型的性能也会变得更差。 ”Karpathy建议道。</p><p></p><p></p><h4>代码</h4><p></p><p></p><p>“虽然我可能有点倾向性，但llm.c真的非常优雅”Karpathy介绍道：</p><p></p><p></p><p>它只需要基本CUDA依赖即可运行。它是C/CUDA中最直接、最小且可读的实现。llm.c总计约有5000行C/CUDA代码。Karpathy 主要尝试使用C，而非C++，以保持代码简洁。神经网络训练只是对单个浮点数组执行相同的简单算术运算（就是加减乘除）的一个while循环，实在没必要搞得太过复杂。它的编译和运行速度极快（几秒钟内），因此可以执行更多步骤并减少等待时间。它会在开始时一次性分配所有GPU内存，并在之后的训练期间将内存占用量保持恒定。因此只要执行步骤启动，我们就能保证接下来的运行状态始终良好、不会发生OOM。具备按bit确定性。 运行效率很高，MFU略低于约50%。主要入口点和大部分代码位于文件tarin_gpt2.cu当中。该文件包含GPT-2模型定义和约2000 LOC中的训练循环，并从llmc目录处导入了一大堆带有各种实用程序和各层实现的辅助文件。cloc llmc报告了23个文件，3170 LOC，而cloc train_gpt2.cu目前为1353 LOC。</p><p></p><p></p><h4>多节点训练</h4><p></p><p></p><p>如果您是位手握大量GPU的“土豪”，llm.c也支持多节点训练。Karpathy表示，其见过的llm.c训练最多支持约500张GPU。</p><p></p><p>“个人迄今为止进行过最大的一次运行，是依靠Lambda全新一键集群功能上实现的，当时是在2个节点上使用了16张H100 GPU。Lambda团队提供了关于如何在其一键集群上训练llm.c模型的详细说明。例如在使用512-GPU H100集群时，每小时费用为2300美元，这时候整个GPT-2训练周期就仅为30分钟。当然，这时您需要增加总批次大小（例如增加到约8M）并稍微调整一下超参数。我还没有尝试过，但相信会有效而且相当爽快！ ”Karpathy说道。</p><p></p><p></p><h4>PyTorch比较</h4><p></p><p></p><p>使用Karpathy的并行PyTorch实现，与PyTorch的运行效果对比应该类似于以下形式：</p><p></p><p><code lang="text">torchrun --standalone --nproc_per_node=8 train_gpt2.py \
    --input_bin "dev/data/edu_fineweb100B/edu_fineweb_train_*.bin" \
    --input_val_bin "dev/data/edu_fineweb100B/edu_fineweb_val_*.bin" \
    --write_tensors 0 \
    --model d48 \
    --batch_size 8 --sequence_length 1024 --total_batch_size 1048576 \
    --dtype bfloat16 \
    --compile 1 \
    --tensorcores 1 \
    --flash 1 \
    --num_iterations 32000 \
    --warmup_iters 700 \
    --weight_decay 0.1 \
    --overfit_single_batch 0 \
    --learning_rate 0.0006 \
    --zero_stage 1</code></p><p></p><p>这里的PyTorch代码仅供参考，而非实际实现，因为其中的训练循环在某些位置可能略有不同（例如，数据加载器不会对分片进行置换等），总之大家看看就好。Karpathy还将默认词汇大小修改为50257 -&gt; 50304 以提高效率。经过一夜运行，PyTorch给出以下结果：</p><p></p><p><code lang="text">step   16/32000 | train loss 8.903997 | norm 8.3474 | lr 1.37e-05 | (3381.88 ms | 310057 tok/s)
step   17/32000 | train loss 8.870140 | norm 3.7936 | lr 1.46e-05 | (3381.95 ms | 310051 tok/s)
step   18/32000 | train loss 8.875732 | norm 9.4993 | lr 1.54e-05 | (3393.09 ms | 309033 tok/s)
step   19/32000 | train loss 8.817432 | norm 2.8345 | lr 1.63e-05 | (3379.75 ms | 310253 tok/s)
step   20/32000 | train loss 8.798056 | norm 4.1234 | lr 1.71e-05 | (3386.53 ms | 309631 tok/s)
step   21/32000 | train loss 8.777574 | norm 2.8010 | lr 1.80e-05 | (3386.05 ms | 309675 tok/s)
...</code></p><p></p><p>Karpathy 强调，这份PyTorch脚本可能还有很大的优化空间，但至少可以当作观察基准。PyTorch占用的内存量似乎更大（此次运行约为80 GB），而llm.c仅占用了57 GB（节约比例为29%）。内存资源非常重要，因为它能帮助我们容纳更大的训练批次（例如，llm.c在这里可以将微批次提升至24个），从而加快训练速度。</p><p></p><p>其次，我们看到每次迭代大约为3386毫秒，而llm.c的迭代为2750毫秒，速度要快约19%。</p><p></p><p>另外还有一些已知优势，例如llm.c包含启动反向传递的融合分类器等优化选项，据Karpathy所说，目前的torch.compile还做不到。但Karpathy表示，这样的性能差异可能是因为他的脚本没有充分调优，所以比较结果仅供大家看看、试试和作为调试思路的启发。</p><p></p><p>“我想表达的是，llm.c的优化程度和速度水平已经相当不错，当然只是在GPT-2/3训练的特定场景之下。 ”Karpathy说道。</p><p></p><p></p><h4>最终模型</h4><p></p><p></p><p>感兴趣的朋友可以参考以下几条链接：</p><p></p><p></p><p>main.log<a href="http://llmc.s3-us-west-2.amazonaws.com/gpt2_1558M/main.log">文件</a>"。model_00032000.bin&nbsp;<a href="http://llmc.s3-us-west-2.amazonaws.com/gpt2_1558M/model_00032000.bin">llm.c</a>"&nbsp;bin模型文件我已经将模型转换为huggingface transformers GPT-2并上传至这里:&nbsp;karpathy/gpt2_1558M_final2_hf。</p><p></p><p></p><h4>模型导出</h4><p></p><p></p><p>模型导出可以按如下方式进行，例如：</p><p></p><p><code lang="text">python dev/eval/export_hf.py --input log_gpt2_128M/model_00032000.bin --output gpt2_1558M_export</code></p><p></p><p>之后大家可以运行Eleuther评估工具，或者运行huggingface采样管线以获取模型样本：</p><p><code lang="text"># take model for spin
import torch

output = "./gpt2_1558M_final2_hf"

# set pytorch seeds
torch.manual_seed(42)
torch.cuda.manual_seed(42)

prompt = "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English."
from transformers import AutoModelForCausalLM, AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(output)
model = AutoModelForCausalLM.from_pretrained(output, attn_implementation="flash_attention_2", torch_dtype=torch.bfloat16, device_map='cuda')
model.eval()
tokens = tokenizer.encode(prompt, return_tensors="pt")
tokens = tokens.to('cuda')

output = model.generate(tokens, max_new_tokens=500, pad_token_id=tokenizer.eos_token_id, do_sample=True, top_k=50, num_return_sequences=4)
samples = tokenizer.batch_decode(output)
for sample in samples:
    print('-'*30)
    print(sample)</code></p><p></p><p>另外大家也可以查看dev/eval 以获取关于如何运行Eleuther Evaluation Harness、HuggingFace Open LLM Leaderboard的具体说明。</p><p></p><p></p><h5>400B token运行</h5><p></p><p></p><p>Karpathy 还尝试用远超33B token的规模训练了GPT-2。具体来讲，Karpathy将-x更改为400000以训练420B token（规模甚至比300B 的GPT-3还要大）。</p><p></p><p>结果显示，这套模型前半阶段运行得不错，但到大约33万步时开始出问题： 这套模型在HellaSwag上全面碾压了GPT-2及同等体量的GPT-3（最高性能优势可达约61%），但遗憾的是之后新模型开始不稳定并发生崩溃。</p><p></p><p>在此过程中虽然也出现过一些较小的峰值，Karpathy将代码配置为当检测到瞬时不稳定时跳过更新（使用了-sl 5.0 -sg 5.0标记），这有助于缓解并推迟问题的出现。但Karpathy承认，模型在初始化、激活范围和整体模型训练的稳定性方面还不够谨慎，对很多深层次问题也没有涉及。</p><p></p><p>这些问题会令模型逐渐变得不稳定，特别是对于规模较大、训练时间较长的模型更是如此。当然，我的实验仍在进行当中。如果大家对稳定模型训练有任何想法和建议，请在评论区中与我们分享。</p><p></p><p></p><h4>常见问题解答</h4><p></p><p></p><p>Q：我可以从llm.c中的模型里采样吗？</p><p>A：也不是不行，但效率很低而且效果不好。如果大家想要提示模型，推荐使用前文提供的huggingface版本。</p><p></p><p>Q：我能跟它聊天吗？</p><p>A：还不行，目前这个版本只完成了预训练，还没有接受过聊天微调。</p><p></p><p>Q：可以在fp8精度下训练吗？</p><p>A：不行，我们目前主要是在bf16下训练，但早期版本正在尝试当中。</p><p></p><p>Q：我的GPU不是英伟达的，可以运行llm.c吗？</p><p>A：不行，llm.c目前仅支持C/CUDA，但已经提供良好分支。比如@anothonix积极维护的AMD分叉（https://github.com/anthonix/llm.c）就相当不错。 GPT-2(124M)。这里再贴一篇关于在llm.c中训练GPT-2（124M）模型的老帖，其中包含与llm.c运行相关的更多信息。124M属于GPT-2迷你系列中的小体量模型，只有124M个参数，远低于本文讨论的1558M参数。</p><p></p><p></p><h2>结束语</h2><p></p><p></p><p>Karpathy 让我们看到了更多可能，但这似乎也难以意味着未来整个训练成本会下降。不久前，AI初创公司Anthropic的首席执行官Dario Amodei 就在采访中表示，目前GPT-4o这样的模型训练成本约为1亿美元，而目前其正在开发的AI大模型训练成本可能高达10亿美元。 他还预计，未来三年内，AI大模型的训练成本将上升至100亿美元甚至1000亿美元。</p><p></p><p>参考链接：</p><p>https://x.com/karpathy/status/1811488645175738409</p><p>https://github.com/karpathy/llm.c/discussions/677</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0373pdxIZtgCgWxObktf</id>
            <title>vivo蓝河操作系统首届技术沙龙即将举办，邀您共探Rust与AI新时代</title>
            <link>https://www.infoq.cn/article/0373pdxIZtgCgWxObktf</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0373pdxIZtgCgWxObktf</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jul 2024 07:43:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: vivo, 蓝河操作系统, AI 大模型, Rust语言
<br>
<br>
总结: vivo在2023年11月发布了自研操作系统蓝河操作系统（BlueOS），该系统基于Rust语言编写，接入了vivo蓝心大模型，具备先进的AI能力和多模输入子系统，提升了智慧、流畅性和安全性，同时拥有生态互联的连接技术，为用户和开发者提供更智能、安全的体验和开发平台。 </div>
                        <hr>
                    
                    <p></p><p>2023 年 11 月，vivo 在开发者大会上正式发布了自研操作系统——蓝河操作系统（BlueOS）。据称蓝河为业界首款系统框架基于 Rust 语言编写的操作系统，同时底层还接入了 vivo 蓝心大模型，「蓝河操作系统」一经发布便引起了行业的广泛关注。</p><p></p><p>在智慧方面，通过引入先进的 AI 大模型能力，BlueOS 实现了 AI 服务引擎和多模输入子系统，支持基于自然交互方式的应用开发，用户也可以通过语音、手势等多种方式与系统进行交互；在流畅性方面，通过优化算法和高性能系统架构设计，BlueOS 成功实现了资源的合理分配和高效利用，确保了应用运行丝滑流畅；在安全方面，BlueOS 的系统框架由 <a href="https://xie.infoq.cn/article/018986ea780ce3a32225de6d0?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Rust 语言</a>"编写，能够从源头避免内存使用不当引起的安全漏洞；在生态互联方面，BlueOS 基于分布式设计理念的连接技术，能够广泛兼容行业标准协议，也让 vivo 在智能家居、智慧出行、智慧办公等场景具备了更大的可能性。</p><p></p><p>从某种意义上来说，蓝河操作系统（BlueOS）的诞生，代表了 vivo 在提升用户体验和安全方面的坚定承诺，不仅为用户带来了更加智能和安全的产品体验，也为开发者提供了一个高效、安全的开发平台，同时也对推动国产软件生态的发展具有积极意义。</p><p></p><p>当前，核心技术自主可控仍然是国产软件行业发展的主旋律，大模型与 AIGC 的场景化探索也越发火热。率先入局的 vivo 在这些方面有哪些新思考、新探索，BlueOS 在技术探索和生态建设上又取得了哪些新进展？</p><p></p><p>7 月 27 日，vivo 将在北京举办蓝河技术沙龙，汇聚行业大咖、资深技术专家，共同分享蓝河操作系统的最新发展动态，探讨操作系统技术的未来趋势等。你将有机会与行业大咖面对面交流，也可以探索应用开发新范式和 AI 大模型技术的前沿应用等。</p><p></p><p>蓝河新航，机遇正当时！马上报名技术沙龙，现场开阔技术视野、拓展合作机会， 与 vivo 一起，开辟下一代操作系统的新航道，共同探索未来的无限可能！</p><p></p><p>招募启动日起至活动前一天，分享预热文章至朋友圈，活动当天到场即可凭借朋友圈公开转发记录，领取蓝河定制雨伞，数量有限，先到先得！</p><p></p><p>现场参会者，更有机会获得 vivo 蓝牙耳机等重磅礼品，不容错过~</p><p></p><p>码上报名👇</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/de/de9a583d9d1ee251ead9b0cd3269ceab.png" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Lp1WKzT91CNiKaegxA8F</id>
            <title>“我的代码被微软和 OpenAI 抄了，维权后被他们耗了两年”</title>
            <link>https://www.infoq.cn/article/Lp1WKzT91CNiKaegxA8F</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Lp1WKzT91CNiKaegxA8F</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jul 2024 07:43:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: GitHub, 微软, OpenAI, Copilot
<br>
<br>
总结: 2022年11月，四名身份不明的原告（“J. Doe”）（随后增加到五名）针对GitHub、微软和OpenAI在美国发起集体诉讼，声称Copilot编码助手在训练期间使用了托管在GitHub上的开源软件，因此会向其他程序员用户建议这些公共项目的片段，且完全不考虑许可证条款的要求，例如提供适当的来源标注，因此上述公司侵犯了原创作者的知识产权。他们之所以感到不满，是因为他们认为Copilot可能会将他们编写的、本应受到版权保护的开源代码中的部分内容交付（更确切地说是复制）给其他程序员使用，且未对其劳动成果给予相应的认可、也没有遵守原始许可证的其他条款要求。 </div>
                        <hr>
                    
                    <p>&nbsp;</p><p>2022年11月，四名身份不明的原告（“J. Doe”）（随后增加到五名）针对GitHub、微软和OpenAI在美国发起集体诉讼，声称Copilot编码助手在训练期间使用了托管在GitHub上的开源软件，因此会向其他程序员用户建议这些公共项目的片段，且完全不考虑许可证条款的要求，例如提供适当的来源标注，因此上述公司侵犯了原创作者的知识产权。</p><p>&nbsp;</p><p>他们之所以感到不满，是因为他们认为Copilot可能会将他们编写的、本应受到版权保护的开源代码中的部分内容交付（更确切地说是复制）给其他程序员使用，且未对其劳动成果给予相应的认可、也没有遵守原始许可证的其他条款要求。</p><p>&nbsp;</p><p>此诉讼最初共涉及22项索赔，但随着时间推移，被告企业提出动议要求法庭驳回指控并基本受到法官Jon Tigar的支持，因此索赔数量逐渐减少。</p><p>&nbsp;</p><p>在当地时间7月5日公布的法院庭谕（<a href="https://regmedia.co.uk/2024/07/08/github_copilot_dismiss.pdf">https://regmedia.co.uk/2024/07/08/github_copilot_dismiss.pdf</a>"）中，Tigar 法官对原告提出的另一批索赔作出裁决。总体而言，GitHub、微软及OpenAI获得胜诉。其中三项索赔要求被驳回，只有一项被允许继续讨论。而根据微软及GitHub方面律师的统计，目前总计只余两项指控仍待审理。</p><p>&nbsp;</p><p>这一结果再次引发开发者们的关注和热烈讨论。</p><p>&nbsp;</p><p>“如果微软和 OpenAI 获胜，是否意味着任何人都可以合法获取专有产品的泄露源代码，并对其进行 LLM 培训，然后要求 LLM 给出一个足够不同的版本来避免侵犯版权？”有网友提出质疑后立马有人跟帖称，“我认为这正是将要发生的事情，不仅仅是代码，还有文学和艺术品。有人可能会专门为此设计和训练这样的LLM。"</p><p>&nbsp;</p><p>“我觉得真正重要的是谁有更多的钱投入到法庭上。不知怎的，我觉得如果是‘Adobe 诉称其代码被Copilot使用’，结局不会一样。”还有网友称。</p><p>&nbsp;</p><p></p><h2>22项索赔要求，20个无疾而终</h2><p></p><p>&nbsp;</p><p>为什么说7月5日被驳回的指控相当重要？因为其中一项涉及《数字千年版权法（DMCA）》第1202（b）条的侵权规定，该条款规定不得在未经许可的情况下删除关键的“版权管理”信息。根据许可证中的相关规定，此案情形应明确列出代码由谁编写以及使用条款。</p><p>&nbsp;</p><p>集体诉讼的开发者们认为，Copilot 在为用户提供项目代码片段时删除了这些信息，因此违反了该条的规定。</p><p>&nbsp;</p><p>然而法官Tigar并不同意，理由是Copilot的代码建议与开发者自身编写的版权保护成果并不完全相同，因此上述条款并不适用。</p><p>&nbsp;</p><p>事实上，据称去年GitHub已经调整了这款编程助手，确保生成代码建议时较训练代码做出细微调整，以防止输出内容被指控为直接照搬许可软件片段。</p><p>&nbsp;</p><p>随着法官Tigar驳回该项指控，原告方已无法就DMCA第1202（b）条提出新的索赔理由。</p><p>&nbsp;</p><p>但多位匿名程序员一再坚称，Copilot仍会生成与其亲手编写的代码完全相同的建议，这也成为他们诉讼期间的核心论点，而这恰恰符合DMCA设定的相同性要求。然而，法官Tigar此前裁定原告方实际上无法证明存在这种情况，导致原告只能撤回指控并对内容做出进一步修改。</p><p>&nbsp;</p><p>修改后的指控辩称，如果用户关闭Copilot的反复制安全开关，即可引发非法代码复制。他们还引用一项关于AI生成代码的研究，试图用其中列举的Copilot剽窃源代码证据支持自己的立场。但法官再次表示反对，称并无法认定微软的系统在以具有实际意义的方式剽窃他人成果。</p><p>&nbsp;</p><p>在庭审期间，法官专门引用了研究中的观察结果，即Copilot“在良性情况下很少直接提供其记住的代码，大多数原样照搬只发生在用户提示模型使用与训练数据非常相似的长代码片段的情况下。”</p><p>&nbsp;</p><p>法官总结道，“也就是说原告的主张依赖于一项研究，而该研究仅认为Copilot理论上可能根据用户提示词生成与他人所编写代码相匹配的代码，这缺乏充分的说服力。”</p><p>&nbsp;</p><p>DMCA指控只是刚刚被驳回的三项指控中的一条，另外两条分别是不当得利与惩罚性赔偿，公平来看，这两项指控恐怕也须经历修改及重新提出。</p><p>&nbsp;</p><p>除此之外，最初提出的申诉主张就只剩下两项：其一是违反开源许可，其二则是违反合同，而后者也曾经历过被驳回后重新提出。</p><p>&nbsp;</p><p>作为回应，GitHub在一份声明中表示，“我们坚信AI将改变整个世界的软件开发方式，从而提高生产力水平。更重要的是，能够改善开发者的工作感受。”“我们相信Copilot遵循相关法律，并且从起步阶段就一直致力于负责任地通过Copilot实现创新。我们将继续投资并倡导未来由AI驱动的开发者体验。”</p><p>&nbsp;</p><p></p><h2>争论不休、互相指责</h2><p></p><p>&nbsp;</p><p>从上周五针对该案提交的一份联合案件管理声明（<a href="https://regmedia.co.uk/2024/07/08/github_copilot_joint_management.pdf">https://regmedia.co.uk/2024/07/08/github_copilot_joint_management.pdf</a>"）来看，双方在调查过程中频繁向对方提出种种不满与申诉，而且双方均表示对方没有提供其应当提供的全部文件。</p><p>&nbsp;</p><p>原告的开发者们指责三家公司故意拖延时间，称目前才公开的文件早就应该提交并对外披露。而此前大部分焦点都集中在微软当时提交的唯一一份文件上，原告称这明显是在混淆视听。</p><p>&nbsp;</p><p>不愿具名的程序员们强调，“有微软员工参与了多轮由GitHub发起的讨论，表明微软只提交一份文件完全是为了拖延时间和混淆观点，再无其他实际意义。另外，微软明确知晓但并未披露其员工也直接参与了Copilot及其底层模型的创建、运营和管理。”</p><p>&nbsp;</p><p>作为Windows系统的缔造者，微软辩称文件提交不及时主要是由于收集Slack消息时遇到了“技术困难”，但原告对此并不信服。开发者们同时认为，OpenAI也应该提交更多信息，并表示谷歌图书侵权案中的被告就提交了数万份信息。</p><p>&nbsp;</p><p>而另一方面，微软和GitHub反驳称原告方要求提交的信息太多，指责他们“未能有效且真诚地关注与指控切实相关的证据。”其中包括微软2018年对GitHub的收购。</p><p>&nbsp;</p><p>与此同时，OpenAI则表示原告在索要电子邮件方面没有遵循正确的程序，因此在收到正确申请之前，其无法（或者说不会）提供任何电子邮件。</p><p>&nbsp;</p><p>三家公司还指出，DMCA版权索赔被驳回已经从根本上改变了案件性质，并认为现在应该缩小调查范围。原告方对此提出异议，理由是开源许可证违规指控所涉及的文件，与DMCA所对应的文件几乎相同。</p><p>&nbsp;</p><p>GitHub、微软及OpenAI均认为，原告没有正确回应他们的调查请求，称原告方的文件包含“JSON文件、空白HTML文件、没有任何元数据的电子邮件，以及在Slack上发送的经过不当编辑的PNG文件等信息形式”。</p><p>&nbsp;</p><p>目前原告方要求延长调查时间。尽管三家科技巨头辩称没有必要，但表示愿意接受“合理的延期”。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>该事件继续引发了大家的思考：</p><p>&nbsp;</p><p></p><blockquote>“如果我是作为一个人，做了下面的事情：1. 仔细阅读并记忆一些受版权保护的代码；2. 编写与原代码在文本上完全相同的新代码。但在编写过程中，我会随机机械地调整一些标识符或其他东西，以编写具有完全相同语义但字符不相同的代码。3. 声称是新的原创代码，但没有原始版权。我认为从法律角度来说我会受到严厉惩罚。这在我看来完全就是故意侵犯版权并故意掩盖侵权行为。当机器做同样的事情时有什么不同？”</blockquote><p></p><p>&nbsp;</p><p>有网友对此表示，“关键是，当涉及到公司利益时，法律体系具有高度的选择性。”他以美国作家协会诉谷歌案为例称，谷歌的辩护理由就是合理使用。</p><p>&nbsp;</p><p>还有网友表示，机器本身什么也做不了。用户和机器共同构成了一个更大的系统，而有了自动完成功能后，用户才是主导。继而，该网友表示，“我怀疑很多版权侵权都是通过剪切粘贴和截屏功能实现的，也许我们也需要小心使用自动完成功能，避免这种情况是用户的责任。”</p><p>&nbsp;</p><p>该诉讼案引发的担忧并不是最近才有的，但是从该诉讼开始算起已经接近两年的时间，各方都没有给出答案，而且短时间内该诉讼可能也不会完结，意味着这个问题未来一段时间内仍不会有结果。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://news.ycombinator.com/item?id=40919253">https://news.ycombinator.com/item?id=40919253</a>"</p><p><a href="https://www.theregister.com/2024/07/08/github_copilot_dmca/">https://www.theregister.com/2024/07/08/github_copilot_dmca/</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/IzCokfzuYHjuyU4DSqtz</id>
            <title>软件工程的兴衰轮回：2 年巨变，裁员风暴下小团队逆袭，老技术反迎第二春？</title>
            <link>https://www.infoq.cn/article/IzCokfzuYHjuyU4DSqtz</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IzCokfzuYHjuyU4DSqtz</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jul 2024 07:27:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 职业安全, 科技行业变革, 创业速度, 裁员潮
<br>
<br>
总结: 在当前科技行业变革的环境下，追求职业安全比追求职位安全更为重要。小团队能够更快地行动并构建解决方案，创业速度加快。然而，2022年开始出现意料之外的裁员潮，大型科技企业纷纷裁员，即使盈利能力强也不例外。投资市场在2021年爆发，但2022年开始出现IPO寒冬。整个科技行业正在经历着巨大的变化和挑战。 </div>
                        <hr>
                    
                    <p></p><blockquote>“在当前环境下，我认为我们都应追求职业安全而非职位安全。”</blockquote><p></p><p></p><p>过去18个月来，整个科技行业迎来一系列重大变化：从招聘火热到大规模裁员，从密集上市到个位数IPO，就业市场、风险投资、IPO&nbsp;和大型科技公司都受到变革之风的严重影响。</p><p></p><p>我们正在看到，小团队能够比以往更快地行动，并更迅速地构建解决方案。以Twitter的竞争对手Blue&nbsp;Sky为例，该公司在2022年由一位CEO和两名开发人员创立。仅六个月后，其中一位开发者就独立完成了iOS和Android版本的移动应用的软发布。一年后，他们拥有了百万用户。就在上个月，他们已经达到了550万用户，团队成员也仅增加到了12位开发者。</p><p></p><p>2010年出现的Instagram，也是在第一年内就吸引了百万用户，随后两年连续增长到500万、1000万和3000万用户，且只有13名开发人员来完成这一切。比较两者，开发者人数与时间线惊人地相似。尽管Instagram的用户量更大，但两家公司都在相同的时间框架内构建了iOS和Android应用、网站以及能够处理数百万用户的后端服务。</p><p></p><p>有趣的是，在2022年构建原生应用应该比2010年容易得多，但我们似乎正在经历一种似曾相识的感觉。那么这一切的根本原因是什么？对于企业和开发团队意味着什么？未来的实用派软件工程方法又将向何处去？随之“遭殃”的软件工程师们能够如何转变自身处境？</p><p></p><h2>两年时间，求职、投资市场“高开低走”</h2><p></p><p>下面是我们观察到的科技行业过去两年间的变化：</p><p></p><p><img src="https://static001.geekbang.org/infoq/3e/3ef6eae4a9599fefed42b8fdff8f7205.jpeg" /></p><p></p><h3>从激烈争夺人才，到创史上裁员之“最”</h3><p></p><p>2021年底的科技招聘市场呈现出前所未有的热度，求职者数量大大低于以往水平，物色人物变得愈发困难，候选者的薪酬预期也超过了企业预期。有些人当面表示接受，但转过头就跑去追求更高的报价。</p><p></p><p>招聘经理们纷纷表示：“之前从未出现过如此严峻的状况，而且在全球所有地区均是如此。几年前印度曾经出现过技术人才供不应求的火爆状态，而如今的环境相较当时又放大了许多倍。美国、英国、欧盟、东欧、南美……几乎到处都出现了同样激烈的人才争夺，预计这种情况将持续到今年年底。”</p><p></p><p>从当时的情况来看，整个就业市场已经高度趋近于求职者梦想中的“完美形态”。</p><p></p><p>时间快进到六个月后的2022年2月，《纽约时报》发表的一篇文章也得出类似结论，即科技企业正在面临招聘危机。可就在《纽约时报》讨论这一现象的同时，就业市场的风向已经开始了新一轮变化……</p><p></p><p>2022年4月和5月，意料之外的裁员潮初现端倪：一键结账初创公司Fast在筹集到1亿美元的一年之后，突然宣告破产。Klarna意外迎来大规模裁员，比例达到10%。其他几家公司的裁员计划紧随其后，包括Gorllas、Getir、Zapp（即时配送）、PayPal、SumUp、Kontist、Nuri（金融科技）、Lacework（网络安全）等。2022年秋季，大规模裁员的势头仍在继续。Lyft、Stripe、CloudKitchens、Delivery&nbsp;Hero、OpenDoor、Chime、MessageBird等公司纷纷裁员10%甚至更多。</p><p></p><p>许多选择裁决的企业都有一个共同点：公司正面临亏损，所以决定削减人力似乎也在情理之中。但之后，具备盈利能力的公司也开始裁员。</p><p></p><p>2022年11月，Meta解雇了1.1万人（占员工总数的13%），这也是这家社交媒体巨头有史以来首次裁员。几个月后，谷歌、微软和亚马逊也纷纷效仿，裁员数量创下历史新高。2022年末至2023年初的科技行业裁员，其规模堪称多年来之最。</p><p></p><p><img src="https://static001.geekbang.org/infoq/61/616ae242908519a5f9f9b3c3bf2b11af.jpeg" /></p><p></p><p>科技巨头们从2023年初开始推进大规模裁员，理由是在2020至2022年的疫情期间过度招聘。然而到今年年初，大型科技企业的裁员脚步仍未停歇，这时候过度招聘的影响早已消失、反倒是创纪录的利润实打实呈现在财报之上。</p><p></p><p>谷歌就是其中的典型案例：公司成立于1998年，此前仅在2008年进行过一次大规模裁员，而且实际比例也仅为2%（共300人）。但到2023年1月，谷歌一口气裁撤了约6%的雇员。2024年1月，在创纪录的收入与利润之下，这家搜索巨头再次动手裁员。</p><p></p><p>谷歌的行为也相当具有代表性。我们可以解读为——无论收入和利润达到怎样的创纪录高位，大型科技企业似乎都习惯了定期削减人力。</p><p></p><p>我当时分析了这些裁员背后的理由，认为：&nbsp;“Meta、谷歌和亚马逊的裁员绝非偶然之举，他们似乎是在以战略方式削减各成本中心以及存在严重亏损的部门。此外，他们很可能也是在清退那些绩效不佳的员工。”</p><p></p><h3>一年投资爆发期后，IPO&nbsp;寒冬到来</h3><p></p><p>2020年之前，风险投资对于初创公司的关注一直稳步提升。而到2021年，投资速度呈现出爆发式增长，总额几乎翻了一番：</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a9de5bf21812459eff8f1ca4c06c176.jpeg" /></p><p></p><p>按年度计算的风险投资变化情况</p><p></p><p>并且，&nbsp;2021年是公开募股颇为突出的一年，期间大量科技公司成功登陆股票市场。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3a/3aefab6fa15b57bbd070e0d791560435.png" /></p><p></p><p>2021年出现的IPO数量激增</p><p></p><p>为了让大家对这一年间IPO数量之夸张产生印象，以下列出这段时间值得关注的上市企业：GitLab（版本控制）、Rivian（电动汽车）、Couchbase（NoSQL数据库）、Affirm（先买后付）、Bumble（约会）、Duolingo（语言学习）、Robinhood（交易）、Expensify（记账）、Nubank（金融科技）、Roblox（游戏）、Coinbase（加密货币）、Squarespace（域名）、Coupang（电子商务）、DigitalOcean（托管）、Toast（餐厅科技）、Coursera（教育科技）、Udemy（教育科技）、Amplitude（分析）、AppLovin（移动分析）、UiPath（自动化）、Monday.com（项目管理）、Confluent（数据流）、滴滴出行（拼车服务）、Outbrain&nbsp;（广告）、Nerdwallet（个人理财）。</p><p></p><p>但短暂的辉煌之后，风险投资开始稳步下降。到今年第一季度，已经回落到与2018年相同的水平！</p><p></p><p><img src="https://static001.geekbang.org/infoq/8a/8aedb3951541f3dbf42874b45b99cf28.jpeg" /></p><p></p><p>2019年至2024年各季度风险投资情况</p><p></p><p>2021年之后，IPO&nbsp;也迅速进入寒冬。2022年科技IPO数量迅速归零，而2023年也只有三家（ARM、Instacart以及Klaviyo）。当时的我们显然无法想象，HashiCorp在2021年12月的上市成为未来18个月间最后一例IPO。</p><p></p><p><img src="https://static001.geekbang.org/infoq/58/5831090fb6e3bef5d1e71708f847fc73.jpeg" /></p><p></p><h2>所有变化的根源</h2><p></p><p>2022年到2023年间一系列重大变化确实汹涌而来，但原因究竟是什么？最明显的潜在因素，就是2020年至2021年的疫情爆发与封控结束，推动整个世界慢慢恢复原本的秩序。</p><p></p><p>当时，企业创始人和CTO们曾经就公司为何裁员、业务为何突然面临增长危机做出过解释。他们大多复返提及“宏观经济环境”字眼，而且在之后的公司裁员公告中也会强调这方面因素。很明显，利率变化带来的冲击比普遍预计更大。</p><p></p><p>2022年中期，美联储（FED）开美国几十年来的先河，决定大幅拉升储蓄利率：</p><p></p><p><img src="https://static001.geekbang.org/infoq/a7/a70cc221de651028789906588897a0a4.png" /></p><p></p><p>为什么提高利率就能缓解通货膨胀率？下面来看BBC的解释：</p><p></p><p></p><blockquote>英格兰银行通过调整利率来控制英国的通货膨胀问题——即商品价格随时间推移的上涨趋势。当通货膨胀率高企时，英格兰银行（其目标是将通货膨胀率保持在2%）可能会决定提高利率。这样做的目的在于鼓励人们减少支出，进而通过降低需求来帮助降低通货膨胀率。而随着通货膨胀的缓解，英格兰银行可能会继续维持当前利率或者重新降低利率。英格兰银行必须在减缓物价上涨的需要与损害经济的风险之间，求取微妙的平衡。</blockquote><p></p><p></p><p>如果将其中的“英格兰银行”替换成美联储或者欧洲央行，意思也是一样。总而言之，提高利率是应对通货膨胀的一种行之有效的方法。</p><p></p><p>在美国，不到一年时间里利率就从接近0%跃升到了5%左右。这无论是放到这十五年或几十年来看，超低利率都是少数。实际上，自1955年以来，总共有11.5年处于“接近零”的超低利率时期，其中有11年发生在2009年之后。也正因为如此，这段时间才被称为零利率时代（ZIRP）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/ea9eb2b2320b712814d2f49c8a9e6cce.jpeg" /></p><p></p><h3>利率对科技初创企业的影响</h3><p></p><p>很多朋友可能觉得，科技跟金融或者利率有什么关系？“事实恰恰相反”，这不是我的个人意见，而是彭博分析师兼专栏作家Mark&nbsp;Levine的原话。</p><p></p><p></p><blockquote>“初创公司的存在本身，就是一种低利率现象。当全球各地的利率都很低时，那么20年后的1美元跟当下的1美元基本等价，因此初创公司的商业模式就可以是‘忍受在十年之内开发AI所带来的亏损，指望在遥远的未来赚大钱’，这也确实是可以接受的。但在利率较高时，今天的1美元比未来的1美元更值钱，所以投资者们更想要保住当下看得见的真金白银……”如果2021年时，有一位魅力十足的科技创始人找上银行，表示想要通过AI、自动驾驶出租车、飞行出租车、太空出租车或者区块链之类的东西彻底改变世界，其实贷款经理很少会拒绝。毕竟当时大家不会考虑到美联储在短时间内就会提高利率，而且从业者普遍认为对于这些可能给人类未来带来颠覆性变化的事业，拿它跟利率波动对赌实在有点俗了。但事实证明，这一切的本质就是在跟利率波动对赌。</blockquote><p></p><p></p><p>在读到这段分析时，我也本能性地想要反驳。毕竟利率和科技初创公司之间就算有关联、有强关联，也不至于是这么赤裸裸的因果关系吧？但越往深处想，我就越觉得Levine的观点很有道理。所以我们不妨分析一下，当利率迅速从0%上涨到5%时，究竟会发生什么？</p><p></p><p><img src="https://static001.geekbang.org/infoq/3d/3df6392dd6de2b7acba00ccf0bfe4982.jpeg" /></p><p></p><p>先来看看利率会对风险投资、IPO、大型科技企业和就业市场造成的实际影响：</p><p></p><p>风险投资减少：风险投资是一种高风险/高回报的投资类型，其理念就是把一大笔钱（例如1亿美元）投入风险投资基金，等待10年左右再获取相当可观的回报。这1亿投资可能会变成1.5亿美元甚至2亿美元。另外一种选择则是把钱存进银行，但这会侵蚀其价值，因为每年的通货膨胀率（例如2%）会逐年消弱美元的购买力。可是在利率为5%的情况下，我们几乎能够毫无风险地在十年间把1亿美元变成1.5亿美元，那为什么还要投资那些风险巨大的科技初创公司（成功的不少，但失败的更多）并甘冒十年之后血本无归的后果？科技公司IPO减少：上市的科技企业往往处于亏损状态，因为其长期处于业务增长阶段。2021年上市的大多数科技企业都属于这种情况。在高利率环境下，投资这些公司的吸引力变得更弱，毕竟除非已经拥有明确的盈利途径，否则他们可能会迅速耗尽资金，进而导致投资贬值。RIvian的市值就已经从2021年的1500亿美元降低至2024年的100亿美元（部分原因就来自投资者对其资金耗尽风险的担忧），这明显是个值得警惕的案例。相比之下，投资者只需将钱存进银行，就能获得诱人的回报。大型科技企业努力抬升利润率：在零利率政策期间，“基准”回报率为0%。而随着这条基线上涨到5%，具备盈利能力的公司需要进一步拉高利润率才能维持其估值。也就是说大型科技企业会更积极的削减成本，以确保自己的利润率更加健康，哪怕他们的利润已经相当可观。就业市场恶化：随着大型科技企业裁员，加上初创公司能够吸纳的人力越来越少（因为注入的风险投资资金缩水），市场能够提供的工作岗位也愈发稀缺。</p><p></p><p>让我们将以上结论跟过去两年的变化进行比较。下图所示，为利率提升后逻辑推断得出的影响，再跟我们截至目前观察到的实际情况进行比较：</p><p></p><p><img src="https://static001.geekbang.org/infoq/9d/9dd00cfeee2025ddb9d937fb0b7af207.jpeg" /></p><p></p><p>很明显，二者几乎完全相同！这可能出乎大家的预料，但利率上升确实能够解释科技市场上出现的诸多趋势。</p><p></p><h2>两波技术革命的加成，生成式&nbsp;AI&nbsp;紧随而来</h2><p></p><p>2009年左右开始的利率下调，促使更多风险资本流入初创企业。iPhone于2007年横空出世，两年之后又出现了现已停产的Windows&nbsp;Phone。智能手机改变了消费技术，也成为移动优先型科技企业及产品的催化剂，包括2006年成立的Spotify、2009年出现的WhatsApp、2010年的Instagram、2010年的Uber以及2011年的Snap等数千家企业和产品。</p><p></p><p>就在智能手机出现的同一时期，第一批云服务商也迈出了发展的第一步：</p><p></p><p>2006年:&nbsp;亚马逊云科技2008年:&nbsp;Azure2008年:&nbsp;Google&nbsp;Cloud</p><p></p><p>云服务商能够帮助初创公司以更快的速度、更低的成本完成产品开发。这些客户无需自行购买及运营本地服务器，而只需轻松租用云端虚拟服务器。如果需要更多容量，直接付费即可享受资源扩展。早期亚马逊员工Joshua&nbsp;Burgin（现担任VMWware公司工程副总裁）在《现代后端实践的过去和未来（The&nbsp;past&nbsp;and&nbsp;future&nbsp;of&nbsp;modern&nbsp;backend&nbsp;practices）》一书中对此做出了描述：</p><p></p><p></p><blockquote>这波云转型使得Netflix、Lyft以及Airbnb等早期亚马逊云科技客户得以获取与老牌科技巨头相同的计算容量，为这些尚处于起步阶段的初创公司带来了巨大推力。无需采购订单、长达数月的交付周期以及庞大的IT部门或者主机托管服务商，后发团队们只要输入信用卡号就能立即开始使用！</blockquote><p></p><p></p><p>如今亚马逊云科技最知名的客户包括Netflix、Airbnb、Stripe以及Twitch。在云计算的加持之下，每一家公司都能发展得更快，同时将前期资金投入控制在极低的水平。</p><p></p><h4>技术革命与利率环境两相作用</h4><p></p><p>智能手机与云计算革命，还几乎与随后十年间长期保持近零利率的金融市场实现了完美契合：智能手机与云计算革命中的多个关键事件，都与利率变化有关。</p><p></p><p><img src="https://static001.geekbang.org/infoq/51/51ff3ed92de508afcbf47be8d1b4f821.jpeg" /></p><p></p><p>种种宏观因素，为风险投资公司助力初创企业提供了更为有力的理由：</p><p></p><p>以移动端为优先的全新初创公司类别开始出现，只要他们行动迅速就完全有希望成为价值数十亿美元的大厂。这时候筹集大量资金对其赢得市场自然至关重要，Uber和Spotify就凭借这一策略取得了成功。初创公司可以使用云服务来解决设施扩展问题，而不必耗费数年时间慢慢搭建自己的基础设施，从而有效将投资转化为业务增长。这也是风险投资方帮助初创企业赢取市场份额的一种重要方式。</p><p></p><p>2010年代可以说是科技初创公司的黄金时代，因为这是有史以来最漫长的零利率时期，并在期间激发了两波技术革命。</p><p></p><p>如今，另外一场潜在技术革命正在酝酿：生成式AI与大语言模型（LLM）。AI革命与云计算革命具有相似之处，因为AI同样有助于提高效率（只要AI成本能够在当前的基础上不断下降）。</p><p></p><p>然而，AI革命的性质却与智能手机革命截然不同：因为AI似乎并没能像智能手机那样，为应用程序开发商们提供新的、初期免费且足够广泛的分销渠道。另外，生成式AI革命的起点发生在高利率环境之下。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ed/ed66693d23287f6d49a06d39664de50d.jpeg" /></p><p></p><p>生成式AI革命（ChatGPT的发布等）发生在高利率环境当中</p><p></p><h2>软件工程师们面临新的现实</h2><p></p><p>那么，我们正面对着怎样的“新的现实”？软件工程实践将迎来怎样的改变？</p><p></p><p>概括来讲，软件工程师们越来越难找到工作，职业发展速度也有所减缓，晋升更难了。</p><p></p><p>从2020年开始到2024年的图表显示，在美国、英国和德国的Hacker&nbsp;News和Indeed上的职位列表，工作发布数量在2021年和2022年达到峰值，现在几乎回到了2019年和2020年的水平。与两年前相比，现在可供申请的职位更少，而且职位的申请者也更多。</p><p></p><p>例如，美国亚利桑那州一家初创公司Supply&nbsp;Pike的的CTO透露，他们的实习申请从去年翻了一番，软件工程申请从去年增加了三倍。来自Facebook、Google等大公司的申请人数也增加了，高级工程师不再四处面试、货比三家，而是接受第一个抛来橄榄枝的职位。</p><p></p><p><img src="https://static001.geekbang.org/infoq/57/57c3ce9204d4e52510a35f22a46a50f4.jpeg" /></p><p></p><p>Dominic&nbsp;Jack&nbsp;Weston&nbsp;是创始人守护者论坛成员，他曾向创始人们提问：“你们公司是否有很多人离职？”50%的创始人表示没有变化，但另外50%表示他们看到离职人数明显减少。由于离职的开发人员减少，因此招聘也相对减少，因为需要填补的职位变少了。</p><p></p><p>我们作为软件工程师需要预期的另一件事是职业增长变慢。这是因为当招聘减少，无论是因为职位填补减少还是公司增长放缓，对于高级职位的需求就减少了。如果你有一个10人的团队，并且资历分布合理，那么就不需要那么多的技术主管或高级工程师。从预算的角度来看，我们直到现在才意识到这一点。</p><p></p><p>一些公司正积极适应晋升机会减少的新局面，Shopify就是一个很好的例子。</p><p></p><p>之前Shopify有设定了C1至C10的职级，其中C6级别代表高级软件工程师，C7可能是资深工程师，员工通常会从C4或C5逐步晋升至C6。</p><p></p><p>现在他们引入了一种新的职业发展机制——从0到50的掌握分数，员工有两个途径来推动自己的职业发展：一是提升自己的掌握分数，如从C7的30分提高至40分，这不仅意味着薪资增长，还伴随着奖金和其他福利；二是转换角色范围，比如从C7的团队领导变为C8的领域领导，尽管这可能导致掌握分数的下降，但公司强调这是一项不同的职位，员工需要根据自己的意愿做出选择。</p><p></p><p><img src="https://static001.geekbang.org/infoq/31/31bd4a204e1a1ff864f7a3d5e1cefcc0.jpeg" /></p><p></p><p>Shopify的这种策略非常明智，解决了在一些公司中可能四五年都没有晋升机会的问题。在Shopify，员工可以期待每半年就获得掌握分数的提升，以及随之而来的小额奖金和认可，这无疑会提高员工的满意度和忠诚度。我相信这种创新的做法会被更多具有前瞻性的公司采纳，从而为软件工程师带来更多的职业发展机会和工作动力。</p><p></p><p>另外，当处于资金更少、限制更多、更注重效率的时代，我们开始需要选择更“无聊”的技术来完成工作（Choose&nbsp;simple&nbsp;/&nbsp;boring&nbsp;technology）。虽然新技术通常有更好的表现，可以绕过一些局限，并且代表了“未来”，但实际上越来越多的人为了解决业务问题而选择了简单的旧技术。</p><p></p><p>这可能不是我们的偏好，但我们将面临来自非技术人员，特别是商业领导层的更多压力。单体式架构将变得更受欢迎，“全栈”和Typescript的扩张势头更猛，也有更多责任开始“左移”到开发者身上。</p><p></p><p>为了展示与过去几年技术选择有多大差异，让我们看看在零利率时期，技术决策是如何做出的。微服务方面，Monzo可能是继Uber之后规模最大的公司之一，他们有500名工程师管理着数千个微服务，每个工程师负责多个微服务。在2015年的零利率时期，Monzo的创始人兼CEO在公开会议上以幽默的方式概述了他们的成功步骤：第一步采用Go语言，第二步实现微服务架构，第三步和第四步则是未知和盈利。如果这真的是他们的决策逻辑，那确实引人深思。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8a/8a1705563b4f5f120b47774bcea610f5.jpeg" /></p><p></p><p>我欣赏他们分享这种策略的勇气，因为不仅仅是Monzo，很多公司的CTO甚至CEO都认同这种思维方式，因为那时即使资金充足，吸引软件工程师加入也是非常困难的，你需要提供一些有吸引力的东西来招募他们，比如酷炫的技术。</p><p></p><p>通过与许多初创公司的对话，我认为单体架构正在回归，再次变得流行。目前的趋势是，从一开始建立单体架构并持续使用它，Shopify一直是这方面的先行者。如果预见到在未来几年内团队规模将迅速扩大，微服务架构可以解决组织扩张带来的问题；但如果没有这种扩张，可能并不需要微服务，单体架构就已足够。</p><p></p><p>同时，全栈开发的浪潮正汹涌而来，我们经常听到全栈的概念，现在在各个地方都看到它的身影。以一个典型的小团队为例，他们负责开发iOS、Android、Web和后端。团队可能包括两名后端工程师、一名Web工程师、一名iOS工程师和一名Android工程师。然而，当你采用React&nbsp;Native、Flutter或其他跨平台技术时，你实际上可以拥有三种类型的全栈工程师，他们可以分别侧重于后端、Web开发以及移动开发。</p><p></p><p>尽管存在一些争议，但你可以使用这些技术生成功能相似的iOS、Android和Web应用程序，同时拥有一个更小、沟通更少团队，从管理者的角度来看，这意味着更低的预算。</p><p></p><p>在这个转变中，TypeScript起到了重要的助力，许多近年来成立的初创公司都谈到了TypeScript如何使全栈开发成为可能。我不想只强调TypeScript，但拥有一种技术，让你的工程师能够在整个技术栈中发挥作用，这确实是非常有价值的。</p><p></p><p>例如，Blue&nbsp;Sky公司几乎在所有层面上都使用TypeScript，包括后端、前端以及移动应用，结合React&nbsp;Native和Expo，使得他们所有的工程师都能够处理任何部分的代码。同样，由我Uber的前同事Thomas&nbsp;Arkman创立的Linear流行管理解决方案公司，从后端到前端也在使用单一语言类型，并表示这极大地服务了他们的技术需求。</p><p></p><h3>不同于互联网泡沫破灭时期</h3><p></p><p>变化往往在我们不经意间突然降临。经历过2001年互联网泡沫破灭的朋友，肯定会感觉如今的科技市场衰退就如同是当年那场噩梦的重演。在那段时期，软件工程师们最关注的就是在岗位缺乏保障、人生即将失控的焦虑之下，该获取哪些职业方向和战术性应对策略。以下是两位亲历者的故事：</p><p></p><p>2001年从美国计算机科学专业毕业的Niia&nbsp;Henry回忆说，“我毕业时就被裁员了，我的大多数朋友的初创公司都失败了，我们都在拼命找工作。我们不得不与同样被裁员的资深工程师竞争岗位，每天都会听到大型初创公司倒闭的消息，这真的很令人沮丧。为了积累简历上的经验，我们接受任何兼职或无薪实习的机会。”之后，她从软件工程转向网站开发的咨询工作，最终成为技术项目经理。在五六年的时间里，她不得不在非技术岗位上工作，因为那是她能找到工作的唯一方式，后来她重新回到了软件工程领域。现在她现在已是Spotify的总监了。</p><p></p><p>另一位在网络泡沫时期毕业的谷歌员工分享他的教训说：“20年后，无论你在哪家公司工作，都要确保自己处在收入一方，确保公司是在销售软件，而不是用软件来销售其他东西。”这是一种说法，如果你在利润中心工作，你的工作在注重效率的时代更安全。</p><p></p><p>Kent&nbsp;Beck是一位在2000年代和现在都工作了20年的资深人士，在被问及他所看到的相似之处时提到了一些有趣的现象：固定规格和前期设计重新流行起来，迭代开发变得不那么频繁；开发者之间有更多的交接，比如将工作交接给QA或其他人；还有更少的频繁部署到生产环境，以及更长的反馈周期。</p><p></p><p>但2024年与2000年的不同之处在于，互联网当时正在迅速增长，今天人工智能和大型语言模型正在迅速增长。到2023年年中，所有风险投资中有25%进入了人工智能。并且，2000年的技术行业要小得多。2003年，七家最大的公开交易技术公司中，Google排在第四位，Microsoft和Apple分别是第六和第七，其他的是非技术公司。而在2024年，前四名都是技术公司，实际上七个中五个是技术公司。技术行业已经变得非常庞大，无处不在，我认为唯一的问题是它会变得有多大，它仍在增长，但某个时候增长会放缓。</p><p></p><h2>给软件工程师的真诚建议</h2><p></p><p>我们已经看到，当公司裁员时，会受到影响的更多是执行者而不是规划者。软件工程经理往往成为主要的裁撤对象，因为团队结构正在被精简。我了解到一些初创企业裁掉了大量中层管理人员或工程总监，但他们没有触及软件工程师，因为这些是实际做事的人。因此，对于工程经理来说，成为实际动手实践的人，而不是被视为成本中心的规划者，变得越来越重要。</p><p></p><p>由于找工作变得更加困难，你需要为此做好更充分的准备。以下是一些建议：人脉网络极其重要，无论你是否已经拥有，如果有要利用它，如果没有就要努力构建它。推荐变得更加关键，因为求职申请众多，有时只有前50份才会被真正查看。在面试中投入无薪时间，如带回家作业，将成为常态，这正在变成一种数字游戏，你需要向很多地方申请。</p><p></p><p>还有一个有极大帮助的事情是培养商业或产品思维，这意味着要理解你所在公司的商业模式——他们是如何盈利的、是否有盈利能力、能否在某个时候盈利。建立与产品经理的关系，他们应该非常擅长理解这些并找出如何转变它；然后与工程领域之外的人员交流，如客户支持和研究部门，提出你可以帮助公司的想法。能够帮助公司赚钱或省钱的软件工程师非常有价值，我认为大多数公司都会寻找这样的人才。</p><p></p><p>最后一点是，在当前环境下，我认为我们都应追求职业安全而非职位安全。引用一位评论者的话：“我们总是在为可能的裁员做准备。被裁员的经历教会了我，没有工作是安全的，无论是作为管理者还是员工，我都不能承诺或期望工作安全。我们能做的，也是我们所有人都能做的，就是拥有职业安全——不断学习，持续在公司从事挑战性项目，持续与优秀的人合作，这是你能做的最多的事情。”</p><p></p><p>AI会不会取代软件工程师，这听起来似乎是一个很极端的问题，但我认为这种情况不太可能发生。实际上，我们正在经历的是，软件工程师的工作范围将变得更加广泛，尽管公司仍将保留安全工程师等一些特定的专业角色，但这些特殊职位的数量将会减少，可能在每50到100名软件工程师中只会有少数几个这样的角色。</p><p></p><p>这意味着，虽然专业角色依然重要，但我们会看到更多的工作被集成到软件工程师的职责之中。</p><p></p><p>但我认为，AI使开发者使用工具更加高效。当我使用像Copilot或ChatGPT这样的工具进行头脑风暴、模式识别或者学习不太熟悉的技术时，真的有很大的不同。我的建议是，让这些工具成为你的盟友，理解它们的工作原理；尝试Copilot、Cody或其他任何你能找到的AI工具，形成自己的看法，改善你的工作流程；亲自看看哪些有用，哪些没用。</p><p></p><h2>结语</h2><p></p><p>有时候，为了更好地理解科技行业的变化，我们不妨退后一步，以总览的姿态尝试理解一切。零利率的终结是个标志性事件，对于科技行业产生了重大冲击。尽管我们很难预测未来，特别是对于瞬息万变的科技行业来说，但我发现从驱动行业发展的背后力量入手反而会得到有益的启发。</p><p></p><p>从某种程度上讲，科技行业的发展史就是一部周期性繁荣与萧条的血泪史。创新是全新商业机遇的沃土，毫无疑问科技行业还会迎来更多繁荣时刻，大家唯一要做的就是在机会来临时把握住它！</p><p></p><p>“用足球来打比方：大多数球员都盯着球当前所在的位置，而我想朝着球飞向的位置提前补位。我觉得自己对科技行业这颗‘球’在未来几年的行进方向有了更多了解，所以能够更好地定位自己。”</p><p></p><p>原文链接：</p><p></p><p><a href="https://newsletter.pragmaticengineer.com/p/what-is-old-is-new-again">https://newsletter.pragmaticengineer.com/p/what-is-old-is-new-again</a>"</p><p></p><p><a href="https://www.youtube.com/watch?v=VpPPHDxR9aM">https://www.youtube.com/watch?v=VpPPHDxR9aM</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ONzGRMNCpkM5mVAYbjQi</id>
            <title>龙盈智达、中国银联、富滇银行、平安产险等确认出席FCon，分享金融数智化运营与营销创新实践</title>
            <link>https://www.infoq.cn/article/ONzGRMNCpkM5mVAYbjQi</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ONzGRMNCpkM5mVAYbjQi</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jul 2024 04:30:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 金融, 科技, 金融科技, 技术驱动
<br>
<br>
总结: 金融机构实现高质量发展需要结合科技和金融，实现技术驱动业务转型，但仍需克服最后一公里的障碍。2024年FCon全球金融科技大会将分享数字化转型和技术赋能的经验，展示金融数字化在“十四五”期间的关键进展，以及金融场景创新的实践和探索。演讲内容涵盖卡组织数字化营销实践、商业银行运营营销的挑战与路径，以及构建智能营销生态的案例和经验分享。通过分享和交流，帮助金融机构更好地应对数字化转型带来的挑战，实现业务增长和创新。 </div>
                        <hr>
                    
                    <p>从“金融”和“科技”，到“金融科技”，是金融机构实现高质量发展的必答题。但从实践来看，行业离全面释放技术要素，兑现技术驱动业务转型的价值潜力，还差最后一公里。</p><p></p><p>具体如何完成最后这一步跨越？在8月16日-17日即将于上海举办的<a href="https://fcon.infoq.cn/2024/shanghai/">2024年FCon全球金融科技大会</a>"上，龙盈智达（北京）科技有限公司副总裁宫小奕将分享<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6043">股份制银行数字化转型和技术赋能过程中的“道与术”</a>"，结合前沿技术凝结场景驱动的关键经验，分享如何以场景驱动业技融合，让技术能力深刻融入业务逻辑，让业务依托技术变革经营模式。</p><p></p><p>2024年FCon大会由中国信通院铸基计划作为官方合作机构，以“科技驱动，智启未来——激发数字金融内生力”为主题。在“十四五”收官之际，本届大会将致力于展示金融数字化在“十四五”期间的关键进展，帮助金融机构更具针对性地“查缺补漏”。</p><p></p><p>聚焦金融场景创新，大会特别策划了「<a href="https://fcon.infoq.cn/2024/shanghai/track/1689">金融数字化营销实践</a>"」和「<a href="https://fcon.infoq.cn/2024/shanghai/track/1690">金融数字化管理和运营实践</a>"」专题论坛，来自中国银联、富滇银行、平安产险、平安壹钱包、申万宏源证券、度小满等金融机构的专家将在现场分享各自业务领域的探索和实践。</p><p></p><p>上周上新精彩议题如下：</p><p></p><h4><a href="https://fcon.infoq.cn/2024/shanghai/presentation/6039">卡组织数字化营销实践</a>"</h4><p></p><p></p><p>演讲嘉宾：马永松&nbsp;中国银联智能化创新中心高级总监</p><p></p><p>卡组织营销面临业务角色多（银行机构方、商户方、钱包方、行业APP方等）、场景广泛、链条长等特点，如何发挥好卡组织的核心纽带作用，在多边业务模式中兼顾各方的经营特点与利益，并使用数字化手段更好、更快、更优的服务好卡组织体系中的各参与方，是银联目前面临的一项艰巨挑战。银联从整个生命周期管理角度，从事前、事中、事后3个阶段通过营销策划、客群圈选、活动标签、辅助调优、一站式业务指标监控、黄牛侦测与拦截、客诉诊断、事后复盘等数个环节针对营销的运营进行分解、落地。我们期待将一路走来的解决方案与实践经验，与大家进行面对面的分享与交流。</p><p></p><p>演讲提纲：</p><p>卡组织营销面临的挑战卡组织四方模式与营销挑战潜在解决思路银联数字化营销体系介绍对“数字化”的理解生命周期管理体系介绍技术类特点场景实践一站式用户筛选案例联动商户开展精细化营销活动案例联动银行开展精细化营销活动案例未来展望生命周期运营管理流程自动化从数字化到数智化卡组织网络价值的进一步延伸</p><p>听众受益：</p><p>卡组织营销模式的特点与挑战银联在数字化转型过程中面临的痛点和解决方案面向银行和商户的实践经验卡组织营销模式面向未来的演进方向</p><p></p><h4><a href="https://fcon.infoq.cn/2024/shanghai/presentation/6048">数智化时代商业银行运营营销的“坑”与“路”</a>"</h4><p></p><p></p><p>演讲嘉宾：李涛&nbsp;富滇银行数字金融中心副主任</p><p></p><p>在银行数字化转型的浪潮中，构建企业级数字化运营营销体系已成为行业焦点。其不仅需要构建强大的平台能力，更关键的是构建企业级的数字化业务运营营销体系。尽管银行的数字化营销体系借鉴了主流电商的模式，但由于受到监管和行业特性的影响，银行在实际运营中无论是在流程、合规、技术和风险控制等方面都面临很多的挑战。人工智能技术快速发展，大模型成为金融行业创新的重要发展驱动，本议题将介绍中小银行如何将PPT上的大模型应用用于银行实践，拆解其落地路径与大型银行的异同。</p><p></p><p>演讲提纲：</p><p>富滇银行数字化运营营销体系数字化运营营销的“坑”与“路”自我拷问什么是银行数字化营销和运营银行公私域运营模仿互联网电商可持续吗？点、线、平面、立体化的营销策略演进口碑到品牌的演进北极星指标是个坑吗？大而全的指标标签体系真的能赋能银行数字化营销吗？活动为什么从&nbsp;“高频”&nbsp;到&nbsp;“低频”&nbsp;到&nbsp;“固定频率”&nbsp;再回到“高频”？用户触达“陷阱”与出路公域saas运营和私域运营如何有效结合，公域流量向私域AUM转换的出路是什么？舆情事件的“坑”与机遇人工智能在富滇银行运营实践</p><p>听众受益：</p><p>企业级运营、营销系统建设架构如何构建企业级的数字化运营营销体系，覆盖数据采集、分析、决策全流程引入先进AI技术，实现个性化营销与精细化运营银行运营营销模式与演进经验提炼自富滇银行三年多的实践，展现数字化营销与运营模式的迭代升级路径从基础数据整合到智能决策支持，逐步深化数字化能力，提升客户体验与运营效率银行运营营销运营实践中遇到的挑战与对策分享在数字化转型中遭遇的常见障碍，如数据管理难题、技术整合壁垒等提出针对性解决方案，包括强化数据治理、优化技术栈、培养复合型人才等策略中小银行大模型应用实践探讨大模型在中小银行运营营销中的实际应用案例，突出差异化落地路径中小银行如何利用有限资源，有效整合大模型，赋能业务创新与增长</p><p></p><h4><a href="https://fcon.infoq.cn/2024/shanghai/presentation/6041">构建智能营销生态，让增长自然发生</a>"</h4><p></p><p></p><p>演讲嘉宾：王洪志&nbsp;中电信翼金智慧营销研究院院长&nbsp;/某大型国有银行省行首席增长官</p><p></p><p>演讲提纲：</p><p>洞察新名词背后的瓜葛数字经济数字中国5篇大文章新质生产力银行数字化转型银行数字化营销转型实践案例国有大行省分行从倒数第二到正数第二的蜕变！做对了什么？城商行零售板块全线指标超额增长！内在逻辑是什么？解构银行数字化营销转型实践什么是数字化转型，统一思想认识为什么做数字化转型，全员统一行动“遥遥领先”的数字化转型的理论架构“暴力”拆解数字化转型的实践案例银行数字化转型实践痛与悟转型中组织升维转型中文化升维转型中科技升维转型中人才升维</p><p>听众受益：</p><p>深入剖析银行数字化转型带来的冲击，引导把握银行数字化营销转型制胜关键深入剖析数字化转型战略与路径、数字化转型企业架构搭建、数字化运营与流程重构、数字化营销应用架构、数据架构等关键实现路径深入讲解数字化营销运营的场景搭建和运营方式，现场感受数字化营销运营的倍增能力理论联系实战案例，深度拆解银行数字化转型成功链路，真实场景还原：“接触互动-深度交流-项目启动-战略规划-方案执行-数据分析-策略输出-营销运营-过程监控-结果复盘”</p><p></p><h4><a href="https://fcon.infoq.cn/2024/shanghai/presentation/6045">“治理即应用”：产险数据治理体系探索及实践</a>"</h4><p></p><p></p><p>演讲嘉宾：洪广智&nbsp;平安产险客户大数据团队平台组负责人</p><p></p><p>数据治理在产险公司的数字化转型过程中扮演着至关重要的角色。一方面产险在以“数据标准化”的治理思路，分别在业务、技术和数据三端逐步推进各项工作的落地，夯实数据基础，确保数据准确性和一致性；另一方面秉承“治理即应用”的思路，在“字段”治理的基础上，通过统一ID体系，串联客户全域数据，构建统一的客户画像，解决“客户是谁、做了什么”的问题，并通过“产品化”和算法加持，赋能业务场景，体现数据治理价值。</p><p></p><p>演讲提纲：</p><p>在数字化转型过程中产险数据治理的痛点分析基于业务场景的数据数据治理的实践效果与大模型应用探索数据标准化---开启大数据治理之路大模型应用---探索数据治理新范式从数据治理到赋能业务转变实践探索及业务案例分享构建底座：从“字段”治理延展到围绕“客户”的数据治理，构建统一客户数据视图。案例1：数据产品化--全域客户画像的构建及应用效果案例2：数据应用--基于客户数据的非车策略仿真预测，提升客户转化率</p><p>听众受益：</p><p>从业务应用视角，了解财产保险行业的数据治理模式规划及落地步骤了解数据治理成果如何赋能业务应用案例，提升数据治理的价值</p><p></p><p></p><h4><a href="https://fcon.infoq.cn/2024/shanghai/presentation/6005">基于因果推断的智能经营模型体系</a>"</h4><p></p><p></p><p>演讲嘉宾：李东晨&nbsp;度小满数据智能经营模型负责人</p><p></p><p>在企业决策中，如何在资源有限的情况下提升经营效率，从而达到增长最大或盈利最优的目标，是每个企业持续努力的方向，其中的关键点是如何将资源投入到带来增量更大的地方。传统的机器学习/深度学习适合解决预测问题，而因果推断则直接能够学习到一个经营动作开展后带来的增量，更适合应用于企业决策。相对传统互联网场景，个人信贷存在较强的领域特性，因此在信贷核心的offer优化方向，我们提出了业界领先的因果经营模型体系，并在业务中应用落地，带来了较好的业务收益。</p><p></p><p>演讲提纲：</p><p>经营框架介绍：经营模型框架通常包含哪些模块因果模型实战：如何通过因果推断技术提升经营效率案例分享：因果offer体系在信贷领域落地案例</p><p>听众受益：</p><p>了解信贷领域的经营模型框架体系从预测到决策，因果推断技术能更好地支撑企业决策优化问题从营销到盈利，因果推断可以支撑所有资源有限情况下的最优求解问题</p><p></p><p>截止目前，大会已上线34个演讲议题，更多议题可进入FCon&nbsp;全球金融科技大会官网查看：<a href="https://fcon.infoq.cn/2024/shanghai/">https://fcon.infoq.cn/2024/shanghai/</a>"。来自工银科技、北京银行、平安银行、广发银行、中信银行蚂蚁集团等金融机构及金融科技公司的资深专家将现身说法分享其在金融科技应用实践中的经验与深入洞察。目前大会已进入9折优惠期，单张门票立省&nbsp;480&nbsp;元（原价&nbsp;4800&nbsp;元），欢迎点击链接或扫码查看了解详情：</p><p><img src="https://static001.geekbang.org/infoq/fe/fe88b78623d830bffc29a1edf7bf4896.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/LxsPR3YosPcpd6vuK8Dm</id>
            <title>超越 Transformer 与 Mamba，Meta 联合斯坦福等高校推出最强架构 TTT</title>
            <link>https://www.infoq.cn/article/LxsPR3YosPcpd6vuK8Dm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/LxsPR3YosPcpd6vuK8Dm</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jul 2024 01:18:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 机器学习模型, RNN, 自注意力机制, TTT层
<br>
<br>
总结: 近日，斯坦福、UCSD、UC 伯克利和 Meta 的研究人员提出了一种全新架构，用机器学习模型取代 RNN 的隐藏状态。这个模型通过对输入 token 进行梯度下降来压缩上下文，这种方法被称为「测试时间训练层（Test-Time-Training layers，TTT）」。作者提出了一种具有线性复杂度和表达能力强的隐藏状态的新型序列建模层。论文中提出了两种实例：TTT-Linear 和 TTT-MLP，它们的隐藏状态分别是线性模型和两层 MLP。TTT 层在理论上和实验评估中表现出色，尤其是在长上下文处理和硬件效率方面。 </div>
                        <hr>
                    
                    <p></p><p>近日，斯坦福、UCSD、UC 伯克利和 Meta 的研究人员提出了一种全新架构，用机器学习模型取代 RNN 的隐藏状态。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/11/11a12184c6b8b9f659243ab02ec7295d.png" /></p><p></p><p>这个模型通过对输入 token 进行梯度下降来压缩上下文，这种方法被称为「测试时间训练层（Test-Time-Training layers，TTT）」。该研究作者之一 Karan Dalal 表示，他相信这将根本性地改变语言模型方法。</p><p></p><p>自注意力机制在处理长上下文时表现良好，但其复杂度是二次的。现有的 RNN 层具有线性复杂度，但其在长上下文中的表现受限于其隐藏状态的表达能力。随着上下文长度的增加，成本也会越来越高。</p><p></p><p>作者提出了一种具有线性复杂度和表达能力强的隐藏状态的新型序列建模层。关键思路是让隐藏状态本身成为一个机器学习模型，并将更新规则设为自监督学习的一步。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/48/48ea262cabaa3ff0d973bff7158fd874.png" /></p><p></p><p>论文中提出了两种实例：TTT-Linear 和 TTT-MLP，它们的隐藏状态分别是线性模型和两层 MLP。团队在 125M 到 1.3B 参数规模上评估了实例，并与强大的 Transformer 和现代 RNN Mamba 进行了比较。结果显示，与 Mamba 相比，TTT-Linear 的困惑度更低，FLOP 更少（左），对长上下文的利用更好（右）：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/6d/6d1b866c95df78c416d0c3804123fbc1.png" /></p><p></p><p>这个结果代表了现有 RNN 的尴尬现实。一方面，RNN（与 Transformer 相比）的主要优点是其线性（与二次型）复杂性。这种渐近优势只有在长上下文的实践中才能实现，根据下图，这个长度是 8k。另一方面，一旦上下文足够长，现有的 RNN（如 Mamba）就很难真正利用所依赖的额外信息。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/98/98dbf1684098cc3b25fa44d7a267a3d1.png" /></p><p></p><p>并且，大量的实验结果表明：TTT-Linear 和 TTT-MLP 都匹配或超过基线。与 Transformer 类似，它们可以通过限制更多的代币来不断减少困惑，而 Mamba 在 16k 上下文后则不能。经过初步的系统优化，TTT Linear 在 8k 环境下已经比 Transformer 更快，并且在 wall-clock 时间上与 Mamba 相匹配。</p><p></p><p>TTT 层在理论上和实验评估中表现出色，尤其是在长上下文处理和硬件效率方面。如果在实际应用中能够解决一些潜在的工程挑战，如大规模部署和集成问题，工业界对 TTT 层的接受度也将逐步提升。</p><p></p><p>论文链接：<a href="https://arxiv.org/pdf/2407.04620v1">https://arxiv.org/pdf/2407.04620v1</a>"</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/8u81YeTiZrw8BE0XXXmM</id>
            <title>微软中国CTO韦青：亲身经历大模型落地的体会与思考 | QCon</title>
            <link>https://www.infoq.cn/article/8u81YeTiZrw8BE0XXXmM</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/8u81YeTiZrw8BE0XXXmM</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Jul 2024 14:04:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 演讲嘉宾, 技术浪潮, 看不见的大猩猩, 企业增长
<br>
<br>
总结: 在技术浪潮中，企业增长过程中存在着一些被忽视的关键问题，这些问题就像企业内部的“看不见的大猩猩”，可能成为发展的“卡点”。演讲嘉宾通过分享自身经验，提出解决这些问题的方法。 </div>
                        <hr>
                    
                    <p>演讲嘉宾 | 韦青 微软（中国）公司 / 首席技术官</p><p>编辑 | 蔡芳芳 傅宇琪</p><p></p><p>在大模型、AIGC 的冲击下，大多数人把目光聚焦在技术浪潮上，聚焦在那些“容易解决”的问题上，但实际上企业增长过程中还存在很多显而易见的、需要解决的、关键的问题，这些问题就像“看不见的大猩猩”一样存在于企业之中，这些问题很可能成为企业快速发展的“卡点”。</p><p></p><p>微软中国 CTO 韦青在 2024 年 4 月举办的 QCon 北京发表的<a href="https://qcon.infoq.cn/2024/beijing/presentation/5873">《看不见的大猩猩——智能时代的企业生存和发展之路》</a>"的主题演讲中，结合自身经验，聚焦企业内部这些被忽略的“大猩猩”，分享关键问题的解决之道。本文由 InfoQ 整理，经韦青老师授权发布。</p><p></p><p></p><blockquote>InfoQ 将于 8 月 18-19 日举办 AICon 上海站，我们已经邀请到了「蔚来创始人 李斌」，他将在主论坛分享基于蔚来汽车 10 年来创新创业过程中的思考和实践，聚焦 SmartEV 和 AI 结合的关键问题和解决之道。更多精彩议题可访问官网了解：<a href="https://aicon.infoq.cn/2024/shanghai/track">https://aicon.infoq.cn/2024/shanghai/track</a>"</blockquote><p></p><p></p><p></p><h2>思想的力量</h2><p></p><p></p><p>能够把一件事情做成功不容易。在实现的过程中，会有很多局限。在这个新生事物层出不穷的时代，有一个常见的根本性局限，就是人的思想局限，体现为对事实真相辨析不明和经常用旧的逻辑尝试解决新的问题。</p><p></p><p>在这个世界上，对于事物真相的判断，存在着事实、现象和观点这三个不同的维度。</p><p></p><p>获取大家公认的事实非常困难，理论上讲是不可能的。因为每个人都是通过自己的“有色眼镜”，也就是感觉器官来观察事物，得到的是感觉器官所感受的现象。人们会基于个人经历、背景和认知偏差对所观察到的现象进行解读，从而形成自己的观点。由于观点都带有主观色彩，因此既代表现象，也不代表事实，只可用作讨论的素材。人们有可能从基于观点的辩论，而达成对于现象和事实的一致意见。</p><p></p><p>如果你看过电影《肮脏的哈里》中的演员克林特·伊斯特伍德，他在该片中有一句著名的台词：“观点就像&lt;人体的排泄出口&gt;，每个人都有。（都有怪味，但每个人都认为自己的比别人的好闻）”。</p><p></p><p>理解了这个道理之后，我们在听到社会上的某种流行说法时，先要明确这是某种观点，还是大家已经达成共识的现象，以及它背后所指向的事实大致是个什么样子。不要过早地受观点的影响，以避免陷入“看不见的大猩猩”的陷阱，被信息误导，只关注到媒体让我们看到的事物，而不是正在真实发生的事物。例如，现在我们在网上、在朋友圈里看到的许多现象，很可能都是一些具有引导性的关注点。但真正的事实是什么呢？我们需要超越表面现象，深入探究事物的本质。</p><p></p><p>在技术实践中，我发现人的问题其实是最复杂的。虽然技术难题很难攻克，但技术毕竟是人创造的。一旦我们的思想方法出现问题，那么无论多么优秀的人，再怎么努力，其做事的结果也非常有限。但是思想转型是最难的，要改变思想，我们需要采用成长型思维的方式，不断学习，不断纠偏。这意味着我们必须认识到，我们的思想都是有偏差的，我们常常错误地将所观察到的现象视为事实的全部，并迅速形成一种观点，认为别人是错的，自己是对的。在这个时代，还没有人能够爬上山顶，看到未来。在这个不确定性主导的世界里，未来只要还未发生，对其判断就是一个概率问题。</p><p></p><p>人工智能的实现依靠计算机器基于数据而学习，数据的问题就像是一个房间里的大象，也可以说是看不见的大猩猩。说它是一头房间里的大象，是说这个问题很明显，但是很麻烦，大家都不愿意主动指出来。针对优质数据的积累，它无法简单地靠堆砌资金和人力，或者只要有海量算力就可以解决，它需要漫长的文明积累，不是所有的数据都具备可以被学习的知识，只有那些能够表征一个文明特征的数据才能够让机器学到代表这种文明的知识与价值观。说它是一头看不见的大猩猩，是说明这个问题经常被媒体所误导，被大众所忽视，人们看到的都是有关算力、算法的探讨，而人工智能的实现是一个复杂的系统工程，各种前提条件缺一不可，就像一个水桶，不管构建水桶的木板有多长，它的存水量由最短的那块板决定。人工智能就像一个孩子，是被数据培养出来的。如果提供给它的数据是有偏差的，那么它的行为和决策也一定是有偏差的。如果数据来自他人，那么训练出的模型的行为和偏好也将是别人的。</p><p></p><p>对于所有企业来说，我认为第一步是显而易见的，今天现场发布的报告《中国生成式 AI 开发者洞察 2024》（后台回复「开发者洞察」即可下载）已经给出了答案。但有时候，我们需要关注的是那些显而易见，但大家不愿意去触及的麻烦。例如，人才问题、数据问题、流程再造问题，这些都是我们所说的“硬核”问题。这些问题并不是多么新鲜或者伟大的问题，它们都有一个共性，就是跟脚踏实地的作风和漫长的积累相关，跟保持独立的思想和不盲从潮流相关，与每个个体与组织愿意耐心花多长时间取得成就相关，它们需要因人、因时、因地制宜，不能简单地复制粘贴。每个个体、每个组织都有自己的数据特征、流程特征、人才储备和资金储备，以及行业特征。</p><p></p><p>我们不仅应该学习别人做事的正确方法，更应该借鉴别人犯错误的教训，而不是一味的想找到所谓的“最佳实践”。因为在探索期间，就像踩雷区一样，或者像查理·芒格所说的——智慧不是在于做对每件事，而是在于知道哪些事情是错的。在一个极度不确定的时代，使用这种方法可能不能保证我们成功。但它能让我们成功的概率稍微高一点，哪怕只是一点点。</p><p></p><p>如果大家仔细观察业界所谓的成功公司，你会发现它们的成功大多都概率性的，只是因为活下来了，是幸存者，这种经验的总结很容易陷入“幸存者偏差”的陷阱。没有人在微软、OpenAI、英伟达、谷歌、华为、阿里、百度、腾讯等公司成立之初就敢说他们一定会成功。成功是因为他们在重要的事情上犯的错误少，只要不死，你生存下来的机率就变大了。因此，不是说不要向成功者取经，一是不必照搬，二是还要看看这些公司没有做什么。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a6/a6762dfadecee7ab16a2c03914102d72.png" /></p><p></p><p>什么才是真正的思想变革？我们每个人的思想实际上都是非常固化的。特别是那些越成功、经验越丰富、过去成就越多的人，他们的思想往往更加难以改变，这是一个公认的事实。</p><p></p><p>在计算机历史上，有一位非常著名且直言不讳的人物，他就是艾兹格·W·迪科斯彻。上世纪，当有人问他计算机是否能够思考时，他回答说：“提问‘计算机是否能思考’就像问‘潜艇是否能游泳’”。他的回答正确与否并不重要，重要的是那一代计算机科学家所展现出的探索精神，他们不受经验的束缚，能够洞察事物的本质，这种精神在当今时代尤为重要。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/d5/d54914db5fd62cb2eae0b148a55fce13.png" /></p><p></p><p>我再举一个与今天更相关的例子，那就是马车与发动机的故事。</p><p></p><p>20 多年前，麻省理工学院第一任人工智能研究室主任西摩尔·派普特提出了一个思想实验，让我们想象，如果一名现代喷气发动机工程师穿越回 19 世纪初，向当时的马车夫和马车行展示喷气发动机，并询问这是否能帮到他们。</p><p></p><p>大家首先想到的可能是将喷气发动机安装到马车上，我把这解读为“AI+”，因为将发动机装到马车上，确实可以让马车比马跑得更快、更省事。但这是否是我们的最终目标呢？绝对不是。我们真正要做的，是因为新工具的出现而重构整个行业，甚至是整个社会的基本原型。</p><p></p><p>从马车到汽车的转变，交通的本质目的没有变，依然是将人和物品安全、可靠、及时、高效地从一点移动到另一点。但是，如果我们用马拉车，我们需要考虑的是在马路上每隔两公里设计一个草料堆与化粪池。而如果是汽车，我们则需要每隔 几十 公里建一个加油站和服务站。无论是加油站还是化粪池，都有它们存在的必要，也都有它们存在的前提条件。都是商机，只不过是不同思想层面的商机。这种不同的思想层面代表了不同的思维范式、工业范式和文明范式。要注意这种种发展方向之间，并没有对错，只是因为不同的人生观和价值观而选择不同的发展道路。这就是不同思想与思维方式的不同结果。</p><p></p><p>今天发布的报告《中国生成式 AI 开发者洞察 2024》已经将这些问题阐述得非常清楚。报告中提到，首先，开发需要有场景，需要理解大语言模型的开发，我将其理解为必须知道我们解决的人类问题是什么。其次，我们需要知道如何使用工具。第三，我们需要找到合适的工具。比如，如果我们要在墙上挂一幅画，大概率我们会用到钉子或螺丝，使用锤子或螺丝刀。而如果我们在工厂里组装一辆汽车，那么我们面临的问题和所需的工具及方法就会完全不同。</p><p></p><p>西摩尔·派普特曾经说过：“如果思想不改变，无论你拥有的新工具有多么先进，它又能改变什么呢？如果这个工具只是被用在马车上的一个特别优秀的引擎，它确实能让马车比马跑得快”。在当前的流程中，我们已经在应用人工智能工具，无论是生成式的还是传统的，各种类型的人工智能工具都在被使用。这使得大家普遍感觉到，人工智能工具似乎有用，但又似乎没有达到预期的效果。我的判断是：虽然目前我们在使用这些工具，但最终，我们的整个社会范式将会被人工智能所改变。这意味着从“AI+”（AI 的简单添加）到“AI 化”或“AI 乘”（AI 的深度融合和乘数效应）的转变。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/00/008367c55ce6904aff15fe12babf9379.png" /></p><p></p><p>我们当前所有的话语体系都在讨论应该做些什么，进行什么样的设计，包括人工智能的生成式应用。这就引出了一个问题：人工智能是否仅限于生成式应用？传统人工智能是否已经没有价值？我们其实并不关心它具体是哪种类型的人工智能，只要它能够帮助我们解决问题，通过社会架构的重新构建，它就能成为一个新的工具。</p><p></p><p>理解了这个道理之后，我们可能会认识到，在做事的过程中，一方面我们需要跟随，但我们必须记住，仅仅跟随是永远没有未来的。大多数情况下我们所面对的都是概率问题。未来的一切都与概率相关，无论是贝叶斯思维还是蒙特卡洛方法。想象一下，如果我们在座的所有人都去追求同一个目标，采用同一种范式，那么我们成功的概率是多少？或者更具体地说，你能成功的概率是多少？</p><p></p><p>无论工具多么先进，即使喷气式引擎研发得再好，如果人们想到的只是将其安装在马车上，那么我们很难找到新的出路。那么，汽车到底会是什么样子？整个社会形态将如何变化？我们是否需要建立加油站、铺设柏油马路、设置收费站？是否需要有人开始研究整个统筹学、运筹学、算法优化等科学领域，以把握先机并引领变革？</p><p></p><p>要知道我们真正需要找到的是解决某种问题的方法。那么我们的问题是什么呢？人工智能的方法很多，可以是生成式的人工智能，也可以是传统的人工智能，还可以具身的人工智能。当我们要解决的问题没明确的时候，只是谈人工智能作为一种工具和方法孰优孰劣，那也仅仅是某种观点而已。问题是：我们作为人类，有多大的信息处理能力能够看多少内容？我们到底要的是机器生成的内容还是人生的幸福感？这种幸福感是通过生成一大堆文字、图片和视频就能获得，或者只是要比其他人更强，还是需要借助机器的能力扩大人类的探索边界，扩展人类的知识，增长我们的智慧，加深我们对浩瀚宇宙真相的理解，从而让地球文明能够突破现在的局限，不再受物质与能量的束缚，进入到一个关注思想繁荣与智慧增长的信息文明时代。我认为对于这些问题的回答将决定下一步的人工智能何去何从。</p><p></p><h2>机器的使命</h2><p></p><p></p><p>回顾一下机器的概念，在这里我引用了微软董事长兼 CEO 萨提亚·纳德拉经常引用的来自道格拉斯·恩格尔巴特的观点。道格拉斯·恩格尔巴特是当代计算机文明的奠定者之一，他的技术愿景和对于人类社会发展的洞察仍然指导着当前计算机器的发展方向。恩格尔巴特持有与范内瓦·布什和司马贺这些思想家相似的观点，都认为人类社会接下来最大的挑战就是因为信息过载而造成的复杂度已经远远超出了人类所能够处理的范畴，信息技术提升了人类社会效率，也加大了复杂度，我们需要计算机器帮助我们处理因为计算机器所带来的问题。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/e6/e6768cdd05e25b81296288e79b5f1c19.png" /></p><p></p><p>人类作为一种碳基动物，通过我们的五种传感器感知周边的世界，再通过神经传导的电化学作用将我们用传感器从周围世界感知的信号传送至我们的大脑进行处理与加工。由于信息量的泛滥和通讯的普及，再加上大多数人类的思想能力还没有进化到可以有节制地使用我们的大脑信息处理能力，一味不加选择和节制地摄入信息，就是得我们的大脑像暴饮暴食的身体一样，开始出现了大脑“肥胖症”。当然，人类经过多年的工业文明熏陶，已经知道我们的身体不能随意摄入垃圾食品，我们开始有所选择，有所判断。那我们是怎样对待大脑的呢？让我们反思一下，从今天早上醒来到现在，你所看到的消息中，哪些是真的，哪些是假的，哪些是半真半假的，哪些是有偏差的，哪些是欺骗你的，哪些是对你有帮助的，哪些是无用的，哪些是虚构的？</p><p></p><p>实际上，我们大多数人并没有深思熟虑，而是直接接受了这些信息。大家再回想一下，从今天早上醒来到现在，你所看到的消息，你还记得多少？记住的信息中，又有多少是真正能够用得上的？我们的大脑是在空转，不断地摄入“垃圾食品”，还是真正吸收到了有用的信息，让我们的思维能力得到提升？</p><p></p><p>在讨论接下来的实操内容之前，我想强调，在这个时代，能够“不被骗”就是最大的优势。我们所比拼的是什么呢？是智商、情商、理商。我现在再加上一个信商，即信息智商。我们搞 IT 的都知道网络信息安全有“零信任”原则，那么在面对信息摄取时，我们是否也应该采取信息摄取的“零信任”态度呢？</p><p></p><p>“看不见的大猩猩”的作者在 2023 年又写了一本书，名为《没有人是傻瓜》。他在书中提出，每个人都应该遵循的原则是“少信一点，多验证一点”。我们应该默认所有由像素生成的信息，大概率都有可能是假的。我们也应该像网络一样，对所有像素构成的信息采取零信任的态度，然后训练出我们自己的算法来帮助我们鉴别信息的真伪。否则，我们这些工程师、科学家可能会被一些信息误导，不是受人制约，，而是被人骗，被人牵着鼻子到处乱跑，误以为自己走对了方向。我们天天去学习别人的最佳实践，天天去学习标准答案，却忽略了一个最基本的事实：我们已经进入了一个开卷考试的时代，现在最不缺的就是标准答案，最缺的是经过独立思考而得到的适合我们自己的答案。</p><p></p><p>接下来，我想简单地展望一下未来。我认为现在有很多人在还没有开始攀登之前，就拼命想象山顶会是什么样子，是好是坏，是否可行。但实际上，大多数人甚至还没有开始他们的旅程。就目前而言，我认为智能机——不要过分夸大其作用，它就是一个智能机——但它所开创的，是帮助人类了解极大、极小、极远、极近的领域。所谓极大指的是宇宙，极小指的是量子，极远指的是太空，而极近则是更深入地了解我们自己，最终理解我们是谁。</p><p></p><p>现在的智能机以非常高的效率，推动了"AI for Science"（科学智能）和"AI for Everyone"（人人智能）的发展，尤其是"AI for Science"，科学探索得到智能机的加持后，其研究进展的速度是惊人的。如果你了解一下当前生物学、细胞学、医学以及量子物理学界的研究进展，你会发现这些进展远远超出了我们对生成式人工智能的想象。微软在英国剑桥的 AI 研究院，已经转向"AI for Science"的研究，并取得了许多突破，包括在材料科学领域的应用。众所周知，我们人类对于自身、对于量子层面的理解还是非常有限的，而机器在这些方面可以提供巨大的帮助，比如在分子、原子、细胞、DNA\线粒体、材料、能源构成等领域，还有很多未知等待我们去探索。</p><p></p><p>目前我们所观察到的，比如制作图片、视频或生成文字等，这些在办公自动化中非常有用，但它们只是智能机器能力的一个狭小的领域，。我在下图提到了一些可能性，并用红色进行了标注。但这些也只是美好的愿景，它们需要落地实现，智能技术的落地也是有次第可循的逻辑。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/70/704f24927fc31f6e7132fee0fdc20fe0.png" /></p><p></p><h2>落地的次第</h2><p></p><p></p><p>我之所以一开始讲述这么多有关思想与观念的话题，是因为如果我们不能够放下我们每个人，尤其是成功人士的思想成见，放下我们思想中的历史包袱，我们接下来的行动很可能只是在马车上装上一个引擎，比拼谁的马车装上引擎后跑得更快，而不是认识到汽车虽然开始时可能比你的马车跑得慢，但最终汽车必将远远超越马车。换句话说，对我们来说，每个人的挑战在于，我们可能仍然固守在要给马车装上引擎的想法上，因为我们需要活在当下，但我们需要开始意识到我们的目标不是如何将马车打造得更好，装上多少引擎，或者如何改造它，而是要开发出一整套全新的汽车架构和现代化交通体系。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/46/469caca8b6874da927c95fe5dc2e3c17.png" /></p><p></p><p>我相信大家对下图所展示的架构已经有所了解，它从基础架构出发，经过应用架构，再到开发的架构，最后落实到具体的应用场景。虽然这是一个显而易见的过程，但是即便如此明显，真正能够找到用户实际需要的应用的人却微乎其微。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/de/deeca5d58328098de1aca01d117cbfe6.png" /></p><p></p><p>我的观点是：“知道山在那里，并不意味着你就能登上山顶；而在山脚下，你也无法确定这是不是最高的山峰。”尽管学习、观察和借鉴他人是非常重要的，但这些只是必要条件，并不是充分条件。即便你知道山的位置，即便你有一张登山地图，这也只是其中的一部分真相。地图并不是真正的现场，而且所有的地图都是别人已经走过的路。</p><p></p><p>我印象最深的是，在读书学工程的时候，曾经调整过 PID 参数。第一次调整 PID 参数时感到很困惑：书上的理论知识都是正确的，函数也写对了，但电动机就是在那里抖动。书本的知识和实际现场出现的情况是不一样的。因此，亲身实践和实证非常关键。学习到一定程度固然重要，但最终还是要自己找到解决方案，找到适合自己的方法。</p><p></p><p>大家在观察许多人工智能的开发模型和范式时，可能会注意到有很多夸张的宣传，很多讲解倾向于“一剑封喉”的断言，只要这样做就有那样的结果。但只要一种思维逻辑具象为一种具体的方法，就会有它适用的前提条件约束。只有回归到它的思维逻辑，才能够让借鉴者根据具体情况具体分析与解决。</p><p></p><p>首先，智能机需要明确理解其目的；其次，它需要理解自身的能力边界；第三，它必须了解其操作的约束条件，也就是它所处的约束空间。接下来，智能机需要有能力寻找可用的外部资源和工具，然后开始有步骤地拆解任务，决定是顺序执行还是并行执行。在执行过程中，还需要一个实时的反馈链，也就是持续的反馈机制，不断行动，不断纠偏。</p><p></p><p>现在在网上，有些人将“reflection”翻译为“反思”，这种翻译是有问题的。机器真的会反思吗？实际上，机器只是在反馈的基础上，针对它的预测进行计算纠偏，而并不会像人类那样进行深入的反思。因此，我们不应该将描述人类思维的词汇用在机器上，这样做是非常危险的，因为它可能会导致我们对机器的能力作出错误的判断。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/62/622be485c9e61d78e6837d9c1e15ed05.png" /></p><p></p><p>理解了上述内容之后，我们可以进一步抽象化地看问题。实际上，我们每天从早到晚都在进行人 - 机协同的工作。作为个体，我们每个人都有自己明确的目标。在下图中，用红色表示人类正在执行的任务，用蓝色表示机器正在执行的任务。想想看，从今天早上起床到现在，无论是坐车、打车、开车来，还是听课、看手机，我们是不是一直在这样与机器协同工作？这种协同工作会出现什么样的现象呢？真正的机会在于我们是否能够识别出我们的痛点是什么。一旦我们识别出了自己的痛点，如果这种痛点可以由机器辅助，就成为智能机可以发挥作用的地方。熟悉 TRIZ 创新方法论里面 TESE 方法的人可能就看出，其实这就是逐步减少人类参与的系统创新方法，并不是什么突然出现的话题，只不过随着人类所发明的工具的进步，创新的方法也在不断演变之中。对于这一轮的智能机器而言，数据是核心，也就凸现出过去几十年一直强调数字化的重要性。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/dd/dd38b56e2f35bb2b6f072f5456be464e.png" /></p><p></p><p>没有数据就无法实现智能算法，没有数字化就没有数据。因此，我们仍然需要进行数字化改革。一旦我们理解了这个基本原理，就会发现有很多问题就像是屋子里的大象，虽然没人直接谈论，但它们却明显地挡在我们前进的道路上。</p><p></p><p>仅有人才的思想解放、组织的重构和流程的再造是不够的。为了实现数字化转型，我们还需要结合数字化产品和实时反馈链，以及整个产业链的协同。这不仅仅是个体和公司层面的事情，还需要整个生态系统中上下游合作伙伴的相互匹配和协调。在这种情况下，数据才是真正为了 AI 而生的数据。</p><p></p><p>作为一间合资公司的总经理，我也管理着公司，当公司试图利用智能机器的能力优化公司的客户服务时，我们也以为已经积累了很多服务数据，可以很方便的实现服务的智能化了吧？但当你尝试将 AI 应用上去时，会发现这些过去积累的数据，其建模方式并没有针对智能机器算法而优化，可用，但效果并不好。为什么？人们常常误以为机器可以学习任何东西，尤其是非结构化数据，我认为这是一个非常误导人的说法。扪心自问，机器真的能学习非结构化数据吗？机器学习真的能学习没有标签的数据吗？所谓的非监督学习，其监督已经内嵌在了提供给它的数据结构中。也就是说，数据本身必须带有逻辑，机器才能从中学习。真给机器丢一堆没有内在模式的数据，它是不可能凭空识别出模式的。</p><p></p><p>只有拥有数据，我们才能获得真正的知识，然后再将这些知识转化为 Copilot，加入到每一个流程中去。我认为这将是未来作为人类智能助手的终极解决方案。这也与我之前提到的报告相呼应，实际上并不存在哪个行业或工作内容不会被重构，所有可以被拆解成流程的工作都将被重构。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/37/37315f3153a052fcbff261ee5451d186.png" /></p><p></p><p>流程中的一个关键点是必须同时满足人类能理解和机器能处理的条件。回顾历史，这实际上是真正的 Web 3.0 概念，也就是十多年前的语义网，那时语义网技术被称为 Web 3.0 技术。这里的关键不在于术语本身，而在于其内涵，我们的整体愿景是将我们的生活、世界和社会构建成既能被人理解也能被机器处理的形式。我们每家公司的领域知识、专家知识，以及行业知识是否都能被建模，以至于机器能理解和处理呢？以目前流行的售后服务数字化和智能化为例，你的知识库真的能够做到既被机器理解也能被机器处理吗？以我的经验来看，这通常需要重新构建。这里重点是关注流程的智能化和智能社会的构建，无论是 AI 加法还是 AI 乘法，实际上都要考虑重构，而不是简单地在马车上加一个引擎。</p><p></p><p>精心打磨流程重构、数字建模，实现数据生成与管理，进而实现流程智能化。但遗憾的是，大多数人关注点都集中在最后一点，即流程智能化，而忽视了前三点。前三点是整个金字塔的基座，如果基座不稳固，整个结构将会动摇。此外，还有第五点，即“人在循环中”。因为机器并非万能，因此人与机器的结合是非常关键的，但这还不够。在第五点之上是第六点，即“人在环路上”，涉及到控制论的一阶、二阶和三阶控制与优化，也可称为一阶、二阶和三阶的学习过程，是对做事的“为什么做 - 做什么 - 如何做”(Why-What-How) 的反向优化过程，与机器学习所依赖的反向传播机制在概念上是相通的。</p><p></p><p>我们现在要做的第一步是将所有流程重构，让机器完成工作，赚取利润。但为了预见最终到达的位置，还必须建立一个新的人机文化。如果仅仅关注机器或技术能做什么，而不将人的因素考虑在内，这样的企业是不可能长久的。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/1f/1fedb0b91416ff0beafef4e4ad540271.png" /></p><p></p><p>最后，我想谈谈信息文明的最显著特征，那就是避免被欺骗，这种能力也会被别有意图的人利用来进行欺骗。我们现在生活在一个信息社会中，真的能够辨别出对自己有用的信息，并据此做出正确的判断吗？我认为这是非常困难的。我们每个人都被包裹在自己的信息茧房中，只能看到我们想看到的，只能听到我们想听到的。那么，我们如何确保自己不会被这个看不见的大猩猩——信息茧房所限制呢？这是对我们每个人的最大挑战。</p><p></p><p>今天，我分享这些内容，是因为我认为如果不解决这个问题，仅仅偶尔得到一些答案，偶尔成功一下，是远远不够的。因为如果没有建立起对未来技术发展的坚定信念，那么偶尔的成功并不能代表最终的成功。这是我从微软的一些工程师那里听到的他们的心态历程，现在分享给大家。微软的 CEO、董事长萨提亚·纳德拉在 2022 年 5 月就已经提到了 ChatGPT、OpenAI、Copilot 等概念。2022 年 11 月底 ChatGPT 发布后，大家蜂拥而上。微软的工程师其实也经历了这样的过程。起初是“AI+”，将 AI 应用到所有事物上，但第一代产品发布后发现效果不佳，那还只是一辆马车。</p><p></p><p>因此，他们转向了“Everything for AI”，即利用新的工具重构产品的流程和使用方法，包括人机交互界面、流程、判断和执行。最终他们发现，关键不在于 AI、数据、显卡或算法，而在于解决人世间的风花雪月、衣食住行。我们的产品如果能够解决人类的基本需求，就一定具有生命力。如果你的观点只局限于一个喷气发动机或者一辆马车上，那么我认为你不会取得太大的成就。</p><p></p><p>今天QCon大会的标题是“大模型正在重新定义软件”。大模型是什么？“正在”是什么？重新定义软件所代表的结果是把软件视为一辆马车加上一个引擎，称之为重新定义，还是真正地重新定义软件工程、软件流程，改变整个软件的生命周期？</p><p></p><p>微软研究院的一些研究员专门研究“AI for software engineering”，我认为这可能就是大家在软件开发行业即将面临的未来。当我们还在认为在马车上加引擎的时候，已经有一些人开始意识到这只是暂时的现象。我们真正要的是一个现代化的、以汽油、柴油驱动的交通体系。而在 100 年后，我们发现这个交通体系实际上是由电力驱动的。这将是我们未来几十年面临的机会与挑战。</p><p></p><h4>书籍推荐</h4><p></p><p></p><p>演讲最后，韦青老师向大家推荐了《提示工程：方法、技巧与行业应用》这本书，他提到：“这是本工具使用者，同时也是开发工具的老师傅写给工具使用者的书，了解机器的特性，才能更好地利用机器增强人类自身生存与发展的能力。”</p><p></p><p>对于《BPMN2.0——业务流程建模标准导论（第二版）》、《DATA 数据建模经典教程（第二版）》这两本书，韦青老师的评价是：“简单，不厚，但是有用。”</p><p></p><p>除此之外，还有以下推荐书目，感兴趣的读者可以进一步了解~</p><p></p><p><img src="https://static001.geekbang.org/infoq/7d/7d36c5d7863a6f5ba444bcb7fbbc9310.png" /></p><p></p><p>会议推荐：</p><p></p><p>InfoQ 将于 10 月 18-19 日在上海举办 <a href="https://qcon.infoq.cn/2024/shanghai/">QCon 全球软件开发大会</a>" ，覆盖前后端 / 算法工程师、技术管理者、创业者、投资人等泛开发者群体，内容涵盖当下热点（AI Agent、AI Infra、RAG 等）和传统经典（架构、稳定性、云原生等），侧重实操性和可借鉴性。现在大会已开始正式报名，可以享受 8 折优惠，单张门票立省 960 元（原价 4800 元），详情可联系票务经理 &nbsp;17310043226 咨询。</p><p><img src="https://static001.geekbang.org/wechat/images/a1/a18c960aa61239381eeb060de89d19a2.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/44aSfCiMd905NBRMSFY6</id>
            <title>CPU，正在被 AI 时代抛弃？</title>
            <link>https://www.infoq.cn/article/44aSfCiMd905NBRMSFY6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/44aSfCiMd905NBRMSFY6</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Jul 2024 07:16:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 医院门诊, AI推理技术, 大模型应用, CPU
<br>
<br>
总结: 在医院门诊中，医生们利用AI推理技术和大模型应用提高病例撰写效率，同时保护患者隐私。在其他业务场景中，律师也利用大模型进行法律文件分析。CPU作为通用处理器在AI时代发挥重要作用，具有强大的单核性能和内存容量，能满足大模型任务需求。企业应关注CPU的效价比，而不是仅关注算力规模。 </div>
                        <hr>
                    
                    <p>在某三甲医院的门诊中，汇集了来自各地的病患，医生们正在以最专业的能力和最快的速度进行会诊。期间，医生与患者的对话可以通过语音识别技术被录入到病例系统中，随后大模型 AI 推理技术辅助进行智能总结和诊断，医生们撰写病例的效率显著提高。AI 推理的应用不仅节省了时间，也保护了患者隐私；</p><p></p><p>在法院、律所等业务场景中，律师通过<a href="https://www.infoq.cn/article/FA0iHBPehJsZZ3Xk1HiY">大模型</a>"对海量历史案例进行整理调查，并锁定出拟定法律文件中可能存在的漏洞；</p><p></p><p>……</p><p></p><p>以上场景中的大模型应用，几乎都有一个共同的特点——受行业属性限制，在应用大模型时，除了对算力的高要求，AI 训练过程中经常出现的坏卡问题也是这些行业不允许出现的。同时，为确保服务效率和隐私安全，他们一般需要将模型部署在本地，且非常看重硬件等基础设施层的稳定性和可靠性。一个中等参数或者轻量参数的模型，加上精调就可以满足他们的场景需求。</p><p></p><p>而在大模型技术落地过程中，上述需求其实不在少数，基于 CPU 的推理方案无疑是一种更具性价比的选择。不仅能够满足其业务需求，还能有效控制成本、保证系统的稳定性和数据的安全性。但这也就愈发让我们好奇，作为通用服务器，CPU 在 AI 时代可以发挥怎样的优势？其背后的技术原理又是什么？</p><p></p><h2>一、AI 时代，CPU 是否已被被边缘化？</h2><p></p><p></p><p>提起 AI 训练和 AI 推理，大家普遍会想到 GPU 更擅长处理大量并行任务，在执行计算密集型任务时表现地更出色，却忽视了 CPU 在这其中的价值。</p><p></p><p>AI 技术的不断演进——从深度神经网络（DNN）到 Transformer 大模型，对硬件的要求产生了显著变化。CPU 不仅没有被边缘化，反而持续升级以适应这些变化，并做出了重要改变。</p><p></p><p>AI 大模型也不是只有推理和训练的单一任务，还包括数据预处理、模型训练、推理和后处理等，整个过程中需要非常多软硬件及系统的配合。在 GPU 兴起并广泛应用于 AI 领域之前，CPU 就已经作为执行 AI 推理任务的主要硬件在被广泛使用。其作为通用处理器发挥着非常大的作用，整个系统的调度、任何负载的高效运行都离不开它的协同优化。</p><p></p><p>此外，CPU 的单核性能非常强大，可以处理复杂的计算任务，其核心数量也在不断增加，而且 CPU 的内存容量远大于 GPU 的显存容量，这些优势使得 CPU 能够有效运行生成式大模型任务。经过优化的大模型可以在 CPU 上高效执行，特别是当模型非常大，需要跨异构平台计算时，使用 CPU 反而能提供更快的速度和更高的效率。</p><p></p><p>而 AI 推理过程中两个重要阶段的需求，即在预填充阶段，需要高算力的矩阵乘法运算部件；在解码阶段，尤其是小批量请求时，需要更高的内存访问带宽。这些需求 CPU 都可以很好地满足。</p><p></p><p><img src="https://static001.geekbang.org/infoq/38/38310c34a682db740440271f3bb77a79.webp" /></p><p></p><p>以英特尔举例，从 2017 年第一代至强®&nbsp;可扩展处理器开始就利用英特尔®&nbsp;AVX-512 技术的矢量运算能力进行 AI 加速上的尝试；再接着第二代至强®&nbsp;中导入深度学习加速技术（DL Boost）；第三代到第五代至强®&nbsp;的演进中，从 BF16 的增添再到英特尔® AMX 的入驻，可以说英特尔一直在充分利用 CPU 资源加速 AI 的道路上深耕。</p><p></p><p>在英特尔®&nbsp;AMX 大幅提升矩阵计算能力外，第五代至强® 可扩展处理器还增加了每个时钟周期的指令，有效提升了内存带宽与速度，并通过 PCIe 5.0 实现了更高的 PCIe 带宽提升。在几个时钟的周期内，一条微指令就可以把一个 16×16 的矩阵计算一次性计算出来。另外，至强® 可扩展处理器可支持 High Bandwidth Memory (HBM) 内存，和 DDR5 相比，其具有更多的访存通道和更长的读取位宽。虽然 HBM 的容量相对较小，但足以支撑大多数的大模型推理任务。</p><p></p><p>可以明确的是，AI 技术的演进还远未停止，当前以消耗大量算力为前提的模型结构也可能会发生改变，但 CPU 作为计算机系统的核心，其价值始终是难以被替代的。</p><p></p><p>同时，AI 应用的需求是多样化的，不同的应用场景需要不同的计算资源和优化策略。因此比起相互替代，CPU 和其他加速器之间的互补关系才是它们在 AI 市场中共同发展的长久之道。</p><p></p><h2>二、与其算力焦虑，不如关注效价比</h2><p></p><p></p><p>随着人工智能技术在各个领域的广泛应用，AI 推理成为了推动技术进步的关键因素。然而，随着通用大模型参数和 Token 数量不断增加，模型单次推理所需的算力也在持续增加，企业的算力焦虑扑面而来。与其关注无法短时间达到的算力规模，不如聚焦在“效价比”，即综合考量大模型训练和推理过程中所需软硬件的经济投入成本、使用效果和产品性能。</p><p></p><p>CPU 不仅是企业解决 AI 算力焦虑过程中的重要选项，更是企业追求“效价比”的优选。在大模型技术落地的“效价比”探索层面上，百度智能云和英特尔也不谋而合。</p><p></p><p>百度智能云千帆大模型平台（下文简称“千帆大模型平台”）作为一个面向开发者和企业的人工智能服务平台，提供了丰富的大模型，对大模型的推理及部署服务优化积攒了很多作为开发平台的经验，他们发现，CPU 的 AI 算力潜力将有助于提升 CPU 云服务器的资源利用率，能够满足用户快速部署 LLM 模型的需求，同时还发现了许多很适合 CPU 的使用场景：</p><p></p><p>SFT 长尾模型：每个模型的调用相对稀疏，CPU 的灵活性和通用性得以充分发挥，能够轻松管理和调度这些模型，确保每个模型在需要时都能快速响应。</p><p></p><p>小于 10b 的小参数规模大模型：由于模型规模相对较小，CPU 能够提供足够的计算能力，同时保持较低的能耗和成本。</p><p></p><p>对首 Token 时延不敏感，更注重整体吞吐的离线批量推理场景：这类场景通常要求系统能够高效处理大量的数据，而 CPU 的强大计算能力和高吞吐量特性可以很好地满足要求，能够确保推理任务的快速完成。</p><p></p><p>英特尔的测试数据也验证了千帆大模型平台团队的发现，其通过测试证明，单台双路 CPU 服务器完全可以轻松胜任几 B 到几十 B 参数的大模型推理任务，Token 生成延时完全能够达到数十毫秒的业务需求指标，而针对更大规模参数的模型，例如常用的 Llama 2-70B，CPU 同样可以通过分布式推理方式来支持。此外，批量处理任务在 CPU 集群的闲时进行，忙时可以处理其他任务，而无需维护代价高昂的 GPU 集群，这将极大节省企业的经济成本。</p><p></p><p>也正是出于在“CPU 上跑 AI”的共识，双方展开了业务上的深度合作。<a href="https://www.infoq.cn/article/ufdKm8hZltS1CmssVv1j">百度</a>"智能云千帆大模型平台采⽤基于英特尔® AMX 加速器和大模型推理软件解决方案 xFasterTransformer (xFT)，进⼀步加速英特尔® 至强® 可扩展处理器的 LLM 推理速度。</p><p></p><h2>三、将 CPU 在 AI 方面的潜能发挥到极致</h2><p></p><p></p><p>为了充分发挥 CPU 在 AI 推理方面的极限潜能，需要从两个方面进行技术探索——硬件层面的升级和软件层面的优化适配。</p><p></p><p>千帆大模型平台采用 xFT，主要进行了以下三方面的优化：</p><p></p><p>系统层面：利用英特尔® AMX/AVX512 等硬件特性，高效快速地完成矩阵 / 向量计算；优化实现针对超长上下文和输出的 Flash Attention/Flash Decoding 等核心算子，降低数据类型转换和数据重排布等开销；统一内存分配管理，降低推理任务的内存占用。</p><p></p><p>算法层面：在精度满足任务需求的条件下，提供多种针对网络激活层以及模型权重的低精度和量化方法，大幅度降低访存数据量的同时，充分发挥出英特尔® AMX 等加速部件对 BF16/INT8 等低精度数据计算的计算能力。</p><p></p><p>多节点并行：支持张量并行（Tensor Parallelism）等对模型权重进行切分的并行推理部署。使用异构集合通信的方式提高通信效率，进一步降低 70b 规模及以上 LLM 推理时延，提高较大批处理请求的吞吐。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ae/ae1ec4811551bd07e59fe841c7580d8b.webp" /></p><p></p><p>第五代至强® 可扩展处理器能在 AI 推理上能够取得如此亮眼的效果，同样离不开软件层面的优化适配。为了解决 CPU 推理性能问题，这就不得不提 xFT 开源推理框架了。</p><p></p><p>xFT 底层适用英特尔 AI 软件栈，包括 oneDNN、oneMKL、IG、oneCCL 等高性能库。用户可以调用和组装这些高性能库，形成大模型推理的关键算子，并简单组合算子来支持 Llama、文心一言等大模型。同时，xFT 最上层提供 C++ 和 Python 两套便利接口，很容易集成到现有框架或服务后端。</p><p></p><p>xFT 采用了多种优化策略来提升推理效率，其中包括张量并行和流水线并行技术，这两种技术能够显著提高并行处理的能力。通过高性能融合算子和先进的量化技术，其在保持精度的同时提高推理速度。此外，通过低精度量化和稀疏化技术，xFT 有效地降低了对内存带宽的需求，在推理速度和准确度之间取得平衡，支持多种数据类型来实现模型推理和部署，包括单一精度和混合精度，可充分利用 CPU 的计算资源和带宽资源来提高 LLM 的推理速度。</p><p></p><p>另外值得一提的是，xFT 通过“算子融合”、“最小化数据拷贝”、“重排操作”和“内存重复利用”等手段来进一步优化 LLM 的实现，这些优化策略能够最大限度地减少内存占用、提高缓存命中率并提升整体性能。通过仔细分析 LLM 的工作流程并减少不必要的计算开销，该引擎进一步提高了数据重用度和计算效率，特别是在处理 Attention 机制时，针对不同长度的序列采取了不同的优化算法来确保最高的访存效率。</p><p></p><p>目前，英特尔的大模型加速方案 xFT 已经成功集成到千帆大模型平台中，这项合作使得在千帆大模型平台上部署的多个开源大模型能够在英特尔至强® 可扩展处理器上获得最优的推理性能：</p><p></p><p>在线服务部署：用户可以利用千帆大模型平台的 CPU 资源在线部署多个开源大模型服务，这些服务不仅为客户应用提供了强大的大模型支持，还能够用于千帆大模型平台 prompt 优化工程等相关任务场景。</p><p></p><p>高性能推理：借助英特尔® 至强® 可扩展处理器和 xFT 推理解决方案，千帆大模型平台能够实现大幅提升的推理性能。这包括降低推理时延，提高服务响应速度，以及增强模型的整体吞吐能力。</p><p></p><p>定制化部署：千帆大模型平台提供了灵活的部署选项，允许用户根据具体业务需求选择最适合的硬件资源配置，从而优化大模型在实际应用中的表现和效果。</p><p></p><h2>四、写在最后</h2><p></p><p></p><p>对于千帆大模型平台来说，英特尔帮助其解决了客户在大模型应用过程中对计算资源的需求，进一步提升了大模型的性能和效率，让用户以更低的成本获取高质量的大模型服务。</p><p></p><p>大模型生态要想持续不断地往前演进，无疑要靠一个个实打实的小业务落地把整个生态构建起来，英特尔联合千帆大模型平台正是在帮助企业以最少的成本落地大模型应用，让他们在探索大模型应用时找到了更具效价比的选项。</p><p></p><p>未来，双方计划在更高性能的至强®&nbsp;产品支持、软件优化、更多模型支持以及重点客户联合支持等方面展开深入合作。旨在提升大模型运行效率和性能，为千帆大模型平台提供更完善的软件支持，确保用户能及时利用最新的技术成果，从而加速大模型生态持续向前。</p><p></p><p>更多关于至强®&nbsp;可扩展处理器为千帆大模型平台推理加速的信息，请点击<a href="https://www.intel.cn/content/www/cn/zh/artificial-intelligence/baidu-ai-cloud-accelerates-llm.html?cid=soc&amp;source=Wechat&amp;article_id=5682">链接</a>"查阅。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/a057f01a6a7902481dedd2593</id>
            <title>京东.Vision首登苹果Vision Pro 背后的技术探索</title>
            <link>https://www.infoq.cn/article/a057f01a6a7902481dedd2593</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/a057f01a6a7902481dedd2593</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Jul 2024 03:24:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 苹果, Vision Pro, 空间计算, 京东.Vision
<br>
<br>
总结: 去年6月，苹果发布了首款头显设备Apple Vision Pro，今年6月28号在中国发售。京东.Vision作为首批原生应用登陆Vision Pro平台，提供全新的购物方式。用户可以在visionOS的App Store搜索“京东.Vision”进行下载和体验。Vision Pro带来了明显的技术升级，包括VST技术、眼动追踪和空间计算概念。京东.Vision在Vision Pro上进行了技术探索和应用开发，面临了一些挑战，但也取得了一些成果。 </div>
                        <hr>
                    
                    <p>去年6月，苹果正式发布首款头显设备Apple Vision Pro，今年6月28号，Apple Vision Pro正式在中国发售。京东.Vision作为首批原生应用登陆Vision Pro平台，首期以家电家居与潮流数码产品作为切入口，未来将逐步拓展至全品类，用户可以在visionOS的App Store搜索“京东.Vision”进行下载和体验。</p><p></p><p>京东.Vision利用Vision Pro的空间计算技术，提供了一种全新的购物方式。用户可直接将心仪的家电家居或潮流数码产品以1:1等比例“拖拽”到自己家中，直接在空间计算环境中真实预览每件物品在空间中的布局和外观。负责京东.Vision开发的京东零售技术团队成员在过去一年持续关注技术发展动向，不断进行产品尝试，本文将系统性地对过程中遇到的技术问题、思考和实践做简单总结，欢迎大家一起讨论交流。</p><p></p><h1>一、与以往的头显设备相比，Vision Pro有什么不同？</h1><p></p><p>Apple Vision Pro于今年6.28号正式在中国发售。从我们的持续观察来看，苹果强大的软硬件整体设计能力，使得Vision Pro成为第一款真正意义的空间计算设备，与以往的头显设备相较，它带来了明确的技术方向和能力升级：</p><p></p><p>1.VST(Video See Through)是通过摄像头捕捉真实世界的画面，在头显内屏显示摄像头采集的画面，再实现虚实融合的展示效果。Vision Pro上将VST延迟降低到12ms，远低于其它产品的50ms以上水平。 未来很可能会继续引领其他高端设备的技术发展方向，不只是画面的采集和显示，而是采集后同步进行空间场景的数字化建模。</p><p></p><p>2.Vision Pro结合眼动追踪，实现了准确度极高的“手眼”控制系统，眼动追踪相比手柄的定位精度更高，手势操作比手柄更加方便。未来发展方向将是“手眼”操控，也会引领其它高端设备的操控方式。</p><p></p><p>3.苹果在Vision Pro上提出“空间计算”概念，即先将真实环境全部数字化，在数字化之后的真实3D空间中实现可交互，提供了更加沉浸式的互动体验。</p><p></p><h1>二、京东.Vision背后的技术探索，如何在Vision Pro上做应用开发?</h1><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/3d/3de4c4b10e269e42bf0d87d50b23ce72.png" /></p><p></p><p>作为AR/VR技术开发者，我们过往已经在手机端实现了VR全景、AR摆摆看、AR试穿戴、3D展示等产品功能，也在思考Vision Pro上最适合的产品功能。过去一年，我们重点围绕与用户实体环境相关的功能应用进行探索与创新，希望提供给用户相比手机APP跃升式的使用体验。</p><p></p><p>过程中遇到了诸多挑战： 作为苹果第一款空间计算设备，Apple Vision Pro带来了全新的visionOS平台，开发者需要适应这一平台的特质，理解其提供的无边空间画布式的交互环境。 其次，在Apple Vision Pro上开发原生3D应用，需要涉及大量对新功能的验证与试错，没有太多现成的范例可供参考。以及，由于visionOS和配套的开发工具仍在不断完善中，某些个性化应用所需的能力尚未提供，这就需要开发者进行自定义功能的扩充，如自定义手势、自定义碰撞效果和自定义组件系统等。</p><p></p><p>接下来，我们将从首页3D商品和场景展示、环境融合的空间计算应用、自定义着色器和手势等方面详细介绍。</p><p></p><h2>1. 3D商品和场景展示</h2><p></p><p>作为第一款真正的空间计算设备，Vision Pro提供了3种内容承载容器：Windows、Volumes、Spaces。默认情况下，APP启动时会进入共享空间。为了实现动态可编排的首页，我们采用Windows容器在主界面展示商品内容，并包含可交互的2D、 3D等内容形态，实现实体商品橱窗的3D展示效果。由于Volumes容器对于模型动画的兼容能力有限，我们采用RealityView进行3D模型的装载，实现了在静态首页上的动态模型展示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cd/cddbecf800495af04308b43ddd0d55ae.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/5f/5fe79e484f34173fc0ebc782cd8467bc.gif" /></p><p></p><h2>2. 虚实融合的空间计算应用</h2><p></p><p>Vision Pro搭载摄像头、激光雷达、环境光等多种传感器，通过多种传感器的组合，以及M2、R1等芯片的强大处理能力，实现了对空间环境的高精度、高鲁棒性定位和地图构建。 如下图苹果的ARKitScenes环境感知示例。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fc/fc333febb16943317bff3779e916229b.png" /></p><p></p><p>在空间计算的电商场景中，我们实现了真实空间中的多个虚拟商品摆放问题，来满足用户多品搭配需求。 首先是商品在真实空间的自由移动、旋转、缩放等空间操作，涉及坐标系变换与仿射变换等技术。在3D 视觉中常用的三个坐标系：图像坐标系、相机坐标系、世界坐标系，它们之间可通过仿射变换、投影变换、刚体变换等方式实现运动。Vision Pro中通常涉及SwiftUI CoordinateSpaceProtocol与RealityCoordinateSpace两个坐标系的转换，转换过程中的世界坐标等参数便由空间计算结果提供。</p><p></p><p><img src="https://static001.geekbang.org/infoq/28/28980db8bdd9f44426e0401e7a72bb6c.png" /></p><p></p><p>利用空间计算的环境感知能力实现平面检测和地图建模，结合商品的实际尺寸信息实现虚拟商品与真实空间平面、垂面的吸附、摆放等在实体空间的摆放操作。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d4/d43d2c2cca7c23febb1a111c4f0c3463.png" /></p><p></p><p>3D环绕与AR摆摆看等典型空间计算应用是将现实世界和虚拟世界融合在一起。在Vision Pro中可以使这个过程更加真实，将虚拟模型的遮挡、碰撞、光影反射等各种属性在现实世界模拟呈现。为了实现碰撞效果，首先需要进行模型与周围环境的碰撞检测，通过定义模型的碰撞形状和属性，并赋予物理属性，如质量、摩擦力和恢复系数，可以实现物理碰撞模拟。常见的碰撞形状包括：矩形，球体，胶囊，凸形状等，为了提升碰撞性能通常使用矩形碰撞形状来进行碰撞检测。碰撞检测示意图如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a28a1980ca309bcd5af2ac63fc7c046d.png" /></p><p></p><p>当检测到与实体碰撞后，我们根据已经设置的实体物理属性，实时计算实体在三维空间中的移动速度与位移大小，并更新实体位置。通过模拟碰撞，我们可以实现虚拟模型实体与环境实体，虚拟模型实体与虚拟模型实体之间的碰撞运动，以及虚拟模型实体之间的叠放。</p><p></p><p>解决单个商品在实体空间的摆放之后，进一步实现多品摆放，并使多个虚拟商品可实现真实的碰撞交互，解决了用户体验多件商品搭配效果的需求。为此，我们动态调整虚拟商品的物体属性，允许模型碰撞相交，保证初始化时多个模型在视野中全部可见，之后逐步摆放到合适位置。多品摆放示意图如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/6f/6fe11b2e78b05aee38561194f4e1e5fd.png" /></p><p></p><h2>3. 自定义手势识别</h2><p></p><p>Vision Pro利用摄像头和传感器进行手势识别，例如点击，捏合，缩放，旋转等。这是空间计算所能实现的最具沉浸感的方面之一，因为它允许用户通过显示器操控他们看到的数字对象。这种操控方式流畅且最为熟悉。除了苹果官方提供了Tap，Pinch，Zoom，Rotate等基础手势。</p><p></p><p><img src="https://static001.geekbang.org/infoq/08/082e6d31dc86c94a386a5f11acdd1490.png" /></p><p></p><p>我们利用Hand Tracking以及AI深度学习技术扩展了苹果的手势识别功能，让用户可以用更多自定义手势，例如与3D商品进行更精确、流畅的旋转缩放交互。Vision Pro为每只手掌提供25个关键点的数据，其中每个手指有4个关键点，手腕处一个关键点。官方提供6种基础手势，我们在此基础上丰富手势识别功能。通过将关键点信息输入到Rule-based system、DNN、LSTM等模块中，实现动态手势的识别。</p><p></p><p><img src="https://static001.geekbang.org/infoq/30/308f79346f1edbf4bfb6c4cbf44525ba.png" /></p><p></p><h2>4. 自定义着色器</h2><p></p><p>依托M2芯片+R1芯片的加持，Vision Pro提供了强大的渲染能力，使得我们可以用自定义着色器实现一些特殊的材质表现和渲染效率优化。比如上文提到的碰撞后的网格特效、商品中的呼吸灯、模型指示器中的UI九宫格等。我们通过Composer Shadergraph实现的UI九宫格，用于指示模型在世界空间中的位置，同时对于不同大小的模型需要保证UI九宫格的4角区域不发生形变，Shadergraph方案以及UI九宫格示意如下所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/ea5c230a67db4d010c9c90a1c42f635a.png" /></p><p></p><h2>5. 空间计算优化</h2><p></p><p>空间计算与传统运算相比，需要计算的数据量提升了一个维度。在处理大量3D数据时，我们采取了多种优化措施来保持高效率的资源应用和流畅的操作体验。例如，根据商品类别动态调整3D模型质量，合理分配面数以控制资源大小。使用Reality Composer Pro工具打包3D场景和资源，实现有效压缩。此外，通过资源预加载、动态加载与释放以及缓存减少IO等操作，提升界面流畅度和降低响应时间。</p><p></p><p>通过优化技术，我们实现了“3D无界场景”等功能，在一个“无限大”的空间场景中，我们载入了多套高精度模型组，使得用户可以在一个空间内，一站式沉浸浏览。</p><p></p><p><img src="https://static001.geekbang.org/infoq/92/92f7fd40c3993806ab9a9fbcbeb089ec.gif" /></p><p></p><h1>三、未来探索方向</h1><p></p><p>Apple Vision Pro作为下一代终端设备，正在引入更多交互方式，提升混合现实的体验效果。京东一直致力于提供多快好省的用户体验，探索更多元、丰富的购物方式。未来我们还将持续打造3D体验和全沉浸式场景体验，引入更多高质量的3D模型与场景、景深视频等资源，逐步补齐3D场景搜索、智能导购、试搭等内容，进一步提升沉浸式体验效果。期待随着技术的不断成熟，一起为用户带来更多新鲜的购物体验。</p><p></p><h1>四、 参考文献</h1><p></p><p>Andrei, Constantin-Octavian. “3D affine coordinate transformations.” (2006).</p><p></p><p>A novel hybrid bidirectional unidirectional LSTM network for dynamic hand gesture recognition with Leap Motion[J]</p><p></p><p>Dynamic Hand Gesture Recognition Based on Short-Term Sampling Neural Networks[J]</p><p></p><p><a href="https://www.cnblogs.com/ghjnwk/p/10852264.html">https://www.cnblogs.com/ghjnwk/p/10852264.html</a>"</p><p></p><p><a href="https://developer.mozilla.org/en-US/docs/Games/Techniques/3D_collision_detection">https://developer.mozilla.org/en-US/docs/Games/Techniques/3D_collision_detection</a>"</p><p></p><p><a href="https://developer.apple.com/documentation/realitykit/">https://developer.apple.com/documentation/realitykit/</a>"</p><p></p><p><a href="https://github.com/apple/ARKitScenes">https://github.com/apple/ARKitScenes</a>"</p><p></p><p><a href="https://developer.apple.com/documentation/arkit/arkit_in_ios/configuration_objects/understanding_world_tracking">https://developer.apple.com/documentation/arkit/arkit_in_ios/configuration_objects/understanding_world_tracking</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0VJSOSpO6qdakUj6dMYe</id>
            <title>夸克升级“超级搜索框”推出AI搜索为中心的一站式AI服务</title>
            <link>https://www.infoq.cn/article/0VJSOSpO6qdakUj6dMYe</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0VJSOSpO6qdakUj6dMYe</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Jul 2024 02:09:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型时代, 生成式AI, 夸克, 超级搜索框
<br>
<br>
总结: 阿里智能信息事业群旗下的夸克推出了以AI搜索为中心的一站式AI服务，通过大模型和生成式AI技术革新搜索产品，提供智能回答、智能创作和智能总结等功能，满足用户信息检索、生成和处理的需求。 </div>
                        <hr>
                    
                    <p>大模型时代，生成式AI如何革新搜索产品？阿里智能信息事业群旗下<a href="https://www.infoq.cn/article/d4rIQ0vpsW9b06qToiji">夸克</a>"“举手答题”。7月10日，夸克升级“超级搜索框”，推出以<a href="https://www.infoq.cn/article/opVYCFjJfTrN6NH4xSQY">AI</a>"搜索为中心的一站式AI服务，为用户提供从检索、创作、总结，到编辑、存储、分享的一体化信息服务价值。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/d0/fa/d0ff42c072607d2a93a2a41b5befa8fa.png" /></p><p></p><p>“能回答、能创作、能总结的超级搜索框，是夸克对AI搜索的新定义。”阿里智能信息事业群总裁吴嘉表示，大模型、数据、场景等优势，推动夸克加速革新搜索产品，创造更大用户价值。“跨过大模型应用全新体验的临界点，夸克全面进入AI时代，一站式AI服务的创新涌现将滔滔不绝。”</p><p></p><h2>全新AI搜索，一框实现回答、创作、总结</h2><p></p><p></p><p>过往，搜索引擎依据关键词，提供网站列表排序。反复挑选、点击、阅读，以及大量不相关结果，成为用户高效获取信息的拦路虎，复杂问题也很难得到满意的回答。</p><p></p><p>AI技术跃迁点燃了搜索的价值焕新。用户打开夸克7.0版搜索框，输入问题即可体验智能回答，还有AI写作、文件总结、视频总结、拍题讲解功能。一个“超级搜索框”集纳了智能回答、智能创作和智能总结三大能力。</p><p></p><p>其中，智能回答能够更好地理解用户意图，聚合全网优质内容，更精准、直接、高效地提供图文、视频等。尤其针对复杂逻辑分析和跨学科知识，智能回答更能发挥AI的综合回答能力，为用户呈现准确、丰富的结果。</p><p></p><p>智能创作方面，夸克AI搜索满足用户各类主题、题材、篇幅的高频写作创作需求，包括文案创作、文档写作、PPT写作、简历制作等，让用户直接得到所需内容。</p><p></p><p>智能总结方面，面对几十万字长文和专业信息，夸克数秒钟就能整理出全文摘要。更惊喜的是，夸克还支持最长5小时视频的字幕导出、分段总结、整体总结、生成脑图、抽取课件PPT等，提升工作学习效率。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/f4/0b/f4830b51331e672a5dc45447e7c7600b.png" /></p><p></p><p>此前，夸克升级高考AI搜索，“山东高考580分能否上985”类似问题的个性化志愿推荐能力大大提升。6月高考季，夸克高考AI搜索使用量超过1亿次。</p><p></p><p>夸克产品负责人郑嗣寿介绍，信息检索、创作和总结一直是用户的核心需求，AI搜索让人与信息距离更短。夸克始终坚持在搜索上的价值探索，不断从搜索框中生长出新内容、新工具。“让万事万物都有答案，让答案都有迹可循”。</p><p></p><h2>一站式AI服务，满足信息检索、生成与处理</h2><p></p><p></p><p>始于框但不止于框！从创立之初定位智能搜索，夸克持续突破搜索框的形态与能力边界，在“智能工具+内容+服务”模式下，上新一系列内容产品与智能工具。</p><p></p><p>站在AI时代，夸克以全新的视角去看待产品和需求。夸克7.0版以AI搜索为中心，不断延展功能场景和服务能力，面向用户信息检索、生成、处理的全域、多元需求，一体化设计产品，一站式提供AI服务。</p><p></p><p>一个“超级搜索框”实现回答、创作、总结之外，夸克一站式提供网盘、扫描、文档、CueMe、学习助手、健康助手等内容产品和智能工具，为用户提供从检索、创作、总结，到编辑、存储、分享的一体化信息服务价值。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/e1/01/e118bc8ced6b9979047d830aafd5cc01.png" /></p><p></p><p>用户撰写年中总结PPT，在夸克搜索“年中总结PPT模版”，即可进入AI PPT一键创作，调整PPT内容，最后导出文件，存储和发送。在“超级搜索框”中使用AI写作，用户得到的个性化创作内容，可以保存为Word文档，在夸克网盘编辑、转换格式、分享，告别了多场景反复跳转的割裂体验。</p><p></p><p>此外，在手机、电脑、平板等多个终端上，多端一体的夸克正在逐步构建中，确保用户在不同客户端上都能享受到好用的、高质量的信息服务。</p><p></p><h2>面向用户设计产品，革新性搜索加速迭代</h2><p></p><p></p><p>满足用户最基础、最广泛的信息需求，AI搜索与大语言模型能力的契合已成为行业共识。面向用户创新价值，以下一代搜索为远景目标，夸克AI搜索长期积累了四个方面的能力与资源优势。</p><p></p><p>模型能力方面，夸克大模型去年一经发布即登顶各大性能评测榜，并持续面向用户场景深度迭代，提升性能；搜索能力上，夸克积累了用户理解、内容生态、安全合规等全面能力；数据能力上，夸克多年来在知识、经验、健康、题目等领域拥有海量的优质数据；应用场景方面，夸克长期沉淀了通用搜索和健康、教育、文档等垂直领域的众多场景，且拥有大规模用户群体。</p><p></p><p>“搜索是个生生不息的业务，AI搜索才刚刚开始，夸克AI搜索同样处在全新阶段的开始。”郑嗣寿表示，夸克会加速效果迭代和产品升级，给用户更快更准的搜索体验。他透露，在多模态交互、内容生态建设、多端一体等方面，夸克将进一步加快产品创新节奏，为用户创造无处不在的信息服务价值。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/mofQeXC5geHrabzrVVU5</id>
            <title>一行代码价值百万美元：从工程技术角度看云成本优化</title>
            <link>https://www.infoq.cn/article/mofQeXC5geHrabzrVVU5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/mofQeXC5geHrabzrVVU5</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Jul 2024 02:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 云计算, 软件开发, 成本效率, 工程决策
<br>
<br>
总结: 云计算时代下的软件开发者需要关注成本效率和工程决策，每一个工程决策都是一个购买决策，需要考虑云计算的实际成本投入。软件必须盈利，云计算不仅是简单的计算机，而是一个全新的平台，需要改变传统的编码方式和系统设计思路。工程师在软件盈利能力中扮演重要角色，成本效率反映系统质量，一行代码决定公司盈利。需注意避免烧钱行为，如调试导致高昂费用和API请求造成成本增加。 </div>
                        <hr>
                    
                    <p>没有比现在成为软件开发者更好的时刻，也从来没有哪个时刻可以像现在这样，一个工程师能拥有如此大的影响力，一行代码就能决定一个组织的财务走向。和许多人一样，我一直热衷于开发高效的软件。然而，在以云为中心的世界里，效率不再仅仅关乎性能。我们现在所做的按需计算和基础设施选择都需要实实在在的资金投入，忽视了这一点可能会非常危险。</p><p></p><p></p><h2>每一个工程决策</h2><p></p><p></p><h2>都是一个购买决策</h2><p></p><p></p><p>每一个工程决策都是一个购买决策。相比在云端的花费，人们可能更关注你今天在晚餐或午餐上的花费。财务部门的某些人会盯着那 50 美元的午餐，却没有人盯着你的工程师在云计算上花费的 10000 美元。这很难理解，因为在过去，CTO、CIO 和 CFO 会监督采购流程，而如今，一个初级工程师在采购方面比公司里的任何人都拥有更多的自主权。</p><p></p><p>当今的世界正处于云计算成本时刻。经过多年的大规模增长，人们的关注点已经从不惜一切代价的增长转向了高效、有利可图的增长。有些人在想，也许云计算是个错误，也许它是个骗局。如果我们迁移到了云端，然后发现需要退出，该怎么办？对被云厂商锁定的恐惧导致许多人一只脚仍然踩在数据中心里，而且许多人发现这种方法成本很高。许多人已经发现，提升（lift）和转移（shift）成本极高（这可能是云计算最大的谎言）。这到底是怎么回事？云计算是个骗局吗？不幸的是，正是对云计算及其成本的恐惧，导致了这种云计算浪费的预言成为现实。</p><p></p><p></p><h2>破釜沉舟</h2><p></p><p></p><p>在今天和明天的经济环境中生活的我们，都需要明白，要构建伟大的软件，这些软件必须是盈利的。</p><p></p><p>云计算不只是别人的计算机那么简单，它是一个操作系统，一个全新的平台。就像科尔特斯征服新大陆一样，如果我们想要成功，就必须破釜沉舟，忘掉回家的路。然而，许多人仍在为昨天的大型机编写代码，没有意识到如果要最大限度地利用云计算，就需要重写代码。在 DevOps 运动开始之前，我们会把代码扔给运维人员，然后去解决下一个问题。现在，我们编写代码，然后扔给财务部门，让他们去操心。生活在这些经济体中的所有人都需要明白，我们要构建出色的软件，并且它们必须能够盈利。</p><p></p><p>许多软件仍在等待部署到云端。据估计，目前仍有 4.6 万亿美元的 IT 支出用于数据中心运行。尽管云计算规模在不断增长，但仍处于早期阶段。</p><p></p><p></p><blockquote>我们还有很多事情要弄清楚。如果我们要迁移到云端，它必须具有强大的经济意义。有些人坚信这是不可能的，有些人坚信这完全是个错误。我知道这些人是错的。毕竟我已经在云端，我想继续留在那里。但我也希望在有生之年看到这一天的到来。不幸的是，即使云计算每年以 50% 的速度增长，如果我们不开始用不同的方式构建软件，恐怕我们谁都活不到看到它到来的那一天。</blockquote><p></p><p></p><p>之所以存在这样的讨论，是因为我们在构建软件时还不太清楚云计算是否具有强大的经济意义。</p><p></p><p>我们必须改变这种状况。我已经看到了数据，我可以告诉你云计算具有强大的经济意义。我已经看到了这一点，但你必须用不同的方式构建软件，编写不一样的代码，并以不同的方式思考系统设计。你不能只是将在数据中心中有效运行的东西直接搬到云端，然后期望得到一个好的结果，你必须对此有不一样的思考。</p><p></p><p></p><h2>工程师在软件盈利能力中</h2><p></p><p></p><h2>所扮演的角色</h2><p></p><p></p><p>如今，成本效率常常可以反映出系统的质量。一个架构良好的系统是一个具有成本效益的系统。一行代码就能决定你所工作的公司是否盈利。</p><p></p><p>我们面临着一个共同的挑战，必须找出衡量成本效率的最佳方法。为此，我想深入代码层面。我本质上是一名工程师，所以我收集了一些价值百万美元的代码示例（在某些情况下甚至价值数百万美元）来展示烧钱是一件多么容易的事情。为了保护隐私，所有这些都已匿名化并转换成了 Python 代码。在这些示例中，仅仅几行代码就烧掉了远比他们应该花费的多得多的钱。</p><p></p><p></p><h4>&nbsp;示例 1：因调试而导致的高昂费用（即使是 DevOps 也要花钱）</h4><p></p><p></p><p>在这个示ps 也要花钱例中，一个 AWS Lambda 函数的平均月成本为 628 美元，CloudWatch 的平均月费用为 31000 美元。究竟发生了什么？不幸的是，AWS CloudWatch 的费用远超实际调用 Lambda 函数的情况太常见了。我不知道有多少人经历过这种情况，但感觉任何一个在 AWS 构建无服务器系统的人最终都会遇到这个问题。</p><p></p><p>在这个示例中，仅用于写入日志数据的年度总成本就达到了 110 万美元。造成这种情况的原因是什么？这里有两个导致因素。一些本不应该被发布的代码，却也是曾经非常重要的代码。一行善意的调试代码，当运维团队打开调试日志，并没有多想，然后将大量数据发送到了 CloudWatch。有时候，运维团队与开发团队是脱节的（我知道我们都希望认为 DevOps 总是紧密合作的，但事实并非总是如此），他们假设代码应该按照他们想的那样运行。于是，它运行了很长时间，烧掉了 110 万美元。</p><p></p><p>顺便说一句，如果他们还使用 Datadog 来收集日志，这可能会变得更加昂贵。相比之下，110 万美元算是一笔划算的交易，但无论如何，这同样是悲剧，也是不必要的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0b/0bb8bb286a4428acdc3ebcd7dc8f6693.webp" /></p><p></p><p>有什么办法可以解决这个问题？很简单。去掉调试语句，我们知道这就是问题所在。如果我在电脑上编写示例代码，最后会把调试代码删除，因为到最后我们不需要它们。在开发和测试阶段，这些代码很有用，但部署时不要把它们放进去。它们就像是一个漏洞，一颗等待爆炸的定时炸弹。解决办法就是删除它们。</p><p></p><p></p><h4>示例 2：API 也是要花钱的</h4><p></p><p></p><p>在这个示例中，我们有一个最小可行产品（MVP）进入了生产环境。几年后，这个产品向 S3 发起数十亿次 API 请求，因为是偷偷平稳地增长，以至于没人注意到。这段代码在一年内就烧掉了 130 万美元。</p><p>这段代码存在许多挑战。作为最小可行产品（MVP），它运行得非常完美。一个想法蹦出来，把它写在纸上，然后实现它，交付它。为什么这些东西会在 for 循环里？为什么在运行过程中调用 S3 API？实际上，我们可以把所有这些内容抽离出来，并快速缓存或捕获这些信息。问题是这段代码能正常运行。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a9a80769b8ec4298ff873f943b7127c.webp" /></p><p></p><p>在部署好后，它运行得很好。直到多年后，当它达到一定规模时，才开始烧掉那 130 万美元。我们还发现了一个小细节。也许我不应该把这些文件传递给后续的函数进行进一步处理。这个问题的解决办法是什么？我们可以把它从 for 循环中抽离出来。提前计算或下载这些内容，一次性做完，而不是在函数里运行一百万次。与其通过传递指针方便后续查找文件，不如直接传递实际的数据。一次性使用——多么简单的操作。再次强调，我们都做过这样的事情。我们让代码跑起来，作为原型来说运行得足够好。然后，它们被悄无声息地交付，我们也没有想太多。API 调用是要花钱的。有时候，在 S3 中，API 调用的成本可能比存储本身还要高。</p><p></p><p></p><h4>示例 3：几字节如何让 DynamoDB 写入成本加倍</h4><p></p><p></p><p>在这个示例中，一位开发人员被要求添加一些简单的功能。我们写入 DynamoDB 的记录没有时间戳，我们想知道它是什么时候写入的。为什么不添加个字段呢？这应该非常简单。修改代码只需一秒钟，有人测试了，然后部署了，现在已经上线并运行了。</p><p></p><p>不久之后看看账单，DynamoDB 的成本翻了一番。这个稍微有点难发现。有人知道为什么添加时间戳的代码会让 DynamoDB 的成本比以前翻了一番吗？DynamoDB 按照 1K 元素为单元进行收费。它写入的是 1000 字节，但我们添加了一个时间戳，一个 9 字节的属性。时间戳是 ISO 格式的，即 32 个字节，加起来是 1041 字节，仅一行代码就使成本翻了一倍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4b/4b129a2022132112747a7c66d87d5e87.webp" /></p><p></p><p>这个真的很难被发现。我们必须用不同的方式思考数据是如何在线上传输的。更重要的是，这对我们的成本有何影响？这个问题的解决办法是什么？我们应该做两件事。我们应该减小属性名称的大小。这是 TCP/IP 协议的一个基本属性。将其改为“ts”而不是时间戳。这样就可以减掉几个字节。我们重新格式化时间戳，这样就减少到了 20 个字节，还剩余 2 个字节。我们回到了实际需要的水平——一行代码，成本减半。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0a/0af2982c8e495724df71c14d9347368f.webp" /></p><p></p><p></p><h4>示例 4：基础设施代码泄漏（Terraform 版）</h4><p></p><p></p><p>我们不要忘了基础设施即代码，比如 Terraform 和 CloudFormation。在接下来的示例中，我们有一个 Terraform 模板，用于创建自动伸缩组。它可以同时伸缩具有数百甚至数千个 EC2 实例的集群。有人设计了这样一个系统，每 24 小时就回收一次实例。也许是因为存在内存泄漏，他们认为这是一个很好的解决方法。不幸的是，安全部门的人担心这些数据可能是必要的，所以移除了可删除 EBS 卷的选项。这个系统运行了大约一年，慢慢积累了一堆 EBS 卷。在那一年年底，110 万美元付诸东流。这个示例有点冗长，就像大多数基础设施即代码一样，但导致这个问题的是两行代码，分别在两个不同的文件中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/71/71007324cc7f6b4b2f2a99edf61cd62f.webp" /></p><p></p><p>这两行代码组合每隔 24 小时会为每个创建的 EC2 实例创建一个未连接的 EBS 卷。第一行delete_on_termination被设置为 false，阻止 EBS 卷被删除。第二行max_instance_lifetime是回收时间。因为这两行在不同的文件中，所以很容易被忽略。</p><p></p><p>这两行代码意味着每次 EC2 实例启动都会创建一个 EBS 卷，而这个 EBS 卷永远不会被删除（除非手动删除）。由于自动伸缩组的最大大小为 1000（在这个示例中，在任何给定时刻，这个环境中有 300 到 600 个 EC2 实例），未连接的 EBS 卷的数量迅速增加。一年下来，累计费用超过了一百万美元。然而，解决这个问题稍微会复杂一些。对于这个问题，你必须改变流程。</p><p></p><p>你必须稍微考虑一下你的团队在实现这些东西时是怎么做的。如果创建了资源，就应该知道如何删除它们。这不仅适用于云端的成本，也适用于许多其他的情况。我们中的许多人在过去几年里一直在思考如何扩大规模，但没有足够多地思考如何缩减规模。缩减规模要难得多，也重要得多，它甚至还有可能拯救你的企业。如果你的公司是一家旅游公司，并在经历了新冠疫情后幸存下来，那么一定知道如何缩减规模。我听说过 Expedia 团队的一些了不起的事情，但并非每家公司都那么幸运。要小心那些出于好意的基础设施即代码，特别是当你需要满足来自不同团队的需求时。</p><p></p><p></p><h4>示例 5：网络传输成本</h4><p></p><p></p><p>第五个示例，我把最好的留在最后。我们都喜欢内容分发网络（CDN），它们可以更快地将内容传输给客户，让所有的东西都运行得更快。在最后这个例子中，一家公司在全球部署了 230 万台设备，他们做了一个小小的改动，而这个小改动被部署到了所有设备上。</p><p></p><p>大约 14 小时后，这个改动变成了一个问题。这个问题被修复之前，每小时导致约 4500 美元的损失。虽然最终造成了数十万美元的损失，但这与它可能造成的真正影响相比，简直是小巫见大巫。如果这个改动持续运行一年而没有人注意到——我会解释为什么可能没有人注意到它——它将成为一条价值 3900 万美元的代码行。</p><p></p><p>我确信会有人乞求那笔钱可以回来。希望在第一个月的财务报告中就有人注意到了这个问题。然而，那已经是在损失了 64.8 万美元之后了。这真是一个让人痛苦的 Bug。值得庆幸的是，这个问题在六天后被发现，相比通常情况下需要几个月才会被发现，这也算是一个小小的成功故事了。问题是，这个故事的成功标准实在太低了。</p><p></p><p>那是段怎样的代码？它看起来像这样。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8f/8f68a43b76f0bfce1e183cfd1490c828.webp" /></p><p></p><p>在这段代码中，有一个出于好意的更新函数，可能是很久以前的一个实习生写的。它原本每天被调用一次，用来下载和比较一个大文件，这看起来像是一个糟糕的主意，所以有人决定改为下载元数据，认为这会更高效。具有讽刺意味的是，这个改动实际上是为了降低成本。他们部署了代码，并期望一切都朝着正确的方向发展。当他们突然发现事情并没有按预期进行时，他们并不确定接下来发生了什么。</p><p></p><p></p><blockquote>有多少人能发现这段代码中的 Bug？</blockquote><p></p><p></p><p>只是一个字符，这个字符的拼写错误让这段代码的执行切换到了成本更高的路径。同时，他们将调用频率从每天一次提高到每小时一次。问题在于，CloudFront 非常乐意提供内容。它做得很好，不仅扩展服务满足需求，还成功分发了内容。在整个过程中，没有系统受到影响，也没有错误被检测到。所有人都很高兴，因为数据流动顺畅且高效。</p><p></p><p>他们的客户可能会好奇，为什么他们家的网络会有这么多额外的数据流动，但因为一切工作正常，这个问题难以被发现。运营团队和后台监控工具没有发现错误，也没有任何警报。Datadog 没有报告任何问题，因为 CloudFront 可以轻松地应对流量的增长。唯一的异常指标是他们现在每小时要花掉 4500 美元，这在之前没有发生过——而这一切都是因为一个字符的拼写错误。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b0/b0345671d436bfb1f738d620a89cb943.webp" /></p><p></p><p>真正的解决之道是重新反思整个问题。尽管这个问题可以被快速修复，但因为涉及到产品的一个关键方面，所以需要更深层次的方案。此外，这类错误不太容易通过测试捕捉到，在未达到实际规模的情况下，许多问题在测试中是无法被注意到的。这种微不足道的字符拼写错误可能会导致 3900 万美元的账单。</p><p></p><p></p><h2>学到的教训</h2><p></p><p></p><p>我们从中学到了什么？存储成本依然低廉。我们确实可以认为存储成本依然相当低廉。然而，调用 API 是有成本的，这些成本总是存在的。事实上，我们应该接受这样一个事实：在云端进行的任何操作都是有成本的。可能不多，可能是几美分，可能是几分之一 美分，总之是要花钱的。在调用 API 之前，你最好考虑到这一点。云给了我们几乎无限的规模，问题是，我没有可以无限支付的钱包。</p><p></p><p>我们有一个系统设计约束，这个约束在设计、开发和部署过程中似乎没有人关注。这个重要的经验教训是什么？我们是否应该在成为云平台软件开发者的基础上，再增加一层对成本的关注？我已经考虑了很多，再增加一件需要担心的事情似乎是雪上加霜。我们希望所有的工程师都去操心代码的成本问题吗？即使在这个新的云时代，Donald Knuth 的这句名言依然有深远的意义。</p><p></p><p></p><blockquote>"过早优化是万恶之源" —— Donald Knuth</blockquote><p></p><p></p><p>作为工程师，我们首先需要弄清楚的是，这个该死的东西是否能工作？我能解决这个问题吗？</p><p>我分享的所有这些例子在流量达到一定规模之前都不是问题。事实上，只有在你取得成功之后，它们才会成为问题。除非你在正在开发的产品或服务上取得了实质性的进展，否则这些问题不应该成为你的首要关注点。</p><p></p><p>云工程师应该考虑成本，但这应该是一个渐进持续的过程，而不是一次性的决策。</p><p></p><p>首先，我们要回答这个问题：这是否可行？然后，作为团队的一员，这样做对团队来说是对的吗？其他人如何维护我的代码？接下来，如果规模增长了，会发生什么？这个时候你应该开始考虑成本问题。</p><p></p><p>当我开始在云端构建我的第一个系统时，我对成本的概念还相当模糊。我去找 CFO 并说我想用 AWS 做一个项目时，他说，“Erik，你可以做任何你想做的，但你有 3000 美元的预算，不要一下子花光。”这已经是很久以前的事情了，那时候云计算还是一个新兴领域。我知道如果我能把项目的成本控制在 3000 美元以内，我就可以在这里尽兴地探索。所以，我开始对如何最大化投资回报产生了浓厚兴趣。这种对效率的追求得到了回报，因为我成功地将成本控制在预算之内，并自此一直在云计算领域深耕。所有人都应该这样吗？我们是否应该给每个工程师一个预算？我们更希望赋予工程师的，不仅仅是一个数字，因为一个数字可能意味着每天花费相当于一辆兰博基尼的价格，这听起来既抽象又难以理解。相反，我更希望每个人都能专注于提高效率。</p><p></p><p></p><h2>云效率比率</h2><p></p><p></p><p>为此，我想介绍一个叫做云效率比率（Cloud Efficiency Ratio，CER）的概念。这个概念既简单又直观，旨在引导你在恰当的时机才开始考虑成本优化，避免过早地陷入成本削减的泥潭。你可以通过用收入减去云成本再除以收入来计算云效率比率百分比。例如，假设你的公司每年收入 1 亿美元，云成本是 2000 万美元，那么你每收入一美元就花掉 20 美分，你的云效率比率是 80%。这个比率很棒，但你不一定需要在一开始就达到这个水平。</p><p></p><p>你应该将云效率比率视为一种合理化成本的非功能性需求。对于任何云项目，你应该让产品团队或业务部门在项目开始时定义期望的云效率比率，以及在应用程序生命周期的哪个阶段应该做出调整。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9c/9ce7b96e214e86f0010050f0e539dd86.webp" /></p><p></p><p>记住，在研发阶段，你的主要目标是验证项目是否可行。你的 CER 可能是负的，甚至在某些情况下就应该是负的。如果你在产品发布之前就开始赚钱，那就有问题了。然而，一旦达到了 MVP 阶段，就应该开始努力实现盈亏平衡。这个阶段的 CER 低是可以接受的，它应该在 0% 到 25% 之间，因为你的目标是试图找到产品与市场的契合点。等到人们对你的产品开始感兴趣，并想要买你的产品，你就可以开始设想如果产品变得流行起来会怎样，这时的 CER 就该达到 25% 到 50%。</p><p></p><p>在你扩大产品规模，且人们在抢着购买你的产品时，你需要制定一条明确的路径，以实现健康的盈利水平，你可能想达到 50% 到 70% 的 CER。随着业务逐步进入稳定状态，如果你想拥有一个健康的业务，并成为组织的利润引擎，那么就要努力达到 80% 的 CER。利用云效率率作为目标，你可以将那些抽象的美元成本转化为具体的目标，用以指导你的云项目。这个指标可以贯穿你的整个云平台，或者具体到某个客户、功能、服务或任何新项目。作为经验法则，我建议将目标 CER 设定为 80% 左右。</p><p></p><p></p><h2>结&nbsp; &nbsp;论</h2><p></p><p></p><p>在为撰写本文做准备的过程中，我意外地发现了计算机科学领域的杰出人物 Tony Hoare。一些人认为他推动了“过早优化是万恶之源”这句名言的传播。我发现的东西让我大吃一惊。事实证明，Tony 在多年前就提出了这个概念。早在 2009 年的伦敦 QCon 大会上，他就讲述了一个价值十亿美元的教训。在演讲中，他回顾了 1965 年他发明空引用的决策，并认为这一早期的错误可能导致全球经济损失高达数十亿美元。他的观点可能并无夸张。</p><p></p><p>所有的工程师都知道，随着时间的推移，他们的代码会自成一体。它会流转到其他人手中，并继续演化，我们对它们的掌控会越来越弱。在编写代码时，我们考虑的成本可能只有几分几毛，但当这些代码在某个地方部署并运行时，每年的运行成本可能高达 100 万美元。因此，当你重新审视你的代码库时，请认真思考这一问题。愿你在这一过程中不会有任何令人沮丧的发现。</p><p></p><p>最后，我将以这句话结束：</p><p></p><p></p><blockquote>“每一个工程决策都是一个购买决策。”Erik Peterson —— CloudZero 联合创始人兼 CTO</blockquote><p></p><p></p><p>在今天，当你敲下每一行代码，都是在做一个购买决策。你在你的组织、这个经济体和这个云驱动的世界中扮演着非常重要的角色。希望你能明智地运用这种力量。</p><p></p><p>查看英文原文：</p><p></p><p>https://www.infoq.com/articles/cost-optimization-engineering-perspective/</p><p></p><p>声明：本文由 InfoQ 翻译，未经许可禁止转载。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/lkTb4w34ymUgzXJIY4FN</id>
            <title>微软放弃在 OpenAI 董事会的观察员席位</title>
            <link>https://www.infoq.cn/article/lkTb4w34ymUgzXJIY4FN</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/lkTb4w34ymUgzXJIY4FN</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jul 2024 09:03:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 微软, OpenAI, 人工智能, 监管审查
<br>
<br>
总结: 微软放弃在 OpenAI 董事会的观察员席位，欧盟和美国对生成人工智能进行监管审查，微软可能面临反垄断调查，欧盟委员会正在调查大型数字市场参与者与生成式人工智能开发商和提供商之间的协议，微软已向 OpenAI 投入数十亿美元成为推动 AI 模型发展的领导者。 </div>
                        <hr>
                    
                    <p>当地时间7月10日，据外媒报道，微软表示，在欧洲和美国对生成人工智能进行监管审查之际，它将放弃在 OpenAI 董事会的观察员席位。</p><p>&nbsp;</p><p>微软副法律总顾问 Keith Dolliver 于周二晚些时候致信 OpenAI，称该职位提供了对董事会活动的深入了解，同时又不损害其独立性。</p><p>&nbsp;</p><p>但 CNBC 注意到，这封信的内容还透露，微软不再需要这个席位，因为“新成立的董事会取得了重大进展”。</p><p>&nbsp;</p><p>欧盟委员会此前表示，微软可能面临反垄断调查，因为它关注的是虚拟世界和<a href="https://aicon.infoq.cn/2024/shanghai/presentation/6042">生成人工智能</a>"市场。欧盟委员会是欧盟的执行机构，该委员会今年 1 月份表示，正在“调查大型数字市场参与者与生成式人工智能开发商和提供商之间达成的一些协议”，并将微软与 OpenAI 的合作列为其将要研究的一项具体交易。</p><p>&nbsp;</p><p>欧盟此后得出结论，观察员席位不会改变 OpenAI 的独立性，但欧盟监管机构正在寻求第三方对该交易的更多意见。英国竞争与市场管理局仍然心存疑虑。</p><p>&nbsp;</p><p>去年 11 月，在 OpenAI 首席执行官<a href="https://www.infoq.cn/article/HgI7G6Oth4C6PS4bsqR7?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search"> Sam Altman 被解雇</a>"、随后又迅速重新受雇的动荡时期之后，微软在 OpenAI 董事会中占据了一个无投票权的席位，以平息有关微软对该初创公司的兴趣的一些疑问。</p><p>&nbsp;</p><p>Altman 当时在给员工的一份说明中表示，<a href="https://www.infoq.cn/article/GdkidIFChUxVslICMamx?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">OpenAI</a>"“显然做出了与微软合作的正确选择，我很高兴我们的新董事会将把他们列为无投票权的观察员”。</p><p>&nbsp;</p><p>OpenAI 于 2022 年底发布 ChatGPT 聊天机器人后，成为全球最热门的初创公司之一。微软已向&nbsp;这家初创公司投入了数十亿美元，据报道，迄今为止其总投资额&nbsp;已增至 130 亿美元。鉴于其对 OpenAI 的投资和合作，这家科技巨头实际上已成为推动基础 <a href="https://aicon.infoq.cn/2024/shanghai/track/1710">AI 模型</a>"发展的领导者。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/c2fea73ea78c8371511990973</id>
            <title>为何我们决定从零开始创建 NGINX Gateway Fabric</title>
            <link>https://www.infoq.cn/article/c2fea73ea78c8371511990973</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/c2fea73ea78c8371511990973</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jul 2024 02:12:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: NGINX, Kubernetes, Gateway API, NGINX Gateway Fabric
<br>
<br>
总结: NGINX决定推出自己的Gateway API项目，而不是将Gateway API强塞到现有Ingress产品中。他们认为这样做会限制未来发展，并希望打造一款经得起时间考验的产品，满足未来需求。他们的目标是为Gateway API资源提供全面配置互操作性，规范服务网络的许多组件，摒弃厂商特定的CRD，助力创造更加美好的未来。 </div>
                        <hr>
                    
                    <p></p><blockquote>原文作者：Brian Ehlert - F5 产品管理总监，Matthew Yacobucci - F5 首席软件工程师原文链接：<a href="https://www.nginx-cn.net/blog/why-we-decided-to-start-fresh-with-our-nginx-gateway-fabric/">为何我们决定从零开始创建 NGINX Gateway Fabric</a>"转载来源：<a href="https://www.nginx-cn.net/">NGINX 中文官网</a>"</blockquote><p></p><p></p><p>NGINX 唯一中文官方社区 ，尽在&nbsp;<a href="https://www.nginx.org.cn/">nginx.org.cn</a>"</p><p>  </p><p>在 Kubernetes&nbsp;<a href="https://www.nginx-cn.net/resources/glossary/kubernetes-ingress-controller/">Ingress controllers</a>"&nbsp;领域，NGINX 取得了巨大成功。<a href="https://docs.nginx.com/nginx-ingress-controller/">NGINX Ingress Controller&nbsp;</a>"被广泛部署用于商业 Kubernetes 生产用例，同时也作为开源版进行开发和维护。因此，您可能会想当然地认为，当 Kubernetes 网络（<a href="https://gateway-api.sigs.k8s.io/">Gateway API</a>"）获得重大改进时，我们会再进一步，将其部署到我们的现有 Ingress 产品中。</p><p></p><p>然而，我们选择了一条不同的道路。通过评估全新 Gateway API 的强大潜能以及是否有可能彻底重塑 Kubernetes 中的互联处理方式，我们意识到将 Gateway API 实现强塞到现有 Ingress 产品中会限制未来发展。</p><p></p><p>因此，我们决定推出自己的 Gateway API 项目 —&nbsp;<a href="https://github.com/nginxinc/nginx-gateway-fabric">NGINX Gateway Fabric</a>"。该项目是开源项目，将以透明、协作的方式运行。我们很高兴与外部贡献者合作，并乐于分享最新成果，共同推动创新，造福于整个社区和行业。</p><p> </p><p></p><h2>我们为何决定推出自己的 Gateway API 项目</h2><p></p><p>尽管我们满怀激情和信心做出了围绕 Gateway API 创建一个全新项目的决定，但这一决定也基于合理的业务和产品战略逻辑。</p><p></p><p>想必 Kubernetes 采用者已经对 NGINX Ingress Controller 的开源版和商业版有所了解。两者都部署了经过严格测试的 NGINX 数据平面（在&nbsp;<a href="https://www.nginx-cn.net/products/nginx/">NGINX Plus</a>"&nbsp;和 NGINX 开源版反向代理中运行）。在 Kubernetes 之前，NGINX 的数据平面已在负载均衡和反向代理用例中有不凡表现。在 Kubernetes 中，我们的 Ingress controller 可完成相同类型的关键请求路由和应用交付任务。</p><p></p><p>NGINX 因轻量级、高性能、久经考验并可满足严苛环境要求的商业产品而享负盛名。我们在 Kubernetes Ingress controller 领域的产品战略与我们的反向代理产品策略相一致，即为简单用例提供强大的开源产品，并为关键业务应用环境中的生产 Ingress 控制提供具有更多特性和功能的商业产品。这一策略在 Ingress controller 领域中行之有效，部分原因是过去 Ingress controller 缺乏标准化，需要大量自定义资源定义（CRD）来提供负载均衡和反向代理高级功能，开发人员和架构师在 Kubernetes 以外的网络产品中享用这些高级功能。</p><p></p><p>我们的客户依赖并信任 NGINX Ingress Controller，其商业版本已经具备了 Gateway API 旨在提供的许多关键高级功能。此外，NGINX 很早就参与了 Gateway API 项目，并认识到 Gateway API 生态系统要达到完全成熟还需要几年的时间。（事实上，Gateway API 的许多规范都在不断演进，例如 GAMMA 规范，该规范有助于更好地集成 service mesh（服务网格）。）</p><p></p><p>但我们认为，将测试级 Gateway API 规范强塞到 NGINX Ingress Controller 中会给成熟的企业级 Ingress controller 带来无谓的不确定性和复杂性。我们销售的任何产品都必须具有稳定性和可靠性，并完全符合生产就绪要求。Gateway API 解决方案也会做到这一点，只是目前仍处于起步阶段。</p><p></p><p></p><h2>我们的 NGINX Gateway Fabric 目标</h2><p></p><p>对于 NGINX Gateway Fabric，我们的主要目标是打造一款经得起时间考验的产品，就像 NGINX Plus 和 NGINX 开源版一样。为了确保我们的 Gateway API 项目能够“满足未来需求”，我们意识到需要为其数据平面和控制平面尝试不同的架构选择。例如，我们可能需要研究不同的方法来管理四层和七层互联或尽量减少外部依赖项。我们最好从零开始，而不沿袭任何历史先例和遵从任何要求。虽然我们正在使用业经试用和测试的 NGINX 数据平面作为 NGINX Gateway Fabric 的基础组件，但对新想法也持开放态度。</p><p></p><p>我们还希望为 Gateway API 资源提供不受厂商限制的全面配置互操作性。与现有 Kubernetes Ingress 范式相比，Gateway API 最大的改进之一就是规范了服务网络的许多组件。从理论上讲，这种标准化可支持许多 Gateway API 资源轻松地进行交互和连接，助力创造更加美好的未来。</p><p></p><p>不过，创造这一未来的关键是摒弃厂商特定的 CRD（可能导致厂商锁定）。对于必须支持专为 Ingress controller 领域而设计的 CRD 的混合产品而言，这可能极具挑战性。而在以互操作性为第一要务的开源项目中，做到这一点则相对容易些。为了摒弃紧密关联的 CRD，我们需要构建一个新框架，仅关注 Gateway API 及其组成 API 所暴露的新层面。</p><p></p><p></p><h2>加入我们的 Gateway API 之旅</h2><p></p><p>目前，我们仍处于早期发展阶段。只有少数项目和产品实施了 Gateway API 规范，其中大多数都选择将其融入现有项目和产品中。</p><p></p><p>因此，现在是启动新项目的最佳时机。我们的 NGINX Gateway Fabric 项目完全开放，决策和项目管理高度透明。因为该项目使用<a href="https://go.dev/">&nbsp;Go&nbsp;</a>"编写而成，所以我们诚邀广大 Gopher 社区成员建言献策、提交 PR。</p><p>Gateway API 可能会改变整个 Kubernetes 世界。一些产品可能不再需要，新的产品或将出现。Gateway API 蕴藏着无限可能，虽然不知道它终将走向何方，但我们翘首以待。诚邀您加入我们，共创精彩未来！</p><p></p><p>您可以首先：</p><p>以贡献者的身份加入项目在实验室中试用实现执行测试并提供反馈</p><p>如欲加入该项目，请访问 GitHub 上的<a href="https://github.com/nginxinc/nginx-gateway-fabric">&nbsp;NGINX Gateway Fabric</a>"。</p><p></p><p>NGINX 唯一中文官方社区 ，尽在&nbsp;<a href="https://www.nginx.org.cn/">nginx.org.cn</a>"</p><p>更多 NGINX 相关的技术干货、互动问答、系列课程、活动资源：&nbsp;<a href="https://www.nginx.org.cn/">开源社区官网</a>"&nbsp;|&nbsp;<a href="https://mp.weixin.qq.com/s/XVE5yvDbmJtpV2alsIFwJg">微信公众号</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/cf4168df71c3561883262bdc6</id>
            <title>What's new in PikiwiDB(Pika) v4.0.0</title>
            <link>https://www.infoq.cn/article/cf4168df71c3561883262bdc6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/cf4168df71c3561883262bdc6</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Jul 2024 11:51:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: PikiwiDB, Floyd, RocksDB, Column-Family
<br>
<br>
总结: PikiwiDB 社区宣布 Pika v4.0.0 正式发布，新版本基于第三代存储引擎 Floyd，支持多种数据结构和强类型 key，提高了系统效率和数据质量。新增了 Mget 批量查询缓存和主从复制优化，提升了查询性能和复制效果。 </div>
                        <hr>
                    
                    <p>PikiwiDB 社区荣耀地宣告 —— 经过 9 个月打磨并在生产环境稳定运行 5 个月的 PikiwiDB (Pika) v4.0.0 【下文简称 Pika】今天正式发布。希望基于第三代存储引擎 Floyd 的这个新版本能为社区用户们带来更卓越的体验。</p><p></p><h1>1 重大改进</h1><p></p><p></p><h2>1.1 第三代存储引擎 Floyd</h2><p></p><p>Floyd 如同其前代 Blackwidow，基于 RocksDB，不仅支持基础的 String 结构，也原生支持了 Hash、List、Set、Stream 及 ZSet 等 KKV 形式的复合数据结构。</p><p></p><p>RocksDB 实例数可配置</p><p></p><p>摒弃了 Blackwidow 按数据类型采用 RocksDB 实例的物理隔离模式，Floyd 采用了 RocksDB 的 Column-Family 虚拟隔离机制，在单个 RocksDB 实例下可存储所有类型的数据。用户可自由设定 Pika 实例中每个 DB【等同于 Redis DB】中 RocksDB 实例的数量，而数据的存储则依据 key 的 hash 值分配至相应的 RocksDB 实例，减小了数据的空间放大和读放大效应，实现了机器资源的高效利用。</p><p></p><p>强类型 key</p><p></p><p><img src="https://static001.geekbang.org/infoq/7e/7e964b56b3e007ac9074426e7011236a.png" /></p><p></p><p>基于 RocksDB 的 Column-Family 虚拟隔离机制，Floyd 把所有类型的 key 和 string 一起存储在 Column-Family 0。在此存储基础之上，不同于 Blackwidow，可明确禁止不同类型的 key 重复【强类型】，这一设计旨在杜绝潜在的数据冗余与不一致性，与 Redis 服务特性保持一致，进一步提升了系统的整体效率与数据质量。</p><p></p><p>Pika v2.x 系列版本基于存储引擎 Nemo，v3.x 系列版本基于 Blackwidow，它们因为采用了物理隔离机制，无法低成本实现强类型 key，所有在 Redis TYPE 命令的结果中可能返回多种类型，而 Floyd 则完全兼容 Redis 只返回一种类型。</p><p></p><p>Floyd 详细说明</p><p></p><p>如果对 Floyd 存储引擎感兴趣，请详阅《Floyd 存储引擎》【链接：https://github.com/OpenAtomFoundation/pika/discussions/2052】。由于 Floyd 前后进行了多个版本的迭代，所以阅读该 github discussion 文档时请注意前后时间，如有相关冲突性说法，以最新日期的文字为准。</p><p></p><p>关键 PR：PikiwiDB(Pika) 支持 Floyd 存储引擎https://github.com/OpenAtomFoundation/pika/pull/2413添加 Floyd 的 compaction-filter 的 Gtesthttps://github.com/OpenAtomFoundation/pika/pull/2669Pika 不支持不同类型的重复 key, 写入重复 key 返回非法类型https://github.com/OpenAtomFoundation/pika/pull/2609对 HyperLogLog 和 String 进行类型隔离，确保 HyperLogLog 操作与 String 操作明确区分开https://github.com/OpenAtomFoundation/pika/pull/2720添加支持分区索引过滤的功能https://github.com/OpenAtomFoundation/pika/pull/2601</p><p></p><h2>1.2 Mget 批量查询缓存</h2><p></p><p>Pika v3.5.2 的热数据缓存只实现了对热点 Key 的点查 (如 get/hget)，在后续的 v3.5.3 和 v3.5.4 修复若干 bug 后，对热数据的点查目前已经非常稳定。然而并未支持批量查询 (如 mget etc)。</p><p></p><p>内部业务侧反馈批量查询速度比较慢，在 40C/256GiB/2TiB SATA SSD 规格机器上数据量超过 100GiB 时，Pika v3.3.6 30% 批量查询延迟超过 35ms。但由于 Pika 热数据缓存尚未支持批量查询，性能并未改善。</p><p></p><p>为了满足业务需求，Pika 团队开发了批量查询热数据缓存功能，显著提升了批量查询性能，降低了查询延迟和失败率。相关技术细节请阅读《PikiwiDB (Pika) 混合存储之批量查询》 【链接：https://mp.weixin.qq.com/s/KFLPruSdB66TMRxUfR9PbQ 】。</p><p></p><p>关键 PR ：Mget 支持多 key 查询缓存，记录未命中的 key 去 DB 中查询，提升 Pika 服务的读性能https://github.com/OpenAtomFoundation/pika/pull/2675修复 Mget 没有使用解析 ttl 的函数导致出现部分 key 的 ttl 未被更新，数据不一致的问题https://github.com/OpenAtomFoundation/pika/pull/2730</p><p></p><h2>1.3 主从复制</h2><p></p><p>Pika v3.3.6 有很多主从复制的缺陷。v4.0.0 版本对 Pika 全量复制及增量复制进行了大量优化和 bug 修复，取得了非常好的效果。</p><p></p><p>并在 info 命令中输出了 "repl_connect_status" 指标 (PR 2638)，以方便用户更加明确清晰的确定当前的主从复制状态。</p><p></p><p>关键 PR ：</p><p></p><p>修复批量扩容时，多个 slave 同时连接 master, 短时间多次 bgsave 导致部分从节点数据不完整的问题https://github.com/OpenAtomFoundation/pika/pull/2746修复 Spop 在写 binlog 时可能会出现竞态问题https://github.com/OpenAtomFoundation/pika/pull/2647修复多 DB 下全量同步超时后不重试的问题https://github.com/OpenAtomFoundation/pika/pull/2667修复多 DB 主从超时场景下，可能会出现窗口崩溃的问题https://github.com/OpenAtomFoundation/pika/pull/2666修复主从同步限速逻辑中重复解锁的问题https://github.com/OpenAtomFoundation/pika/pull/2657重构主从复制模式 slave 节点的主从同步线程模型，尽可能减少 binlog 消费阻塞问题https://github.com/OpenAtomFoundation/pika/pull/2638</p><p></p><h2>1.4 Redis Stream</h2><p></p><p>Redis Stream 类似于消息队列（MQ），以便更安全地传递消息。为了确保数据的安全性，底层引擎 BlackWidow 和 Floyd 中特别添加了对 Stream 数据类型的支持。</p><p></p><p>关键 PR：修复 pkpatternmatchdel 命令使用错误导致的 stream 类型数据删除异常的问题https://github.com/OpenAtomFoundation/pika/pull/2726修复 Keyspace 命令未计算 Stream 类型数据的问题https://github.com/OpenAtomFoundation/pika/pull/2705</p><p></p><h2>1.5 Compaction</h2><p></p><p>PikiwiDB (Pika) 的底层磁盘存储引擎 RocksDB 在进行 compaction 时会显著影响 PikiwiDB (Pika) 的读写性能。因此，控制好 compaction 是优化 Pika 读写性能的关键。</p><p></p><p>Floyd 使用了 v8.7.3 版本的 RocksDB，开放了更多 RocksDB 参数，以方便用户优化 RocksDB 性能：</p><p></p><p>enable-partitioned-index-filters： 支持加载分区索引过滤器，加快 RocksDB 查找速度。min-write-buffer-number-to-merge: 默认值为 1，如果将此值设置得更大，意味着需要更多的写缓冲区被填满后才进行 flush。这样可以减少 flush 的频率，增加数据在内存中的累积量，从而可能提高写入吞吐量。level0-stop-writes-trigger: 默认值为 36，定义了 L0 层中 sst 文件的最大数量，一旦达到这个数量，RocksDB 将会采取 暂停写入、强制 compaction 等措施来防止写入操作继续累积，以避免 L0 层变得过于庞大，进而可能导致写入放大、查询性能下降等问题。level0-slowdown-writes-trigger：默认值为 20，用于控制当 Level 0 的 SST 文件数量达到这个阈值时，触发写减速（write slowdown），防止 Level 0 的文件数量过多，导致后续 compaction 操作的压力过大。level0-file-num-compaction-trigger：默认值为 4，当 Level 0 的 SST 文件数量达到这个参数设定的阈值时，RocksDB 会开始执行 compaction 操作，将 Level 0 的文件合并到 Level 1，以减少 Level 0 的文件数量，降低读取延迟，并优化存储空间的利用率。max-subcompactions：默认值为 1，用于控制 RocksDB 中并发执行的 sub-compaction 任务数量，其值为 1 表示关闭 sub-compaction。如果系统资源充足，建议提升该参数以优化 compaction 效率。max-bytes-for-level-base：指定了 L1 SST 文件总的大小。这个大小是 RocksDB 进行数据分层管理和 compaction 决策的重要依据：如果 L1 层的大小设置得太小，可能会导致 L0 层的 compaction 过于频繁，进而影响写性能。反之，如果设置得太大，可能会占用较多的磁盘空间，并且影响读取性能，因为读取操作可能需要跨越更多的层级。Pika 没有在 pika.conf 中开放此参数给用户配置，而是使用其他参数（level0-file-num-compaction-trigger 和 write-buffer-size）计算后的结果。</p><p></p><p><code lang="text">storage_options_.options.max_bytes_for_level_base = g_pika_conf-&gt;level0_file_num_compaction_trigger() * g_pika_conf-&gt;write_buffer_size()
</code></p><p></p><p>关键 PR：添加 Floyd 的 compaction-filter 的 Gtesthttps://github.com/OpenAtomFoundation/pika/pull/2669添加支持分区索引过滤的功能https://github.com/OpenAtomFoundation/pika/pull/2601新增 RocksDB Compaction 策略动态调整参数，用户可以根据业务调整 Compaction 策略，降低 Compaction 操作对服务性能的损耗https://github.com/OpenAtomFoundation/pika/pull/2538</p><p></p><h2>1.6 可观测性</h2><p></p><p>v3.5 版本增加了包括命中率、每秒命中次数、Redis Cache 内存使用量、Redis Cache 个数、Redis Cache DB 个数 等指标，但是在集群方面的可观测性是缺失的。v4.0.0 对 Codis-Proxy 的 P99、P999、延迟等监控指标进行采集和展示，可以直观地反映线上 Codis-proxy 的运行情况。</p><p></p><p>v4.0.0 开始还提供新的工具：根据 pika benchmark 工具压测结果自动生成可视化的统计图表。</p><p></p><p>关键 PR：Codis 支持 info 命令，可以通过该命令查询 Codis-proxy 的 info 信息https://github.com/OpenAtomFoundation/pika/pull/2688Codis-proxy 新增 P99 P95 等监控耗时指标https://github.com/OpenAtomFoundation/pika/pull/2668添加 Pika 压测指标，提升 Pika 压测效率，并输出可视化的统计图表https://github.com/OpenAtomFoundation/pika/pull/2663</p><p></p><h2>1.7 测试集</h2><p></p><p>PikiwiDB(Pika) 测试集由 gtest 单测、Redis TCL 测试集和 Go 测试集组成。v4.0.0 中丰富了诸多特性的 go test 功能，并进一步完善了基本数据类型的 TCL 测试。</p><p></p><p>关键 PR：添加 Floyd 的 compaction-filter 的 Gtesthttps://github.com/OpenAtomFoundation/pika/pull/2669Pika Geo 数据类型增加 TCL 测试，并修复测试过程中遇到的缺陷https://github.com/OpenAtomFoundation/pika/pull/2753</p><p></p><h2>1.8 跨平台</h2><p></p><p>PikiwiDB(Pika) 以往仅支持 centos 和 ubuntu 等 linux 平台，v3.5 开始支持 Mac 等平台。v4.0.0 将对 Mac 平台的支持扩展至 FreeBSD 平台。</p><p></p><p>关键 PR：Pika 支持在 FreeBSD14 平台上进行编译https://github.com/OpenAtomFoundation/pika/pull/2711</p><p></p><h1>2 改进列表</h1><p></p><p>下面详细列出了本次发版的主要功能升级和改进。</p><p></p><h2>2.1 新特性</h2><p></p><p>Pika Geo 数据类型增加 TCL 测试，并修复测试过程中遇到的缺陷https://github.com/OpenAtomFoundation/pika/pull/2753Pika 支持在 FreeBSD14 平台上进行编译打包https://github.com/OpenAtomFoundation/pika/pull/2711Pika 线程整理，避免启动过多无用线程，对不同的线程进行命名，方便问题定位https://github.com/OpenAtomFoundation/pika/pull/2697Mget 支持多 key 查询缓存，记录未命中的 key 去 DB 中查询，提升 Pika 服务的读性能https://github.com/OpenAtomFoundation/pika/pull/2675Codis 支持 info 命令，可以通过该命令查询 Codis-proxy 的 info 信息https://github.com/OpenAtomFoundation/pika/pull/2688添加 Floyd 的 compaction-filter 的 Gtesthttps://github.com/OpenAtomFoundation/pika/pull/2669Codis-proxy 新增 P99 P95 等监控耗时指标https://github.com/OpenAtomFoundation/pika/pull/2668添加 Pika 压测指标，提升 Pika 压测效率，并输出可视化的统计图表https://github.com/OpenAtomFoundation/pika/pull/2663Pika 主从复制新增监控指标 repl_connect_status, 可以更加明确清晰的确定当前的主从复制的状态https://github.com/OpenAtomFoundation/pika/pull/2638Pika 不支持不同类型的重复 key, 写入重复 key 返回非法类型https://github.com/OpenAtomFoundation/pika/pull/2609添加支持分区索引过滤的功能https://github.com/OpenAtomFoundation/pika/pull/2601Pika 支持第三代存储引擎 Floyd, 通过支持多 rocksdb 实例、对 Blob 的使用进行优化、对过期数据的清理进行优化，提升了 Pika 实例的读写性能https://github.com/OpenAtomFoundation/pika/pull/2413</p><p></p><h2>2.2 bug 修复</h2><p></p><p>修复 iter 未被析构，导致 pkpatternmatchdel 在返回之前不会删除 iter，这可能会导致 rocksdb 永远引用一个版本，导致数据不符合预期的问题https://github.com/OpenAtomFoundation/pika/pull/2785修复 config 参数 min-blob-size 带单位时解析错误的问题https://github.com/OpenAtomFoundation/pika/pull/2767修复 zverank 返回值异常的问题https://github.com/OpenAtomFoundation/pika/pull/2673修复 Pika-port 传输数据过程中报错的问题https://github.com/OpenAtomFoundation/pika/pull/2758修复因为堆上分配的缓冲区越界导致 Dbsize 命令运行时崩溃的问题https://github.com/OpenAtomFoundation/pika/pull/2749修复批量扩容时，多个 slave 同时连接 master, 短时间多次 bgsave 导致部分从节点数据不完整的问题https://github.com/OpenAtomFoundation/pika/pull/2746修复参数未初始化导致 slotsscan 等命令不能和 bgsave 命令相互制衡的问题https://github.com/OpenAtomFoundation/pika/pull/2745修复 Slotmigrate 迁移数据的过程中，返回值设置错误，异常场景下会终止数据迁移的问题https://github.com/OpenAtomFoundation/pika/pull/2741修复 Mget 没有使用解析 ttl 的函数导致出现部分 key 的 ttl 未被更新，数据不一致的问题https://github.com/OpenAtomFoundation/pika/pull/2730修复 pkpatternmatchdel 命令使用错误导致的 stream 类型数据删除异常的问题https://github.com/OpenAtomFoundation/pika/pull/2726修复 pkpatternmatchdel 不能正确删除掉对应的 keys 的问题https://github.com/OpenAtomFoundation/pika/pull/2717修复 ACL 密码验证错误问题https://github.com/OpenAtomFoundation/pika/pull/2714修复 Keyspace 命令未计算 Stream 类型数据的问题https://github.com/OpenAtomFoundation/pika/pull/2705对部分命令定制化处理逻辑，避免写 binlog 导致从节点的 binlog 解析失败的问题https://github.com/OpenAtomFoundation/pika/pull/2793修复 Pika cmdID 赋值在 Cmd 初始函数中，可能会导致并发构造的时候出现内存泄漏的问题https://github.com/OpenAtomFoundation/pika/pull/2692修复 ExpectedStale 未考虑 String 类型，如果存在已经过期的 String 类型的 key, ExpectedStale 会返回错误的问题https://github.com/OpenAtomFoundation/pika/pull/2682修复 Spop 在写 binlog 时可能会出现竞态问题https://github.com/OpenAtomFoundation/pika/pull/2674db instance 设置不合理时，给用户错误提示https://github.com/OpenAtomFoundation/pika/pull/2672修复 server_stat 中的数据竞态问题https://github.com/OpenAtomFoundation/pika/pull/2671修复多 DB 下全量同步超时后不重试的问题https://github.com/OpenAtomFoundation/pika/pull/2667修复多 DB 下全量同步超时后不重试的问题https://github.com/OpenAtomFoundation/pika/pull/2666修复主从同步限速逻辑中重复解锁的问题https://github.com/OpenAtomFoundation/pika/pull/2657发版支持自动打包 centos7 和 centos8 平台的二进制编译包https://github.com/OpenAtomFoundation/pika/pull/2535修复 Codis 侧的 getrange 命令没有返回预期结果的问题https://github.com/OpenAtomFoundation/pika/pull/2510</p><p></p><h2>2.3 提升改进项</h2><p></p><p>更新 Pika Docker Readme, 可以按照 Readme 在 Docker 中部署 Pika 服务https://github.com/OpenAtomFoundation/pika/pull/2743优化重复查询 meta value 导致影响 Pika 服务读写性能的问题https://github.com/OpenAtomFoundation/pika/pull/2735支持对更多的 RocksDB 参数进行动态调整，用户根据不同的业务使用场景调整参数提升 Pika 的读写性能https://github.com/OpenAtomFoundation/pika/pull/2728对 HyperLogLog 和 String 进行类型隔离，确保 HyperLogLog 操作与 String 操作明确区分开https://github.com/OpenAtomFoundation/pika/pull/2720更新了 PR 标题验证，不允许在标题末尾出现中文字符https://github.com/OpenAtomFoundation/pika/pull/2718重构主从复制模式 slave 节点的主从同步线程模型，尽可能减少 binlog 消费阻塞问题https://github.com/OpenAtomFoundation/pika/pull/2638新增 RocksDB Compaction 策略动态调整参数，用户可以根据业务调整 Compaction 策略，降低 Compaction 操作对服务性能的损耗https://github.com/OpenAtomFoundation/pika/pull/2538</p><p></p><h2>2.4 发版 tag</h2><p></p><p>​    https://github.com/OpenAtomFoundation/pika/releases/tag/v4.0.0</p><p></p><h1>3 社区</h1><p></p><p>感谢所有为 v4.0.0 做出贡献的社区成员，包括 issue/PR 提交者、代码 reviewer 【排名不分先后，依据字母序列】：</p><p></p><p>AlexStocksbaerwangchejingecheniujhchienguoguangkun123gukj-spellongfar-ncylqxhubluky116Mixficsolsaz97wangshao1</p><p></p><p>PikiwiDB (Pika) 开源社区热烈欢迎您的参与和支持。如果您有任何问题、意见或建议，请扫码添加 PikiwiDB 小助手【微信号: PikiwiDB】为好友，它会拉您加入官方微信群。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>