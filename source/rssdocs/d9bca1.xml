<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/oF3SIQhV1ZLgRD8Zj3Zp</id>
            <title>如何借助数学打造更好的AI算法基础并消除大模型幻觉？</title>
            <link>https://www.infoq.cn/article/oF3SIQhV1ZLgRD8Zj3Zp</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/oF3SIQhV1ZLgRD8Zj3Zp</guid>
            <pubDate></pubDate>
            <updated>Sat, 06 Jul 2024 15:14:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 世界人工智能大会, 数学与人工智能, 机器学习, 深度神经网络
<br>
<br>
总结: 2024年世界人工智能大会在上海举办，围绕数学与人工智能展开讨论，涉及机器学习和深度神经网络等主题，探讨最新研究成果和未来发展趋势。 </div>
                        <hr>
                    
                    <p>7月4日，由斯梅尔数学与计算研究院（Smale Institue of Mathematics &amp; Computation）主办的2024年世界人工智能大会（WAIC）“数学与人工智能”学术会议在上海世博中心圆满落幕。</p><p>&nbsp;</p><p>作为全球性高级别学术研讨会，此次会议由华院计算技术（上海）股份有限公司创始人董事长、斯梅尔数学与计算研究院执行院长宣晓华担任主持，美国卡内基梅隆大学计算机科学学院名誉教授、1995年图灵奖获得者及美国三院院士Manuel Blum，欧洲人文和自然科学院外籍院士、欧洲科学院院士、上海交通大学自然科学研究院院长、上海交通大学数学科学学院讲席教授金石，欧洲科学院院士、牛津大学应用数学教授Jose A.Carrillo，牛津大学DeepMind人工智能教授Michael Bronstein，伦敦大学学院人工智能中心主任、英国研究与创新署基础人工智能博士培训中心主任、UiPath杰出科学家David Barber，世界科学院院士、阿勒福赞杰出青年科学家国际奖得主、南非布隆方丹自由州大学和台湾中华医科大学教授Abdon Atangana，南非北方大学应用数学系教授、系主任Oluwole Daniel Makinde，阿联酋人工智能大学副教授、副系主任Martin Takac出席，菲尔兹奖得主、法国高等科学研究所（IHES）终身数学教授Laurent Lafforgue，澳大利亚国立大学计算机科学研究学院（RSCS）荣誉教授Marcus Hutter以及西南财经大学统计学院教授、统计研究中心主任、博士生导师林华珍通过线上方式参与了此次圆桌讨论。上海市经济和信息化委员会主任张英出席会议并致辞。</p><p></p><p><img src="https://uploader.shimo.im/f/SkVLkXWoPWHkrVAk.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjAyNzg3OTksImZpbGVHVUlEIjoiVk1BUEw3bEJwTVM4T2JBZyIsImlhdCI6MTcyMDI3ODQ5OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNTU0MDc1MH0.NckQM81jLl4lF2TX-HmuCsZJJRs5tvwK-cXmj5PD720" /></p><p></p><p>这些全球顶尖的数学家和科学家们围绕机器学习与人工智能的数学基础、人工智能中的算法研究、AI4Science以及AI4Math等主题进行深入讨论，共同探讨数学与人工智能领域的最新研究成果和未来发展趋势。</p><p>&nbsp;</p><p>会上，94岁高龄的斯梅尔数学与计算研究院名誉主席斯蒂芬·斯梅尔教授以线上视频的形式发表了他关于“21世纪的18道数学问题”中的“智能的极限”的观点。1998年，斯梅尔列出了21世纪的18道数学问题。“斯梅尔问题”，沿袭了1900年著名的希尔伯特数学问题的精神。“斯梅尔问题”有一部分就来自希尔伯特数学问题。</p><p></p><p><img src="https://uploader.shimo.im/f/xQeWKl8nhpOEuYim.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjAyNzg3OTksImZpbGVHVUlEIjoiVk1BUEw3bEJwTVM4T2JBZyIsImlhdCI6MTcyMDI3ODQ5OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNTU0MDc1MH0.NckQM81jLl4lF2TX-HmuCsZJJRs5tvwK-cXmj5PD720" /></p><p></p><p>会议围绕三大议题进行探讨，在关于“如何借助数学打造更好的人工智能算法基础（特别是深度神经网络和Transformer领域），从而提升人工智能算法的效率和鲁棒性、增加因果推理能力和可解释性，消除模型的幻觉现象等？”的议题上，Michael Bronstein教授发表了深刻见解，他高度肯定了数学家在构筑人工智能算法基础方面的卓越贡献。Bronstein教授从两个维度进行了详尽剖析：一方面，不论是预测性人工智能还是生成式人工智能，其核心均离不开优化过程，因此数学家的任务就是不断地探索与开发更好的算法，提高算法的效率。另一方面，他强调数学分析对于理解人工智能理论的重要性，特别是生成式人工智能，其执行任务的能力在很大程度上依赖算法的设计，而数学家通过优化算法，不仅提升了预测的准确性，还赋予了AI更强的预见性，使其在面对复杂任务时能够做出更为合理的决策。</p><p></p><p><img src="https://uploader.shimo.im/f/pYX87ZfOPfdescwl.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjAyNzg3OTksImZpbGVHVUlEIjoiVk1BUEw3bEJwTVM4T2JBZyIsImlhdCI6MTcyMDI3ODQ5OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNTU0MDc1MH0.NckQM81jLl4lF2TX-HmuCsZJJRs5tvwK-cXmj5PD720" /></p><p></p><p>这一观点得到了在场嘉宾的广泛共鸣，Martin Takac教授则进一步阐述道：“我们希望可以不断推进、拓宽并深化算法的边界，追求算法更高的效率与效能。”此番讨论奠定了数学研究在人工智能方面的演进中所扮演的重要角色。</p><p></p><p><img src="https://uploader.shimo.im/f/8Vbaj8UB7duLzFdr.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjAyNzg3OTksImZpbGVHVUlEIjoiVk1BUEw3bEJwTVM4T2JBZyIsImlhdCI6MTcyMDI3ODQ5OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNTU0MDc1MH0.NckQM81jLl4lF2TX-HmuCsZJJRs5tvwK-cXmj5PD720" /></p><p></p><p>随着人工智能的飞速发展，它已广泛渗透至生活的各个角落，虽然为现代人类社会提高了前所未有的效率，带来了诸多的便捷与福祉。然而，人工智能依旧面临着一系列理论和实践上的挑战。因此，会议也以“对于通用人工智能（AGI）、大模型的涌现现象、意识智能等前沿研究领域有哪些好的数学模型？智能的极限又是什么？”为议题，深入剖析现有的数学模型是如何推动人工智能、大模型的发展，以此探讨数学与人工智能之间双向促进、共同发展的互动关系。针对此议题，David Barber教授深刻地指出数学的纯粹、清晰性和复杂的人类推理、语言、知识以及人工智能之间存在着巨大鸿沟。他强调，利用数学模型来驱动人工智能，促使人工智能更理解人类语言，是一项充满挑战又极具潜力的任务。同时，他也乐观地表示，目前已有的统计学、逻辑推理等已经为人工智能的发展奠定了坚实的基础，相信未来也会有更精准高效的数学模型来协助人工智能的发展。</p><p></p><p><img src="https://uploader.shimo.im/f/RphJqWZNISKrFGHF.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjAyNzg3OTksImZpbGVHVUlEIjoiVk1BUEw3bEJwTVM4T2JBZyIsImlhdCI6MTcyMDI3ODQ5OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNTU0MDc1MH0.NckQM81jLl4lF2TX-HmuCsZJJRs5tvwK-cXmj5PD720" /></p><p></p><p>金石教授则从另一个角度切入，他认为一个理想的数学模型应当是要将领域知识和物理定律完美结合，这样的模型才能更有效地解决复杂的问题。Michael Bronstein在探讨“智能的极限”时，以富有哲理的比喻阐述了人类对于智能认知的演进过程。他提出现在的人类看到人工智能的进步，如同十年前我们看科幻小说幻想今天一样，虽然今日我们见证了人工智能的显著进步，却仍感觉有些不一样。他强调，人工智能的极限就如同人类不断追求与设定的新目标，是一个动态变化、永无止境的过程。</p><p>&nbsp;</p><p><img src="https://uploader.shimo.im/f/rFy56IrlYkqTHmPo.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjAyNzg3OTksImZpbGVHVUlEIjoiVk1BUEw3bEJwTVM4T2JBZyIsImlhdCI6MTcyMDI3ODQ5OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNTU0MDc1MH0.NckQM81jLl4lF2TX-HmuCsZJJRs5tvwK-cXmj5PD720" /></p><p></p><p>Marcus Hutter教授对于人工智能的见解深刻且前瞻，他坚信人工智能的作用不仅仅是预测，而是拥有影响世界的决策能力。因此，他提出通过将最优决策理论与未知世界的预测理论相结合，可以构建在任意未知环境中都能做出最优决策的AGI系统，如ASI（强人工智能）。在过去的几年里，Marcus Hutter教授已经证实了我们拥有很多优化的概率，他做出了将智力这一非正式概念数学化的努力，提出了一个从0到1的评分系统来评估AI的智能程度。他认为理想的智能测量应能捕捉所有智力的关键特征，包括记忆、概括能力、推理、理解力和创造力等。虽然目前的研究仍处于基础的开发阶段，但他积极利用逼近法来让计划得以继续执行，使其更优化、更完美，以确保每一步都朝着既定目前稳步前进。对于当前的数据模型，Marcus Hutter教授也持乐观的态度。他表示目前的数据模型虽然尚在发展之中，但未来他会让数据模型更加接近理想的状态。</p><p>&nbsp;</p><p><img src="https://uploader.shimo.im/f/JHoL1bL35ZxoX6JR.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjAyNzg3OTksImZpbGVHVUlEIjoiVk1BUEw3bEJwTVM4T2JBZyIsImlhdCI6MTcyMDI3ODQ5OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNTU0MDc1MH0.NckQM81jLl4lF2TX-HmuCsZJJRs5tvwK-cXmj5PD720" /></p><p></p><p>Abdon Atangana教授对此表示：我们每天都在创造与发明，然后通过验证我们的成果，来为人工智能注入更多的内容，让它接受更多的培训和进步。诚然，人工智能的出现可能让人类不再需要用自己的大脑进行研究，但是实际上人工智的发展仍然需要靠人类去进行深度参与和补充，尤其是它无法直接向我们验证新定理和新方法论。因此，关于人工智能的未来，我更想看到的是它可以超越现在的界限，可以替代人类验证一些新方向和新主题。</p><p></p><p><img src="https://uploader.shimo.im/f/RaXRDiFmniqjlLkb.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjAyNzg3OTksImZpbGVHVUlEIjoiVk1BUEw3bEJwTVM4T2JBZyIsImlhdCI6MTcyMDI3ODQ5OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNTU0MDc1MH0.NckQM81jLl4lF2TX-HmuCsZJJRs5tvwK-cXmj5PD720" /></p><p></p><p>数学作为人工智能的基石之一，其基本理论和应用技术的深入研究是人工智能行稳致远的关键。因此，要想让人工智能在各行各业取得稳健的发展，必须先确保数学基础问题的有效解决。同时，人工智能的飞速发展和广泛引用，也推动了数学领域的研究不断向前，为数学提供了更多的研究视角、方法和挑战，促进了数学理论的创新和突破。既然人工智能的发展离不开数学的支持，那么人工智能是否也能反过来对数学产生促进作用？</p><p></p><p><img src="https://uploader.shimo.im/f/HsZ4pUw7Kngy7guT.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjAyNzg3OTksImZpbGVHVUlEIjoiVk1BUEw3bEJwTVM4T2JBZyIsImlhdCI6MTcyMDI3ODQ5OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNTU0MDc1MH0.NckQM81jLl4lF2TX-HmuCsZJJRs5tvwK-cXmj5PD720" /></p><p></p><p>“人工智能如何助力数学研究，特别是在定理证明、证明验证以及猜想生成方面？”这是本次会议的最后一个议题。在这个议题上，Jose A.Carrillo教授以风趣又不失深刻的言辞表达了自己的看法：“对于我个人而言，我并不担心我的工作会被替代。诚然，目前的人工智能虽然在一定程度上可以可以辅助数学家进行错误的检查，避免失误，但是人工智能的发展仍然面临着诸多未解难题，至少我（作为一名数学家）目前这几年仍不会失业。”</p><p></p><p><img src="https://uploader.shimo.im/f/9X5C2bqQW4AgiCvl.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjAyNzg3OTksImZpbGVHVUlEIjoiVk1BUEw3bEJwTVM4T2JBZyIsImlhdCI6MTcyMDI3ODQ5OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNTU0MDc1MH0.NckQM81jLl4lF2TX-HmuCsZJJRs5tvwK-cXmj5PD720" /></p><p>&nbsp;</p><p>世间万物兼具两面性，数学与人工智能相互间的促成关系背后也可能潜藏风险。在现场观众对这一问题感到疑惑时，Manuel Blum教授以深邃的洞察力提出了独到见解，他指出：“人类总有一天可能都会毁灭，但是人工智能的出现并非这一宿命的必然推手，相反地，人工智能可能是协助人类规避风险的关键钥匙。”Oluwole Daniel Makinde教授对此表示赞同，他补充道：“我们应当以积极乐观的心态，相信人工智能会给我们带来创新！”</p><p></p><p><img src="https://uploader.shimo.im/f/7XTrRxxNyWtOzMMP.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjAyNzg3OTksImZpbGVHVUlEIjoiVk1BUEw3bEJwTVM4T2JBZyIsImlhdCI6MTcyMDI3ODQ5OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNTU0MDc1MH0.NckQM81jLl4lF2TX-HmuCsZJJRs5tvwK-cXmj5PD720" /></p><p></p><p>上海市经济和信息化委员会主任张英代表上海市政府到场祝贺并欢迎全球各位数学家来到上海参加2024世界人工智能大会及“数学与人工智能”学术会议。张英主任强调，李强总理在WAIC会议开幕式讲话指出上海正全力构建一个技术策源、应用示范和制度创新人才集聚的高地。从技术策源的角度来看，数学就是推动技术策源最为核心与关键的力量。她进一步指出，鉴于数学与人工智能之间不可分割的紧密联系，政府高度重视人工智能的应用发展，以及数学和人工智能之间的关系和推动力。这不仅是本次“数学与人工智能”学术会议得以成功举办的重要背景，也是主办方对本次会议寄予厚望的根源所在。</p><p>&nbsp;</p><p><img src="https://uploader.shimo.im/f/0yNFGyB4N33iwrM9.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjAyNzg3OTksImZpbGVHVUlEIjoiVk1BUEw3bEJwTVM4T2JBZyIsImlhdCI6MTcyMDI3ODQ5OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNTU0MDc1MH0.NckQM81jLl4lF2TX-HmuCsZJJRs5tvwK-cXmj5PD720" /></p><p></p><p>此次在WAIC会议主会场举办全球性数学与人工智能会议，充分体现了本次WAIC会议的全球性和理论前沿性，也体现了上海致力于打造全球性人工智能基础研究前沿和数学等AI算法技术相关基础学科研究高地的期许。</p><p>&nbsp;</p><p><img src="https://uploader.shimo.im/f/lwhSlaLsPJo1rY5O.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjAyNzg3OTksImZpbGVHVUlEIjoiVk1BUEw3bEJwTVM4T2JBZyIsImlhdCI6MTcyMDI3ODQ5OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNTU0MDc1MH0.NckQM81jLl4lF2TX-HmuCsZJJRs5tvwK-cXmj5PD720" /></p><p></p><p>“数学与人工智能”学术会议，作为一次思想的盛宴、智慧的碰撞，见证了数学家们围绕前沿问题展开的激烈讨论与深刻洞见。在这里，思想的火花汇聚成照亮前行道路的明灯，预示着数学理论与人工智能技术的深度融合将开启一个充满无限可能的新纪元。在这个充满无限可能的新时代，我们有理由相信，数学与人工智能将携手并进，共同书写人类科技进步的崭新篇章。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/btNrraN8b6wpsaaR71vK</id>
            <title>阶跃星辰姜大昕：要实现AGI，“万亿参数”和“多模融合”缺一不可</title>
            <link>https://www.infoq.cn/article/btNrraN8b6wpsaaR71vK</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/btNrraN8b6wpsaaR71vK</guid>
            <pubDate></pubDate>
            <updated>Sat, 06 Jul 2024 10:02:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AGI, 万亿参数, 多模融合, Scaling Law
<br>
<br>
总结: 阶跃星辰姜大昕在演讲中指出，要实现人工智能通用智能（AGI），必须同时注重“万亿参数”和“多模融合”两个方向的发展。通过对Scaling Law的探索和多模态能力的提升，才能最终实现AGI的目标。 </div>
                        <hr>
                    
                    <p>阶跃星辰姜大昕：要实现AGI，“万亿参数”和“多模融合”缺一不可</p><p></p><p>近日，在世界智能人工大会WAIC启明创投·创业与投资论坛上，阶跃星辰创始人、CEO 姜大昕博士发表了主题为《攀登 AGI 的路径与实践：万亿参数和多模融合》的演讲，分享了对于大模型发展现状与趋势的观察与思考。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6b/6bd883870939b909e763c4c9355c610d.jpeg" /></p><p></p><p>阶跃星辰创始人、CEO 姜大昕博士发表演讲</p><p></p><p>在演讲中，姜大昕重点阐述了一个核心观点：探索AGI路径，“Scaling Law”和“多模态”是相辅相成、缺一不可的两个方向。两者齐头并进，最终到达AGI。</p><p></p><h2>Scaling Law 仍处于陡峭区间，万亿参数是基本出发点</h2><p></p><p></p><p>近年来，GPT 系列模型的演进，客观上验证了 Scaling Law 的有效性。模型参数量决定模型能力的上限。从模型效果看，参数量增大确实带来了性能上的飞跃。虽然业内围绕“Scaling Law还能走多远”尚未形成共识，但阶跃星辰认为，参数量接下来再提高一个数量级是依然成立的。Scaling Law 目前依然奏效，模型性能仍然在随着参数量、数据量和计算量的增加呈幂次方增长。在此发展过程中，万亿参数量已经成为一个基本的入门门槛。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/ca9095397dad987fe937d7da4313022b.png" /></p><p></p><p>正是基于这样的认知，阶跃星辰很早便启动了万亿参数模型的训练。从千亿到万亿，模型的参数规模提升了一个量级，难度也提升了十倍以上。为此，阶跃星辰加大资源投入，尤其在系统和算法上积极探索，最终走通了万亿参数 MoE 大模型训练的道路。在 WAIC 2024 上，阶跃星辰发布了全新的 Step-2 万亿参数语言大模型正式版。根据从逻辑推理、世界知识、数学和编程等多个维度进行的权威测试，Step-2 模型能力都已全面接近国际主流模型，在部分测试集甚至实现了超越。</p><p></p><p>多模态是构建世界模型的基础能力，将走向理解与生成的统一</p><p></p><p>在不断攀登 Scaling Law 的同时，阶跃星辰也强调，多模态是构建世界模型的基础能力，是通向 AGI 的必经之路。从算法角度看，世界模型的演进会分为三个阶段：</p><p></p><p>第一阶段是模拟物理世界；</p><p></p><p>第二阶段是通过具身智能和物理世界交互，主动探索物理世界；</p><p></p><p>第三阶段是通过发展系统能力，发现新的物理规律，归纳物理世界。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7f/7f3fa68826934a1cb6ac29e22a46e0f1.png" /></p><p></p><p>从模拟世界，到探索世界，再到归纳世界，多模态是贯穿这三个阶段的基本能力。目前，全球科技巨头正在积极探索并布局多模融合的路径，多模态大模型研发的脚步正越走越快。然而，多模态领域目前存在的问题是，视觉的理解模型和生成模型是分开发展的。其造成的结果就是理解模型的理解能力强而生成能力弱，或者生成模型的生成能力强而理解能力弱。因此，多模态大模型接下来面临的一项关键挑战，就是能否将理解和生成统一在一个模型里。</p><p></p><p>目前，阶跃星辰正在朝着这个方向努力，并取得了一些阶段性的进展。在 WAIC 2024 上，新升级的 Step-1.5V 千亿参数多模态大模型性能大幅提升，具备更出色的视频理解能力；新发布的 Step-1X 图像生成大模型，则是阶跃星辰首次推出多模态生成大模型。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0et63jbeKZQxvIgWP6kX</id>
            <title>智谱新发开源大模型：9B参数，覆盖编程场景</title>
            <link>https://www.infoq.cn/article/0et63jbeKZQxvIgWP6kX</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0et63jbeKZQxvIgWP6kX</guid>
            <pubDate></pubDate>
            <updated>Sat, 06 Jul 2024 09:05:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, AI热潮, GLM-4, CodeGeeX
<br>
<br>
总结: 大模型是解决成本和收益平衡问题的关键，GLM-4和CodeGeeX代表了新一代基座大模型技术的前沿，引领着AI热潮的发展。 </div>
                        <hr>
                    
                    <p>“大模型能够在一个模型上提供泛化能力，解决一系列场景和应用的多样需求，从而解决成本和收益的平衡的问题，这是它的本质特点。”</p><p></p><p>7月4日，在世界人工智能大会的产业发展主论坛上，智谱AI CEO张鹏表示，当下因大模型而掀起的AI热潮和之前有所不同，在过去，AI技术解决了一些实际问题，但如今的大模型发展带来了更重要的类人认知能力。张鹏表示，在过去AI泛用性不够且成本太高。但大模型带来了一个新的机遇，它能够在一个模型上提供泛用化能力，这也是用新一代大模型技术赋能实体经济的主要方向——把原来一个底座投入很大但是收益很小的结构，变成一个倒金字塔结构，真正放大它的价值。</p><p></p><p><img src="https://static001.geekbang.org/infoq/85/85f147036f20f5b468abc4d4ed399bcf.png" /></p><p></p><p></p><h2>GLM-新一代基座大模型技术前沿与产业应用论坛举办</h2><p></p><p>&nbsp;</p><p>7月5日，在由清华大学计算机系知识工程实验室主办，AI TIME承办，东浩兰生（集团）有限公司和智谱AI协办的GLM-新一代基座大模型技术前沿与产业应用论坛上，嘉宾们聚焦GLM-4大模型，共同分享了GLM-4大模型的最新研究成果和理论突破，探索GLM-4的技术前沿、产业生态和落地应用。</p><p></p><h2>第4代CodeGeeX发布免费智能AI编程助手</h2><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/34/345ffc593098689440b96bc85d549a91.png" /></p><p></p><p>论坛上，智谱AI CodeGeeX技术负责人郑勤锴发布了第4代CodeGeeX代码大模型CodeGeeX4-ALL-9B。CodeGeeX4-ALL-9B作为最新一代CodeGeeX4系列模型的开源版本，在GLM-4强大语言能力的基础上继续迭代，大幅增强代码生成能力。使用CodeGeeX4-ALL-9B单一模型，即可支持代码补全和生成、代码解释器、联网搜索、工具调用、仓库级长代码问答及生成等全面功能，覆盖了编程开发的各种场景。在多个权威代码能力评测集的表现，是百亿参数量级以下性能最强的模型，甚至超过数倍规模的通用模型，在推理性能和模型效果上得到最佳平衡。</p><p></p><p><img src="https://static001.geekbang.org/infoq/78/788e1a25691168e6db4ff4992f187056.png" /></p><p></p><p>目前CodeGeeX的个人用户数量已经超过100万，目前CodeGeeX对个⼈用户完全免费，在各种主流IDE均可免费下载使⽤。</p><p></p><p>除了第4代CodeGeeX发布，论坛现场，清华大学计算机科学与技术系长聘教授黄民烈、中国人民大学信息学院计算机系副教授张静、浙江大学计算机科学与技术学院副教授杨洋、上海交通大学电子信息与电气工程学院长聘教轨副教授戴国浩、幂律智能创始人兼CEO涂存超等顶尖学者，深入探讨了GLM大模型对行业及产业发展的影响。</p><p></p><p>张鹏分享了GLM-4在应用中的多个创新案例，特别是在智能内容生成、行业自动化以及用户个性化定制服务等方面的突破。展示了GLM-4在复杂商业环境中的价值。</p><p></p><p>“过去几年智谱的商业实践为我们积累了非常多的经验，不敢说是 best practice，但是 better practice。”张鹏在演讲中表示。GLM-4 在智能体和工具调用等方面能力的突飞猛进，让企业内部原生 AI 架构的实现变为可能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c2/c26f438a5203fbac4922ab38293c5f98.png" /></p><p></p><h2>GLM基座大模型携应用成果亮相WAIC 2024</h2><p></p><p>&nbsp;</p><p>WAIC 2024智谱AI展位展示了以智谱大模型开放平台bigmodel.cn和智谱大模型产品矩阵为核心的系列创新成果。</p><p></p><p>作为本届WAIC镇馆之宝，智谱大模型开放平台 bigmodel.cn 是体验智谱 GLM 系列大模型的最佳方式。全新升级的bigmodel.cn已经接入最新GLM大模型全家桶，一键微调、All Tools API 调用等新功能也已上线。</p><p>&nbsp;</p><p>不管是技术极客、专业工程师，又或者是寻求大模型能力的企业，都可以在平台上找到适合自己的产品和服务。目前bigmodel.cn已有超过40万企业客户和开发者，日均调用量为600亿Tokens，过去4个月API每日消费量增长了90倍以上。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/s98WkDlZDxxf0P633O71</id>
            <title>隐私计算被推向新高度！信通院牵头编写《隐私计算产品通用安全分级白皮书》，现已发布</title>
            <link>https://www.infoq.cn/article/s98WkDlZDxxf0P633O71</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/s98WkDlZDxxf0P633O71</guid>
            <pubDate></pubDate>
            <updated>Sat, 06 Jul 2024 07:43:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据要素市场, 隐私计算技术, 安全分级, 个人信息匿名化
<br>
<br>
总结: 产学研共同构建新技术标准体系，推动数据要素可信流通，解决隐私计算产品安全分级和个人信息匿名化制度，为数据要素流通行业提供技术思考和实践。 </div>
                        <hr>
                    
                    <p>如何让大规模高价值数据可信流通，成为数据要素市场发展的核心议题，亟需产学研届共同构建新的技术标准体系。7月5日，在2024世界人工智能大会上，围绕隐私计算产品通用安全分级和个人信息匿名化制度，国内多家产学研机构联合发布两份白皮书，为数据要素流通行业当下普遍遇到的挑战，提供最新的技术思考和行业实践。</p><p>&nbsp;</p><p>推动数据要素可信流通需要技术研发与标准制定通力配合。为了确保数据要素流通合规、安全和高效，产学研届正积极推进一系列的技术标准制定，聚焦解决不同隐私计算技术产品的通用安全分级，受控环境下的数据匿名化，以及数据离开运维域后的有效管控等问题。</p><p></p><h2>通用的安全分级框架，推动隐私计算规模化落地</h2><p></p><p>&nbsp;</p><p>隐私计算技术可以在保护隐私安全的前提下释放数据价值，是数据可信流通的核心技术之一，然而由于隐私计算技术路线众多，在产业落地应用中出现“讲不清”、“看不懂”、“不敢用”的情况。隐私计算产品需要安全分级方法，可以为实际产品选型提供指导，推动隐私计算技术实现大规模落地。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4e/4e32b8291e16decfe2b23f60a5d3917e.png" /></p><p></p><p>一方面，隐私计算技术路线众多，且不断有新的技术涌现，应用场景方难以评估不同技术的安全程度。另一方面，因为各参与方信任程度不同、数据类型不同，不同场景需要达到的数据可控程度也是不同的，一味追求高安全，抑或是完全忽视安全，都是不可取的。</p><p>&nbsp;</p><p>当前，虽然针对单一技术路线已经有一些安全分级标准，但是不同技术路线的分级标准完全无法对应，用户无法对所有的产品进行横向比较，这些标准也不适用于新出现的技术路线。因此，适用所有技术路线的通用安全分级思路亟需明确，来引导数据跨域流通场景中不同隐私计算产品的技术选型和安全评估工作。</p><p>&nbsp;</p><p>本次发布的《隐私计算产品通用安全分级白皮书》逐一讨论隐私计算安全分级面临的诸多难点，包括技术路线特征不同难以进行统一分级、部分重要安全能力难以被分级和量化、安全是系统性问题涉及的维度多、范围广。针对以上挑战，给出通用安全分级的设计思路，包括按照攻防效果分级来屏蔽不同技术路线差异，在“可证安全”和“不安全”之间增加一个“抵御已知攻击”的分级水位，引入软件信誉度等更多维度量化“实现安全”，明确所有技术特征与安全分级的对应关系。</p><p>&nbsp;</p><p>该白皮书由蚂蚁集团、中国通信标准化协会大数据技术标准推进委员会、深圳国家金融科技测评中心、清华大学牵头编写，另有国内16家机构参与编写。编写指导组成员包括中国科学院院士、国际密码协会会士王小云，浙江大学计算机科学与技术学院院长、区块链与数据安全全国重点实验室副主任任奎等权威学者。</p><p></p><h2>破解个人信息“匿名化”困境，从技术与法律视角探索路径</h2><p></p><p>&nbsp;</p><p>数据作为新型生产要素，将深刻影响并重构经济社会结构，而数据要素的价值发挥关键在于不同主体、不同场景下的数据流通复用。其中，个人数据是当前利用价值最高、使用场景最多样、处理措施最成熟的数据，在数据要素市场中有着不可替代的作用。</p><p>&nbsp;</p><p>如何在个人隐私保护的基础上，实现数据价值开发，是产业界面对的棘手挑战。一方面，对于描述或标识特定自然人信息的数据，如自然人的姓名、身份证号码等，数据持有者掌握这类信息后，有可能出现隐私泄露、滥用等风险。对于自然人与数据持有者交互产生的描述行为痕迹信息的数据，数据持有者汇集大量个人痕迹数据后，经数据挖掘与分析可将数据价值不断放大，但也可能出现“大数据杀熟”等风险。另一方面，部分技术处理方式导致数据精度损失很大，实际场景无法使用，使得数据失去流通和应用价值。</p><p>&nbsp;</p><p>5日下午发布的《个人信息匿名化制度白皮书：技术与法律（2024）》由对外经济贸易大学、大数据技术标准推进委员会和蚂蚁集团共同发布。这是学术与产业界首次联合从技术与法律双重维度对个人信息匿名化问题做系统性梳理与阐释、探寻可落地技术方案与数据流通解决路径。</p><p>&nbsp;</p><p>当前，为平衡“数据流通”和“个人信息保护”的双重目标，《网络安全法》《个人信息保护法》特别设置了“个人信息匿名化条款”，将匿名化后的个人数据排除在个人信息保护之外。然而，由于匿名化条款的法律内涵和实施标准有待厘清，匿名化条款往往存而不用。</p><p>&nbsp;</p><p>“个人信息匿名化条款存而不用已经成为，数据交易流通和数据要素市场建构的最大瓶颈之一，”对外经济贸易大学数字经济与法律创新研究中心主任许可在发布现场表示。许可介绍，白皮书着重考察和对比了各国与匿名化制度密切相关的个人信息定义、去标识化或假名化制度、匿名化标准和开展匿名化的具体指引。</p><p>&nbsp;</p><p>白皮书建议，在数据基础设施的规划与建设过程中，应充分考虑个人信息匿名化相关处理技术与制度规范内容。为破解“个人信息匿名化”的困境，必须从单一的法律视角转向复合的“数据基础设施”的路径。匿名化条款可以拓展为一套融合法律和技术的基础设施，从而推动在不同行业、不同机构之间实现可信、安全的数据共享、开放、交易。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/drUo4YLaqKU1OCxSS2SF</id>
            <title>国内首份！清华大学、中关村实验室等机构联合发布《大模型安全实践（2024）》白皮书</title>
            <link>https://www.infoq.cn/article/drUo4YLaqKU1OCxSS2SF</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/drUo4YLaqKU1OCxSS2SF</guid>
            <pubDate></pubDate>
            <updated>Sat, 06 Jul 2024 06:04:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型安全实践, 技术实施方案, 大模型安全应用案例, 五维一体协同共治
<br>
<br>
总结: 《大模型安全实践（2024）》白皮书首次系统化提出安全实践总体框架，以确保大模型在安全性、可靠性、可控性等维度下的技术实施方案。白皮书还介绍了金融、医疗、政务等领域的大模型安全应用案例，以及提出了“五维一体”协同共治的治理框架，为行业打造高价值参考体系。 </div>
                        <hr>
                    
                    <p>7月5日下午，清华大学、中关村实验室、蚂蚁集团等机构联合撰写的《大模型安全实践（2024）》白皮书（以下简称“白皮书”）在2024世界人工智能大会上正式发布。这也是国内首份“大模型安全实践”研究报告，为行业打造高价值参考体系。白皮书首次系统化提出安全实践总体框架，从安全性、可靠性、可控性等维度给到了技术实施方案，同时提供了金融、医疗、政务等领域的大模型安全应用案例，以及“五维一体”协同共治的治理框架。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/1f/1f6e488362529f42684c9ba245553298.png" /></p><p></p><p>&nbsp;（图：《大模型安全实践（2024）》白皮书发布现场）</p><p>&nbsp;</p><p>大模型技术正成为推动社会进步和创新的关键力量。然而随着大模型能力的不断增强，其安全性、可靠性、可控性受到前所未有的挑战，如研发过程中引发信息泄露、价值对齐、机器幻觉等问题，以及落地过程中面临的数据、模型、算法及其运行的软硬件环境安全风险。</p><p>&nbsp;</p><p>面对以上挑战，白皮书提出了大模型安全实践总体框架。该白皮书确立了“以人为本，AI向善”为大模型安全建设的核心，确保技术进步服务于人类福祉；以“安全、可靠、可控”三个核心维度的大模型安全技术体系，并涵盖了大模型安全测评与防御的综合技术方案；以及“端、边、云”为大模型安全技术的主要承载实体。</p><p>&nbsp;</p><p>作为报告核心，大模型安全技术体系里，安全性意味着确保模型在所有阶段都受到保护，涉及数据安全、模型安全、系统安全、内容安全、认知安全和伦理安全等；可靠性要求大模型在各种情境下都能持续提供准确、一致、真实的结果；可控性关乎模型在提供结果和决策时能否让人类了解和介入，可根据人类需要进行调适和操作。通过这三个维度，可提升大模型的鲁棒性、可解释性、公平性、真实性、价值对齐、隐私保护等方向的能力。</p><p>&nbsp;</p><p>白皮书指出安全评测技术和安全防御技术也是保障大模型安全的有效手段，但目前大模型的安全评测绝大多数是针对内容类场景，随着大模型技术快速发展和广泛应用，对Agent这类复杂大模型应用架构和未来通用AGI的评估是当下面临的挑战。制定标准建立面向未来的大模型可信测评体系将会变得越来越重要，这需要政府、高校等机构，联合有相关经验的企业共同合作。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ef/ef4c6508983c3df41d1ecebe941540ce.png" /></p><p>&nbsp;</p><p>（图：大模型安全实践总体框架）</p><p>&nbsp;</p><p>白皮书以蚂蚁集团自研的大模型安全一体化解决方案“蚁天鉴”为例，介绍了国内机构和企业在探索大模型安全应用的优秀实践。</p><p>&nbsp;</p><p>蚁天鉴是一款兼具大模型安全测评和防御的产品，目前已开放给20余家外部机构和企业，在金融、政务、医疗等重要领域得到采用，为行业大模型数据、训练、部署、应用等环节提供安全保障。</p><p>&nbsp;</p><p>例如，在金融场景，蚂蚁AI金融助理“支小宝”，通过“蚁天鉴”从大模型训练与推理风险管控、大模型风险点全方位评测、大模型用户交互风险管控三个方面保障大模型应用安全；针对金融业务，通过内嵌一致性检验和金融价值对齐，确保数据的准确性和金融逻辑的严格性。在医疗场景，上海市第一人民医院通过引入“蚁天鉴”平台，在其首创安全前置护栏技术保障下，可精准杜绝医院最关注的风险的出现，保障医疗大模型生成的内容更符合医疗垂类的安全和专业，有效应对大模型应用中的信息安全与隐私保护、双向内容风险防控等问题。在政务领域，“赣服通”政务AI助理在端侧实施的安全措施具有借鉴意义，其结合“蚁天鉴”通过千万政务预料训练来实现精准意图识别、智能追问反问和高频事项即问即办等功能；针对政务行业大模型应用中生成不可控、安全覆盖面广、内容对抗强、时效要求高等挑战，构建安全护栏和安全防御两大核心能力，覆盖数百项大模型内容生成风险，可应对单次50万量级的饱和攻击。</p><p>&nbsp;</p><p>清华大学长聘副教授、博士生导师李琦指出，大模型安全应用是一个新兴领域，研究和应用尚处于起步阶段。不少企业是在原有的传统数据安全、信息安全、系统安全等经验基础上，进行能力迁移，应用于大模型安全。随着新的大模型安全实践的不断深入，技术也会持续升级，为大模型安全构建实践范式，打造高价值参考体系。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/2f/2f5be00487616a82656a84c4805ffcc2.png" /></p><p></p><p>（图：蚂蚁集团安全实验室首席科学家王维强主题演讲）</p><p>&nbsp;</p><p>蚂蚁集团安全实验室首席科学家王维强在会上做了《大模型应用安全可信实践探索》的主题演讲。王维强认为，随着大模型的深度应用，在原有可信人工智能治理体系框架基础上，提升大模型的安全、可靠、可控建设，确保技术进步服务于人类福祉，是未来人工智能可持续发展的重要保障。</p><p>&nbsp;</p><p>白皮书最后还提出了构建集大模型安全政府监管、大模型安全生态培育、大模型安全企业自律、大模型安全人才培养、大模型安全测试验证“五维一体”多元参与、协同共治的治理框架。这对于大模型安全生态形成、大模型可持续发展具有非常重要和积极的意义。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/wKgHwEaN5kybOstLSFSy</id>
            <title>清华大学汪玉：大模型能效提升，有几条必经之路？</title>
            <link>https://www.infoq.cn/article/wKgHwEaN5kybOstLSFSy</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/wKgHwEaN5kybOstLSFSy</guid>
            <pubDate></pubDate>
            <updated>Sat, 06 Jul 2024 05:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 智能算法, 芯片设计, AI 2.0, 能量效率
<br>
<br>
总结: 本文讨论了人工智能发展的三个阶段，以及在AI 2.0时代面临的挑战。文章指出，大模型训练任务占据了大部分算力，而硬件能力提升速度跟不上计算需求增长的问题。同时，文章还提到了中国在芯片技术水平和算力规模方面受到的限制，以及团队在提升计算能量效率方面的研究目标。 </div>
                        <hr>
                    
                    <p>演讲嘉宾 | 汪玉 清华大学电子工程系教授、系主任 &amp; 无问芯穹发起人</p><p>审核｜傅宇琪 褚杏娟</p><p>策划 | 蔡芳芳</p><p>&nbsp;</p><p>进入生成式 AI 时代后，应用侧日益高涨的服务需求给基础设施的算力规模提出了巨大的挑战。与此同时，不断扩张的计算设施对能源供应和生态环境的压力也在飞速增长，迫使产业采取多种手段提升从芯片到集群，再到整个数据中心生态的能耗效率。SemiAnalysis 不久前发布的一篇报告指出，能耗水平将成为 AI 计算产业的核心竞争力要素，对整个产业的发展起到关键作用。</p><p></p><p></p><blockquote>在2024 年 5 月举办的 <a href="https://aicon.infoq.cn/2024/beijing">AICon 全球人工智能开发与应用大会暨大模型应用生态展</a>"上，清华大学电子工程系教授、系主任&amp;无问芯穹发起人汪玉围绕生成式 AI 时代的高能效计算发表了题为《<a href="https://aicon.infoq.cn/2024/beijing/presentation/5972">可持续的智能：大模型高能效系统前瞻</a>"》的演讲报告，本篇内容根据该报告编写、更新而来。InfoQ 将于 8 月 18-19 日举办 <a href="https://aicon.infoq.cn/2024/shanghai/track">AICon 上海站</a>"，我们已经邀请到了「蔚来创始人 李斌」，他将在主论坛分享基于蔚来汽车 10 年来创新创业过程中的思考和实践，聚焦 SmartEV 和 AI 结合的关键问题和解决之道。更多精彩议题可访问官网了解：<a href="https://aicon.infoq.cn/2024/shanghai/track">https://aicon.infoq.cn/2024/shanghai/track</a>"</blockquote><p></p><p>&nbsp;</p><p></p><h2>研究背景</h2><p></p><p>&nbsp;</p><p>在分析人工智能这个主题之前，首先要思考的一个问题是“到底何为智能”？诺贝尔奖获得者 Daniel Kahneman 在他的著作《思考，快与慢》中从人类智能的角度给出了一个视角：Daniel将人类的智能分成两类系统，第一类系统是“大脑快速、自动、直观的方法”，第二类是“思维的慢速、理性占主导的分析模式”。</p><p>&nbsp;</p><p>与人类智能的两类系统类似，人工智能的发展也经历了计算智能、感知智能与认知智能三个阶段。回顾人工智能的发展，我们可以将人工智能算法抽象为函数y=f(x)，其中 f 代表人工智能算法的计算规则，x 代表数据，y 则是决策。在计算智能时代，由人类制定f()的计算规则；在感知智能时代，人工智能算法从大量数据中进行学习，通过拟合的方式得到f()；而在认知智能时代，人工智能将从海量数据中挖掘并学习对象之间关系，获得模拟人类的认知能力的f()。</p><p>&nbsp;</p><p>而在芯片行业，从业者所解决的问题就是如何更高效地实现y=f(x)的计算。早期芯片设计人员将函数中所有最基本的元件抽象成加减乘除，设计指令集与通用处理器CPU ，然后用软件的方式实现各种各样的功能，追求的目标是让芯片可以快速支持通用计算任务。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/6c/6c38045641a8acbd252f2fdec908f6d5.png" /></p><p>&nbsp;</p><p>在AI 1.0时代，人们设计了一系列面向不同应用的智能算法，例如面向图像处理的卷积神经网络算法，面向自然语言处理的循环神经网络算法。芯片设计人员面向特定算法设计领域定制集成电路（ASIC），研究特定算法向硬件的部署优化方法。</p><p>&nbsp;</p><p>从过去的 AI 1.0 到今天的 AI 2.0 时代，最大的变化是：过去，算法研究者会面向每一个应用类别的数据来开发一个专门的算法模型，而现在，我们会使用所有的数据训练一个基础模型，再利用各行各业的专业数据，对基础模型进行微调，来完成各行各业的任务。对做系统或做硬件的人来说，只需要考虑如何优化这一个基础算法就可以了。</p><p>&nbsp;</p><p>AI 2.0 背后的挑战不言而喻。从规模角度来看，2018 年到 2022 年，模型参数量增加了至少 5 个数量级，现在还在不断增长。以 SORA 为例，其参数规模推测达到 300 亿，单次训练算力需求可达 8.4*10^23 Flops 水平。</p><p>&nbsp;</p><p>在硬件层面，行业面临的主要挑战是硬件能力的提升速度很难跟上计算需求的增长速度。以中国市场为例，如果将来 14 亿人同时在云端使用大模型，中国现有的智能算力基础设施将难以支撑14亿人的算力需求，差距可达3~4个数量级。虽然国内的基础设施建设在飞速前进，但我们确实也面临芯片短缺等挑战。</p><p>&nbsp;</p><p>从另外一个角度来看，现在大模型训练任务大致占到所有算力的 70%，大模型训练的能耗开销可达国内数百个家庭一年的用电量。但如果大模型应用开始普及，未来推理任务的算力占比大概会达到 70~80%，这才意味着大模型应用真正达到了成熟和流行的状态。</p><p>&nbsp;</p><p>2023 年美国对芯片出口提出了新的管制规定，限制了中国可以获得的芯片技术水平，也限制了中国在海外生产的算力规模。所以我们也在很努力地推进中国自主的芯片制造厂和工艺，这是整个国家和企业界在努力推进的方向，我相信在 5~10 年内我们是有希望解决这个问题的。</p><p>&nbsp;</p><p>回到我们团队的研究工作，我们的研究目标瞄准计算能量效率的提升。在我读博的2002 到 2007 年器件，我主要关注的是芯片工艺的进步，也就是 Scaling Down。比如：芯片工艺从 45nm 进化到 28-14nm 的过程中是提高能量效率的，因为晶体管越做越小，电容就会变小，每一次充电的能量就会变小，每一次的翻转的速度会变快。所以晶体管做小后，速度变快了，需要的能量变小了，所以能量效率就提升了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/55/554f0f588aa12b7c95582e612f407455.png" /></p><p></p><p>但是到了 2007 年之后，提升能量效率主要的路径就变成了加速器。因为工艺发展变缓了，大家发现多核甚至异构多核可能是一条路，芯片铺很多核就会有计算性能与能效的线性提升，所以我们会画一条线性的线来表示能效水平的提升。2010 年到 2020 年是 AI 加速器飞速发展的阶段，我们看到了 5 个数量级的能量效率提升。第三条途径则是新器件，包括量子计算、光计算、存内计算等，有希望突破现有的计算范式，以获得更高的能量效率。</p><p>&nbsp;</p><p>面向非神经网络的传统算法，一般来说会在算法设计完后再去做电路设计，但我们发现在人工智能时代，是有可能打通算法和电路的协同优化空间，而且该优化空间足够庞大。这也就意味着，任何一个算法，如果底层硬件是给定的，就可以通过微调，甚至是重新训练、重新选结构等方法针对底层硬件对算法进行优化，使算法在硬件上跑起来更快。这也是我所有的成果里最核心的一个方法论，也就是利用智能算法的可学习特性，同时优化算法和电路来实现能效的数量级提升，从不到 1GOPS/W 提升至 100TOPS/W。</p><p></p><h2>硅片上的能效系统：面向智能的软硬件协同优化</h2><p></p><p>&nbsp;</p><p>以 Llama-2-7b 大模型和 RTX4090 计算卡为例，模型直接部署在硬件平台上的能量效率是很低的，直接用 fp16 存储会遇到存储空间不足的问题，但改用 Int2 存储又会出现算法准确率很低的现象，这是当下大模型计算的一个关键矛盾。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fc/fcca589ffa3cff4d35395b66af428399.png" /></p><p></p><p>对此，要运用可学习的特性，从算法到数据结构、数据表示、计算图，再到硬件，一同做联合优化。如果底层数据变成了 4bit、2bit 的乘法器，会比 32bit、16bit 的乘法器小很多，在固定的面积里可以放的计算单元会更多，从而提高峰值能力。而对计算图和架构的设计优化将可以有效提高计算资源的利用效率。</p><p>&nbsp;</p><p>在算法模型优化方面，我们发现大语言模型的输出具有结构上的并行性，因此，我们可以先根据用户输入，由大模型生成回答内容的提纲，然后再从提纲中的要点进行并行展开，使用这样的思想就可以把一部分大模型算法加速 2~3 倍。</p><p>&nbsp;</p><p>在数据结构优化方面，大模型的注意力计算层是比较占计算量的，当 token 数变多时计算量会很大。对此可以通过稀疏方法减小计算和存储的复杂度，并针对不同的注意力头（Attention Head）使用不同的稀疏模板，从而在将计算量存储量降低50%的情况下，使端到端推理速度再提升 2~3 倍。</p><p>&nbsp;</p><p>数据表示优化层面的主要方法是量化。算法的数据特征在各层都是不一样的，大模型参数与数据的动态范围也比较大。针对数据离群点问题，我们将大部分数据表示变成 Int2，一小部分管件数据还是用 fp16，从而将平均位宽做到 2.8bit 时，且平均精度损失也只有 1% 左右，这样就可以进行实际应用，而且塞到显存较小的卡里做算子优化。</p><p>&nbsp;</p><p>对于算子优化，我们发现不同的算子特征是不一样的，比如说 softmax 的输入分布比较集中，还有 decode 阶段的矩阵计算都是一些矮胖的矩阵。所以我们通过这些特征来调整计算流程、重写算子，也可以实现很高的性能提升，同时这个思想也可以用在所有的国产卡上。</p><p>&nbsp;</p><p>我们自己也做了硬件，把算法放到硬件上。我们使用单块 FPGA 对比工艺接近的 GPU，可以实现 6 倍左右的能量效率提升。所以这也是告诉大家，其实做推理芯片是可以的，但推理芯片怎样能去适应算法的变化是比较大的挑战。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/0a/0a03edb3b14d08ea7d464d181e5c6a28.png" /></p><p></p><p>整体来看，第一部分从算法本身到模型数据结构表示，再到计算图和结构上都可以做优化。再考虑到工艺进步，将还有 5～10 倍的能量效率提升。</p><p></p><h2>产业中的能效系统：AI 2.0 时代的算力生态建设</h2><p></p><p>&nbsp;</p><p>最近清华成立了人工智能学院，专注于两部分的研究，一部分叫 AI core，核心算法和算力；另外一部分是 AI+，即AI与其他各行各业的结合。尽管我们发现算力中心的规划涨幅没有我们预想的 100 倍那么大，但在算力规模方面的发展还是很迅速的。</p><p><img src="https://static001.geekbang.org/infoq/fa/fa2e8ed9746bc1ee5d213589311d68a1.png" /></p><p>&nbsp;</p><p>在核心供给方面，芯片逻辑工艺小于 10nm 的代工厂里，92% 的份额被 TSMC 拿走。也就是说中国要扩展算力规模，不得不大量使用 TSMC 工艺，但这时就会受到美国禁令对于算力总量和算力密度的限制。</p><p>&nbsp;</p><p>整体来看，我们总的生产能力是受限的，同时我们也没有那么多的进口算力支持，怎么办？算法层面，大家有很多算法，在向万亿参数量级发展；算力层面，有很多国产芯片公司在努力，设法让我们的模型和算力能够更好地结合起来。</p><p>&nbsp;</p><p>如何把中国的算力都用起来就是一个非常值得探讨的问题。我们同海外的生态不太一样，国外主要还是英伟达以及英伟达上面的这套体系，包括CUDA和一系列优化库，但中国有各种各样的芯片，软件接口层也有很多选项。</p><p>&nbsp;</p><p>但从我的角度看，关键要做好三件事，也就是说产品维度有三个。一是大规模训练，二是适当小规模的训练和微调，三是大规模推理。底层硬件平台其实并不需要用户或算法研究者关心，有一些厂商或者软件、云服务能够把底层屏蔽掉。</p><p>&nbsp;</p><p>我们也在向这个方向努力，我们来提供中间层的训练能力、混训能力、运管能力、部署能力和对国产芯片的支持，让大家用起来更方便。</p><p>&nbsp;</p><p>英伟达的训练大家都可以干，但怎样把国产的千卡维度的训练做起来？就在前几天，我们发布了HETHUB，这是一个用于大规模模型的异构分布式混合训练系统，是业内首次实现六种不同品牌芯片间的交叉混合训练，异构混合训练集群算力利用率最高达到了97.6%。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/28/28944e8de73ef91ca3440ea377b67fb7.png" /></p><p></p><p>这给了我们极大的信心。混训的难度在于，如果都是一样的卡，把任务均匀拆分就好了，但由于算力总量不足，以及工艺被限制，我们这里不是一样的卡。因此我们必须去做各种各样的切分，然后做异构数据的并行，用异构的卡去训练大模型。目前这个异构千卡混训的能力已经结合进无问芯穹的Infini-AI云平台了，把高效互联互通、精密的分布式并行策略比如张量并行、数据并行、通信overlap等封装起来给大家提供服务。</p><p>&nbsp;</p><p>在大规模集群推理层面，特别是在缓存层面上，原来是有一部分无效缓存，但如果我们把顺序稍微调整一下，就可以做到缓存的极致利用，把模型的显存占用降到最低。这个云平台也集成了我们Serving优化技术能力，当并发量很高，多个用户同时发送请求时，这个系统会通过请求调度、提示词缓存、并行解码等方式优化计算任务的派发和计算结果的返回，累计可以实现30倍以上的token吞吐率提升。</p><p>&nbsp;</p><p>预期到今年底，我们能做到模型到芯片的M×N的自动路由，让大家想用什么卡就用什么卡。</p><p></p><h2>比特/瓦特的能效系统：算电融合，面向可持续的未来</h2><p></p><p>&nbsp;</p><p>如果大模型的算力提升真能达到 100 倍的规模，它对能源的需求会变得非常大。2025 年人工智能业务在全球数据中心用电量的占比将从 2% 猛增到 10%，相关用能成本、碳排放量也会飙升。那么电力系统如何进一步提升稳定性，如何消纳风光等绿色能源，都是我们要思考的问题。</p><p>&nbsp;</p><p>我们应该把算力中心尽可能放到能源集中的地方，但这里又面临着通信的问题，要把延迟和带宽挑战处理好，做好联合优化。我们系里也有团队在研究如何打造能源领域的大模型，支撑算力中心的用能方案的综合优化，来提高用能效率，赋能电力系统，解决电网的绿电消纳和峰值调频等问题。比如说电价是有波动的，计算任务也是有变化的，怎样把计算中心和能源模型适当结合起来做调配？还有预测发电的情况、波动的情况等等。</p><p>&nbsp;</p><p>我们还在同我校电机系一起做算电融合的研究，希望在这个方向上能够和大家一同推进。这个领域还处于早期规划阶段，这也是中国最优势的一个方向，就是怎样把能源和基建算力结合起来，赋能我们的大模型发展。</p><p>&nbsp;</p><p>从单算法到芯片的联合优化，到多算法和多芯片的联合优化，再到算力和能源的联合优化，我们有希望对整个巨大的系统进行优化，让我们的人工智能有充沛的能源和算力。</p><p></p><p>活动推荐：</p><p></p><p><a href="https://aicon.infoq.cn/2024/shanghai/speaker">AICon 全球人工智能开发与应用大会</a>"，为资深工程师、产品经理、数据分析师等专业人群搭建深度交流平台。聚焦大模型训练与推理、AI Agent、RAG 技术、多模态等前沿议题，汇聚 AI 和大模型超全落地场景与最佳实践，期望帮助与会者在大模型时代把握先机，实现技术与业务的双重飞跃。</p><p></p><p>在主题演讲环节，我们已经邀请到了「蔚来创始人 李斌」，分享基于蔚来汽车 10 年来创新创业过程中的思考和实践，聚焦 SmartEV 和 AI 结合的关键问题和解决之道。大会火热报名中，7 月 31 日前可以享受 9 折优惠，单张门票节省 480 元（原价 4800 元），详情可联系票务经理 13269078023 咨询。</p><p></p><p><img src="https://static001.geekbang.org/infoq/65/6573657a90550f91dc3658ad05122b02.webp" /></p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/M20BQoqPDarRq9LpXHGv</id>
            <title>面壁WAIC新发布：新一代高效低能耗架构面壁小钢炮、一键开发大模型APP的全栈工具集</title>
            <link>https://www.infoq.cn/article/M20BQoqPDarRq9LpXHGv</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/M20BQoqPDarRq9LpXHGv</guid>
            <pubDate></pubDate>
            <updated>Sat, 06 Jul 2024 04:22:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型时代, 面壁智能, MiniCPM-S, 稀疏激活
<br>
<br>
总结: 面壁智能创始人在WAIC 2024论坛上介绍了新一代高效、低能耗的MiniCPM-S模型，采用稀疏激活技术，提高了知识密度并降低推理算力消耗，助力开发者打造大模型SuperAPP。MiniCPM-S具备高稀疏低能耗、神仙推理和无损强大性能等特点，提升了知识密度，同时面壁智能还推出了端侧大模型工具集MobileCPM，帮助开发者一键集成大模型到APP，降低开发门槛。 </div>
                        <hr>
                    
                    <p>7月5日，面壁智能联合创始人、首席科学家刘知远在WAIC 2024 “模型即服务（Mass）加速大模型应用落地”论坛进行了《大模型时代的摩尔定律，迈入更高效的大模型时代》主题演讲，并首次对外介绍：</p><p>&nbsp;</p><p>开源新一代高效、低能耗面壁小钢炮MiniCPM-S模型助力开发者一键打造大模型SuperAPP的全栈工具集MobileCPM</p><p>&nbsp;</p><p>演讲开场，刘知远表示：“摩尔定律揭示了集成电路可容纳晶体管数目约每隔18个月便会增加一倍的规律，在过去几十年中给半导体和互联网行业的发展带来了科学指导意义；身处大模型时代，我们亟需新的“摩尔定律”。我们根据过去几年在大模型领域的深耕和实践，对大模型的发展趋势进行观察总结，提出了大模型时代的面壁定律：大模型的知识密度不断提升，平均每8个月提升一倍。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/6f/6f4cc69078af34395928ebf1e7b58554.png" /></p><p></p><p>其中，知识密度=模型能力 / 推理算力消耗。</p><p></p><p>如下图所示，相比 OpenAI 于2020年发布的1750亿参数的 GPT-3，2024 年初，面壁发布具备 GPT-3 同等性能但参数仅为24亿的 MiniCPM-2.4B ，把知识密度提高了大概 86 倍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0a/0abcdf21dc3d1ffd2a8e54059135b73a.png" /></p><p></p><p>不过这还不是面壁的极限，面壁持续优化 Scaling Law，使模型知识密度不断提升，不断训练出计算更加高效且表现更强（参数规模降低，数值位宽降低，结构更加高效）的基础大模型。面壁新一代高效稀疏架构大模型由此而生。</p><p></p><h2>MiniCPM-S：新一代高效低能耗「面壁小钢炮」</h2><p></p><p></p><p>为何人脑中的神经元数量与当代最大的基础模型可比，但能源和时间消耗却远低于大模型？这背后，稀疏激活是大脑得以实现低能耗的一大核心“技术”，通过不同任务调取不同⼤脑分区神经元，能源与时间消耗⼤⼤降低。</p><p><img src="https://static001.geekbang.org/infoq/0a/0ade2d3ca8a2763a2a3c4cae3b3b0fe6.png" /></p><p></p><p>和大脑类似，采用稀疏激活也能够在同等参数下减少大模型的推理能耗——稀疏度越高，每个词元（token）激活的神经元越少，大模型的推理成本和能耗就越少。MiniCPM-S 1.2B 采用了高度稀疏架构，通过将激活函数替换为 ReLU及通过带渐进约束的稀疏感知训练 ，巧妙地解决了此前主流大模型在稀疏激活上面临的困境。</p><p></p><p>和同规模的稠密模型 MiniCPM 1.2B 相比，MiniCPM-S 1.2 具备：</p><p>Sparse-高稀疏低能耗：在FFN层实现了高达 87.89% 平均稀疏度，推理算力下降 84%；Speed-神仙推理： 更少计算，迅疾响应。纯 CPU 环境下，结合Powerinfer推理框架，推理解码速度提升约 2.8 倍；Strong-无损强大性能：更少计算量，无损下游任务性能；</p><p></p><p><img src="https://static001.geekbang.org/infoq/89/89808d9e7b848545ae561b912f38dde0.png" /></p><p></p><p>另外，MiniCPM-S 1.2B 将知识密度空前提升：达到同规模稠密模型 MiniCPM 1.2B 的 2.57 倍，Mistral-7B 的 12.1 倍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/43/431cb772fac3a4d7d8d3d09c6110e1fa.png" /></p><p></p><p>面壁“高效 Scaling Law” 仍在持续演绎。</p><p></p><p>相关开源链接：</p><p>论文地址：https://arxiv.org/pdf/2402.13516.pdf模型地址：https://huggingface.co/openbmb/MiniCPM-S-1B-llama-formatPowerInfer可直接运行格式：https://huggingface.co/openbmb/MiniCPM-S-1B-sft-gguf</p><p></p><h2>开源大模型APP神器MobileCPM：一键集成端侧大模型到APP</h2><p></p><p></p><p>此外面壁智能最新开源了业内首个端侧大模型工具集 “MobileCPM "，帮助开发者一键集成大模型到APP。MobileCPM 开箱即用，包含了开源<a href="https://aicon.infoq.cn/2024/shanghai/track/1724">端侧大模型</a>"、SDK开发套件以及翻译、摘要等丰富的 intent ，人人都可以一站式灵活地定制出满足不同应用场景需求的大模型 APP，低门槛速成「大模型创业者」。</p><p></p><p><img src="https://static001.geekbang.org/infoq/53/53c96af252c823e73c562d92bdf1efff.png" /></p><p></p><p>MobileCPM 为开发者提供了三种模式：</p><p>基础模式：包含了丰富的适配端侧⼤模型 APP 的 SDK 套件发者基于此即可⾃由灵活地搭建⼤模型 APP，但在这个过程中，基座模型和智能体仍需要开发者⾃⾏开发和接⼊；精装模式：在基础模式基础上，提供 1.2B 参数的⾯壁新⼀代⾼效稀疏⼤模型 MiniCPM-S，并且MobileCPM 还支持任意端侧模型的集成，开发者可以根据具体需求选择替换其它端侧模型，并可以通过增加或修改prompt的方式定制多种API，满足不同业务场景需求。全包配件模式：在精装模式的基础上预装丰富的 intent，并提供保姆式教程，开发者也可使用自定义 intent，减少开发时间，⼤幅提升应⽤的丰富性。</p><p>&nbsp;</p><p>本次发布，MobileCPM 默认集成了面壁新一代高效稀疏架构模型 MiniCPM-S 1.2B ，一次性将智能密度拉满，更兼具：</p><p>毫秒极速响应：得益于面壁小钢炮MiniCPM系列背后的高效大模型训练和推理工厂，MiniCPM-S 能够在毫秒级时间内完成推理和响应，确保用户体验的流畅性。零推理成本：无需云端 GPU，MiniCPM-S 专为端侧设备而生，在保证性能强大的同时大幅降低了计算资源的消耗，使得端侧推理几乎0成本。一键集成：大模型与APP无缝衔接；预装多种 intent，提供保姆式教程；</p><p><img src="https://static001.geekbang.org/infoq/52/52f49e0b58df2ad7ef707e0003c0cbb8.png" /></p><p></p><p>基于 MobileCPM 一键开发的示例 APP（端侧模型由MiniCPM-S支持），在 iPhone 15 离线环境下毫秒级对话响应，推理速度轻松可达约30 tokens/s，相当于人类语速的 18~30 倍。</p><p></p><p>MobileCPM 拉开了<a href="https://aicon.infoq.cn/2024/shanghai/track/1724">端侧AI</a>"生态序幕，基于MobileCPM，任何开发者都可以轻松打造自己的 SuperAPP，有效解决推理成本问题。PC和智能手机时代，所有原有的应用都值得用高效端侧模型尝试一遍！</p><p></p><p>MobileCPM 现已全面支持 iOS系统，Android 版本也即将上线，敬请期待。</p><p></p><p>开源地址：</p><p>https://github.com/OpenBMB/MobileCPM</p><p>TestFlight外测链接：</p><p>https://testflight.apple.com/join/dJt5vfOZ</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ZZPFpmq9tOUdwhTy1Mql</id>
            <title>2024版国家人工智能标准化指南揭晓！涉及7个重点方向</title>
            <link>https://www.infoq.cn/article/ZZPFpmq9tOUdwhTy1Mql</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ZZPFpmq9tOUdwhTy1Mql</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jul 2024 10:34:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能, 标准体系, 新型工业化, 技术创新
<br>
<br>
总结: 人工智能作为新一轮科技革命和产业变革的基础性和战略性技术，正在成为发展新质生产力的重要引擎。近年来，我国人工智能产业链在技术创新、产品创造和行业应用等方面实现了快速发展，形成了庞大的市场规模。为进一步规范和引领该领域的发展，《国家人工智能产业综合标准化体系建设指南》发布，旨在加快构建满足人工智能产业高质量发展和“人工智能 +”高水平赋能需求的标准体系，推动人工智能赋能新型工业化。 </div>
                        <hr>
                    
                    <p>人工智能作为新一轮科技革命和产业变革的基础性和战略性技术，正在成为发展新质生产力的重要引擎。近年来，我国人工智能产业链在技术创新、产品创造和行业应用等方面实现了快速发展，形成了庞大的市场规模。特别是以大模型为代表的新技术加速迭代，呈现出创新技术群体突破、行业应用融合发展、国际合作深度协同等新特点。然而，随着人工智能技术和产业的迅猛发展，完善的标准体系显得尤为重要。</p><p></p><p>为进一步规范和引领该领域的发展，近日，国家发改委等四部门联合印发《国家人工智能产业综合标准化体系建设指南（2024 版）》（以下统称《指南》），聚焦基础共性标准、基础支撑标准、关键技术标准、智能产品与服务标准、赋能新型工业化标准、行业应用标准、安全／治理标准等 7 个重点方向，加快构建满足人工智能产业高质量发展和“人工智能 +”高水平赋能需求的标准体系，推动人工智能赋能新型工业化。</p><p></p><h2>总体目标，实现人工智能产业全球化</h2><p></p><p>《指南》明确指出，以习近平新时代中国特色社会主义思想为指导，全面贯彻党的二十大和二十届二中全会精神，统筹高质量发展和高水平安全，加快赋能新型工业化。</p><p></p><p>到 2026 年，标准与产业科技创新的联动水平将持续提升，新制定国家标准和行业标准 50 项以上，推动形成引领人工智能产业高质量发展的标准体系。预计参与标准宣贯和实施推广的企业将超过 1000 家，国际标准的制定也将超过 20 项，进一步促进人工智能产业全球化发展。</p><p></p><h2>《指南》要点解读</h2><p></p><p></p><h3>建设思路：多层次、系统化</h3><p></p><p>人工智能产业的标准化建设是一个多层次、系统化的过程，由一系列互相关联的标准构成。根据《指南》，人工智能标准体系结构包括基础共性、基础支撑、关键技术、智能产品与服务、赋能新型工业化、行业应用、安全 / 治理等七个部分。每个部分都涵盖了具体的标准制定方向和要求：</p><p></p><p>基础共性标准：规范人工智能术语、参考架构、测试评估、管理、可持续等方面的标准。基础支撑标准：包括基础数据服务、智能芯片、智能传感器、计算设备、算力中心、系统软件、开发框架、软硬件协同等标准，为人工智能产业发展夯实技术底座。关键技术标准：主要规范人工智能文本、语音、图像，以及人机混合增强智能、智能体、跨媒体智能、具身智能等的技术要求，推动人工智能技术创新和应用。智能产品与服务标准：规范智能机器人、智能运载工具、智能移动终端、数字人、智能服务等方面的标准。赋能新型工业化标准：涵盖研发设计、中试验证、生产制造、营销服务、运营管理等制造业全流程智能化标准，以及重点行业智能升级标准。行业应用标准：规范人工智能技术在各行业场景中的应用，推动产业智能化发展。安全 / 治理标准：规范人工智能安全、治理等要求，为人工智能产业发展提供安全保障。</p><p></p><p><img src="https://static001.geekbang.org/infoq/be/be452ccefc0ba2ab2c78ae5cbc5aeb18.jpeg" /></p><p></p><h3>指导原则：创新、牵引、协同、开放</h3><p></p><p>为了确保标准体系的科学性和实用性，《指南》还提出了一系列战略性指导原则，通过创新驱动、应用牵引、产业协同和开放合作，加速人工智能产业的高质量发展。</p><p>始终秉持创新驱动的理念。优化产业科技创新与标准化联动机制，加快人工智能领域关键共性技术研究，推动先进适用的科技创新成果高效转化成标准。严格遵循应用牵引的原则。以企业为主体，市场为导向，面向行业应用需求，强化创新成果迭代和应用场景构建，协同推进人工智能与重点行业的融合应用。高度注重产业协同的发展。加强人工智能全产业链标准化工作协同，推动跨行业、跨领域标准化技术组织的协作，打造大中小企业融通发展的标准化模式。着重强调开放合作的策略。深化国际标准化交流与合作，鼓励我国企事业单位积极参与国际标准化活动，与全球产业链上下游企业共同制定国际标准。</p><p></p><h3>新增重点：赋能新型工业化标准</h3><p></p><p>与今年 1 月发布的《国家人工智能产业综合标准化体系建设指南》（征求意见稿）相比，最终版的《指南》在核心内容上有了显著的拓展，特别是新增了“赋能新型工业化标准”这一关键环节。该部分主要着眼于规范人工智能技术如何为制造业全流程智能化及重点行业的智能升级提供技术支撑。具体而言，它涵盖了从研发设计、中试验证，到生产制造、营销服务以及运营管理等制造业全链条的智能化标准设定，并针对关键行业的智能升级提出了明确要求。</p><p></p><p>工业和信息化部电子第五研究所的高级工程师涂珍兰表示，“标准规范体系的建设可以促进科技创新与产业发展的结合，推动科技创新成果快速转化为产品和服务，实现产业升级和经济增长。”</p><p></p><h2>写在最后</h2><p></p><p>人工智能产业标准化体系的构建，离不开产业链上各环节携手共建。《指南》还提出，建立健全人工智能领域标准化技术组织，统筹产学研用各方、产业链各环节优势力量，协同推进人工智能标准建设，共同构建先进适用的人工智能产业标准体系。</p><p></p><p>总的来说，本次《指南》的发布，是我国人工智能产业标准化工作的一项重要举措。通过构建完善的标准体系，将有效推动人工智能技术进步，促进企业发展，引领产业升级，保障产业安全，从而更好地赋能新型工业化。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/lUMvRGuebW5x5VUM2pJS</id>
            <title>生成式推荐系统与京东联盟广告 - 综述与应用</title>
            <link>https://www.infoq.cn/article/lUMvRGuebW5x5VUM2pJS</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/lUMvRGuebW5x5VUM2pJS</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jul 2024 10:20:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大型语言模型, 自然语言处理, 推荐系统, 生成式推荐系统
<br>
<br>
总结: 本文介绍了大型语言模型对推荐系统的影响，特别是生成式推荐系统的应用。通过深入分析生成式推荐系统的优势和京东联盟广告的挑战，探讨了如何利用大型语言模型重塑推荐系统，为广告领域带来新的见解和启发。文章还详细介绍了生成式推荐系统的四个基本环节，强调了在实践中需要考虑和平衡的细节。 </div>
                        <hr>
                    
                    <p>大型语言模型（LLM）正在深刻地影响自然语言处理（NLP）领域，其强大的处理各种任务的能力也为其他领域的从业者带来了新的探索路径。推荐系统（RS）作为解决信息过载的有效手段，已经紧密融入我们的日常生活，如何用 LLM 有效重塑 RS 是一个有前景的研究问题[20, 25]。</p><p>这篇文章从生成式推荐系统和京东联盟广告的背景入手，首先引出两者结合的动因与策略，随后我们对当前的流程和方法进行了细致的回顾与整理，最后详细介绍了我们在京东联盟广告领域的应用实践。通过深入分析与案例展示，本文旨在为广告领域的推荐系统带来新的见解和启发。</p><p>﻿</p><p></p><h2>一、背景</h2><p></p><p></p><h4>生成式推荐系统</h4><p></p><p></p><p></p><blockquote>A generative recommender system directly generates recommendations or recommendation-related content without the need to calculate each candidate’s ranking score one by one[25].</blockquote><p></p><p></p><p>由于现实系统中的物料（item）数量巨大，传统 RS 通常采用多级过滤范式，包括召回、粗排、精排、重排等流程，首先使用一些简单而有效的方法（例如，基于规则/策略的过滤）来减少候选物料的数量，从数千万甚至数亿到数百个，然后对这些物料应用较复杂的推荐算法，以进一步选择较少数量的物料进行推荐。受限于响应时间的要求，复杂推荐算法并不适用于规模很大的所有物料。</p><p></p><p>LLM 的生成能力有可能重塑 RS，相较于传统 RS，生成式推荐系统具备如下的优势：1）简化推荐流程。LLM 可以直接生成要推荐的物料，而非计算候选集中每个物料的排名分数，实现从多级过滤范式（discriminative-based，判别式）到单级过滤范式（generative-based，生成式）的变迁。LLM 在每个解码步生成一个向量，表示在所有可能词元（token）上的概率分布。经过几个解码步，生成的 token 就可以构成代表目标物料的完整标识符，该过程隐式枚举所有候选物料以生成推荐目标物料[25]。2）具备更好的泛化性和稳定性。利用 LLM 中的世界知识和推理能力，在具有新用户和物料的冷启动和新领域场景下具备更好的推荐效果和迁移效果。同时，相比于传统 RS，生成式推荐系统的方法也更加具备稳定性和可复用性。特征处理的策略随场景和业务的变化将变小、训练数据量将变少，模型更新频率将变低。</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/a0/a0e903e83dd5d39a091a150978f44168.png" /></p><p>﻿﻿</p><p>•图 1. 传统推荐系统与基于 LLM 的生成式推荐系统的流程比较[25]</p><p>﻿</p><p></p><h4>京东联盟广告</h4><p></p><p></p><p>京东联盟是京东的一个联盟营销平台，以投放站外 CPS 广告为主。联盟合作伙伴通过生成的链接在其他网站或社交媒体平台上推广京东商品，引导用户点击这些链接并在京东购物，从而获得销售提成（佣金）。京东联盟借此吸引流量，扩大平台的可见度和与用户的接触范围，实现拉新促活等目标。</p><p></p><p>联盟广告推荐主要针对低活跃度用户进行多场景推荐，这样的推荐面临如下的挑战：1）数据稀疏性：低活跃度用户提供的数据较少，导致更加明显的数据稀疏性问题。数据不足使得基于 ID 的传统推荐模型难以充分地对物料和用户进行表征，进而影响推荐系统的预测准确性。2）冷启动问题：对于新用户或低活跃度用户，冷启动问题尤为严重。由于缺乏足够的历史交互数据，推荐系统难以对这些用户进行有效的个性化推荐。3）场景理解困难：在多场景推荐系统中，理解不同场景下用户的具体需求尤为关键。对于低活跃度用户，由于交互数据有限，推荐系统更难以识别出用户在不同场景下的行为差异和需求变化。4）多样性和新颖性：保持推荐内容的多样性和新颖性对于吸引低活跃度用户至关重要。然而，由于对这些用户的了解有限，推荐系统难以平衡推荐的准确性与多样性。</p><p></p><p></p><h4>京东联盟广告+生成式推荐系统</h4><p></p><p></p><p>将 LLM 融入推荐系统的关键优势在于，它们能够提取高质量的文本表示，并利用其中编码的世界知识对用户和物料进行理解和推荐。与传统的推荐系统不同，基于 LLM 的模型擅长捕获上下文信息，更有效地理解用户信息、物料描述和其他文本数据。通过理解上下文，生成式推荐系统可以提高推荐的准确性和相关性，从而提升用户满意度。同时，面对有限的历史交互数据带来的冷启动和数据稀疏问题，LLM 还可通过零/少样本推荐能力为推荐系统带来新的可能性。这些模型可以推广到未见过的新物料和新场景，因为它们通过事实信息、领域专业知识和常识推理进行了广泛的预训练，具备较好的迁移和扩展能力。</p><p></p><p>由此可见，京东联盟广告是生成式推荐系统一个天然的应用场。</p><p></p><p></p><h2>二、生成式推荐系统的四个环节</h2><p></p><p></p><p>为了实现如上的范式变迁，有四个基本环节需要考虑[26]：1）物料表示：在实践中，直接生成物料（文档或商品描述）几乎是不可能的。因此，需要用短文本序列，即物料标识符，表示物料。2）模型输入表示：通过提示词定义任务，并将用户相关信息（例如，用户画像和用户历史行为数据）转换为文本序列。3）模型训练：一旦确定了生成模型的输入（用户表示）和输出（物料标识符），就可以基于 Next Token Prediction 任务实现训练。4）模型推理：训练后，生成模型可以接收用户信息来预测对应的物料标识符，并且物料标识符可以对应于数据集中的真实物料。</p><p></p><p>虽然整个过程看起来很简单，但实现有效的生成式推荐并非易事。在上述四个环节中需要考虑和平衡许多细节。下面详细梳理了现有工作在四个环节上的应用与探索：</p><p></p><h4>物料表示</h4><p></p><p></p><p></p><blockquote>An identifier in recommender systems is a sequence of tokens that can uniquely identify an entity, such as a user or an item. An identifier can take various forms, such as an embedding, a sequence of numerical tokens, and a sequence of word tokens (including an item title, a description of the item, or even a complete news article), as long as it can uniquely identify the entity[25].</blockquote><p></p><p></p><p>推荐系统中的物料通常包含来自不同模态的各种信息，例如，视频的缩略图、音乐的音频和新闻的标题。因此，物料标识符需要在文本空间中展示每个物料的复杂特征，以便进行生成式推荐。一个好的物料标识符构建方法至少应满足两个标准：</p><p>1）保持合适的长度以减轻文本生成的难度。 2）将先验信息集成到物料索引结构中，以确保相似项目在可区分的同时共享最多的 token，不相似项目共享最少的 token。</p><p></p><p>以下是几种构建物料标识符的方法：</p><p></p><p>（1）数字 ID（Numeric ID）</p><p>由于数字在传统 RS 中被广泛地使用，一个直接的策略是在生成式推荐系统中也使用数字 ID 来表示物料。传统 RS 将每个物料 ID 视为一个独立且完整的 token，不能被进一步分割。如果将这些 token 加入到模型中，需要 1）大量的内存来存储每个 token 的向量表示，以及 2）充足的数据来训练这些向量表示。为了解决这些问题，生成式推荐系统将数字 ID 分割成多个 token 组成的序列，使得用有限的 token 来代表无限的物料成为可能。为了有效地以 token 序列表示一个物料，现有的工作探索了不同的策略。1）顺序索引：基于时间顺序，利用连续的数字表示物料，例如，“1001, 1002, ...”，这可以捕捉与同一用户交互的物料的共现（基于 SentencePiece 分词器进行分词时，“1001”和“1002”分别被分词为“100”“1”和“100”“2”）。2）协同索引：基于共现矩阵或者协同过滤信息构建物料标识符，使得共现次数更多的物料或者具有相似交互数据的物料拥有相似的标识符前缀。尽管在生成式推荐系统中使用数字 ID 效果显著，但它通常缺乏语义信息，因此会遭受冷启动问题，并且未能利用 LLM 中编码的世界知识。</p><p></p><p>（2）文本元数据（Textual Metadata）</p><p>为了解决数字 ID 中缺乏语义信息的问题，一些研究工作利用了物料的文本元数据，例如，电影标题，产品名称，书名，新闻标题等。在与 LLM 结合时可借助 LLM 中编码的世界知识更好地理解物料特性。但这种方式有两个问题：1）当物料表示文本非常长时，进行生成的计算成本会很高。此外，长文本很难在数据集中找到精确匹配；仔细检查每个长文本的存在性或相关性将使我们回到判别性推荐的范式，因为我们需要将其与数据集中的每个物料计算匹配得分。2）虽然自然语言是一种强大且富有表现力的媒介，但在许多情况下它也可能是模糊的。两个不相关的物料可能具有相同的名称，例如，“苹果”既可以是一种水果也可以特指苹果公司，而两个密切相关的物料可能具有不同的标题，例如，数据挖掘中著名的“啤酒和尿布”示例[25]。</p><p></p><p>（3）语义 ID（Semantic-based ID，SID）</p><p>为了同时获得具有语义和区分性的物料标识符，现有方法主要通过如下方式对物料向量进行离散化：1）基于 RQ-VAE 模型[8]。RQ-VAE 模型由编码器，残差量化和解码器三部分构成，其输入是从预训练的语言模型（例如，LLaMA[9]和 BERT[28]）提取的物料语义表示，输出是物料对应的 token 序列。在这个分支中，TIGER[7]是一个代表性的工作，它通过物料的文本描述生成对应的 token 序列，并将 token 序列命名为 Semantic ID。LC-Rec[4]设计了多种微调 LLM 的任务，旨在实现 Semantic ID 与用户交互数据或物料文本描述的语义对齐。这两种方法首先将物料的语义相关性捕获到标识符中，即具有相似语义的项目将拥有相似的标识符。然后，标识符表示将通过在推荐数据上训练来优化，以获取交互相关性。相比之下，LETTER[6]通过整合层次化的语义、协同信号和编码分配的多样性来构建高质量的物料标识符。2）基于语义层次化聚类方法。ColaRec[1]首先利用协同模型编码物料，并利用 k-means 聚类算法对物料进行层次化聚类，将分类类别作为物料标识符，之后在微调任务中对齐物料语义信息和交互信息。Hi-Gen[5]则在获取物料标识符的阶段同时考虑了交互信息和语义信息，利用 metric learning 对两种信息进行融合。</p><p></p><p>（4）小结</p><p>以上三类表示方法的对比如下：</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/13/13b469423138a71932589c4693203a46.png" /></p><p>﻿﻿</p><p>表 1. 不同离散化物料表示方法的对比</p><p></p><h4>模型输入表示</h4><p></p><p></p><p>在生成式推荐系统中，模型输入由如下的三个部分组成：任务描述、用户信息、上下文及外部信息。其中，用户信息主要包括用户历史交互数据和用户画像。</p><p></p><p>（1）任务描述</p><p>为了利用生成模型的理解能力，任务描述主要用来引导生成模型完成推荐任务，即将推荐任务建模为下一个物料的预测（类比语言模型的 Next Token Prediction，此处是 Next Item Prediction）。任务描述定义了提示词模版，将可利用的数据嵌入其中。例如，“这是一个用户的历史交互数据：{historical behavior}，他的偏好如下：{preference}，请提供推荐。”同时将用户历史交互数据和偏好作为模型输入内容[26]。</p><p></p><p>（2）用户历史交互数据</p><p>用户的历史交互数据在推荐系统中扮演着至关重要的角色，这种互动数据隐性地传达了用户对物料的偏好。用户历史交互数据的表示与上文介绍的物料表示密切相关，现有方法将其表示为：1）物料数字 ID 序列。物料数字 ID 被 LLM 作为纯文本处理，由分词器分割成几个 token。2）物料文本序列。将物料文本元数据进行拼接送入预训练语言模型，语言模型可根据世界知识建模物料之间的相关性。3）物料文本向量加物料 ID 向量序列。LLaRA[2]在物料标题向量后拼接了物料 ID 向量，以补充来自协同模型的交互信息。</p><p></p><p>（3）用户画像</p><p>为了增强用户建模，集成用户画像（例如，关于用户的基础信息和偏好信息）是推荐系统中建模用户特征的一种有效方式。在大多数情况下，用户的基础信息（例如，性别）可以直接从在线推荐平台获取。这些用户信息可与描述性文本结合使用，例如，“用户描述：女性，25-34 岁，在销售/市场营销领域工作”[26]。然而，由于用户隐私问题，获取用户画像可能具有挑战性，导致一些研究直接采用用户 ID 或 ID 向量[3]进行用户建模。</p><p></p><p>（4）上下文及外部信息</p><p>上下文信息（例如，位置、场景和时间）可能会影响用户决策，例如，在户外用品推荐中，用户可能更倾向于购买帐篷而水龙头。因此，在 LLM 中结合诸如时间之类的上下文信息，可以实现有效的用户理解。此外，外部知识也可以用来增强生成式推荐模型的性能，例如，用户-物料交互图中的结构化信息。</p><p>﻿</p><p></p><h4>模型训练</h4><p></p><p></p><p>在推荐数据上训练生成式推荐模型包括两个主要步骤：文本数据构建和模型优化[26]。文本数据构建将推荐数据转换为具有文本输入和输出的样本，其中输入和输出的选择取决于任务定义和物料表示方法。基于数字 ID 和文本元数据的物料表示方法可以直接构建文本数据，基于语义 ID 的方法则需要基于向量进行物料标识符的学习和获取。在模型优化方面，给定&lt;输入，输出&gt;数据，生成式模型的训练目标是最大化给定输入预测输出的条件似然。</p><p></p><p>针对生成式推荐系统，“用户到物料标识符的训练”是主要任务，即输入是用户构建，输出是下一个物料的标识符。基于数字 ID 和文本元数据的方法利用该任务进行模型训练。对于基于语义 ID 的方法，由于语义 ID 和自然语言之间存在差距，一般会利用如下辅助任务来增强物料文本和标识符之间的对齐[4]：1）“物料文本到物料标识符的训练”或“物料标识符到物料文本的训练”。对于每个训练样本，输入输出对包括同一物料的标识符和文本内容，可以互换地作为输入或输出。2）“用户到物料文本的训练”。通过将用户信息与下一个物料的文本内容配对来隐式对齐物料标识符和物料文本。</p><p></p><p>对于训练如 LLaMA 这样的大型语言模型，可采用多种策略来提高训练效率，例如，参数高效微调，模型蒸馏和推荐数据筛选。</p><p></p><h4>模型推理</h4><p></p><p></p><p>为了实现物料推荐，生成式推荐系统在推理阶段需要对生成结果进行定位，即实现生成的物料标识符与数据集中物料的有效关联。给定用户输入表示，生成式推荐系统首先通过束搜索自回归地生成物料标识符。这里的生成方式分为两种：自由生成和受限生成[26]。对于自由生成，在每一个解码步中，模型在整个词表中搜索，并选择概率最高的前 K 个 token（K 值取决于束搜索中定义的束大小）作为下一步生成的输入。然而，在整个词表上的搜索可能会导致生成不在数据集中的标识符，从而使推荐无效。</p><p></p><p>为了解决这个问题，早期工作使用精确匹配进行物料定位，即进行自由生成并简单地丢弃无效的标识符。尽管如此，它们仍然由于无效标识符而导致准确率低，特别是对于基于文本元数据的标识符。为了提高准确性，BIGRec[23]提出将生成的标识符通过生成的 token 序列的表示和物料表示之间的 L2 距离来定位到有效物料上。如此，每个生成的标识符都确保被定位到有效的物料上。与此同时，受限生成也在推理阶段被使用，例如，使用 Trie（prefix tree）或者 FM-index 进行受限生成，保证标识符的有效生成。</p><p></p><p>在预测下一个物料这样的典型推荐任务之外，也可充分利用自由生成产生新的物料描述或预测接下来 N 个物料。</p><p></p><h4>现有工作总结</h4><p></p><p></p><p>当前生成式推荐系统的代表性工作（RecSysLLM[22]，P5[20][24]，How2index[18]，PAP-REC[17]，VIP5[19]，UniMAP[27]，TIGER[7]，LC-Rec[4]，TransRec[16]，M6-Rec[21]，BIGRec[23]，LMRecSys[10]，NIR[12]，RecRanker[13]，InstructRec[11]，Rec-GPT4V[14]，DEALRec[15]）可总结为：</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/53/5331491f59babfc82dd3323fcfd9d645.png" /></p><p>﻿﻿</p><p>表 2. 生成式推荐系统的代表性工作[26]</p><p>﻿</p><p></p><h2>三、实践方案</h2><p></p><p></p><h4>总体设计</h4><p></p><p></p><p>基于对现有工作的调研和总结，我们的方案以“基于语义 ID 的物料表示”和“对齐协同信息和文本信息的训练任务”展开：</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/de/def063ae2565f101295b50a2a0b96759.png" /></p><p>﻿﻿</p><p>图 2. 总体设计框架图</p><p></p><h4>功能模块</h4><p></p><p></p><p>（1）基于语义 ID（SID）的物料表示</p><p>物料文本描述：基于商品标题表示物料。</p><p>物料向量：通过预训练的 bert-base-chinese 和 Yi-6B 分别提取文本描述对应的向量，向量维度为 768（bert-base-chinese）和 4096（Yi-6B）。</p><p>物料 SID：基于 RQ-VAE 模型对物料向量进行量化。RQ-VAE 模型由编码器，残差量化和解码器三部分构成，其输入是从预训练的语言模型中提取的向量，输出是物料对应的 SID 序列。针对冲突数据，我们采取了两种方式，一种是不进行处理，即一个 SID 对应多个商品；另一种是采用 TIGER 的方案，对有冲突的商品增加随机的一维，使得一个 SID 唯一对应一个商品。例如，商品“ThinkPad 联想 ThinkBook 14+ 2024 14.5 英寸轻薄本英特尔酷睿 ultra AI 全能本高性能独显商务办公笔记本电脑”可表示为：或。</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/be/be1853aec112f9b587b6e0cb7a6afa68.png" /></p><p>﻿﻿</p><p>图 3. RQ-VAE 模型图[8]</p><p></p><p>（2）对齐协同信息和文本信息的训练任务</p><p>Next Item Prediction：推荐系统的主任务，即针对给定的用户描述（用户画像+历史交互数据），预测下一个推荐的物料。</p><p>Additional Alignment：由于 SID 和自然语言之前存在差距，通过额外的对齐训练，建立物料 SID 和物料文本描述之间的联系，包括 SID 到文本描述和文本描述到 SID 的两个双向任务。</p><p>﻿</p><p></p><h2>四、离线与在线实验</h2><p></p><p></p><h4>训练数据</h4><p></p><p>（1）Next Item Prediction</p><p></p><p><code lang="text">{
    "instruction": "该用户为都市女性。用户已按时间顺序点击了如下商品：, , , , , , , , , , , , ，你能预测用户下一个可能点击的商品吗？",
    "response": ""
}</code></p><p></p><p>（2）Item and SID Alignment1 - SID2Title</p><p></p><p><code lang="text">{
    "instruction": "商品的标题是什么？",
    "response": "ThinkPad 联想ThinkBook 14+ 2024 14.5英寸轻薄本英特尔酷睿ultra AI全能本高性能独显商务办公笔记本电脑 Ultra5 125H 32G 1T 3K屏 高刷屏"
}</code></p><p></p><p>（3）Item and SID Alignment2 - Title2SID</p><p><code lang="text">{
    "instruction": "哪个商品的标题是\"ss109威震天变形MP威震玩具天金刚飞机威男孩机器人战机模型合金 震天战机（战损涂装版）\"？",
    "response": ""
}</code></p><p></p><p></p><h4>基座模型、训练及推理</h4><p></p><p></p><p>（1）base model: Qwen1.5-0.5B/1.8B/4B 和 Yi-6B</p><p>（2）基于 SID 增加新 tokens，并利用交互数据进行训练</p><p>（3）采用基于 beam search 的受限解码策略，beam size=20</p><p>（4）实验方式：离线实验+线上小流量实验</p><p>（5）离线评估指标：HR@1,5,10; NDCG@1,5,10</p><p>（6）在线评估指标：UCTR</p><p></p><h4>实验结果</h4><p></p><p></p><p>（1）离线实验——同一基座模型不同参数规模的对比：</p><p>◦对比 0.5B/1.8B/4B 的结果可得，模型参数量越大，处理多种任务的能力越强，评估指标值越高；</p><p>◦由于 0.5B 模型能力较弱，不适宜处理多种任务数据，单一任务训练得到的模型相较混合任务有 8 倍提升；</p><p>◦在离线训练和测试数据有 3 个月的时间差的情况下，模型的表现仍然可观。</p><p></p><p>（2）离线实验——不同基座模型的对比：</p><p>◦Yi-6B 模型在不使用受限解码的情况下就有最佳的表现；</p><p>◦微调后的 Yi-6B 指令遵循的能力较好，可进行 next item prediction 和标题文本生成。</p><p></p><p>（3）离线实验——与协同模型结果对比：</p><p>◦在相同的数据规模和数据预处理的情况下，Yi-6B 模型的效果更好；</p><p>◦对稀疏数据进行过滤后训练的协同模型效果会有显著提升，传统模型对数据和特征的处理方式更为敏感。</p><p></p><p>（4）线上小流量实验：</p><p>◦多个置信的站外投放页面的小流量实验显示，基于生成式模型 base 版本可与传统多路召回+排序的 top1 推荐对应的 UCTR 结果持平，在部分页面更优，UCTR+5%以上。</p><p>◦更适合数据稀疏、用户行为不丰富的场景。</p><p>﻿</p><p></p><h2>五、优化方向</h2><p></p><p></p><p>在生成式推荐系统中，构建高质量的数据集是实现精准推荐的关键。在物料表示和输入-输出数据构建层面，将语义信息、多模态信息与协同信息结合，基于联盟场景特点，可以显著提升物料表示的准确性和相关性。</p><p></p><p>为了支持 RQ-VAE 的稳定训练和语义 ID 的增量式推理，需要开发一种可扩展的 SID 训练和推理框架，确保语义 ID 能够快速适应物料变化。</p><p></p><p>此外，优化基座模型是提高生成式推荐系统性能的另一个关键领域。通过训练任务的组合和采用多种训练方式，例如，多 LoRA 技术和混合数据策略，可以进一步增强模型的表现。推理加速也是优化的一个重要方面，通过模型蒸馏、剪枝和量化等技术，可以提高系统的响应速度和效率。同时，基座模型的选型与变更，也是持续追求优化效果的一部分。</p><p></p><p>未来，可考虑引入搜索 query 内容进行搜推一体化建模。此外，引入如用户推荐理由生成和用户偏好生成等任务，可丰富系统的功能并提高用户的互动体验。</p><p>﻿</p><p>我们的目标是通过持续的技术革新，推动推荐系统的发展，实现更高效、更个性化的用户服务。欢迎对这一领域感兴趣的合作伙伴加入我们，共同探索生成式推荐系统技术的未来。</p><p>﻿</p><p></p><h2>六、参考文献</h2><p></p><p>1.Wang Y, Ren Z, Sun W, et al. Enhanced generative recommendation via content and collaboration integration[J]. arXiv preprint arXiv:2403.18480, 2024.</p><p>2.Liao J, Li S, Yang Z, et al. Llara: Large language-recommendation assistant[J]. Preprint, 2024.</p><p>3.Zhang Y, Feng F, Zhang J, et al. Collm: Integrating collaborative embeddings into large language models for recommendation[J]. arXiv preprint arXiv:2310.19488, 2023.</p><p>4.Zheng B, Hou Y, Lu H, et al. Adapting large language models by integrating collaborative semantics for recommendation[J]. arXiv preprint arXiv:2311.09049, 2023.</p><p>5.Wu Y, Feng Y, Wang J, et al. Hi-Gen: Generative Retrieval For Large-Scale Personalized E-commerce Search[J]. arXiv preprint arXiv:2404.15675, 2024.</p><p>6.Wang W, Bao H, Lin X, et al. Learnable Tokenizer for LLM-based Generative Recommendation[J]. arXiv preprint arXiv:2405.07314, 2024.</p><p>7.Rajput S, Mehta N, Singh A, et al. Recommender systems with generative retrieval[J]. Advances in Neural Information Processing Systems, 2024, 36.</p><p>8.Zeghidour N, Luebs A, Omran A, et al. Soundstream: An end-to-end neural audio codec[J]. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2021, 30: 495-507.</p><p>9.Touvron H, Lavril T, Izacard G, et al. Llama: Open and efficient foundation language models[J]. arXiv preprint arXiv:2302.13971, 2023.</p><p>10.Zhang Y, Ding H, Shui Z, et al. Language models as recommender systems: Evaluations and limitations[C]//I (Still) Can't Believe It's Not Better! NeurIPS 2021 Workshop. 2021.</p><p>11.Zhang J, Xie R, Hou Y, et al. Recommendation as instruction following: A large language model empowered recommendation approach[J]. arXiv preprint arXiv:2305.07001, 2023.</p><p>12.Wang L, Lim E P. Zero-shot next-item recommendation using large pretrained language models[J]. arXiv preprint arXiv:2304.03153, 2023.</p><p>13.Luo S, He B, Zhao H, et al. RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k Recommendation[J]. arXiv preprint arXiv:2312.16018, 2023.</p><p>14.Liu Y, Wang Y, Sun L, et al. Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models[J]. arXiv preprint arXiv:2402.08670, 2024.</p><p>15.Lin X, Wang W, Li Y, et al. Data-efficient Fine-tuning for LLM-based Recommendation[J]. arXiv preprint arXiv:2401.17197, 2024.</p><p>16.Lin X, Wang W, Li Y, et al. A multi-facet paradigm to bridge large language model and recommendation[J]. arXiv preprint arXiv:2310.06491, 2023.</p><p>17.Li Z, Ji J, Ge Y, et al. PAP-REC: Personalized Automatic Prompt for Recommendation Language Model[J]. arXiv preprint arXiv:2402.00284, 2024.</p><p>18.Hua W, Xu S, Ge Y, et al. How to index item ids for recommendation foundation models[C]//Proceedings of the Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region. 2023: 195-204.</p><p>19.Geng S, Tan J, Liu S, et al. VIP5: Towards Multimodal Foundation Models for Recommendation[C]//Findings of the Association for Computational Linguistics: EMNLP 2023. 2023: 9606-9620.</p><p>20.Geng S, Liu S, Fu Z, et al. Recommendation as language processing (rlp): A unified pretrain, personalized prompt &amp; predict paradigm (p5)[C]//Proceedings of the 16th ACM Conference on Recommender Systems. 2022: 299-315.</p><p>21.Cui Z, Ma J, Zhou C, et al. M6-rec: Generative pretrained language models are open-ended recommender systems[J]. arXiv preprint arXiv:2205.08084, 2022.</p><p>22.Chu Z, Hao H, Ouyang X, et al. Leveraging large language models for pre-trained recommender systems[J]. arXiv preprint arXiv:2308.10837, 2023.</p><p>23.Bao K, Zhang J, Wang W, et al. A bi-step grounding paradigm for large language models in recommendation systems[J]. arXiv preprint arXiv:2308.08434, 2023.</p><p>24.Xu S, Hua W, Zhang Y. Openp5: Benchmarking foundation models for recommendation[J]. arXiv preprint arXiv:2306.11134, 2023.</p><p>25.Li L, Zhang Y, Liu D, et al. Large language models for generative recommendation: A survey and visionary discussions[J]. arXiv preprint arXiv:2309.01157, 2023.</p><p>26.Li Y, Lin X, Wang W, et al. A Survey of Generative Search and Recommendation in the Era of Large Language Models[J]. arXiv preprint arXiv:2404.16924, 2024.</p><p>27.Wei T, Jin B, Li R, et al. Towards Universal Multi-Modal Personalization: A Language Model Empowered Generative Paradigm[C]//The Twelfth International Conference on Learning Representations. 2023.</p><p>28.Kenton J D M W C, Toutanova L K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding[C]//Proceedings of NAACL-HLT. 2019: 4171-4186.</p><p>29.Zhai J, Liao L, Liu X, et al. Actions speak louder than words: Trillion-parameter sequential transducers for generative recommendations[J]. arXiv preprint arXiv:2402.17152, 2024.</p><p></p><p>作者：广告研发部&nbsp;申磊</p><p>来源：京东零售技术 转载请注明来源</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/4Ir7pxsrorgsvbuzgXiZ</id>
            <title>AI 卷生卷死的 Q2 终于结束了，InfoQ研究中心内容洞察集锦助你 Q3 先人一步</title>
            <link>https://www.infoq.cn/article/4Ir7pxsrorgsvbuzgXiZ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/4Ir7pxsrorgsvbuzgXiZ</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jul 2024 09:34:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: InfoQ研究中心, 人工智能, 大模型, AGI
<br>
<br>
总结: InfoQ研究中心持续关注人工智能领域发展，发布多份人工智能相关研究报告，今年聚焦大模型及其产业应用，关注生成式AI开发者、技术动态趋势和5大行业实践情况，致力于助力中国大模型及AGI的商业落地。 </div>
                        <hr>
                    
                    <p>InfoQ&nbsp;研究中心自创立以来就持续关注&nbsp;AI&nbsp;领域的发展和更新，并持续推出了《中国开源生态图谱&nbsp;2023——人工智能领域》、《大语言模型综合评测报告&nbsp;2023》、《2023&nbsp;中国人工智能成熟度模型报告》、《大语言模型综合评测报告&nbsp;2024》等人工智能相关的研究报告。</p><p>今年以来，InfoQ研究中心将大模型及其产业应用，作为今年的研究主线之一。我们也认识到，技术本身无法孤立存在，本轮大模型的浪潮亦是如此，InfoQ研究中心对技术市场的用户分析和趋势分析，也离不开目前千行百业的大模型实践。因此第二季度，InfoQ研究中心继续聚焦生成式AI开发者、技术动态趋势以及5大行业实践情况，持续关注本轮大模型及AGI的浪潮。我们期望通过报告、文章、指南等多种形式，分享我们的研究成果和见解，共同助力中国大模型及AGI的商业落地。</p><p>更多精彩内容欢迎大家，点击文末海报，关注「AI前线」公众号，回复对应关键词领取，也可以扫描海报右下方二维码，直达「行业研究报告」专题。</p><p></p><h3>热门报告</h3><p></p><p></p><h4>《中国生成式AI开发者洞察2024》——聚焦技术市场人才分析</h4><p></p><p>报告回答关键问题：生成式AI的开发者面临怎样的薪资变化？什么样的岗位紧缺，开发者又需要具备什么样的技能目前的生成式&nbsp;AI&nbsp;开发者都在关注哪些领域的应用？</p><p></p><h4>《2024年第1季度中国大模型季度监测报告》——聚焦技术市场动态监测</h4><p></p><p>报告回答关键问题：Sora&nbsp;来袭，国内发展文生视频模型的土壤如何？各公司用脚投票开闭源路线的当下，开源在大模型市场进程中的价值正在被重新定义吗？人型机器人重回视野，大模型是否助力其刷新能力上限？Devin&nbsp;和智能编码助手是同一条赛道上的不同节点？多家企业宣布All&nbsp;in&nbsp;AI，对市场意味着什么？</p><p></p><h4>《中国AGI市场发展研究报告&nbsp;2024》——聚焦行业应用与实践</h4><p></p><p>报告回答关键问题：AGI&nbsp;概念引发热议，那么&nbsp;AGI&nbsp;究竟是什么？从技术架构来看，AGI&nbsp;又包括哪些？AI&nbsp;Agent&nbsp;如何助力人工智能走向&nbsp;AGI&nbsp;时代？现阶段营销、金融、教育、零售、企服等行业场景下，AGI应用程度如何？有哪些典型应用案例了吗？</p><p></p><h3>热门文章</h3><p></p><p>大模型的“瘦身”革命：巨头逐鹿轻量化大模型&nbsp;|&nbsp;大模型一周大事Stability、Mistral、Databricks、通义、A21&nbsp;Labs&nbsp;开源领域五连招，其中三个是&nbsp;MoE！|大模型一周大事国产版&nbsp;Sora&nbsp;到来！视频大模型更上一层楼&nbsp;|&nbsp;大模型一周大事文生视频模型“卷”出新天际；多家手机厂商&nbsp;AlI&nbsp;in&nbsp;Al，终端&nbsp;AI&nbsp;时代来临？|大模型一周大事让智能设备更懂你，主动式&nbsp;AI&nbsp;正在崛起&nbsp;|&nbsp;大模型一周大事</p><p></p><h3>热门图谱</h3><p></p><p></p><h4>中国大模型产品罗盘，涵盖&nbsp;200+&nbsp;主流大模型产品</h4><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/98/985fef90d93c8a33a6509ca5d08402da.png" /></p><p></p><p>2024年第三季度，InfoQ研究中心也将继续前行，陆续发布各类重磅报告，欢迎大家持续关注。</p><p>7月：《中国开发者画像洞察研究报告&nbsp;2024》8月：《中国&nbsp;AI&nbsp;Agent&nbsp;应用研究报告》8月：《AGI&nbsp;在金融领域的应用实践洞察》</p><p></p><p><img src="https://static001.geekbang.org/infoq/a3/a3bbe3a53a92e322aae7ab2025d6c21e.jpeg" /></p><p></p><p>机构介绍</p><p>InfoQ&nbsp;研究中心隶属于极客邦科技双数研究院，秉承客观、深度的内容原则，追求研究扎实、观点鲜明、生态互动的目标，聚焦创新技术与科技行业，围绕数字经济观察、数字人才发展进行研究。</p><p>InfoQ&nbsp;研究中心主要聚焦在前沿科技领域、数字化产业应用和数字人才三方面，旨在加速创新技术的孵化、落地与传播，服务相关产业与更广阔的市场、投资机构，&nbsp;C-level&nbsp;人士、架构师/高阶工程师等行业观察者，为全行业架设沟通与理解的桥梁，跨越从认知到决策的信息鸿沟。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/lwLkRJbs5YbDMTKJGjQI</id>
            <title>李彦宏WAIC圆桌访谈：开源模型是智商税，智能体正在爆发</title>
            <link>https://www.infoq.cn/article/lwLkRJbs5YbDMTKJGjQI</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/lwLkRJbs5YbDMTKJGjQI</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jul 2024 08:08:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 世界人工智能大会, 李彦宏, 大模型, 智能体
<br>
<br>
总结: 在2024世界人工智能大会期间，百度创始人李彦宏与其他嘉宾进行了一场圆桌访谈，讨论了大模型和智能体在人工智能领域的重要性。李彦宏强调了大模型在各行业应用中的潜力，以及智能体作为未来趋势的重要性。他认为，大模型的应用将带来更多的效率提升和成本节约，而智能体的低门槛将促进更多人参与创新，可能会诞生出未来的超级应用。 </div>
                        <hr>
                    
                    <p>在2024世界人工智能大会（WAIC 2024）期间，百度创始人、董事长兼首席执行官李彦宏，与第一财经传媒集团总编辑杨宇东和《硅谷101》创始人陈茜，进行了一场圆桌访谈。</p><p></p><p>在一个小时的对话中，李彦宏讲了几个独特观点：</p><p>一，现在很少有幻觉问题了。解决幻觉问题是在原来Transformer架构上，增加一些东西，“专业词语叫RAG”。</p><p>二，模型的推理成本，其实几乎是可以忽略不计。</p><p>三，开源其实是一种智商税。你永远应该选择闭源模型。</p><p></p><p>另外，李彦宏认为“没有应用的大模型一文不值”，呼吁行业不要卷模型了，要去卷应用。应用其实离大家并不遥远，基于基准模型应用在各行各业已经开始逐步渗透。他援引文心一言的调用量数据，两个月前还在2亿，现在已经到了5亿，说明大模型背后代表了真实的需求，有人真的从大模型当中获益了。</p><p></p><p></p><h3>以下是“百度”官方公众号发布的圆桌对话原文：</h3><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/71/7139e76f64f5b5cb97ea9a342f981ff7.webp" /></p><p></p><p></p><h3>超级应用什么时候出现？基础模型之上将诞生数以百万计的应用</h3><p></p><p></p><p>杨宇东：由ChatGPT掀起的这个热潮已经持续一年多了，你也曾表达，接下来超级应用什么时候出现？我们看到国内面向C端的大模型产品形态，看起来都差不多，都是搜索框+问答这种模式，你怎么看？有没有可能产生一种差异化的竞争？什么样的好产品会出现？</p><p></p><p>李彦宏：我倒不是说一定在等待一个超级应用的出现。我更觉得，在基础模型之上，应该能够诞生数以百万计各种各样的应用。这些应用有些是很小的领域，一个大家可能不太容易想到的应用，但它对于那个领域的问题，解决得比以前好很多。确切的讲，我现在还没有看到，能够比肩移动互联网时期超级应用那样的AI时代的原生应用。但是已经看到，越来越多在各种各样场景、尤其是To B场景中，利用大模型提升了效率，产生了更多的收入，或者说节省了更多成本的情况出现。</p><p></p><p>今天，大家都在想，我能不能从0到1，做出一个人们想也没想到过的东西？变成一个DAU10亿的超级应用？这个当然很重要，假以时日也一定会出现。但是，更重要的是大模型在各个领域、各个场景的应用。</p><p>从百度文心一言的日调用量来看，已经非常明显。我们在4月份曾经公布过一个数据，文心一言的调用量每天有2亿次。前几天，我们再公布的时候，文心一言调用量已经到了5亿次。也就是说，两个月的时间调用量是double。调用背后意味着什么？意味着它在给应用产生价值。因为没有价值的话，人家也不会花钱去调用。</p><p></p><p>杨宇东：C端用户会有什么样很好的场景？包括端侧、手机上的APP，如何去调用AI能力？</p><p></p><p>李彦宏：我觉得分两类：一类是大家比较关注的，过去从来没有过的应用。现在比较流行的、类似于ChatGPT这样的ChatBot，就是聊天机器人。国内每一个大模型公司，都会推出一个相应的APP，或者是网站来做ChatBot。</p><p></p><p>对于现有这些To C的应用，其实它的信息增益作用也是非常大的。我们在4月份的时候，公布过一个数据，百度搜索今天有11%的搜索结果会由AI来生成，这个比例还在不断提升。再比如说百度文库，过去，百度文库是大家在上面找一些现成的文档。今天，百度文库经过大模型改造之后，已经更多地变成了生成式AI应用。你不管想要生产什么样的文档，是PPT、是论文的格式、甚至是漫画，它都可以根据你的要求生成。今年以来，文库已经有大约2600万付费用户。如果说用超级应用的标准来看，它也没有达到超级应用的水准，但是要看它实际产生的价值，有那么多人愿意为这个产品付费，还是很厉害。这些产品都是过去已经存在，但经过了大模型改造之后，它的能力跟以前完全不一样了。</p><p></p><p>陈茜：我特别同意你最近在多个场合强调的，去卷AI原生应用，大模型才有意义。但到今天，我们还没有看到应用的爆发，可能很多应用出来也不太尽人意。所以我的问题或者疑惑在于，如果从模型能力上看，是不是现在还没有到去卷应用的时候？</p><p></p><p>李彦宏：大模型应用其实已经逐步在浮现出来，它对于现有业态的改造作用，比从0到1的颠覆作用，更早到来。这个过程一开始大家觉得，没有那么性感，但是它对于人的工作效率的提升，对于成本的下降，对于打开新的可能性，产生的促进作用，是比那些从0到1的应用，反而更大。如果仅仅是从0到1，你可能会希望出现某几个Super APP，也就是几个公司从中受益。但是今天，几乎各行各业所有的公司，被大模型加持之后，它都能受益。这种影响力，对于整个社会、对于人类来说，无疑是更大的。</p><p></p><p>只是大家觉得，以前都存在，这个东西我以前见过，所以没有新鲜感。或者它更多诞生在生产力场景，它的受众群体，或者单一应用的受众群体，不会过亿过十亿。尤其在C端，在公众层面体感没有那么集中。这是大家一直在寻找一个Super APP的原因。</p><p></p><h3>为什么智能体是未来趋势？门槛足够低，跑通了就是Super APP</h3><p></p><p></p><p>杨宇东：我们前面聊的是“卷应用”，接下来还有一个关键词叫“智能体”。你说过好多次，AI时代最看好的应用是智能体。但我们目前并没有看到智能体的爆发，为什么你认为智能体是AI时代的未来趋势呢？</p><p></p><p>李彦宏：我觉得智能体正在爆发，只是说它现在基数还比较小，大家的体感没有那么强烈。但是你要看业界大模型公司，都在做智能体。智能体就是一个几乎可以“放之四海而皆准”的基于大模型的应用。今天大多数AI原生应用，你用智能体的方式都可以做出来，效果也不错。由于它门槛足够低，可能你连编程都不用，就可以做出一个效果不错的智能体。</p><p></p><p>门槛足够低，就意味着越来越多的人，可以做出他想要的智能体。这个有点像90年代中期时候的互联网网站。你可以把它做得非常复杂，比如雅虎就是很厉害的网站。但是在学校读书的大学生，他也可以做一个自己的Home Page。由于做网站很简单，在90年代中后期，就诞生了数以百万计的网站。大浪淘沙之后，最终出来了一些非常优秀的网站，像Google、Facebook，这是若干年之后才出现。但是早期看，这些网站都是乱糟糟的，一个大学生就能做一个网站出来，这有啥价值？但是你必须得门槛足够低的时候，让更多人进来，他们发挥聪明才智，指不定哪条路跑通了，它就是一个Super APP。</p><p></p><p>陈茜：业界对AI Agent的定义，还是有一点不同。你对Agent的定义是什么？</p><p></p><p>李彦宏：我首先要考虑，这个门槛要足够低，一个小白，大一的学生，他也可以很方便地制作一个智能体。当然在此之上，可以有各种各样比较fancy的玩法，调用工具、反思、长期的记忆等等，这些能力会逐步加进去。</p><p></p><p>不是说用了最先进的这些能力之后，它才叫一个AI Agent。我反而觉得，我们要把门槛降得足够低，让大家觉得，我也可以搞一个AI Agent。</p><p></p><p>说实话，我认为现在AI Agent用到的这些能力，仍然是非常初级的，未来还会产生我们今天想也想不到的Agent能力。但是这些能力的诞生，反而要依赖数以百万计的开发者，去开发各种各样的应用。在他们使用的过程当中产生新的需求，这些需求被解决的过程，就是一个创新过程，就是AI Agent进化的过程。</p><p></p><p>陈茜：百度有什么比较有意思的AI Agent案例，可以给我们分享一下吗？</p><p></p><p>李彦宏：有很多。国内高考是一个非常大的事件，不仅是学生，还有家长都非常重视。过去大模型在干什么事？高考有作文题，我们用大模型来写一个作文，看它能得多少分。其实你想一想，这个东西在哪用呢？不可能让一个考生带着大模型去参加高考。但是高考完了之后，你要估分，要报志愿，要选择学校，你要选择专业，一个考生他该报哪个学校，哪个专业，每个人情况都是不一样，每个人的问题也都是不一样。这种情况下，哪有一个全能的辅导老师可以告诉你，你最适合的是哪个学校哪个专业？但是AI Agent就可以干这个事情。我们开发了一个高考填报志愿的AI Agent。在高峰时期，一天有200万人在使用，足见大家对这个东西的认可度和依赖度还是非常高的。</p><p></p><p></p><h3>大模型对B端的改造比互联网更大，规模更小一点的模型市场需求量更大</h3><p></p><p></p><p>杨宇东：通用大模型和行业垂直大模型，它将来到底是什么样的关系？</p><p></p><p>李彦宏：大模型在各个垂直场景里怎么用？我们经过了一个探索过程。最初我们的想法是，我把这个基础模型做得越来越强大，大家叫通用人工智能，在什么场景我都能做得很好。后来发现这件事情没有那么容易，每个场景都有它自己的道。当应用场景需要反应快的时候，我们需要更小的模型。这种小的模型，它由于没有大模型通用的能力，所以在垂直场景当中，还要对它做精调，就是SFT，把你行业的数据怎么灌进去，再经过各种各样的调教，在这个场景里的效果，就能够跟大模型相比差不多。</p><p></p><p>类似这种场景，我们也见了很多。去年10月份，我们发了文心4.0之后，主要精力在做什么呢？就是根据最强大的模型，去裁剪各种体量的小模型，比如说十亿量级的模型，百亿量级的模型，千亿量级的模型，这个模型也许擅长角色扮演，那个模型也许擅长逻辑推理等等，根据客户不同使用场景的不同要求，做出各种各样的模型。这些模型大多数速度都比EB4要快，推理成本比它要低，所以这种应用是大家更爱用的。今天你要看市场需求的话，规模更小一点的模型，市场需求量是更大的。</p><p></p><p>杨宇东：你为什么认为，大模型对B端的改造，比互联网对B端的影响更大？</p><p></p><p>李彦宏：互联网对C端的改造，我们都是感同身受的，是非常彻底的，是颠覆性的。但是互联网对B端的改造，我觉得一般般。用的技术比较简单，产生的增益也没有那么明显。但大模型不一样。我们接触到的一些能源电力、生产制造等企业，都有类似的需求。比如说，现在国内电动车卷得也很厉害，车内的对话系统，很多也在用文心大模型，使用量也不小，但是对百度来说，这就是一个To B的应用，我们不直接提供给用户，它是经过了OEM，经过了车厂的集成之后，把这个应用提供给了终端消费者。这种事情其实非常多，而且我们就看调用量，如果调用量上得很快，这就说明我们的客户需要这些东西，B端靠着这个大模型，靠着AI原生应用产生了价值。</p><p></p><p>杨宇东：在金融、医疗等这些比较严谨的领域，生成式AI的幻觉问题，怎么破解？</p><p></p><p>李彦宏：今天，应该说你会很少发现幻觉问题了，尤其是用最大规模、最强大模型的时候，已经很少出现幻觉问题了。为什么呢？一开始，纯粹用原来的Transformer去做出来的大模型，它确实是非常难避免幻觉的，因为它是个概率模型。</p><p></p><p>要解这个问题，就要在原来Transformer架构上，增加一些东西，专业词语叫RAG。我只要稍微借助一点工具，就可以消除这样的幻觉。随着使用这种工具的能力越来越强，你就会发现，在各种场景下，幻觉是越来越少的。</p><p></p><p>当然，今天这种生成式人工智能，更像是一个Copilot，在特别严肃、对准确度要求特别高的场景下，我们还不能让它全部自动实现，还要靠人把最后一道关。这样，一方面可以提升效率；另一方面，在准确度上、在消除幻觉上，也能够起到比较重要的作用。</p><p></p><p>陈茜：现在企业对AI的使用成本怎么看？是否愿意为AI付费？你在跟一些企业客户交流的时候，他们的态度是什么样子的？</p><p></p><p>李彦宏：当你处在市场经济环境当中，企业其实是非常理性的。尤其是中小企业，账算得非常精。如果这件事情能够让我降本增效，能够让我赚到更多的钱，那我就会用它。如果不能，你再吹破天，我也不会用。市场会告诉你，大模型到底有用还是没用？我们看到调用量的迅速提升，确实是因为在用户侧、在客户侧，它为企业产生了降本增效的作用。</p><p></p><p>我再举个例子，比如说在招聘场景。过去是怎么做的？是HR坐在那，一份一份简历筛查，然后一个一个面试，面试100个人，最后筛出来10个人，再进行下一步面试，效率是非常非常低。但是大模型进来之后，它可以非常明显地去提升效率。因为，用大模型去理解这是一个什么人，理解这个老板要招什么样的人，然后进行匹配，它的效率就会高很多。</p><p></p><p>而且，你去算一算模型的推理成本，其实几乎是可以忽略不计的。尤其在国内，现在大模型价格战是非常厉害的，百度的轻量级模型都是免费的，这个免费不仅仅指的是模型免费，实际上算力也送你了，你本来要有电脑，要有带宽等等，这些都没有了，你只要来用就好。</p><p></p><p></p><h3>如何看“开源闭源之争”？开源是一种智商税，闭源模型比开源模型更强大</h3><p></p><p></p><p>杨宇东：开源闭源问题是业界关注焦点。你认为，闭源模型会持续领先。但我们看到，开源大模型越来越多，甚至有些能力都不亚于我们说谓的GPT4了，这个问题你怎么看，你们还是会坚定的走闭源路线？</p><p></p><p>李彦宏：我觉得，开源其实是一种智商税。你仔细想一想，我们为什么要做大模型？是它能够有应用，这些应用在各种场景下，能够为客户为用户提升效率、降低成本，产生过去产生不了的作用。所以当你理性的去想，大模型能够给我带来什么价值？以什么样的成本给我带来价值？你永远应该选择闭源模型。今天这些闭源模型，无论是ChatGPT还是文心一言，以及其他各种各样的闭源模型，它的平均水平，一定是比这些开源模型更强大，推理成本一定是比开源模型更低。</p><p></p><p>陈茜：百度对To B客户，是“闭源+公有云”这样一套打法，有什么考量吗？</p><p></p><p>李彦宏：ToB的客户，他要选择的是一个性价比最好的模型。一方面，模型要对他的应用产生价值，另外一方面，成本要足够低。很多时候，你看着有用，一算成本不划算，客户就放弃了。这是为什么我刚才讲，开源模型打不过闭源模型。你只要理性的去看待，你的收益是啥，你的成本是啥，你就会发现，最好还是去选择闭源模型。当然，闭源模型不是一个模型，它是一系列的模型，根据你的使用场景去平衡一下，要多好的效果，要多快的推理速度，要多低的成本。模型有非常多的变种，可以根据用户的需求，让他来做选择。</p><p></p><p>闭源模型还有一个开源模型不具备的优势：这些相对来说规模更小的模型，都是从最大最powerful的模型裁剪出来的，裁剪出来这些更小规模的模型，仍然比那些同样规模的开源模型要效果更好。</p><p></p><p>陈茜：百度对于中小模型、模型蒸馏上，有什么样的策划？</p><p></p><p>李彦宏：我们看到的真实需求，在绝大多数情况下都不是最大的模型，它都要求这个模型变小。变小意味着什么？速度快，成本低。比如说，我干这个事儿，总共能够给我带来每年100万的增益，但使用最大的模型要120万的成本，那我肯定不干了。那我就会给大模型公司提要求，把成本降到80万，甚至降到8万。那我们就得想，怎么把最强大的模型，蒸馏到足够小，成本足够低，满足这个场景需求。因为闭源有一个最强大的基础模型，根据模型蒸馏或者裁剪出来的小模型，比那些开源模型做出来的东西更有竞争力。所以我们觉得，To B的机会仍然在闭源不在开源。</p><p></p><p></p><h3>大模型价格战不可避免，最终还是比谁的技术好、效率高</h3><p></p><p></p><p>杨宇东：我们现在看到价格战已经开始打起来，其实还是蛮出乎我们的预料，这么快。</p><p></p><p>李彦宏：价格战几乎不可避免，在中国互联网干了这么长时间，其实已经对价格战非常熟悉。但就像你讲的，确实来得比我想象的更早一点，这么早就开始把价格打到几乎不可思议低的地步。但某种意义上讲也不是坏事儿，当你足够低，甚至免费的时候，就会有更多人有积极性来尝试，在大模型基础上去开发各种各样的应用，大模型对于各行各业的渗透速度会更快。</p><p></p><p>杨宇东：很多闭源大模型API调用费越来越低，大模型靠推理收费的商业模式未来成不成立？以后大模型比拼的是哪些点？</p><p></p><p>李彦宏：大模型技术天花板还是很高的，今天我们还是对于大模型的能力有很多不满意的地方，仍然需要很多非常优秀的技术人员、需要很多算力、需要很多数据，才能训练出下一代大模型，我们还可能需要下下一代、下下下一代的大模型。</p><p></p><p>所以最终我觉得大家是要去拼谁的技术更好，你的技术好，你为客户产生了更好的价值。今天之所以把这个模型打到足够低，是因为现在模型的这个能力其实还没有到最好，没到最好的时候，大家都差不多的时候，就会谁的价格低就用谁的。</p><p></p><p>时间长了之后，市场本身会回归理性。最终还是比谁的技术好，谁的效率高，谁会胜出。</p><p></p><p>陈茜：你觉得这个价格战会持续多久的一个时间呢？</p><p></p><p>李彦宏：这个很难讲，现在有些创业公司是玩家，也有很多非常大型的互联网平台公司是玩家，其实理论上讲是可以烧很长时间。但我觉得烧钱不是事情本质，事情本质仍然是谁的技术更好，谁的效率更高，当你的技术好、效率高的时候，你就不怕去打这个价格战，所以多长时间都OK，最终会是优胜劣汰的过程。</p><p></p><p>陈茜：你觉得在中国市场会是一个赢家通吃这样的一个局面吗？还是说等价格战之后会剩下几个主要的？可能还有一些更小一点的？</p><p></p><p>李彦宏：这次生成式AI是对整个IT技术栈的大变革，过去IT技术栈是芯片层、操作系统层、应用层或者软件层，就这三层。到生成式AI之后，IT技术栈变成了四层，芯片、深度学习框架层、模型层、应用层，我认为在每一层可能都会诞生至少2—3个大玩家。</p><p></p><p>应用层的话，可能会有数以百万计、甚至数以千万计的各种各样应用出来，也会逐步出现超级应用，既然是超级应用，当然不会很多，可能是三五个。</p><p></p><p>模型层我觉得也许两三个就足够了，因为最后大家比拼的是效率，你的效率如果不够高的话，慢慢就觉得说还不如用别的。</p><p></p><p></p><h3>Scaling Law短期内不会被颠覆，图灵测试不再是标准，AGI需要十年以上才能实现</h3><p></p><p></p><p>杨宇东：Scaling Law还会持续有效吗？</p><p></p><p>李彦宏：Scaling Law可能还会有若干年的生命周期。但与此同时，在此之上会叠加各种各样的创新。刚才讲的智能体，它的反思、进化能力等，其实跟Scaling Law已经是两个路线在发展，但它仍然是基于Transformer这类大模型往上做。未来再过一两年，还会出现什么新的技术创新，在此基础上再去叠加，大家都在探索。换句话说，我觉得Scaling Law短期之内不会被颠覆，但是在Scaling Law之上会叠加出来很多我们现在可能还无法想象的创新。</p><p></p><p>杨宇东：你认为AGI实现的标准是什么？还有哪些路径可以让我们更快地通向AGI？</p><p></p><p>李彦宏：业界确实还没有一个标准答案。以前大家觉得，通过图灵测试就实现AGI了，实际上现在大模型已经通过了图灵测试。人们所说的AGI，其实大多数时候已经不是只通过图灵测试了。</p><p></p><p>那么什么叫AGI？在我心目中，AGI就是机器或者说AI，能够具备人在任何场景下所具备的能力。Artificial General Intelligence，就是通用人工智能，它不管在什么场景下，能力都是跟人一样的，这是一个很高的要求。</p><p></p><p>所以真正要实现AGI，我认为还需要很多很多年。业界有人说AGI可能再过2年，或者再过5年能实现。我自己的判断是10年以上，也许更长的时间。我们听到很多人讲，AGI是一种信仰，当你把它当做一种信仰的时候，谁的信仰会明年就实现？这是自相矛盾的。如果是一个信仰，它就是你值得为之长期奋斗的一个目标。</p><p></p><p>陈茜：现在GPT5一直在延后，担忧的声音也越来越高，AGI没有办法用Scaling Law这个方式去带我们实现了，你对这个有担忧吗？</p><p></p><p>李彦宏：我不是很担心这件事情，我觉得大家应该更关注应用，而不是关注基础模型，某种意义上基础模型迭代速度稍微放缓一点不是坏事，如果今天的应用开发者，有一个相对稳定的基础来开发应用，其实是效率更高一些的，如果模型天天在那儿练，每天都要重写一遍过去的代码，那是很累的。但是在现有基础模型上不断去做微调，去做一些渐进式的迭代和和创新，其实你看到是一直在发生的，无论是OpenAI不断在推的，还有百度我们的Turbo模型、更小量级的模型等等，都是在根据市场的需求在做迭代。</p><p></p><p>但长远来讲，我确实认为下一代大模型一定会比现在这一代模型强大得多。什么时候推出来我不是很着急，我们应该更多的去看真实的市场需求，下一代模型在迭代的时候，要根据市场需求来迭代。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/uQub8q2LPGtzPP4S0mbq</id>
            <title>成立半年多就敢踢馆 OpenAI ，首个开源模型不输 GPT-4o，LeCun 、PyTorch 之父齐声叫好！</title>
            <link>https://www.infoq.cn/article/uQub8q2LPGtzPP4S0mbq</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/uQub8q2LPGtzPP4S0mbq</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jul 2024 07:43:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI模型, Moshi, Kyutai, 多模态模型
<br>
<br>
总结: Kyutai团队开发了一种名为"Moshi"的AI模型，具有多种情绪表达和语音模仿能力，同时处理两个音频流。这个模型被称为世界上首个具有自然对话能力的AI助手，具有改变人机通信的潜力。Moshi还能处理文本和音频，支持同时听和说，具有文本思想和情商，能够在半秒内回复。 </div>
                        <hr>
                    
                    <p>整理&nbsp;|&nbsp;华卫</p><p>&nbsp;</p><p>想象一下，一个&nbsp;AI&nbsp;模型可以表达&nbsp;70&nbsp;多种情绪，以不同的风格说话，甚至令人信服地模仿口音。并且，它能够同时处理两个音频流，同时听和说。这不是科幻小说，而是Kyutai在语音AI技术上的最新突破。</p><p>&nbsp;</p><p>只用短短&nbsp;6&nbsp;个月的时间，这个由&nbsp;8&nbsp;人组成的非营利性&nbsp;AI&nbsp;研究实验室从零开发出了一种名为&nbsp;"Moshi&nbsp;"的实时原生多模态基础&nbsp;AI&nbsp;模型。根据&nbsp;Kyutai&nbsp;的说法，Moshi&nbsp;是世界上首个具有自然对话能力的可公开访问&nbsp;AI&nbsp;助手。OpenAI&nbsp;之前曾展示过<a href="https://www.infoq.cn/article/42ROdXw5VHrfFMsITd07">GPT-4o&nbsp;</a>"的语音引擎和语音模式功能，但尚未发布。</p><p>&nbsp;</p><p>据称，该模型具备的功能可与&nbsp;<a href="https://www.infoq.cn/article/a0XsHUI5y7sVUzlqCXC7?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">OpenAI&nbsp;</a>"的&nbsp;GPT-4o&nbsp;和&nbsp;Google&nbsp;Astra&nbsp;相媲美，但模型要小得多。“Moshi&nbsp;在说话时思考。”Kyutai&nbsp;首席执行官帕特里克·佩雷斯&nbsp;（Patrick&nbsp;Pérez）&nbsp;表示，Moshi&nbsp;具有彻底改变人机通信的潜力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/84/84b023c0fca40c5d913743b7c2743b86.jpeg" /></p><p></p><p>7月4日，Kyutai在法国巴黎公开发布了Moshi&nbsp;的实验原型，用户可以在网上自由<a href="https://moshi.chat/?queue_id=talktomoshi">测试体验</a>"。值得一提的是，Kyutai的所有模型都是开源的。之后，该团队不仅计划发布完整模型，包括推理代码库、7B 模型、音频编解码器和优化堆栈。</p><p></p><p>图灵奖得主<a href="https://www.infoq.cn/article/Gf8Z4CVHwvqLEOXGlY9c?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Yann&nbsp;LeCun</a>"分享说：“Moshi可以听懂带有法国口音的英语。”就连&nbsp;PyTorch&nbsp;之父Soumith&nbsp;Chintala也向Kyutai表示了祝贺，并透露该团队某成员是他在Meta&nbsp;的&nbsp;AI&nbsp;研究团队&nbsp;FAIR&nbsp;的前同事。</p><p></p><p><img src="https://static001.geekbang.org/infoq/80/80aba663529836be3c65b6cf65fc02a2.png" /></p><p>Kyutai团队</p><p>&nbsp;</p><p>据悉，这家成立于&nbsp;2023&nbsp;年&nbsp;11&nbsp;月的初创团队，得到了包括法国亿万富翁&nbsp;Xavier&nbsp;Niel&nbsp;在内投资的近&nbsp;3&nbsp;亿欧元的支持，旨在为&nbsp;AI&nbsp;的开放研究做出贡献并促进生态系统发展。Kyutai&nbsp;还组建了一支由知名人工智能研究人员组成的科学顾问团队——计算机科学家、2022&nbsp;年麦克阿瑟“天才”奖获得者Yejin&nbsp;Choi，Meta&nbsp;首席&nbsp;AI&nbsp;科学家、ACM图灵奖获得者Yann&nbsp;LeCun&nbsp;和德国马克斯·普朗克智能系统研究所研究所所长Bernhard&nbsp;Schölkopf。</p><p></p><h1>对话流畅又会整活，甚至还会“抢话”</h1><p></p><p>在现场演示过程中，Kyutai&nbsp;团队与&nbsp;Moshi&nbsp;进行互动，展示了其在各种说话风格之间无缝切换，以及在角色扮演中迅速化身的创造力。</p><p>&nbsp;</p><p>当被要求用法国口音说话时，Moshi&nbsp;朗诵了一首关于巴黎的诗；在被要求变身为一个热情洋溢的海盗时，Moshi&nbsp;讲述了七大洋上的勇敢和冒险故事；Moshi&nbsp;还能用一种低语的讲述神秘故事的语气，表达《黑客帝国》的电影情节。</p><p></p><p></p><p></p><p>Moshi还能一秒化身太空助手，和对话用户一同“进入”太空之旅。并且，Moshi&nbsp;的反应似乎比人类更快，经常在问题或提示被完全提出之前就做出了回答。</p><p></p><p></p><p></p><p></p><p>在发布现场的一系列演示中，Moshi&nbsp;是在没有互联网连接的标准&nbsp;MacBook&nbsp;Pro&nbsp;上运行。Kyutai&nbsp;还计划进一步优化移动设备的&nbsp;Moshi，确保其广泛采用。这将使Moshi更加通用，从个人助理到便携式教育工具，可以在各种环境中使用。</p><p></p><h1>有思想、有情商，半秒内就能回复</h1><p></p><p>据介绍，&nbsp;Moshi不仅仅是一个语音&nbsp;AI，还是一个能够处理文本和音频的多模态模型，主要功能特点包括：</p><p>&nbsp;</p><p>同时听和说：Moshi支持多流音频，使其能够同时收听和响应，从而实现自然流畅的前后对话，其中中断和重叠的语音很常见。与依靠语音活动检测来切换轮次的传统系统不同，Moshi&nbsp;保持连续的对话流。文本思想：在用音频说话时，Moshi&nbsp;会产生文本思想。这种双重方法增强了其产生准确和符合具体情况的响应的能力。通过文本思考，Moshi&nbsp;可以更有效地组织其响应，并从更丰富的知识库中汲取灵感。富有情商：Moshi&nbsp;不仅仅是文字，而是关于理解它们背后的意图。该模型经过训练，可以识别情绪，甚至可以生成传达特定情绪的语音。实时交互：Kyutai&nbsp;声称&nbsp;Moshi&nbsp;的理论延迟仅为&nbsp;160&nbsp;毫秒，而实际上，它在&nbsp;200&nbsp;到&nbsp;240&nbsp;毫秒之间。人人可访问：不仅是开源项目，公司、研究人员都可以集成、试验，而且开发了一种可以在个人计算机上运行的较小版本，使这项技术能够被大型研究实验室以外的更广泛的用户使用。负责任的&nbsp;AI&nbsp;：Kyutai&nbsp;正在整合水印技术帮助识别&nbsp;AI&nbsp;生成的音频，以确保透明度。</p><p>&nbsp;</p><p>其中，Moshi&nbsp;最令人印象深刻的方面之一是它能够在设备上运行。此功能解决了隐私问题，并使&nbsp;AI&nbsp;在实时应用程序中更易于访问和响应。用户可以与Moshi进行交互，而不必担心数据被发送到远程服务器。</p><p></p><h1>70&nbsp;亿参数提供支持，Moshi是如何训练的?</h1><p></p><p>Moshi&nbsp;因其同时处理音频和文本的能力而脱颖而出，而这种实时交互是由&nbsp;Kyutai&nbsp;创新的联合预训练过程提供支持。</p><p>&nbsp;</p><p>据了解，Moshi&nbsp;基于&nbsp;Helium&nbsp;7B&nbsp;模型构建，集成了文本和音频训练，针对&nbsp;CUDA、Metal&nbsp;和&nbsp;CPU&nbsp;后端进行了优化，支持&nbsp;4&nbsp;位和&nbsp;8&nbsp;位量化。在训练方面，Kyutai&nbsp;使用了各种数据源，包括人体运动数据和&nbsp;YouTube&nbsp;视频。</p><p>&nbsp;</p><p>Moshi&nbsp;还集成了基于&nbsp;Kyutai&nbsp;的&nbsp;Mimi&nbsp;模型的高压缩语音编解码器，可以高效处理音频信息。</p><p>&nbsp;</p><p>训练中，Moshi涉及一些创新的开创性技术，使其对自然语言和对话流程有了深刻的理解。</p><p>&nbsp;</p><p>音频语言模型：Moshi&nbsp;的模型不是只在文本上训练，而是在语音数据上训练。语音被压缩成伪词，然后用这些伪词来训练模型以预测下一段音频。这种方法使模型能够理解口语的内容和上下文。合成对话：为了训练Moshi进行对话，Kyutai从纯文本语言模型中生成了合成对话。然后，这些对话通过内部文本转语音引擎进行合成。这种方法确保其学会了处理真实的对话动态。</p><p>&nbsp;</p><p>同时，Kyutai&nbsp;以新颖的方法正面解决了传统的语音&nbsp;AI&nbsp;系统面临的问题，如延迟和处理过程中非文本信息的丢失，创造了一种响应更灵敏、听起来更自然的&nbsp;AI。</p><p>&nbsp;</p><p>集成深度神经网络：Kyutai&nbsp;没有依赖每个任务的单独模型，而是将所有内容合并到一个深度神经网络中。这种集成减少了延迟，并保留了语音通信的丰富性，而语音通信在纯文本处理中通常会丢失。基于语音的训练：Moshi的模型从大量压缩的带注释的语音片段中学习，使其能够理解语音的复杂性，包括特定的声音特征和声学条件。</p><p>&nbsp;</p><p>此外，Kyutai&nbsp;敏锐地意识到高级语音&nbsp;AI&nbsp;可能被滥用于恶意目的，如网络钓鱼。为了降低这些风险，Kyutai&nbsp;实施了识别&nbsp;Moshi&nbsp;生成内容的策略，包括维护生成的音频签名的数据库，并使用水印技术在音频中嵌入听不见的标记。</p><p></p><h1>结语</h1><p></p><p>Moshi代表了语音AI技术的重大飞跃。更广泛地说，Moshi&nbsp;有可能彻底改变数字世界中语音的使用。例如，它的文本到语音功能在情感和多人语音互动方面非常出色。它能够传达情感、调整说话风格和进行自然对话，这将彻底改变我们与人工智能互动的方式，并开启了一个充满可能性的世界：</p><p>&nbsp;</p><p>客服支持：由&nbsp;Moshi&nbsp;提供支持的&nbsp;AI&nbsp;助手可以提供富有同理心和高效的客服支持，提高用户满意度并减少等待时间。语言学习：Moshi&nbsp;模仿母语口音和传达情感的能力可以彻底改变语言学习，使其更加身临其境和有效。医疗保健：Moshi可以作为患者的伴侣，提供支持和信息，同时根据用户的情绪状态调整其语气。娱乐：Moshi可以凭借其多样化的声音和情感将角色带入生活，丰富互动式讲故事体验。</p><p>&nbsp;</p><p>与此同时，Moshi的出现隔空对OpenAI等主要人工智能公司提出了挑战，这些公司因安全问题而推迟发布类似的语音功能产品而受到不少用户的批评。</p><p>&nbsp;</p><p>不过，也有Moshi的使用者表示，其在第一分钟左右的速度和响应速度都非常快，但对话进行的时间越长，就会变得越不连贯；并且，Moshi明显缺乏知识，在犯了错误而受到责备时，就会惊慌失措，陷入“对不起，对不起...”的循环回复。</p><p>&nbsp;</p><p>虽然&nbsp;OpenAI&nbsp;暂时还不需要担心来自&nbsp;Moshi&nbsp;的竞争，但确实表明，许多公司正在迎头赶上OpenAI。就像Sora一样，现在Luma&nbsp;Labs、Runway&nbsp;等其他公司都在推出表现不弱的竞对产品挑战其模型质量和市场地位。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://medium.com/@shrimangalevallabh789/moshi-voice-ai-the-advanced-voice-ai-that-feels-almost-human-d185d85da97d">https://medium.com/@shrimangalevallabh789/moshi-voice-ai-the-advanced-voice-ai-that-feels-almost-human-d</a>"<a href="https://medium.com/@shrimangalevallabh789/moshi-voice-ai-the-advanced-voice-ai-that-feels-almost-human-d185d85da97d">1</a>"<a href="https://medium.com/@shrimangalevallabh789/moshi-voice-ai-the-advanced-voice-ai-that-feels-almost-human-d185d85da97d">85d85da97d</a>"</p><p><a href="https://analyticsindiamag.com/french-ai-lab-kyutai-releases-openai-gpt-4o-killer-moshi/">https://analyticsindiamag.com/french-ai-lab-kyutai-releases-openai-gpt-4o-killer-moshi/</a>"</p><p><a href="https://www.tomsguide.com/ai/moshi-chats-gpt-4o-advanced-voice-competitor-tried-to-argue-with-me-openai-doesnt-need-to-worry-just-yet">https://www.tomsguide.com/ai/moshi-chats-gpt-4o-advanced-voice-competitor-tried-to-argue-with-me-openai-doesnt-need-to-worry-just-yet</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/JYWVsTTgmx3eENoGcpRr</id>
            <title>Andrej Karpathy 提出新构想：未来 2.0 计算机将完全由神经网络驱动</title>
            <link>https://www.infoq.cn/article/JYWVsTTgmx3eENoGcpRr</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/JYWVsTTgmx3eENoGcpRr</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jul 2024 02:07:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能, 神经网络, 软件2.0, Andrej Karpathy
<br>
<br>
总结: Andrej Karpathy提出了一个关于未来计算机的构想：“100％ Fully Software2.0”，计算机未来将完全由神经网络驱动，不依赖传统软件代码。这种架构下，设备的输入将直接传递给神经网络，输出则直接显示为结果，可能是音频/视频，也可能显示为交互界面。未来计算机系统将可以全面处理复杂任务，但也可能面临透明度、算力、安全性和技术依赖等挑战。Karpathy的构想与之前发布的Apple Intelligence有相似之处，展望了未来计算机与人类互动的可能性。 </div>
                        <hr>
                    
                    <p></p><p>7 月 2 日凌晨，知名人工智能专家、OpenAI 的联合创始人 Andrej Karpathy 在社交平台上发帖，提出了一个关于未来计算机的构想：“100％ Fully Software2.0”， 计算机未来将完全由神经网络驱动，不依赖传统软件代码。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/5a/5a187896975f09cdf6c4f5e9286b8ed1.png" /></p><p></p><p>这就相当于人类大脑和躯体的关系：大脑负责处理，而躯干（外设）负责执行输出。</p><p></p><p>Karpathy 表示，在这种架构下，设备的输入（如音频、视频、触摸，甚至自然语言）将直接传递给神经网络，输出则直接显示为结果，可能是音频 / 视频，也可能显示为交互界面。整个计算过程完全依赖于神经网络的处理能力。</p><p></p><p>这也就意味着，“100％ Fully Software2.0”计算机将有潜力彻底改变我们与设备进行互动的方式，将架构简化为一个强大的单一神经网络。与当前传统软件与 AI 元素结合的系统相比，未来计算机系统将可以全面处理复杂任务。</p><p></p><p>这一概念的提出引起了网友的广泛关注和讨论。有网友认为，这一构想看上去太宏观且不切实际，甚至无法看到未来。</p><p></p><p>也有网友对 Karpathy 的构想表示担忧：</p><p></p><p>透明度和可解释性：完全依赖神经网络的系统可能难以解释其决策过程，导致“黑匣子”问题，增加了监管和信任的难度。算力和能源消耗：如此大规模的神经网络计算需要极高的算力和能源，可能对资源和环境造成巨大压力。安全性和隐私：神经网络驱动的系统可能容易受到攻击，尤其是如果数据输入未经严格验证，可能导致安全和隐私问题。技术依赖：过度依赖神经网络技术可能限制计算机的灵活性和适应性，尤其在面对非结构化或突发性问题时。</p><p></p><p>Karpathy 的这一构想似乎与之前发布的 Apple Intelligence 有异曲同工之处，如支持文本、音频、视频的读写功能；高度无摩擦、快速、”始终在线“和情景化地全面集成这些功能，根据用户需要调整界面等等。</p><p></p><p>此前，Karpathy 也曾表示了自己对 Apple Intelligence 的期待：”我们正在快速走向这样一个世界：当我们打开手机，直接对着手机说你想做的事情，它就会像人一样进行思考、理解并回复你，就好像它很了解你一样。作为用户，我很期待它。“</p><p></p><p>参考链接：</p><p></p><p><a href="https://x.com/karpathy/status/1807497426816946333">https://x.com/karpathy/status/1807497426816946333</a>"</p><p></p><p><a href="https://x.com/imxiaohu/status/1807772757448618285">https://x.com/imxiaohu/status/1807772757448618285</a>"</p><p></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzA4NzgzMjA4MQ==&amp;mid=2453434106&amp;idx=4&amp;sn=4fbc96fe7c13ad9cbd6642623ea00376&amp;scene=21#wechat_redirect">https://mp.weixin.qq.com/s/HEI4DkVzdLP7_QBetfTFfg</a>"</p><p></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&amp;mid=2247588955&amp;idx=1&amp;sn=85303ba7c3b86be27b3e08e5c4d95cd8&amp;scene=21#wechat_redirect">https://mp.weixin.qq.com/s/ZOhvA66bB2r6eD2eQFEjqA</a>"</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/sqaUMyNg6B8OrCcwg4vo</id>
            <title>下一代 RAG 技术来了！微软正式开源 GraphRAG：大模型行业将迎来新的升级？</title>
            <link>https://www.infoq.cn/article/sqaUMyNg6B8OrCcwg4vo</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/sqaUMyNg6B8OrCcwg4vo</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jul 2024 01:43:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: GraphRAG, LLM, 私有数据集, 知识图谱
<br>
<br>
总结: GraphRAG 是一种基于图的检索增强生成方法，通过结合大型语言模型和图机器学习技术，极大增强了处理私有数据时的性能。它利用知识图谱、社区分层和语义总结，提供了在处理私有数据集时的高效性能。GraphRAG 在私有数据集中的应用展示了显著的改进，为企业私有数据分析带来了全新的可能性。 </div>
                        <hr>
                    
                    <p></p><p></p><p></p><blockquote>这是增强大语言模型能力的一大进步，也是一种彻底改变企业私有数据分析的技术。</blockquote><p></p><p></p><p>7 月 2 日，微软开源了 GraphRAG，一种基于图的检索增强生成 (RAG) 方法，可以对私有或以前未见过的数据集进行问答。在 GitHub 上推出后，该项目快速获得了 2700 颗 star！</p><p></p><p>开源地址：<a href="https://github.com/microsoft/graphrag">https://github.com/microsoft/graphrag</a>"</p><p></p><p>通过 LLM 构建知识图谱结合图机器学习，GraphRAG 极大增强 LLM 在处理私有数据时的性能，同时具备连点成线的跨大型数据集的复杂语义问题推理能力。普通 RAG 技术在私有数据，如企业的专有研究、商业文档表现非常差，而 GraphRAG 则基于前置的知识图谱、社区分层和语义总结以及图机器学习技术可以大幅度提供此类场景的性能。</p><p></p><p>微软在其博客上介绍说，他们在大规模播客以及新闻数据集上进行了测试，在全面性、多样性、赋权性方面，结果显示 GraphRAG 都优于朴素 RAG（70~80% 获胜率）。</p><p></p><p>与我们传统的 RAG 不同，GraphRAG 方法可以归结为：利用大型语言模型 (LLMs) 从您的来源中提取知识图谱；将此图谱聚类成不同粒度级别的相关实体社区；对于 RAG 操作，遍历所有社区以创建“社区答案”，并进行缩减以创建最终答案。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/02/02dd1bfe3ddf43b3933744bb7987c388.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/99/9989ab9874a6aa6cf13a2745c91f2819.jpeg" /></p><p></p><p>这个方法用微软高大上的说法是：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/4a/4a74a04c91cf45c2a049cd98d86973b1.png" /></p><p></p><p>微软研究院于 4 月首次宣布推出 GraphRAG ，仅看到论文就让很多人有点等不及上手一试了，如今这项成果终于开源了，开发者们对此表现得超级兴奋：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/6f/6f9767882f2dea8cf7ab0c443ae679b2.jpeg" /></p><p></p><p>太棒了，微软开源了 GraphRAG！看完演示视频后，我的脑海里充满了 GraphRAG 带来的各种可能性。我打算在 &nbsp;MacBook 上尝试使用 GraphRAG + Llama3，因为它有 96GB 的统一内存 (VRAM)。我认为这个工具绝对会带来颠覆性的改变。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f7/f7353e8e630efc8c9fdb53106bf26ff1.jpeg" /></p><p></p><p>从看了论文后，我就一直期待着能玩玩它。我曾想过根据论文自己实现它，不过我想官方的代码应该只会晚几周发布，事实证明我的耐心确实得到了回报 :)</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/e4/e4d9049474c7be6893ef6394dc9d8a23.jpeg" /></p><p></p><p>我一直在等这一天！知识图谱并不是传统语义搜索的替代品，但它们确实在执行 RAG 操作时解锁了一系列全新能力，例如既可以沿着非常长的上下文向下遍历，又可以以一种连贯、高效的方式跨越不同的上下文进行遍历。</p><p></p><p>但值得一提的是，所有性能改进技术都有一个缺陷：token 的使用和推理时间都会增加…</p><p></p><p></p><h2>解锁 LLM 在私有数据集中的探索能力</h2><p></p><p></p><p>大语言模型最大的挑战和机遇或许在于如何将其强大的能力，应用到训练数据以外的问题解决中，利用大语言模型没有见过的数据取得可对比的结果。这将为数据调查开拓新的可能性，例如根据数据集的上下文和 ground 确定其主题和语义概念。</p><p></p><p>下面我们将具体介绍下微软研究院创建的 GraphRAG，这是增强大语言模型能力的一大进步。</p><p></p><p>检索增强生成（RAG）是一种根据用户的查询语句搜索信息，并以搜索结果为 AI 参考从而生成回答。这项技术是多数基于 LLM 工具的重要组成部分，而多数的 RAG 都采用向量相似性作为搜索的技术。在文档中复杂信息的分析时，GraphRAG 利用 LLM 生成的知识图谱大幅提升了问答的性能，这一点是建立在近期关于私有数据集中执行发现时提示词增强能力的研究之上。微软将私有数据集定义为未被 LLM 训练使用，且 LLM 从未见过的数据，例如某企业的专有研究、商业文件或通讯。</p><p></p><p>基线 RAG（Baseline RAG）因此而生，但基准 RAG 在某些情况下表现非常差，例如：基线 RAG 很难连点成线。这种情况出现在问题的回答需要通过共用属性遍历不同信息片段以提供新的综合见解时。基线 RAG 在需要全面地理解大型数据集或单一大型文档的语义概念时，表现会很差。</p><p></p><p>为解决这一问题，业界正在努力开发扩展和增强 RAG 的方法（如 LlamaIndex）。微软研究院的新方法 GraphRAG 便是基于私有数据集创建知识图谱，并将图谱与机器学习一同用于在查询时执行提示词的增强。在回答上述两类问题情况时，GraphRAG 展示了显著的改进，其智能或者说精通的程度远超先前应用私有数据集的其他方法。</p><p></p><p></p><h3>应用 RAG 于私有数据集</h3><p></p><p></p><p>为证明 GraphRAG 的有效性，GraphRAG 先以新闻文章中暴力事件信息（VIINA）数据集为例，该数据集复杂且存在相左的意见和不完整的信息，是一个现实世界中杂乱的测试示例，又因其出现时间过于近期，所以并未被纳入 LLM 基础模型的训练中。</p><p></p><p>在这项研究中，微软采用了俄罗斯和乌克兰双方新闻来源在 2023 年 6 月中的上千篇新闻报道，将其翻译为英文后建成了这份将被用于基于 LLM 检索的私有数据集。由于数据集过大无法放入 LLM 上下文的窗口，因此需采用 RAG 方法。</p><p></p><p>微软团队首先向基线 RAG 系统和 GraphRAG 提出一个探索查询：</p><p></p><p>查询语句：“Novorossiya 是什么？”</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7c/7c387ec1c789f7780c007555fb1141a0.jpeg" /></p><p></p><p>通过结果可以看出，两个系统表现都很好，这是基线 RAG 表现出色的一类查询。然后他们换成了一段需要连点成线的查询：</p><p></p><p>查询语句：“Novorossiya 做了什么？”</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/32/32de8c7107040534e6818b249114d457.jpeg" /></p><p></p><p>基线 RAG 没能回答这一问题，根据图一中插入上下文窗口的源文件来看，没有任何文本片段提及“Novorossiya”，从而导致了这一失败。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/72/7219462a6412d4c1878838c00e174be9.png" /></p><p></p><p>图一：基线 RAG 检索到的上下文</p><p></p><p>相较之下，GraphRAG 方法发现了查询语句中的实体“Novorossiya”，让 LLM 能以此为基础建立图谱，连接原始支持文本从而生成包含出处的优质答案。举例来说，图二中展示了 LLM 在生成语句时所截取的内容，“Novorossiya 与摧毁自动取款机的计划有所关联。”可以从原始文本的片段（翻译为英文后）中看出，LLM 是通过图谱中两个实体之间的关系，断言 Novorossiya 将某一银行作为目标的。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/6b/6b8e8c261eb0de60428cedb351961315.png" /></p><p></p><p>图二：GraphRAG 出处</p><p></p><p>通过 LLM 生成的知识图谱，GraphRAG 大幅改善了 RAG 的“检索”能力；在上下文窗口中填充相关性更高的内容、捕捉出处论据从而提供更为优质的答案。</p><p></p><p>信任和验证 LLM 所生成的结果始终是重要的。微软希望结果总是事实性正确、连贯一致，并且能准确地反映原始材料中的内容。GraphRAG 每次生成回答时总会提供出处或源基础信息，表明它的回答时以数据集为基础的。每个论断的引用来源都一目了然，人类用户能够直接对照原始材料，快速且准确地审核 LLM 的输出结果。</p><p></p><p>不过这还不是 GraphRAG 可以实现的全部功能。</p><p></p><p></p><h3>完整数据集推理</h3><p></p><p></p><p>基线 RAG 不擅长处理需要汇总全部数据集信息才能得出答案的查询。类似“数据中排行前五的主题是什么？”的查询表现不佳，是因为基线 RAG 依赖对数据集中语义相似文本内容的矢量搜索，而查询语句中却没有任何能引导它找到正确信息的关键词。</p><p></p><p>但 GraphRAG 却可以回答这类问题。LLM 生成的知识图谱结构给出了数据集的整体结构和其中主题，让私有数据集也能被组织成有意义的语义集群并对其进行预总结。在回应用户查询时，LLM 会使用这些聚类对主题进行总结。</p><p></p><p>通过下面这条语句，可以展示出两套系统对数据集整体的推理能力：</p><p></p><p>查询语句：“数据中排行前五的主题有哪些？”</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7e/7ec28b8bc55e62cc78cbe554a5ee89f3.jpeg" /></p><p></p><p>从基线 RAG 的结果来看，列出的主题中没有一个提及两者之间的纷争。正如预期，矢量搜索检索到了无关的文本，并将其插入 LLM 的上下文窗口中。生成的结果很可能是根据关键词“主题”进行搜索，导致了其对数据集内容的评估不够有用。</p><p></p><p>再看 GraphRAG 的结果，可以清楚看到其生成的结果与数据集整体内容更为吻合。回答中提供了五大主题及其在数据集中观察刀的辅助细节。其中参考的报告是由 LLM 为 GraphRAG 根据每个语义集合预先生成，提供了对原始材料出处的对照。</p><p></p><p></p><h3>创建 LLM 生成的知识图谱</h3><p></p><p></p><p>支持 GraphRAG 的基本流程是建立在先前对图机器学习的研究和代码库上的：LLM 处理全部私有数据集，为源数据中所有实体和关系创建引用，并将其用于创建 LLM 生成的知识图谱。利用生成的图谱创建自下而上的聚类，将数据分层并组织成语义聚类（在图三中由颜色标识）。这种划分让预先总结语义概念和主题成为可能，从而更全面地理解数据集。在查询时，两种结构均被用于填充 LLM 回答问题时的上下文窗口。</p><p></p><p>图三为图谱可视化的示例，每个圆圈都代表一个实体（如人物、地点或组织），圆圈大小代表该实体拥有的关系数量，颜色代表相似实体的分组。颜色分区时建立在图结构基础上的一种从下至上的聚类方法，让 GraphRAG 能回答不同抽象程度的问题。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/52/52ab5b5d8da0892b696ca7df0ed723dd.png" /></p><p></p><p>图三：利用 GPT-4 Turbo 和私有数据集创建 LLM 生成的知识图谱</p><p></p><p></p><h3>结果指标</h3><p></p><p></p><p>上述示例中表现了 GraphRAG 在多个跨领域数据集上的持续改进。微软采用 LLM 的一个评分器给 GraphRAG 和基线 RAG 的表现进行评估和对比，设定了一系列定性指标，其中包括全面性（问题指向背景框架内的完整性）、人性化（提供辅助原始材料或其他背景信息），以及多样性（提供问题回答的不同角度或观点）。初步结果显示，GraphRAG 在这些指标上始终优于基线 RAG。</p><p></p><p>除了对比评估，他们还采用 SelfCheckGPT 对 GraphGPT 进行了忠实性的测试，以验证其基于原始材料的真实且连贯的生成结果。结果显示，GraphRAG 达到了与基线 RAG 相似的忠实度水平。</p><p></p><p>通过将 LLM 生成的知识图谱与图机器学习相结合，GraphRAG 能回答重要的问题类别，而这些问题是无法单独使用基线 RAG 完成的。在将这项技术应用于社交媒体、新闻文章、工作中生产力及化学等场景后，微软已经观察到了可喜的成果，未来他们将继续在各类新领域中应用这项技术。</p><p></p><p>参考链接：</p><p></p><p><a href="https://www.youtube.com/watch?v=r09tJfON6kE">https://www.youtube.com/watch?v=r09tJfON6kE</a>"</p><p></p><p><a href="https://news.ycombinator.com/item?id=40857174https://arxiv.org/html/2404.16130v1">https://news.ycombinator.com/item?id=40857174https://arxiv.org/html/2404.16130v1</a>"</p><p></p><p><a href="https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/">https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/</a>"</p><p></p><p><a href="https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/">https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/</a>"</p><p></p><p>活动推荐：</p><p></p><p></p><blockquote>在 8 月 18-19 日即将举办的 AICon 上海站，我们设置了<a href="https://aicon.infoq.cn/2024/shanghai/track/1705">【RAG 落地应用与探索】专题</a>"，本专题将深入探讨 RAG 的最新进展、成果和实践案例。我们将详细分析面向 RAG 的信息检索的创新方法，包括知识抽取、向量化、重排序、混合检索等在不同行业和场景下的微调和优化方法。感兴趣的同学请锁定大会官网：<a href="https://aicon.infoq.cn/2024/shanghai/track">https://aicon.infoq.cn/2024/shanghai/track</a>"大会火热报名中，7 月 31 日前可以享受 9 折优惠，单张门票节省 480 元（原价 4800 元），详情可联系票务经理 13269078023 咨询。</blockquote><p></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/65/6573657a90550f91dc3658ad05122b02.other" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/7BMREy4NUmvxEYJ5jhY2</id>
            <title>周伯文：通专融合是通往AGI的战略路径</title>
            <link>https://www.infoq.cn/article/7BMREy4NUmvxEYJ5jhY2</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/7BMREy4NUmvxEYJ5jhY2</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jul 2024 16:49:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 世界人工智能大会, 通用人工智能, ABI, AGI
<br>
<br>
总结: 2024年世界人工智能大会在上海举行，周伯文在会上分享了关于通用人工智能的主题。他提到通用人工智能是新的生产力引擎，是生产力的生产力。他还探讨了通向AGI的必经之路，即广义人工智能。他认为实现AGI的路径是二维的，需要结合泛化能力和专业性。周伯文介绍了通专融合的新范式，强调构建具有泛化性和专业能力的AI系统的重要性。 </div>
                        <hr>
                    
                    <p>7月4日，2024世界人工智能大会暨人工智能全球治理高级别会议（WAIC 2024）在上海开幕。上海人工智能实验室主任、首席科学家，清华大学惠妍讲席教授，衔远科技创始人周伯文在WAIC 2024科学前沿主论坛上发表开场报告。以下为报告全文：</p><p>&nbsp;</p><p>尊敬的各位领导、各位来宾，大家下午好。我是上海人工智能实验室周伯文，非常有幸在这个隆重的场合下代表实验室与大家进行主旨分享。我的报告主题是《通专融合：通用人工智能前沿探索与价值实现》。自21世纪初以来，我们进入了以人工智能的兴起为代表，并逐步走向通用人工智能的第四次工业革命，因此又称为智能化时代。这一时代的特点是知识发现加速，人类能力的边界得以拓展，产业的数字化和智能化持续升级，从而带来生产范式的变革。通用人工智能对于人、工具、资源、技术等生产力要素具有广泛赋能的特性，可以显著提升其他生产力，因此我们说它是新质生产力的重要引擎，是“生产力的生产力”。</p><p></p><h2>AGI路径的思考</h2><p></p><p>我本人深入思考通用人工智能始于2015、2016年。2016年AlphaGo击败了人类的世界冠军，大家开始讨论通用人工智能什么时候会到来。坦率讲当时大家对AGI是缺乏认识的，但我在思考什么样的研究可以导致AGI。我们需要回答很多问题，例如，什么时候AGI会来，AGI会怎么来，我们要如何防御，如何让AGI变得更好等。那时候大家都知道了AGI是什么，但不知道怎么做。对应AGI我创造了两个词：ANI狭义人工智能和ABI广义人工智能。右边就是我当时的PPT原版。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4a/4a6625652f0fdfa2eb73d1bc157ce2ea.png" /></p><p></p><p>通向AGI的必经之路是ABI，即广义人工智能。从学术上我给出了严格的定义：自监督、端对端、从判别式走向生成式。</p><p></p><p>回头来看，2022年ChatGPT出现的时候基本上实现了这三个要素，也就说2022年底开始我们已经进入了ABI的时代。但2016年未能预测出大模型的一些要素，例如模型的涌现能力。站在2024年的节点上，如果要做同样的思考讨论，那么接下来，AGI应该是一种怎样的达成路径，这是我们所有研究者和从业者都必须思考的问题。</p><p></p><p>这里提供一个我们的思考视角：实现AGI的路径应该是二维的，而非一维的。回看发展历史，在2016、2017年以前，人工智能在专业能力上拥有非常迅猛的进展。从“深蓝”到“AlphaGo”，人工智能因一次次击败“地表最强人类”而成为新闻的主体。但当时的巨大挑战在于，这些模型不具备泛化能力，只能在专有的任务上表现突出。在2017年Transformer提出以后，我们看到的是大模型在泛化能力上的“狂飙”。但大模型当前的另一个挑战是，在专业能力的进展上极其缓慢。同时带来的能源消耗、数据消耗、资源消耗均在让人思考，这条路径是通向AGI的有效路径吗？</p><p></p><p>Sam Altman曾提到，GPT-4的专业能力，大概相当于10%-15%的专业人士，即使到未来的GPT-5，预期将会提高四到五个点，也就是说将用指数级的能源消耗增长换来缓慢的专业能力提升。</p><p>在这里我们想提出一个判断：人工智能AGI落地会有一个高价值区域，同时要求模型兼备很强的泛化能力和足够的专业性。这个区域离原点最近的位置，我们把它叫做通专融合的“价值引爆点”。根据对历史生产力提升的分析，我们认为处在这个点的大模型，在专业能力上应超过90%的专业人类，同时具备强泛化能力，即ABI的能力。谁先进入高价值区域，即意味着谁的能力更强，拥有更多的场景和数据飞轮，并因此更早拥有自我进化迭代的能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6e/6e6ac6131fe8c228adfd794762b748db.png" /></p><p></p><p></p><h2>强泛化之上的专业能力是AI皇冠上的明珠：通专融合新范式</h2><p></p><p>强泛化之上的专业能力是AI皇冠上的明珠，通专融合的发展新范式。瞄准构建一个既具有泛化性又具备专业能力的AI系统，这样的系统能够更高效、更好地适应和解决现实世界中的复杂问题。实现这一目标需要一个完整的技术体系，它包含三层重点工作：</p><p></p><p>基础模型层：我们专注于更高效地构建通用泛化能力，尤其是其高阶能力，如数理、因果推理等。通过高质量数据的清洗和合成，研发高性能训练框架、高效的模型架构。一部分这样的原始创新体现在我们的书生·浦语大语言模型、书生·万象多模态模型等基础模型，并在数学和推理等高阶能力上实现了突破。但我们还有很多工作要做。融合协同层：这一层负责将泛化性和专业性有效地结合起来。我们采用多路线协同的算法和技术，构建比肩人类优秀水平的专业能力。我们的原创工作包括高密度监督信号的生成、复杂任务规划，以及新的架构来实现系统1（即快速、直觉反应的系统）和系统2（慢速、逻辑分析的系统）之间的交互。通过这些技术，AI能够在复杂环境中做出决策，将复杂任务分解为更易管理的子任务，制定行动计划，并有效地协调多个智能体，以实现群体智能的涌现。自主进化与交互层：在这一层，我们强调AI的自主探索和反馈闭环的重要性。AI系统需要能够在真实或仿真世界中自主地收集数据、学习并适应环境。通过与环境的交互，AI能够获得反馈，这些反馈对于其自我进化至关重要。自主进化与交互层使AI能够进行具身自主学习，最终对世界模型有更深刻的理解并与之交互，完成开放世界任务。</p><p>&nbsp;</p><p>接下来，我分别介绍在这个框架下的几项前沿进展。</p><p>&nbsp;</p><p></p><h2>更高效地构建通用基础模型</h2><p></p><p>为更高效地构建通用基础模型，实验室在并行训练及软硬适配协同、高效数据处理、新型架构及推理增强等方面进行了一系列原创的探索。</p><p></p><p>例如，在长序列并行训练方面，我们实现了性能突破，较国际知名的框架Megatron高达4倍。我们研发的大模型训练系统，基于真实训练需求不断沉淀技术能力，已连续两年获得计算机系统顶会ASPLOS杰出论文奖及最佳论文奖。</p><p></p><p>在基础模型方面，通过稀缺数据的合成与增广，实验室最新的大语言模型书生·浦语2.5，实现了综合性能比肩开源大模型参数的性能。</p><p></p><p>多模态大模型书生·万象，通过渐进式对齐、向量链接等创新技术，构建以更少算力资源训练高性能大模型的道路。以260亿参数，达到了在关键评测中比肩GPT-4的水平。</p><p></p><h2>模型通用泛化能力与专业能力融合</h2><p></p><p>围绕构造通用模型的高阶专业能力，我介绍两项代表性成果。</p><p></p><p>首先，是关于大模型专业推理能力。最近大家可能看到过这个新闻：“AI参加高考，数学全不及格”。这些AI考生里面，也包含了我们的书生·浦语，它在其中拿到了数学的最高分75分。这要得益于我们的开源数学模型，它沉淀了密集过程监督、模型辅助的思维链校验、多轮强化自训练、文本推理和代码解释器联合迭代等一系列技术，具备了良好的自然语言推理、代码解题及形式化数学语言性能，所以能以200亿参数在高考数学上超过GPT-4o，我们不但效果最好，而且参数体量最小、能源消耗最低。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2e/2e020a57bf3c0a56a44b076899896f03.png" /></p><p></p><p>第二项是关于新的系统架构，我们原创提出模拟人脑的系统1与系统2架构来实现通专融合。大家知道系统1是人脑的快决策，反映的是长期训练下的专业能力；系统2是慢系统，体现的是深度思考下的泛化能力。我们今年的这篇CVPR论文通过设计系统1与系统2的协同模式，提出了交互式持续学习新概念，让通用模型与专业模型能互相学习，通过通专融合来更高效、更专业地解决问题。同一个架构在图像识别、专业文本生成方面都获得了很好的效果。</p><p></p><h2>具身自主探索与世界模型</h2><p></p><p>具身自主探索是实现通专融合的有效手段，也是理解物理世界的AGI的必经之路。但具身智能绝不仅仅是大模型加机器人的应用，而是物理世界的反馈需要及时进化大模型。我们光靠看书或看视频，永远学不会游泳，你得亲身扎到水里才能学会。大模型得通过机器人，扎进现实世界，才能真正理解物理世界。</p><p>为帮助建立世界模型，我们构建了“软硬虚实”一体的机器人训练场——“浦源·桃源”，同时攻关具身智能的“大脑”与“小脑”。“浦源·桃源”是首个城市级的具身智能数字训练场，构建了集场景数据、工具链、具身模型评测三位一体的开源具身智能研究平台。作为大模型与机器人的连接层，涵盖89种功能性场景、10万+高质量可交互数据，有望解决领域内数据匮乏、评测困难的问题。&nbsp; &nbsp;</p><p></p><p>在大脑方面，我们通过具身智能体自身状态认知、复杂任务分解分配、底层技能协同控制三方面创新，首次实现了大模型驱动的无人机、机械臂、机器狗三种异构智能体协同。在小脑方面，我们通过GPU高性能并行仿真和强化学习，可以高效实现机器人在真实世界里快速学习，并完成高难度动作。我们发现，单卡1小时的训练就能实现真实世界380天的训练效果。</p><p></p><p>无人驾驶可以理解为一个具身智能体。我们提出了开源且通用的自动驾驶视频预测模型GenAD，类似于自动驾驶领域的“SORA”，能够根据一张照片输入，生成后续较高质量、连续、多样化、符合物理世界规律的未来世界预测，并可泛化到任意场景，被多种驾驶行为操控。</p><p></p><h2>通专融合实践：科学发现</h2><p></p><p>对于科学发现领域，通专融合无疑也有着巨大的潜在价值。</p><p></p><p>2023年初，Nature曾发表过一篇封面文章，展示了对科研论文发展现状的悲观态度，指出“科学进步正在‘降速’”。文章认为，近年来科研论文数量激增，但没有颠覆性创新。因为科学本身的发展规律便是不断深入，每个学科形成了信息茧房，不同学科之间壁垒增加。对于顶尖科学家来说，即使穷尽一生也没有办法掌握一个学科所有的知识。这就启发我们需要新的科研组织方式来适配学科信息茧房，这也需要科研工作者与时俱进，采用AI工具赋能科研、加速创新。</p><p></p><p>由于大模型内部压缩着世界知识，同时具备不确定性生成的特性，因此有可能帮助我们打破不同学科领域知识茧房，进行创新式探索。我们认为大模型的不确定性和幻觉生成，并不总是它的缺陷，而是它的一个特点。合理利用这种特点，通过人机协同有助于促进科研创新。</p><p></p><p>事实上，就人类科学家而言，通过“做梦”找到研究思路的例子也不胜其数，最典型的就是，德国有机化学家奥古斯特·凯库勒梦见衔尾蛇，进而发现苯环结构。</p><p></p><p>我们探讨了大模型在生物医学领域的知识发现问题，针对最新的医学文献构建知识发现测试集，并对于最先进的大模型进行评测。我们发现大模型能够提出新的生物医学知识假设，并在最新的文献中得以验证。</p><p>这里给出一个我们发现新假设过程的简单示例：我们将已有的背景知识输入到2023年1月发布的大模型，并让大模型生成可能的假设。大模型提出的假设中，第一条假设是背景已知信息，还不是新的知识；但是第二条假设是之前文献中所没有的。两个月后，这条假设在2023年3月发表的论文中得到了验证。</p><p></p><p>这只是一个非常简单的例子，但已经显示出大模型具有很大的潜力，可以促进科研知识发现，并且能够提出新的有价值的未知假设。</p><p></p><p>通过通专融合，AI不只可以提出科学假设，还可以掌握科学知识、分析实验结果、预测科学现象。进而在反思的基础上，提升AI提出科学假设的能力。</p><p></p><p>在掌握科学知识方面，我们基于大语言基座模型能力进行专项能力强化，分别在化学和育种两个方向构建了首个开源大模型——书生·化学和书生·丰登；在分析实验结果方面，我们研发的晶体结构解析算法AI4XRD具备专家级的准确率，并将解析时间从小时级降低到秒级；在预测科学现象方面，我们训练并持续迭代了风乌气象大模型，在全球中期气象预报上具有当前世界领先的时间和空间分辨率；在提出科学假设方面，我们提出“人在环路大模型多智能体与工具协同”概念框架，对于科学假设的链路进行升级。构建了AI分析师、AI工程师、AI科学家和AI批判家多种角色，接入工具调用能力来协同提出新的假设。</p><p></p><h2>下一代AI for Science</h2><p></p><p>为什么提出一个好问题在科研中如此重要？早在1900年，德国数学家大卫·希尔伯特（David Hilbert）提出了著名的“23个问题”，引领了数学很多子领域数百年的发展。在科学上，提出一个好问题往往比解决问题更重要。希尔伯特还有一句名言，这也是他的墓志铭：“We must know. We will know.”我们必须知道。我们终将知道。今天，我们踏上通专融合的路线，探索通用人工智能AGI的未来，展望下一代的AI for Science，更可以从这句话中汲取灵感和激励。对于可信AGI的未来，正如我今天上午在全体大会的演讲，我们的态度是坚定而积极的：We must be there. We will be there！我们必须达成，我们终将抵达。</p><p></p><p>我今天站在这里也非常感慨，想起了去年汤晓鸥老师在WAIC大会上提到我们原创的成果、我们年轻的科学家，提到了我们的书生大模型。正是我们实验室一群有创造力的年轻科学家，让我们坚信：We must be there and we will be there！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/k6MYPKCAmN6dfD9l47oj</id>
            <title>2024 世界人工智能大会（WAIC）开幕，图灵得主的巅峰举首共商AI如何普惠全人类｜WAIC专题报道</title>
            <link>https://www.infoq.cn/article/k6MYPKCAmN6dfD9l47oj</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/k6MYPKCAmN6dfD9l47oj</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jul 2024 16:42:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能, 全球治理, 多元交融, 图灵奖得主
<br>
<br>
总结: 2024年7月4日，2024世界人工智能大会暨人工智能全球治理高级别会议在上海举行，吸引了来自联合国、各国政府、专业国际组织、知名专家、企业家和投资家等1000余人参加。会议围绕人工智能的发展、安全和治理展开深入研讨，强调了人工智能对经济社会发展和人类文明进步的重要影响。图灵奖得主们也在会上展开了关于人工智能治理的讨论，为全球人工智能发展和治理提供了宝贵观点和启示。会议还回顾了世界人工智能大会的发展历程，展示了上海在人工智能领域的重要助力和影响力。 </div>
                        <hr>
                    
                    <p>2024年7月4日，2024世界人工智能大会暨人工智能全球治理高级别会议-全体会议在上海世博中心举办。联合国以及各国政府代表、专业国际组织代表，全球知名专家、企业家、投资家1000余人参加了本次会议，围绕“以共商促共享，以善治促善智”的大会主题展开深入交流研讨。</p><p></p><h2>多元交融的全球议题</h2><p></p><p>人工智能是人类发展新领域，其快速发展对经济社会发展和人类文明进步产生了深远影响，也带来了未知风险和复杂挑战。本届大会全体会议直面人工智能治理这一全球性议程，聚焦发展、安全、治理，开展了一系列国际性、跨领域、多视角的深入研讨。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9a/9a095ffc62437332454449e5671b7488.png" /></p><p></p><p>清华大学苏世民书院院长、清华大学人工智能国际治理研究院院长薛澜，上海人工智能实验室主任、首席科学家、清华大学惠妍讲席教授周伯文，新思科技总裁兼首席执行官盖思新分别基于公共政策、科学、产业等不同视角，分享了他们关于人工智能领域技术创新和安全治理的最新成果和最新思考。黑石集团董事长、首席执行官兼联合创始人苏世民，索奈顾问及投资公司董事长、首席执行官乔舒亚·雷默，立足于商业投资视角，以及在人工智能国际治理中的长期实践，共同演绎了关于人工智能浪潮影响下的全球商业变革和治理创新的独到见解。通过来自演讲嘉宾不同角度的系统诠释，表明了在人工智能领域坚持发展和安全并重的必要性，以及加强人工智能国际对话与合作的迫切性。演讲嘉宾还共同表达了要推动人工智能健康发展，赋能经济增长、增进各国人民福祉的一致共识，为全球人工智能发展和治理提供了宝贵观点和启示。</p><p></p><h2>图灵得主的巅峰举首</h2><p></p><p>图灵奖是全球计算机领域的最高荣誉。本次全体会议现场，姚期智、罗杰·瑞迪、曼纽尔·布卢姆等三位享誉全球的图灵奖得主，与原微软执行副总裁、美国国家工程院外籍院士沈向洋，一同联袂进行了一场围绕治理协同创新的巅峰论道。通过极具思辨性的对话，深入探讨了人工智能的“双刃剑”属性、人工智能的可解释性和可预测性、人工智能的严谨底色和变革气质等人工智能领域全球瞩目的核心命题。针对加强人工智能全球治理的会议动议，三位图灵奖得主表现出一致的高度认同，并同时指出人才培养对于应对人工智能未来风险的重要价值。这些来自人工智能标志性人物的深刻见解，将对全球人工智能发展和治理产生深远影响，也将在世界人工智能发展史中，留下属于本次会议的闪亮印记。</p><p></p><h2>世界人工智能大会发展历程</h2><p></p><p>自2018年首次在上海举办，世界人工智能大会已成为上海打造人工智能这一城市新名片的重要助力。基于大会平台和上海支点，越来越多嘉宾选择更加紧密地与上海同行。又一次登台的图灵奖获得者、中国科学院院士姚期智，2020年在上海成立了以自己名字命名的上海期智研究院，专攻人工智能、量子智能方向的基础研究。清华大学惠妍讲席教授周伯文，不久前获得了上海人工智能实验室的邀请担任主任、首席科学家。全体会议共同上演“图灵圆桌”的沈向洋、罗杰·瑞迪、曼纽尔·布卢姆，此前都曾与大会多次携手。某种意义上说，本次他们之间进行的“图灵圆桌”也是关于大会的一次“老友记”。</p><p></p><p>以他们为缩影，追溯更多全球顶尖科学家和先锋企业家们的选择。不难发现承担“科技风向标、应用展示台、产业加速器、治理议事厅”作用的世界人工智能大会，正在为上海加快打造人工智能世界级高端产业集群，源源不断注入新活力和新动能，也将为上海以深入落实人工智能“上海方案”，率先履行《人工智能全球治理上海宣言》，服务构建“以善治促善智”的中国城市样本提供的有益启发和重要助力。</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/SS6434mmPjzwPmqSu2yb</id>
            <title>万卡万P万亿参数通用算力！摩尔线程夸娥智算中心再升级｜WAIC专题报道</title>
            <link>https://www.infoq.cn/article/SS6434mmPjzwPmqSu2yb</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/SS6434mmPjzwPmqSu2yb</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jul 2024 16:32:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 摩尔线程, AI旗舰产品, 夸娥智算集群, 万卡规模
<br>
<br>
总结: 摩尔线程宣布其AI旗舰产品夸娥智算集群实现重大升级，从千卡级别扩展至万卡规模，旨在打造具备万P级浮点运算能力的国产通用加速计算平台，专为万亿参数级别的复杂大模型训练而设计。 </div>
                        <hr>
                    
                    <p>7月3日，摩尔线程重磅宣布其AI旗舰产品夸娥（KUAE）智算集群解决方案实现重大升级，从当前的千卡级别大幅扩展至万卡规模。摩尔线程夸娥（KUAE）万卡智算集群，以全功能GPU为底座，旨在打造能够承载万卡规模、具备万P级浮点运算能力的国产通用加速计算平台，专为万亿参数级别的复杂大模型训练而设计。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/4b/4b3ece8869d98d45ad95d80ff452f5c1.png" /></p><p>&nbsp;</p><p>摩尔线程创始人兼CEO张建中表示：“当前，我们正处在生成式人工智能的黄金时代，技术交织催动智能涌现，GPU成为加速新技术浪潮来临的创新引擎。摩尔线程矢志投身于这一历史性的创造进程，致力于向全球提供加速计算的基础设施和一站式解决方案，为融合人工智能和数字孪生的数智世界打造先进的加速计算平台。夸娥万卡智算集群作为摩尔线程全栈AI战略的一块重要拼图，可为各行各业数智化转型提供澎湃算力，不仅有力彰显了摩尔线程在技术创新和工程实践上的实力，更将成为推动AI产业发展的新起点。”&nbsp;</p><p></p><h2>AI主战场，万卡通用算力是标配</h2><p></p><p>大模型自问世以来，关于其未来的走向和发展趋势亟待时间验证，但从当前来看，几种演进趋势值得关注，使得其对算力的核心需求也愈发明晰。</p><p>&nbsp;</p><p>首先，Scaling Law将持续奏效。Scaling Law自2020年提出以来，已揭示了大模型发展背后的“暴力美学”，即通过算力、算法、数据的深度融合与经验积累，实现模型性能的飞跃，这也成为业界公认的将持续影响未来大模型的发展趋势。Scaling Law将持续奏效，需要单点规模够大并且通用的算力才能快速跟上技术演进。</p><p>&nbsp;</p><p>其次，Transformer架构不能实现大一统，和其他架构会持续演进并共存，形成多元化的技术生态。生成式AI的进化并非仅依赖于规模的简单膨胀，技术架构的革新同样至关重要。Transformer架构虽然是当前主流，但新兴架构如Mamba、RWKV和RetNet等不断刷新计算效率，加快创新速度。随着技术迭代与演进，Transformer架构并不能实现大一统，从稠密到稀疏模型，再到多模态模型的融合，技术的进步都展现了对更高性能计算资源的渴望。</p><p>&nbsp;</p><p>与此同时，AI、3D和HPC跨技术与跨领域融合不断加速，推动着空间智能、物理AI和AI 4Science、世界模型等领域的边界拓展，使得大模型的训练和应用环境更加复杂多元，市场对于能够支持AI+3D、AI+物理仿真、AI+科学计算等多元计算融合发展的通用加速计算平台的需求日益迫切。</p><p>&nbsp;</p><p>多元趋势下，AI模型训练的主战场，万卡已是标配。随着计算量不断攀升，大模型训练亟需超级工厂，即一个“大且通用”的加速计算平台，以缩短训练时间，实现模型能力的快速迭代。当前，国际科技巨头都在通过积极部署千卡乃至超万卡规模的计算集群，以确保大模型产品的竞争力。随着模型参数量从千亿迈向万亿，模型能力更加泛化，大模型对底层算力的诉求进一步升级，万卡甚至超万卡集群成为这一轮大模型竞赛的入场券。</p><p>&nbsp;</p><p>然而，构建万卡集群并非一万张GPU卡的简单堆叠，而是一项高度复杂的超级系统工程。它涉及到超大规模的组网互联、高效率的集群计算、长期稳定性和高可用性等诸多技术难题。这是难而正确的事情，摩尔线程希望能够建设一个规模超万卡、场景够通用、生态兼容好的加速计算平台，并优先解决大模型训练的难题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6f/6ff2589b83ac7618ef043715022df6bb.png" /></p><p></p><h2>夸娥：国产万卡万P万亿大模型训练平台</h2><p></p><p></p><p>夸娥（KUAE）是摩尔线程智算中心全栈解决方案，是以全功能GPU为底座，软硬一体化、完整的系统级算力解决方案，包括以夸娥计算集群为核心的基础设施、夸娥集群管理平台（KUAE Platform）以及夸娥大模型服务平台（KUAE ModelStudio），旨在以一体化交付的方式解决大规模GPU算力的建设和运营管理问题。</p><p>&nbsp;</p><p>基于对AI算力需求的深刻洞察和前瞻性布局，摩尔线程夸娥智算集群可实现从千卡至万卡集群的无缝扩展，旨在满足大模型时代对于算力“规模够大+计算通用+生态兼容”的核心需求。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/c9/c998de1e872b93c39a4f27fbf6c583dd.png" /></p><p></p><p>夸娥万卡智算解决方案具备多个核心特性：</p><p>超大算力，万卡万P：在集群计算性能方面，全新一代夸娥智算集群实现单集群规模超万卡，浮点运算能力达到10Exa-Flops，大幅提升单集群计算性能，能够为万亿参数级别大模型训练提供坚实算力基础。同时，在GPU显存和传输带宽方面，夸娥万卡集群达到PB级的超大显存总容量、每秒PB级的超高速卡间互联总带宽和每秒PB级超高速节点互联总带宽，实现算力、显存和带宽的系统性协同优化，全面提升集群计算性能。超高稳定，月级长稳训练：稳定性是衡量超万卡集群性能的关键。在集群稳定性方面，摩尔线程夸娥万卡集群平均无故障运行时间超过15天，最长可实现大模型稳定训练30天以上，周均训练有效率在99%以上，远超行业平均水平。这得益于摩尔线程自主研发的一系列可预测、可诊断的多级可靠机制，包括：软硬件故障的自动定位与诊断预测实现分钟级的故障定位，Checkpoint多级存储机制实现内存秒级存储和训练任务分钟级恢复以及高容错高效能的万卡集群管理平台实现秒级纳管分配与作业调度。极致优化，超高MFU：MFU是评估大模型训练效率的通用指标，可以直接反应端到端的集群训练效率。夸娥万卡集群在系统软件、框架、算法等层面一系列优化，实现大模型的高效率训练，MFU最高可达到60%。其中，在系统软件层面，基于极致的计算和通讯效率优化等技术手段，大幅提升集群的执行效率和性能表现。在框架和算法层面，夸娥万卡集群支持多种自适应混合并行策略与高效显存优化等，可以根据应用负载选择并自动配置最优的并行策略，大幅提升训练效率和显存利用。同时，针对超长序列大模型，夸娥万卡集群通过CP并行、RingAttention等优化技术，有效缩减计算时间和显存占用，大幅提升集群训练效率。全能通用，生态友好：夸娥万卡集群是一个通用加速计算平台，计算能力为通用场景设计，可加速LLM、MoE、多模态、Mamba等不同架构、不同模态的大模型。同时，基于高效易用的MUSA编程语言、完整兼容CUDA能力和自动化迁移工具Musify，加速新模型“Day0”级迁移，实现生态适配“Instant On”，助力客户业务快速上线。&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/jRJbK8ll7KaVcr9AW4DD</id>
            <title>蚂蚁顾进杰：真正的AI管家，不仅会吟诗作画，还要是可靠的生活助手</title>
            <link>https://www.infoq.cn/article/jRJbK8ll7KaVcr9AW4DD</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/jRJbK8ll7KaVcr9AW4DD</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jul 2024 16:32:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 世界人工智能大会, 专业智能体, 大模型, 专业知识引擎
<br>
<br>
总结: 2024年世界人工智能大会在上海开幕，蚂蚁集团董事长表示专业智能体能解决大模型在产业应用中的难题，正在构建专业智能体生态加速产业应用。蚂蚁通过专业知识引擎提供领域专业知识，构建专业智能体框架，推动AI技术服务升级。 </div>
                        <hr>
                    
                    <p>2024世界人工智能大会进入第三天，一些行业权威成果陆续发布。</p><p></p><p>在2024世界人工智能大会“大模型焕新与产业赋能”论坛上，中国信通院华东分院、上海人工智能实验室及相关代表企业联合发布了《2024大模型典型示范应用案例集》，旨在展现具有先进性、引领性、示范性的典型案例，推动大模型产业生态持续繁荣发展。蚂蚁集团基于百灵大模型的支付宝智能助理、AI金融管家、基于百灵大模型的“医保小智”、AI研发助手Codefuse、DB-GPT数据智能体、AI标注智能体、安全智选7项应用入选。蚂蚁集团大模型应用部总经理顾进杰作为企业代表参与了案例集发布仪式，并发表主旨演讲。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7d/7d6ecc3f2c0f89b1955ed21a2b352c0c.jpeg" /></p><p></p><p>（中国信通院发布“2024大模型典型示范应用案例集”）</p><p></p><p>今年以来，国内外科技公司竞相推出了大模型应用产品，逐鹿大模型竞争的下半场。</p><p>&nbsp;</p><p>在大模型应用路线选择上，蚂蚁集团认为，专业智能体能够破解通用大模型在严谨产业应用的关键难题，正在携手产业合作伙伴共建专业智能体生态，加速产业应用。</p><p>&nbsp;</p><p>以入选案例集的支付宝智能助理为例，这是国内首个办事型AI生活管家，围绕用户的吃、喝、行、游、办事、买票、娱乐等数十种生活场景，不仅“有脑有嘴能对话”，还“有手有脚能办事”。 在它背后，是以蚂蚁百灵大模型为基座，不同的专业性智能体协作来提供智能化的服务。</p><p>&nbsp;</p><p>顾进杰介绍，伴随AI走向产业共建，支付宝智能助理也将成为专业智能体生态的平台入口之一，用户通过对话就能一键连接生活、金融、医疗等垂直行业的AI智能体，获得更专业丰富的服务。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f4/f43dd8e2b62c810ca9b0461c0af1c5c9.jpeg" /></p><p></p><p>（蚂蚁集团大模型应用部总经理顾进杰发布主旨演讲）</p><p>&nbsp;</p><p>据了解，支付宝智能助理还入选了2024世界人工智能大会“镇馆之宝”，这一奖项旨在奖励全球人工智能领域的最新技术成果，并展示发展趋势及商业潜力。</p><p>&nbsp;</p><p>本次入选案例集的其他应用，也是蚂蚁探索专业智能体助力大模型在严谨产业落地的成果。如AI金融管家，它背后融合了百亿级金融知识数据存储的知识力、以及多智能体协同，重塑了理财问答的体验，从原本机械化的回答，到逐步接近人类专家的沟通分析水平，金融意图识别准确率达到了95%。</p><p>&nbsp;</p><p>“真正的AI管家，不仅仅只是吟诗作画，而是要成为懂用户、会办事、靠得住的贴心助手”。我们希望，通过专业智能体的深度连接，为AI带来互联网式跃升”。顾进杰在演讲中表示。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/OVL8Z6NwTHBiPmNCHbbM</id>
            <title>非Transformer架构大模型公司岩芯数智RockAl走通类脑机制：端侧AI也可以很智能｜WAIC专题报道</title>
            <link>https://www.infoq.cn/article/OVL8Z6NwTHBiPmNCHbbM</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/OVL8Z6NwTHBiPmNCHbbM</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jul 2024 16:31:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 世界人工智能大会, 多模态机器人, 端侧AI, Yan架构大模型
<br>
<br>
总结: 2024年世界人工智能大会在上海举行，展示了多模态机器人和端侧AI技术的最新成果，其中RockAI的Yan架构大模型引人注目。通过创新的技术突破和实践，端侧AI正逐步克服技术壁垒，向更广泛的应用场景迈进，为智慧生活的未来布局。 </div>
                        <hr>
                    
                    <p>7月4日-7月7日，2024世界人工智能大会（WAIC）在上海举行，来自国内外的数百款大模型集体亮相，呈现了AI大模型智能涌现、赋能千行百业的生动场景。在各色技术及应用的创新体验区中，一个具备高度交互能力的多模态机器人引起了人们的注意。</p><p></p><p>它就是来自岩芯数智RockAI，搭载了树莓派5代芯片的“小智”，在极低算力的设备上实现了强大的多模态能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5b/5bf0bfa73a351631e687da8e4892bfca.jpeg" /></p><p></p><p>继年初推出超强性能的Yan1.0云端大模型后，RockAI再次突破了端侧AI“原生无损”门槛，并在这款机器人上部署了此次首发的Yan 1.2大模型。</p><p></p><p>与传统的自动控制机器人不同，小智具备多模态认知能力，能够基于Yan 1.2的语音和视觉处理能力，准确理解用户的模糊指令和意图，并据此控制其机械躯体完成各类复杂任务。随着这款智能机器人在各种模糊指令下描述“视觉”场景、展现“四步成诗”，一场关于端侧AI的全新想象也铺展开来。</p><p></p><p><img src="https://static001.geekbang.org/infoq/16/16853c4be6ba0a3da3466ef885a84ac9.png" /></p><p></p><h2>端侧AI打响突围赛</h2><p></p><p>端侧AI通常指在终端设备上直接运行和处理人工智能算法的技术，其优势在于可以直接利用设备的计算能力进行数据处理，不需要将数据发送到云端或服务器进行处理，从而降低对云端计算资源的依赖。且无论网络环境如何都能即时生成响应，确保用户数据安全性的同时，减少了相关算力成本开支。</p><p></p><p>自去年下半年以来，随着大模型的竞争从技术驱动过渡到市场驱动，端侧AI以其广泛的应用场景备受青睐，开始释放出全新的发展价值。全球范围内各大模型厂商纷纷通过各种技术手段，尝试将大模型在物理意义上融入终端。</p><p></p><p>但受制于端侧AI落地的算力和功耗等挑战，传统轻量化部署往往均以性能损失为代价。如目前大热的AIPC是把Transformer架构的模型通过量化压缩部署到个人电脑，仅70亿参数的大模型还需要额外定制PC芯片为其提供算力。而此前在微型电脑主板树莓派5上打出“1.89tokens每秒速度运行、支持8K上下文窗口”战绩的Llama3 8B，同样止步于“有损压缩”。压缩后的模型不仅性能大打折扣，还会失去再训练、再学习的能力，成为无法定时更新底层知识的“一次性AI”。</p><p></p><h2>基于仿生神经元驱动的选择算法，Yan 1.2更专注于端侧设备</h2><p></p><p>RockAI此次首发的Yan 1.2大模型，可以“原生无损”地以6+tokens/s的速度运行于算力仅普通电脑八分之一的树莓派上，并在这个仅有信用卡大小的芯片上实现超强的多模态能力，不仅能“听说读”，还可以识别模糊指令，进行学习、创作及互动。</p><p></p><p>这一成果，起初是得益于对于大模型基础架构的“破坏式”创新。早在今年1月，RockAI发布了国内首个非Transformer的Yan架构大模型。该架构通过对Attention机制的替换，将计算复杂度降为线性，大幅降低了对算力的需求，用百亿级参数达成千亿参数大模型的性能效果，并且率先实现了在主流消费级CPU等端侧设备上的原生无损运行。</p><p></p><p>为了实现树莓派等更多更低端设备的无损适配，RockAI基于全新自研的Yan架构，在实验室对人工神经网络最底层的反向传播算法进行挑战，寻找反向传播的更优解尝试，进一步实现Yan模型的降本增效。同时在算法侧，RockAI采用了基于仿生神经元驱动的选择算法，实现了类脑分区激活的工作机制，使大模型可以根据学习的类型和知识的范围分区激活，大幅减少了数据训练量，同时也能有效发挥多模态的潜力。故而，模型迭代到1.2版本，已经可以实现在PC端、手机端、树莓派端和机器人端等设备上的无损运行。</p><p></p><h2>“同步学习”打造设备端“最强大脑”</h2><p></p><p></p><p>历经了卷参数、卷市场的阶段，大模型当下正集中于一个“卷智能”的时代，因此，让大模型无损跑通更多低算力设备只是第一步，接下来就要思考如何提高端侧大模型的知识密度、智能密度。但RockAI CEO刘凡平还有一个更高的目标，就是在实现通用人工智能的同时，将AI与每个人独特的地方结合在一起，模型具备自主学习能力，让每个设备都拥有个性化的智能。</p><p></p><p>为了实现这种个性化的通用人工智能，RockAI团队首创了“同步学习”理念，让模型具备像人一样实时学习的能力，在推理的同时进行知识更新和学习，无需像云端大模型一样“返厂”进行再次更新或预训练。从而实时、有效且持续性地提升大模型的智能密度，应对各类个性化场景中出现的问题。</p><p></p><p>基于神经网络的底层技术创新，RockAI不断尝试寻找反向传播的更优解，试图能更低代价更新神经网络，实现对现有知识体系的快速更新，辅以模型分区激活降低功耗、实现部分更新，使大模型像人类学习一样建立自己独有的知识体系，实现模型的边跑边进化。会上，RockAI展示了“同步学习”的实验室示例，并表示该机制已处于实验室最后验证阶段。</p><p></p><p>而对于Yan模型在设备端的落地，刘凡平则透露，团队正加紧进行设备端的适配工作，目前已与众多硬件和芯片厂商建立了沟通与合作。</p><p></p><p>RockAI以Yan架构大模型为核心的技术突破与创新实践，标志着端侧AI正逐步克服技术壁垒，向更广泛的应用场景迈进。不仅是对现有计算范式的挑战与超越，更是对未来智慧生活的前瞻布局。</p><p>随着全模态支持+实时人机交互+同步学习的落地，Yan 2.0或将重新定义设备的价值，成为设备的“最强大脑”，真正做到“让世界上每一台设备都拥有自己的智能”。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Q65kQ7ELVLf6pKxBDsIs</id>
            <title>国内最重视生成式AI的企业和最卷的同事们都在哪？｜InfoQ技术大会年中盘点</title>
            <link>https://www.infoq.cn/article/Q65kQ7ELVLf6pKxBDsIs</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Q65kQ7ELVLf6pKxBDsIs</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jul 2024 05:53:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 技术人, 生成式 AI, 企业实践, 技术大会
<br>
<br>
总结: 近年来，技术人员受到裁员新闻的影响，但生成式 AI 技术的兴起让企业重新关注技术改造。在技术大会上，企业对生成式 AI 的实践非常重视，尤其是传统企业的导入速度更快。互联网企业和金融领域企业在生成式 AI 方面表现突出，而技术服务企业仍然是最活跃的。企业通过技术大会培训员工，提升竞争优势，展示了对技术和人才的持续投入。 </div>
                        <hr>
                    
                    <p>过去几年，技术人仿佛被“裁员新闻”深深笼罩着，甚至有段时间，个别自媒体是按月播报互联网公司裁员新闻的。生成式 AI 时代的到来，让我们看到各大企业再次开始用技术改造一切，去年至今最常被媒体提到的一句话是“移动互联网时代做的事情，都值得用生成式 AI 重来一遍”。</p><p></p><p>那么，哪些企业对生成式 AI 技术实践更加重视呢？今年至今，InfoQ 共举办了三场技术大会，分别是 4 月份的 <a href="https://qcon.infoq.cn/2024/beijing">QCon 北京站</a>"，5 月份的 <a href="https://aicon.infoq.cn/2024/beijing/schedule">AICon 北京站</a>"和 <a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">ArchSummit 深圳站</a>"，这三场大会的单场参会人次均在 1000 以上。在三场大会的所有分享中，与生成式 AI 相关的议题占比高达 80%。根据对这三场大会的购票企业的统计，我们可以看到哪些企业还在对技术、对技术人的成长做持续性投入，而这些企业极有可能因此汇聚更多优秀人才，进而在生成式 AI 时代脱颖而出。</p><p></p><h2>互联网依旧“最卷”，人才不仅要争夺也要培训</h2><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/3a/3ae20f1af9ae38ba26c13670226a473c.webp" /></p><p></p><p>根据 InfoQ 的数据统计，截至上半年共有 520+ 企业通过购票的方式让内部员工来到大会现场参会学习。其中，华为上半年累计已有数百位技术人员、业务人员、产品人员、管理者等来到大会现场交流学习。不仅如此，华为也在积极地通过 InfoQ 技术大会的平台对外输出内部技术实践，比如今年 QCon 北京的<a href="https://qcon.infoq.cn/2024/beijing/track/1698">《鸿蒙原生应用开发关键技术与创新竞争力》</a>"专场。此外，其他上榜企业也至少曾派出数十位员工来到大会现场交流学习。令人欣喜的是，相比于以前互联网企业霸屏，我们这次看到了三家传统企业上榜。</p><p></p><p>究其原因，导入生成式 AI 技术对技术团队本身提出了更高挑战，原本具有人才优势的大厂依旧会坚定地选择通过各种培训方式增加员工的知识储备，不具备人才优势的企业更加需要快速行动起来，通过人才引进和内部员工培训来提升企业在数字化时代的竞争优势。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a7/a7b22a9252ee8cc19f17dedb19026c62.webp" /></p><p></p><p>如果分行业看，“最卷”的依旧是技术服务类型企业，主要指通过对外提供技术服务解决方案获利的企业，无论是现场参会还是参与大会对外输出实践案例都非常积极。随后依次是互联网、金融、制造 / 汽车、政府 / 高校 / 公共事业、能源 / 电力等。</p><p></p><p>在互联网的众多上榜企业中也可以看到一个很明显的趋势：内部有广泛应用场景的企业会格外重视这一轮浪潮，会更加迫切地希望内部员工快速掌握相关技能，从而完成企业内部的技术升级换代。如果结合大会讲师所在企业再看这一数据也是如此，内部场景丰富的阿里巴巴、字节跳动、腾讯、百度、华为、小红书、快手、哔哩哔哩、饿了么、京东、去哪儿网等企业依旧是实践领先且乐于分享的厂商代表。</p><p></p><h2>生成式 AI 的这一次浪潮，传统企业的导入速度明显加快</h2><p></p><p></p><p>根据 InfoQ 的观察，相比于前几次技术浪潮，这一批浪潮中的金融、制造 / 汽车等相对传统领域的企业导入速度非常快，这类企业对技术交流、人才培养的重视程度高、推进速度快，内部场景丰富，这批参会者的占比在今年上半年上升非常明显，且这些领域的成果也非常丰富。</p><p></p><p><img src="https://static001.geekbang.org/infoq/dc/dc5f243f9176ced3237258e82ffec48b.webp" /></p><p></p><p>以金融领域为例，无论是银行、保险还是证券，大众所熟知的企业基本全部都瞄定了生成式 AI 方向。从技术布局上来看，金融企业也完全不逊色于互联网，且部分企业在相对前沿的多模态、数字人、具身智能等领域均有尝试。在实践方面，金融领域在智能体（包括多智能体的协同）、多模态智能风控落地、数据资产化运营与数据智能应用、数字化营销等层面均有不同程度的落地，这些话题也同样是 InfoQ 与铸基计划联合主办的<a href="http://gk.link/a/12oH7"> FCon 全球金融科技大会</a>"的重要议题，届时来自北京银行、广发银行、平安银行、中信银行、华夏银行、太平洋保险、中泰证券、新疆银行、度小满、国投证券、华泰证券、天弘基金、华安保险、工银科技等企业的专家将会同台分享（以上名称不分先后）。</p><p></p><p>FCon 大会官网：<a href="http://gk.link/a/12oH7">http://gk.link/a/12oH7</a>"</p><p></p><p>根据对参会者的调研，大家普遍最关注的内容集中在 Agent、RAG、大模型应用探索，多模态、业务架构、成本优先的技术架构等层面。一方面是大部分企业内部正在走的技术方向，比如 Agent 和 RAG，自然会受到更多关注；一方面是对前沿技术的布局，希望从领先的企业中获取实践经验，比如多模态、大模型应用探索等。</p><p><img src="https://static001.geekbang.org/infoq/29/29fd86b8058287e4bb5bfa9abd3fa7d7.webp" /></p><p></p><p>结合参会者的关注重点及企业专家的意见，InfoQ 即将于 8 月份召开的 <a href="http://gk.link/a/12oH8">AICon 上海站</a>"特别设置了大模型训练以及推理加速、RAG 落地应用与探索、多模态大语言模型的前沿应用与创新、大模型产品应用构建、大模型与企业工具集成的提效实践、大模型产学研结合探索、端侧模型落地探索、大模型数据集构建及评测技术落地、AI Agent 技术突破与应用、大模型场景 + 行业应用落地实践、大模型在搜索、广告、推荐领域的探索、大模型安全性实践等话题，将会邀请 AI 创企、互联网的先行企业、学术和科研机构等专家同台分享。</p><p></p><p>AICon 大会官网：<a href="http://gk.link/a/12oH8">http://gk.link/a/12oH8</a>"</p><p></p><p>与此同时，新能源汽车领域在今年上半年获取了业内的超高关注，其所带来的智能体验和所具备的辅助驾驶能力备受关注，包括汽车企业本身和其生产链基于大模型也做了众多革新。在 <a href="http://gk.link/a/12oH8">AICon 上海站</a>"的主会场，我们有幸邀请到了蔚来汽车的创始人、董事长、CEO 李斌分享蔚来在智能化层面的相关规划和实践。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d5/d5a3659c4efe679602d00158e5544b77.webp" /></p><p></p><p></p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/lYd372BHvHjBILDQao3d</id>
            <title>GPU 集群规模从 4K 飙升至 24K，Meta 如何引领大规模语言模型训练突破</title>
            <link>https://www.infoq.cn/article/lYd372BHvHjBILDQao3d</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/lYd372BHvHjBILDQao3d</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jul 2024 02:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI研究, 大型语言模型, GenAI, 大规模模型训练
<br>
<br>
总结: 在AI研究中，随着大型语言模型的训练需求不断增加，GenAI的出现导致了大规模模型训练的转变。这种转变带来了许多挑战，包括硬件可靠性、故障时快速恢复、训练状态的有效保存以及GPU之间的最佳连接。为了应对这些挑战，需要在训练软件、调度、硬件和数据中心部署等方面进行创新和优化。同时，网络基础设施的选择也是关键，RoCE和InfiniBand架构都是可行的选项，而同时构建两个24k集群则是一种学习和实践的方式。 </div>
                        <hr>
                    
                    <p>在我们继续将 AI 研究和开发的重点放在解决一系列日益复杂的问题上时，我们经历的最重大和最具挑战性的转变之一是训练大型语言模型（LLM）所需的巨大计算规模。</p><p></p><p>传统上，我们的 AI 模型训练任务会训练大量模型，而这些模型需要的 GPU 相对较少。我们的推荐模型（例如 feed 和排名模型）就是这种情况，这些模型能够获取大量信息以提供准确的建议，为我们的大多数产品提供支持。</p><p></p><p><img src="https://static001.geekbang.org/infoq/72/7297de5540e03de93569edc85d41efc8.webp" /></p><p></p><p>随着生成式 AI（GenAI）的出现，我们看到了模型训练在向更少的模型数量与更庞大的作业转变。大规模支持 GenAI 意味着重新思考我们的软件、硬件和网络基础设施结合在一起的方式。</p><p></p><p></p><h2>大规模模型训练的挑战</h2><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/55/55abddef125cbc5b3ed7526f473df44e.webp" /></p><p></p><p>在我们增加作业中 GPU 数量的同时，由于硬件故障而中断的可能性也会增长。此外，所有这些 GPU 仍然需要在同一个高速结构上通信才能实现最佳性能。这就引出了四大重要因素：</p><p></p><p>硬件可靠性：确保硬件可靠是非常重要的。我们需要尽量减少硬件故障中断训练作业的可能性。这涉及严格的测试和质量控制措施，以及自动化的快速检测和问题补救机制。</p><p></p><p>故障时快速恢复：尽管我们尽了最大努力，但硬件故障仍会发生。当它们发生时，我们需要能够快速恢复。这就需要减少重新调度开销和快速实现训练重初始化。</p><p></p><p>训练状态的有效保存：如果发生故障，我们需要能够从中断的地方继续。这意味着我们需要定期检查我们的训练状态，并有效地存储和检索训练数据。</p><p></p><p>GPU 之间的最佳连接：大规模模型训练需要以同步方式在 GPU 之间传输大量数据。GPU 子集之间的缓慢数据交换会拖累整个作业的速度。解决这个问题需要强大而高速的网络基础设施，以及高效的数据传输协议和算法。</p><p></p><p></p><h2>跨基础设施栈进行创新</h2><p></p><p></p><p>由于 GenAI 的大规模需求，完善基础设施栈的每一层都很重要。这需要在众多领域取得广泛进展。</p><p>训练软件</p><p></p><p>我们使研究人员能够使用 PyTorch 和其他新的开源开发工具，从而实现极快的研究到生产开发速度。这些努力包括了开发新的算法和技术以进行高效的大规模训练，并将新的软件工具和框架集成到我们的基础设施中。</p><p></p><p></p><h3>调度</h3><p></p><p></p><p>高效的调度有助于确保我们的资源得到最佳利用。这方面的成果包括了可以根据不同作业的需求分配资源的复杂算法，和能够适应不断变化的负载的动态调度。</p><p></p><p></p><h3>硬件</h3><p></p><p></p><p>我们需要高性能硬件来处理大规模模型训练的计算需求。除了大小和规模之外，许多硬件配置和属性都需要针对 GenAI 进行最佳优化。鉴于硬件开发时间通常很长，我们必须调整现有硬件，为此，我们探索了包括功率、HBM 容量和速度以及 I/O 在内的各个方面。</p><p></p><p>我们还修改调整了使用 NVIDIA H100 GPU 开发的 Grand Teton 平台，将 GPU 的 TDP 提高到 700W，并迁移到了 GPU 上的 HBM3 内存上。由于我们没有时间改进冷却基础设施，所以只能留在风冷环境中。机械和热设计必须做出改变以适应这种情况，这触发了一个用来支持大规模部署的验证周期。</p><p></p><p>所有这些与硬件相关的更改都颇具挑战性，因为我们必须找到一种适合现有资源限制的解决方案，并且方案的更改自由度很小，还要满足紧迫的时间表。</p><p></p><p></p><h3>数据中心部署</h3><p></p><p></p><p>一旦我们选定了 GPU 和系统，将它们放置在数据中心中来充分利用各种资源（电源、冷却、网络等）的任务，就需要重新考虑为其他类型的负载所做的许多权衡。数据中心的电源和冷却基础设施无法快速（或轻松）调整，我们必须找到一种最佳布局，以在数据大厅内实现最大算力。这需要将读取器等支持服务移出数据大厅，并安装尽可能多的 GPU 机架，以最大限度地提高功率和网络能力，从而通过最大的网络集群实现最高的计算密度。</p><p></p><p></p><h3>可靠性</h3><p></p><p></p><p>我们需要规划检测和补救措施，以尽可能减少硬件故障期间的停机时间。故障数量与集群的大小成正比，而跨集群的作业需要保留足够的备用容量，以便尽快重新启动作业。此外，我们还会监控各种故障，有时可以采取预防措施来减少停机时间。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b38890d52cf9ec4f52181491fa6f0ec6.webp" /></p><p></p><p>我们观察到的一些最常见的故障模式包括：</p><p></p><p>GPU 脱落：在这种情况下，主机无法在 PCIe 接口上检测到 GPU。这种故障有多种原因，但这种故障模式在早期更常见，并随着服务器使用时间增加而逐渐减少。</p><p></p><p>DRAM 和 SRAM UCE：内存中经常出现不可纠正的错误，我们监控和识别重复犯错的单元，跟踪阈值，并在错误率超过供应商阈值时启动 RMA。</p><p></p><p>硬件网络电缆：在常见的服务器无法访问的错误类别中，这些故障也最常出现在服务器刚开始部署的时期。</p><p></p><p></p><h3>网络</h3><p></p><p></p><p>大规模模型训练需要在 GPU 之间快速传输大量数据。这需要强大而高速的网络基础设施以及高效的数据传输协议和算法。</p><p></p><p>业界有两种符合这些要求的领先选项：RoCE 和 InfiniBand 架构。这两个选项都有各自的权衡。一方面，Meta 在过去四年中构建了一些 RoCE 集群，但其中最大的集群仅支持 4K GPU。我们需要更大的 RoCE 集群。另一方面，Meta 已经使用 InfiniBand 构建了多达 16K GPU 的研究集群。但是，这些集群并没有紧密集成到 Meta 的生产环境中，也不是为最新一代的 GPU/ 网络构建的。这让我们很难决定使用哪种架构来构建。</p><p></p><p>因此，我们决定同时构建两个 24k 集群，一个使用 RoCE，另一个使用 InfiniBand。我们的目的是构建并从运营经验中学习。这些经验将为 GenAI 网络架构的未来发展方向提供参考。我们优化了 RoCE 集群以缩短构建时间，并优化了 InfiniBand 集群以提供全双工带宽。我们使用 InfiniBand 和 RoCE 集群来训练 Llama 3，其中 RoCE 集群用于训练最大的模型。尽管这些集群之间存在底层网络技术的差异，但我们可以调整它们，为这些大型 GenAI 负载提供同等的性能</p><p></p><p><img src="https://static001.geekbang.org/infoq/a4/a48f46f79274501b6f128e397bd3ddf1.webp" /></p><p></p><p>我们优化了整个堆栈的三个方面，使 GenAI 模型的网络通信在两个集群上都有很高的性能：</p><p></p><p>我们将由不同模型、数据和管道并行性产生的通信模式分配给网络拓扑的不同层，以便有效利用网络能力。</p><p></p><p>2.我们实现了具有网络拓扑感知的集体通信模式，降低它们对延迟的敏感度。为了做到这一点，我们使用自定义算法（例如递归加倍或减半）代替传统算法（如环），更改了集体的默认实现。</p><p></p><p>3.就像排名作业一样，GenAI 作业会产生额外的胖流，这使我们很难在所有可能的网络路径上分配流量。这就要求我们进一步投资网络负载平衡和路由，以实现跨网络资源的最佳流量分配。</p><p>我们在 Networking @Scale 2023 上深入讨论了我们的 RoCE 负载平衡技术。</p><p></p><p><img src="https://static001.geekbang.org/infoq/aa/aac40e365bfae27e4862bcb826c5f364.webp" /></p><p></p><p></p><h3>存储</h3><p></p><p></p><p>我们需要高效的数据存储解决方案来存储模型训练中使用的大量数据。这需要我们投资高容量和高速存储技术，以及为特定负载开发新的数据存储解决方案。</p><p></p><p></p><h2>展望未来</h2><p></p><p></p><p>在未来几年中，我们将使用数十万个 GPU 处理更大量的数据，并应对更长的距离和延迟。我们将采用很多新的硬件技术（包括更新的 GPU 架构）并改进我们的基础设施。</p><p></p><p>这些挑战将推动我们以自己尚无法完全预测的方式来创新和适应变化。但有一件事是肯定的：我们这段旅程才刚刚开始。随着我们继续探索不断发展的 AI 格局，我们还在努力突破可能的边界。</p><p></p><p>原文链接：</p><p></p><p>https://engineering.fb.com/2024/06/12/data-infrastructure/training-large-language-models-at-scale-meta/</p><p></p><p>声明：本文为 InfoQ 翻译，未经许可禁止转载。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ATVgBDcFNoFHJOX9Oi7H</id>
            <title>Runway 的 Gen-3 向所有用户开放付费使用，网友：免费的可灵更香</title>
            <link>https://www.infoq.cn/article/ATVgBDcFNoFHJOX9Oi7H</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ATVgBDcFNoFHJOX9Oi7H</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jul 2024 07:26:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Runway, Gen-3 Alpha, 视频生成, 创意工作者
<br>
<br>
总结: Runway 平台发布了新的生成式 AI 模型 Gen-3 Alpha，具有强大的视频生成功能，吸引了创意工作者的关注和使用。Gen-3 Alpha 在视频真实性和创作方式上有显著提升，用户可以通过简单的提示词和修饰词来生成具有高质感的视频内容。尽管存在一些 bug，但整体效果仍然令人满意。对于普通爱好者来说，Gen-3 Alpha 的收费政策可能会成为一定的阻碍。 </div>
                        <hr>
                    
                    <p></p><p>7 月 2 日凌晨，著名生成式 AI 平台 Runway 在官网宣布，其文生视频模型 Gen-3 Alpha 向所有用户开放使用。而就在上周，Runway 才宣布 Gen-3 Alpha 向部分用户开启测试，短短几天内便全面开放，其速度之快令人惊喜。用户只需要登录 Runway 官网，点击“Get Started”就能够开启体验了。</p><p></p><p>与上个版本的 Gen-2 相比，Gen-3 Alpha 具有更加强大的功能：</p><p></p><p>精细动作控制：能够精确控制视频中对象的动作和过渡，实现复杂场景的流畅动画。逼真人物生成：能够生成具有自然动作、表情和情感的逼真人类角色。多模态输入：支持文字转视频、图像转视频、文字转图像等多种创作方式。先进工具：支持运动画笔、相机控制和导演模式等专业创作工具。</p><p></p><p>Gen-3 在图像的真实性、场景的连贯性以及动态表现上都实现了显著的飞跃，进一步推动了构建一个全面的通用世界模型（General World Models，简称 GWMs）的进程。</p><p></p><p>根据官方的说明，生成一个视频需要以下几个步骤：</p><p></p><p>用户首先需要输入一个简单的提示词，如“瀑布”，然后添加修饰词语来影响视频的风格、构图和整体情绪；制作文本提示后，选择视频的时长（最长 10 秒），然后点击“生成”；生成视频后，用户可尝试用固定的种子编号来获得一致的样式，或者调整文本提示，产生不同的结果。（当提示词遵循清晰的结构，划分为“场景”、“主体”、“相机移动方式”时，提示最有效。）</p><p></p><p>网友们用 Gen-3 制作的视频，无论是美食介绍、微电影宣传，还是人与自然的创意短片，每一个画面都充满了饱和度、光影效果、动作一致性和连贯性。这得益于 Gen-3 的物理模拟功能，它能够让生成的内容严格遵守现实世界的特点。有网友表示，Gen-3 生成速度非常快，10 秒的视频大概只用了一分半就能跑出来，比十几分钟才能生成的 Luma 体验感好多了。</p><p></p><p>效果演示：</p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p>不过，也有网友实测发现，虽然 Gen-3 功能强大，但其生成的视频有些还是存在明显 bug。以写实风格为例，人物特写和风景最稳，但是一旦涉及到全景或者中景，当人物没有足够的面积空间时，肢体变形就极为严重。但总体来说，视频的氛围和质感还是很到位的。</p><p></p><p>对于 Runway 如此迅速地开放 Gen-3 使用权限，网友们纷纷表示兴奋，甚至有人认为它已经超越了 Sora。毕竟，Sora 从首次展示到现在已经有 4 个多月了，还在邀请测试阶段，而 Gen-3 的全面开放，无疑是给创意工作者们的一剂强心针。</p><p></p><p>Runway 的创意总监也表示：“Runway 创造了历史，将再次改变文生视频赛道。”</p><p></p><p>不过，比较遗憾的是，这次 Gen-3 并没有像前两代和 Luma 那样免费提供试用，大概是因为算力的问题限流，每个月最少 12 美元才能使用。对此，有网友表示，虽然 RunwayGen-3 实力很强，但依然不得不承认，对于普通爱好者来说，完全免费的可灵更加具有吸引力。</p><p></p><p>参考链接：</p><p></p><p><a href="https://runwayml.com/blog/introducing-gen-3-alpha/">https://runwayml.com/blog/introducing-gen-3-alpha/</a>"</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ZytQH3mqoCwJz8tK3Jtt</id>
            <title>AI Infra 现状：一边追求 10 万卡 GPU 集群，一边用网络榨取算力</title>
            <link>https://www.infoq.cn/article/ZytQH3mqoCwJz8tK3Jtt</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ZytQH3mqoCwJz8tK3Jtt</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jul 2024 07:21:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 云行业, AI时代, 星脉网络, 大模型
<br>
<br>
总结: 云计算行业正迎来AI时代，头部企业纷纷投入解决算力和互联问题。腾讯宣布升级星脉高性能计算网络，支持超10万卡大规模组网，探讨改革算力互联方式。AI大模型训练需大规模GPU计算，网络需提升带宽和处理能力。网络通信效率成为集群算力瓶颈，需要技术创新应对。英伟达的InfiniBand在AI训练网络领域占主导地位。 </div>
                        <hr>
                    
                    <p></p><p>云行业进入了生成式 AI 时代，除模型算法外，头部企业纷纷将大量精力投入到解决算力和互联问题上。然而，如果没有网络支持，计算的篇章就无法开启。</p><p></p><p>7 月 1 日，腾讯宣布其自研星脉高性能计算网络全面升级，升级后的星脉 2.0 支持超 10 万卡大规模组网。借此机会，InfoQ 独家专访了腾讯云副总裁兼腾讯云网络总经理王亚晨，探讨了腾讯在改革算力互联方式方面的思考。</p><p></p><p></p><h2>将整个数据中心变成一个“大芯片”？</h2><p></p><p></p><p>前几天，百年风投机构 BVP 发布了一份云计算现状报告，副标题直接使用了这样一句话：“传统云已死，AI 云长存！（The Legacy Cloud is dead , &nbsp;long live AI Cloud!）”他们承认传统云仍然有重大发展机遇，但更震惊于 AI 带来技术变革加速，现如今我们已经很难找到一家不做 AI 的云计算企业了。该报告特别指出，“这是一场关键的‘地盘争夺战’，决定了未来几年哪些大型科技公司将在云和计算市场占据主导地位。”</p><p></p><p>AI 大模型靠的是大力出奇迹，注定了训练它的基础设施跟传统云不一样。</p><p></p><p>由于 AI 的大流行，数据中心也开始从以 CPU 计算为中心到以 GPU 计算为中心。在 CPU 环境中，大规模并行计算的任务可以被分割得很零散，以微信为例，虽然它也是一个庞大的业务，但它的任务是零散且琐碎的。每个用户和每个进程的任务都是不同的，因此可以将任务分散处理。然而，大模型不同，它依赖于强大的计算能力，通常使用 GPU 通过不同的模型或通信方式来处理同一个任务。大模型很难将任务分割得如此零散，希望开发下一代基础模型的企业就不得不投入越来越大的集群来对应挑战。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b3/b3bc8179da089fa9958dc6d6698811c6.png" /></p><p></p><p>集群规模不断上涨，从千卡到万卡，再到十万卡，据王亚晨的描述，“去年大家都在谈论实现万卡集群，只在理论上讲如何实现十万卡。今年的情况有所不同，现在大家实际上已经在实践十万卡集群了。”</p><p></p><p>投入数以万计的 GPU，再通过网络将它们“粘合”起来，导致服务器的带宽接入比以前的服务器大了几十倍，网络设计也需要对应带宽的变化。以往云厂家的主流服务器通常以 100GB 带宽接入，而运营商的接入带宽可能更低。然而，两年前刚推出的 GPU 服务器带宽就达到了 800G 或 1.6T，甚至现在已经达到 3.2T。</p><p></p><p>大模型的训练和推理使得 GPU 卡之间的数据交换量非常大，因此要求数据中心的网络还要具备强大的处理能力。CPU 时代，通常情况下网络带宽利用率在 30% 到 40% 左右，不会让网络跑满，因为需要应对流量突发情况，比如春节或其他用户高峰情况。而当我们将 GPU 服务器做成一个很大的集群后，不再像以前那样以虚拟化单点运算为主，而是大量 GPU 服务器来共同处理一个任务。那么，对于 GPU 来说，由于当前的 AI 业务模型相对单一，尤其在大规模训练时，带宽利用率需要达到 90% 甚至更高，将带宽尽量撑满，GPU 一直忙着才能让训练效率更高。所以需要在硬件和网络协议各方面做出改变。</p><p></p><p>这些资源投入、物理设施和相关技术的巨大变化，使得大多数企业无法参与到竞争中来，王亚晨表示，“不是所有厂家都有能力卷大资源模型。”</p><p></p><p>由此可见，科技界并没有换人掌舵，反而成为云计算老将们的新战场。</p><p></p><p></p><h2>集群算力瓶颈：“网络迭代速度没有算力增长速度快”</h2><p></p><p></p><p>OpenAI 的 Jared Kaplan 在 2020 年首次提出了 Scaling Law，他指出模型大小和计算之间存在缩放关系。不少追随者认为，加以更多 GPU，投入更多数据，就能得到更好的智能。大量的计算意味着需要更大的计算集群，但实践中大家发现这并不简单。</p><p></p><p>第一个瓶颈是能耗，建设 10 万卡 GPU 集群大概需要 120 兆瓦甚至更多电力功耗。3.2 万卡曾被视为数据中心 GPU 数量的上限，一个说法是这是因为电网无法跟上 AI 发展带来的能源需求激增。</p><p></p><p>另外一个瓶颈是行业里运营手段需要提升。当你利用数万张 GPU，连续几十天不停地运行同一个任务时，可靠性和稳定性就成了重中之重。GPU 整个规模上去之后，GPU 故障率是逐渐上升的。</p><p></p><p>更重要的是，网络通信效率亟待提升。网络丢包、拥塞、时延都会导致集群利用率下降，有数据表明，1% 的丢包，GPU 利用率会下降 50%。所以，就算物理上建起来了一个 4 万、5 万、10 万的集群，但是真正能够带多大规模任务跑起来也需要逐步摸索和提升。怎么能够减少故障率，快速发现故障的同时能够让它快速恢复，让训练中断时间越短越好，这是确保大规模训练任务顺利进行的关键。</p><p></p><p>之前 Meta 也有过一个统计，在 AI 训练中网络通信时长占比平均占据了 35% 的时间（最高时 57%），一个直观的解释是：这等于花费数百万或数十亿美元购买的 GPU 有 35% 的时间是无所事事的。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/05/05f6ccb5e4d9ceff8ea821be4de90261.png" /></p><p></p><p>这些年来，GPU 的迭代速度非常快，算力增长迅速。“网络迭代速度没有算力增长速度快，如何在网络速度相对滞后于 GPU 算力发展的情况下，确保 GPU 性能不降低，或者至少保持较强的发展势头，成为未来云基础设施在组网层面面临的一个重大挑战。”</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/6c/6c9566c98f557e88cc6533a526bed8a5.png" /></p><p></p><p>为了能够把集群里 GPU 的性能发挥极致，腾讯这两年在网络里面，网络协议、网络软件、端网协同等各方面做了很多技术创新。</p><p></p><p></p><h2>十万卡集群的网络技术壁垒，自研高性能网络</h2><p></p><p></p><p>英伟达的网络连接主要有两种，实现卡间互联的 NVLinks，实现服务器间互联的 Infiniband。</p><p></p><p>InfiniBand 在 AI 训练低延迟网络领域拥有霸主地位，基于英伟达自己的一套协议，配合 GPU 运算特点自成一套体系。</p><p></p><p>虽然从以太网技术本身来讲，想超过 Infiniband 很难，但 infiniband 体系封闭，成本高昂。</p><p></p><p>早在两年前，腾讯就着手自研高性能网络。在大模型兴起之前，腾讯在广告场景中通过软件优化进行 AI 训练和推理时发现，以太网的性能可以达到与 Infiniband 相当的水平。</p><p></p><p>另外，InfiniBand 成本也比以太网技术高很多。在 HPC 和超大规模 AI 云市场中，网络占集群成本的 20% 或更多的情况并不少见。外媒 Nextplatform 根据英伟达的数据算了一笔账，如果你有 10 亿美元，那差不多需要分配 4 亿美元购买 16,000 个 H100 GPU ，还要再花 1 亿美元购买 Nvidia 的 InfiniBand 网络将它们全部连接在一起，剩下的 5 亿美元用于建设数据中心，并在四年内运营、供电和冷却。相比之下，用以太网来建设的成本基本不会超过以上金额的 10%。</p><p></p><p>“基于这几个因素，我们才敢在大模型出现时，选择以太网，并通过自研的方式来解决网络问题。”</p><p></p><p>如何用以太网技术解决拥塞问题，尤其是在拥塞时不丢包，这是星脉团队首先要解决的问题。</p><p></p><p>早期业界没有其他标杆，只能参照英伟达的 Benchmark。以此为基准，腾讯将星脉 1.0 在网络指标上提升至与 Infiniband 相同的水平，并努力做到更优。</p><p></p><p>Benchmark 里面有几个关键数据，第一个是训练过程中的通信时长占比，7%、8% 是目前业界较为领先的水平。而星脉团队将星脉的通信时长占比做到了 6%，这实际远低于 10% 的业界水平。</p><p></p><p>另一个很关键的是网络负载率，星脉优化到 90%，与 IB 网络（Infiniband）持平，相较于标准以太网提升 60%。</p><p></p><p>除了组网技术，更大的壁垒则转向了端网协同能力和运营能力上。这些壁垒，在自研以太网基础上，显然更灵活更容易实现。</p><p></p><p>星脉本身有一套自研协议。通过高性能通信库 TCCL，星脉能看到网络拓扑，能知道什么路径最短。路径最短，拥塞也会变少，丢包概率也会降低。通过自研端云协同协议 TiTa，星脉可以在网络拥塞的时候，将流量做调度，不会产生丢包，也能让网络负载跑得更均匀。</p><p></p><p>以前是依靠软件库与网络的配合，星脉进一步的在网卡层面与整个网络形成一个闭环的控制能力，这样可以实现更好的拥塞控制算法。</p><p></p><p>而快速定位和解决问题的运营能力，也能够在基础设施层面形成另一个非常强的差异化。星脉可以快速感知网络质量，定位因网络问题导致的训练中断等问题，故障时间在整个训练时间中的占比已经降到了一个相对较低的水平。</p><p></p><p>如今，这一决策被证明是正确的。英伟达最近也推出了自家的以太网解决方案，搭配网卡使用，其思路与腾讯的星脉 2.0 不谋而合。</p><p></p><p>行业里实际也已经有了不少使用以太网的企业，比如 Meta 的训练 Llama 3 的集群，一半使用的是 Infiniband，一半是以太网，并且他们宣称以太网集群的性能不比 Infiniband 差。</p><p></p><p>国内腾讯和阿里则都是纯以太网。这些企业也都加入了 Linux 基金会发起的超级以太网联盟 UEC（Ultra Ethernet Consortium），到今年 3 月总共有 55 家公司参与，共同为 AI 发展构建完整的基于以太网的通信堆栈架构。</p><p></p><p></p><h2>从星脉 1.0 到星脉 2.0 的进阶：在工程上支持 10 万卡</h2><p></p><p></p><p>腾讯最早于 2022 年就开始做星脉研发，当时主要是用于广告大模型训练。这个时间点比 OpenAI 推出 chatGPT 还要早上半年。也正是因为有了技术储备，所以能在初期快速构建起星脉 1.0，并将带宽利用率做到 90%，做到无丢包，保证算力不损失，另外还达到了极低时延的要求。</p><p></p><p>在这个背景下，星脉 1.0 实现了单个服务器 3.2T 的接入带宽，业界第一次提出多轨道大规模组网，让集群组网规模更大。同时打造了初步的运营系统平台，主要解决了应用系统中的网络上监控和故障修复问题。</p><p></p><p>星脉 2.0 则希望在工程上实际支持 10 万卡，实现训练推理一体化，进一步去解决推理的成本效率问题。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/2a/2a8d9d95fe530067e736c14a1de8e988.png" /></p><p></p><p>在硬件层面，星脉 2.0 引入了自研交换机、自研光模块、自研网卡三套新的硬件。其中，网络交换芯片由 25.6T 升级到 51.2T， 这样对应的整个组网规模就会翻倍。</p><p></p><p>另一个重要方面是星脉 2.0 首次在业内采用了自研的 400G 单口硅光芯片。这一创新的最大特点在于显著降低了能耗、模块能耗以及成本。</p><p></p><p>为了解决 10 万卡集群的性能瓶颈问题，需要实现端和网的协同。因此，除了商业网卡，星脉也首次引入了自研的算力网卡，与自研的软件系统相结合，大幅提升整体性能。</p><p></p><p>拓扑架构设计层面，星脉 2.0 延续了多轨道设计，并且每个节点的容量都升级了，这样就足够支持到十万卡的集群规模的组网。同时未来也能满足 SORA 这种模型架构需要的在网计算（也叫算力卸载）能力。</p><p></p><p>在软件层面，TCCL 从路径规划变为了自适应性能加速，并打通了异构并行计算中的卡间互联网络，从而能够将 NVLinks 以及星脉两种网络在同一个任务中用起来：当机内带宽不足时，可以将外部带宽用起来，利用外部带宽弥补内部卡间互联速率的不足，同时也能够感知两种网络拓扑的使用状态，这种方式能让通信性能提升约 30%。</p><p></p><p>TiTa 在 1.0 阶段，拥塞发生后才会进行调整，而在 2.0 阶段，通过主动干预速度以避免发生拥塞。通过协议和硬件的端到端配合，可以有效地控制传输速率，使得网络从可能会产生拥塞但不会丢包，转变为根本不会产生拥塞的网络。目前来看，这种端网结合也是业界非常重要的发展方向。通过这种方式，能将通信性能再提升 30%，集群训练时长降低 10%。</p><p></p><p>另外，星脉 2.0 的运营系统也进行了升级，引入了仿真系统的概念。在训练过程中，在 GPU 训练中某个卡出了问题，或运算效率突然变慢，是经常出现的问题。尤其是变慢的这种情况下，服务器是不会没有报故障的，因为节点失速并不是故障。新的运营系统可以通过仿真模拟，再结合实际训练过程中产生的日志进行对比，就能知道到底这次训练中哪些 GPU 它到底是失速了，还是有故障节点了，然后快速找出这些节点，进行干预。在实践中，运营系统的升级能将训练问题定位时长从数小时缩短到 10 分钟内。</p><p></p><p>如今星脉在整个系统的层面上也形成了自己的独特优势，包括 GPU 拓扑感知能力、网络仿真系统能快速定位慢失速节点的能力。</p><p></p><p>现在这个技术体系不仅能十万卡集群的真正跑起来，还能做到更精细化运营，整体网络通信效率比上一代提升 60%，让大模型训练效率提升 20%。这意味着，如果原来训练中某个计算结果的同步需要花 100 秒完成，现在只需要 40 秒；原来需要花 50 天训练的模型，只需要花 40 天。</p><p></p><p></p><h2>实现“算力供需平衡”的愿景</h2><p></p><p></p><p>星脉网络作为底层技术支撑了腾讯混元大模型训练。今年，混元大模型的参数规模更是突破了万亿级别，而企业微信、腾讯会议及腾讯文档等都部署了生成式 AI 功能。过程中遇到各种问题，比如训练中断，星脉网络都能凭借强大的技术和稳定的性能，轻松应对。</p><p></p><p>现在，基础大模型还在卷，还在发展，GPT5 也将很快发布。各种应用也开始出现，这些都需要大量算力。大家希望未来算力要像电力一样无处不在，但现在算力短缺是整个人工智能行业面临的一道难题。</p><p></p><p>为应对算力紧缺，OpenAI 今年还出台计划，打算耗资 1150 亿美元，打造星际之门（Stargate）来支持大模型的发展。只是，除了不断扩张数据中心数量和规模之外，我们也应该有足够的技术去“榨取”已有 GPU 资源中的算力。</p><p></p><p>“我觉得未来算力供需要达到相对变化的平衡，很重要一点是能够提升 GPU 算力调度和利用率来缓解相应压力。我们也在讲算力网络，算力网络本身来讲就想让我们的算力调度能力以及算力利用率能够长的更好。”</p><p></p><p>“我们一直有一个愿景，希望算力网络能为大家提供服务，让大家‘用得更快，用得更好，用得更稳’。用得更快指的是算力调度、建设交付和供应响应更快，让大家能够第一时间获取所需资源。用得更好则是指性能更佳，体现在 GPU 利用率、网络各种指标和负载率等方面，性能达到最佳。用得更稳是指运营质量高，不出问题，或在出问题时能够快速定位和恢复，让运营更稳定。”</p><p></p><p>今日好文推荐</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651201822&amp;idx=1&amp;sn=3426e28e7320c75c51cbcd4e3a032c58&amp;chksm=bdbbd94d8acc505b83b755476510dd7fd210cbb898b1ea0138942cd52ff0c2a605a4c3744150&amp;scene=21#wechat_redirect">德国再次拥抱Linux：数万系统从windows迁出，能否避开二十年前的“坑”？</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651210738&amp;idx=1&amp;sn=eace455941268e51ecc73bd882c13caf&amp;chksm=bdbbbba18acc32b7a354031fd9dc9aac0156043d9e22b77c484d07790a9bc6cc00a7c058f775&amp;scene=21#wechat_redirect">英伟达老员工集体“躺平”，在印钞机上数钱的快乐谁懂？</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651210650&amp;idx=1&amp;sn=09fe1190862f0e8104cb351bf2d26e7f&amp;chksm=bdbbbbc98acc32dfa351f7b9264570533571cafbb471a173c67bbf56dd3a1fa839db7a0b2d60&amp;scene=21#wechat_redirect">哈佛退学本科生开发史上最快芯片；居然之家汪林朋：AI时代名校毕业生不如厨师司机，北大的到我那就八千元；英伟达高层频频套现｜Q资讯</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651210364&amp;idx=1&amp;sn=c386ad171334259eee6136ecd77101f7&amp;chksm=bdbbba2f8acc33397ba6928e052102e21727361b6d2688eb77ff7f1817d2993958fc6ba75177&amp;scene=21#wechat_redirect">被全球最大用户弃用！曾经的数据库霸主 HBase 正在消亡</a>"</p><p></p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/P1N2B208sNrShbffLqNW</id>
            <title>微软130亿美元换的OpenAI 董事席，苹果仅靠“刷脸”就拿下了！硅谷明星创企积极投靠大厂</title>
            <link>https://www.infoq.cn/article/P1N2B208sNrShbffLqNW</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/P1N2B208sNrShbffLqNW</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jul 2024 06:48:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI创企, 苹果, 微软, 合作伙伴关系
<br>
<br>
总结: 国外AI创企面临压力，苹果和微软加入OpenAI董事会，展开合作伙伴关系，微软投资OpenAI并分享利润，大厂竞购AI创企如Character.AI，合作协议涉及知识产权共享和研发能力提升。 </div>
                        <hr>
                    
                    <p>作者&nbsp;|&nbsp;华卫</p><p>&nbsp;</p><p>现在，国外那些AI创企似乎面临的压力越来越大，并在自我独立发展上开始呈现颓态。OpenAI的董事会里现在“入驻”着苹果和微软的核心高管，Character.AI也计划卖给谷歌和Meta。</p><p>&nbsp;</p><p>而与此同时，科技巨头们也在积极接洽AI创企对外投来的“橄榄枝”。由于他们正相互竞争开发尖端技术，寻求与顶级人工智能初创企业建立合作伙伴关系和投资便不失为一条好路子。</p><p>&nbsp;</p><p></p><h1>OpenAI&nbsp;董事会“失守”</h1><p></p><p>&nbsp;</p><p>今早，据外媒报道，苹果已安排&nbsp;App&nbsp;Store&nbsp;首席执行官兼前营销主管&nbsp;Phil&nbsp;Schiller&nbsp;代表其参加&nbsp;OpenAI&nbsp;的非营利性董事会。据悉，Schiller&nbsp;将获得观察员的角色，这意味着他可以参加董事会会议，但不能投票或行使其他董事权力。</p><p>&nbsp;</p><p>然而，加入董事会将使Schiller&nbsp;能够更多地了解&nbsp;OpenAI&nbsp;的内部运作，以及该公司是如何做出决策的。更重要的是，董事会观察员的这一角色，将使苹果与OpenAI最大的支持者和主要的人工智能技术提供商微软在地位上相提并论。</p><p>&nbsp;</p><p>据报道，去年微软也以无投票权的观察员身份加入了能够控制OpenAI的董事会。显然，让苹果和微软同时加入OpenAI的董事会，可能会使OpenAI与任何一家合作公司的讨论计划都变得更加复杂。未来，他们将如何在OpenAI董事会中共存也是一个新问题。</p><p>&nbsp;</p><p>目前，苹果与OpenAI的此项合作并未涉及到任何双方的资金交易。不过，苹果有望从通过其平台订阅的&nbsp;ChatGPT&nbsp;中获得一定比例的收益。</p><p>&nbsp;</p><p>现在苹果公司正致力于在今年晚些时候将&nbsp;ChatGPT&nbsp;整合到&nbsp;iOS&nbsp;和&nbsp;macOS&nbsp;中，如果用户同意，整合后的&nbsp;Siri&nbsp;将可以向&nbsp;ChatGPT&nbsp;发送更高级的查询。而苹果认为，对OpenAI来说，&nbsp;iOS&nbsp;中&nbsp;ChatGPT&nbsp;的曝光比现金“具有等值或更大的价值”。毕竟，这笔交易将使OpenAI能够接触到数亿用户。</p><p>&nbsp;</p><p>但微软的情况又与苹果不同，该公司可是实打实给OpenAI做了资金投入的。</p><p>&nbsp;</p><p>作为战略合作伙伴关系的一部分，微软已向OpenAI投资了约130亿美元，该合作伙伴关系允许ChatGPT制造商使用微软的海量计算和云资源，同时保持独立业务。而根据交易条款，微软有权获得OpenAI利润的一半左右，直到投资得到偿还。</p><p>&nbsp;</p><p>此外，值得注意的是，此前苹果高管很少在与他们合作的公司中占据董事会席位。这次，苹果在OpenAI的安排将于今年晚些时候生效，双方的合作细节也仍在不断变化，现在Schiller&nbsp;尚未参加OpenAI董事会的任何会议。</p><p>&nbsp;</p><p>据了解，Schiller&nbsp;自1997年以来一直担任苹果App&nbsp;Store负责人、执行团队成员。在&nbsp;2020&nbsp;年转任&nbsp;Apple&nbsp;Fellow&nbsp;之前，他曾担任&nbsp;Apple&nbsp;的长期营销主管。在此职位上，Schiller&nbsp;继续领导&nbsp;App&nbsp;Store&nbsp;和&nbsp;Apple&nbsp;活动，并直接向&nbsp;Apple&nbsp;首席执行官蒂姆·库克&nbsp;（Tim&nbsp;Cook）&nbsp;汇报。此前，Schiller&nbsp;还领导苹果公司为App&nbsp;Store辩护，使其免受全球反垄断指控。</p><p>&nbsp;</p><p></p><h1>大厂竞购“缺钱”的&nbsp;AI&nbsp;创企</h1><p></p><p>&nbsp;</p><p>还有一些曾经爆火的&nbsp;AI&nbsp;产品，如今也可能被更大的科技公司变相“收购”，如&nbsp;AI聊天机器人Character.AI。</p><p>&nbsp;</p><p>7月&nbsp;1&nbsp;日，据外媒报道，Character.AI已开始与谷歌和埃隆·马斯克&nbsp;（Elon&nbsp;Musk）&nbsp;的xAI公司、Meta等竞争对手初步讨论了潜在合作机会。这些合作协议可能包括Character.AI利用合作伙伴的计算资源提升研发能力，作为交换，Character.AI将提供一定程度的知识产权共享。</p><p>&nbsp;</p><p>而早在今年5月，就有报道称，Meta&nbsp;和&nbsp;xAI&nbsp;一直在争夺与Character.AI的合作伙伴关系。当时，据四位熟悉内情的人士透露，Meta&nbsp;在与Character.AI进行的合作早期讨论中，谈到了双方顶级研究人员密切合作的问题，比如预训练和开发模型。</p><p>&nbsp;</p><p>两位知情人士说，Character.AI与&nbsp;xAI&nbsp;也就类似的合作关系进行了试探性会谈。但其中一位知情人士表示，Character.AI与他们的讨论重点是推进研究，而不是收购。</p><p>&nbsp;</p><p>据了解，大型科技集团一直对试图全面收购人工智能初创企业持谨慎态度，因为担心全球范围内的监管行动。微软与OpenAI的130亿美元合作就正在接受英国和美国竞争当局的审查，尽管这两家企业坚称他们的合作伙伴关系不是合并。</p><p>&nbsp;</p><p>公开资料显示，AI初创公司Character.AI由两位前谷歌AI技术大佬于2021年11月创立，从安德森·霍洛维茨（Andreessen&nbsp;Horowitz）等风险投资公司筹集了超过1.5亿美元的资金，用于创建包含动漫角色、游戏角色等的人工智能聊天机器人，吸引了数百万用户的关注。</p><p>&nbsp;</p><p>Character.AI的创始人之一、前谷歌研究员&nbsp;Noam&nbsp;Shazeer&nbsp;是&nbsp;2017&nbsp;年一篇论文的作者之一，该论文提出了&nbsp;transformer&nbsp;模型，目前该模型支撑着当今最好的&nbsp;AI&nbsp;模型。</p><p>&nbsp;</p><p>据一位了解&nbsp;Shazeer&nbsp;的人称，&nbsp;Shazeer&nbsp;专注于构建&nbsp;AGI，并为此寻找更多资源。“Character.AI&nbsp;还在探索与其他团体的合作。”一位熟悉该公司战略的人士说。</p><p>&nbsp;</p><p>但在筹集新资金方面，Character.AI似乎遇到了一些困难。据报道，过去一年中，该公司与包括红杉资本在内的投资者进行了多次洽谈，但有知情人士透露，公司尚未完成新一轮的风险资金募集。</p><p>当前，AI初创公司面临的竞争与发展压力似乎越来越大，不仅OpenAI&nbsp;和Character.AI在采取和寻求与科技巨头公司合作的方式，其他AI初创公司也走向了相似的命运。</p><p>&nbsp;</p><p>有爆料称，亚马逊和谷歌正在竞购Anthropic。上个月，Anthropic刚推出了&nbsp;Claude&nbsp;3.5&nbsp;Sonnet，被称为是该公司迄今为止最强大的视觉模型，在标准视觉基准上超过了&nbsp;Claude&nbsp;3&nbsp;Opus。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d1dc9a95ef2e207652d5ea0898691609.jpeg" /></p><p></p><p>&nbsp;</p><p>据报道，此前，亚马逊和谷歌这两家巨头都分别向Anthropic大额注资。今年3月，亚马逊宣布已向&nbsp;Anthropic&nbsp;投资高达&nbsp;40&nbsp;亿美元以获得该公司少数股权地位的消息。去年10月，谷歌同意向Anthropic投资高达20亿美元，涉及5亿美元的前期投资和15亿美元的额外投资。</p><p>&nbsp;</p><p>去年年底，Anthropic曾表示，预计到2024年底其将产生超过8.5亿美元的年收入。而一些接近该公司的人士认为，Anthropic2024的年收入可能达到10亿美元，即每月8300万美元的收入。</p><p>&nbsp;</p><p>目前，Anthropic暂没有披露其最新营收与整体财务状况，但近期该公司在公司的财务战略和运营领导上“换帅”。并且，从其最新发布的业务计划来看，Anthropic似乎确实难以独立为之了。</p><p>&nbsp;</p><p>今年5月，曾担任&nbsp;Airbnb&nbsp;企业和业务发展全球主管、帮助该公司度过疫情时期并筹集超过&nbsp;100&nbsp;亿美元的股权和债务资本的Krishna&nbsp;Rao，接任了&nbsp;Anthropic&nbsp;的首席财务官。当时，Anthropic联合创始人兼总裁Daniela&nbsp;Amodei表示：“希望Rao帮助指导Anthropic进入下一阶段的增长。”</p><p>&nbsp;</p><p>7月2日，Anthropic宣布启动一项“为开发评估AI模型性能的第三方新型基准测试提供资金”的计划。该公司表示，它已为该计划聘请了一名全职协调员，并可能购买或扩大它认为有潜力扩大规模的项目。</p><p>&nbsp;</p><p>Anthropic&nbsp;支持新人工智能基准的努力值得称赞，但前提是背后有足够的资金和人力支持。但考虑到该公司在人工智能竞赛中的商业野心，要完全相信它可能很难。</p><p></p><h1>结语</h1><p></p><p>对于这些AI创企当前呈现出的发展颓态，某AI领域知名专家在接受AI前线采访时表示，“这是因为许多AI创企一直没有找到好的商业模式。生成式AI最近几年的宣传比较多，但现在估值撑不下去了，之后可能还会出现不少受此影响的企业。”</p><p></p><p>谈及整个&nbsp;AI&nbsp;创业群体，该人士直言：“OpenAI是八二定律中的80%甚至98%，其他企业都是陪跑的。”</p><p></p><p>而在&nbsp;Engineer/Investor张俊伟博士看来，AI&nbsp;创企纷纷投靠大厂似乎也不是件坏事。他表示，&nbsp;像目前&nbsp;Character.AI&nbsp;针对小众圈子做的内容，由于没有产生一个正向的社会生产力价值，无法支撑未来的长期变现；如果能被&nbsp;Meta&nbsp;买了，有望获得新的生产力。对OpenAI&nbsp;而言，手机长期是&nbsp;AI&nbsp;在&nbsp;C端的直接稳定触达点，&nbsp;苹果在这方面有非常强的溢价能力；至于苹果入主OpenAI董事会，可能是因为大模型做好终端性能的情况下，需要手搓大量算子优化的代码，如果苹果不进董事会，大家缺乏深层次的信任，也就没办法互相开放。</p><p></p><p>另外，张俊伟称，“Character.AI&nbsp;在&nbsp;C&nbsp;端遇到的问题不必太吃惊，因为国内做C端才是最强的，是我们卷出了TikTok，实际上是他们在抄我们。Character.AI本身做了一些创新，也有自己的模型，如果都艰难到这个地步，那也意味着中国“套壳”公司就是会死掉。虽然有人能薅到一些VC的钱，但这肯定不会长久。”</p><p></p><p>并且，张俊伟指出，国内的公司如果因此而死掉，要么是想赚快钱，没有遵循商业规则，要么是&nbsp;AI&nbsp;太快了，没时间去调整业务链了。</p><p></p><p>参考链接：</p><p><a href="https://www.theverge.com/2024/7/2/24191105/apple-phil-schiller-join-openai-board">https://www.theverge.com/2024/7/2/24191105/apple-phil-schiller-join-openai-board</a>"</p><p><a href="https://www.ft.com/content/3414cd0d-09e0-4246-a7db-4ef3032af8b8">https://www.ft.com/content/3414cd0d-09e0-4246-a7db-4ef3032af8b8</a>"</p><p><a href="https://seekingalpha.com/news/4121137-characterai-held-talks-with-google-meta-xai-about-tie-ups-report">https://seekingalpha.com/news/4121137-characterai-held-talks-with-google-meta-xai-about-tie-ups-report</a>"</p><p><a href="https://www.ft.com/content/5cf24fdd-30ed-44ec-afe3-aefa6f4ad90e?trk=public_post_comment-text">https://www.ft.com/content/5cf24fdd-30ed-44ec-afe3-aefa6f4ad90e?trk=public_post_comment-text</a>"</p><p><a href="https://techcrunch.com/2024/07/01/anthropic-looks-to-fund-a-new-more-comprehensive-generation-of-ai-benchmarks/">https://techcrunch.com/2024/07/01/anthropic-looks-to-fund-a-new-more-comprehensive-generation-of-ai-benchmarks/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/haTaSEkmqp5pEaiAYi6X</id>
            <title>动态图结构熵的高效增量计算</title>
            <link>https://www.infoq.cn/article/haTaSEkmqp5pEaiAYi6X</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/haTaSEkmqp5pEaiAYi6X</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jul 2024 03:14:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 结构熵, 动态图, 增量算法, 社区划分
<br>
<br>
总结: 本文介绍了一种新的增量度量框架 - Incre-2dSE，用于动态图的结构熵计算和社区划分更新。作者提出了朴素调整策略和节点偏移策略来解决传统方法的时间消耗和复杂度问题，同时设计了增量框架Incre-2dSE来有效度量更新后的二维结构熵。该算法在人工和现实数据集上进行了实验，证明了其有效性和可解释性。 </div>
                        <hr>
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/95/950a16925089cc7e416010ca91867c47.png" /></p><p></p><p></p><blockquote>本文介绍来自北京航空航天大学彭浩老师团队发表在 The journal of Artificial Intelligence (AIJ) 2024上的一篇文章“Incremental Measurement of Structural Entropy for Dynamic Graphs”。为了解决当前方法不支持动态编码树更新和增量结构熵计算的问题，作者提出一种新的增量度量框架 - Incre-2dSE，它可以动态调整社区划分，支持更新后二维结构熵的实时度量。作者在人工和现实世界的数据集上进行了广泛的实验，实验结果证明，该增量算法有效地捕捉了社区的动态演化，减少了时间消耗，并具有良好的可解释性。<blockquote>论文名称：Incremental Measurement of Structural Entropy for Dynamic Graphs论文链接：<a href="https://doi.org/10.48550/arXiv.2207.12653">https://doi.org/10.48550/arXiv.2207.12653</a>"代码链接：<a href="https://github.com/SELGroup/IncreSE">https://github.com/SELGroup/IncreSE</a>"</blockquote></blockquote><p></p><p></p><h1>引言</h1><p></p><p></p><p>近年来，有学者提出一种基于编码树的图结构信息度量，即结构熵，用于发现图中嵌入的自然层次结构。结构熵在生物数据挖掘、信息安全、图神经网络等领域得到了广泛的应用。</p><p></p><p>在动态场景中，一个图在时间序列中从初始状态演变为许多更新后的图。为了有效地度量不断变化的社区划分的质量，我们需要在任何给定时间增量地计算更新的结构熵。不幸的是，由于以下两个挑战，目前的结构熵方法不支持有效的增量计算。</p><p></p><p>挑战 1：为每个更新的图重建编码树将导致大量的时间消耗</p><p></p><p>为了解决这个问题，作者提出了两种二维编码树的动态调整策略，即朴素调整策略和节点偏移策略。前者保持原有的社区划分，支持理论结构熵分析；后者基于结构熵最小化原则，通过在社区之间移动节点，动态调整社区划分。</p><p></p><p>挑战 2：传统定义的结构熵计算具有较高的时间复杂度</p><p></p><p>为了解决这个问题，作者设计了一个增量框架，即 Incre-2dSE，用于有效地度量更新的二维结构熵。具体而言，Incre-2dSE首先利用两种动态调整策略生成调整量，即重要统计量从原始图到更新图的变化，然后利用调整量通过新设计的增量公式计算更新后的结构熵。此外，作者还将增量方法推广到无向加权图，并对有向加权图的一维结构熵的计算进行了详细的讨论。</p><p></p><h1>方法</h1><p></p><p><img src="https://static001.geekbang.org/infoq/2a/2a93cd474424467b7e2ac9f2283e94eb.png" /></p><p></p><p>图 1 Incre-2dSE与传统离线算法的示意图</p><p></p><h2>二维编码树的动态调整策略</h2><p></p><p></p><h3>朴素调整策略</h3><p></p><p></p><p>朴素调整策略包括两部分：边策略和点策略。边策略规定增量边不会改变编码树的结构；点策略规定，当一个新节点  与已有节点  连接时，且  对应二维编码树中的叶节点 ，即  时，将设置一个标签为  的新叶节点  作为  父节点的直接后继节点，而不是另一个1高度的节点。我们可以从社区的角度来描述编码树的修改。具体来说，增量边不改变现有节点的社区，而新节点被分配到其邻居的社区，而不是另一个任意社区。显然，给定大小为  的增量序列，我们可以在时间复杂度为  的情况下得到更新后的编码树，即更新后的社区划分。</p><p></p><p>在这一部分中，作者引入了全局不变量和局部变化量两个量，通过朴素调整策略实现了更新结构熵的逼近和快速增量计算。对图  施加大小为  的增量序列  ，采用朴素调整策略得到新的图  及其对应的二维编码树  ，更新后的二维结构熵可表示为：</p><p></p><p>$$H^{T'}(G')=\sum_{\alpha_i \in A}(-\frac{g'_{\alpha_i}}{2m+2n}log\frac{V'_{\alpha_i}}{2m+2n}+\sum_{v_j \in T_{\alpha_i}}-\frac{d'_j}{2m+2n}log\frac{d'_j}{V'_{\alpha_i}}) (1)$$</p><p></p><p>然而，增量大小  会影响上式中求和方程中的所有项。因此，更新和计算过程的成本至少为  ，当图变得非常大时，这个成本是巨大的。一种直观的尝试是在更新的结构熵和原始的结构熵之间作差，并尝试以  计算增量熵。然而，由于在上式的所有项中  都变为  ，因此很难通过作差推导出简洁的  增量计算公式。为了解决这个问题，作者在这里引入了全局不变量和局部变化量。作者将全局不变量定义为更新后结构熵的近似，局部变化量定义为更新后的结构熵与全局不变量之差，也可视为近似误差。总的来说，通过计算和求和全局不变量和局部变化量，可以在  内计算出更新后的二维结构熵。</p><p></p><h3>节点偏移策略</h3><p></p><p></p><p>虽然朴素调整策略可以快速获得更新后的二维编码树及其相应的结构熵，但我们仍然需要一种更有效的策略来获得具有较低结构熵的更好的社区划分。因此，作者提出了另一种新的动态调整策略，即节点偏移策略，其主要思想是迭代地将节点移动到其最优偏好社区。与朴素调整策略不同，边变化可以改变现有节点的社区，使结构熵最小化。此外，该策略支持同时增加多个边和删除现有边。因此，节点偏移策略基本克服了朴素调整策略的局限性。</p><p></p><p>首先将最优偏好社区（OPC）定义为目标节点的最佳社区，即如果目标节点进入其OPC，则总体二维结构熵与进入OPC以外的其他社区相比一定是最小的。节点偏移策略可描述为：（1）设涉及节点为增量序列中出现的所有节点；（2）对于每个涉及节点，将其移动到其OPC；（3）将涉及节点更新为与发生移动的节点连接但在不同社区的所有节点，然后重复步骤（2）。</p><p></p><h2>Incre-2dSE：增量度量框架</h2><p></p><p>图1展示了增量度量框架（包括初始化和度量两个阶段）和传统离线算法（TOA）。Incre-2dSE的目的是在给定原始图、原始编码树和增量序列的情况下，在动态调整社区划分的同时，有效地度量更新后的二维结构熵。</p><p></p><h3>阶段1：初始化</h3><p></p><p>给定图  为稀疏矩阵，其二维编码树由如下字典表示：{社区ID 1：节点列表1，社区ID 2：节点列表2，…}时，可以很容易地获取并保存结构数据，其时间复杂度为  。然后使用保存在  中的结构数据计算结构表达式。总的来说，初始化阶段需要总时间复杂度为 。</p><p></p><h3>阶段2：度量</h3><p></p><p></p><p>在这个阶段，我们首先需要生成从  到  的调整。通过提出的两种动态调整策略，作者提供了两种算法来生成调整量，即朴素调整量生成算法（NAGA）和节点偏移调整量生成算法（NSGA）（图1中的①）。两种算法的输入都是原始图的结构数据和一个增量序列，输出是一个调整。NAGA的时间复杂度为  ，因为它需要在增量序列中遍历  条边，而每条边只需要花费  。在NSGA中，我们首先需要  来初始化调整。其次，在节点移动部分，我们需要确定所有涉及节点的OPC，这需要花费  。此步骤重复  次，时间开销为  ，其中  表示第  次迭代中涉及的节点数。由于大多数情况下满足  和 ，所以NSGA的总时间复杂度为 。</p><p></p><p>得到调整值后，可以增量计算更新后的二维结构熵:</p><p></p><p></p><p></p><p>为了实现上述增量计算过程，作者还提供了基于调整的增量更新算法（AIUA）（图1中的②）。给定输入，即原始图的结构数据和结构表达式以及更新后的图的调整，我们可以增量计算更新后的二维结构熵，并在新的调整到来时有效地更新结构数据和结构表达式，为下一个AIUA过程做好准备。更新结构数据的时间复杂度为 。更新结构表达式的时间复杂度为 。计算更新后的二维结构熵的时间复杂度为 。综上，AIUA的总时间复杂度为 。</p><p></p><h2>基线：传统离线算法（TOA）</h2><p></p><p></p><p>传统离线算法（TOA）对每一个更新的图重构编码树，并通过定义计算更新后的二维结构熵。TOA由以下四个步骤组成。首先，将原始图与增量序列结合生成更新后的图（图1中的a）。其次，使用几种不同的静态社区检测算法，如Infomap、Louvain、Leiden，将图节点集划分为社区，构建二维编码树（图1中的b）。第三，对更新后的图的节点级、社区级、图级结构数据进行计数并保存（图1中的c）。更新后的结构熵通过式1计算（图1中的d）。TOA的总时间成本为  加上所选社区检测算法的成本。</p><p></p><p>作者给出了传统离线算法的伪代码，如下图所示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/76/765ef9f9a511d4fe1af2166c43e9a89b.png" /></p><p></p><p><font size="1"></font></p><center><font size="1">图 2 传统离线算法的伪代码。</font></center><p></p><p></p><h2>复杂图的扩展</h2><p></p><p></p><p>作者在文章中讨论了将此方法扩展到无向加权图或有向图的可行性。首先，作者论证了无向加权图的方法可以由无向无权图的方法自然推广。其次，分析了有向图结构熵增量计算范式与无向图结构熵增量计算范式的根本区别，提出了有向加权图一维结构熵增量计算的新方法。</p><p></p><p>无向加权图：无向加权图结构熵的增量度量方法可以直观、方便地从之前提出的无向无权图结构熵增量度量方法中扩展出来。作者首先介绍了无向加权图的二维结构熵的定义。在此基础上，更新了结构熵调整的定义，提出了新情况下结构熵计算的增量公式。</p><p></p><p>有向图：由于有向图的结构熵度量与无向图的结构熵度量有本质的不同，因此本文提出的主要方法难以转移到有向图场景中。其中关键的区别在于有向图需要转换成一个转移矩阵，并计算平稳分布。由于二维结构熵的增量计算非常复杂，在这一部分中，作者简要地提出了一种度量有向权图一维结构熵的增量方案。具体来说，首先定义了有向加权图及其非负矩阵表示。然后，引入了有向加权图的结构熵公式。最后，回顾了有向加权图一维结构熵精确或近似计算的传统方法，即特征向量计算和全局聚合，并提出了一种增量迭代逼近算法，即局部传播算法，如图3所示。</p><p></p><p>在全局聚合中，每次迭代都需要遍历所有的节点和边，这导致了很高的计算冗余。在这一部分中，作者提出了一种快速逼近更新后的一维结构熵的新方法，即局部传播。顾名思义，其关键思想是利用式（3）将局部受到增量影响的节点的信息进行传播，动态地更新平稳分布，从而获得低于全局聚合的时间复杂度。</p><p></p><p>$$\pi^{(\theta +1)}i=\sum{v_j \in N(v_i)} \pi^{(\theta)}j b{ji} (3)$$</p><p></p><p><img src="https://static001.geekbang.org/infoq/be/be250de1ed8d97909e52ba50c8cabed5.png" /></p><p></p><p>图 3 局部传播算法的示意图</p><p></p><h1>实验与评估</h1><p></p><p></p><p>作者基于动态图形实时监控和社区优化的应用进行了广泛的实验。</p><p></p><h2>数据集介绍</h2><p></p><p></p><p>人工数据集：首先，作者利用“Networkx”（一个Python库）中的随机分区图(random)、高斯随机分区图(gaussian)和随机块模型(SBM)方法生成动态图的3种不同初始状态。之后，通过Hawkes Process对每个初始状态生成增量序列和更新图。霍克斯过程通过假设历史事件可以影响当前事件的发生，对离散序列事件进行建模。</p><p></p><p><img src="https://static001.geekbang.org/infoq/86/869ccd293af78cefba9cc4dfbab610d4.png" /></p><p></p><p>图 4 人工Hawkes数据集生成过程。</p><p></p><p>真实数据集：对于现实世界的数据集，作者选择了Cit-HepPh、DBLP和Facebook进行实验。对于每个数据集，作者截取了21个连续的快照（一个初始状态和20个更新的图）。由于结构熵仅在连通图上定义，因此只保留每个快照的最大连通分量。总的来说，图5简要显示了人工数据集和真实数据集的统计数据。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3f/3fee7ab0c423b2855f529b5192383cae.png" /></p><p></p><p>图 5 人工数据集和真实数据集的统计描述</p><p></p><h2>3.2 实验结果与分析</h2><p></p><p></p><h3>应用：动态图形实时监控和社区优化</h3><p></p><p>在本应用中，我们旨在通过NAGA+AIUA和NSGA+AIUA的增量算法优化社区划分并监控相应的二维结构熵，以及基线TOA来实时量化动态图的每个快照的社区质量。具体来说，对于每个数据集，我们首先从Infomap、Louvain和Leiden中选择一种静态社区检测方法（简称静态方法）生成初始状态的社区划分。实验结果如图6（真实数据集）和图7（人工数据集）所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c6/c603f342a7d14797c95363d4ef9885b4.png" /></p><p></p><p>图 6 NAGA+AIUA、NSGA+AIUA和TOA在真实数据集上使用不同静态方法度量的更新后的结构熵。结构熵越低，性能越好</p><p></p><p><img src="https://static001.geekbang.org/infoq/c5/c597ec4ab6a8cfc709351760382a5bc4.png" /></p><p></p><p><font size="1"></font></p><center><font size="1">图 7 NAGA+AIUA、NSGA+AIUA和TOA在不同静态方法人工数据集上度量的更新结构熵。由于人工数据集的三条曲线比真实数据集的曲线更接近，因此所有显示的结构熵值都从NAGA+AIUA的结构熵值中减去，以更好地显示曲线之间的差异。</font></center><p></p><p></p><h3>超参数研究</h3><p></p><p></p><p>在这一部分中，作者评估了节点偏移策略的不同迭代次数对更新结构熵的影响。作者使用迭代次数的NSGA+AIUA分别度量前一小节中每种情况下20个更新图的平均更新结构熵。实验结果如图8所示，更新的结构熵随着迭代次数的增加而减少。这是因为，随着迭代次数的增加，更多的节点将转移到它们的OPC，这导致结构熵进一步降低。实验还表明，节点偏移策略具有良好的可解释性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/79/797f25b960a2cb26fc1d1b94324ee86a.png" /></p><p></p><p>图 8 不同迭代次数下节点偏移策略更新的结构熵。黑体数字表示最低结构熵</p><p></p><h3>时间消耗评估</h3><p></p><p></p><p>图9给出了NAGA+AIUA和NSGA+AIUA（N=3,5,7,9）这两种增量算法在所有6个数据集上的耗时比较。图中的纵轴表示所选增量算法在所有20个快照中的平均耗时。横轴表示3个选定的静态方法。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9e/9ed4e87d4c24510e75a693f05a1e7ec8.png" /></p><p></p><p>图 9 NAGA+AIUA和NSGA+AIUA （N=3,5,7,9）在不同静态方法下每个数据集超过20个时间戳上的平均耗时。</p><p></p><p>图10给出了在线算法NSGA+AIUA（N = 5）与离线算法TOA的时间对比。从结果可以看出，作者提出的所有增量算法都比现有的静态方法快得多。</p><p></p><p><img src="https://static001.geekbang.org/infoq/53/5378a329bd04c596c72a04aac9df7af5.png" /></p><p>图 10 增量算法（在线时间）与基线传统离线算法（离线时间）的耗时比较。</p><p></p><h3>Incre-2dSE与当前静态结构熵度量方法的差距</h3><p></p><p></p><p>在这一部分中，作者研究Incre-2dSE与当前静态算法之间的差距。目前主流的结构熵度量静态算法称为结构熵最小化（SEM），是一种以结构熵为目标函数的静态图 k 维编码树的贪心构造算法。作者在六个数据集上的所有时间戳上度量了Incre-2dSE（NAGA/NSGA+AIUA）和2d-SEM的结构熵，如图11所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/32/327d0bd353a05eed3e03447cacd053da.png" /></p><p></p><p>图 11 六个数据集上的时间戳度量Incre-2dSE（NAGA/NSGA+AIUA）和2d-SEM的结构熵。</p><p></p><p></p><h3>有向加权图的一维结构熵度量</h3><p></p><p></p><p>作者还评估了两种近似一维结构熵度量方法，即全局聚集和局部传播，在两个人工数据集上的时间消耗（ER数据集和Cycle数据集）。耗时实验结果如图12所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/65/659764a68e1f5668af55c1e78a96d9a7.png" /></p><p></p><p>图 12 ER和Cycle数据集上全局聚合和局部传播的时间消耗。</p><p></p><p>除以上列出的实验结果之外，作者还进行了更新阈值分析、鲁棒性分析、收敛性分析。这些分析的结果表明，①设置更新的阈值可以提高效率，并更好地适应频繁更改的图形；②本文的增量算法使结构熵保持在一个稳定和较低的水平上，对不断增加的噪声具有很高的鲁棒性；③局部差值总是小于它的上界，有力地支持了局部变化量及其一阶绝对矩的收敛性。</p><p></p><h1>结论及展望</h1><p></p><p></p><p>本文提出了两种新的动态调整策略，即朴素调整策略和节点偏移策略，以分析更新的结构熵，并逐步调整原有的社区划分，使其朝着更低的结构熵方向发展。作者还实现了一个增量框架，即支持更新的二维结构熵的实时度量。进一步，作者讨论了提出的方法在无向加权图上的推广，以及在有向加权图上的一维结构熵计算。在未来，作者的目标是开发更多的动态调整策略，用于层次化社区划分和高维结构熵的增量度量算法。</p><p></p><p>篇幅原因，我们在本文中省略了诸多细节，更多细节可以在论文中找到。感谢阅读！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/OfSAi8R4p5OKlI0sBy6Z</id>
            <title>解码RAG：智谱 RAG 技术的探索与实践 ｜ AICon</title>
            <link>https://www.infoq.cn/article/OfSAi8R4p5OKlI0sBy6Z</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/OfSAi8R4p5OKlI0sBy6Z</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jul 2024 01:40:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AICon, RAG, 智谱, 大模型技术
<br>
<br>
总结: 在AICon北京站上，智谱企业商业技术中心的总经理柴思远分享了RAG在智谱的探索与实践，介绍了RAG的三个关键步骤：Indexing、Retrieval、Generation。智谱AI长期专注于大模型技术研究，通过RAG技术解决了大模型应用中的幻觉、知识更新不及时等问题，降低了实施成本，提高了问答的精度和效率。 </div>
                        <hr>
                    
                    <p></p><blockquote>在 5 月份刚结束的<a href="https://aicon.infoq.cn/202405/beijing/">AICon </a>"北京站上，智谱智谱企业商业技术中心的总经理柴思远分享了RAG 在智谱的探索与实践，本文为演讲内容整理文章，期待给你带来启发。在 8 月 18-19 日即将举办的 AICon 上海站，我们也设置了<a href="https://aicon.infoq.cn/2024/shanghai/track/1705">【RAG 落地应用与探索】专题</a>"，本专题将深入探讨 RAG 的最新进展、成果和实践案例。我们将详细分析面向 RAG 的信息检索的创新方法，包括知识抽取、向量化、重排序、混合检索等在不同行业和场景下的微调和优化方法。目前大会 9 折购票优惠中，感兴趣的同学请锁定大会官网：<a href="https://aicon.infoq.cn/2024/shanghai/track">https://aicon.infoq.cn/2024/shanghai/track</a>"</blockquote><p></p><p></p><p>智谱 AI 长期专注于大模型技术的研究，从 23 年开始，大模型受到了各行各业的关注，智谱 AI 也深度的参与到各种场景的大模型应用的建设当中，积累了丰富的模型落地应用的实战经验，其中 RAG 类应用占据了较大的比重。</p><p></p><p>所谓 RAG，简单来说，包含三件事情。第一，Indexing。即怎么更好地把知识存起来。第二，Retrieval。即怎么在大量的知识中，找到一小部分有用的，给到模型参考。第三，Generation。即怎么结合用户的提问和检索到的知识，让模型生成有用的答案。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a0/a08ddc53fbfd6b7e503adc6897348e5f.png" /></p><p></p><p>这三个步骤虽然看似简单，但在 RAG 应用从构建到落地实施的整个过程中，涉及较多复杂的工作内容。为此，智谱 AI 组建了一支专业团队，专注于打造企业服务场景的 RAG 系统，致力于为客户提供全面的支持与服务。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/59/59aae3a64c032837358d0d3e6dfd901c.png" /></p><p></p><p>那么使用 RAG，有哪些优势呢？我们总结有以下几个方面：</p><p></p><p>1.与直接跟大模型对话的方法相比，RAG 可以更好地解决模型的幻觉、知识更新不及时等问题。</p><p></p><p>2.与传统的 FAQ 或者搜索的方式相比，RAG 可以显著降低实施成本。例如传统需要人工整理的 FAQ 的场景，今天我们只需要把手册资料交给 RAG，就能实现高效准确的问答。</p><p></p><p>3.相较于大模型直接生成内容的方式，基于 RAG 的生成可以追溯到内容的来源，知道答案具体来源于哪条知识。大模型就像是计算机的 CPU，负责计算答案；而知识库就像是计算机的硬盘，负责存储知识，这种计算和存储分离的架构，便可以对知识回答的范围进行权限管理。</p><p></p><p>4.目前大模型已具备了处理长上下文的能力，然后，如果每次问答都需要把几十万字的文档输入进去，那么会导致问答的成本成倍增加，特别是在客服场景。实际上我们只需要使用整个文档中一个很小的片段，就可以完成任务。所以在同样精度的情况下，利用 RAG 技术可以大大地降低整个成本。</p><p></p><h3>智谱&nbsp;-RAG 解决方案</h3><p></p><p></p><h4>技术方案</h4><p></p><p></p><p>下图是技术方案的全景图</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7f/7fc0827e50274a41a32a975297cb42ab.png" /></p><p></p><p>整个技术方案包括三个层面：文件上传、用户提问和答案输出。这三个层面都需要有大量的工程和策略的工作去进行打磨。</p><p></p><p>以文件上传为例。在文件解析过程中，我们需要将无关的信息（页眉页脚等）过滤掉、将图片改写成特定标识符、将表格改写成模型易于理解的 html 格式等操作。同时，我们会对目录、标题等进行识别，有效提取文档的结构信息；也会对文件中的序列信息进行识别，以确保知识的连续完整。</p><p></p><p>此外，Embedding 模型本身因为有窗口限制，文档切片过大会导致检索信息不准确。为了解决这个问题，我们采用了 small to big 的策略，即在原始文档切片基础上，扩展了更多粒度更小的文档切片。检索文档时如果检索到粒度细致的切片，会递归检索到其原始大切片，然后再将原始节点做为检索结果提交给 LLM。</p><p></p><h4>产品方案</h4><p></p><p></p><p>下面是产品方案的全景图</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/35/35e69bab8ce479d3f377d3361d0af509.png" /></p><p></p><p>在知识构建过程，我们提供了包括知识类型管理、切片管理、索引管理和数据运营等知识运营和管理的工具，以此来辅助提升企业服务场景的落地效果。</p><p></p><p>在知识问答过程，我们提供了包括历史消息、输入提示、原文索引、图文混排、原文查看等功能，以此来加强用户对模型回复答案的信任。</p><p></p><p>从产品应用层面，一般有三种常见的落地类型，分别为个人使用，企业对内赋能，企业 toC 提供服务等。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b0/b06791205f7b9438e516ed92ec1dc639.png" /></p><p></p><p></p><h3>智谱&nbsp;-RAG 在智能客服的实践</h3><p></p><p></p><p>下面我以「公共事务客服问答场景」为例，介绍我们在 RAG 上的实践。</p><p></p><p>这个场景其实大家都比较熟悉。例如 12329 公积金便民热线。针对这样的场景，原来的做法主要是两大技术内容：对话引擎（脚本编排）和文档引擎（检索系统）。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f3/f356f58d4e0ce19100cfc6d2d285d506.png" /></p><p></p><p>但这样的技术面临着几个痛点：</p><p></p><p>1.知识整理成本高。例如，公积金领域，全国各市有不同政策。启动项目时，一个城市大约需要 3,000 个 FAQ，运营过程中会增加至 6,000 个，导致高昂的维护成本。</p><p></p><p>2.知识复用性差。人力专家是能全面解答全国各地的公积金问题，然而原有的智能系统无法跨城市复用知识，缺乏模型上的通用学习能力。</p><p></p><p>3.知识更新频繁。各市每年都会有年度政策版本出台，每隔几个月还会有补充性政策，增加维护成本。4、知识晦涩难懂。虽然涉及日常场景，但政策内容复杂，不易为大众理解。</p><p></p><p>此外，在交互层面，也同样存在问题：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/0c/0c71c0255e47b4798b56e628d763d04d.png" /></p><p></p><p>1.FAQ 模式的回答范围有限，无法涵盖所有问题，容易导致用户体验下降。</p><p></p><p>2.交互方式如电话菜单或文本弹窗缺乏拟人化体验，若无法命中问题，用户会快速失去对智能客服的耐心，转而寻求人工服务。</p><p></p><p>3.传统 NLP 技术缺乏对人类对话的理解能力，智谱 ChatGLM 大模型原生的就能够理解对话的上下文。</p><p></p><p>4.旧方法只能提供固定答案，无法针对特定情况精准回答，而智谱 ChatGLM 大模型能够生成有效答案或者推理生成更有针对性的答案。</p><p></p><p>针对同样的场景问题，智谱通过“ChatGLM 大模型 +RAG”的方案来解决。整个成本和效果可以有大幅提升如，下图所示：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a3/a345304e9d88d8b74140d13a6e38453b.png" /></p><p></p><p>此项目面临如下几个技术挑战：</p><p></p><h4>Embedding</h4><p></p><p></p><p>第一个挑战是知识召回。</p><p></p><p>切片问题：传统按长度切片方法效果不佳，因为政策内容知识密度高，每句话都可能包含答案，且条款间关联性强，需要连续多个条款才能完整回答问题。Embedding 微调：通用 Embedding 模型不足以应对用户口语化严重的问题，需要针对具体业务场景进行微调，以过滤无关信息并提高准确度。</p><p></p><p>针对前者，我们采用文章结构切片以及 small to big 的索引策略可以很好地解决。针对后者，则需要对 Embedding 模型进行微调。我们有四种不同的构造数据的方案，在实践中都有不错的表现：</p><p></p><p>Query vs Original：简单高效，数据结构是直接使用用户 query 召回知识库片段；Query vs Query：便于维护，即使用用户的 query 召回 query，冷启动的时候可以利用模型自动化从对应的知识片段中抽取 query；Query vs Summary：使用 query 召回知识片段的摘要，构建摘要和知识片段之间的映射关系；F-Answer vs Original：根据用户 query 生成 fake answer 去召回知识片段。</p><p></p><p>经过微调后的 Embedding 模型在召回上会有大幅地提升。top 5 召回达到 100%，而且不同 Embedding 模型微调后的召回差异在 1 个点之内，模型的参数规模影响极小。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/67/679fe95cc1ab93b98e3329bf9ddf1a36.jpeg" /></p><p></p><p></p><h4>SFT&amp;DPO</h4><p></p><p></p><p>另外一个挑战是答案生成。在生成环节中，我们面临以下数据挑战：</p><p></p><p>数据标注难度大：业务人员虽然知道正确答案，但难以标注出满足一致性和多样性要求的模型微调数据。因此，我们需要在获取基础答案后，通过模型润色改写答案或增加 COT 的语言逻辑，以提高数据的多样性和一致性。问答种类多样：业务需要模型能够正确回答、拒答不相关问题和反问以获取完整信息。这要求我们通过构造特定的数据来训练提升模型在这些方面的能力。知识混淆度高：在问答场景中，召回精度有限，模型需要先从大量相关知识片段中找到有效答案，这个过程在政务等领域难度很大，需要通过增加噪声数据来强化模型的知识搜索能力。答案专业度高：在公共服务的客服场景，答案往往没有绝对准确性，资深的客服人员总能给出更有帮助性的答案。用户问题通常含糊，更加考验专业人员的回答能力。因此我们需要通过 DPO 方式训练模型，使模型能够在众多答案中找到最好最优的答案。为此，我们需要分别构造数据，并针对模型做 SFT 和 DPO。</p><p></p><p>在构造数据时，通常情况下，提供更多的高质量训练数据，微调效果越好。反之，如果训练数据中存在错误、瑕疵，将对微调效果产生一定的负面影响。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/76/7698f4bceab100093b79bb08c32733dc.png" /></p><p></p><p>当构造了优质的数据后，模型微调上，我们一般会采用分阶段微调，即首先用开源通用问答数据进行微调，然后用垂域问答数据微调，最后用人工标注的高质量问答数据进行微调。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/fe/fee31ac56a7d56a8c6619517cae40261.png" /></p><p></p><p>DPO 的训练目标就是让正样本概率加大，负样本概率变低。不仅教会模型什么是好的，也会告诉模型什么是差的。对于问答类场景非常有效果，从而让模型能够更好地向人类的真实需求进行对齐。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/95/95996723dfc2029731053b692620068c.png" /></p><p></p><p>通过以上的方案，我们能够将原本只有 60% 左右的正确率，提升到 90% 以上。</p><p></p><h4>评测</h4><p></p><p></p><p>评测是模型训练过程中的指南针，好的评测集可以快速的帮助我们找到优化的方向，拉齐算法和业务之间的分歧。构建评测数据集要确保遵循几个原则：</p><p></p><p>真实性：评测集要能真实的反应业务实际需求，与实际发生的业务场景一致。例如评测问题应该尽量覆盖用户平时会问的问题，保持用户平时对问题的表述风格。多样性：评测集要能够覆盖不同的业务内容，包括：不同的用户输入类型、期待的输出类型、以及答案生成的逻辑等。等比例：评测集各种类型数据的分布比例应与实际业务场景接近，如果已有线上数据的可以根据线上数据抽样。难度区分：生成式模型模拟人脑的思路来推断答案，题目的难度是一个非常重要的维度。业务人员往往很难系统的梳理这些难度，所以我们的算法同学需要主动的引导，构造出覆盖不同难度问题的评测集。</p><p></p><h3>结尾</h3><p></p><p></p><p>展望未来，RAG 技术将会在更多领域得到应用，并与其它 AI 技术相结合，例如多模态交互、个性化推荐、用户长期记忆等。智谱 AI 将继续致力于 RAG 技术的探索与实践，为企业在更多的领域落地大模型应用，提供更加智能、高效的服务体验。</p><p></p><p>嘉宾介绍</p><p></p><p>柴思远，智谱企业商业技术中心的总经理，大数据算法技术专家，组建智谱解决方案团队，支持过美团、360、金山、小米等重点大模型项目落地；曾历任大搜车数据中台负责人、妙计旅行联合创始人、搜狗搜索 NLP 研究员等。</p><p></p><p>活动推荐：</p><p></p><p>InfoQ 将于 8 月 18 日至 19 日在<a href="https://aicon.infoq.cn/202408/shanghai/">上海举办 AICon 全球人工智能开发</a>"与应用大会，汇聚顶尖企业专家，深入端侧AI、大模型训练、安全实践、RAG应用、多模态创新等前沿话题。现在大会已开始正式报名，7 月 31 日前可以享受 9 折优惠，详情可联系票务经理 13269078023 咨询。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/vjdav8rUBDbXk9pQLedR</id>
            <title>英伟达老员工集体“躺平”，在印钞机上数钱的快乐谁懂？</title>
            <link>https://www.infoq.cn/article/vjdav8rUBDbXk9pQLedR</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/vjdav8rUBDbXk9pQLedR</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jul 2024 08:49:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 英伟达, 股价飙升, 员工财富, AI芯片市场
<br>
<br>
总结: 英伟达近年来股价飙升，公司市值达到惊人的3.2万亿美元，员工财富积累随之增长。公司在AI芯片市场占据主导地位，但面临着日益激烈的竞争压力。公司高管提醒员工保持创新和卓越，以维持市场领先地位。 </div>
                        <hr>
                    
                    <p></p><h2>实现财富自由的英伟达高管们，被爆已集体躺平</h2><p></p><p>&nbsp;</p><p>在科技界，很少有公司能像英伟达近年来那样实现如此惊人的增长。自 2024 年初以来，英伟达的股价飙升了惊人的 167%，标志着该公司的增长故事又翻开了新的篇章。</p><p>&nbsp;</p><p>得益于多年的技术积累，英伟达满足了全球几乎所有主要云计算和 AI 公司对 GPU 的需求。在过去五年中，英伟达股价上涨超3000%，证明了英伟达在半导体和人工智能市场的主导地位。公司总裁兼首席执行官黄仁勋 (Jensen Huang) 也成为了科技界超级明星，几乎每周都能听到老黄接受媒体采访的新闻。</p><p>&nbsp;</p><p>这一惊人的增长不仅使公司的市值达到惊人的 3.2 万亿美元，而且还改变了许多员工的财务状况。随着公司股价飙升，五年前或者更早加入公司的员工现在都是百万富翁了，他们的财富积累跟随着公司的股价一路水涨船高。</p><p>&nbsp;</p><p>据美国科技公司薪酬、福利数据收集网站Levels.fyi数据显示，英伟达的产品经理（总共八个层级中的第三层级）每年平均可获得 77700美元的股票收入。</p><p>&nbsp;</p><p>根据Finlo 的投资计算器和《企业家》网站统计，2019 年收到的 77700 美元的股票赠与的价值如今已经超过 160 万美元——这还不包括近年来累积的股票红利的价值。</p><p>&nbsp;</p><p>按照同样的算法：假设他们都在五年前加入，那么入门级软件工程师将获得近 50 万美元，高级解决方案架构师将获得 130 万美元，四级数据科学家仅从最初的股票奖励中就能获得 200 万美元。不仅仅是高管，甚至中层管理人员的年薪也超过 100 万美元。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/5d/5d26ebad7fdf444a3e9b45455a124495.png" /></p><p></p><p>英伟达各级别产品经理薪酬，更新日期：2024年7月1日</p><p>&nbsp;</p><p>英伟达从生成式 AI 的繁荣中获益最多。其数据中心 GPU 和相关 AI 产品的销售将 Team Green 的市值推高至 1.19 万亿美元。尽管让员工因公司的成功而变得富有似乎是件好事，但不好的一面也随之而来——坐拥巨额财富也让其中一些员工感到自满。这些在英伟达工作了许多年的老员工们看到他们的股票期权和 RSU（限制性股票单位）大幅升值，有可能使他们成为百万富翁后似乎没有以前那么努力工作了。</p><p>&nbsp;</p><p>据报道，许多资深的英伟达高管和中层管理者现在处于“半退休”状态，这种情况让其他英伟达员工感到恼火。</p><p>&nbsp;</p><p>一位年薪 25 万美元、常驻西海岸的英伟达工程师向《商业内幕》分享了自己的观点。他解释说，尽管英伟达员工的薪水乍一看很可观，但并不一定能转化为长期财富。虽然看起来所有英伟达员工都在从公司的成功中获益，但现实情况却有所不同。</p><p>&nbsp;</p><p></p><blockquote>这位工程师以 RSU 的形式获得了近一半的基本工资，他指出，并不是每个人都能获得大量股票单位。员工可以获得的 RSU 数量是有上限的，即使是表现最好的员工，每年获得的股票也只能相当于基本工资的 50%。</blockquote><p></p><p>&nbsp;</p><p>他说：“你最终会将股票兑现，以履行年度个人所得税、财产税和其他任何费用义务。”这一现实凸显了一个重要观点：对许多员工而言，并没有吃到英伟达飞速发展的红利。</p><p>&nbsp;</p><p>英伟达员工的经历在科技行业并非独一无二。正如特斯拉前人工智能总监 Andrej Karpathy 所说，“大多数人不会持有股票，美国政府拿走了一半。”这种情绪反映了英伟达和特斯拉等公司的员工面临的更广泛挑战。虽然成为百万富翁的潜力是真实存在的，但许多员工最终还是会提前出售股票以满足眼前的财务需求和偿还债务。</p><p>&nbsp;</p><p>随着内部不公平现象愈演愈烈，去年年底，老黄不得不在内部全体会议上提及了外界质疑的“英伟达高管半退休”状态的问题。</p><p></p><h2>竞争日益加剧，黄老板暗示老员工“卷起来”</h2><p></p><p>&nbsp;</p><p>接受《商业内幕》采访的与会者称，黄仁勋在回答有关资深员工不尽职的问题时表示，在英伟达 工作就像一项“自愿运动”，每位员工都应该像自己时代的“CEO”一样行事。他补充说，每个人都应该确定自己的工作水平，因为这些都是成年人的判断。</p><p>&nbsp;</p><p>其中一名在场人员对《商业内幕》表示：“黄老板正在严肃地强调，‘做好你的本职工作’。”</p><p>&nbsp;</p><p>老黄在会上强调了个人责任和职业道德的重要性，他传达的信息很明确：创新和卓越的动力必须保持强劲。</p><p>&nbsp;</p><p>之所以如此着急整顿企业文化，是因为他看到了AI芯片市场日益竞争的市场环境。</p><p>&nbsp;</p><p>尽管英伟达目前毫无争议地占据了 AI 芯片市场的主导地位，狂揽了超过80%的市场份额，但竞争也愈演愈烈。英特尔和AMD等老牌科技巨头以及Etched、Cerebras和D-Matrix等新兴初创公司都在争夺价值数十亿美元的高利润空间。</p><p>&nbsp;</p><p>据报道，英伟达目前约 40% 的收入来自四家公司：微软、Meta、亚马逊和 Alphabet。所有这些公司都有能力在未来某一天完全自主开发 AI 芯片。</p><p>&nbsp;</p><p>也就是说，英伟达的现有客户有一天可能会成为其最大的竞争对手。</p><p>&nbsp;</p><p>黄仁勋也在前不久的股东大会上谈到了竞争威胁，但没有特别点名任何竞争对手。在回答股东问题时，他说英伟达的策略是制造“总拥有成本最低”的 AI 芯片。</p><p>&nbsp;</p><p>这五个字并不一定意味着英伟达的芯片是市场上最便宜的，其每块芯片的价格高达3万美元。相反，当潜在客户考虑性能、运行芯片的成本及其更广泛的影响力时，英伟达的芯片总体上可以呈现出“最低的总成本”。</p><p>&nbsp;</p><p>黄仁勋在接受CNBC 采访时表示：“NVIDIA 平台可通过各大云提供商和计算机制造商广泛使用，为开发人员和客户创造了庞大且具有吸引力的安装基础，这使得我们的平台对客户更有价值。”</p><p>&nbsp;</p><p>事实上，英伟达的芯片已经存在 30 年了，但直到最近，它们才被用作<a href="https://www.gamesradar.com/hardware/desktop-pc/your-nvidia-graphics-card-will-soon-be-able-to-help-you-when-youre-stuck-in-games/">显卡</a>"。</p><p>&nbsp;</p><p>黄仁勋相信这些芯片可以做更多的事情。2016 年，他要求他的团队使用这些芯片构建一个 AI 服务器，最终这个服务器像公文包一样大，制造成本为 129,000 美元。然后他把这个服务器作为礼物亲手交给了 OpenAI。</p><p>&nbsp;</p><p>目前，数以万计的英伟达芯片为OpenAI 的 ChatGPT提供支持。</p><p>&nbsp;</p><p>黄仁勋在会上强调，英伟达在人工智能芯片方面占据先机，因为该公司十年前就开始投资这项技术，投入了数十亿美元，并招募了数千名工程师参与研发。</p><p></p><p></p><h2>老板不裁员是员工“躺平”的主要原因吗？</h2><p></p><p>&nbsp;</p><p>与黄老板对于外部竞争的焦虑形成对比的是英伟达内部员工们对于外部环境“一片祥和”的主观判断。</p><p>&nbsp;</p><p>不少躺在”功劳簿“上的英伟达老员工认为，目前英伟达面临的外部竞争不足。这也是他们认为没有必要努力工作的原因之一。“我们没有竞争，”其中一位知情人士说。“但我们正慢慢变得臃肿。有些人什么都不做。”</p><p>&nbsp;</p><p>另一个让他们“躺平”的原因是因为老黄是一位不爱裁员的老板。没有哪位 CEO 像黄仁勋一样深受员工爱戴。他在去年 10 月份最受欢迎的 CEO调查中名列榜首，支持率高达 96%，比排名第二的沃尔玛老板道格·麦克米伦高出 8%。黄仁勋之所以受欢迎，是因为他不愿裁员。去年夏天，当英伟达未能实现盈利预期，经济形势更加糟糕时，黄仁勋向员工保证，公司会加薪，而不是裁员。该公司上一次正式裁员是在 2008 年金融危机期间。</p><p>&nbsp;</p><p>虽然这种行为能激发员工对老板的忠诚度，提高员工的幸福感，但也会带来意想不到的问题。“在这里，被解雇比被录用更难，”其中一位知情人士说。</p><p>&nbsp;</p><p>一些长期在英伟达任职的员工可能会因为公司的成功而变得懒惰，但黄仁勋肯定不会放松警惕。他最近承认，他一直担心公司有一天会倒闭——英伟达过去曾多次濒临破产。</p><p>&nbsp;</p><p>不得不承认的事实是，英伟达许多老员工如今仍然可以在“半退休”模式下看着自己的股票价值不断上涨。</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href="https://www.entrepreneur.com/business-news/nvidia-long-term-employees-semi-retired-multimillionaires/476271">https://www.entrepreneur.com/business-news/nvidia-long-term-employees-semi-retired-multimillionaires/476271</a>"</p><p><a href="https://news.ycombinator.com/item?id=40826421">https://news.ycombinator.com/item?id=40826421</a>"</p><p><a href="https://www.hexmarkets.com/how-are-nvidia-employees-becoming-millionaires-with-a-semi-retirement-plan/">https://www.hexmarkets.com/how-are-nvidia-employees-becoming-millionaires-with-a-semi-retirement-plan/</a>"</p><p><a href="https://thedeveloperstory.com/2024/06/28/nvidia-is-suffering-from-success-despite-being-one-of-the-most-valuable-companies/">https://thedeveloperstory.com/2024/06/28/nvidia-is-suffering-from-success-despite-being-one-of-the-most-valuable-companies/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/HGYTFfpeD4wmaKIoClcv</id>
            <title>全员降薪60%、300亿市值几乎跌成零！这个曾剑指英伟达的国产芯片公司被曝造假，业内评其“老鼠屎”</title>
            <link>https://www.infoq.cn/article/HGYTFfpeD4wmaKIoClcv</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/HGYTFfpeD4wmaKIoClcv</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jul 2024 08:47:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 全球半导体市场, 左江科技, DPU概念, 财务造假
<br>
<br>
总结: 全球半导体市场在人工智能、物联网、5G通讯数字化和智能化的浪潮下不断发展，左江科技曾是市值最高300亿的国产芯片公司，因财务造假被深交所退市，公司股价急剧下跌。左江科技在DPU概念股中一度赚得盆满钵满，但由于DPU产品交付问题导致公司陷入困境。左江科技在DPU领域缺乏深刻积累，面临着DPU产业商业化落地的挑战。 </div>
                        <hr>
                    
                    <p>全球半导体市场在人工智能、物联网、5G通讯数字化和智能化的浪潮下不断发展，众多本土芯片制造商如雨后春笋般崭露头角。然而，在这场激烈的商业竞争中，并非所有公司都能一帆风顺。有些企业凭借其卓越的表现赢得了声誉，而有些则因为决策失败、管理不善、经营混乱等问题走到了穷途末路。</p><p></p><p>因财务造假，市值最高300亿芯片公司宣布退市近日，在最新提交的监管文件中，左江科技宣布，其股票将于7月26日在深交所停止交易。此前，该公司未能为2023年财务业绩提交一份干净的审计报告，这促使深交所采取行动将其退市。</p><p></p><p>据悉，左江科技股票将自2024年7月8日进入退市整理期交易，预计最后交易日期为2024年7月26日。退市整理期满的下一个交易日，交易所将对公司股票予以摘牌。这家曾号称要“对标英伟达”、市值最高突破300亿元的国产芯片公司最终没能避免被淘汰的命运。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bf/bf4f0da692c43fcc25ef1b7073df4b44.png" /></p><p></p><p>左江科技成立于2007年，最初是一家网络安全应用硬件的设计、制造和销售商，后来左江科技自称主要从事信息安全领域相关的软硬件平台、板卡和芯片的设计、开发、生产与销售。2019年10月在创业板上市，随后连续“斩获”17个涨停，一度成为资本市场的“香饽饽”。</p><p></p><p>沾上DPU概念是公司股价“起飞”的主要原因。DPU是数据中心面向算力时代重构的关键芯片，被称为数据中心继CPU、GPU之后的“第三颗主力芯片”。</p><p></p><p>左江科技从2021年起不断披露DPU（可编程数据处理芯片）的研发进度，尤其是号称正在研发的NE6000性能可媲美NVIDIA Bluefield-2，称新DPU将于2022年下半年流片返回。</p><p></p><p>而恰逢那时以ChatGPT为代表的大语言模型产品爆火，人工智能服务器对专用芯片的需求飞涨。左江科技成为一时稀缺的DPU概念股，即使业绩下滑严重，但股价却一路上涨，在2023年7月一度涨到299.8元，公司总市值超300亿元。</p><p></p><p>本来局面一片大好，但左江科技却在DPU产品交付上出了问题。</p><p></p><p>据《21 世纪经济报道》报道，2022年12月27日，左江科技（*ST左江）和北京昊天旭辉科技有限责任公司（下称“昊天旭辉”）签署合同，2023年1月3日即完成交付400片“NE6000”系列DPU芯片，并在2023年1月确认合同收入1261万元。</p><p></p><p>但实际上，左江科技已卖出的上述DPU芯片，绝大部分正在仓库堆积。同时，该笔交易的终端用户巨贤科技法定代表人，还与左江科技董事长同名。也就是说，左江科技卖出的这些芯片，最终实际上又回到了自家仓库里。这一笔收入的商业合理性也被交易所发函质疑。</p><p></p><p>2024年1月30日，证监会通报了对左江科技财务造假案阶段性调查进展情况。证监会初步查明，左江科技2023年披露的财务信息严重不实，涉嫌重大财务造假。</p><p></p><p>自那时起，左江科技股价迅速跳水，截至其停牌前最后一个交易日，左江科技股价只剩6.94元，总市值7.08亿元，还有1.2万户股东，股价较去年7月的最高点已跌了97%。</p><p></p><p>今年5月7日，左江科技收到深交所退市告知书，根据《告知书》，深交所指出，左江科技2023年度经审计的净利润亏损2.23亿元，且扣除与主营业务无关的业务收入和不具备商业实质的收入后的营业收入为5217.27万元，同时公司2023年财务会计报告被出具无法表示意见的审计报告。触及深交所《创业板股票上市规则（2023年8月修订）》第10.3.10条第一款第一项、第三项规定的股票终止上市情形，深交所拟决定终止ST左江（左江科技）股票上市交易。</p><p></p><p></p><h2>全员降薪60%，业内人士：不看好</h2><p></p><p></p><p>自OpenAI在全球范围内掀起生成式AI热潮后，资本市场也对AI相关环节，包括AI软硬件基础设施青睐有加，最典型的就是英伟达凭借GPU在AI时代一骑绝尘，市值直冲2万亿美元。而左江科技也借着这股DPU东风赚得盆满钵满。</p><p></p><p>在资本加持下，左江科技交付了一款名为“NE6000”的DPU芯片。据左江科技官方微信消息，2022年11月，鲭鲨NE6000系列网络数据处理芯片（DPU）研制成功，NE6000是国内首颗可提供25G和100G接口能力的自主可控芯片，也是国内首颗拥有200Gbps的数据平面可编程的网络数据处理芯片。同时，左江科技还在回复2022年年报问询函时称，NE6000与国外同类产品的差异主要体现在芯片工艺不同，NE6000研制对标英伟达（Nvidia）2020年推出的上一代Bluefield2 DPU。</p><p></p><p>但不少业内人士对左江科技下场参与DPU产业的举动并不看好。</p><p></p><p>某DPU芯片公司技术专家Michael Liu在接受AI前线采访时表示：“研发一款DPU芯片需要投入的资金和时间成本都是巨大的，甚至每年需要投入近10亿元来做研发，左江科技在DPU领域没有深刻的积累，他们的基因也并非做DPU起家的，所以他们走到今天这一步并非偶然。”</p><p></p><p>Michael Liu介绍道，与CPU和GPU相比，DPU更像是个综合体，它集芯片、软件和云于一体，DPU是算网融合的关键组件，其中网中有算这件事情只有DPU可以做，这种负载类型CPU是无法处理的，因此DPU在当前的技术趋势下将会大有可为。</p><p></p><p></p><blockquote>Michael Liu也提到，尽管DPU前景乐观，但要做到大规模商业化落地还有两点挑战：第一点是成本问题，第二是软硬件的成熟度问题。“如果一颗DPU芯片卖5万块钱，做得再好都不太可能大规模商业化。现在DPU通常都不便宜，英伟达的DPU也很贵，要3000-4000美金以上。要想达到比较大规模的量产，在成本上还要进一步降低。此外，我们需要关注DPU的软硬件成熟度问题。DPU的发展是伴随着AI对算力基础设施的巨大需求而兴起。然而，AI对整个算力的需求仅仅是一个新兴的趋势。以前的数据中心并没有DPU的存在，但随着算力需求的兴起，算力基础设施系统结构正在从原来的网络加交换节点这种分布式结构，向“三U一体”（即计算、存储、网络）的结构演进，这也凸显了DPU的重要性。尽管这一趋势是正确的，但是对于大型芯片而言，期望在3到5年内就能达到成熟是不现实的，实际上可能需要5到10年的时间。这尚且是一个相对乐观的预测。DPU最初发布时，并没有预料到后面一年多时间内大模型的快速发展，对算力的需求增长如此之快，也许AI算力需求的快速增长会加速DPU的成熟。”</blockquote><p></p><p></p><p>可见，想做好一款DPU，并非一朝一夕的事。</p><p></p><p>值得一提的是，有左江科技内部员工向AI前线独家爆料，公司于今年年初曾告知员工，称自今年12月起执行全员降薪，所有员工只发40%的工资。</p><p></p><p>参考链接：</p><p>https://finance.eastmoney.com/a/202406293117426159.html</p><p>http://www.cinno.com.cn/industry/news/china-semi-investment2023</p><p>https://www.uxingroup.com/info/news-i03224i1.html</p><p>https://www.21jingji.com/article/20231214/herald/f1b6612da523ae03313da65e692b0b5e.html</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/4bkJqv7M5UWTiTMHQNep</id>
            <title>挖矿不行了找AI接盘！挖矿公司们来抢云厂商生意：收入涨10倍，今年的算力早就卖完了！</title>
            <link>https://www.infoq.cn/article/4bkJqv7M5UWTiTMHQNep</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/4bkJqv7M5UWTiTMHQNep</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jul 2024 07:04:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能, 比特币矿工, CoreWeave, 英伟达
<br>
<br>
总结: 人工智能技术的发展催生了比特币矿工企业的转型，其中CoreWeave成为了人工智能云计算领域的领导者。通过与英伟达合作，CoreWeave成功转型为云服务提供商，为高性能计算需求的特定客户群体提供服务，吸引了多家投资方的支持。其成功转型和发展展示了人工智能技术对于传统行业的影响和改变。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>挖矿公司突然成为交易中心，催化剂是人工智能。</blockquote><p></p><p>&nbsp;</p><p>随着AI厂商疯狂提升产品智能性与实用水平，他们对于低成本、高供应量能源的需求也在同步猛增。而这股淘金热的升温，又给一批意料之外的受益者带来了巨额利润：比特币矿工。</p><p>&nbsp;</p><p>“不少身陷困境的加密货币矿场开始全面投身其他行业，这恐怕已经成为必然。”数据中心及比特币挖矿公司IREN 首席商务官Kent Draper说道。</p><p>&nbsp;</p><p>最近几个月来，各主要比特币挖矿公司已经开始将部分计算设备更换成用于运行和训练AI系统的硬件。这些公司认为，与动荡不断的加密货币行业相比，AI训练能够提供更安全、更稳定的收入来源。</p><p>&nbsp;</p><p></p><h2>典型代表 CoreWeave 的惊人崛起</h2><p></p><p>&nbsp;</p><p>从早期一个默默无闻的加密货币挖矿公司摇身一变成为人工智能云计算领域的领导者，CoreWeave 借势完成了华丽蜕变。</p><p>&nbsp;</p><p>2016 年时候，三位商品交易员Michael Intrator、Brian Venturo 和 Brannin McBee 在曼哈顿的一间办公室里开始了他们的小爱好：购买了一块性能一般的 GPU 来挖以太坊，希望能偶尔“赚个外快”。</p><p>&nbsp;</p><p>得到好处的三人从身边朋友拿到了一些小额早期投资，把挖矿地点从台球桌变成了新泽西州的一个车库（数据中心）。不久之后，他们决定创业，CoreWeave的前身Atlantic Crypto正式成立。</p><p>&nbsp;</p><p>作为挖矿企业，他们的核心生产资料就是GPU。2019年左右的加密寒冬让不少挖矿企业倒闭，他们趁机抄底显卡，从拥有几百张显卡一下变成了有数万张，数据中心也增加到了七个，占以太坊网络总量的1%以上。</p><p>&nbsp;</p><p>在加密寒冬中，他们一方面尝试为其他加密矿工提供GPU云服务器，同时也发现了一项新“需求”：大量依赖GPU加速的企业找到他们，希望他们提供算力支持。这些企业都有一个共同的痛点：传统云服务提供商提供有限的算力选项，同时垄断价格，让大规模的业务扩展变得非常困难。</p><p>&nbsp;</p><p>这家挖矿企业的转型之路其实并不算太波折，因为背后有贵人“英伟达”相助。</p><p>&nbsp;</p><p>2019年，CoreWeave转型做IaaS，并将消费级GPU全面转向英伟达的企业级GPU。2020年，CoreWeave宣布加入英伟达合作伙伴网络计划，成为“算力黄牛”。直到2022年，大规模显卡挖矿时代结束，CoreWeave 彻底转型成为一家云服务提供商，并在11月成为首批提供采用英伟达 HGX H100超级芯片的云服务商之一。</p><p>&nbsp;</p><p>随着微软支持的OpenAI于2022年11月推出席卷全球的ChatGPT，整个世界对于AI计算的巨大需求也被随之点燃。</p><p>&nbsp;</p><p>为了把握机会，该公司迅速扩大了融资力度。CoreWeave在2023年上半年通过股权融资拿到超过4.2亿美元，几个月后又通过债务融资筹集到23亿美元。部分原股东则在去年12月向富达等企业出售了价值6.42亿美元的股票。5月份，他们再次达成两笔交易，分别以债务和股权形式筹集到75亿美元和11亿美元。</p><p>&nbsp;</p><p>2023年4月，CoreWeave 还获得了来自英伟达的2.21亿美元B1轮融资。8月，CoreWeave 将英伟达 H100作为抵押品，获得了另外 23 亿美元的债务融资，资金将用于收购更多芯片，以及建设更多数据中心。</p><p>&nbsp;</p><p>Intrator表示，CoreWeave需要巨量交钱以便为“扩大业务规模，从而为任何想要投身于AI热潮的参与者提供支持”，也就是满足对方的一切芯片需求。</p><p>&nbsp;</p><p>CoreWeave如今的主营业务，就是出租其数据中心内运行着的大量英伟达芯片，包括大受欢迎的H100和即将推出的B200。该公司CEO Michael Intartor表示，CoreWeave的基础设施旨在满足高性能计算的特殊需求，包括用于连接AI芯片集群的调整网络以及算力强劲的液冷服务器。</p><p>&nbsp;</p><p>尽管CoreWeave的服务核心离不开对英伟达GPU的倚重，但Intrator强调，千万不要误解CoreWeave与这家全球最具价值芯片制造商间的关系。</p><p>&nbsp;</p><p>“英伟达之所以向我们赋予GPU使用权，绝不是因为他们能在这里攫取既得利益，也不是因为我们有什么优先级更高的门路。”Intrator表示，相反，CoreWeave的竞争优势也绝不仅仅体现在掌握GPU芯片上，例如CoreWeave开发出能自动管理并维护GPU集群的软件。</p><p>&nbsp;</p><p>他还曾回答关于一边公司从英伟达手中筹集资金，另一边却把大部分资金花在采购该公司产品上的问题。“情况并不是大家想象的那样。英伟达向我们投资了1亿美元，而我们通过债务和股权融资总计筹集到了120亿美元。与我们采购的基础设施规模相比，英伟达的注资额度显得微不足道。”</p><p>&nbsp;</p><p>英伟达则否认了其投资的公司能够优先拿到新款GPU产品。英伟达旗下风险投资部门NVentures负责人Mohamed Siddeek去年在接受英国《金融时报》采访时表示，“我们绝不会帮助任何人插队。”</p><p>&nbsp;</p><p>尽管如此，Intrator仍然承认，允许英伟达审查CoreWeave业务并决定投资，在对于这样一家年轻企业在市场上的资金筹集有着“非常重大的意义”。他指出，“我愿意回答英伟达提出的各种问题，因为他们比任何人都更了解我们在做什么、想做什么，也更愿意为此投入大量资金。”</p><p>&nbsp;</p><p>挖矿出身的CoreWeave如今早已远离加密货币。</p><p>&nbsp;</p><p>与亚马逊云科技和微软Azure一样，CoreWeave在采购和维护自有服务器之外，为企业客户们提供了一种新的替代选项，可实现对算力资源的灵活访问。</p><p>&nbsp;</p><p>但与2006年成立、面向几乎一切应用程序和数据需求的亚马逊云科技不同，CoreWeave的数据中心只服务于具有极高性能计算需求的特定客户群体，主要涵盖AI、药物研究和媒体集团等受众。</p><p>&nbsp;</p><p>CoreWeave的各位投资方，包括对冲基金Magnetar Capital、Blackstone和Coatue，也都坚信对于专业AI服务的需求飙升必将重塑整个价值达5000亿美元的云计算市场，有望在已经投入数百亿美元的各大科技巨头之间再开辟出一条新的赛道。</p><p>&nbsp;</p><p>Intrator指出，“下一代云计算的使用方式将与20年前云计算的使用方式截然不同。”他甚至将CoreWeave比作特斯拉，而传统科技巨头则类似于福特。</p><p>&nbsp;</p><p>在Intrator看来，向早期投资方推销这个观念“极其困难”，因为对方必须“在这个自己原本一无所知的领域内成为专家，才会愿意供出数十亿美元并将其交给投资委员会，最终创造出新的的资产类别”，例如将英伟达的图形处理单元视为新的抵押物。</p><p>&nbsp;</p><p>CoreWeave联合创始人和首席战略官Brannin McBee表示，Coreweave今年的收入会增长10倍，到2024年底的所有算力已经售罄。该公司现在有大约500名员工，年底将会接近800人。而其中很多需求是训练到推理的转换推动的，比如训练可能需要1万卡训练，但像ChatGPT这种一旦进入推理，则需要一百万张卡。</p><p>&nbsp;</p><p>根据 Omdia 数据，英伟达 H100 分配数量为：微软 Azure (15 万张)、Meta (15 万张)、亚马逊云科技 (5 万张)、谷歌云 (5 万张)、甲骨文 (5 万张)、腾讯 (5 万张)、百度 (3 万张) 和阿里巴巴 (2.5 万张)。但 CoreWeave 就有4 万张、Lambda 就有2 万张。此外，字节跳动 有2 万张、特斯拉 1.5 万张。</p><p>&nbsp;</p><p>根据 Intrator的计划，他可以利用GPU资产、与客户间签订的长期合同价值以及“经过验证的执行能力”等优势，成功说服贷方掏出数十亿美元。</p><p>&nbsp;</p><p>如今，CoreWeave正着眼于欧洲区域的快速扩张。该公司计划在明年年底之前投资22亿美元在挪威、瑞典和西班牙建设三处数据中心。该公司最近还承诺在英国投资13亿美元建设两处设施，并将英国作为其欧洲总部所在地。</p><p>&nbsp;</p><p>而为了在美国市场加速扩张，CoreWeave还与比特币挖矿公司Core Scientific建立了合作伙伴关系，将后者的多处数据中心转用于托管自己的GPU硬件。CoreWeave还提出以超过10亿美元的价码直接收购Core Scientific，但由于Core Scientific认为CoreWeave对其估值不合理作罢。</p><p>&nbsp;</p><p>Intrator表示，到2024年底，CoreWeave将在美国和欧洲等地坐拥28处数据中心，并计划在未来几年内“真正将业务足迹铺向全世界。”Intrator总结称，“我们将继续尽一切可能，加快规模扩张的步伐。”</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>AI厂商积极拉拢矿场</h2><p></p><p></p><p>除了 CoreWeave，还有不少比特币矿场开始将设施出租给AI客户。</p><p>&nbsp;</p><p>Core Scientific公司CEO Adam Sullivan在4月接受采访时指出，AI厂商正在积极出价拉拢比特币挖矿设施。“他们已经开始以高于加密货币市场的价码认购挖矿设施。”他同时补充道，AI厂商的申请数量“如雪片般飞来，我们也开始评估最合适的资产盈利方式。”</p><p>&nbsp;</p><p>也有一些比特币挖矿企业选择自主运营GPU。</p><p>&nbsp;</p><p>6月24日，比特币矿商Hut 8从Coatue Management处获得了1.5亿美元投资，用于建设AI基础设施。Hut 8在今年的<a href="https://hut8.com/2024/05/15/hut-8-reports-first-quarter-2024/">第一季度财报</a>"中表示，已购买了首批 1,000 块 Nvidia GPU，并与一家风险投资支持的 AI 云平台达成了客户协议。该公司CEO Asher Genoot 表示，预计今年下半年开始，公司将以每年约 2000 万美元的速度创收。</p><p>&nbsp;</p><p>在部分IREN的设施当中，用于AI训练和推理的GPU及ASIC（专为比特币挖矿提供动力的专用集成电路）正在并行运作。</p><p>&nbsp;</p><p>“我们认为这两项业务可以彼此互补，且分别对应完全不同的商业形态。比特币属于即时收益，但波动性更大。而AI业务则更依赖于客户——但只要有了稳定的客源，收益就能持续不断地稳定流入。” Draper 解释道。</p><p>&nbsp;</p><p>Bit Digital 则截至 4 月底已经拥有 251 台服务器，该公司表示，当月从其第一份 AI 合同中获得了约 410 万美元的收入。Iris Energy 预计其 AI 云服务每年可带来 1400 万至 1700 万美元的收入。</p><p>&nbsp;</p><p>据 CoinShares 消息，Bit Digital 27% 的营收来自人工智能；Hut 8 6% 的销售额来自人工智能；在加拿大和瑞典设有数据中心的 Hive 则有4% 的营收来自人工智能服务。</p><p>&nbsp;</p><p>摩根大通6月24日的报道指出，截至目前，这种转变也受到了投资者们的热烈欢迎，这导致14家主要比特币挖矿公司的市值自6月初以来猛增22%，达到40亿美元之巨。</p><p>&nbsp;</p><p>不过，转向人工智能并不像重新利用现有基础设施和机器那么简单，因为人工智能要求的高性能计算 (HPC) 数据中心、数据网络等与挖矿设备ASIC不同，ASIC几乎也不能用于做其他事情。</p><p>&nbsp;</p><p>“除了变压器、变电站和一些开关设备外，矿工目前拥有的几乎所有基础设施都需要推倒并从头开始建造，以适应 HPC。”Needham 分析师在 5 月 30 日的一份报告中写道。</p><p>&nbsp;</p><p>Needham 估计，HPC 数据中心的资本支出为每兆瓦 800 万至 1000 万美元（不包括 GPU），而比特币挖矿的资本支出通常为每兆瓦 30 万至 80 万美元（不包括 ASIC）。</p><p>&nbsp;</p><p>不过，很多挖矿公司们至少目前表示要将比特币挖矿基础设施转换为 HPC 数据中心。</p><p>&nbsp;</p><p>“改造是可行的，因为该公司拥有并控制其所有的数据中心基础设施。”Core Scientific CEO Adam Sullivan 说道。他曾向 CNBC 表示：“看待比特币挖矿设施最好的方式是，我们本质上是数据中心行业的电力外壳。”</p><p>&nbsp;</p><p></p><h2>历时多年的转变</h2><p></p><p>&nbsp;</p><p>&nbsp;</p><p>考虑到双方的需求，AI与比特币挖矿产业之间的携手可说是一拍即合。AI厂商需要比特币矿场已经成型的土地空间、廉价能源与基础设施；比特币矿场则看重AI计算的收入稳定性，以及当前AI炒作周期带来的巨大潜在利润。</p><p>&nbsp;</p><p>这种转变也反映出当下的几个趋势：AI技术的炒作热度飙升，电力供应减少，而比特币产量减半后挖矿业务的前景则逐渐势微。</p><p>&nbsp;</p><p>事实也证明，相当一部分设施其实就在比特币矿场们的掌握之中。</p><p>&nbsp;</p><p>在比特币诞生之初，矿工们发现增加计算机设备的规模能够大大增加自己的利润，并因此建立起巨大的服务器农场，利用廉价能源日夜运行。从历史上看，大规模开采比特币曾经是项利润丰厚的业务，但也同样受制于动荡不断的加密货币行情。</p><p>&nbsp;</p><p>在2022年加密货币崩盘之后（这场大崩盘由Sam Bankman-Fried及Do Kwon等企业家的冒险行为所引发），许多矿场已经被迫破产或者彻底关门。但在崩盘当中幸存下来的挖矿公司，很快在2023年到2024年初重新回到盈利的正轨之上。但今年4月新的挑战接踵而至：比特币宣布名“减半”（矿工奖励减少 50%），直接将矿场的挖矿产出削减了一半。</p><p>&nbsp;</p><p>挖矿公司指望着产出减半能够拉动比特币价格大幅上涨，就如同之前加密货币曾经出现的好几轮爆发周期一样，从而抵消这种奖励缩水。但自4月以来，比特币的价格基本横盘不动、挤压了利润空间，迫使矿工们只能寻求更加多样的商业化探索。</p><p>&nbsp;</p><p>以ChatGPT为代表的生成式AI模型凭借数据中心内强大的计算能力而得到改进，这里的基础设施负责从海量数据集内寻找模式并改进响应效果。但由于算力资源太过昂贵，多年以来对于大部分数据中心运营来说，专门为AI训练部署硬件似乎并不划算。</p><p>&nbsp;</p><p>直到四年之前，Draper仍然认为“从商业角度来看，目前的规模效应还不足以带来合理收益。”</p><p>&nbsp;</p><p>但2022年底ChatGPT取得的巨大成功改变了这一格局，其他AI厂商也开始竞相训练并运行自己的模型，希望在效能层面超越OpenAI推出的这位当家花旦。而这自然也对能源供应提出了极高要求：以ChatGPT为例，其处理查询的能耗就高达标准Google搜索的10倍。</p><p>&nbsp;</p><p>于是乎，一众AI厂商开始努力寻求更廉价的电力、能够容纳塞满数千台计算设备的大片数据中心建设土地，以及用于冷却设备的水或巨型风扇等资源。</p><p>&nbsp;</p><p>在旺盛的市场需求之下，符合这些标准的站点也变得越来越炙手可热，尤其是北美地区。一部分司法管辖区甚至开始为等待接入电网的大型数据中心整理出长长的队列名单。哪怕企业获得了初步批准，从头开始建设数据中心也可能需要数年时间、投入数百万美元，并经历漫长的监管和官僚程序。</p><p>&nbsp;</p><p>比特币挖矿公司Terawulf首席运营官兼首席技术官Nazar Khan表示，“把时间倒回五到十年前，当时80%的数据中心负载都来自六到七个主要市场。这部分供应能力已经被占满，部分市场甚至暂停了数据中心的进一步建设工作。因此，新的数据中心负载只能寻找新的容身之所。”</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://time.com/6993603/ai-bitcoin-mining-artificial-intelligence-energy-use/">https://time.com/6993603/ai-bitcoin-mining-artificial-intelligence-energy-use/</a>"</p><p><a href="https://www.cnbc.com/2024/06/03/bitcoin-miners-sink-millions-into-ai-business-seek-billions-in-return.html">https://www.cnbc.com/2024/06/03/bitcoin-miners-sink-millions-into-ai-business-seek-billions-in-return.html</a>"</p><p><a href="https://www.ft.com/content/f4085e30-da81-40f0-8217-507268743f71">https://www.ft.com/content/f4085e30-da81-40f0-8217-507268743f71</a>"</p><p><a href="https://www.nextplatform.com/2024/05/02/how-to-make-more-money-renting-a-gpu-than-nvidia-makes-selling-it/">https://www.nextplatform.com/2024/05/02/how-to-make-more-money-renting-a-gpu-than-nvidia-makes-selling-it/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/80adbf38a27f8a10c4e61bf3b</id>
            <title>PikiwiDB(Pika) 3.5 最佳实践</title>
            <link>https://www.infoq.cn/article/80adbf38a27f8a10c4e61bf3b</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/80adbf38a27f8a10c4e61bf3b</guid>
            <pubDate></pubDate>
            <updated>Mon, 01 Jul 2024 09:53:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: PikiwiDB, RocksDB, Redis, 性能优化
<br>
<br>
总结: PikiwiDB(Pika) 是 360 技术中台中间件团队基于 RocksDB 开发的大容量类 Redis 存储系统，通过持久化存储方式解决 Redis 在大容量场景下的问题。在使用过程中，需要注意线程数量和工作线程池数量的设置，以及与 IO 性能相关的硬件规格。此外，还需注意数据结构的设计和参数调整，以及避免单副本运行的情况。最新版本提供了一些性能优化的命令和建议，帮助用户提高系统性能和稳定性。 </div>
                        <hr>
                    
                    <p>PikiwiDB(Pika) 是 360 技术中台中间件团队基于 RocksDB 开发的大容量类 Redis 存储系统，力求在完全兼容 Redis 协议、继承 Redis 便捷运维设计的前提下通过持久化存储方式解决 Redis 在大容量场景下主从同步代价高、恢复时间慢、单线程相对脆弱、内存成本高等问题。</p><p></p><p>我们根据 360 内部的 PikiwiDB(Pika) 使用经验及社区用户的问题反馈，整理了本文并在这里分享给大家。</p><p></p><h1>之一</h1><p></p><p>在微信群（群管理员微信号：PikiwiDB）中提问时，请主动带上版本号，可大幅度加快问题解决速度。</p><p></p><h1>之二</h1><p></p><p>PikiwiDB(Pika)  已在 2024 年 5 月更新至 3.5.4，但仍然有大量用户停留在 3.3.6 或3.3.2，我们建议使用 3.5.4 的最新版（预计本周内发布 v4.0.0），你会发现你遇到的很多问题都在我们的 bug 修复列表中。</p><p></p><h1>之三</h1><p></p><p>PikiwiDB(Pika) 的线程数量 thread-num 建议设置为 CPU core 数目的 80% 左右，如果是单机多实例的部署，每个 PikiwiDB(Pika) 实例的线程数量可以酌情降低，但不建议低于 CPU core 数的 1/2。</p><p></p><h1>之四</h1><p></p><p>PikiwiDB(Pika) 的工作线程池数量 thread-pool-size 建议和 CPU core 数目一致，如果是单机多实例的部署，每个 PikiwiDB(Pika) 实例的线程数量可以酌情降低，但不建议低于 1/2 CPU core 数。</p><p></p><h1>之五</h1><p></p><p>PikiwiDB(Pika) 的性能和 IO 性能息息相关，如果对耗时非常敏感，建议使用 NVMe SSD。另外，主从服务器的硬件规格应当尽量一致。</p><p></p><h1>之六</h1><p></p><p>在使用 PikiwiDB(Pika) 复合数据结构（hash，list，zset，zset）时，尽量确保每个 key 中的二级 key（或者成为 field）不要太多（不要超过 1 万个），在业务层或者代理层对大 key 符合数据结构进行拆分（类似于分库分表）， 这样可以避免超大 key 带来很多潜在的性能风险。</p><p></p><h1>之七</h1><p></p><p>root-connection-num 参数非常有用，意为“允许通过 127.0.0.1 登录 PikiwiDB(Pika) 的连接数”，它不会被算进客户端最大连接数配置项 maxclients，因此在发生异常 maxclients 被用尽的场景中，管理员仍然可以登录 PikiwiDB(Pika) 所在服务器并通过 127.0.0.1 登入 PikiwiDB(Pika) 处理问题，可以认为是超级管理员通道。</p><p></p><h1>之八</h1><p></p><p>client kill 命令被加强了，如果你想一次性杀掉当前 PikiwiDB(Pika) 的所有客户端连接，只需要执行 client kill all 命令即可。注意，主从同步的网络连接不受影响。</p><p></p><h1>之九</h1><p></p><p>适当地调整 timeout 参数，PikiwiDB(Pika) 会主动断开不活跃时间超过 timeout 值的连接，避免连接数耗尽。由于网络连接会占用主机内存，因此合理的配置 timeout 参数也能够在一定程度上降低 PikiwiDB(Pika) 的内存使用量。</p><p></p><h1>之十</h1><p></p><p>PikiwiDB(Pika) 的内存占用主要集中在 SST 文件的 cache 和网络连接内存占用量，通常网络连接内存量会比 SST 的 cache 大，PikiwiDB(Pika) 目前已支持连接申请内存的动态调整与回收，因此连接占用的总内存大小是可以粗略估算的，如果你的 PikiwiDB(Pika) 内存占用远超预估（如大于 10GiB），那么可能为你当前使用的版本存在内存泄漏问题，尝试依次执行命令 client kill all 对连接内存进行强制回收，或者升级到最新版本。</p><p></p><h1>之十一</h1><p></p><p>非常不建议单副本运行 PikiwiDB(Pika)，单副本的数据安全性无法保障，诸如 RocksDB Bug 或者资源不够（如：ERR IO error: While fdatasync: /data1/db/zsets/16566747.log: Cannot allocate memory）导致 RocksDB 存储数据被污染，此时无法全量恢复数据。 最简集群状态应为一主一从。</p><p></p><h1>之十二</h1><p></p><p>如果 PikiwiDB(Pika) 单副本运行（非主从集群），只在乎性能，且不在乎数据安全性（如缓存场景），可以考虑通过关闭 binlog（将 write-binlog 参数设置为 no）来提高写入性能。</p><p></p><h1>之十三</h1><p></p><p>PikiwiDB(Pika) v3.5.2 以及之后的版本提供了关闭 RocksDB WAL (DisableWAL true) 的命令，如果你的 PikiwiDB(Pika) 实例出现间断性的写性能阻塞的情况，你可以通过关闭 WAL 命令暂时关闭 WAL，这种方式有断电情况下数据丢失的风险，待性能恢复时，请及时再打开。对数据完整性要求不高时，建议关闭 WAL。</p><p></p><h1>之十四</h1><p></p><p>PikiwiDB(Pika) 的数据目录中有大量的 SST 文件，这些文件随着 PikiwiDB(Pika) 数据量的增加而增加，建议为 PikiwiDB(Pika) 配置一个较大的 open_file_limit ，以避免 fd 不够用，如果不希望 Pika 占用太多的文件描述符，可以通过适当增大单个 SST 的体积来降低 SST 的总数量，对应参数为 target-file-size-base。</p><p></p><h1>之十五</h1><p></p><p>不要修改 log 目录中的 write2file 文件和 manifest。write2file 记录了 binlog 文件列表等关键信息，而 manifest 则记录了 RocksDB 的 version 信息，二者关乎 PikiwiDB(Pika) 实例重启后的 binlog 续写及 slave 断点续传时的数据正确性。</p><p></p><h1>之十六</h1><p></p><p>自 PikiwiDB(Pika) v3.5.0 之后的版本摒弃了用 rsync 进程进行全量同步，PikiwiDB(Pika) 进程内部重新实现了一套新的全量同步机制（通过名称为 rsync 的线程传输）。PikiwiDB(Pika) 提供了 rsync 的总传输限速参数 throttle-bytes-per-second 和并发 rsync 线程数 max-rsync-parallel-num，throttle-bytes-per-second  参数的单位是 MiB，建议在千兆环境中该参数设置不应高于 45，而在万兆环境中不应高于 500，以避免 PikiwiDB(Pika) 在全量同步的时候将所在服务器网卡流量用尽而影响到 PikiwiDB(Pika) 服务客户端。</p><p></p><h1>之十七</h1><p></p><p>在 PikiwiDB(Pika) 中执行 “ key * ” 并不会造成 Pika 阻塞（PikiwiDB(Pika) 是多线程的），但在存在巨量 key 的场景下可能会造成临时占用巨量内存（这些内存用于该连接存放 key *的执行结果，会在 “ key * ”执行完毕后释放），因此使用 “ key * ” 一定要小心谨慎。</p><p></p><h1>之十八</h1><p></p><p>如果发现 PikiwiDB(Pika) 有数据但 info keyspace 的显示均为 0，这是因为 Pika 并没有像 Redis 那样对 key 的数量进行实时统计，PikiwiDB(Pika) 中 key 的统计需要人工触发，执行 info keyspace 1，注意执行 info keyspace 是不会触发统计的，没有带上最后的参数 1 将会仅仅展示上一次的统计结果，key 的统计是需要时间的，执行状态可以通过 info stats 中的 is_scaning_keyspace 进行查看，该项值为 yes 表明统计正在进行，为 no 时表明没有正在进行的统计/上一次统计已结束，在统计执行完毕前 info keyspace 不会更新，info keyspace 的数据是存放在内存里的，重启将清零。</p><p></p><h1>之十九</h1><p></p><p>不要在 PikiwiDB(Pika) 执行全量 compact 的时候触发 key 统计（info keyspace 1）或执行 keys *，否则会造成数据体积暂时膨胀直到 key 统计、keys *执行结束。</p><p></p><h1>之二十</h1><p></p><p>对存在大量过期数据的 PikiwiDB(Pika) 实例，compact-cron 配置项可以在固定时段（一般配置为低峰流量时间段）进行过期数据清理。自 PikiwiDB(Pika) v3.5.0 之后还提供了 auto_compact 配置型，启用后 PikiwiDB(Pika) 会自动周期性执行 compact。</p><p></p><p>异常的数据体积（大于估算值 10%以上），可以通过执行 compact 命令，在 compact 执行完毕后观察数据体积是否恢复正常。</p><p></p><p>请求耗时突然异常增大，可以通过执行 compact 命令，在 compact 执行完毕后观察请求耗时是否恢复正常。</p><p></p><h1>之二十一</h1><p></p><p>自 PikiwiDB(Pika) v3.5.0 之后可统计过期 key（可通过 info keyspace 1 来触发统计，通过 info keyspace 查看统计结果），统计结果中的 invaild_keys 的值为“已删除/过期但还未被物理删除的 key 的数量”，PikiwiDB(Pika) 会在后台逐步地对已删除/过期的 key 进行物理清理，由于这是一个后台行为，因此在存在大规模过期 key 的场景下这些 key 可能无法被及时清理，因此建议关注该值，若发现无效 key 数量过多可通过 compact 命令进行全面清理，这样能够将未物理清理的无效数据控制在一个较好的程度从而确保 Pika 的性能稳定，如果 PikiwiDB(Pika) 中存储的数据是规律性过期的，例如每个 key 的过期时间为 7 天，那么建议通过配置 compact-cron 参数来实现每天的定时自动进行全量 compact，compact 会占用一定的 IO 资源，因此如果磁盘 IO 压力过大，建议将其配置为业务低峰期执行，例如深夜。</p><p></p><h1>之二十二</h1><p></p><p>write2file 的角色相当于 binlog，建议 write2file 保留周期/数量不低于 48 小时，足够的 write2file 有利于 大数据集群的从库扩容、从库服务器关机维修、从库迁移 等工作，不会因为主库 write2file 过期而被迫全量重传。</p><p></p><h1>之二十三</h1><p></p><p>PikiwiDB(Pika) 的备份生成为快照式，通过硬链接存放在 dump 目录下，以日期为后缀，每天只生成一份，多次生成备份时新的备份会覆盖之前的旧文件。在生成备份快照的时，为了确保数据的一致性 PikiwiDB(Pika) 会暂时阻塞写入，阻塞时间与实际数据量相关，根据测试PikiwiDB(Pika) 生成 500GiB  备份快照仅需 50ms。在写入阻塞的过程中连接不会中断，但 client 会感觉到 “在那一瞬间请求耗时增加了一些”。由于PikiwiDB(Pika)Pika 的快照是 db 目录中 sst 文件的硬连接，因此最初这个目录是不会占用磁盘空间的。</p><p></p><p>但在 PikiwiDB(Pika) db 目录中的 SST 文件发生了合并、删除后，硬链接的旧文件并不删除，这会导致 PikiwiDB(Pika) 占用的磁盘空间超出预估，所以请根据实际的磁盘空间调整备份保留天数，避免备份太多而造成磁盘空间用尽。</p><p></p><h1>之二十四</h1><p></p><p>如果写入量巨大且磁盘性能不足以满足 RocksDB memtable 的及时刷盘需求，那么 RocksDB 很可能会进入写保护模式（write stall，写入将被全部阻塞），建议更换性能更好的存储系统来支撑，或者降低写入频率（例如将集中写数据的 2 小时拉长到 4 小时），也可适当加大 write-buffer-size 的值来提高 memtable 的总容量从而降低整个 memtable 被写满的可能。</p><p></p><h1>之二十五</h1><p></p><p>PikiwiDB(Pika) 对数据进行了压缩，默认压缩算法为 snappy，并允许改为 zlib，因此每一次数据的存入、读出都需要经过压缩、解压，这对 CPU 有一定的消耗，建议像使用 Redis 一样使用 PikiwiDB(Pika)：在 PikiwiDB(Pika) 中关闭压缩，而在 client 中完成数据的压缩、解压，这样不仅能够降低数据体积，还能有效降低 Pikiw。注意关闭和开启压缩后，需要重启 PikiwiDB(Pika) 实例。</p><p></p><h1>之二十六</h1><p></p><p>读写分离很重要，PikiwiDB(Pika) 在常见的主从集群中由于写入是单点的（仅 master 支持写），因此写入性能是有极限的。可通过多个 slave 来共同支撑读流量，因此 PikiwiDB(Pika) 集群的读性能是随着 slave 数量的增加而增加的，所以对于读量很大的场景，建议在业务层代码加入读写分离策略，同时在 PikiwiDB(Pika) 层增加 slave 数量。</p><p></p><h1>之二十七</h1><p></p><p>全量 compact 的原理是逐步对 RocksDB 的每一层做数据合并、清理工作，在这个过程中会新增、删除大量的 SST 文件，因此在执行全量 compact 的时候可以发现数据体积先增大后减小并最终减小到一个稳定值（无效、重复数据合并、清理完毕仅剩有效数据），建议在执行 compact 前确保磁盘空余空间不低于 30%，以避免新增 SST 文件时将磁盘空间耗尽，另外 PikiwiDB(Pika) 支持对指定数据结构进行 compact，例如一个实例中已知 hashtable 结构的无效数据很少但 hashtable 结构数据量很大，set 结构数据量很大且无效数据很多，在这个例子中 hashtable 结构的 compaction（命令是 compact hash） 是没有必要的，你可以通过 compact set 实现只对 set 结构进行 compaction。</p><p></p><p>注意：在 PikiwiDB v4.0.0 版本之后，不再支持对特定类型的 compaction。因为 PikiwiDB v3.x 使用的存储引擎是 Blackwidow，每个数据类型使用一个 RocksDB，而 v4.0.0 的存储引擎升级为 Floyd，可以在单个 RocksDB 中存储所有类型的数据。</p><p></p><h1>之二十八</h1><p></p><p>PikiwiDB(Pika) 3.5.0 以后的版本支持通过 rate-limiter-bandwidth 配置项以限制磁盘 IO 速率，可以通过调整该配置参数来调整读写速度。在 v4.0.0 之前只支持写限速，在  v4.0.0  之后支持读写限速，可以通过调整配置参数中的  rate-limiter-mode 来设置限速模式。</p><p></p><h1>之二十九</h1><p></p><p>PikiwiDB(Pika) 和 Redis 一样支持慢日志功能，可通过 slowlog 命令查看。slowlog 的原始内容只存于内存中，内存空间有上限，且这个上限可配置，当然如果配置过大会造成 slowlog 占用太多内存。PikiwiDB(Pika) 也允许将 slowlog-write-errorlog 设置为 yes，以把慢日志记录到 pika.ERROR 日志中，用于追溯、分析。</p><p></p><h1>之三十</h1><p></p><p>PikiwiDB(Pika) v3.5.2 以后的版本支持冷热数据分离，并在 Pika 磁盘存储之上增加了内存缓存层（称之为 RedisCache），将用户访问的热数据放在缓存层，冷数据放在磁盘，可减少查询磁盘的次数，提升服务的读性能，不论 PikiwiDB(Pika) 使用的是主从复制模式还是集群模式，可以配置 cache-mode 为 1 ，并设置缓存的大小和个数，以提升读性能。如果实例内存较小，不足以支撑缓存层的资源耗费，你可以选择将 cache-mode 设置成为 0 将缓存层关闭掉。</p><p></p><h1>之三十一</h1><p></p><p>PikiwiDB(Pika) 3.5.3 以后的版本支持了 Redis ACL 功能，设置用户密码的方式发生了变化，ACL的认证方式和 Redis 保持一致，在 config 文件中按照 ACL 规则对 user 进行配置。PikiwiDB(Pika) 3.5.3 仍然兼容以前旧版本的认证方式。</p><p></p><h1>之三十二</h1><p></p><p>PikiwiDB(Pika) 3.5.3 以后的版本支持快、慢命令分离，有快、慢两个线程池，可以防止慢命令对快命令线程池阻塞的影响。可以通过  slow-cmd-list 配置项设置慢命令列表，通过设置 slow-cmd-thread-pool-size 设置慢命令线程池个数。</p><p></p><h1>之三十三</h1><p></p><p>欲知后事如何，且待微信群里分解。请添加 PikiwiDB 小助手【微信号: PikiwiDB】为好友，它会拉您加入官方微信群。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>