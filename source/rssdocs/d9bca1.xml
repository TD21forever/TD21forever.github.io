<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/0qY4CoUbcZIATKEgWUF3</id>
            <title>股价暴跌20%，英特尔宣布裁员15000人！基辛格：这是我职业生涯中最艰难的决定</title>
            <link>https://www.infoq.cn/article/0qY4CoUbcZIATKEgWUF3</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0qY4CoUbcZIATKEgWUF3</guid>
            <pubDate></pubDate>
            <updated>Fri, 02 Aug 2024 12:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 英特尔, 裁员, 成本结构, 芯片制造
<br>
<br>
总结: 英特尔宣布裁员15%，CEO基辛格表示公司需要改变成本结构以应对萎缩的利润率和未充分受益于人工智能等趋势的情况。英特尔正面临着芯片制造领域的挑战，需要加速发展芯片代工业务以应对竞争对手的崛起。基辛格计划通过降低运营成本、简化产品组合、消除复杂性等措施来重塑公司。英特尔在人工智能和移动计算领域的地位已受到挑战，需要采取更大胆的行动来应对市场变化。 </div>
                        <hr>
                    
                    <p>美国当地时间8月1日，英特尔表示将裁减 15% 的员工（约 15000 个工作岗位），以扭转业务局面，与英伟达和 AMD 等竞争对手展开竞争。此次裁员是英特尔 56 年历史上最严重的裁员之一。</p><p></p><p>英特尔公司首席执行官帕特·基辛格周四在给员工的一份备忘录中表示，公司计划在 2025 年节省 100 亿美元。</p><p></p><h2>英特尔宣布裁员15%，CEO基辛格：我很痛苦</h2><p></p><p></p><p>他在英特尔网站上发布的备忘录中写道：“简而言之，我们必须将成本结构与新的运营模式相结合，从根本上改变我们的运营方式。我们的收入没有像预期的那样增长——我们还没有充分受益于人工智能等强大的趋势。我们的成本太高，利润太低。”</p><p></p><p>对我来说，这是一个痛苦的消息。我知道这对你们来说会更加难受。今天对英特尔来说是极其艰难的一天，因为我们正在进行公司历史上一些最重要的变革。</p><p></p><p>简而言之，我们必须将成本结构与新的运营模式相结合，从根本上改变我们的运营方式。我们的收入没有像预期的那样增长——而且我们还没有充分受益于人工智能等强大的趋势。我们的成本太高，利润率太低。我们需要采取更大胆的行动来解决这两个问题——尤其是考虑到我们的财务业绩和 2024 年下半年的前景，这比之前预期的要艰难。</p><p></p><p>这些决定对我的内心产生了巨大的挑战，这是我职业生涯中做过的最艰难的事情。我向你们保证，在未来的几周和几个月里，我们将优先考虑诚实、透明和尊重的文化。</p><p></p><p>下周，我们将宣布一项在全公司范围内为符合条件员工提供改善的退休待遇的计划，并广泛提供自愿离职申请程序。我认为，我们如何实施这些变革与变革本身同样重要，我们将在整个过程中坚持英特尔价值观。</p><p></p><p>基辛格也在备忘录中向外界解释了为什么会选择在此时间节点上进行裁员，他说道：“除了成本之外，我们还需要改变运营方式——这是我们在员工体验调查中许多人都提到的。流程太复杂了，所以我们需要自动化和简化流程。决策需要很长时间，所以我们需要消除官僚主义。系统中效率太低，所以我们需要加快工作流程。”</p><p></p><p>基辛格还坦言，为了将使英特尔成为一家更精简、更简单、更敏捷的公司，接下来英特尔将重点在以下几个方面进行调整：</p><p></p><p>降低运营成本：推动全公司的运营和成本效率，包括上面提到的成本节约和员工减少。简化产品组合：将在本月完成简化业务的行动。每个业务部门都在进行产品组合审查，并找出表现不佳的产品。还要把关键软件资产整合到业务部门中，以加快向基于系统的解决方案的转变。此外，还将把孵化重点缩小到更少、更有影响力的项目上。消除复杂性：英特尔内部将减少层级，消除职责重叠，停止非必要工作，并培养一种更具主人翁精神和责任感的文化。例如，将把客户成功部门整合到销售、营销和传播部门，以简化上市流程。降低资本和其他成本：随着英特尔历史性的“四年五节点”路线图的完成，英特尔将审查所有活跃的项目和设备，以便能够更进一步降本提效。这项举措将使英特尔 2024 年的资本支出减少 20% 以上，英特尔计划在 2025 年将非可变销售成本降低约 10 亿美元。暂停派发股息：从下个季度开始，英特尔将暂停派发股票股息，以优先投资业务并实现更持续的盈利能力。保持增长投资：英特尔的IDM2.0战略没有改变。在努力重建创新引擎之后，英特尔将继续对工艺技术和核心产品领导力进行重点投资。</p><p></p><p>昨天，英特尔公布了2024财年第二财季财报，在第二季度，英特尔净亏损 16 亿美元，即每股亏损 38 美分。与去年同期的 15 亿美元利润（即每股盈利 35 美分）相比有所下降。扣除特殊项目后的调整后收益为每股 2 美分。收入从 129 亿美元下滑 1% 至 128 亿美元。</p><p></p><p>FactSet 的调查显示，分析师平均预计该公司每股收益为 10 美分，营收为 129 亿美元。周四英特尔股价盘后暴跌20%。</p><p></p><p>eMarketer 分析师 Jacob Bourne 表示：“英特尔宣布了一项包括裁员在内的重大成本削减计划，这可能会提振其近期的财务状况，但仅凭这一举措不足以重新定义其在不断发展的芯片市场中的地位。”“英特尔正面临一个关键时刻，因为它要利用美国对国内制造业的投资和全球对人工智能芯片的激增需求，在芯片制造领域站稳脚跟。”</p><p></p><p>基辛格在与分析师的电话会议中指出，英特尔此前曾表示，其在人工智能 PC 市场的投资将在短期内对其利润率造成压力，但从长远来看将给公司带来好处。</p><p></p><p>“我们认为这种权衡是值得的。到 2026 年，AI PC 的市场份额将从目前的不到 10% 增长到 50% 以上。”基辛格说道。</p><p></p><h2>英特尔走到了不得不变革的时刻</h2><p></p><p></p><p>英特尔曾是全球最强大的芯片制造商，统治个人电脑和 Mac 市场长达数十年，但近年来，英特尔的地位似乎已从巅峰滑落。过去二十年的移动计算浪潮令英特尔措手不及，此后，其市值已被移动芯片领域的领头羊高通超越。</p><p></p><p>遗憾的是，英特尔在人工智能浪潮中也未能抢占鳌头。这家芯片制造商正努力追赶强大的竞争对手英伟达的步伐，后者已成为人工智能热潮中全球最有价值的上市公司之一。英特尔在 AI 服务器芯片领域甚至可能不如AMD，因为英特尔进入图形领域的时间相对较短，尚未给人留下深刻印象，所以它不得不对其旗舰笔记本电脑芯片进行重大改造，以应对高通和苹果等公司推出的 Arm 芯片带来的生存威胁，这些芯片的电池寿命比英特尔更长。</p><p></p><p>事实上，让英特尔如此难受的主要原因之一是其芯片代工业务的大幅收缩。英特尔面临的主要挑战是其芯片制造工艺落后于台湾台积电，后者的客户包括 AMD、苹果、英伟达和高通。甚至英特尔自己的一些芯片，包括即将推出的笔记本电脑Lunar Lake CPU，也将使用台积电的芯片制造技术。</p><p></p><p>但这种局面不能一直持续下去。</p><p></p><p>在外界看来，基辛格解决这场危机的办法是——加速发展芯片代工业务，让英特尔生产其他公司设计的芯片。传统上，英特尔生产自己开发的芯片，而不像英伟达和苹果等公司那样，它们设计自己的芯片，但依靠台积电等制造公司来生产。在基辛格的领导下，英特尔在过去两年里一直在积极寻求建立代工业务。</p><p></p><p>有分析师认为，在理想情况下，这可能会让英伟达从竞争对手变成客户，并吸引其他客户，如苹果和微软（后者于今年2月与英特尔签署了一项价值 150 亿美元的芯片制造协议）。</p><p></p><p>然而，建立代工厂需要大量投资——数百亿美元用于工厂和先进的生产设施，就像英特尔在美国的几个地方建立的工厂和先进的生产设施一样，并计划在以色列建立工厂和生产设施。虽然部分必要投资来自政府的大量补贴，但英特尔仍需要从自己的储备中拨出大量资金。这让英特尔最初的问题再次成为焦点：在一个几年内才能见效的项目上投资数百亿美元是很有挑战性的，尤其是在收入和利润停滞不前或下降、股价低迷、投资者焦虑不安的情况下。</p><p></p><p>更难的是，还没等英特尔的代工厂建好，一些原有客户已经流失了。微软最近效仿苹果，在其最新消费硬件（包括Surface Laptop和 Surface Pro）中放弃了英特尔芯片，并与高通独家合作推出了Copilot Plus PC 计划，而无需等待英特尔（或 AMD）的新旗舰笔记本电脑芯片加入其中。</p><p></p><p>Emarketer 分析师雅各布·伯恩 (Jacob Bourne) 表示：“英特尔宣布包括裁员在内的重大成本削减计划可能会增强其短期财务状况，但仅靠这一举措不足以重新定义其在不断发展的芯片市场中的地位。”</p><p></p><p>英特尔还在冒险改变其整个商业模式。它希望生产竞争对手的处理器，为苹果等公司提供某种白标工厂，后者设计自己的芯片，但将制造外包。但该计划将耗资巨大，艰难的转型将导致数千名工人失业。</p><p></p><p>投资者对该公司一直处于困境并不满意：在本次季度亏损之前的过去两年中，该公司总体上一直在亏损和盈利之间摇摆不定，2022 年第二季度至 2024 年第一季度期间累计亏损仅为 11 亿美元。</p><p></p><p>参考链接：</p><p>https://www.intel.com/content/www/us/en/newsroom/news/actions-accelerate-our-progress.html#gs.cgvs85</p><p>https://www.calcalistech.com/ctechnews/article/ryjy00totc</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/tclZz01e2NWMG8UukOPl</id>
            <title>ISC.AI 2024人工智能峰会：赋能千行百业数转智改，助力探索AI共融创生</title>
            <link>https://www.infoq.cn/article/tclZz01e2NWMG8UukOPl</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/tclZz01e2NWMG8UukOPl</guid>
            <pubDate></pubDate>
            <updated>Fri, 02 Aug 2024 12:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: ISC.AI 2024, 人工智能峰会, 大模型技术, AI普惠
<br>
<br>
总结: ISC.AI 2024第十二届互联网安全大会人工智能峰会在北京举办，聚集了业界专家学者和技术领袖，探讨了大模型关键技术与应用、数转智改驱动行业变革、AI技术安全建设等议题，展示了人工智能领域的最新研究成果，推动AI技术与全行业的共融创生。 </div>
                        <hr>
                    
                    <p>8月1日，ISC.AI 2024第十二届互联网安全大会人工智能峰会在北京盛大开幕。本峰会作为ISC.AI 2024人工智能日的重要环节，集聚业界知名专家学者、技术领袖，围绕大模型关键技术与应用、数转智改驱动行业变革、AI技术安全建设等热点议题，全面展现人工智能领域最前沿的研究成果及实践，助力探索AI技术与全行业的共融创生。</p><p>&nbsp;</p><p>探寻路径，破解人工智能时代安全难题</p><p></p><p>人工智能快速发展的同时，也带来了非常复杂的安全问题，可能引发国家、社会、企业和个人等层面的安全风险。为此，中国互联网协会副理事长黄澄清在致辞提出，要加快安全技术创新，提升整体防护水平；推动智能赋能安全，助力加快产业升级；注重安全人才培养，激发行业创新活力。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/f9/f8/f99e229d27523b2c27e630e2c9aac8f8.png" /></p><p></p><p>随后，中国网络空间安全协会副理事长卢卫在致辞中指出，二十届三中全会为人工智能发展安全治理提供了根本的遵循和行动指南，人工智能的发展要注重技术创新、应用服务和安全治理等方面。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/e1/a1/e1296f649af6d659c3f8657b9deffea1.png" /></p><p></p><p>人工智能是新一轮科技革命和产业变革的重要驱动力量，大模型已成为数字经济高质量发展的新引擎。中国信息通信研究院副院长魏亮在致辞中指出，以大模型为代表的新一轮人工智能技术发展浪潮持续席卷全球，呈现出基础愈发坚实、能力愈发完善、融合愈发深入等态势。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/e1/e6/e19f2879b3a0ac292b179521b4379ce6.png" /></p><p></p><p>生成式人工智能拥有语言生成、自然语言交互和迁移三大能力，但同时也存在“幻觉”缺陷。AI产业化要从与人类对齐、多模态生成、构建智能体、具身智能四大方向发展。中国科学院院士，清华大学计算机系教授张钹在《生成式人工智能时代的AI产业-迈向第三代人工智能》主题演讲中指出，发展第三代人工智能，要利用好知识、数据、算法和算力四大要素。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/53/e8/5321c579f9027fedb2ccfe27b9bd54e8.png" /></p><p></p><p>共建AI明星场景，助推AI普惠</p><p></p><p>大模型不是产品，大模型能力要结合场景才能真正发挥价值，要找到高频、刚需、有痛点的AI明星场景。360集团创始人，ISC大会主席周鸿祎在《大模型强强联合，让AI普惠10亿+用户》演讲中指出，“2024年是场景之年，我们探索了AI搜索、AI浏览器和大模型儿童手表三大AI明星单品。”</p><p></p><p><img src="https://static001.infoq.cn/resource/image/8c/9e/8c68b9a3f8e45f0f7626bf22a10b989e.png" /></p><p></p><p>周鸿祎宣布，360开放安全卫士、安全浏览器、搜索、智能硬件四大国民级场景，打造新一代AI产品“AI助手”。与智谱AI、商汤科技、百川智能、火山引擎、百度智能云、腾讯、科大讯飞、华为云、MiniMAX、零一万物、面壁智能等15家大模型厂商合作，全面内置到360国民级入口产品，不需要安装插件就能获取场景，让AI普惠10亿+用户。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/2b/72/2b9325e431b2b6eed4feba0a0ebyy872.png" /></p><p></p><p>随后，周鸿祎与360互联网事业群总裁赵君共同发布360 AI办公一站式学习办公工具集，汇集多家大模型能力，提供一站式AI智能办公解决方案，低使用门槛的AI工具集和40w+海量优质模板及实用工具，打造AI图片、AI文档写作、视频音频、PPT、办公工具及模板大全等不断丰富的AI办公功能矩阵。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/4f/16/4f7cbd1be8702ee2df0f7686bd279c16.png" /></p><p></p><p>360集团副总裁、360数智化集团CEO殷宇辉带来《360数智：建立AI信仰，赋能千行百业》主题演讲。他表示，AIGC是一场新的生产力革命，原有的C端B端的工具都值得重做一遍。360大模型以安全、智能、数字化工程为核心主张，为城市、行业、企业客户提供一体化的数转智改产品和解决方案。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/74/e4/74381cbf6d75bb0cd7a02c2f664984e4.png" /></p><p>360集团副总裁、360数智化集团CEO 殷宇辉</p><p></p><p>打造协同生态，加速数转智改</p><p></p><p>当前，中国大模型市场发展不断提速，呈现百花齐放的繁荣态势，众多大模型厂商相继涌现，提供了多样化的大模型产品和解决方案。本次人工智能峰会邀请了众多知名厂商，致力于通过多维的思维碰撞，共同推动行业的高质量发展。</p><p></p><p>商汤科技副总裁张少霆在《博极医源 精勤不倦：医疗大模型的通专融合之路》演讲中分享了商汤科技在医疗大模型领域的实践，他指出，商汤医疗通过打造医疗大模型工厂，以医疗大模型为中枢大脑，灵活调用多模态专用模型，实现通专融合，进而驱动智慧医院全线升级。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/25/61/25279a2d6ff3846ef7ea33ff7e1f7461.png" /></p><p>商汤科技副总裁 张少霆</p><p></p><p>大模型的能力正在快速落地为业务价值。智谱AI COO张帆在《大模型的探索与实践》主题演讲中对智谱AI进行全面介绍，他提到，智谱AI具有完备的模型矩阵和成熟的应用平台，并分享了大模型在智能座舱、智能手机助理、智能问答系统、旅行AI助手、智能办公等场景的落地案例。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/02/b4/028b62dcb2ee7ba927ae21dc97beb0b4.png" /></p><p>智谱AI COO 张帆</p><p></p><p>随后，360集团与战略合作伙伴举行“同舟共济扬帆起, 乘风破浪万里航”签约仪式。360集团首席运营官叶健与航天云网科技发展有限责任公司副总经理徐汕，中国电子投资控股有限公司副总经理谢竞彤，中国电信上海分公司信息网络部副总经理陈霄航，用友网络科技股份有限公司副总裁董波，摩尔线程智能科技（北京）有限责任公司副总裁胡晓东，北京易华录信息技术股份有限公司副总裁梁敏燕等合作伙伴代表进行战略签约。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/57/03/57c83252482e996daba6849c6aa72703.png" /></p><p></p><p>随后，由AI艺术家数字生命卡兹克主持的“圆桌论坛”上，围绕《超级应用爆发，需要什么土壤？》，360集团副总裁、360AI产品负责人梁志辉，商汤科技副总裁张少霆，百度智能云泛科技行业解决方案总监栗伟，零一万物 API平台负责人蓝雨川针对AI应用商业模式、垂直类技术范式商业化趋势等议题分享了各自的观点。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/74/yy/74928f03a4d336a51aa8a58e23a52byy.png" /></p><p></p><p>人工智能技术正在以前所未有的速度重塑社会格局，ISC愿与各方携手，共同拓展人工智能边界，加速数字化转型与智能化升级的进程，为人工智能时代发展保驾护航。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/rYl2EiLZmT7sxVpONIp8</id>
            <title>真假Agent大讨论：我的 Agent 可能是个 Chatbot？</title>
            <link>https://www.infoq.cn/article/rYl2EiLZmT7sxVpONIp8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/rYl2EiLZmT7sxVpONIp8</guid>
            <pubDate></pubDate>
            <updated>Fri, 02 Aug 2024 08:38:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Agent, Chatbot, 大语言模型, 记忆
<br>
<br>
总结: Agent 是当前人工智能领域的热门话题，具有广泛的应用前景，与 Chatbot 在处理复杂任务和协作方面有所不同。Agent 不一定要模拟人类行为，可以是基于大型语言模型的辅助工具。在技术发展中，Agent 的记忆能力是一个重要研究方向，需要超越人类的记忆能力。 </div>
                        <hr>
                    
                    <p></p><p>目前，Agent（智能体）已经成为当前人工智能领域的热门话题。在很多产品和业务上，Agent都具有广泛的应用前景，不少人认为Agent 会是大模型未来的入口。在企业内部，Agent可以用于复杂的任务场景，帮助企业尽可能提高劳动生产力。但是，由于 Agent 多以Chatbot 形式出现，因此很多人对 Agent 与 Chatbot 之间的差异、 Agent 的技术发展等并不清楚。</p><p></p><p>在日前的 InfoQ 《极客有约》X<a href="https://aicon.infoq.cn/2024/shanghai/track">AICon</a>"直播中，我们邀请了&nbsp;DeepWisdom（MetaGPT）创始人兼CEO吴承霖、腾讯&nbsp;PCG&nbsp;大模型中台&nbsp;Agent&nbsp;技术负责人陈浩蓝，一同探讨Agent的定义、技术挑战、数据合成、智力测试以及落地应用等问题。对话部分亮点如下：</p><p></p><p>Agent不一定要模拟人类行为，可以是基于大型语言模型的辅助工具；合成大量数据以训练 Agent的成本非常高，这可能是未来研究的一个重要方向；AI Agent与Chatbot在处理复杂任务和协作方面有所不同，Agent更复杂且不一定基于对话；Agent实际上和人类的分工相似，但并不完全相同；具身机器人是一个未被充分探索的领域，尽管它具有吸引力，但仍需要证明其商业化可行性。</p><p></p><p>以下为访谈实录，为方便读者阅读，我们在不改变嘉宾原意上进行了整理编辑。完整视频可查看：</p><p><a href="https://www.infoq.cn/video/ev3E7P0dTAGAAwMbVgxQ?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">https://www.infoq.cn/video/ev3E7P0dTAGAAwMbVgxQ</a>"</p><p></p><p></p><blockquote>在 8 月 18-19 日将于上海举办的 AICon 全球人工智能开发与应用大会上，吴承霖老师将出品<a href="https://aicon.infoq.cn/2024/shanghai/track/1707">【AI Agent技术突破与应用】</a>"专题，深入探讨 AI Agent的当前技术现状与发展趋势，揭示其在各行业中的广泛应用和未来潜力。陈浩蓝老师也将在专题论坛上带来分享<a href="https://aicon.infoq.cn/2024/shanghai/presentation/6002">《多智能体技术在开放剧情扮演玩法中的探索》</a>"。大会演讲议题已上线 90%，查看大会日程解锁更多精彩议题：<a href="https://aicon.infoq.cn/2024/shanghai/schedule">https://aicon.infoq.cn/2024/shanghai/schedule</a>"</blockquote><p></p><p></p><p></p><h2>Agent与Chatbot 有什么不同</h2><p></p><p></p><p>InfoQ：在两位老师眼中，Agent 的定义是什么？与Chatbot有什么不一样吗？</p><p></p><p>陈浩蓝：在LLM出现之前，我们对Agent 有一个定义：能够观测环境的输入，对其进行规划、进行输出。在LLM出现之后，LLM对不同输入的泛化能力和其自身的先验知识有了显著提升，使得Agent的工作可以在此基础上进一步展开。</p><p></p><p>对于Chatbot，我认为它和Agent是两个正交的维度。Agent是一种技术解决方案，而Chatbot则更像是一种产品形态。这实际上是两个不同层面的概念。Agent的狭义定义是，能够接收输入、观察并规划动作，对工具的使用有记忆。而更加广义的定义是，任何以LLM为核心组件构建的工作流程都可以被称为Agent。Agent不一定需要完美模拟人类行为。人脑的架构只是自然选择中的一个不错选项，但基于新的底层架构，如神经网络，未来可能会出现更优的思考组织架构。例如，100年前我们可能认为今天的高科技是飞行汽车，但实际上却是微信支付和美团外卖。</p><p></p><p></p><p>吴承霖：我和我的一个同事进行了一次长期讨论，我们得出了一个非常有趣的结论：Agent和人类不是同一物种，它们的形态和人类不同，主要原因是智能体是共享心智，所有智能体拥有共同的心智模型，这可以类比柏拉图表征，就是说它们拥有相同的内心世界。这种共享心智的概念与《星际迷航》中的Borg种族非常相似，Borg种族是共享心智但能独立存在。因此，我们在MetaGPT的第一版代码中设计了一个并行参数，允许智能体并行执行，这个参数名为N-Borg，即定义几个borg来执行任务。实际上，Agent作为共享心智的个体，是非常有趣的。</p><p></p><p>从另一个角度来看，许多人认为在大语言模型上添加一些东西就可以构成Agent，这个定义相当粗糙。我们需要添加什么？是一段提示词、一个函数还是其他东西？根据OpenAI GPTs的定义，可能大部分人认为只需要添加一段提示词，甚至这些提示词可以自动生成。在我们已经看到的智能体应用中，也有添加一个或多个函数的情况，这算不算是一个Agent？我认为这些做法可能会使Agent的定义变得更加模糊。因为我们进行这些尝试的目的是为了解决大语言模型的一些问题，因此才会有这样的定义。</p><p></p><p>我们发现，Agent的主要研究方向包括四个方面。首先是记忆。语言模型本身没有任何记忆，它与人类的记忆结构完全不同。人类的记忆分为工作记忆、短期记忆和长期记忆，而在语言模型中，我们只能得到工作记忆的粗略等价物，短期记忆和长期记忆基本上是无法实现的。这是因为从原理上讲，现有的语言模型是对现有世界所有知识的压缩，它只能做一件事情，很难进行除压缩外的其他大部分增量工作。</p><p></p><p>人脑则是通过一系列非常特殊的机制形成记忆。一般来说，长期记忆的形成需要两周到两个月的时间，短期记忆的形成所需时间更少。但无论是长期记忆还是短期记忆，它们都是以分布式的方式存在于我们的大脑中，这意味着我们的神经元本身是存算一体的。</p><p></p><p>现在的Agent 要实现记忆，大家可能会自然地想到RAG。然而，RAG与人类的记忆有很大不同，因为人的记忆是有基础的可靠性保障。益于神经元连接的强度保障，一旦某些东西被强行记住，人们就很难或不会忘记。当然，人的记忆并非完全可靠，但它依然比现有Agent的RAG方式更为可靠。根据人类以往对AI的经验，只有当Agent能够广泛超越人类的记忆能力时，我们才会认为它是可靠的。</p><p></p><p>陈浩蓝：我对吴老师的观点深有同感。目前我们设计Agent时，往往认为输入的prompt就是记忆。但实际上，一件事不一定非得以文字形式存在，它可能是一个模糊的概念、一个念头，或者是我们神经元中的一组参数。我认为，当前的Agent只是目前技术水平下，在LLM工具上临时增加的辅助工具，它不一定是最终形态的智能体。</p><p></p><p>吴承霖：Andrej Karpathy在最近发布的推特中提出，计算机2.0的未来可能由一个语言模型直接接管并执行所有逻辑。我对他的观点既有认同也有保留。因为目前代码逻辑的整体效率可能比语言模型的权重逻辑更高，我们可以将权重视为另一种形式的代码，并且能够执行一些较为模糊的推理。</p><p></p><p>从这个角度来看，目前的智能体更擅长控制计算机的底层操作，如果要达到极致，自然语言编程可能是一个必经之路，但我们先不讨论这个话题。目前，智能体还需要解决一系列关键问题。首先是多步推理，究竟是应该由智能体来解决，还是直接包含在大语言模型内？这个问题尚未被充分讨论。外部流传的OpenAI的Q*、Claude Sonnet等用于数据合成的迭代方法，或多或少会用到多步推理的技巧。</p><p></p><p>人类的话，无论什么样的人都有推理过程，这个过程可长可短。一般来说，我们会将其描述为一系列的推理算法。在推理过程中，我们可能会考虑对下一个状态的预测、对下一个动作的预测以及对价值的预测，这些问题在整个行业中尚未被充分讨论和解决。</p><p></p><p>可能八年前的AlphaGo和一系列相关工作解决了一部分特定子领域的问题，但我们认为，从AlphaGo的推理到相对比较通用的状态可能需要两年时间，可能要到明年的下半年这些工作才会被完整地推进。当这些工作完全完成时，我们会发现有一些大的进步，比如幻觉问题可能会得到大幅度解决。</p><p></p><p>我们认为Foundation Agent很可能在明年年底诞生。它可能会有许多特性。首先，它可能会理解大部分应用、能够执行人类能力范围内的大部分工作；其次，它将拥有一个不同于大语言模型的心智模型，使其能够基于权重对现实世界的任务进行推理。当然，它可能还会有许多其他特性，比如自带工具。</p><p></p><p>但在这一过程中，我们会遇到许多问题，比如如何在足够丰富、真实的世界数据上进行训练，这可能是所有问题中最关键的。总之，我们认为可能会在一年半内出现一个Foundation Agent，它可能是我们真正称之为Agent起点的抽象。</p><p></p><p>陈浩蓝：Foundation Agent的具体定义是什么？</p><p></p><p>吴承霖：我们所说的Foundation Agent，更多是对其能力的一个描述，它能够理解当前的现实世界，包括屏幕中的特定应用和相关的交互形式，并且能够理解和交互现实世界、物理规律、三维条件与时间等因素混合起来的事件。更准确地说，它可以应用于许多不同的场景，例如，将来可能有许多智能体存在于云端，但它可以在云端操作一些虚拟机，如虚拟手机或虚拟PC；可以让一些化学实验室、工业实验室自动化运行。</p><p></p><p>但在这个过程中会有许多问题，其中最关键的是数据问题。例如，这个过程中有哪些通用的数据收集方式，这是所有人绕不开的问题。</p><p></p><p>陈浩蓝：如果我们这个行业真正出现了一个Foundation Agent，我怀疑它可能没有一个复杂的Agent架构，它就是一个极其强大的多模态模型，类似于大脑中的神经元。它可能不是按照达尔文进化论描述的那样，由不同模块按某种逻辑组织在一起然后共同工作，而是一团能够接受不同输入的神经元，中间有复杂的参数，在大量数据的冲刷下，最终能够搜索出一套网络结构，然后再进行各个部分的分区。</p><p></p><p>数据问题确实是特别关键的。如果有人问Agent技术发展面临的最大挑战是什么，我认为就是数据问题。我们现在所做的一切都是对理想情况的一个近似。我们这个世界及其复杂程度，远非AlphaGo那样的19x19 世界可比，我们这个世界缺乏这样的规则，这也导致我们给模型的输入是不够的，我们只能用人类能够抽象出来的方式提供足够多且高质量的样本。</p><p></p><p>例如，我们认为一个Agent需要使用工具，我们就会给Agent添加一个使用工具的组件，并训练这个组件在需要的时候启动。但一个更好的方式是，让Agent在现实模拟器中自行运行100万遍，然后自己学会使用工具。这种深度推理的样本在我们的现实世界中太少了，我们不得不使用一些原始的样本训练模型，并在这个过程中让模型自己去制备推理逻辑更复杂的样本，以增强自己的能力。</p><p></p><p></p><h4>合成数据带来的成本问题</h4><p></p><p></p><p>InfoQ：合成一些对现实世界认知之外的、更高级的数据，目前大家很难在技术上实现。</p><p></p><p>吴承霖：核心数据是所有人都在追求的一件事，但它也会带来巨大的开销。一个核心问题是，如果你能合成3倍、10倍甚至30倍的数据，那么最终的倍率是多少、迭代的次数是多少？并且，随着数据倍率的增加，所需成本也在等比例增加。假设架构不变，即仍然依赖Transformer进行多层GPT-like架构的构建，这实际上是不经济的。</p><p></p><p>尽管我们可以合成一些数据，但它带来的边际效益并不显著，反而推高了整体成本。以目前的数据来看，在Claude Sonnet 3.5版本中，其合成比例已经非常高，如果有多达30倍的合成数据，那么成本也要乘以30，训练成本急剧上升。</p><p></p><p>这就引出了另一个话题：现在的语言模型架构合理吗？或者说高效吗？与人脑相比，它一点都不高效，因为人脑看一次样本就可以学会，而现在语言模型需要大量的数据来喂养。在ICL（In Context Learning，上下文学习）的背景下，提供一个样本可能会有一些效果，但这似乎并不是它的正规学习方式。</p><p></p><p>从功耗角度讲，一般来说，人脑的整体功耗大概是GPT的一万倍到十万倍，因此现在GPT-like模型的整体效能并不高。如何降低训练成本，使其能够合成更多比例的数据，可能是之后最大的研究方向。OpenAI在去年发布GPT-4时就明确表示要进行这项工作，但现在能做好的团队并不多。</p><p></p><p>目前来说，使模型能够自我提升的方法没有上限，但不可避免的是，这些方法都很昂贵，随着迭代次数的增加，整体成本也在增加。去年底到今年初，绝大部分团队只能迭代三次。我们注意到，在过去的一两个月里，一些团队已经有能迭代十次以上的方法，更多的次数就是通过自己左脚踩右脚实现自我提升的。我们目前还没有看到上限。但不可避免的问题是，所有方法都非常昂贵，迭代次数越多、合成数据越多，整体成本就会比之前高一个数量级到两个数量级。</p><p></p><p>陈浩蓝：我理解，迭代的本质上可以说是将人类大部分的知识压缩在文字里面，通过反复琢磨这些文字，最后“悟”了。它“悟”的来源实际上是所有的文字，但我不确定人类所有的文献语料加起来是否能够实现完全的智能，我觉得这条路可能也是有极限的。</p><p></p><p>吴承霖：对，Ilya在2015年的观点确实很明确，他认为压缩即智能。然而，他对智能的定义更多地侧重于推理能力，并没有包括记忆和长期交互。因此，实际上大家对“智能”这个词的定义可能会有所偏差。</p><p></p><p>从纯粹的推理能力来看，目前GPT-4和Claude 3&nbsp;opus的整体智力水平大约在101左右，而国人的平均智力水平在106左右，它们尚未超过平均智力水平。这里需要从两个角度来看：一是知识，一是智力，两者完全不同。知识可以通过记忆获得，但智力则需要通过推理逻辑来实现。如果问大模型何时能大规模应用，关键在于它何时能达到智力的临界变化点，或者记忆的临界变化点，这两个变化点可能都很关键。例如，如果它的智力达到130，你问它大部分问题它都能立即回答，不需要依赖记忆，这时它可以大规模应用，我们也不需要构建一些复杂的架构。这可能是一个五年左右会发生的事情。</p><p></p><p>另外一方面，人的记忆分为内隐记忆与外显记忆，那是否有其他的方式能够进行记忆的代偿？实际上是有的，过去一年中，一些团队已经取得了显著的成果，但他们开发的机制可能与人类大脑的机制不同。因此，我认为，硅基生命的最终存在形式大概率与人类的存在形式不同。</p><p></p><p></p><h2>Agent 为何表现比单个大模型更亮眼</h2><p></p><p></p><p>InfoQ：那么现在落地上的一些应用，有哪些让两位印象深刻的地方？</p><p></p><p>吴承霖&nbsp;：现在业界主要有四个大的方向：</p><p>语言模型：例如，ChatGPT通过订阅服务获得了20亿美元的ARR（年度经常性收入），这在人类商业历史上极为罕见，它可能是SaaS领域增长最快的一家。要在这个赛道中取得成功，最核心的要求是成为稳定领先的第一名，这样才能有显著的品牌影响力。代码：在这个领域，GitHub Copilot已经取得了显著的影响力和商业收入。面向开发者的服务是一个地域性场景，目前在中国还没有很有影响力的公司。在北美，GitHub Copilot的ARR已经超过了1亿美元。泛娱乐：这个方向可以分为游戏和非游戏两个市场，两者之间的区别较大。游戏方面，如陪玩等服务；非游戏方面，如通过简短文本生成小说、漫画、视频等内容。泛娱乐市场非常大，像抖音等具有很大的影响力和商业收入。具身机器人：国内有许多优秀的具身机器人公司，但这个领域仍然是一个未被充分探索的、需要证明商业化可行性的方向，尽管它非常具有吸引力。</p><p></p><p>此外，还有许多其他市场方向，如可视化编程的Agent平台、基础设施和中间件等。</p><p></p><p>陈浩蓝：Agent本身可能只是大模型的一个过渡或中间状态，随着底层模型能力和视觉模型的逐步完善，它们最终可能做的是相同的事情、实现相同的目标。</p><p></p><p>InfoQ：前段时间吴恩达分享自己用 GPT-3.5 做的一个Agent，整体的工作流表现要高于用 GPT-4。这个原因是什么？</p><p></p><p>吴承霖&nbsp;：从流程工程的角度看，MataGPT 本身是一个大型的流程工程，我们会使用SOP（标准操作程序）来定义这些流程。SOP的本质是最佳实践流程，我们有许多典型的SOP，如敏捷、迭代、瀑布等，中间会有许多具体的SOP细分。许多领先公司也有大量的SOP，例如国内SOP最多的公司可能有上千个SOP来确保流程的顺畅运行。</p><p></p><p>流程工程本质上与SOP是一致的，我们用相同的方法对待人和智能体。由于智能体现有的局限性，我们的SOP需要更加精确，因为人的记忆有上下文，但智能体需要精心设计其上下文，以确保它能准确理解你的问题。特别是现在的Agent或LLM通常是无状态的，它们不会记住任何东西，所有信息都需要你提供给它们。</p><p></p><p>OpenAI也有许多实现，例如OpenAI的助手就是其API模块的一部分。但据我所知，之前Lang Chain和Llama Index的测评认为，助手模块只是一个高资源消耗的RAG模块，只是尽可能将RAG推向极限。然而，即使RAG达到极限，也很难满足我们的所有需求。</p><p></p><p>例如，在某些特定场景的问题上，我们脑海中可能想到了对应的场景，但我们不会明确提及这些场景。这意味着RAG或其他简单的召回形式，包括现在流行的主动召回形式，可能很难解决现有问题。当然，一些公司和团队正在开发外置记忆模块，但这些工作尚未证明一个通用记忆模块功能的普适性。</p><p></p><p>陈浩蓝：刚刚提到的人和大模型最大的差别在于，人能够进行one-shot learning（单样本学习），而大模型则需要few-shot learning（小样本学习），这也不一定总是正确的。例如，如果我们需要开发一个新特性，即使是人脑可能也无法立即理解它，需要一些交互来形成最终的定义。比如，产品经理说“要做一个特性，这个特性就是跟这个一模一样”，人脑对这句话的理解也可能不够充分，同样需要做一些交互来给出最终定义。</p><p></p><p>吴恩达通过结合Agent和GPT-3.5能够获得比GPT-4更好的效果，这可能更多地归功于问题阐释上的提升。例如，在解决特定问题时，人类会有相应的SOP，但如果仅仅是一次调用，那不一定是大模型的问题，有可能是人类语言协议的问题。一句话可能无法清楚地表达你想要什么，但通过反复沟通或遵循一套SOP进行沟通，实际上最终能够达到你想要的结果。</p><p></p><p>吴承霖&nbsp;：我有一些补充。我们在面对许多问题时会觉得难以解决，但通过逐级分解，会发现问题的难度在逐级降低。为什么问题难度可以可以通过逐级分解而降低？这是一个很有意思的话题。一个问题中的原子化问题是什么？提出原子化问题需要什么样的技能？这些可能可以依赖语言模型或人脑来完成。这样的机制对当前的Transformer架构是有意义的。当前堆叠出的语言模型的推理步长是固定的，超过某个推理复杂度它就无法继续推理。但如果我们能把一个复杂度为10的任务拆分成k个复杂度为7的任务，再拆分成m个复杂度为5的任务，逐级降低复杂度，这个任务就会变得可解。</p><p></p><p>人实际上也是用类似的方式处理问题的。在软件开发中，有产品经理、架构师、工程师等不同角色，他们都在拆解问题与解决拆解问题。我们最终可以总结为：输出实际上改变了内部权重，或者说更改了它的上下文，使它的输出分布发生了变化。但目前还没有一个成体系的理论来说明难度降低了多少，以及最小的可解问题是什么。</p><p></p><p>陈浩蓝：问题的拆解是一个非常有前景的方向。只要我们能够把问题无限细拆，最终它一定能够被简化并解决。</p><p></p><p>我们之前讨论过一个问题，即Agent落地会遇到哪些挑战？例如，在一个To B或离线场景下，我们可以大量进行这种拆解和多步推理，最终获得一个相对较好的结果。当然，这样的推理成本是较高的。像吴老师做的MetaGPT，每个代码都还有推理预算的限制，这也反映了我们在设计一个极其复杂的Agent时，应用的成本是需要充分考虑的。</p><p></p><p>我专门查了YC 2024年入营项目中Agent项目的分布，发现大部分都是To B的，个人陪伴和娱乐是在一个非常窄的角落。我们最开始设想一个Agent有特别复杂的架构、具有超强的智能，但是我们发现很多成功的产品，如CharacterAI、海外的Talkie等，它们的Agent架构非常简单。这不是说大家没办法把它做得特别智能，而是最后在用户响应耗时和开销之间大家做了平衡和选择，这其实也是合理的。</p><p></p><p>所以，我觉得在一些追求AGI 的场景，我们可以把Agent设计得特别复杂，让它不断地推理。但对于一些在线服务、娱乐场景，它可以简单展现大模型和Agent的能力，同样也能较好地满足用户需求。</p><p></p><p>InfoQ：更复杂带来的延迟性会不会变高？如何解决？</p><p></p><p>陈浩蓝：我认为这还是取决于任务。以吴老师的产品（软件公司多智能体）为例，它的响应时间虽然比说一句话要长，但比我自己开发肯定是要短。在现在的技术架构下，未来的推理速度有几个数量级的提升都不成问题，可能更多要考虑的是在不同问题场景下应用不同架构的复杂度。</p><p></p><p>吴承霖 ：它的推理速度实际上是在显著加快的。Groq的推理速度可能是OpenAI的30倍左右。这些问题都有特定的解决方案，现在的问题放在一年以后可能已经不再是问题了。</p><p></p><p>这里也没有明确的定义Multi-Agent。Multi-Agent最初是一个用来构建框架的基点，但我们从来没有精确的定义过，但是常见的定义是否准确吗？比如：每个角色有不同的提示词、不同的工具，它需要不同的模型吗？不同的记忆模块吗？其中还需要探究更多细节。但回过头看，它的速度和用户体验并不是一成不变的，一年以后大概率单跳的速度会提升几倍到十倍，这意味着现在能接受一跳的时间，明年你就能接受3跳到10跳的时间。这意味着等待可能不会成为一个特殊的问题。</p><p></p><p>如果我们要把人的职业和Agent做映射的话，以OpenAI在2022年的定义，大约有20%的职业会完全被语言模型影响，80%的职业会受到影响。随着Agent能力、语言模型能力越来越强，这个20%和80%的比例会快速变化，这意味着它可能不仅仅是用户体验的问题，而是市场最终选择的问题。</p><p></p><p></p><h2>Multi-Agent 与人类分工有什么异同</h2><p></p><p></p><p>InfoQ：如何从组织角度定义多智能体？</p><p></p><p>吴承霖 ：在OpenAI的调研中，人类职业大约有2,000种。这2000种职业是从大约400年前开始逐渐发展的，所有的一切可能源自亚当·斯密的分工理论。人类文明进入工业社会后，不可避免地要摆脱农耕形态并进行分工，以提升整体的社会效率，这时职业才大规模产生。之后，我们才发现，一个组织需要有不同的职业来形成一个最优结构，以获得最大效率。</p><p></p><p>Agent实际上和人类的分工相似，但并不完全相同。</p><p></p><p>在西方社会，很多很强的个体公司可能会模糊所有的职业，比如所有人都叫工程师。更进一步，我们可以看到像Google、微软、Amazon等公司，每一家的拓扑结构都完全不一样。例如，Google可能更多会采用OKR的形式进行360度的绩效评估，而Amazon则是一个非常典型的以To B为起点的紧密小团队结构。</p><p></p><p>每家公司的组织结构分工、职业上升路径可能完全不一样，但这并不意味着其中有任何一家不合理，因为他们面对的社会形态和商业环境迫使他们形成了这些结构。因此，Agent大概率也会形成很多结构，这些结构不一定是我们预定义的，最终是因为市场的需要才会让这些结构存在。</p><p></p><p>陈浩蓝：如何定义Agent？定义Multi-Agent？实际上是我们做出的一种“不差”的选择，即参考人类社会来定义。为什么这是一个不差的选择？因为我们知道历史上有其他分工方式，但这些方式已被淘汰，而现在的分工方式能在人与人之间正常运作。此外我们充分了解每一个角色，例如一个产品经理写出的PRD（产品需求文档）是什么样的，这样可以很好地评估对应的工作样本，了解工作是否在正常开展。</p><p></p><p>当然，可能还有另一种更高效的分工方式。这种更高效的分工可能是由一个Agent负责编写头文件，其他Agent负责随机生成代码，这也是一种分工形式，但它尚未经过验证，且没有一种能够批量、廉价找到优质样本的方法。因此，我认为这还是基于现状做出的一种不差的妥协。回到刚才的话题，Agent随着外部环境和内在能力的差异，可能会形成其他更优的分工方式，但这种更优的分工方式仍然需要一个上帝模型或世界模型给予足够多、足够快的反馈，以使组织架构能够迭代。</p><p></p><p>例如，吴老师在游戏中做了一些Agent，我认为这还挺令人兴奋的，因为我们可以认为游戏本身就是一个小的世界模型（围棋最小，现实世界最大，游戏介于两者之间），那在游戏规则下，我们实际上可以充分验证其环境和奖励是否能很好地刺激Agent协作，最终在游戏内部形成一种分工。</p><p></p><p>当然，在游戏中单纯执行动作的话，已经有大量能做得很好的工作了。但在游戏中进行社交、聊天、探索，这些方面仍然值得研究。假设我们现在这个世界是一个性能更好的AI模拟出来的，那么现在人类的分工也是一种Multi-Agent的分工。如果我们能够模拟出一个小的环境，我们就可以逐步探索新的Multi-Agent组织形式。</p><p></p><p>InfoQ：Multi-Agent能否借鉴 MOE 的思路？</p><p></p><p>吴承霖 ：MOE（混合专家模型）主要用于语言模型内部的路由。MOE的工作现在做得非常多，有很多人在做记忆时会采用MOE + LoRA的方式。那么，Multi-Agent是否能够使用类似MOE的方式？实际上，很多公司已经这样做了，也有人验证了它的效果是很好的。例如，Samba-CoE v0.3验证了几个开源模型组合在一起能够超越之前的最优模型。</p><p>当然，也有很多其他类似思路的模型，虽然做得并不完美，但我们倾向于认为这是退化成为机器学习的&nbsp;Ensemble形式。Ensemble是一个经过充分讨论的话题，在这之前可能有数千篇到数万篇文章都是围绕这个话题展开的，可能有更多可参考的工作。</p><p></p><p>陈浩蓝：我稍微补充一点。我认为Multi-Agent和MOE还有一些差别。MOE更多是逻辑上的组合，而Multi-Agent或workflow还包括时序上的组合。例如，Agent A完成推理后，可能会将信息传递给Agent B进行下一步推理；而MOE则是每次激活特定的专家，让他们进行一轮推理。因此，我认为，至少在目前的架构中，这两个并不是严格等效的。</p><p></p><p>但如果有人开发出“Recurrent”&nbsp;MOE，可能会创造出一个相当于序列的MOE，这样每次推理就会在时序上有了先后依赖关系，最终获得一个更好的结果。</p><p></p><p></p><h2>Agent 的多样应用</h2><p></p><p></p><p>InfoQ：单一Agent 与多智能体的应用场景有什么不一样吗？</p><p></p><p>吴承霖 ：之前阿里数学竞赛有一个AI赛道，第二名和第三名都是MetaGPT的贡献者，他们都是通过多智能体赢得比赛的。多智能体在许多不同的比赛中都展现出了非凡的效果，包括我们参与或关注的一些比赛中，绝大部分排名靠前的架构都是多智能体。这主要是因为智能体能带来工具、动作、记忆等不同维度上的细分，相当于他们有了记忆，有了特定的先验行为，随之发展出了一系列不同的行为，并成为他们的经验。</p><p></p><p>InfoQ：两位目前觉得最好或印象深刻的Agent 落地案例是什么？</p><p></p><p>吴承霖：这取决于Agent的定义是什么。如果说现在企业内部的private search（私有搜索）是一个Agent，那么它的定义会比较宽泛。许多企业内部有private search，做得也很成功，还可以做搜索总结、比较精细的调研，甚至可以出财报、review法律合同等。在一定意义上，我觉得这确实是一个Agent，北美有很多这类做private search和比较偏SaaS search的公司。</p><p></p><p>所以说，核心是我们如何定义Agent。假如说应用大语言模型获得了生意和融资，我们认为它就是成功的话，那么这个定义可能太宽泛了。但如果从一年后的Foundation Agent节点回过头来看，这个概念可能又过于狭隘。</p><p></p><p>陈浩蓝：我觉得企业内的应用可能比大家说得要更细、更激进一些，但又比吴老师说的更保守一些。</p><p></p><p>就我观察，我们公司内部的AI应用还是铺得比较开的，各个场景都在寻找结合点。但是，我觉得很多场景其实也不是所谓的AI原生应用，我很少见到仅用单个语言模型来处理的，基本上都是一个Agent或Multi-Agent，还有像混元workflow编排可以把整个流水线通过配置的方式生成；在业务上，像腾讯会议的会议纪要、文档处理等都做了各种各样的尝试，还有浏览器里的文件、网页阅读助手等，这些还是比较激进的。大家也都希望能够探索到哪些是用户真正需要的功能、哪些是伪需求。</p><p></p><p>我个人印象比较深刻的是，我们现在有的广告素材生成其实就是由一个Agent来做的。可能跟开源工作或行业内宣传得比较多有关，大家认为Agent很多时候对应的是一个bot，但其实也有很多离线的workflow。例如，我们要生成一个广告素材，首先找到这个商品的核心卖点，结合核心卖点和一些大数据产生视频素材的脚本，然后输出一些文案，有了文案后可以用大模型产生对应的分镜脚本，有了分镜脚本后再产生关联原始素材，有了这些后再进一步进行视频的合成及自动审核。这个工作是离线的，相对比较复杂，实际上也确实给业务带来了一些提升。</p><p></p><p>InfoQ：人形机器人是否可以认为是一个Agent，给它内置大模型和知识库，然后可以通过互联网摄像头、音频进行自我学习和迭代？</p><p></p><p>吴承霖 ：Robot一般来说分为四个流派：强化学习、模仿学习、RFM（ Robotics Foundation&nbsp;Model）和&nbsp;PRL（程序强化学习），第三和第四个流派基本上都要用VLM（视觉语言模型）或LLM（大型语言模型）来做，也是目前最主流的流派。所以基本上现在具身机器人都是在大语言模型之上去做的。</p><p></p><p>当然，也有很多直接用VLM+LLM训练的，比如说直接加一个PPO（Proximal Policy Optimization，近端策略优化），也有人想再加上DPO（Differentiable Policy Optimization，可微策略优化）。这使得训练语料在文本上会比较好构造，但在视觉环境，尤其是三维的视觉环境下，PPO会更简单一些。</p><p></p><p>InfoQ：多智能的社会属性是可以从大模型单一架构中涌现出来，还是需要更多的符号注入去强化？</p><p></p><p>吴承霖 ：这其实是Neuro-symbolic方向的问题。吴恩达在前一段时间发表了一个博客，其中他提到Neuro-symbolic是未来最有希望的方向之一。这个观点肯定没错，在过去几十年Neuro-symbolic一直是非常主流的学派。</p><p></p><p>图灵机诞生之后，编程也变得真正可行，我们现在做的语言模型和智能体也只是让它能够更好地去做模糊推理。之前的编程我们可能更多会认为是基于集合的精确推理，因为计算机本质上就只能做集合的事情。模糊推理和精准推理结合在一起，才能真正形成智能。我们不是很确定最后需要多少符号，但从整个业界的认识看，我们认为Neuro-symbolic是非常有希望的一个方向，也可能是学术上会出大量论文的一个方向。</p><p></p><p>陈浩蓝：稍微补充一下，我认为短期内，符号计算的方向基于Multi-Agent的框架会更主流一些，长期的话应该还是从LLM的单一架构中涌现出来，然后再加上世界模型。</p><p></p><p>吴承霖：这里我要提出一个很有意思的观点，我认为LLM是涌现不出物理规则的。现在的物理公式不是自己搜出来的，而是一个人为定义的东西，是那种虚幻引擎里面编辑器的配置，我觉得LLM可能很难在底层方面写出来这些东西。</p><p></p><p>陈浩蓝：但我觉得如果一个生物的LLM能够写出来，那没有理由一个跟它同构的另一个架构写不出来。</p><p></p><p>吴承霖 ：生物的LLM经过了充分的推理，然后完成了论文的输出，所以它需要一个标准的流程，我们称为critical thinking。这个过程大概率语言模型也得走一遍。它的智商能到一个很高值，但同样也得按生物的逻辑从某一个点开始往后进行推理，这个推理过程存在，但很难发生在它的网络内部。</p><p></p><p>陈浩蓝：我之所以得出之前的观点，其实也借鉴了推荐系统或者传统的对话系统发展的路线。最开始可能是一个比较简单的工作，后来人类或者相关研究者会往里加入各种各样的先验知识，然后让它短期能够获得比较大的提升。随着整体算力的增加，最终它又会被千亿级的LLM替代之前整个chatbot的各种精细设计。我感觉随着LLM的继续发展，Multi-Agent架构虽然不是绝对的符号计算或大模型涌现，但是它的倾向性会逐步由后者向前者转换。</p><p></p><h4>活动推荐</h4><p></p><p></p><p>8 月 18-19 日，AICon 全球人工智能开发与应用大会将在上海举办。来自字节跳动、华为、阿里巴巴、微软亚洲研究院、智源研究院、上海人工智能实验室、蔚来汽车、小红书、零一万物等头部企业及研究机构的 60+ 资深专家，将带来 AI 和大模型超全落地场景与最佳实践分享，帮助与会者提升技术视野、获得有价值的实践指导。大会火热报名中，7 月 31 日前可以享受 9 折优惠，单张门票节省 480 元（原价 4800 元），详情可联系票务经理 13269078023 咨询。</p><p></p><p><img src="https://static001.geekbang.org/infoq/35/35014bd5f1e8c93fd4cc748450969079.webp" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/j3IFYoevydyrhI1hDOay</id>
            <title>拜登又要出芯片新规！六家中国头部厂商遭禁，新增 120 家实体，美国的盟友先拍桌子了！</title>
            <link>https://www.infoq.cn/article/j3IFYoevydyrhI1hDOay</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/j3IFYoevydyrhI1hDOay</guid>
            <pubDate></pubDate>
            <updated>Fri, 02 Aug 2024 01:51:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 美国政府, 芯片新规, 中国半导体企业, 出口管制
<br>
<br>
总结: 美国政府计划发布一项新的芯片出口管制规定，将扩大对中国半导体企业的限制。这项规定将影响中国最先进的芯片制造厂商，并限制与中国公司进行出口合作的台湾等国家的半导体公司。同时，美国还计划将约120家中国实体公司列入限制贸易名单，但豁免了其他30多个国家。美国的出口管制措施旨在保护国家安全并维护技术生态系统，但也可能引起盟国的反应。 </div>
                        <hr>
                    
                    <p>整理 | 华卫</p><p></p><p>7 月 31 日，据外媒报道，两位消息人士称，下个月美国政府计划公布一项芯片新规，该规定将扩大美国阻止他国向中国芯片制造商出口半导体设备的权力。</p><p></p><p>据其中一位消息人士透露，这项新规定是对所谓的《外国直接产品规则》的扩展，将禁止大约六家中国半导体企业获得来自其他众多国家的出口产品，而这些企业是中国最先进的芯片制造厂商。</p><p></p><p>目前，暂无法确定哪些中国芯片厂将受到影响。已知的是，位于以色列、台湾、新加坡和马来西亚的半导体公司将被限制与中国公司进行出口合作，台湾是芯片制造巨头台湾积体电路制造公司（TSM）的所在地。</p><p></p><p>一位不愿透露姓名的消息人士表示，日本、荷兰和韩国等出口关键芯片制造设备的美国盟友将被排除在外，从而限制了该规则的影响。</p><p></p><p>因此，譬如 ASML 以及东京电子等芯片设备制造商都不会受到影响。消息一出，这两家公司的股价均大幅上涨。当日，ASML早盘交易中股价上涨 6.5%，东京电子股价收盘上涨 7.4%。其他日本芯片相关设备制造商也取得了强劲增长，Screen Holdings 股价上涨9%，Advantest 上涨4.5%。</p><p></p><p>注：名为《外国直接产品规则》（FDPR）的条款于 1959 年首次出台，旨在控制美国技术贸易。该条款的基本内容是，如果某种产品是使用美国技术制造的，美国政府有权阻止其销售，包括在外国制造的产品。2022年10月，美国将该规则应用于中国先进计算和超级计算机行业，以阻止其获取先进计算芯片。</p><p></p><h1>限制名单新增 120 家中国实体，其他30多个国家被豁免</h1><p></p><p></p><p>近年来，《外国直接产品规则》一直被用来限制中国科技巨头华为在海外的芯片生产活动。华为在与美国的限制作斗争后进行了自我革新，现在已成为中国先进芯片生产和研发领域的核心企业。该规则还于 2022 年被用来切断中国与世界任何地方生产的某些半导体芯片的联系。</p><p></p><p>消息人士称，作为最新出口管制方案的一部分，美国计划进一步降低界定外国产品是否受美国管控的美国技术成分比例。并补充称，此举将弥补《外国直接产品规则》中的一些漏洞。</p><p></p><p>这意味着，受美国出口管制方案影响的产品范围将进一步扩大。举例来说，某些设备可能仅仅因为内置了含有美国技术的芯片就被指定为属于出口管制范围。</p><p></p><p>美国还计划将约 120 家中国实体公司列入其限制贸易名单，其中包括受该规则影响的晶圆厂以及工具制造商、EDA（电子设计自动化）软件供应商和相关公司。据悉，名单上的实体供应商需要获得许可证才能向其发货，但这些许可证的授予很可能会遭到拒绝。</p><p></p><p>消息人士称，计划中的新规则仅处于草案形式，之后还可能会发生变化，但下个月发布的目标是确定的。</p><p></p><p>除日本、荷兰和韩国外，该规则草案还豁免了同属A：5 集团的其他30多个国家。截至 3 月 15 日，A：5 国家的名单包括加拿大、德国、日本和荷兰等 37 个美国主要盟友。</p><p></p><p>美国商务部在其网站上表示，其 “根据外交关系和安全关切等因素 ”对世界各国进行分类，这些分类有助于确定许可要求和简化出口管制条例，以确保国际贸易的合法和安全。</p><p></p><p>A：5国家的豁免条例对荷兰光刻公司ASML等企业至关重要，ASML是唯一一家尖端极紫外光刻（EUV）设备制造商，其第一季度近一半的收入来自中国。ASML 已经受到美国现有的限制： 自今年 1 月起，ASML 的 EUV 光刻机和较老的深紫外（DUV）光刻机均不得销往中国。</p><p></p><p>该计划中的豁免条例也表明，美国在实施限制措施时需要采取外交手段。一位不愿透露姓名的美国官员说："有效的出口管制依赖于多边支持。我们不断与志同道合的国家合作，以实现我们共同的国家安全目标"。</p><p></p><p>注：美国《出口管制条例》（Export Administration Regulation, 以下简称“EAR”） 中的国家组别清单（Country Groups）将全球各国家（地区）分为A、B、D、E四组，不同组别获得许可证例外的优待程度不同，特定物项的额外管控要求也不同。优待程度越高的组别，意味着美国受控物项的出口或再出口到相应组别国家，能够享受更多的许可证例外，且特定物项基于CCL（管制清单）及最终用途/最终用户的额外管控也较少。</p><p></p><p></p><h1>“美国不会放弃对中国的技术限制”，但不希望激怒盟友</h1><p></p><p>当被问及即将出台的出口管制方案时，中国外交部发言人林建表示，美国“胁迫其他国家打压中国半导体产业”的做法破坏了全球贸易，损害了各方利益。</p><p></p><p>林建还补充道，中方希望有关国家能够抵制美国的努力，维护自身的长远利益。他指出，“遏制和打压阻挡不了中国的发展，只会增强中国自主发展科技的决心和能力。”</p><p></p><p>美国商务部发言人则在一份声明中表示：“美国商务部正在不断评估不断变化的威胁环境，并在必要时更新我们的出口管制，以保护美国国家安全并维护我们的技术生态系统。我们将继续致力于和与我们拥有共同价值观的盟友密切合作。”</p><p></p><p>为了阻碍可能使中国军方受益的超级计算和人工智能突破，美国在 2022 年和 2023 年对中国先进芯片和芯片制造设备实施了出口管制，限制了来自加州的 Nvidia以及 Lam Research 等公司的出货量。</p><p></p><p>去年，在华盛顿认识到几个关键国家达成一致的出口管制措施是必要的后，美国与日本和荷兰达成协议，限制半导体制造工具出口到中国。一直以来，这三个国家在先进芯片制造设备的生产中占据主导地位。</p><p></p><p>消息人士称，美国一直试图对该协议增加更多限制，并让韩国和德国加入该联盟。</p><p></p><p>而此次新规则中对盟国的豁免权，或是迫于美国大选和盟国反应两方面的压力而采取的应对措施。华盛顿战略与国际研究中心研究员James Lewis谈到，“他们在使用这项规则时非常谨慎，因为这会让我们的盟友感到不安。如果不让人们跳船，你只能把这项规则推到一定程度。”</p><p></p><p>半个月前，有外媒报道称，美国试图进一步严控ASML与东京电子等国际芯片大厂对中国大陆供货。当时，这一消息放出之后，东电大跌7.5%，ASML下跌11%，美国芯片设备制造商Applied Materials和Lam Research也应声大跌。</p><p></p><p>日本、荷兰等美国盟友认为，在美国总统大选即将到来之际，并没有必要实施限制措施。而且，美国芯片公司也感受到不公平的出口管制挤压。据报道，包括Applied Materials和Lam Research 在内的一些美国公司告诉美国官员，这样的贸易限制措施损害了他们的利益，但对阻碍中国的发展也不那么有效。</p><p></p><p>目前，这项新规则还处于草案阶段，也表明华盛顿正寻求在不激怒盟友的情况下对中国蓬勃发展的半导体产业保持压力。</p><p></p><p>但Lewis 表示，“美国不会放弃对中国的技术限制，欧洲人获得了暂时通行证，（其他）国家也获得了暂时通行证。但这项规定就像一个承诺，我们会继续努力这样做。”</p><p></p><h1>结语</h1><p></p><p></p><p>目前还不清楚更新后的《外国直接产品规则》会对阻碍中国的芯片发展有多大影响，因为世界上有近五分之一的国家被豁免。</p><p></p><p>不过，这可能会鼓励中国及其国内科技公司与美国友好国家开展更多业务，至少在向中国出口不违法的产品方面是这样。</p><p></p><p>参考链接：</p><p><a href="https://www.reuters.com/technology/new-us-rule-foreign-chip-equipment-exports-china-exempt-some-allies-sources-say-2024-07-31/">https://www.reuters.com/technology/new-us-rule-foreign-chip-equipment-exports-china-exempt-some-allies-sources-say-2024-07-31/</a>"</p><p><a href="https://www.theregister.com/2024/07/31/us_export_rules_chipmaking/">https://www.theregister.com/2024/07/31/us_export_rules_chipmaking/</a>"</p><p><a href="https://qz.com/biden-weighs-trade-rules-block-chipmaking-exports-china-1851596265">https://qz.com/biden-weighs-trade-rules-block-chipmaking-exports-china-1851596265</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/RG9SdRwoBbltWJsRXF94</id>
            <title>服务器仅靠 4 颗 CPU 运行千亿大模型的“算法秘籍”</title>
            <link>https://www.infoq.cn/article/RG9SdRwoBbltWJsRXF94</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/RG9SdRwoBbltWJsRXF94</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Aug 2024 09:12:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 巨量模型, AI加速卡, 通用服务器, 大模型推理
<br>
<br>
总结: 巨量模型的智能生产力正在逐步渗透到各行各业，但它们的部署和运行通常需要专用的AI加速卡，能否在CPU上运行千亿大模型，对千行百业智能化转型的深化与普惠至关重要。浪潮信息研发工程师基于2U4路旗舰通用服务器NF8260G7，通过张量并行、模型压缩量化等技术，解决了通用服务器的CPU计算资源不足、内存带宽瓶颈、缺乏大规模并行计算环境等问题，实现服务器仅依靠4颗CPU即可运行千亿参数“源2.0”大模型。该方案建设成本更低，首次投入可节约80%以上建设成本，且通用服务器功耗更低，运维更便捷，能够有效降低客户TCO。 </div>
                        <hr>
                    
                    <p>巨量模型的智能生产力正在逐步渗透到各行各业，但它们的部署和运行通常需要专用的AI加速卡，能否在CPU上运行千亿大模型，对千行百业智能化转型的深化与普惠至关重要。</p><p></p><p>日前，浪潮信息研发工程师基于2U4路旗舰通用服务器NF8260G7，通过张量并行、模型压缩量化等技术，解决了通用服务器的CPU计算资源不足、内存带宽瓶颈、缺乏大规模并行计算环境等问题，在业内首次实现服务器仅依靠4颗CPU即可运行千亿参数“源2.0”大模型。该方案建设成本更低，首次投入可节约80%以上建设成本，且通用服务器功耗更低，运维更便捷，能够有效降低客户TCO。</p><p></p><h2>一、大模型推理的硬件需求：内存与带宽的双重考验</h2><p></p><p>当前，大模型的推理计算面临多方面的挑战，制约了大模型服务成本的降低和应用落地。</p><p></p><p>首先是对内存容量的需求。大模型的推理过程中，需要将全部的模型权重参数、计算过程中的KV Cache等数据存放在内存中，一般需要占用相当于模型参数量2-3倍的内存空间。随着业界LLM的网络架构从GPT架构走向MOE架构，主流开源模型的尺寸越来越大，千亿及以上参数的模型已经成为主流，运行一个千亿大模型（100B），则需要200-300GB的显存空间。</p><p></p><p>其次是对计算和内存读写带宽的需求。大模型的推理主要分为预填充和解码两个阶段。预填充阶段把Prompt一次性输入给模型进行计算，对显存的需求更大；解码阶段，每次推理仅生成1个token，计算访存较低，对内存带宽的需求更大。因此，千亿大模型的实时推理，计算设备需要具备较高的计算能力，以及较高的存储单元到计算单元的数据搬运效率。</p><p></p><p>NF8260G7作为一款采用高密度设计的2U4路服务器，支持16TB大内存容量，配置了4颗具有AMX（高级矩阵扩展）的AI加速功能的英特尔至强处理器，内存带宽极限值为1200GB/s。尽管NF8260G7服务器可以轻松满足千亿大模型推理的内存需求，甚至于万亿参数的MOE架构大模型推理的内存需求。但是，按照BF16的精度计算，千亿参数大模型运行时延要小于100ms，内存与计算单元之间的通信带宽至少要在2TB/s以上。因此，要在NF8260G7上实现千亿大模型的高效运行，仅靠硬件升级还远远不够，硬件资源与软件算法协同优化至关重要。</p><p></p><h2>二、张量并行+NF4量化，实现千亿模型极致优化</h2><p></p><p>Yuan2.0-102B是浪潮信息发布的新一代基础语言大模型，参数量为1026亿，通过提出全新的局部注意力过滤增强机制（LFA：Localized Filtering-based Attention），有效提升了自然语言的关联语义理解能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/56/56386d33a07c4bcbce314ca962307c74.png" /></p><p></p><p>为了尽可能提升Yuan2.0-102B模型在NF8260G7服务器上的推理计算效率，浪潮信息算法工程师采用了张量并行（tensor parallel）策略。该策略改变了传统CPU服务器串行运行的模式，把Yuan2.0-102B模型中的注意力层和前馈层的矩阵计算分别拆分到多个处理器，实现同时使用4颗CPU进行计算加速。然而，张量并行对模型参数的切分粒度较细，要求CPU在每次张量计算后进行数据同步，增加了对CPU间通信带宽的需求。在传统的使用多个基于PCIe互联的AI芯片进行张量并行时，通信占比往往会高达50%，也就是AI芯片有50%的时间都在等待数据传输，极大影响了推理效率。</p><p></p><p>NF8260G7服务器的4颗CPU通过全链路UPI（Ultra Path Interconnect）总线互连，该设计带来了两个优势：首先，全链路UPI互连允许任意两个CPU之间直接进行数据传输，减少了通信延迟；其次，全链路UPI互连提供了高传输速率，高达16GT/s（Giga Transfers per second），远高于PCIe的通信带宽，保障了4颗处理器间高效的数据传输，从而支持张量并行策略下的数据同步需求。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c4/c4e4abb05c4891bf632f967199d008d6.png" /></p><p></p><p>UPI总线互连示意图</p><p></p><p>为了进一步提升Yuan2.0-102B模型在NF8260G7服务器上的推理效率，浪潮信息算法工程师还采用了NF4量化技术，来进一步提升推理的解码效率，从而达到实时推理的解码需求。NF4（4位NormalFloat）是一种分位数量化方法，适合于正态分布的数据。它通过确保量化区间内输入张量的值数量相等，来实现对数据的最优量化。由于大型语言模型（LLM）的权重通常呈现零中心的正态分布，NF4量化技术可以通过调整标准差来适配量化数据类型的范围，从而获得比传统的4位整数或4位浮点数量化（这些量化方法的数据间隔通常是平均分布或指数分布的）更高的精度。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8d/8d11c659237684c11d5bfd333ae632e5.png" /></p><p></p><p>INT4数据类型与NF4数据类型对比</p><p></p><p>为了进一步压缩Yuan2.0-102B模型的权重参数，浪潮信息算法工程师采用了嵌套量化（Double Quant）技术，这是在NF4量化基础上进行的二次量化。NF4量化后，由于会产生大量的scale参数，如果使用32位浮点数（FP32）存储，会占用大量的内存空间。若以64个参数作为一个量化块（block size=64）来计算，对于一个千亿参数的大模型，仅存储scale参数就需要额外的6GB内存：</p><p></p><p>(100B/64) * 4 = 6GB</p><p></p><p>为了减少内存占用，浪潮信息工程师通过将这些scale参数量化到8位浮点数（FP8），可以显著减少所需的存储空间。在采用256为量化块大小（block size=256）的情况下，存储所有scale参数所需的额外空间仅为1.57GB：</p><p></p><p>（100B/64/256）* 4 + (100B/64) * 1 = 1.57GB</p><p></p><p>通过嵌套量化，模型的每个权重参数最终仅占用4字节的内存空间，这比原始的FP32存储方式减少了大量的内存占用，从内存到CPU的数据搬运效率提高了4倍。这样的优化显著减轻了内存带宽对Yuan2.0-102B模型推理解码效率的限制，从而进一步提升了模型的推理性能。</p><p></p><h2>三、高算效，低成本</h2><p></p><p>通过在NF8260G7服务器上应用张量并行和NF4量化技术，浪潮信息工程师成功实现了千亿大模型Yuan2.0-102B的实时推理，根据性能分析（profiling）的结果，可以清晰地看到模型中不同部分的计算时间分布：线性层运行时间占比50%，卷积运行时间占比20%，聚合通信时间占比20%，其它计算占比10%。在整个推理过程中，计算时间占比达到了80%，和此前相比，计算时间占比提升30%，大幅提升了算力利用率。</p><p></p><p><img src="https://static001.geekbang.org/infoq/20/20083e4429da031c6bf98da4f1337895.png" /></p><p></p><p>Yuan2.0-102B模型推理性能分析（profiling）结果图</p><p></p><p>浪潮信息基于通用服务器NF8260G7的软硬件协同创新，为千亿参数AI大模型在通用服务器的推理部署，提供了性能更强，成本更经济的选择，让AI大模型应用可以与云、大数据、数据库等应用能够实现更紧密的融合，从而充分释放人工智能在千行百业中的创新活力。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/wHfL2KWZWBeO1OWWxaEb</id>
            <title>《AGI在金融领域的应用实践洞察》报告将在FCon首发，一手内容和参会攻略已备好！</title>
            <link>https://www.infoq.cn/article/wHfL2KWZWBeO1OWWxaEb</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/wHfL2KWZWBeO1OWWxaEb</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Aug 2024 08:22:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 通用人工智能, AGI, 中国市场, 金融行业
<br>
<br>
总结: 中国AGI市场发展报告指出，2030年中国AGI应用市场规模将达到4543.6亿元人民币，金融行业正逐步向产品测试期发展，各企业处于不同阶段的探索和应用实践。 </div>
                        <hr>
                    
                    <p>围绕通用人工智能（AGI）在各行各业的应用现状和趋势，InfoQ 研究中心在今年6月发布了《<a href="https://www.infoq.cn/minibook/6WyXxdu179Di1O75JPUM">中国 AGI 市场发展研究报告 2024</a>"》。报告指出，预计 2030 年中国 AGI 应用市场规模将达到 4543.6 亿元人民币。2024-2027 年 中国 AGI 应用市场将经历快速启动期；年增速持续走高。2028 年起，市场将进入平稳发展期，年市场增速保持在 50% 左右，并预计于 2027 年突破千亿人民币市场规模。</p><p></p><p>可以看到，目前AGI正处于应用层创新的关键衔接期，企业机构在积极探索新的应用方向，同时也在谨慎评估，避免盲目投入导致的策略失误。战略调整、投入试水、应用产品快速创新等一系列动作将推动企业机构AI应用迈向复杂应用期。</p><p></p><p>聚焦金融行业，由清华大学经济管理学院、度小满、《麻省理工科技评论》中国联合编写的《2024年金融业生成式人工智能应用报告》显示，我国金融业虽然拥有全球最大规模的实时数据，但这些金融数据本身并不能同步带来商业价值。通过构建垂直领域AI大模型并推动AGI应用实践，不仅可以充分发挥这些数据资源，还能驱动金融科技创新发展。</p><p></p><p>在这一背景下，中国信通院“铸基计划”联合InfoQ 研究中心经过2个多月对来自银行、保险、证券等金融机构的调研采访，将重磅发布<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6106">《AGI在金融领域的应用实践洞察》</a>"报告。</p><p></p><p>报告显示，整体来看，国内金融行业通向AGI应用的步伐尚处于探索期，正逐渐向产品测试期发展。绝大部分中小型金融机构尚未找到AI技术与业务的融合点，对AGI应用处于观望阶段或将AGI应用产品仅应用于运营场景中。部分头部金融机构积极创新，将AGI产品应用于运营环节及非决策类业务环节。个别大型金融科技公司已推出Al Agent产品或相关框架，即将迈进市场投放期。</p><p><img src="https://static001.geekbang.org/infoq/b5/b53bb8158ce8c58fa36be082a3b3f6bc.png" /></p><p></p><p>《AGI 在金融领域的应用实践洞察》报告详细内容将于 2024 年8月16日-17日举办的 FCon 全球金融科技大会首发。届时，将面向金融领域的技术决策者和业务领导者，共同探讨和交流 AGI 技术如何助力金融行业的数字化转型和高质量发展。</p><p></p><p>目前，已有来自中国信通院泰尔终端实验室、工商银行、交通银行、华夏银行、中信银行、广发银行、北京银行、汇丰创新实验室，平安证券、华泰证券、国投证券、方正证券，中国人民人寿保险、平安产险、太平洋保险，以及度小满、蚂蚁集团等不同领域的金融机构的50+专家确认出席大会进行分享，其中近20个演讲主题与AI大模型相关。</p><p><img src="https://static001.geekbang.org/infoq/9c/9cbe33ff6393cd0cdec97e202431f2c9.png" /></p><p></p><p>对照上图「AGI技术在金融领域应用成熟度模型」，我们为大家备好了2天大会、50多个演讲议题的「参会攻略」：</p><p></p><p>处于“应用探索期”的企业可关注以下议题：</p><p>蚂蚁财富投研支小助技术负责人纪韩：&nbsp;<a href="https://fcon.infoq.cn/2024/shanghai/presentation/5996">《多智能体协同范式在金融产业中的应用实践》</a>"浙里信征信有限公司副总经理兼CTO李响：<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6068">《大数据和大模型在征信赛道的应用》</a>"数势科技数据智能产品总经理岑润哲：<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6053">《智能分析AI Agent在金融行业的先进实践与展望》</a>"</p><p></p><p>处于“产品测试期”的企业可关注以下议题：</p><p>北京银行软件开发中心副总经理代铁&nbsp;：<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6076">《北京银行人工智能应用平台建设与实践》</a>"文因互联董事长/创始人鲍捷博士：<a href="https://fcon.infoq.cn/2024/shanghai/presentation/5944">《精益地打造金融专家智能体》</a>"嘉银科技技术中心人工智能经理姜睿思：<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6033">《大模型在金融知识和作业密集型场景的挑战和实践》</a>"中关村科金资深AI产品总监曹阳：<a href="https://fcon.infoq.cn/2024/shanghai/presentation/5993">《基于知识助手的金融大模型应用实践》</a>"中邮消费金融科技发展部AI算法专家陈盛福：<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6066">《消费金融风控新防线：智能反欺诈技术体系全解析》</a>"平安壹钱包大数据研发部算法负责人王永合：<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6031">《大模型驱动的账户风险管理》</a>"新希望金融科技风险科学部AI中心总经理王小东<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6011">：《大模型下的多模态智能风控落地实践》</a>"eBay Payments&amp;Risk高级技术专家魏瑶：<a href="https://fcon.infoq.cn/2024/shanghai/presentation/5991">《eBay支付风控智能数据标注实践：提效数据标注，加速模型生产化》</a>"</p><p></p><p>处于“市场投放期”的企业可关注以下议题：</p><p>中国工商银行项目办公室资深经理叶雪婷：<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6070">《工商银行研发数智化转型新范式》</a>"&nbsp;广发银行信用卡中心商业智能负责人徐小磊：<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6012">《AIGC在银行线上渠道的应用实践》</a>"富滇银行数字金融中心副主任李涛：<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6048">《数智化时代商业银行运营营销的“坑”与“路”》</a>"中国人民人寿保险信息科技部总经理何东川：&nbsp;<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6105">《知识无界，智启未来：人保寿险大模型建设思考》</a>"</p><p></p><p>根据调研，暂无处于“应用成熟期”的场景，距离AGI在金融领域的大规模成熟应用，仍有很长的路要走。</p><p>当然，除了落地应用之外，在技术建设和实践层面，仍有很多待探讨的话题：</p><p>度小满金融技术委员会执行主席、数据智能应用部总经理杨青《<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6079">人工智能，助力书写数字金融大文章</a>"》汇丰科技创新实验室量子和AI科学家朱兵：<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6093">《金融业中的新技术风险：从大模型到量子计算》</a>"交通银行软件开发中心二级金融科技专家仇钧：<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6089">《金融业大模型平台搭建及应用实践》</a>"澜码科技创始人兼CEO周健：<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6024">《基于大语言模型的AI Agent架构及金融行业实践》</a>"</p><p></p><p>本届大会由中国信通院铸基计划作为官方合作机构，除了以上嘉宾之外，还有来自中信银行、华夏银行、平安证券、华泰证券、中国银联、度小满、蚂蚁集团等金融机构及金融科技公司的资深专家将现身说法分享其在金融科技应用实践中的经验与深入洞察。大会火热报名中，详情可点击链接或扫码联系票务人员咨询：<a href="https://fcon.infoq.cn/2024/shanghai/">https://fcon.infoq.cn/2024/shanghai/</a>"</p><p></p><p><img src="https://static001.geekbang.org/infoq/19/19a95b9adecd6b4e4c2a40feafe2416f.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/jYTv4GAMDxM62ANxHkOG</id>
            <title>大模型时代的操作系统：融合Rust和大模型，vivo打造AI操作系统</title>
            <link>https://www.infoq.cn/article/jYTv4GAMDxM62ANxHkOG</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/jYTv4GAMDxM62ANxHkOG</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Aug 2024 08:03:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 技术革命, 操作系统, 大模型, AI OS
<br>
<br>
总结: 技术革命中，操作系统是计算机系统的核心，大模型的出现带来了AI OS的概念，各种AI OS的发展对操作系统产生了影响。 </div>
                        <hr>
                    
                    <p>采访嘉宾 ｜袁东</p><p>编辑 | Tina</p><p></p><p>每次技术革命，无论是个人电脑、互联网还是移动设备，总是从硬件开始，然后演化到软件层。而操作系统是计算机系统的核心，没有它，计算机就只是一堆硬件，无法运行任何程序。</p><p></p><p>微软 CEO 萨蒂亚·纳德拉曾将生成式 AI 带来的转变比作从蒸汽机到电力的转变。“你不能简单地把电动机放在蒸汽机的位置，而其他一切都保持不变，你必须重新布线整个工厂。”这一两年，“围绕大模型重建操作系统”一直是一个热门话题，产生了各种将大模型作为操作系统或引入操作系统的想法，进而又出现了各种场景下的 AI OS。</p><p></p><p>不管是手机还是全新的 AI 终端，操作系统都是贯穿其中的灵魂，如今手机厂商的“AI OS”角逐也正在上演。苹果在 WWDC 上宣布了“Apple Intelligence”，为 iPhone、Mac 等设备提供一系列 AI 功能。随着苹果正式进军“AI 战场”，生成式能力加持的 AI 手机显然有加速发展的趋势。</p><p></p><p>实际上，国内 AI 手机起风更早，vivo 去年发布了自研 AI 大模型矩阵“蓝心大模型”，以及面向通用人工智能时代自主研发的蓝河操作系统 BlueOS。BlueOS 的系统架构选择了用 Rust 语言编写，减少安全漏洞，并引入大模型的能力，支持复杂的意图识别和声音、图片、手势等多模态交互方式，还并为开发者提供了自动编码等应用开发新范式。</p><p></p><p>大模型会给操作系统带来什么变化？7 月 27 日，vivo 在北京举办了首场蓝河操作系统技术沙龙，我们在会后也邀请到了 vivo 技术规划专家袁东参加 InfoQ 的“极客有约”直播，为我们详细解读了蓝河操作系统的设计理念和技术细节，以下是采访整理。</p><p></p><p></p><h3>大模型时代，我们到底需要一个什么样的操作系统</h3><p></p><p></p><p>InfoQ：最近一两年，我们有了各种关于大模型操作系统的说法，举例来说，传统意义上的 OS、AI-powerd OS，还有 Andrej Karpathy 提出的 AIOS/LLM OS 等各种定义。与传统操作系统相比， AI-powerd OS 和 AIOS 各呈现出哪些新的架构特征？蓝河操作系统比较接近哪一种？</p><p></p><p>袁东： 从最近大模型代表的 GenAI 的火爆，到最近 WWDC 和 Google IO 对公众越来越多的披露，从业者意识到，每天我们朝夕相处的操作系统在这个时代将会有非常大的革新。</p><p>目前业界对 AI OS 或者 AI-powered OS 没有明确的概念或者界限，但可以确定的是，技术架构层面，端侧模型原生入驻操作系统提供系统级别的智能能力，这将在人机交互、技术架构和生态方面会有很大影响。</p><p></p><p>在技术架构方面，端侧模型原生入驻操作系统，提供系统级别的智能生成能力。</p><p></p><p>蓝河操作系统原生集成蓝心大模型，意味着 App 可以基于大模型进行内容构建，后续随着 AI 系统的进一步强化，除了架构的革新外，会有更多的符合 AI 时代的特性推出。例如，普通人可以利用系统创造出符合自己风格的内容。</p><p></p><p>InfoQ：大模型热了后，“围绕大模型重建操作系统”就成了一个热门的话题，可能大家一开始希望大模型更具颠覆性，希望能给底层也带来革命。这让我想起了不久前 Rabbit R1 翻车事件，我认为其中一个关键原因是它的宣传策略。Rabbit R1 宣称其操作系统与之前的安卓系统不同，它是一个全新的系统，能够运行大模型。这种宣传可能给消费者带来了误解或过高的期望，因为实际上它可能并没有达到所宣称的创新水平。那么您认为大模型时代，我们是否有必要重建一个跟安卓不同的操作系统？另外，您认为大模型到来后对操作系统的发展产生了什么样的影响？</p><p></p><p>袁东：Rabbit R1、Ai pin 等在我看来是行业对于 AI 时代大胆的尝试，希望探索出更适合 AI 时代的消费电子产品。目前来看，手机依然是最重要，AI 受益最多的个人产品之一。操作系统在 AI 时代需要明显的升级，借助 AI 智慧化提升用户体验。</p><p></p><p>我认为操作系统会因为大模型在人机交互、架构、生态，三个方面会有很大影响与改变。大模型产的智能涌现，类比移动互联网之于手机。 操作系统会围绕着交互范式、生态范式的改变，相应的做出很多调整。例如，为了打造个性化的系统，需要尽可能获取用户关乎自身的数据，相应的会有系统级别的方式（比如通过系统 App，用户操作）来获取这些私人数据，同时基于这些来给出更贴近用户的行动建议。</p><p>交互范式的变化，意味着服务类 App-Agent 之间的关系与形态慢慢发生变化。Agent 成为一个系统级别的超级 App，随之而来的是 生态发生变化。</p><p></p><p>架构方面，AI 大模型入驻操作系统，其提供了智能的能力，除了自身生成的内容要保证安全，同时我们需要在操作系统中原生地集成安全检测机制，以防止用户遭受不必要的损失。</p><p></p><p>InfoQ：在面向大模型的发展过程中，操作系统面临的挑战和机遇是什么？</p><p></p><p>袁东：</p><p>从用户角度来看，需要考虑如何设计好交互入口（智能助手）：</p><p>即交互方式，多模态智能化交互；用户的意图理解，用户主动发起 - 系统主动发起对用户意图的理解；用户需求拆分后的任务分发，系统级 App 的 AI 升级 到 第三方 App 都可以被智能调度。</p><p></p><p>从开发者生态角度来看，需要考虑如何建造一个共赢的 AI 时代的开发者生态。AI 时代新的 AI 生态架构策略，即围绕智能助手展开的智能生态：</p><p>三方程序向系统级别的智能助手提供 App 的能力描述、App 的应用数据；这类改变类比于 2008 年，App Store 的提出，再次改变了 App 的分发策略，与商业策略。</p><p></p><p>从架构角度来看：</p><p>软件系统架构：持续迭代 AI 系统的设计硬件架构：个人觉得不同时代的硬件也会有相应的革新，图形的兴盛带动了 GPU 的产生，神经网络的计算如果越来越重要 NPU 的发展也会有很大需求。</p><p></p><p>从原生 AI 硬件角度来看：</p><p>人类的五感——听觉、视觉、味觉、触觉和嗅觉——是我们与自然界交互的主要方式。在这些感官中，视觉和听觉是获取信息的主要途径。随着 AI 技术的发展，未来可能会出现原生的 AI 硬件，这些硬件将根据新的交互逻辑和形态进行设计。</p><p></p><p>InfoQ：刚您提到了交互方式的改变，之前也有一个“No App”的概念，但有人认为“No App”是不现实的，对此老师您对此有什么看法？</p><p></p><p>袁东： 我个人的观点是，从满足用户需求来看，用户更多可能希望与系统级别的智能助手交互来满足譬如点外卖、打车等服务类需求。这对于 App - Agent 助手来说，清晰的调用架构 +App 直达服务可能是未来用户更期望的组合形态。</p><p></p><p>但是，对于像游戏、视频和企业级办公这样的应用，它们各自有着特殊的需求，比如对隐私的严格保护、对高性能显卡的依赖，或是对特定功能的高度专业化。这些应用很可能会继续以独立的形式存在，但同时，它们与智能助手之间的互动也将成为增强用户体验的关键。通过智能助手与这些应用的智能联动，我们能够为用户提供一种更加完整和连贯的操作体验。而这种整合不仅对用户来说是一个体验的增强，对于整个技术生态系统和系统发展同样积极的影响。</p><p></p><p>InfoQ：谷歌和苹果开发者大会也提到了它们已经打通了一些 App，这个难度主要在哪里？</p><p></p><p>袁东： 这个问题的核心在于 Agent 与应用程序之间的协同。Agent 需要与两类应用程序进行交互：一类是自有生态的应用程序，另一类是第三方应用程序。 自有生态的应用程序可能包括办公、系统管理、用户行程安排和出行服务等。而第三方应用程序，尤其是长尾应用，在移动互联网时代积累了大量关键用户数据，这些数据可以被用来产生商业价值并提供服务。</p><p></p><p>以苹果和谷歌为例，谷歌的 Gemini 在演示时主要展示了其与自有生态应用程序的整合，如 YouTube 和日历应用。Gemini 内部使用了类似于 Web 应用的 Firebase 扩展，通过自有生态来实现 Agent 与应用程序之间的跨域交流。苹果则更为激进，它通过意图理解和 APP Intents（应用程序增强）的概念，允许 Agent 与第三方应用程序进行交互。在发布会上，苹果展示了如何通过捷径（Shortcuts）和桌面小组件与第三方应用程序进行整合，基本上就是将应用程序的行为能力描述注册到苹果的意图系统中。Siri 会根据用户需求，调用不同的第三方应用程序功能来完成用户的需求，类似于 OpenAI 之前提出的函数调用能力。</p><p></p><p>无论是苹果、谷歌还是国内的厂商，他们都希望未来的服务能够更加便捷。最关键的是充分理解用户的意图和需求。生态建设比技术本身更需要长远发展。技术方面相对清晰，但生态建设，尤其是服务类需求与智能代理之间的交互和交流会很快推进。对于一些社交类或更长尾的应用程序，可能还需要更多的时间来实现整合。</p><p></p><p>InfoQ：有人认为未来操作系统会朝着用 LLM 替换所有或部分 Linux 内核的方向发展，您认同这个观点吗？能否完全取代 Linux 内核？我们应该如何将 LLM 的能力有效融入或嫁接到操作系统内核中？vivo 的操作系统，融入了哪些大模型能力？</p><p></p><p>袁东： 操作系统内核的核心作用是，管理和协调计算机硬件资源，为应用程序提供一个统一的抽象接口，实现硬件与软件之间的高效交互。</p><p>行业有人提出 LLM Kernel 但其架构与内核是并存的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fc/fc3ebeb1dce404217c4b6770e9c5a877.webp" /></p><p></p><p>首先我觉得，在短期内还是一个并存的状态，因为对于现在我们做产品开发，更多需要的是一个通用的操作系统。</p><p></p><p>对于通用的操作系统，由于要满足用户不同的场景需求，LLM Kernel 不太可能替代操作系统内核。</p><p>特别是有人提出来 LLM kernel 不光是包括这个 LLM，它甚至也会有一些 Agent 的调度，还有内存管理、Tool Management 等等，但它还是把它放在了跟 OS kernel 并列的一个状态，它甚至不属于 OS kernel 层的一个 kernel，所以这个 kernel 不是真正的 OS kernel，而是一个抽象的 kernel。</p><p></p><p>然而，在某些垂类产品中，主要通过 Agent 来满足用户的需求的情况下，如果它仅仅是通过 Agent 来满足用户需求，比如说我们看到有一些很有意思的视频分享，展示了有一两个桌面级的小机器人，或者一个小的机器宠物。它其实只要一个生成式的能力就可以满足，背后 OS Kernel 可以只服务与之对应的 LLM，或者 LLM 与 OS Kernel 融合也是有可能的。</p><p></p><p>vivo 的蓝心大模型支持多模态，云 + 端服务于用户。比如用户可以在手表上基于语音交互生成表盘。</p><p></p><p>InfoQ：面向未来发展，哪些 OS 组件需要 AI 化？您们心目中的智慧 OS 应该是怎么样的？</p><p></p><p>袁东： 操作系统正在经历一个明显的 AI 化趋势，个人观点， 这在服务卡片等组件中表现得尤为明显，它们正朝着智能化方向发展。在我看来，有两个主要的发展方向：</p><p></p><p>AI 能力的提升：AI 的加入使得操作系统的组件具备了生成能力，比如能够提取和翻译文本、图像的二次生成等。这种 AI 化的能力提升，使得组件不仅仅能够执行基本任务，还能够进行更复杂的处理和创造性工作。系统级别的 AI 调度：AI 技术开放给系统级别，可以被 Agent 进行调度，成为智慧调度的一部分，以满足用户需求。这意味着操作系统能够更主动地与用户交互，理解他们的意图，并提供个性化的服务。</p><p></p><p>智慧 OS 的特点主要体现在以下几个方面：</p><p></p><p>主动交互：智慧 OS 能够理解用户的意图，并主动与用户进行交互，这种交互方式更加人性化和主动。拟人特性：与以往的多模态和自然交互相比，智慧 OS 通过大模型和 Agent，展现出更加智能和拟人的特性。需求化解：智慧 OS 能够帮助用户将复杂需求简化，例如，通过智能代理帮助用户完成一系列相关任务，如打车、订餐厅、导航等，而不需要用户逐一打开不同的应用程序。</p><p></p><p>将大型模型整合到手机中需要考虑的改进包括：</p><p></p><p>安全：保证端侧模型生成内容的安全，还要时刻兼顾用户使用手机的场景安全。例如，监测 - 抵御外来通过不法手段对用户的诈骗。存储：存储也需要改进，尤其是在容量方面。未来操作系统可能会将更多用户数据存储在本地而非云端，出于安全性和隐私性的考虑。用户的数据可能会被持续记录，关键信息如微软的“Recall”和苹果的“On Screen Awareness”（屏幕理解能力）可能会将用户在应用程序级别的操作数据进行拆解和存储。长期来看，这些数据将占用大量内存空间，未来可能会考虑将这些数据存储在特殊的内存位置，类似于苹果发布 Touch ID 时存储用户指纹数据的方式。计算：模型的能力依赖神经网络计算的能力，神经网络计算能力的发展是一个新需求。如何在端侧保证模型能力越来越强的同时，还能兼顾内存、耗电等资源的占用是需要取舍。</p><p></p><p>大模型生成能力与操作系统的融合方面，我们之前有推出一个智能表盘，我们发现大家使用智能手表很喜欢按照自己的喜好去自定义表盘，所以根据这个需求，我们开发了一款可以通过对话自动生成壁纸的智能表盘，用户只需要描述自己想要什么壁纸，就能直接生成。未来我们还会有更多更令人兴奋的功能和产品持续推出，敬请关注。</p><p></p><p>InfoQ：大模型对开发者会带来什么样的变化？对 App 开发会产生什么样的影响？</p><p></p><p>袁东： 大模型背后代表的是一种智能的产生，这种智能元素可以类比于开发中的新基础元素，就像水和电一样是基础设施的一部分。这种变化首先会 改变开发范式。传统的开发方式是程序员通过输入、存储、计算数据，然后输出确定的数据，使用计算机语言进行编程和运算。未来，编程可能会转变为使用自然语言进行交互，计算将变成一种概率性的计算。开发流程将包括数据的收集和整理、学习、预训练后的模型校验，直至模型能够满足用户需求并生成内容。开发者将利用这一流程，对程序进行相应的变化。其中最关键的是如何提高准确度。有许多方法可以提高准确度，包括结构化输入输出和优化提示工程等技术手段。</p><p></p><p>生态系统也在发生变化。开发者不仅开发满足用户需求的功能，还需要考虑如何获取商业价值。比如开发 AI 原生应用，例如 ChatGPT 就是一个 AI 原生应用的例子。尽管 AI 原生应用具有一定的风险，因为模型或智能能力尚未完全成熟，存在很大的不确定性，但短期内在特定垂直领域开发 AI 应用仍有其价值。例如，某些专注于短期内开发垂直领域的黏土图片生成的 AI 应用，通过精准定位用户需求，短期内可以获得收益。</p><p></p><p>长期来看，Agent 应用可能成为更超级的应用程序。如果行业内有 Agent 的规范，开发者可以在生态系统中遵循相应的规范，结合各种 Agent，从而满足用户需求。例如，苹果的 Siri 提出了一些生态系统规范，开发者可以在这些规范下进行开发，既能满足用户需求，也能实现商业变现。</p><p></p><p>InfoQ：我个人对当前应用开发的趋势还有一些疑问。例如，我们观察到一些应用，比如之前提到的黏土风格图片生成应用，它们实际上可能并不需要开发成一个完整的应用程序。这引发了一个问题：在大模型时代，是否意味着我们之前讨论的快应用以及小程序等轻量级应用形式会具有更广阔的发展前景？</p><p></p><p>袁东： 在 AI 时代，应用程序的形态，Web App 可能会更加适应 AI 技术的发展。Web App 的优势在于它不需要用户进行安装和升级，始终能够保持最新状态。这种即时更新的特性意味着 Web App 能够与 AI 模型保持天然的兼容性，因为 AI 模型可以不断地进行训练和优化，而 Web App 可以即时利用这些最新的模型。</p><p></p><p>随着 AI 技术的发展，Web App 甚至可能与 Agent 进行更多的交互，逐渐演变成插件形态，不再需要传统的图形用户界面。这种形态的应用程序在 AI 时代将有很大的发展空间。更多的内容请关注 8 月 8 号，快应用大会。</p><p></p><p></p><h3>vivo 蓝河操作系统的演进和迭代</h3><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9b161e434d58bcefdc1a12735aaa60fc.webp" /></p><p></p><p>InfoQ：蓝河应该是在 ChatGPT 热起来之前就已经开始规划的项目？是否能分阶段介绍下它的发展历史？另外，蓝河操作系统在发展过程中遇到的最大挑战是什么？</p><p></p><p>袁东：2018 年伊始， vivo 建立了 AI 研究院，自研操作系统团队，并且在当时我们就认为 AI 时代 Web App 是天生适合 AI 时代的 App 形态。历经 6 年我们研发并发布了蓝河操作系统。</p><p></p><p>ChatGPT 代表的大模型带来了智能涌现，我们在 2023 年顺势而为发布了蓝河 OS。天生更智慧，天生更安全，天生更流畅。智慧是核心，安全、流畅是基石。</p><p></p><p>它从一开始就融入了大模型技术，而且在安全性和流畅性方面也进行了全面的重新架构。特别是在架构方面，我们采用了 Rust 语言来实现系统架构，这种语言不仅能够确保用户操作的流畅度，还能在内存安全方面提供强有力的保障。埃隆·马斯克（Elon Musk）也曾提出：“Rust 是实现 AGI 的最佳语言”。目前，Rust 也被尝试用于实现模型推理等任务，例如可以在模型分布式推理中使用。</p><p></p><p>我们认为在这个 AI 技术迅速发展的时期推出蓝河 OS 是非常正确的决定，它具有重大的意义，不仅代表了技术的前沿，也预示着操作系统未来发展的方向。</p><p></p><p>InfoQ：在大模型技术流行之前，你们就已经决定使用 Rust 语言进行开发，这个决定背后的逻辑是什么呢？有没有一些明确的数据可以证明 Rust 对用户体验带来的正影响呢？</p><p></p><p>袁东：Rust 语言的开发与大模型技术并没有直接的硬性关联。Rust 最初由 Mozilla 提出，旨在解决操作系统中的内存安全问题。C 和 C++ 虽然在实现操作系统内核方面非常高效，但它们在内存管理上存在一些挑战，一旦出现问题，排查成本和时间都非常高。相比之下，Rust 语言在保持与 C++ 相当的运行效率的同时，其编译器能够在编译时就避免很多内存错误，从而减少运行时的内存问题。我们选择使用 Rust 开发操作系统，是出于提供更流畅、更安全系统的考虑。</p><p></p><p>Rust 的优势方面，更多还是处于对安全性的考虑，比如像最近的 Windows 蓝屏事件，可能我们看到的一个原因是它的内存在 unsafe 状态下指向了一个别的地址，导致它崩溃，最终对行业造成了非常巨大的损失，内存安全的重要性不言而喻而这块也是Rust的优势。</p><p></p><p>InfoQ：蓝河操作系统的技术迭代的规划是怎样的（包括 AI 能力，以及编译器、编程框架、编程语言、IDE 等工具）？</p><p></p><p>袁东： 蓝河操作系统主要从智慧、安全、流畅等三个方向持续保证技术迭代。</p><p>智慧：蓝河操作系统做了智慧的架构设计，重点架设了 AI 能力，实现了更复杂的意图识别和推理决策能力。蓝河操作系统带来了多模态输入输出，模拟人与人的交互方式。它打破了应用和设备边界，让用户不用在各个 APP 和设备中来回切换。同时，AI 的多模态能力将拓宽输入和输出方式，语音、文字、图片、音乐、视频等 AI 都能理解和生成。蓝河操作系统，从系统、应用、到工具链全面突破，通过 VCAP 能力实现对推理决策的支持，基于大模型能力实现了 AI 服务引擎和多模输入子系统。同时，基于 AI 能力打造了诸多智慧操作系统的新型应用。Copilot 提供代码生成、图文生成等能力，带来应用开发的全新生产力工具。蓝河操作系统结合 AI 大模型的能力，探索出了应用开发的全新范式——它可以理解你的需求，自动编写代码，生成专属于你的应用、主题或壁纸，满足你对个性化的需求。安全：安全与隐私是操作系统的基石，行业数据中操作系统大约 70% 的严重安全漏洞都和内存使用不当相关，修复安全漏洞治标不治本，难以彻底解决。蓝河操作系统从性能和安全两个维度选择了 Rust 语言作为系统开发语言，Rust 语言的所有权模型、生命周期等一系列安全特性，保障了代码在编译阶段就可以发现内存使用不当导致的安全问题，进而保障系统安全。流畅：蓝河操作系统从全栈技术视角出发，对多个技术方向进行探索，例如编程语言、运行时 Runtime、系统调度、显示和内存。充分发挥软硬件资源的利用效率，高性能系统架构实现了一系列关键技术，虚拟显卡框架、超级协程机制、Runtime 等，提升了计算、存储、显示的资源效率。系统框架的编写我们创新性的采用了兼具高性能和高安全的 Rust 语言；应用开发还要考虑开发效率和生态兼容，目前采用了 js。Runtime 执行引擎，将前端框架下沉，针对应用使用场景，没有采用传统虚拟机机制，而是直通调用接口，一步直达内核，进一步降低运行时的开销、提升性能。在线程和进程之下，实现了超级协程机制，无论是滑动屏幕还是打开应用，都可以优先响应当前操作，实现丝滑流畅的使用体验。蓝河实现了虚拟显卡框架，在虚拟显卡框架上，创新实现了超级渲染树、并行渲染、异构渲染，解决了丢帧、掉帧、帧同步的问题，保障蓝河操作系统的显示天生更流畅。对于内存管理，设计了全新的内存管理双向动态调整算法，按照算法来分配不同的内存，减少应用启动时间。</p><p></p><p>InfoQ：您能否详细介绍一下蓝河在构建开发者生态系统方面的具体策略和计划？对于蓝河的开发者来说，您认为他们的机遇在哪里？</p><p></p><p>袁东： 蓝河在构建开发者生态系统方面的策略和计划是多方面的，旨在创造一个智能应用生态解决方案，同时为开发者提供丰富的机遇。</p><p></p><p>我们认识到每个生态系统都有其特色，蓝河生态中用户的场景与其他生态不同，特别是在阅读和服务类应用方面。蓝河寻求在这些场景中进行智慧升级，以提升用户体验，使他们更加喜爱这些场景。长期目标是将蓝河操作系统打造成这个时代的智能应用生态解决方案，更加智能地满足用户的各种需求场景。</p><p>为了鼓励开发者，蓝河的运营团队持续进行各种活动。例如，去年蓝河 OS 举办了一场比赛，吸引了 300 多支队伍参加，奖金池达到 75 万。赛题包括利用 AI 技术将操作系统内核从 C 语言转换为 Rust 语言，以及生成智慧应用。比赛中涌现出许多有潜力和创意的 App 和系统级解决方案。今年，蓝河将继续举办符合这个时代特征的创新比赛，并进行线上和线下推广，同时邀请专业团队为开发者提供指导。不论比赛结果如何，蓝河都会发掘有潜力的选手，他们有可能成为蓝河团队的一员。</p><p></p><p>总的来说，未来蓝河的大模型和操作系统将持续朝智慧化方向迭代。传统应用服务的生态将得到重塑，包括原子化服务、个性化定制、智能分发、跨设备协同以及更拟人化的多模态交互等新设计。</p><p></p><p>对于开发者而言，蓝河生态中的机遇在于 AI、大模型和操作系统的升级。开发者应关注 AI 和大模型能力的提升，以及新操作系统变革带来的影响。我们一方面会从开发效率上帮开发者去减负，包括提供更智能的代码生成、校验、单元测试等能力；另一方面，我们也在探索未来 AI、Agent 跟 APP 之间的新交互方式，去满足 AI 时代的用户的需求，从而获得更大的商业变现机会，这是我们持续在做的一些事情。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/LvQs5lG7et17I3wxvTkO</id>
            <title>检索增强生成：革命性技术还是过度承诺？</title>
            <link>https://www.infoq.cn/article/LvQs5lG7et17I3wxvTkO</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/LvQs5lG7et17I3wxvTkO</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Aug 2024 04:12:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: RAG, 检索增强生成, AI, 数据搜索
<br>
<br>
总结: 本文深入剖析了RAG技术的能力和在实际应用中的表现。RAG通过整合外部最新信息来增强生成式模型的能力，提高了人工智能系统的准确性和相关性。虽然RAG技术为人工智能领域带来了革命性变化，但在不同应用场景中仍需定制化落地方案。 </div>
                        <hr>
                    
                    <p>本文将深入剖析 RAG（Retrieval-Augmented Generation）所宣称的能力和其在实际应用中的表现。我们首先将探讨 RAG 的工作原理，评估其潜在的优势。随后，我们将分享在实践中遇到的一些挑战，以及我们为应对这些挑战所开发的解决方案。此外，我们还将讨论那些我们仍在探索中的未解决的问题。通过这些内容，您将获得对 RAG 能力的全面了解，并认识到它在推动人工智能领域发展中所扮演的不断进化的角色。</p><p>&nbsp;</p><p>设想一下，你正在与某人交谈，这个人不仅对当前事件缺乏了解，而且在面对不确定性时，还倾向于自信地编造细节。这种情况恰恰反映了传统生成式人工智能所面临的困境。尽管 AI 拥有广泛的知识储备，但它往往依赖于过时的数据源，并且容易陷入所谓的“幻觉”现象——即在缺乏确凿信息的情况下，虚构出细节。这种行为模式导致了一个严重的问题：AI 可能会基于某种虚构的细节，以一种不切实际的自信态度，提供错误的信息。</p><p>&nbsp;</p><p>检索增强生成（Retrieval-Augmented Generation, RAG）技术为人工智能领域带来了革命性的变化。它的作用可以类比于为一个原本对当前事件一无所知的人提供了一部能够即时访问互联网上最新信息的智能手机。通过 RAG ，人工智能系统现在能够获取并整合实时数据，显著提升了其响应的准确性与相关性。然而，值得注意的是，RAG 技术并非一剂万能药。在不同的应用场景中，它仍在探索未知的领域，并没有一种放之四海而皆准的策略。有效的 RAG 落地方案往往需要根据具体的使用案例来定制，这是一个反复试验和不断试错的过程。</p><p>&nbsp;</p><p></p><h1>什么是 RAG 以及其工作原理</h1><p></p><p>检索增强生成（RAG）是一种人工智能技术，它宣称通过在响应生成过程中整合外部最新信息,可以显著增强生成式模型的能力。该方法使人工智能系统能够获取最新可用数据，从而使生成的响应不仅准确，而且与当前上下文高度相关。</p><p>&nbsp;</p><p>下面是 RAG 技术所涉及的各个关键步骤：</p><p>&nbsp;</p><p>发起查询：整个过程始于用户向 AI 聊天机器人提出一个问题。这是用户与 AI 之间的初始互动，用户提出一个特定的主题或查询，以此启动对话。编码以检索：随后，用户提出的查询被转换成文本的向量表示形式。这些向量是用户查询内容的数字化表达，它们是模型能够计算和分析的格式，并且包含了问题的核心信息。寻找相关数据：接下来，RAG 的检索组件开始工作，利用查询的向量表示在数据集中进行深入的语义搜索。这种搜索超越了简单的关键词匹配，它旨在理解查询背后的深层意图，并寻找与之相匹配的相关数据。生成答复：在成功整合了相关的外部数据之后，RAG 生成器结合 AI 模型的训练知识以及新检索到的特定信息，构建出回复。这一过程确保了生成的回复不仅有着可靠的依据，而且与当前上下文高度相关，提供了一个既准确又富有洞察力的答案。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/c2/c29b9bbc7062d7bfae4501e251a978b8.png" /></p><p></p><p></p><h1>&nbsp;</h1><p></p><p></p><h1>RAG 的开发过程</h1><p></p><p>开发一个用于生成式人工智能的检索增强生成（RAG）系统是一个多步骤的过程，关键在于确保系统不仅能检索到相关信息，还能有效地整合这些信息，以提升响应的质量。以下是该过程的详细概述：</p><p>收集自定义数据：首要步骤是收集人工智能将访问的外部数据。这要求我们构建一个多样化且与人工智能处理主题紧密相关的数据集。数据源可能包括书本、设备手册、统计数据和项目文档，这些数据为人工智能的响应提供了坚实的事实基础。分块和格式化数据：收集到数据后，接下来的任务是对数据进行预处理。分块是将大型数据集分解成更小、更易于管理的段落，以便于后续处理。将数据转换为嵌入（向量）：这一步涉及将数据块转换成密集的数值表示形式，即嵌入或向量。这样的转换有助于人工智能更高效地分析和比较数据。开发数据搜索：该系统必须采用一系列高级搜索算法，其中包括语义搜索技术，以突破传统关键词匹配的局限。此外，系统还需整合自然语言处理（NLP）技术，以确保即便用户的输入术语存在不精确性，也能够准确捕捉到查询背后的深层意图，检索出与用户查询高度相关的数据。准备提示词系统：最后一步是调制提示词系统，这些提示词将指导大型语言模型（LLM）如何使用检索到的数据来生成响应。这些提示词确保了人工智能的输出不仅信息丰富，而且在上下文上与用户的查询高度匹配。</p><p>&nbsp;</p><p>这些步骤构成了 RAG 开发的理想蓝图。然而，在实际实施过程中，通常需要进行额外的调整和优化，以适应特定项目的目标。因为在开发的每个阶段，都可能会遇到需要解决的难题。</p><p>&nbsp;</p><p></p><h1>RAG的承诺</h1><p></p><p>RAG 技术在人工智能系统中承诺可以发挥双重作用。首先，它致力于简化用户获取答案的流程。通过提供更准确和相关的响应，可以显著提升用户体验。这种改进使得用户在查询信息时能够体验到更加便捷和直观的交互过程。其次，RAG 技术为企业提供了深度利用其数据的能力。它使得原本庞大的信息库变得易于搜索，从而极大地促进了信息的可访问性。这种能力不仅提高了企业处理数据的效率，而且为企业的决策制定和洞见发掘提供了有力支持。</p><p></p><h3>准确性提升</h3><p></p><p>准确性仍然是大型语言模型的一个关键限制，这种表现出来通常有以下几种方式：</p><p>&nbsp;</p><p>错误信息。当不确定时,大型语言模型可能会提供看似合理但实际上不正确的信息。过时或样板式的响应。寻求特定和最新信息的用户往往会收到空泛或过时的响应。不可靠的信息来源。大型语言模型有时会根据不可靠的来源生成响应。术语混淆。不同来源可能在不同语境中使用相似的术语，导致不准确或混乱的响应。</p><p>&nbsp;</p><p>使用 RAG ，你可以限定模型只使用正确的数据，以确保响应与当前的任务相关且准确。</p><p>&nbsp;</p><p></p><h3>会话式搜索</h3><p></p><p>RAG 旨在增强我们搜索信息的方式，允许用户通过类似人类的对话而非一系列不连贯的搜索查询来找到所需信息，从而有望超越传统搜索引擎(如谷歌)的表现。这一承诺提供了一种更流畅、更自然的交互方式，其中人工智能能够理解并在正常对话的流程中响应查询。</p><p>&nbsp;</p><p></p><h3>实事求是的看法</h3><p></p><p>然而，不管 RAG 的承诺看起来多么诱人，重要的是要记住这项技术并非万能良药。虽然 RAG 可以提供无可否认的好处，但它并不能解决所有挑战。我们已在几个项目中实施了这项技术，我们将分享我们的经验，包括我们面临的障碍和我们找到的解决方案。这种来自实践的见解旨在提供一个客观的观点，说明 RAG 真正能提供什么，以及哪些方面仍需要持续进步。</p><p>&nbsp;</p><p></p><h1>RAG 落地的挑战</h1><p></p><p>在现实世界的应用场景中实施检索增强生成（RAG）会带来一系列独特的挑战，这些挑战可能深刻影响人工智能的性能。尽管这种方法提高了准确答案的可能性，但依然无法保证完美无误的准确性。</p><p>&nbsp;</p><p>我们在一个发电机维护项目中的经验表明，在确保人工智能正确使用检索到的数据方面存在重大障碍。通常，它会误解或误用信息，导致生成误导性的答案。</p><p>&nbsp;</p><p>此外，处理会话中的细微语义差异、浏览庞大的数据库以及纠正人工智能“幻觉”——即它虚构信息的情况——进一步增加了 RAG 落地的复杂性。</p><p>&nbsp;</p><p>这些挑战突显了 RAG 的落地方案必须视具体项目而定制化，同时也强调了在人工智能的发展中持续创新和适应的必要性。</p><p>&nbsp;</p><p></p><h1>确保准确性</h1><p></p><p>尽管检索增强生成（RAG）显著提高了提供正确答案的可能性，但更重要的是要认识到它并不能保证 100% 的准确性。</p><p>&nbsp;</p><p>在我们实际应用中，我们发现仅仅让模型从我们提供的外部数据源中获取正确的信息是不够的；它还必须有效地利用这些信息。即使模型确实使用了检索到的数据，但仍然存在它可能误解或扭曲这些信息的风险，使得这些信息变得不那么有用甚至不准确。</p><p>&nbsp;</p><p>例如，当我们为发电机维护开发了一个 AI 助手时，我们努力让模型找到并使用正确的信息。AI 偶尔会“破坏”这些宝贵的数据，要么误用它，要么以削弱其效用的方式改变它。</p><p>&nbsp;</p><p>这次经历突出了 RAG 实施的复杂性，其中检索信息仅仅只是第一步。真正的任务是将这些信息有效且准确地整合到 AI 的响应中。</p><p>&nbsp;</p><p></p><h1>会话搜索中的语义误差</h1><p></p><p>使用搜索引擎搜索信息和与聊天机器人交谈之间存在很大差异。使用搜索引擎时，你通常会确保你的问题定义得很好以获得最佳结果。但在与聊天机器人的对话中，问题可以不那么正式和完整，比如问：“X 怎么样？”。例如，在我们为发电机维护开发 AI 助手的项目中，用户可能从询问一个发电机型号开始，然后突然转换到另一个型号。</p><p>&nbsp;</p><p>处理这些快速变化和突然的问题要求聊天机器人理解对话的完整上下文，这是一个主要挑战。我们发现 RAG 很难根据正在进行的对话查找正确信息。</p><p>&nbsp;</p><p>为了改进这一点，我们调整了我们的系统，让底层的大型语言模型（LLM）在尝试查找信息之前，使用对话的上下文重述用户的查询。这种方法帮助聊天机器人更好地理解和响应不完整的问题，并使交互更加准确和相关，尽管它并不总是完美的。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/3d/3d10a56fd4b4165edc39d81cef3a97b1.png" /></p><p></p><p></p><h1>数据库访问</h1><p></p><p>在实施检索增强生成（RAG）时，访问庞大的数据库以检索正确的信息是一个重大挑战。当我们有了明确定义的查询并理解了所需的信息，下一步就不仅仅是搜索，而是有效搜索。我们的经验表明，尝试梳理整个外部数据库是不切实际的。如果项目包括数百份文档，每份文档可能又包含数百页，那么这个体量就变得难以管理。</p><p>&nbsp;</p><p>为了解决这个问题，我们开发了一种方法，首先通过将焦点缩小到可能包含所需信息的特定文档来简化流程。我们使用元数据来实现这一点——为我们数据库中的每份文档分配清晰、描述性的标题和详细的描述。这些元数据就像一个向导，帮助模型快速识别并选择响应用户查询的最相关文档。</p><p>&nbsp;</p><p>一旦确定了正确的文档，我们随后在该文档内执行向量搜索，以定位最相关的部分或数据。这种有针对性的方法不仅加快了检索过程，还显著提高了检索信息的准确性，确保了由 AI 生成的响应尽可能与上下文相关和精确。在深入内容检索之前先细化搜索范围的策略，对于有效管理和访问 RAG 系统中的大型数据库至关重要。</p><p>&nbsp;</p><p></p><h1>幻觉</h1><p></p><p>如果用户请求的信息在外部数据库中没找到，会发生什么？根据我们的经验，大型语言模型（LLM）可能会编造回答。这个问题——被称为“幻觉”——是一个重大挑战，我们仍在寻找解决方案。</p><p>&nbsp;</p><p>例如，在我们的发电机项目中，用户可能会询问我们数据库中没有记录的型号。理想情况下，助手应该承认缺少信息，并声明它无法提供帮助。然而，LLM 有时会提取关于类似型号的信息，并将其呈现为相关。目前，我们正在探索解决这个问题的方法，以确保 AI 在无法根据现有数据提供准确信息时可靠地指出来。</p><p>&nbsp;</p><p></p><h1>寻找“正确”的方法</h1><p></p><p>另一个我们从使用 RAG 的工作中学到的关键教训是，它的实施没有放之四海而皆准的解决方案。例如，我们为发电机维护项目开发的 AI 助手所采用的成功策略并不能直接应用到其他不同背景的项目中。</p><p>&nbsp;</p><p>我们尝试将相同的 RAG 设置应用于为销售团队创建 AI 助手的项目中，该助手的目的是简化入职流程并增强知识传递。和许多其他企业一样，我们也需要花费精力处理大量且难以筛选的内部文档。因此该项目的目标是部署一个 AI 助手，使这些丰富的信息更容易被获取。</p><p>&nbsp;</p><p>然而，销售文档的性质——相对于发电机项目文档，更侧重于流程和协议而非技术规格——与前一个项目中使用的技术设备手册大相径庭。内容类型和使用方式的差异意味着相同的 RAG 技术并未如预期那样发挥作用。销售文档的独特特点要求 AI 检索和呈现信息的方式有所不同。</p><p>&nbsp;</p><p>这次经历强调了根据每个新项目的内容、目的和用户期望量身定制 RAG 策略的必要性，而不是依赖于通用模板。</p><p>&nbsp;</p><p></p><h1>关键收获与 RAG 的未来</h1><p></p><p>当我们回顾在检索增强生成的挑战和复杂性中的过程时，出现了几个关键教训，这些教训不仅强调了该技术目前的能力，也暗示了其不断发展的未来。</p><p>&nbsp;</p><p>适应性至关重要。RAG 在不同项目中的不同结果，展示了在其应用中适应性的必要性。由于每个项目的数据和需求的多样性，一种放之四海而皆准的方法是不存在的。持续改进。实施 RAG 需要不断的调整和创新。正如我们所看到的，克服幻觉等障碍、改进会话搜索和优化数据检索对于发挥 RAG 的全部潜力至关重要。数据管理的重要性。有效的数据管理，特别是在组织和准备数据方面，被证明是成功实施 RAG 的基石。这包括了对数据如何分块、格式化和实现可搜索性的细致关注。</p><p>&nbsp;</p><p></p><h1>展望未来：RAG 的前景</h1><p></p><p>加强上下文理解。RAG 的未来发展方向旨在更好地处理对话和上下文的语义误差。自然语言处理（NLP）和机器学习的进步可能会带来更精细的模型，这些模型能够以更高的精确度理解和处理用户查询。更广泛的应用。随着企业认识到使数据更易于访问和操作的好处，RAG 可能会在各个行业中看到更广泛的实施，从医疗保健到客户服务乃至更广泛的领域都可能出现它的身影。通过创新解决现有挑战。持续的研究和开发可能会产生创新的解决方案来应对当前的局限，例如幻觉问题，从而提高 AI 助手的可靠性和可信度。</p><p>&nbsp;</p><p>总之，尽管 RAG 在人工智能技术方面展现出了富有希望的前景，但它并非没有挑战。未来的路将需要持续的创新、定制化的策略和开放的心态，以充分实现 RAG 的潜力，使人工智能交互更加准确、相关且有用。</p><p>&nbsp;</p><p>原文链接：<a href="https://www.sitepoint.com/retrieval-augmented-generation-revolution-overpromise/#realworldragchallenges">https://www.sitepoint.com/retrieval-augmented-generation-revolution-overpromise/#realworldragchallenges</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/fdfO9SbelAzLkQWDUeET</id>
            <title>开源神器！向量、张量、全文搜索一网打尽，打造最强 RAG！</title>
            <link>https://www.infoq.cn/article/fdfO9SbelAzLkQWDUeET</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/fdfO9SbelAzLkQWDUeET</guid>
            <pubDate></pubDate>
            <updated>Wed, 31 Jul 2024 12:42:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 开源, AI, 数据库, 混合搜索
<br>
<br>
总结: Infinity 0.2 release发布了新的数据类型稀疏向量和张量，提供更多召回手段，包括全文搜索和向量搜索的混合搜索，解决了向量搜索无法提供精确查询的问题。采用多路召回的混合搜索方案，结合全文搜索、稠密向量和稀疏向量，提供更好的召回效果，适合RAG的选择。Infinity内置了这种混合搜索功能，简化了技术架构，提高了搜索效率。 </div>
                        <hr>
                    
                    <p></p><p>开源 AI 原生数据库 Infinity 0.2 release 正式发布，提供了 2 种新数据类型：稀疏向量 Sparse Vector 和 张量 Tensor，在此前的全文搜索和向量搜索之外， Infinity 提供了更多的召回手段，如下图所示，用户可以采用任意 N 路召回（N ≥ 2）进行混合搜索，这是目前功能最强大的 RAG 专用数据库。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/de/ded27ec4a0545dded535705153be0190.png" /></p><p></p><p></p><h2>为什么需要混合搜索（多路召回）？</h2><p></p><p></p><p>我们知道，仅仅依靠向量搜索（默认情况下，它用来特指稠密向量）并不总能提供令人满意的结果。当用户问题中的特定关键词与存储的数据不准确匹配时，这种问题尤为明显。这是因为向量本身不具备精确语义表征能力：一个词，一句话，乃至一篇文章，都可以只用一个向量来表示，这时向量本质上表达的是这段文字的“语义”，也就是这段文字跟其他文字在一个上下文窗口内共同出现概率的压缩表示 ，因此向量天然无法表示精确的查询。例如如果用户询问“2024 年 3 月我们公司财务计划包含哪些组合”，那么很可能得到的结果是其他时间段的数据，或者得到运营计划，营销管理等其他类型的数据。</p><p></p><p>因此，在一种好的解决方案是，利用基于关键词的全文搜索提供精确查询，它跟向量搜索共同工作，这就是全文搜索 + 向量搜索 的 2 路召回，又被称为混合搜索（hybrid search）。</p><p></p><p>多路召回，在 RAG 的使用场景中，有时候还被解释为其他选择：</p><p></p><p>一种是仍然用向量搜索，但是采用多种方式将改写查询，然后合并多个查询的返回结果，这其实解决的仍然是向量本身语义表征粒度难以控制的问题，并不能解决向量无法进行精确查询的问题。</p><p></p><p>另一种就是引入稀疏向量，跟稠密向量组合到一起提供混合搜索（hybrid search）。稀疏向量跟稠密向量并不是一回事，它并没有稠密向量那种针对语义的压缩表示，而是试图针对全文搜索的一种替代，它解决的是全文搜索过程中如何针对倒排索引的词典进一步对关键词进行裁剪、扩展和定义权重。这样，一篇原始文档，就可以用这些裁剪后的关键词组成的稀疏向量来表征。例如下边的例子，上边是稠密向量，下边则是稀疏向量，它的维度一般要远高于稠密向量，例如会有 3 万维，由于大多数维度并没有值，因此可以采用 （位置，值）的形式表达向量中每个存在权重的维度。</p><p></p><p><code lang="cs">[0.2, 0.3, 0.5, 0.7,......]
[{331: 0.5}, {14136: 0.7}]
</code></p><p></p><p>把文本转为稀疏向量最知名和有代表性的工作就是 SPLADE （参考文献 [1]），它利用一个标准的预训练数据，将文档中的冗余词删除，并且增加扩展词，从而形成一个标准的 3 万维的稀疏向量输出。这里的冗余词删除，其实就类似传统搜索引擎中分词过程中的“去停用词”（在构建索引的过程中，将英文中的 the、a、等频率很高但信息密度很低的词跳过，这并不影响整体召回的效果，中文也同样的道理）；增加相应扩展，也类似传统搜索引擎的查询同义词等扩展技术。站在使用的角度，它可以把任何文档都表征为一个 3 万维的稀疏向量，向量每个维度表征这个单词的权重。在典型的信息检索 ( Information Retrieval) 评测任务中，采用 SPLADE 稀疏向量取得了比传统搜索引擎基于 BM25 排序方式更好的表现。而利用稀疏向量 + 稠密向量的混合搜索，其效果在近期的一篇采用 BGE M3 embedding 模型的论文中也得到了验证（参考文献 [2]），在典型评测中，取得了比 BM25 要好很多的效果：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a9/a98790e18abfe394910f6a8a5c21c588.png" /></p><p></p><p>那么看起来，似乎稠密向量 + 稀疏向量就是更好的多路召回方案，采用全文搜索 + 稠密向量，似乎没有必要？</p><p></p><p></p><h2>为什么需要三路召回？</h2><p></p><p></p><p>我们知道， RAG 在技术上总是会遇到很多挑战，尽管信息检索理论为 RAG 提供了很多支撑，包括信息检索的评测等，但在实际中仍然会遇到很多问题。稀疏向量通过预训练模型删除了很多无用词，并增加了许多用来做查询扩展的词，这在通用查询任务上必然会表现更好，然而在实际使用中，依然有大量用户提问的关键词，并不在生成稀疏向量的预训练模型中，例如各种机器型号，说明书，专用词汇等等，用 3 万维表达全部关键词，还是多语言，依然会有诸多信息损失，此外还有各类业务所必须的短语查询等等，这些都是必须采用全文搜索才能实现的功能。因此，近期 IBM 的研究文章（参考文献 [3]）对比了各种召回方式的组合，包括 BM25，稠密向量，BM25 + 稠密向量， 稠密向量 + 稀疏向量，以及 BM25 + 稠密向量 + 稀疏向量，最终得出结论：采用 3 路召回是所有组合中最适合 RAG 的选择，而 Infinity 已经完全内置了这种混合搜索功能。</p><p></p><p>3 路召回表现好非常容易理解，因为稠密向量可以表征语义， 稀疏向量可以在训练数据类似的场景下提供更好的精确召回，而关键词全文搜索则在各种场景下提供更加鲁棒的精确召回选择。3 种查询选择，一种都不能少，这使得 RAG 的方案设计更加复杂化，如果这些查询方式不能在一个数据库内完成，就需要用户组合多个数据库的 pipeline 来完成这一个功能，从而引入更多的工程 tricks 和复杂度，也影响了 RAG 技术的推广。例如：采用向量数据库提供稠密向量搜索和稀疏向量搜索，采用 Elasticsearch 提供关键词全文搜索，因此确保两者的数据同步就带来了技术挑战，如果一个文档在一个存储中存在，但在另一个存储中却不存在，就会带来一些错误的查询结果，所以可能需要一些其他 workaround ：例如再引入一个 OLTP 数据库如 PostgreSQL 用来存放元数据，然后用一些对象存储来存放原始的文档数据，两者结合向量数据库和 Elasticsearch 来提供最终的数据同步，这是一个非常复杂的后端架构。而如果采用 Infinity ，就无需依赖这种复杂架构，3 种格式的存储，连同原始数据一起，一次全部插入且保证数据 ACID ，3 路召回的混合搜索，一条语句就可以完成，方便且高效。</p><p></p><p></p><h3>如何排序？</h3><p></p><p></p><p>三路召回完成后，一个直接的问题就是如何进行融合排序。Infinity 内置了多种融合排序算法：</p><p></p><p>Reciprocal Rank Fusion (RRF) 算法，它是这样工作的：为每路召回的结果列表中的每个文档都根据其排序位置分配一个分数，通常，得分是其排名的倒数。例如，排名第一的文档得分为 1，排名第二的得分为 0.5，排名第三的得分为 0.33，以此类推。那么最终文档的得分就是各路召回结果的累加。RRF 算法的好处在于鲁棒性，它的简单使得这种排序不容易过拟合，针对各种用户的不同场景无需大量参数调整就可以适应。简单权重加权融合，RRF 算法非常鲁棒，但它完全按照各路召回的排名进行打分，丢掉了原始召回中的相似度信息。在某些情况下，仍然需要进一步控制。例如这样一个问题“请问型号为 ADX-156 的机器不能工作该如何处理”，我们需要对关键词得分提升权重。基于外部模型的重排序，Infinity 原生支持基于 ColBERT 的重排序功能，关于这部分细节，我们在下文进一步讲解。</p><p></p><p>Infinity 提供了强大的排序组合机制，给用户提供多样化选择，如下图的 2 个例子：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/75/75e492511f79056385e18227b0ceda17.png" /></p><p></p><p>第一种是 3 路召回的结果直接用 RRF 融合排序后返回。使用方式极其简单：</p><p></p><p><code lang="javascript">res = table_obj.output(['*'])
               .match_vector('vec', [3.0, 2.8, 2.7, 3.1], 'float', 'ip', 1)
               .match_sparse('sparse_vec', {"indices": [0, 10, 20], "values": [0.1, 0.2, 0.3]}, 'float', 'ip', 1)
               .match_text('title, body', 'hello world', 'topn=10')
               .fusion('rrf')
               .to_pl()
</code></p><p></p><p>第二种向量搜索用 ColBERT 重排序，然后结果跟关键词全文搜索做权重叠加再返回。</p><p></p><p><code lang="javascript">res = table_obj.output(['*'])
               .match_vector('vec', [3.0, 2.8, 2.7, 3.1], 'float', 'ip', 1)
               .match_sparse('sparse_vec', {"indices": [0, 10, 20], "values": [0.1, 0.2, 0.3]}, 'float', 'ip', 1)
               .fusion('match_tensor','column_name=t;search_tensor=[[0.0, -10.0, 0.0, 0.7], [9.2, 45.6, -55.8, 3.5]];tensor_data_type=float;match_method=MaxSim;topn=2')
               .match_text('title, body', 'hello world', 'topn=10')
               .fusion('weighted_sum', 'weights=0.8, 0.2')
               .to_pl()
</code></p><p></p><p>这些召回和融合排序手段，使得 Infinity 可以为 RAG 提供最强大易用的多路召回能力。在多路召回之外，Infinity 0.2 release 还引入了 引入了一种全新的数据类型——Tensor，在计算机科学中，Tensor 可以用来表达多个向量，多维数组，或者一个矩阵，为什么要支持这种类型呢？这要从 ColBERT 谈起。</p><p></p><p>ColBERT（参考文献 [4]）是一种排序模型，距离今天已经有四年了，是信息检索领域近年来引用次数非常多的知名论文。目前排序模型的架构有这样几类范式：</p><p></p><p>1. 双编码器。以 BERT 模型为例，它针对查询和文档分别编码，最后再经过一个 Pooling 层，使得输出仅包含一个向量。在查询时的 Ranking 阶段，只需要计算两个向量相似度即可，如下图所示。双编码器既可以用于 Ranking 也可以用于 Reranking 阶段。由于双编码器针对查询和文档分别编码，因此无法捕获查询和文档的 Token 之间的复杂交互关系。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/88/88d9c9a08aa8db66d0c023899490eaf0.png" /></p><p></p><p>2. 交叉编码器（Cross Encoder）。Cross-Encoder 使用单编码器模型来同时编码查询和文档，它能够捕捉查询和文档之间的复杂交互关系，因此通常能够提供更精准的搜索排序结果。Cross-Encoder 并不输出查询和文档的 Token 所对应的向量，而是再添加一个分类器直接输出查询和文档的相似度得分。它的缺点在于，由于需要在查询时对每个文档和查询共同编码，这使得排序的速度非常慢，因此 Cross-Encoder 只能用于最终结果的重排序。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a7/a7a8270f26694570a7a02a8dcd9e0caf.png" /></p><p></p><p>3. 延迟交互模型（ Late Interaction Model ），就是以 ColBERT 为代表的工作。它具备一些显著区分于其他排序模型的特点：其一是相比于 Cross Encoder，ColBERT 仍采用双编码器策略，将查询和文档分别采用独立的编码器编码，因此查询的 Token 和文档的 Token 在编码时互不影响，这种分离使得文档编码可以离线处理，查询时仅针对 Query 编码，因此处理的速度大大高于 Cross Encoder；其二是相比于双编码器，ColBERT 输出的是多向量而非单向量，这是从 Transformer 的最后输出层直接获得的，而双编码器则通过一个 Pooling 层把多个向量转成一个向量输出，因此丢失了部分语义。在排序计算时，ColBERT 引入了延迟交互计算相似度函数，并将其命名为最大相似性（MaxSim），计算方法如下：对于每个查询 Token 的向量都要与所有文档 Token 对应的向量进行相似度计算，并跟踪每个查询 Token 的最大得分。查询和文档的总分就是这些最大余弦分数的总和。例如对于一个有 32 个 Token 向量的查询（最大查询长度为 32）和一个有 128 个 Token 的文档，需要执行 32*128 次相似性操作，如下图所示。因此相比之下， Cross Encoder 可以称作早期交互模型 （Early Interaction Model）。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/fc/fcfed2822cdb43e8f594fff170cc62f9.png" /></p><p></p><p>下图从性能和排序质量上，分别对以上排序模型进行对比，并且也包含了全文搜索。图中的 Dense Encoder 既为双编码器，代表普通的向量搜索，它既可以做 Retriever，也可以做 Reranker。由于 ColBERT 的延迟交互机制，它既满足了对排序过程中查询和文档之间复杂交互的捕获，也能实现较快的排序性能，相同数据规模下， ColBERT 的效率可达 Cross Encoder 的 100 倍以上，兼顾了性能与效果，因此 ColBERT 是一种非常有前景的排序模型。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7d/7dd99d83ea24d351ef26bf010093d661.png" /></p><p></p><p>尽管如此，在使用上，ColBERT 仍然面临 2 个问题：</p><p></p><p>尽管采用了 MaxSim 延迟交互相似度函数，使得效率大大高于 Cross Encoder，但相比普通向量搜索，计算开销仍然很大：因为查询和文档之间的相似度，是多向量计算，因此 MaxSim 的开销是普通向量相似度计算的 M * N 倍 （M 为查询的 Token 数， N 为 文档的 Token 数）。除此之外，原始的 ColBERT 在排序质量上相比 Cross Encoder 略有差距，针对这些，ColBERT 作者在 2021 年推出了 ColBERT v2 （参考文献 [5]），通过 Cross Encoder 和模型蒸馏，改进了生成的 embedding 质量，并且采用压缩技术，对生成的文档向量进行量化，从而改善 MaxSim 的计算性能。基于 ColBERT v2 包装的项目 RAGatouille （参考文献 [6]）成为高质量 RAG 问答的解决方案。然而，ColBERT v2 只是一个算法库，端到端的让它在企业级 RAG 系统使用，仍然是一件困难的事情。由于 ColBERT 是预训练模型，而训练数据来自于搜索引擎的查询和返回结果，这些文本数据并不大，例如查询 Token 数 32 ， 文档 Token 数 128 是典型的长度限制。因此将 ColBERT 用于真实数据时， 超过限制的长度会被截断，这对于长文档检索并不友好。</p><p></p><p>基于以上原因， Infinity 在 0.2 版本中提供了 Tensor 数据类型，并基于此原生地提供端到端的 ColBERT 方案。</p><p></p><p>首先，Tensor 作为一种数据类型，ColBERT 编码输出的多向量，可以直接用一个 Tensor 来存放，因此 Tensor 之间的相似度就可以直接得出 MaxSim 打分。针对 MaxSim 计算，Infinity 给出了 2 种方案， 一种是 binary 量化，它可以让原始 Tensor 的空间只需原始尺寸的 1/32 ， 但并不改变 MaxSim 计算的相对排序结果。这种方案主要用于 Reranker，因为需要根据前一阶段排序的结果取出对应的 Tensor 。另一种是 Tensor 索引， Infinity 采用 EMVB 技术（参考文献 [7]）实现了 Tensor Index。EMVB 可以看作是 ColBERT v2 的改进，它主要通过量化和预过滤技术，并在关键操作上引入 SIMD 指令来加速实现。Tensor 索引可以用来服务 Retriever 而非 Reranker，因此结合 Infinity 的多路召回能力，用户可以进行如下各种召回选择：例如可以选择直接用 Tensor 提供语义搜索，从而实现比向量搜索更高的排序质量，也可以组合 Tensor 和全文搜索，用来做高质量的 RAG 所必备的 2 路召回，甚至可以组合向量搜索和 Tensor ，前者用来在大规模数据上粗筛，然后用 ColBERT 来快速精排，等等。Infinity 提供了足够强大的能力可以满足对于各种搜索召回的需求。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8c/8c5be21d1ecdd30043fe0ecd652c5d6c.png" /></p><p></p><p><code lang="nginx">res = table_obj.output(['*'])
               .match_tensor('t', [[0.0, -10.0, 0.0, 0.7], [9.2, 45.6, -55.8, 3.5]], 'float', 'maxsim')
               .match_text('title, body', 'hello world', 'topn=10')
               .fusion('weighted_sum', 'weights=0.8, 0.2')
               .to_pl(
</code></p><p></p><p>其次，针对超过 Token 限制的长文本，Infinity 引入了 Tensor Array 类型：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/0c/0c6296928d5a0295b3262fb5403d5bfa.png" /></p><p></p><p>一篇超过 ColBERT 限制的文档，会被切分成多个段落，分别编码生成 Tensor 后，都跟原始文档保存在一行。计算 MaxSim 的时候，查询跟这些段落分别计算，然后取最大值作为整个文档的打分。</p><p></p><p>从 0.2 release 开始， Infinity 提供了内置的 Tensor 数据类型，并解锁了端到端的 ColBERT 应用，这使得这种以延迟交互模型为代表的排序模型，可以在较大规模数据上直接提供高质量的排序结果，对于提升 RAG 的检索质量具有非常重要的意义。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c8/c8cbcad4e049fbf848abab8b2c257f4a.png" /></p><p></p><p>有了这么多召回手段，我们需要在真实数据集上进行相应的评测，以验证这些手段的效果。下边是 Infinity 在 MLDR 数据集上进行的评测结果，这也是 MTEB 默认采用的数据集之一。MTEB 是评估 Embedding 模型质量最权威的 Benchmark，目前排行榜上排名前列的模型基本都是基于 Cross Encoder 的编码器。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/37/3782330db0f38f9531bbf43b904fb593.png" /></p><p></p><p>从图中看到，混合 BM25 全文搜索，可以比单纯向量搜索有显著的提升。而采用全文搜索 + 向量搜索 + 稀疏向量，就是 Blended RAG[参考文献 3]，确实可以比单路搜索，以及两路混合搜索，有更好的查询质量。在 3 路混合搜索的基础上，进一步添加 ColBERT 做 Reranker，可以有进一步大的提升。同采用外部的 Reranker （例如 MTEB 排名前列的那些编码器）相比，采用 Hybrid search + ColBERT Reranker，它可以在数据库内部完成重排序，有着更高的效率，因此混合搜索可以进一步扩大 Top K 的范围（例如扩大到 Top 1000）之后再重排序，从而既保证最终召回质量还不影响性能，因此是一种性价比很高的高召回混合搜索方案。下图是各种召回方式添加 ColBERT Reranker 之后的提升效果总揽。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/e4/e45a76c28c10f956414aa5f0f833e8fb.png" /></p><p></p><p>需要说明的是，在不同数据集上，相同的召回手段可能得到不同的返回结果，但有一点是确定的，就是混合搜索的手段越多，返回质量越好。此外，上边的评测并没有涵盖用 Tensor Index 做 Ranker 组合，这是因为在具体实验中，我们发现用 Tensor 做 Reranker 的性价比要高很多，这会在我们后边的文章中详细阐述。因此推荐的最佳混合搜索方案是 Blended RAG + ColBERT Reranker。</p><p></p><p>Infinity 0.2 release，不仅提供了行业最全的混合搜索能力，还提供最快的混合搜索能力。下文来描述 Infinity 如何做到这一点。</p><p></p><p>Infinity 是一款在存储引擎和执行引擎层面都精细设计的数据库。如下是 Infinity 的执行引擎工作流程，可以看到，在完成针对 API 的查询绑定后，接下来执行计划会被编译成一个流水线执行计划。这种机制，常见于一些现代数据仓库，所不同的是，数据仓库的流水线执行，通常服务于并行执行，而 Infinity 的流水线，则同时服务查询的并行和并发执行，需要保证了高并发执行时查询算子的最佳调度策略和 CPU 亲和性，避免了无效上下文切换导致的开销。这种设计，使得查询的端到端开销非常小，完整的查询延迟并不会比运行单独的算法库增加多少。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/21/212e7380fe8495806bdb7284026cc306.png" /></p><p></p><p>上图的右边是一个多路召回的查询样例，图中包含 2 个数据 segment 上的向量搜索执行算子，2 个算子并行执行，以及一个向量搜索的 Top K 合并算子负责合并来自 2 个 segment 的向量搜索结果；还有一个全文搜索算子，两路召回最后是一个 Fusion 融合算子，这些算子在内存中形成一个 DAG 图，由查询执行器负责运行期调度。</p><p></p><p>存储引擎方面，Infinity 建立了完整的以列存为基础的索引体系，对于多路召回的每一路，都有相对应的索引负责高性能检索，这也使得 Infinity 添加新的类型支持变得非常方便。因此，Infinity 可以看做是一个以列存为基础的全索引数据库，这跟近期 OpenAI 收购的 Rockset 有着相似的特性，而在索引的类型上，Infinity 则提供更加丰富的选择。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/12/1256f2eb7a48746bcdea9927b43fceb3.png" /></p><p></p><p>下边来看 Infinity 的索引实现。</p><p></p><p>跟许多向量数据库一样，默认情况下 Infinity 也采用了 HNSW 作为向量索引的实现。但 Infinity 的 HNSW 进行了一系列深入优化，具体来讲，就是对每个需要建立索引的向量进行局部自适应量化——通过对每个向量进行缩放和量化操作来提升搜索性能，使得相似性计算速度极快，有效带宽降低，同时减少内存占用，却几乎不影响准确性，因此实质上是这一种压缩技术。</p><p></p><p>具体的，Infinity 对每个向量采用了两级量化：其中一级量化是针对每个向量和全部向量的均值之间的差值进行量化编码。一级量化主要在 HNSW 图遍历期间使用，通过将向量压缩到较少的规模，从而有效减少实际消耗的内存带宽，提高搜索性能。二级量化负责对前述差值后的残差进行量化编码，它主要用于最后的相似度比对，提高查询精度。局部量化技术因为只针对每个向量进行量化，并不改变任何向量之间的最近邻关系，因此具备随机内存访问模式，所以特别适合基于图索引的相似度搜索。在 Infinity 中，HNSW 索引只基于一级量化的结果来构建，所以查询性能和内存占用都大大优于传统的 HNSW 索引。除了局部量化技术之外，Infinity 采用大量 SIMD 针对距离做加速计算，得益于这些设计，Infinity 的向量搜索性能超出同类许多，下图是 Infinity 和其他向量数据库的 benchmark 对比：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/5b/5be587662785f0b93918f70b81ceaef0.png" /></p><p></p><p>针对全文索引，Infinity 也是采取了全新实现的方案，而没有引入一些流行的全文索引库如 Tantivy，Lucene 等等。这是因为全文索引只是 Infinity 的一个组件，它需要紧密地跟存储引擎和执行引擎高效率协同工作。这体现在几个方面：</p><p></p><p>全文索引需要支持实时数据插入。全文索引包含倒排索引和前向索引，而前向索引的能力，跟数据库的功能是重叠的，因此简单地整合，必然导致不必要的冗余。全文索引还需要跟其他索引返回的结果一起做融合排序，这需要搜索的逻辑跟执行引擎紧密配合。</p><p></p><p>除却以上因素，Infinity 是一款专用于 RAG 的数据库，对于 RAG 来说，它需要根据用户的提问搜索到答案，由于用户的提问可能会比较长，因此在默认情况下，查询的关键词之间不能提供“AND”语义而应提供“OR”语义，否则很容易导致零召回。然而，“OR”语义对性能是极大地损害，因为任何一个关键词命中的结果都会被打分，并送到最终的结果排序，所以全文索引需要采用动态查询剪枝技术，减少不必要的打分和排序。</p><p></p><p>例如近十年来学术界最佳的动态查询剪枝方案，是以 WAND，MaxScore 等为代表的系列技术。尽管全文搜索是一个相对成熟的领域，然而在当下，也只有 Lucene，Tantivy 等少数全文索引库具备生产级的算法实现。Infinity 实现了完整的 Block Max WAND 和 Block Max MaxScore 技术，两种查询动态剪枝策略适应的场景略有不同，在默认情况下，Infinity 选择采用 Block Max WAND （参考文献 [9]）来作为首选剪枝策略。</p><p></p><p>WAND 是 Weak AND 的缩写，它针对全文搜索最常见的打分手段 BM25 进行查询时动态剪枝，通过计算每个关键词贡献的上限来估计最终 Top K 结果的上限，并以此为阀值来决定在倒排索引的上如何快速跳过不必要的文档 ID，从而得到提速的效果。每个关键词贡献的上限，根据该关键词的的 IDF（在多少文档中出现） 和最大 TF（在文档中出现的最大词频） 来确定。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f0/f05a8e6d7b23cb7d880756f128191d72.png" /></p><p></p><p>上图是 Infinity 和 Elasticsearch 的全文搜索性能对比，测试方法如下：</p><p></p><p>索引数据集为 wikipedia 33M，数据集大小 32GB。从数据集中根据词频生成词表，按照 IDF 词频分布百分比分别随机选取关键词生成查询。查询长度从 3 个 Term 到 19 个 Term，生成的查询文件在这里_（<a href="https://github.com/infiniflow/benchmark/tree/main/enwiki_queries%EF%BC%89_%E3%80%82Infinity">https://github.com/infiniflow/benchmark/tree/main/enwiki_queries）_。Infinity</a>" 和 Elasticsearch 均采用默认 Top-K Union 的语义（OR）进行查询。Infinity 和 Elasticsearch 均给予一定预热时间，使得索引数据尽可能缓存在操作系统的 pagecache 中。</p><p></p><p>可以看到，不论是长查询，还是短查询， Infinity 相比 Elasticsearch 均具备压倒性优势，并且在测试过程中 Infinity 的内存消耗仅有 Elasticsearch 的 1/2。因此，提供 RAG 所必备的混合搜索能力（全文搜索 + 向量搜索），此前用户的唯一选择是 Elasticsearch（包括 Opensearch），而现在不仅仅多了 Infinity 这个选项，而且在性能上也远远超过了这些选择。</p><p></p><p>针对稀疏向量索引，Infinity 采用了跟全文搜索类似的设计，都采用倒排索引 + 查询动态剪枝的策略，所不同的是，稀疏向量首先按照区块组织成前向索引，倒排索引只用来存放跟固定区块有关的信息，查询时用来从一个区块跳转到另一个区块，而具体的相似度计算，则通过前向索引来进行。因此，稀疏向量索引并没有包含一个标准的倒排索引，而是基于 Block 的倒排索引跟前向索引的混合方案。该具体算法来源于 SIGIR 2024 的 Best Paper Runner Up 论文（参考文献 9）。</p><p></p><p>下图是 Infinity 跟知名向量数据库 Qdrant 在稀疏向量索引上的性能评测：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c0/c0f25fd2d5fb68736917fab8889fdf76.jpeg" /></p><p></p><p>由此可见，在稠密向量、稀疏向量、全文搜索三种召回手段上， Infinity 的性能均达到了极致，再加上强大的多路召回能力，以及各种的 Reranker 尤其是基于张量的 Reranker，可以说 &nbsp;Infinity 不仅仅是目前最快的 RAG 专用数据库，也是最强大的 RAG 数据库选择。欢迎关注和 Infinity ：<a href="https://github.com/infiniflow/infinity">https://github.com/infiniflow/infinity</a>"</p><p></p><p>参考文献</p><p></p><p>SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval, <a href="https://arxiv.org/abs/2109.10086">https://arxiv.org/abs/2109.10086</a>" , 2021Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation, <a href="https://arxiv.org/abs/2402.03216">https://arxiv.org/abs/2402.03216</a>"Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers, <a href="https://arxiv.org/abs/2404.07220">https://arxiv.org/abs/2404.07220</a>" , 2024Colbert: Efficient and effective passage search via contextualized late interaction over bert, SIGIR 2020.Colbertv2: Effective and efficient retrieval via lightweight late interaction, arXiv:2112.01488, 2021.RAGatouille <a href="https://github.com/bclavie/RAGatouille">https://github.com/bclavie/RAGatouille</a>"Efficient Multi-vector Dense Retrieval with Bit Vectors, ECIR 2024.Ding, Shuai and Suel, Torsten. Faster top-k document retrieval using block-max indexes. SIGIR 2011Mallia, Antonio and Suel, Torsten and Tonellotto, Nicola, Faster learned sparse retrieval with block-max pruning. SIGIR 2024</p><p></p><p>今日好文推荐</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651207433&amp;idx=1&amp;sn=b0d3776443b6844e19dcc2e371d790cf&amp;chksm=bdbbcf5a8acc464c548ee33611d68afb58a21d5407fae1d87f4c88b11ed8ccd699a1815d33fb&amp;scene=21#wechat_redirect">剥离几百万行代码，复制核心算法去美国？TikTok 最新回应来了</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651213953&amp;idx=1&amp;sn=b156eb405598aca141cc6386729c7d5d&amp;chksm=bdbba8d28acc21c443167e281526d1872a82e052d862bd7a661f3481b9c8a134fe66133d3d17&amp;scene=21#wechat_redirect">GitHub 删除代码等于“任何人均可永久访问”！微软回应：我们有意为之</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651213942&amp;idx=1&amp;sn=7c3fe2deb89258036864bd208b83946a&amp;chksm=bdbba8258acc21332a97b8f37d34e284761295eb5959d310696d1d49556e68eaba3df3f4240c&amp;scene=21#wechat_redirect">中科大保卫处要求硕士以上学历，校方回应：偏技术型；字节跳动“代码抄袭”案在美获受理；私人文档被“投喂”豆包？官方否认 | Q资讯</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651213781&amp;idx=1&amp;sn=6aa2bf475e97beec6e82508aab16fae6&amp;chksm=bdbbb7868acc3e90e0ac1e2183b049f0aaf62d3277b58ad7c875ff33fc4f3834a6ff0b1b09df&amp;scene=21#wechat_redirect">程序员三个月前就攻破并玩透的 SearchGPT，OpenAI 可算发布了</a>"</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/93/93a6e30a75e8e663d639c54513765ef5.gif" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/m7Rkreo4lWQjZ1LFBO8I</id>
            <title>程序员三个月前就攻破并玩透的SearchGPT，OpenAI 可算发布了</title>
            <link>https://www.infoq.cn/article/m7Rkreo4lWQjZ1LFBO8I</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/m7Rkreo4lWQjZ1LFBO8I</guid>
            <pubDate></pubDate>
            <updated>Wed, 31 Jul 2024 12:20:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, SearchGPT, AI 驱动, 搜索引擎
<br>
<br>
总结: OpenAI 正式发布了备受期待的搜索市场新产品——SearchGPT，这是一款由 AI 驱动的搜索引擎，能够实时访问互联网信息。该搜索引擎试图对用户提出的问题进行整理和解释，提供实时信息和来源链接。目前处于原型阶段，由 GPT-4 系列模型支持，初期仅向少量测试用户开放。未来计划将搜索功能整合到 ChatGPT 中。 </div>
                        <hr>
                    
                    <p>OpenAI 正式宣布备受期待的搜索市场新产品——SearchGPT，这是一款由 AI 驱动的搜索引擎，能够实时访问互联网信息。</p><p>&nbsp;</p><p>该搜索引擎以一个大型文本框开始，询问用户“您在寻找什么？”但与返回普通链接列表不同，SearchGPT 试图对这些信息进行整理和解释。</p><p>&nbsp;</p><p>例如，用户在 SearchGPT 中搜索“2024 年 8 月北卡罗来纳州布恩的音乐节”。该模型提供了从网络抓取的实时信息，包括来源链接。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/f0/f01a254b0967cc149b317a77aff2d95b.png" /></p><p></p><p>&nbsp;</p><p>在另一个示例中，SearchGPT 解释了何时种植西红柿，并详细介绍了不同品种的西红柿。结果出现后，用户可以继续提问或点击侧边栏打开其他相关链接。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/90/90b2163dff420fa539de17af3fcf59f9.jpeg" /></p><p></p><p>&nbsp;</p><p>还有一个名为“视觉答案”的功能，但OpenAI 没有详细解释其工作原理。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/e7/e7a352cb9b37fa5c41184e91bc7b90e8.png" /></p><p></p><p>&nbsp;</p><p>SearchGPT 的“视觉答案”功能展示了由 OpenAI 的 Sora 生成的 AI 视频。</p><p>&nbsp;</p><p>SearchGPT 目前仅是一个“原型”，该服务由 GPT-4 系列模型提供支持，初期仅向 10,000 名测试用户开放。OpenAI CTO Mira Murati表示最终目标是将搜索功能直接整合到 ChatGPT 中。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/63/631fd56ff45ff4bd78878f55a4258da7.jpeg" /></p><p></p><p>&nbsp;</p><p></p><blockquote>我们的 SearchGPT 原型现已上线。我们正在寻找反馈意见，以便准备将这一体验集成到 ChatGPT 中。</blockquote><p></p><p>&nbsp;</p><p></p><h2>谷歌股价暴跌</h2><p></p><p>&nbsp;</p><p>这个新产品已经被传闻了几个月，一些 X 用户还注意到 OpenAI 一直在开发的新网站。据说原本是计划于4月发布的产品，推迟到现在足足晚了三个月。另外，据外媒 The Verge 五月份的报道，OpenAI 一直在积极招募 Google 搜索团队的员工，但引用的消息人士没有透露 OpenAI 已经招募了多少位员工。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/8a/8a0528f1a57d5e73e43cc2c721412fb1.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>早在2月份，The Information就曝出消息称，OpenAI正在开发一款网络搜索产品来挑战谷歌。</p><p>&nbsp;</p><p>到了4月，AIPRM Corp 首席工程师Tibor Blaho在推特上表示，Sonic - SNC（SearchGPT）代理似乎已经处于评估阶段，具有图像搜索、各种小组件（如天气、计算器、体育、金融和时区差异），还可以进行后续提问。模型选用了GPT-4 Lite（Scallion；POR）、GPT-4 或 GPT3.5（Sahara-V），并结合了不同的搜索引擎，包括 Bing（POR）、Sydney、Fortis 和内部搜索（Labrador）。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/f3/f3b319714703e3f638272811c9e6aead.png" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/bc/bcc52c16a8eff75371dfd05dd38ca8eb.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>同时他还于4月29日给出了SearchGPT的短视频预览，基本与其当前展示的SearchGPT预览视频相差无几。</p><p>&nbsp;</p><p>虽然今天只发布了几个示例，但已经有眼尖的网友挑出其中的错误：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/e6/e6945e91ac336320196b7f738eddc1c3.jpeg" /></p><p></p><p>&nbsp;</p><p>有网友表示，在这种情况下，搜索结果应该给出“找不到答案，但这是最接近的匹配项”，而不是给出“幻觉”。</p><p>&nbsp;</p><p>某种程度来说，除了引用来源之外，它与现今的 ChatGPT 并没有太大区别。</p><p>&nbsp;</p><p>另一个问题是速度。Google 之所以成为互联网的入口，是因为它非常快。Google 对其速度非常自豪，甚至会显示生成响应所需的时间，而且总是以秒的几分之一计。相比之下，生成式 AI 的速度更适合用“每分钟多少字”来衡量，就像评判打字员一样。当你只是想做些简单的事情或去某个地方时，坐在那里等待几秒钟，看着文字一个字一个字地慢慢出现，可能会令人烦躁。比如现在一次 Google 搜索用了 0.41 秒生成了一整页文本，OpenAI 的搜索引擎需要多长时间呢？</p><p>&nbsp;</p><p>虽然目前还看不出SearchGPT比Google搜索强在哪里，但Sam Altman倒是雄心勃勃，他认为现在的搜索还有更多改进空间，并且alpha 版本将于下周开始向 Plus 订阅用户推出！</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/8a/8a140d7572f18f18498ff125595b49fd.jpeg" /></p><p></p><p>&nbsp;</p><p>这可能还是标志着对 Google 构成重大威胁的开始。Google 急于在其搜索引擎中加入 AI 功能，担心用户会转向这些新的竞争产品，以至于Google 在推出 AI Overviews&nbsp;时建议我们在披萨上放胶水。这也使 OpenAI 与初创公司 Perplexity 形成更直接的竞争，后者自称为 AI “答案”引擎。Perplexity 最近因其 AI 摘要功能被批评，有出版商声称该功能剽窃了他们的作品。</p><p>&nbsp;</p><p>OpenAI 似乎已经注意到此前的反响，并表示将采取截然不同的方法。在一篇博客文章中，该公司强调，SearchGPT 是与多家新闻合作伙伴合作开发的，这些合作伙伴包括《华尔街日报》、美联社等组织。Wood 表示：“新闻合作伙伴提供了宝贵的反馈意见，我们将继续寻求他们的意见。”他们写道，出版商将有办法“管理他们在 OpenAI 搜索功能中的展示方式”。他们可以选择不将其内容用于训练 OpenAI 的模型，但仍然可以出现在搜索结果中。</p><p>&nbsp;</p><p>根据 OpenAI 的博客文章，“SearchGPT 旨在通过在搜索结果中显著引用并链接到出版商，帮助用户与出版商建立联系。”回答中有明确的内嵌命名引用和链接，因此用户可以知道信息的来源，并可以快速通过侧边栏的来源链接与更多结果互动。</p><p>&nbsp;</p><p>谷歌股价在OpenAI演示SearchGPT立即暴跌。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/a4/a45044a6cf9b739ff4f2fa2ab3f4c6bc.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>服务器和人才成本高昂</h2><p></p><p>&nbsp;</p><p>OpenAI 的快速进展为 ChatGPT 赢得了数百万用户，但该公司的成本也在不断增加。The Information 基于此前未披露的内部财务数据和该公司知情人士的说法，认为这家 ChatGPT 开发商今年可能亏损高达 50 亿美元。</p><p>&nbsp;</p><p>具体来看，在成本方面，据一位直接了解支出的人士透露，截至今年 3 月，OpenAI 已花费近 40 亿美元租用微软的服务器，为 ChatGPT 及其底层 LLM 提供支持（即推理成本）。除了运行 ChatGPT 外，OpenAI 的训练成本（包括数据费用）今年可能会飙升至 30 亿美元。</p><p>&nbsp;</p><p>一位直接了解决策的人士表示，去年，OpenAI 加快了训练新 AI 的步伐，超出了最初的计划。该公司早些时候计划在这类成本上花费约 8 亿美元，但最终支出远高于预期。《The Information》估计，今年这类成本将翻番，因为 OpenAI 不仅在训练其旗舰 LLM 的新版本，还开始训练一种新的旗舰模型。</p><p>&nbsp;</p><p>此外，OpenAI 目前雇佣了约 1500 名员工，员工数量还在迅速增加，预计员工成本约为 15 亿美元。这主要是由于与谷歌等巨头激烈争夺技术人才。</p><p>&nbsp;</p><p>根据知情人士透露，OpenAI 预计 2023 年的人力成本为 5 亿美元，到 2023 年年底，员工人数增加一倍，达到约 800 人。从那以后，员工人数几乎又增加了一倍。该公司在官网上列出的近 200 个空缺职位，也许意味着 2024 年下半年可能会增加更多员工。</p><p>&nbsp;</p><p>综合来看，OpenAI 今年的运营成本可能高达 85 亿美元。而就收入而言，ChatGPT 最近的年收入有望达到约 20 亿美元。</p><p>&nbsp;</p><p>OpenAI 向访问其大模型API的开发人员收费，截至今年 3 月，该业务每月创造的收入超过 8000 万美元。</p><p>&nbsp;</p><p>最近，OpenAI 每月的总收入为 2.83 亿美元，这意味着其全年收入可能在 35 亿美元至 45 亿美元之间，具体取决于下半年的销售额。</p><p>&nbsp;</p><p>如果从最高 45 亿美元的收入中扣除 85 亿美元的潜在成本，则可能导致 40 亿美元至 50 亿美元的亏损。另外，SearchGPT 用户只会进一步推高计算成本。SearchGPT 在初期发布时对订阅用户将是免费的，鉴于该功能目前没有广告，显然公司需要尽快解决货币化问题。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://chatgpt.com/search">https://chatgpt.com/search</a>"</p><p><a href="https://www.theverge.com/2024/5/7/24151616/openai-is-entering-the-search-game">https://www.theverge.com/2024/5/7/24151616/openai-is-entering-the-search-game</a>"</p><p><a href="https://x.com/btibor91/status/1783603187993252338">https://x.com/btibor91/status/1783603187993252338</a>"</p><p><a href="https://x.com/kifleswing/status/1816542216678179083">https://x.com/kifleswing/status/1816542216678179083</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/qyQm6YYJLt1Bw1jbOjUE</id>
            <title>缺卡、缺电、缺组网技术！谁能为马斯克构建出全球最强大的10万卡超级集群？</title>
            <link>https://www.infoq.cn/article/qyQm6YYJLt1Bw1jbOjUE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/qyQm6YYJLt1Bw1jbOjUE</guid>
            <pubDate></pubDate>
            <updated>Wed, 31 Jul 2024 12:12:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 埃隆·马斯克, GPU, xAI, 融资
<br>
<br>
总结: 埃隆·马斯克掌控的公司需要大量GPU，为此他必须筹集资金并规划最优用途，xAI公司成立后获得大笔融资，马斯克也从特斯拉获得巨额薪酬用于发展。在竞争中，xAI必须展现出对计算、存储和网络的需求，推出的Grok系列模型也在不断发展，马斯克为了获得更多GPU甚至建立了“计算超级工厂”。英伟达等公司也在积极参与这一领域的竞争。 </div>
                        <hr>
                    
                    <p>埃隆·马斯克掌控的那几家公司——包括SpaceX、特斯拉、xAI乃至X（原Twitter）——都需要大量的GPU，而且也都是为自己的特定AI或者高性能计算（HPC）项目服务。但问题在于，市场上根本就没有充足的GPU能够满足他们各自宏伟目标所承载的勃勃野心。为此，马斯克必须为自己所能得到的有限GPU规划出最优用途。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/1b/1b0b7eb60f9ff65c38d4f42d1367ad0a.png" /></p><p></p><p>&nbsp;</p><p></p><h2>筹集资金比筹集GPU容易得多</h2><p></p><p>&nbsp;</p><p>早在2015年，马斯克就慧眼独具地成为OpenAI的联合创始人。而在2018年的一场权力斗争之后（我们猜测这场斗争很可能与推动AI模型所消耗的巨额资金，以及对于此类AI模型的治理思路有直接关系），马斯克离开OpenAI并让微软有了可乘之机。软件巨头携大笔资金入驻，并推动OpenAI迅速成长为一股开发生产级生成式AI的主导性力量。面对这样的现实，马斯克果断于2023年4月成立xAI公司，自此之后这家初创公司也一直在努力筹集资金并争取GPU配额，希望建立起足以对抗OpenAI/微软、谷歌、亚马逊云科技、Anthropic等知名大厂的计算基础设施。</p><p>而其中，筹集资金显然是最简单的部分。</p><p>&nbsp;</p><p>截至5月底，Andreessen Horowitz、红杉资本、Fidelity Management、Lightspeed Venture Partners、Tribe Capital、Valor Equity Partners、Vy Capital和Kingdom Holding（沙特王室控股公司）纷纷加入xAI总额60亿美元的B轮融资，一举推动其融资总值来到64亿美元。这是个好的开始，更幸运的是马斯克从特斯拉的全球经营中拿到了450亿美元的薪酬收益，因此可以随时把这笔巨款投入到xAI GPU的后续发展身上。（当然，更明智的作法应该是保留一部分作为特斯拉、X和SpaceX的GPU采购基金。）</p><p>&nbsp;</p><p>从特定角度来讲，特斯拉相当于是一次性付清了马斯克于2022年4月收购X所投入的全部440亿美元，同时又额外给了他10亿美元。这笔钱足够作为备用资金买下2.4万个GPU集群。必须承认，作为电动汽车的先驱力量，特斯拉已经撼动了整个汽车行业，其2023年的销售额为968亿美元，其中净利润为150亿美元，公司目前掌握的现金则为291亿美元。但即使是在如今这个财富分配极不公平的时代，450亿美元的回报仍然是个相当离谱的薪酬方案。但马斯克有他的大事要做，所以他主导的董事会愿意牺牲掉特斯拉的利益，拿出更多资本哄这位时代的骄子开心。</p><p>&nbsp;</p><p>不过按照同样的市值逻辑来判断，我们似乎也可以用6500亿美元买下摩根大通，而资金来源仍然是美国银行、阿布扎比、美联储以及我们能说动的其他资方。这样到了明年，我们就能给自己开出比收购成本略高一点点的薪酬——比如说6750亿美元。这样还清贷款之后，咱还能剩下250亿美元随便花花……抱歉跑题了，但这种情景真是想想都让人开心。</p><p>&nbsp;</p><p>总之从目前的情况看，xAI必须在计算、存储和网络层面表现出旺盛的需求。</p><p>&nbsp;</p><p>Grok-0大语言模型拥有330亿个参数，是在xAI成立几周之后就于2023年8月开始训练。Grok-1拥有可响应提示词的对话式AI功能，有着3140亿参数，于2023年11月上市。该模型随后于2024年3月开源，很快Grok-1.5模型也正式亮相。与Grok-1相比，1.5版本有着更长的上下文窗口和更高的认知测试平均绩点。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/f9/f96fa87cf49eb2f914f61594ae59dab5.png" /></p><p></p><p>&nbsp;</p><p>可以看到，Grok-1.5的智能程度略低于谷歌、OpenAI和Anthropic等竞争对手打造的同类模型。</p><p>即将推出的Grok-2模型将于8月之内与大家见面，该模型计划在2.4万张英伟达H100 GPU上进行训练。另据报道，该模型采用的是甲骨文的云基础设施。（甲骨文已经与OpenAI签署一项协议，允许其使用xAI未能尽用的剩余GPU容量。）</p><p>&nbsp;</p><p>马斯克曾在多条推文中表示，Grok-3也将在今年年底问世，需要10万个英伟达H100 GPU集群上接受训练，并将能够与OpenAI和微软正在开发的下一代GPT-5模型相媲美。甲骨文和xAI也积极就GPU容量分配方式讨论协议。但三周前价值100亿美元的GPU集群交易破坏消息一出，马斯克当即决定转变方向，在田纳西州孟菲斯南部的一处旧伊莱克斯工厂建造起“计算超级工厂”，用以容纳他自有的10万个GPU集群。如果大家恰好身在孟菲斯周边，接下来的情况可能有点疯狂——因为xAI号称将占用150兆瓦的区域供电。</p><p>&nbsp;</p><p>据彭博社的报道，目前该处工厂已经分配到8兆瓦供电，未来几个月内有望增加到50兆瓦。而要想继续超越这个数字，则需要经过田纳西河谷管理局的繁琐审批。</p><p>&nbsp;</p><p>不过目前来看除非英伟达愿意鼎力相助，否则马斯克似乎不太可能在今年12月之前拿到自己全部的10万张H100&nbsp;GPU。</p><p>&nbsp;</p><p>寻求英伟达这种芯片的公司名单很长，可能包括当今大多数大型科技公司，但只有少数几家公司公开宣称他们拥有多少H100芯片。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/8d/8d6981217df534dccffd238a82f0ba6a.jpeg" /></p><p></p><p>来源：The Information</p><p>&nbsp;</p><p>据《The Information》报道，风险投资公司Andreesen Horowitz正囤积超过2万块昂贵的GPU，作用是将其出租给AI初创公司以换取对方公司股份。</p><p>&nbsp;</p><p>OpenAI也一直没有透露他们拥有多少H100芯片，但据《The Information》报道，该公司以大幅折扣租用了微软提供的专用于训练的处理器集群，这是微软对OpenAI 100亿美元投资的一部分。据报道，这个训练集群的算力相当于12万块Nvidia上一代的A100 GPU，并将在未来两年内花费50亿美元从Oracle租用更多的训练集群。</p><p>&nbsp;</p><p>特斯拉一直在努力收集H100。今年4月，马斯克在一次财报电话会议上表示，特斯拉希望在年底前拥有3.5万到8.5万块H100。</p><p>&nbsp;</p><p>为了给xAI筹集GPU，马斯克最近还被特斯拉股东起诉，指控他将原本用于汽车制造商AI训练基础设施的12,000块H100芯片转给了xAI。在昨天的特斯拉第二季度财报电话会议上，当被问及这一调配问题时，马斯克表示，这些GPU之所以被送往xAI，是因为“特斯拉的数据中心已经满了，实际上没有地方可以放置它们。”</p><p>&nbsp;</p><p></p><h2>10万张H100的单一集群，谁有能力构建出来？</h2><p></p><p>&nbsp;</p><p>上周马斯克曾发推文表示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/65/65277adffbf83961417b987bc82f4a60.png" /></p><p></p><p></p><blockquote>xAI、X、英伟达和各支持部门都做得很好，孟菲斯超级集群训练已经于当地时间凌晨4：20启动。其单一RDMA结构上承载有10万张液冷H100 GPU，这是世界上最强大的AI训练集群！要实现在今年12月之前训练出全球最强AI模型的目标，这一切无疑是个显著的优势。</blockquote><p></p><p>&nbsp;</p><p>也许马斯克的这套系统最终会被称为SuperCluster，也就是Meta Platforms对于采购来、而非自建AI训练系统时指定的称呼。</p><p>&nbsp;</p><p>另外10万张GPU这个结论恐怕只是个愿景，也许到12月时xAI能拿到的GPU总共也只有2.5万张。但即使是这样，此等规模仍足以训练出一套体量庞大的模型。我们看到的部分报告指出，孟菲斯超级集群要到2025年晚些时候才能最终完成扩展，按目前的GPU供应能力来说这话其实颇为合理。</p><p>&nbsp;</p><p>另外，上线后，孟菲斯超级集群的供电也是一个问题，不过马斯克也并没有说到底启动了多少张H100。有网友讽刺道，马斯克的这种说法在极端情况下确实是成立的，比如只启动了 1 个 GPU 进行训练，而其他 99,999 个 GPU 并没有足够的电源来连接。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/be/be5bac1bbef5fd419129bd2583ceea9b.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><blockquote>目前只有3.2万块上线，其余将在第四季度上线。如果达到10万块GPU，要么变电站提前完工，要么需要更多这样的设备。</blockquote><p></p><p>&nbsp;</p><p>我们还可以从Supermicro公司创始人兼CEO Charles Liang的推文中做点推断，该公司正负责为xAI孟菲斯数据中心部署水冷设备：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/08/08297374353fdc30478c778c62dc3097.png" /></p><p></p><p>&nbsp;</p><p></p><blockquote>很高兴能与马斯克一同创造历史，与他的孟菲斯团队合作也是一段美好的经历！为了达成目标，我们必须尽可能完美、快速、高效且环保地推进工作——虽然需要付出很多努力，但也同样极具意义而且令人兴奋！</blockquote><p></p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/87/87e515e2e83952e0eded52fcdfeb393b.png" /></p><p></p><p>图片来源：Charles Liang</p><p>&nbsp;</p><p>目前还不清楚关于服务器基础设施的具体信息，但我们强烈怀疑这套系统将采用八路HGX GPU基板，并且属于Supermicro的机架式系统，其设计灵活来自英伟达的SuperPOD配置方案，但同时又有独特的工程调整以降低价格水平。采用八路HGX基板，该系统总计可容纳1.25万个节点，后端网络将承载10万张GPU和10万个端点；前端网络同样拥有1.25万个端点，即用于访问集群中数据和管理类负载的节点。</p><p>&nbsp;</p><p>瞻博网络首席执行官Rami Rahim也讨论了该公司参与孟菲斯超级集群项目的情况：</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bdd0a0b28d465d62b8c201ad014d8f05.png" /></p><p></p><p>&nbsp;</p><p></p><blockquote>恭喜马斯克、xAI和X！很高兴瞻博网络成为孟菲斯超级集群团队中的一员，并将我们的网络解决方案融入到这项创新工程当中。</blockquote><p></p><p>&nbsp;</p><p>从这些推文的内容来看，瞻博方面似乎是以某种方式拿下了孟菲斯超级集群的网络交易。考虑到Arista Networks和英伟达也在AI集群网络方面拥有深厚积累，马斯克最终选择瞻博着实令人感到惊讶。我们还没有从Arista那里看到与孟菲斯项目有关的任何消息；但在5月22日，英伟达在发布其2025财年第一季度财报时，公司首席财务官Colette Kress曾经表示：</p><p>&nbsp;</p><p></p><blockquote>“今年第一季度，我们开始针对AI发布经过优化的全新Spectrum-X以太网网络解决方案。其中包括我们的Spectrum-4交换机、BlueField-3 DPU和新的软件技术，用以克服以太网承载AI工作负载时面临的挑战，为AI处理提供1.6倍于传统以太网的网络性能。Spectrum-X的销量也在不断增长，吸引到众多客户，包括一个庞大的10万GPU集群项目。Spectrum-X为英伟达网络开辟出了全新的市场，使得纯以太网数据中心也能够容纳大规模AI类负载。我们预计Spectrum-X将在未来一年内跃升为价值数十亿美元的产品线。”</blockquote><p></p><p>&nbsp;</p><p>首先需要承认一点，这个世界上肯定没有多少项目能够豪爽地叫出“10万张GPU”这么夸张的体量，所以英伟达在5月声明中提到的几乎必然就是孟菲斯超级集群。再结合最近马斯克对于该系统的评价，我们认为英伟达应该是依靠Spectrum-X设备拿下了后端（或者叫东西向）网络部分，而瞻博则负责实现前端（或者叫南北向）网络部分。Arista那边则没有任何动静。</p><p>&nbsp;</p><p>但截至目前，我们仍不清楚孟菲斯超级集群具体会使用哪种存储解决方案。其可能是基于Supermicro的闪存加硬盘混合型原始存储阵列，可运行任意数量的文件系统；也可能是Vast Data或者Pure Storage提供的全闪存阵列。但如果非要选出一种赢面最大的方案，那我们会大胆认为Vast Data应该是参与了这笔交易，并拿下规模可观的存储订单。不过这种猜测也没有明确的依据，只是根据该公司大规模存储阵列过去两年在高性能计算和AI领域表现出的市场吸引力提出的假设。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.nextplatform.com/2024/07/30/so-who-is-building-that-100000-gpu-cluster-for-xai/">https://www.nextplatform.com/2024/07/30/so-who-is-building-that-100000-gpu-cluster-for-xai/</a>"</p><p><a href="https://sherwood.news/tech/companies-hoarding-nvidia-gpu-chips-meta-tesla/">https://sherwood.news/tech/companies-hoarding-nvidia-gpu-chips-meta-tesla/</a>"</p><p><a href="https://techcrunch.com/2024/06/13/tesla-shareholders-sue-musk-for-starting-competing-ai-company/">https://techcrunch.com/2024/06/13/tesla-shareholders-sue-musk-for-starting-competing-ai-company/</a>"</p><p><a href="https://www.youtube.com/watch?v=ktkCRVxTuEI&amp;t=1325s">https://www.youtube.com/watch?v=ktkCRVxTuEI&amp;t=1325s</a>"</p><p><a href="https://digitalassets.tesla.com/tesla-contents/image/upload/IR/TSLA-Q2-2024-Update.pdf">https://digitalassets.tesla.com/tesla-contents/image/upload/IR/TSLA-Q2-2024-Update.pdf</a>"</p><p><a href="https://x.com/dylan522p/status/1815710429089509675">https://x.com/dylan522p/status/1815710429089509675</a>"</p><p><a href="https://www.reddit.com/r/mlscaling/comments/1ea3vu1/xais_100k_h100_computing_cluster_goes_online/">https://www.reddit.com/r/mlscaling/comments/1ea3vu1/xais_100k_h100_computing_cluster_goes_online/</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/G0S20B379QecxZu8LL4g</id>
            <title>电商搜索革命：大模型如何重塑购物体验？| AICon</title>
            <link>https://www.infoq.cn/article/G0S20B379QecxZu8LL4g</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/G0S20B379QecxZu8LL4g</guid>
            <pubDate></pubDate>
            <updated>Wed, 31 Jul 2024 08:26:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 电商搜索技术, 技术演进, 大模型应用, 电商AI助手
<br>
<br>
总结: 随着电商行业的发展，搜索技术在连接用户与商品方面变得越来越重要。电商搜索技术经历了文本检索、机器学习、深度学习和大模型阶段。大模型在电商领域主要应用于用户交互、意图理解、商品召回和相关性等方向。然而，大模型应用也面临着商品知识理解、个性化效果、时效性、成本和速度、安全等实际落地困难。京东通过持续预训练和RAG等技术手段解决实时信息更新问题。构建一个好的大模型电商搜索引擎需要考虑大模型的能力、用户理解、商品理解、电商场景理解等原则。企业对大模型的投入产出比目前还不高，但大模型应用的商业潜力值得探索。理想中的下一代AI电商搜索应该是完全大模型驱动，具有全模态自然语言交互和智能下单等功能。 </div>
                        <hr>
                    
                    <p>随着电商行业的蓬勃发展，搜索技术作为连接用户与商品的桥梁，其重要性日益凸显。在技术不断革新的今天，电商搜索技术经历了哪些阶段？面对大模型的飞速发展，企业又将如何把握趋势，应对挑战？为了深入探讨这些问题，我们特别采访了京东技术总监翟周伟，探讨了电商搜索技术的发展历程、当前的应用状况以及面临的挑战和未来的发展方向。以下是采访的详细内容。</p><p></p><p>另外翟周伟老师将在 8 月 18 日至 19 日的 AICon 上海站上，带来题为《<a href="https://aicon.infoq.cn/202408/shanghai/presentation/6016">电商大模型及搜索应用实践</a>"》的精彩演讲。此外，大会还将涉及更多关于大模型在搜索、广告、推荐领域的探索等热门话题。感兴趣的朋友，不妨点击原文链接，查看大会的详细日程安排，期待与您在 AICon 上海站相遇！</p><p></p><p></p><h5>InfoQ：在您看来电商搜索经历了哪些阶段？</h5><p></p><p></p><p>翟周伟：我从技术发展的角度讲下，本质上电商搜索技术演进的驱动力是通过不断的技术创新去实现更低的成本，更高的效率，以及更好的用户体验，可以划分为 4 个阶段：</p><p></p><p>第一是文本检索阶段，主要基于基础文本检索技术和以规则统计为主的人货匹配；</p><p></p><p>第二是机器学习阶段，以统计 NLP 技术为核心的用户意图理解和商品理解，利用机器学习模型对 UCTR 和 UCVR 进行建模提升转化，并在人货匹配上引入 LTR 等排序模型提升相关性，同时利用用户搜索行为反馈数据来优化效果；</p><p></p><p>第三是深度学习阶段，核心是 DNN 技术驱动，包括基于深度模型的意图理解和商品理解显著提升了需求分发的准确性，在商品搜索上引入了 ANN 语义向量召回，多模态召回，DNN 匹配技术，交互上除了文本交互还支持以 DNN 技术为核心的语音和图像商品搜索交互，排序上支持个性化搜索可以千人千面的商品展示；</p><p></p><p>第四是大模型阶段，正是当下正在经历的阶段，首先是交互的改变，从单向的需求引导到双向的对话式自然语言交互，基于大模型的用户理解和商品理解有效解决了长尾泛化问题，在召回和相关性上大模型也正在重构整个技术架构，包括极具有颠覆潜力的大模型生成式检索技术的探索和应用。</p><p></p><p></p><h5>InfoQ：在京东或者在电商平台，大模型主要应用于什么方向？可否举几个例子？</h5><p></p><p></p><p>翟周伟：在电商领域主要在用户交互，意图理解和商品理解，商品召回和相关性，以及文案创意生成等方向。在用户交互上重点利用大模型的对话能力进行对话式交互导购，例如我们的京言 AI 助手，意图理解和商品理解上核心是利用大模型的超强理解能力进一步提升用户需求识别的准确性以及商品信息的精准建模，商品召回和相关性上的一个典型例子就是用大模型做商品的增强召回，用大模型对用户需求和商品 SKU 做相关性，文案生成应用上利用大模型来生成图文并茂的营销文案，大模型评论总结等。</p><p></p><p></p><h5>InfoQ：这些应用有哪些实际落地的困难？</h5><p></p><p></p><p>翟周伟：第一个首要问题就是通用大模型对商品知识的理解能力比较弱，直接应用没有明显效果优势；第二个问题就是个性化 Context 理解问题，大模型在理解用户购物偏好，用户评论，商品细节上存在个性化效果挑战；第三个就是时效性问题，大模型本身数据更新很慢，知识陈旧，而新商品，促销，价格等时效性更新超高频；第四个就是成本和速度问题，大模型训练和推理成本很大，大规模使用会面临 ROI 低的问题，在线推理速度也很难满足系统实时性要求；最后就是安全问题，大模型存在敏感数据泄露风险，以及生成内容的安全合规等问题。</p><p></p><p></p><h5>InfoQ：大模型实时信息获取和专业信息处理能力的能力应该是非常重要的一环，京东的解决思路是什么？目前可以做到什么程度或者说效果？</h5><p></p><p></p><p>翟周伟：我们的解决思路主要通过两种方法，第一是新数据和新知识的增强持续预训练，第二就是 RAG，包括电商知识图谱 KG-RAG，商品搜索 RAG，Web 搜索 RAG。通过这些技术手段我们的大模型在电商领域任务上显著高于通用大模型，并且通过 RAG 可以做到实时性信息更新。</p><p></p><p></p><h5>InfoQ：如何构建一个好的大模型电商搜索引擎？在您看来需要哪些原则或者考虑？</h5><p></p><p></p><p>翟周伟：大模型电商搜索引擎的核心是所使用大模型的能力决定的，这个能力不仅仅是在通用领域的性能，关键在于大模型对于电商用户的理解，对商品的深刻理解，对电商场景的理解，以及在整个电商应用任务上的性能表现，因此核心考虑是如何构建一个高性能的电商大模型能力底座，以及性能评估体系。</p><p></p><p>这个也是当前我们的工作重点，启发于人类学习总是在前人积累的知识和经验上进一步学习，我们提出了一种继承学习方法来持续学习，在数据上通过提升知识密度和配比调整，通过模型结构优化，退火学习，多阶段指令对齐优化，增强安全治理对齐等方法提升我们电商大模型的性能表现。</p><p></p><p></p><h5>InfoQ：大家都说大模型好，但是目前行得通的商业模式目前还不多，您认为企业，大家对于大模型的投入产出比如何？</h5><p></p><p></p><p>翟周伟：大模型表现的涌现能力让人震惊，基于大模型的各种应用层出不穷，但到目前为止还没有出现所谓的大模型超级应用，也没有出现颠覆性的商业模式，由于大模型研发成本非常大，各种基于大模型的原生应用，包括一线大厂和创业公司的各种 AI 助手类应用还都处于亏损阶段，因此 ROI 是很低的，但大模型应用的商业潜力巨大，这种商业探索非常值得。</p><p></p><p>同时另一个方面是利用大模型去优化成熟商业模式业务中的效果，重点在于解决中长尾问题，在这个方向上 ROI 还是可以的，已经产生了商业价值。</p><p></p><p></p><h5>InfoQ：理想中的下一代 AI 电商搜索是什么样的？</h5><p></p><p></p><p>翟周伟：理想的下一代 AI 电商搜索在技术上应该是完全大模型驱动或 AGI 技术驱动，产品形态上应该是一个数字虚拟助理，类似电影《Her》中出现的超级 AI 助手，在交互上可以和人类进行全模态的自然语言交互，可以直接进行无障碍的流畅语音交互，同时具有听觉，视觉，以及空间感知等能力，可以精准基于用户需求直接推荐最匹配的商品，并给出精准的商品总结，以及为什么满足需求，性价比等，在需求不明的时候可以进行拟人的交互式导购，并可以智能的通过 AI Agent 技术在用户授权下自动下单，包括后续的物流，售后服务都可以由 Agent 来完成，用户只需要下达命令即可。</p><p></p><p></p><h5>InfoQ：可以看的出京东也是比较重视大模型的投入的，在您看来一般企业开发或应用大模型的门槛是不是比较高？对于企业开发和应用大模型有哪些建议？</h5><p></p><p></p><p>翟周伟：在我看来，对于一般企业做大模型应用的门槛其实是相对比较低，至少是比传统的 AI 应用门槛低，是因为大模型改变了 AI 应用的研发范式，只需要会写 prompt 就可以使用大模型，而且效果不错，虽然应用门槛变低了，但要做好大模型应用的门槛是变高了，是因为一旦涉及到具体的业务场景深度效果优化就需要优化大模型本身，而大模型研发成本和技术门槛还是比较高，首先训练和推理资源投入就很大，数据也需要很大成本，技术上涉及到预训练，SFT，DPO，PPO，MOE 等技术，要做好还是有门槛的。在应用上我有两点建议：</p><p></p><p>第一：如果偏向创新产品，建议初期直接调用大模型云服务 API，以 prompt 方式先把产品做出来，先跑通商业模式，尽快建立用户反馈，有一定用户规模后再考虑是否研发自己的大模型。</p><p></p><p>第二：如果需要优化大模型，建议以性能和商用友好的开源大模型作为底座从而降低训练成本，可结合应用在预训练增强，指令对齐上进行优化，也可以开源模型做初始化进行模型扩展或压缩以适应业务需求。</p><p></p><p></p><h5>嘉宾介绍：</h5><p></p><p></p><p>翟周伟，现任职京东零售大模型技术总监，前华为智能协作领域 AI 助手首席专家，前昆仑万维天工大模型高级总监，在 AI 助手，NLP 和搜索领域有十多年研发实践经验，在 AI/NLP 领域申请超过 15 项发明专利并出版两部著作，曾任华为 - 北大联合语音语义实验室研究观察员，在华为，百度期间主导构建了业界一流的 AI 算法系统并落地 AI 助手以及搜索场景，在大模型方向上主导过业界知名大模型的核心技术研发，目前专注于大模型技术以及在 AI 助手搜推等领域的应用探索和实践。</p><p></p><p></p><h5>活动推荐</h5><p></p><p></p><p>8 月 18-19 日，AICon 全球人工智能开发与应用大会将在上海举办。来自字节跳动、华为、阿里巴巴、微软亚洲研究院、智源研究院、上海人工智能实验室、蔚来汽车、小红书、零一万物等头部企业及研究机构的 60+ 资深专家，将带来 AI 和大模型超全落地场景与最佳实践分享，帮助与会者提升技术视野、获得有价值的实践指导。大会火热报名中，7 月 31 日前可以享受 9 折优惠，单张门票节省 480 元（原价 4800 元），详情可联系票务经理 13269078023 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/35/35014bd5f1e8c93fd4cc748450969079.jpeg" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/LwtOzQJuVh94QqQIy7PA</id>
            <title>打破垄断，迎接AI革命的多样化未来</title>
            <link>https://www.infoq.cn/article/LwtOzQJuVh94QqQIy7PA</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/LwtOzQJuVh94QqQIy7PA</guid>
            <pubDate></pubDate>
            <updated>Wed, 31 Jul 2024 07:07:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Ines Montani, 大型语言模型, 开源软件, AI领域
<br>
<br>
总结: Ines Montani 在伦敦 QCon 大会上演讲，探讨了大型语言模型如何改变AI领域。开源软件打破了AI领域的垄断控制，带来透明度、无锁定、内部运行、社区审核、保持最新、可编程性、易上手使用和可扩展性等益处。开源软件的经济维度不仅是免费，更在于可访问性和自由度。AI领域的开源涉及代码和数据的协同作用，包括任务特定模型、编码器模型和大型生成模型。对LLM的误解在于区分编码器模型和大型生成模型。规模经济作用于大型生成模型，使其通过API访问。AI领域的重要区别在于面向人类和面向机器的系统之间的区别。 </div>
                        <hr>
                    
                    <p>Ines Montani 在 2024 年 4 月 <a href="https://qconlondon.com/presentation/apr2024/ai-revolution-will-not-be-monopolized-how-open-source-beats-economies-scale?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjI0MDg2NzEsImZpbGVHVUlEIjoiMWQzYU05cDQyN0hYNHIzZyIsImlhdCI6MTcyMjQwODM3MSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTA2fQ.xHjpGwTdVefq6x9DKcoyHb46ZfXJKkBywU9_h1tR2mM">伦敦 QCon 大会</a>"上的演讲阐述了大型语言模型（LLM）如何深刻地改变了 AI 领域。这些模型背后的创新核心其实非常简单：让模型变得更大。随着每一次迭代，这些模型的能力边界都在不断扩展，而这就引发了一个关键问题：我们是否正步入一个由少数科技巨头掌控的黑箱时代，而这些巨头隐藏在 API 和专有技术的帷幕之后？</p><p></p><p></p><h2>开源软件的对立</h2><p></p><p></p><p>与这种忧虑形成鲜明对比的是，开源软件正在打破 AI 领域的垄断控制。开源项目确保了没有任何单一实体能够独揽AI领域的主导权。开源软件带来了诸多益处，使其成为个人和企业的理想选择：</p><p></p><p>透明度：开源软件是透明的，你可以确切地了解你将从中得到什么。无锁定：你不会被特定的供应商锁定。虽然存在一些承诺，但你永远不会失去对软件的访问权。内部运行：开源软件可以在内部环境运行，这对于保护私有数据来说至关重要，特别是对于那些不愿意将数据发送到外部服务器的人来说。社区审核：社区审核意味着你可以知道哪些功能受到广泛欢迎，哪些解决方案被哪些用户采用，确保一定程度的信任和可靠性。保持最新：开源项目通过合并请求和社区贡献，不断融入最新的研究成果，因此能够保持最新状态。可编程性：软件非常具有可编程性，很少需要端到端的解决方案，可以轻松集成到现有的流程中。易上手使用：开源软件可以轻松上手使用，你只需要使用像 pip install 这样简单地命令即可下载并开始使用。可扩展性：软件可扩展，用户可以自行分叉和运行。</p><p></p><p></p><h2>开源软件的经济维度</h2><p></p><p></p><p>关于开源软件的一个普遍误解是，企业选择它主要是因为它是免费的。虽然许多开源项目可以免费获取，但它们的真正价值在于它们的可访问性和提供的自由度。虽然成本因素在初始采纳阶段起到了推动作用，但开源解决方案之所以能够占据主导地位，还有许多其他令人信服的理由。</p><p></p><p>AI 和机器学习领域的开源不仅是关于软件本身，更涉及代码和数据的协同作用。不断增长的开源模型生态系统，涵盖了从底层代码到训练数据和模型权重的各个层面，极大地提高了这些工具的可访问性。为了更清晰地理解这个领域，我们可以将这些模型大致分为以下三种类型：</p><p></p><p>任务特定模型：这些是专为特定任务量身定制的专用模型。例如，与 <a href="https://spacy.io/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjI0MDg2NzEsImZpbGVHVUlEIjoiMWQzYU05cDQyN0hYNHIzZyIsImlhdCI6MTcyMjQwODM3MSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTA2fQ.xHjpGwTdVefq6x9DKcoyHb46ZfXJKkBywU9_h1tR2mM">spaCy</a>" 及其社区项目一同发布的模型、斯坦福大学Stanza库中的模型，以及 Hugging Face 等平台上的众多模型。这些模型通常体积小巧、响应迅速且运行成本低廉。不过，它们在泛化能力上可能有所局限，往往需要通过特定领域的数据进行微调。</p><p></p><p>编码器模型：这些模型，例如谷歌的 BERT 及其各种衍生版本，被设计用来为任务特定模型生成嵌入向量。它们以相对较小的体积、快速的处理速度以及低廉的内部运行成本著称，相较于任务特定模型，它们具有更好的泛化能力。尽管如此，为了适应特定的应用场景，它们仍然需要进行一定程度的微调。</p><p></p><p>大型生成模型：这一类模型包括 Falcon、Mistral 和 LLaMA 等。这些模型明显更大，运行速度较慢，运行成本更高，但在泛化和适应性方面表现出色，几乎不需要微调即可执行特定任务。</p><p></p><p></p><h2>对 LLM 的误解</h2><p></p><p></p><p>“大语言模型”一词被泛泛而用，缺乏精确性，导致人们对这些模型能力和应用的讨论变得模糊不清。因此，弄清楚编码器模型和大型生成模型之间的区别非常重要。编码器模型专注于任务特定的网络架构，用于预测结构化数据，而大型生成模型依赖提示词生成自由形式的文本，并需要进一步的逻辑分析来提取有用的信息。</p><p></p><p></p><h2>规模经济的作用</h2><p></p><p></p><p>大型生成模型因其复杂性和高昂的运营成本，通常只能通过 OpenAI 和谷歌等公司提供的 API 来访问。这些公司利用规模经济的优势，从汇聚顶尖人才、批发计算资源和大规模请求中受益。这种运营模式就像繁忙都市中的火车时刻表，高需求确保了服务的频繁和规律性。</p><p></p><p></p><h2>面向人类和面向机器的 AI 之间的区别</h2><p></p><p></p><p>AI 领域的一个重要区别是面向人类的系统和面向机器的模型之间的区别。对于像 ChatGPT 和 Google Gemini 这样面向人类的系统，核心优势在于产品特性，包括用户体验、用户界面和定制化，通常包含了约束机制，以防生成不恰当的内容。这些产品直接与用户交互，并依赖用户的反馈来不断优化和增强其功能。相比之下，像 GPT-4 和 Bard 这样的底层模型是构成更广泛系统的关键组件，是上面这些面向消费者应用的基础。面向机器的模型是可互换的组件，基于公开发布的研究和数据，可以通过速度、准确性、延迟和成本来评估其性能。</p><p></p><p>理解这些 AI 应用类型之间的区别至关重要，这有助于消除人们对 AI 垄断的误解。像 OpenAI 这样的公司可能在面向用户的产品市场上占据主导地位，但这并不意味着它们在背后的AI和软件组件领域同样占据主导。虽然用户数据对于优化面向人类的产品至关重要，但对于提升基础的面向机器的任务，其重要性相对较小。获取一般性知识并不依赖于特定数据，而这恰恰是大型生成模型背后的创新精髓。</p><p></p><p></p><h2>AI 在实践中的能力</h2><p></p><p></p><p>AI 在实践中的能力可以分为生成性任务和预测性任务：</p><p></p><p>生成式任务：摘要、推理、问题解决、问答、释义和风格转换是由生成模型提供的能力。预测式任务：文本分类、实体识别、关系提取、指代消解、语法分析与形态学研究、语义解析以及话语结构分析这些任务涉及将非结构化文本数据转换为结构化表示形式，以便人类更好地理解和利用这些数据。</p><p></p><p>尽管生成式 AI 提供了许多新的可能性，但许多行业挑战仍然存在，主要集中在如何结构化非结构化数据（如自然语言）上。AI 的出现使我们能够更高效、更大规模地解决这些问题，从而生成更多结构化数据和顺利完成项目。</p><p></p><p></p><h2>计算机指令交互方式演变</h2><p></p><p></p><p>向计算机发出指令的过程经历了几个阶段的演变：</p><p></p><p>基于规则的系统：最初，我们使用条件逻辑和正则表达式提供规则或指令。机器学习：引入示例编程，也称为监督学习，其中模型是使用特定示例训练的。上下文学习：通过自然语言形式（提示词）提供规则和指令。</p><p></p><p>每种方法都有其优缺点。指令直观且易于非专家使用，但对数据变化过于敏感，很容易受到数据漂移的影响。示例非常具体，能够精确表达复杂的行为，但生成它们往往需要大量的人力和时间。那么，结合这两种方法，使用大型通用模型和特定数据来开发专注的、针对特定任务的模型的工作流程会是什么样子呢？</p><p></p><p></p><h2>实际应用和迁移学习</h2><p></p><p></p><p>实际的 AI 工作流涉及迭代评估和纠正模型预测，使用迁移学习将通用模型提炼为特定模型。迁移学习在实际应用中仍然具有相关性，可以实现模块化、可解释和具有成本效益的解决方案。</p><p></p><p>使用大型生成模型有助于解决冷启动问题，使原型能够立即可用。这些原型在后续可以被提炼和转化为更小、更快、更专注的模型。这种方法避免了在一开始就生成大量示例的劳动密集型过程，并减少了在运行时对庞大、复杂模型的依赖。</p><p></p><p></p><h2>任务特定模型的人工参与提炼</h2><p></p><p></p><p>提炼任务特定模型遵循同样的软件开发最佳实践：</p><p></p><p>模块化：高度模块化符合软件开发最佳实践，有助于维护现代工作流程并相应地调整模型开发。无锁定：用户不受任何特定供应商的约束。模型可以由不同的供应商开发，但用户可以独立控制和管理它们。可测试性：组件可以独立测试，与不透明的单一黑盒系统相比，更容易监控和检测故障。灵活且运行成本低：模型是系统中的灵活组件，可以针对特定的硬件平台进行优化以实现高效运行（甚至是在 CPU 上）或占用较小的空间，从而显著降低运营成本。内部运行：这对于安全处理敏感数据来说至关重要，因为不依赖外部API可以确保数据的隐私性和合规性。透明度和可预测性：用户可以了解模型的工作原理，更好地理解和预测模型行为。可编程性：模型可以通过编程的方式集成到现有的工作流中，这不仅符合业务需求，也将集成过程中可能出现的挑战降至最低。</p><p></p><p>这些也是公司选择开源软件的原因，这并非巧合：AI 开发仍然是一种软件开发类型，相同的原则也适用于 AI 开发。</p><p></p><p></p><h2>解决关注点和监管</h2><p></p><p></p><p>规模经济一度被认为是形成垄断的关键因素，但现在这在技术领域面临着挑战，因为激烈的竞争降低了成本。在开发而不是生产阶段依赖成本已经不那么高的开源模型，使得规模经济的重要性进一步降低。</p><p></p><p>监管成为大科技公司为确保其在该领域垄断地位而采取的另一种策略，他们游说政府制定一系列 AI 法规，而这些法规只有他们这些具备相应资源和专业知识的公司能够遵守。</p><p></p><p>确保监管的透明度对于 AI 在没有垄断控制的情况下健康发展来说至关重要。政策制定者应当明确区分 AI 在应用层面和核心层面的技术，培养鼓励创新的竞争环境，并确保消费者利益得到保护。这种区分对于引导 AI 朝着创新和普及的方向发展来说至关重要，在这样的未来，没有任何单一实体能够对市场施加过度的影响。</p><p></p><p></p><h2>结论</h2><p></p><p></p><p>AI 领域的开发和部署以透明和可访问性为特征，摒弃了以往依赖秘密优势的模式。在大语言模型领域，它们是系统的组成部分而不是独立的产品，并不能固有地依赖专有知识或独家数据访问获得垄断优势。</p><p></p><p>这些模型的可替代性和可补充性是促进互操作性和竞争的关键，这与垄断形成鲜明对比。开源软件在这方面扮演着至关重要的角色，不仅确保了技术灵活性，还通过协作开发和社区审查为创新提供了强大的推动力。</p><p></p><p>然而，监管措施可能在无意中偏袒那些已经占据市场主导地位的企业，所以监管的重点应该放在规范市场行为和具体的使用案例上，而不是针对特定的技术或软件组件。</p><p></p><p>这种平衡的监管策略对于确保 AI 领域的开发既具有竞争力又具有包容性来说至关重要。它还有助于抵御来自行业游说团体的不当影响，这些团体可能会试图为了自己的利益而扭曲监管框架。</p><p></p><p></p><p>原文链接：</p><p><a href="https://www.infoq.com/articles/ai-revolution-not-monopolized/">https://www.infoq.com/articles/ai-revolution-not-monopolized/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/l9Icsq0VbPVSP4XVbx9b</id>
            <title>直播预告| 技术峰会早班车：AI+重塑技术生产力</title>
            <link>https://www.infoq.cn/article/l9Icsq0VbPVSP4XVbx9b</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/l9Icsq0VbPVSP4XVbx9b</guid>
            <pubDate></pubDate>
            <updated>Wed, 31 Jul 2024 01:51:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能技术, 企业数智化, AI+, 2024全球商业创新大会
<br>
<br>
总结: 随着人工智能技术的飞速发展，特别是大模型的兴起，AI已经深刻地融入了我们的工作和生活之中。企业正迈入数字化与智能化并重的数智化时代。2024全球商业创新大会将探讨AI如何重塑企业技术生产力，加速企业数智化进程，展示AI对企业运营和技术的重塑，以及AI Agent的落地前景与挑战。 </div>
                        <hr>
                    
                    <p>随着人工智能技术的飞速发展，特别是大模型的兴起，AI已经不再是遥不可及的概念，而是深刻地融入了我们的工作和生活之中。企业正迈入数字化与智能化并重的数智化时代。8月10日，用友主办的2024全球商业创新大会-企业数智化技术峰会即将在北京召开。</p><p>&nbsp;</p><p>为了深入探讨A+如何重塑企业技术生产力？AI+如何加速企业数智化进程？企业数智化技术峰会有哪些值得期待的亮点？8月2日极客邦科技CGO汪丹将对话用友网络副总裁兼数智平台解决方案事业部罗小江，并进行线上直播，为企业界和科技爱好者带来一场思想盛宴。</p><p></p><p><img src="https://static001.geekbang.org/infoq/88/8861b2f3c7be60d4ead317010f1474ee.png" /></p><p></p><p></p><h2>直播亮点抢先看：</h2><p></p><p></p><p>一、&nbsp;AI如何深度改变企业运营</p><p>AI为企业带来的最直接改变和最有价值的点进行深入剖析。罗总将从智能运营的角度，分享AI如何助力企业优化流程、提升效率，实现更精准的决策和更快速的市场响应。</p><p></p><p>二、AI对企业级技术的重塑</p><p>AI如何作为一种基础技术能力，提升技术生产力，特别是大模型的出现如何带来生产力的飞跃。用友BIP AI整体能力将成为本次分享的重点，展示其在推动企业数智化转型中的关键作用。</p><p>&nbsp;</p><p>三、大模型应用现状与挑战</p><p>虽然大模型技术备受瞩目，但企业在实际应用中仍面临诸多挑战。您将会听到大模型的竞争格局，以及企业该如何明确大模型的应用场景和需求。</p><p>&nbsp;</p><p>四、AI Agent的落地前景与挑战</p><p>2024年被视为AI Agent应用落地元年，直播将深入探讨AI Agent受关注的原因、本质以及落地的难点，为企业探索AI Agent应用提供有价值的参考。</p><p>&nbsp;</p><p>五、2024全球商业创新大会技术亮点剧透</p><p>8月10日在北京召开的2024全球商业创新大会中，将展示用友BIP在平台与技术方面有何新进展。</p><p>&nbsp;</p><p>这是一次深入了解AI如何重塑企业技术生产力的绝佳机会。无论您是企业管理者、技术人员还是科技爱好者，都不可错过这场思想碰撞的盛宴。8月2日19:00，一起探索AI+的无限可能！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/kg8TWm1LxSiXi0Hqh6ut</id>
            <title>颠覆传统架构！华人科学家20年心血：AI 能效提高1000倍，未来需求井喷！</title>
            <link>https://www.infoq.cn/article/kg8TWm1LxSiXi0Hqh6ut</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/kg8TWm1LxSiXi0Hqh6ut</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jul 2024 09:54:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: CRAM, 自旋电子器件, 冯·诺依曼架构, 内存中计算
<br>
<br>
总结: 明尼苏达大学科学与工程学院的研究人员展示了一种名为CRAM的新型数据存储模型，通过使用自旋电子器件在内存中进行计算，颠覆了传统的冯·诺依曼架构，实现了内存中计算的概念，大幅降低了人工智能应用的能源需求。 </div>
                        <hr>
                    
                    <p>&nbsp;</p><p>近日，明尼苏达大学科学与工程学院的一组研究人员展示了一种数据永远不会离开内存的新模型，称为计算随机存取存储器（CRAM）。与目前的方法相比，它可以将人工智能应用的能源需求降低1,000 倍甚至更多。在一次模拟中，CRAM 技术显示出令人难以置信的 2,500 倍节能效果。</p><p>&nbsp;</p><p>论文地址：</p><p><a href="https://www.nature.com/articles/s44335-024-00003-3">https://www.nature.com/articles/s44335-024-00003-3</a>"&nbsp;</p><p></p><h2>怎么做到的？</h2><p></p><p>&nbsp;</p><p>众所周知，传统计算依赖于已有数十年历史的冯·诺依曼架构，该架构由独立的处理器和内存单元组成，需要不断来回移动数据，这是一个耗能过程。</p><p>&nbsp;</p><p>明尼苏达团队的 CRAM 完全颠覆了该模型，使用称为磁隧道结 (MTJ) 的自旋电子器件直接在内存内部进行计算。自旋电子设备并不依赖电荷来存储数据，而是利用电子自旋，为传统的基于晶体管的芯片提供了更有效的替代品。</p><p>&nbsp;</p><p>“作为一种极其节能的数字内存计算基板，CRAM 非常灵活，可以在内存阵列的任何位置执行计算。因此，我们可以重新配置 CRAM，来最好地满足各种 AI 算法的性能需求，”计算架构专家、论文合著者、明尼苏达大学电气与计算机工程系副教授 Ulya Karpuzcu 表示。“它比当今 AI 系统的传统构建块更节能。”</p><p>&nbsp;</p><p>Karpuzcu 解释道，CRAM 直接在存储单元内执行计算，有效利用阵列结构，从而无需缓慢且耗能的数据传输。</p><p>&nbsp;</p><p>据介绍，最高效的短期随机存取存储器 (RAM) 设备使用四到五个晶体管来编码 1 或 0，但 MTJ可以以极低的能量执行相同的功能，速度更快，并且能够适应恶劣环境。自旋电子器件利用电子自旋而不是电荷来存储数据，为传统基于晶体管的芯片提供了更高效的替代方案。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/99/99ec517edd372384abdfb7ea3f4bbfb5.png" /></p><p></p><p>受内存逻辑传输瓶颈困扰的传统计算机架构 (a) 与CRAM (b) 对比</p><p>&nbsp;</p><p>&nbsp;</p><p>CRAM 架构实现了真正的在内存中进行计算，打破了传统冯·诺依曼架构中计算与内存之间的瓶颈。这种“内存中计算”（in-memory computing）的方法，通过消除逻辑和内存之间耗电的数据传输，尤其适用于需要大量数据并行处理的应用，如深度学习、图像处理和大数据分析。</p><p>&nbsp;</p><p>研究人员估计，基于 CRAM 的机器学习推理加速器在能量延迟积方面比最先进的解决方案实现了 1000 倍的改进。另一个例子表明，CRAM（在 10 nm 技术节点）执行 MNIST 手写数字分类器任务分别消耗 0.47 µJ 和 434 ns 的能量和时间，与 16 nm 技术节点的近内存处理系统相比，它的能量和时间分别减少了 2500 倍和 1700 倍。</p><p>&nbsp;</p><p>明尼苏达大学电气与计算机工程系博士后研究员、论文第一作者杨吕说：“这项工作是 CRAM 的首次实验演示，其中数据可以完全在存储器阵列内处理，而无需离开计算机存储信息的网格。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/12/12c8625a27e7aff326af256776e37995.png" /></p><p></p><p>&nbsp;CRAM实验装置，该装置由定制硬件和控制软件套件组成</p><p>&nbsp;</p><p>CRAM 以全数字化方式运行，与报道的其他大多数内存计算方案不同，后者部分或大部分是模拟的。CRAM 还具有独特的附加功能，例如数据和操作数的随机访问、大规模并行计算能力以及操作的可重构性。</p><p>&nbsp;</p><p>另外，尽管大多数以前的内存计算范例所采用的无晶体管（交叉开关）架构支持更高的密度，但由于潜行路径问题，内存阵列的大小通常受到严重限制。CRAM 在其每个单元中都包含晶体管，以此来实现更好的可访问性，因此阵列尺寸更大。</p><p>&nbsp;</p><p>类似这种原型的 CRAM 技术对于在人工智能能源需求激增的时代大幅提高其能源效率至关重要。</p><p>&nbsp;</p><p>国际能源署 3 月份预测，全球用于人工智能训练和应用的电力消耗可能会增长一倍以上，从 2022 年的 460 太瓦时增至 2026 年的 1,000 多太瓦时——几乎相当于日本全国的用电量。</p><p>&nbsp;</p><p>国外研究报告显示，ChatGPT每天要响应大约2亿个请求，在此过程中消耗超过50万度电力，即ChatGPT每天用电量相当于1.7万个美国家庭的用电量。而随着生成式AI的广泛应用，预计到2027年，整个人工智能行业每年将消耗85-134太瓦时（1太瓦时=10亿千瓦时）的电力。</p><p>&nbsp;</p><p>研究人员2023年10月10日在《Joule》上发布的论文显示，在AI技术加持下，传统互联网运行的耗电量会成倍数增长。论文数据显示，一次标准谷歌搜索耗电0.3瓦时，AI大语言模型ChatGPT响应一次用户请求耗电约2.96瓦时，在AI大模型驱动下的一次谷歌搜索则耗电8.9瓦时。</p><p>&nbsp;</p><p>除了耗电，和ChatGPT或其他生成式AI聊天，也会消耗水资源。加州大学河滨分校研究显示，ChatGPT每与用户交流25-50个问题，就可消耗500毫升的水。而ChatGPT有超过1亿的活跃用户，这背后消耗的水资源无疑是令人震惊的。</p><p>&nbsp;</p><p>CRAM 的千倍能耗降低能力，显然有助于解决越来越被关注的AI耗能问题。</p><p>&nbsp;</p><p></p><h2>背后功臣：一位美籍华人的科研梦想与现实挑战</h2><p></p><p>&nbsp;</p><p>研究人员表示，这项研究已经进行了二十多年， “我们 20 年前直接使用存储单元进行计算的最初想法被认为是疯狂的”，该论文的资深作者、明尼苏达大学电气与计算机工程系杰出 McKnight 教授兼 Robert F. Hartmann 主席王建平 (Jian-Ping Wang) 说道。</p><p>&nbsp;</p><p>但明尼苏达团队坚持了下来，在王建平教授的专利 MTJ 研究基础上，开发出了磁性 RAM (MRAM)。MTJ 器件是一种纳米结构器件，用于改进硬盘、传感器和其他微电子系统，包括磁性随机存取存储器 (MRAM)，已用于微控制器和智能手表等嵌入式系统。</p><p>&nbsp;</p><p>王建平教授表示：“自 2003 年以来，随着学生群体的不断发展，以及明尼苏达大学建立起一支真正的跨学科教师团队——从物理学、材料科学与工程、计算机科学与工程到建模和基准测试以及硬件创建——我们能够取得积极的成果，现在已经证明这种技术是可行的，并且已经准备好融入技术中。”</p><p>&nbsp;</p><p>王建平是一位在美国待了二十多年的美籍华人，本科和硕士就读于兰州大学。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a26dc7fc2d6d7fd18ede37ab6eaba778.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>自2002年起，王建平教授在明尼苏达大学理工学院任职，担任电气和计算机工程系的罗伯特·哈特曼讲席教授，并担任先进信息技术自旋电子材料中心（SMART中心）的主任。</p><p>&nbsp;</p><p>作为新型磁性材料和自旋电子器件领域的世界知名专家，王建平教授的研究侧重于信息存储、记忆和计算以及生物医学传感方向。自 2008 年以来，王建平教授通过交换耦合复合介质（ECC）的开创性实验演示，高效利用了HDD（硬盘驱动器）的技术，通过减少数据中心的整体数量，节约了全球能量消耗。</p><p>&nbsp;</p><p>2013年，明尼苏达大学SMART中心获得了美国国家标准与技术研究院（NIST）和纳米电子计算机研究联盟（nCORE）2900万美元赞助，以研究下一代微电子技术。</p><p>&nbsp;</p><p>2022年，王建平教授当选美国国家发明家科学院（National Academy of Inventors - NAI）院士。当选之后，他在接受“移民国家（A Nation of Immigrants）”对话时，谈到自己的身份：不仅是教授，也是企业家和工程师，并且名下有三家公司。这三家公司中，Niron Magnetics是一家开发无稀土且环保的氮化铁永磁体的公司。而Zepto Life Technology公司则是利用了可用于疾病的早期检测的磁性生物传感技术。王建平教授还曾于2019年获得了“半导体公司创新奖”。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/6e/6e70b7edc4166f55e9519e4916338347.jpeg" /></p><p></p><p>&nbsp;图右为王建平，截图来自“移民国家（A Nation of Immigrants）”</p><p>&nbsp;</p><p>但另一方面，也有很多人对他的研究表示不理解。</p><p>&nbsp;</p><p>2013年的时候，王建平所在的研究团队对其“能提高计算机处理和内存效率”的新材料申请了专利，但是半导体行业内的人要求提供出该材料的样品出来，以证实他们的发明。当时他对材料的形容是：“我们使用了一种在过去几年中备受半导体行业关注的量子材料，但是我们以独特的生产方式使其具备了新的物理和自旋电子特性，从而可以极大地提高计算和内存效率。”</p><p>&nbsp;</p><p>如今，对于这个可以应用到目前热门的AI行业中的内存技术方案，业界也充满了不理解：</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/f2/f212de01f8b9ec6d0f98569d9ba2e3c9.jpeg" /></p><p></p><p></p><blockquote>作为一个研究项目，这很有趣，但近期内可能不会成为商业产品。宽松估计下，一个300毫米的晶圆上有67,000平方毫米的可用面积，但通常会少一些。一个450微米乘400微米的结点是0.18平方毫米，因此在一个晶圆上最多可以有约372,000个这样的MJTs，假设上面没有其他任何东西。这些数据不太对劲。研究人员需要发布更多数据，否则我无法相信这在经济上有潜力实现。</blockquote><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/9c/9c9c6c09dc8fefec13dba0b77adc94eb.jpeg" /></p><p></p><p></p><blockquote>在此使用 MRAM 并不能解决细粒度设计问题。</blockquote><p></p><p>&nbsp;</p><p>但如果这些真的从“研究”走向实际应用，也许人工智能的需求会迅速爆发。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2f/2fdc2425b030a8438dc60ab27b6126e1.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href="https://www.techspot.com/news/104005-breakthrough-cram-technology-ditches-von-neumann-model-makes.html">https://www.techspot.com/news/104005-breakthrough-cram-technology-ditches-von-neumann-model-makes.html</a>"</p><p><a href="https://www.nature.com/articles/s44335-024-00003-3#auth-Jian_Ping-Wang-Aff1">https://www.nature.com/articles/s44335-024-00003-3#auth-Jian_Ping-Wang-Aff1</a>"</p><p><a href="https://www.siscmag.com/PDF/2018/1011/Research1.pdf">https://www.siscmag.com/PDF/2018/1011/Research1.pdf</a>"</p><p><a href="https://www.reddit.com/r/technology/comments/1eexab8/breakthrough_cram_technology_ditches_von_neumann/">https://www.reddit.com/r/technology/comments/1eexab8/breakthrough_cram_technology_ditches_von_neumann/</a>"</p><p><a href="https://www.techspot.com/news/104005-breakthrough-cram-technology-ditches-von-neumann-model-makes.html#commentsOffset">https://www.techspot.com/news/104005-breakthrough-cram-technology-ditches-von-neumann-model-makes.html#commentsOffset</a>"</p><p><a href="https://www.sohu.com/a/518826972_121123994">https://www.sohu.com/a/518826972_121123994</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/KT82EUQ8O2Z4pMPTwrc8</id>
            <title>七麦数据：AI 搜索按下加速键，场景化能力让夸克率先突围</title>
            <link>https://www.infoq.cn/article/KT82EUQ8O2Z4pMPTwrc8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/KT82EUQ8O2Z4pMPTwrc8</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jul 2024 07:08:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 七麦数据, AI搜索, 夸克, 高考信息服务
<br>
<br>
总结: 七麦数据发布了2024年第二季度iOS实力AI产品排行榜，夸克作为AI搜索产品新兴势力以高分跃居榜首，验证了AI搜索在搜索赛道的商业革命。夸克将AI搜索能力应用在高考信息服务中，取得了巨大成功，展示了AI技术与信息服务场景的结合。夸克推出了AI搜索的超级搜索框，集合了智能回答、智能创作和智能总结三大能力，为用户提供一体化信息服务。七麦数据指出，夸克之所以脱颖而出，是因为将AI搜索的创新与用户需求和落地场景深度融合。 </div>
                        <hr>
                    
                    <p>近日，国内专业数据分析平台七麦数据发布了《2024 年第二季度 iOS 实力AI产品排行榜》，其中夸克作为 AI 搜索产品新兴势力，以 99.71 的高分在一众AI应用中跃居榜首。七麦数据提出，海内外均将 AI 搜索视作重要蓝海赛道，按下了加速键。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/8b/5b/8bb37ecd549f54eea34385596816615b.png" /></p><p></p><p>「Top50 AI 产品榜」是七麦数据基于 App 下载量和收入、用户好评度、榜单实力、品牌搜索热度等综合表现，考评得出的季度榜单。二季度统计七麦数据增加了「亮点领域」维度。在榜单前 10 名中，该维度下「智能助手」占据了四席，反映了当下 AI 产品的追逐热点。而「AI 搜索」维度下的「夸克」属于首次被纳入榜单，从一众智能助手中突围。</p><p></p><p>七麦数据表示，搜索不仅是绝大多数互联网产品的基础功能，同样也是互联网用户的核心需求。在AI的赋能下，搜索赛道的新一轮商业革命已然开场。</p><p></p><p>高考信息服务，就是夸克将AI搜索这个新能力，应用在了成熟场景。6 月 13 日，夸克宣布全面升级高考信息服务，考生、家长等可以通过高考 AI 搜索，询问各类与高考志愿相关的问题。也就是说，夸克将AI搜索能力率先在「夸克高考」这一个应用场景中落地。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/24/d7/2457a918051024b66b38561386ab14d7.png" /></p><p></p><p>七麦数据提到，在 6 月高考季「夸克高考」AI 搜索的使用量超过 1 亿次，夸克 App 在苹果 App Store 应用商店免费榜中多次霸榜并多次获得官方推荐。夸克 6 月份的表现，验证了刚需场景的爆发力。夸克不仅加速了 AI 搜索赛道战火的升级，也为整个行业的发展提供了新的思路和解法。</p><p></p><p>七麦数据进一步提出，自 2018 年发力智能搜索引擎开始，夸克一直在探索AI技术与信息服务场景的结合。除了高考信息服务之外，夸克在学习、办公等领域均有深厚积累。</p><p></p><p>这也是为何夸克在 7 月 10 日正式推出AI搜索，其核心交互形式是「超级搜索框」。夸克的搜索框聚合的是一个个刚需场景，用户打开搜索框，输入问题即可体验智能回答，还有 AI 写作、文件总结、视频总结、拍题讲解功能。一个「超级搜索框」集纳了智能回答、智能创作和智能总结三大能力。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/43/61/43d6b680e0328f7a3440c12d4486d961.png" /></p><p></p><p>此外，夸克还一站式提供网盘、扫描、文档、CueMe、学习助手、健康助手等内容产品和智能工具，为用户提供从检索、创作、总结，到编辑、存储、分享的一体化信息服务。</p><p></p><p>七麦在榜单结语中提出，如何创新并持续和用户需求及落地场景深度融合，是各大厂商竞逐时无法逃避的课题。而夸克 AI 搜索能够迅速脱颖而出，原因恰恰就是将 AI 搜索的创新与用户需求和落地场景进行了深度融合。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/IFr72JxbII5l83NBs1BH</id>
            <title>“豆包”编织职场梦：提效？搞创意？还有什么是AI做不到的？</title>
            <link>https://www.infoq.cn/article/IFr72JxbII5l83NBs1BH</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IFr72JxbII5l83NBs1BH</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Jul 2024 08:35:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 信息爆炸, 职场挑战, 智能助手, 创意生产
<br>
<br>
总结: 在信息爆炸、效率至上的时代，职场中的每一个角色都面临着令人头疼的工作挑战，市场分析师深陷信息海洋难以自拔，项目经理面对成山的文档束手无策，公关经理急需灵感点燃创意火花，创意总监渴望将抽象思维跃然纸上。智能助手“豆包”引领了提效之旅，解锁职场高效与创造力的金钥匙，创造了全新的创意生产方式。 </div>
                        <hr>
                    
                    <p>在信息爆炸、效率至上的时代，职场中的每一个角色都面临着令人头疼的工作挑战，市场分析师深陷信息海洋难以自拔，项目经理面对成山的文档束手无策，公关经理急需灵感点燃创意火花，创意总监渴望将抽象思维跃然纸上……大家都在寻找那把能够解锁职场高效与创造力的金钥匙，于是，一场由智能助手“豆包”引领的提效之旅正悄然展开，它不仅将职场人在枯燥的机械工作中解放出来，还创造了全新的创意生产方式。</p><p></p><p>今天的故事便从「四位职场角色借助“豆包”这一创新工具，在“搜”、“读”、“写”、“绘”四大领域实现前所未有的突破」出发，完成讲述——从信息洪流中精准捕捞关键数据，到长篇累牍中迅速提炼决策要点；从灵感枯竭到创意如泉涌，再到将无形理念转化为震撼人心的视觉盛宴，“豆包”以其强大的功能，成为了职场人得力的助手、最坚实的后盾。</p><p></p><p></p><h2>“搜”的突破：市场分析师的效率进化</h2><p></p><p></p><p>晚上九点，市场分析师李寒正坐在他的办公桌前赶着他的行业季度报告，桌面上堆满了报告和数据表格，电脑屏幕上的窗口不断闪烁，仿佛在提醒他——这是一个信息爆炸的时代。如何在这些海量数据中迅速捕捉到那些能够写进报告的关键信息，是他今天晚上必须要完成的命题。</p><p></p><p>按照往常习惯，在持续索引资料的李寒惊喜地发现了一个叫“豆包”的平台。</p><p></p><p>启动“豆包”，李寒输入一个问题：“第三季度消费电子市场哪些品牌表现最为抢眼？”豆包立刻返回了相关品牌的销售数据和市场份额，还附上了所有数据的来源，它们是来自专业媒体、机构的28篇报道报告。如果以传统的搜索方式，李寒很难迅速在浩如烟海的信息中快速抽取出这些权威内容，通过豆包，大量无效数据和网页信息被过滤掉了，搜索过程快到了极致。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b1ce23a695bd3a7c168ec327c2237780.gif" /></p><p></p><p>通过回答背后的这些内容链接，他也可以迅速反向进行搜索，从而为行业季度的分析提供灵感与帮助。</p><p></p><p>当然，豆包也同样可以辅助报告中的分析部分，李寒紧接着追问：“这些品牌的市场增长背后的原因是什么？”豆包理解了他的意图，迅速给到了消费电子市场的出货量等具体数据，数据之间的同比、环比、增长率等需要专业工具处理的内容被缩略在了一个答案内，简要快速的呈现在了屏幕上。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9c/9c562341c16bb521d1ae5cfd83e97b03.jpeg" /></p><p></p><p>但行业报告最重要的仍是“深度”。以往，李寒搜集完信息后就要开始“头脑风暴”，进一步对内容展开分析，但用豆包，只需要一键“深入搜索”，顷刻间就生成了一篇深度报告，除了出货量增长、竞争格局转变等基础分析外，市场价格受限的深层因素、消费电子新兴需求等很多重要角度也包含在其中，豆包给到了他之前从未想到过的思路。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fa/fa12c3c0986d983c91c178a3d6595729.png" /></p><p></p><p>对于一篇行业报告来说，大而全的分析是必要的，但产品案例、亮点提取同样重要，李寒选中了“NAND FLASH ”这个关键词，点击弹出的“AI搜索”，相关内容迅速被整合成一篇回答并呈现出来。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0c/0ce7e557e3a8873f4f6b245c66c1b675.gif" /></p><p></p><p>除此之外，针对回答中的部分内容，同样通过选中，也可以进行更细致的编辑与调整，比如翻译、语法修改、调整语气、文案改写、关键词提炼等，李寒不需要将其复制到文档中再进行修改，而是可以直接对内容进行调整。</p><p></p><p>问答+深入搜索+AI搜索+各种工具，来自全网的数据、内容，以及繁杂的AI工具被收束在一个工作流中。李寒只需要用自然、口语化的问题与豆包交流，并使用平台已有或者自己添加的工具进行修正，就为报告的内容搜集节省了大量时间。不用再翻阅如山的资料，遍览十几条网页，豆包帮助他快速构建起报告的框架，还为他提供了清晰、有力的市场分析，从而为企业战略提供有力的决策依据。</p><p></p><p><img src="https://static001.geekbang.org/infoq/25/254da5b27dcf3c8cb6a5e94926b5a4b9.png" /></p><p></p><p>通过使用豆包，李寒只用了半个小时的时间便完成了报告数据收集及分析的工作，并且在质量上也有所提升。有了这样的工具，他的工作节奏更加从容不迫，技术对于工作效率的赋能正切实的发生。</p><p></p><p></p><h2>“读”的智慧：项目经理的思维升级</h2><p></p><p></p><p>在日常的项目管理过程中，会议纪要和项目文档是项目经理最核心的文档，在项目周期紧张的情况下，面对着堆积如山的会议记录和项目文档，项目经理必须快速完成关键信息整理，并完成流程推进。</p><p></p><p>负责跨国项目的项目经理李明，每天在整理国外客户会议记录的工作上就要付出半天的时间，在同事的推荐下，李明开始尝试使用电脑版豆包，希望借助这个平台快速的解决“读”的问题。</p><p></p><p>首先要面对的第一个问题即是语言。与繁琐的阅读工作量相比，语言障碍更是横亘在他面前的一座大山，难以迅速把握其核心需求并作出精准项目安排。此时，豆包的伴读模式派上了用场，李明点击豆包的阅读总结，通过快速拖拽的形式，他将一篇关于AI Deepfake的外文报告导入进豆包中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1c/1c91d78d5bff1c91af822d81e53a9427.gif" /></p><p></p><p>值得注意的是，除了PDF、Word文档等文件形式，针对只能在线阅读的内容，豆包也可以直接输入网页链接进行阅读总结，这也能快速解决下载上传等问题，加快工作效率。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3e/3ed5039bd6b5ce608bef433e918eeccd.png" /></p><p></p><p>报告上传，随即开始生成翻译过后的总结内容，李明初步了解了报告的大致情况后，决定进行更细致的研究。点击“AI伴读”，豆包随即进入“阅读模式”，不仅能够对报告的具体内容进行实时翻译，还在侧边生成了问答栏，李明可以“边看边问”，豆包瞬间化身成为了“阅读助手”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/31/315428d2d44cab2a4109ae232ba40f01.gif" /></p><p></p><p>这种助力，能够为李明在阅读长篇文档时提供了强有力的支持，它能够迅速提炼出文档中的关键点。李明输入“帮我总结第八页的内容”，豆包瞬间生成了精炼的摘要，搭配着原文，大大提高了他的阅读效率。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e1/e12cb54896ad1d6544a80f714165256e.png" /></p><p></p><p>不仅是针对报告文档，在处理国外客户的会议记录时，通过其翻译与伴读功能，李明也能够快速准确理解客户需求，同时通过问答模式智能推荐相关资料，辅助他做出更加敏捷的决策。</p><p></p><p>与前面提到的“搜索”场景一样，在阅读总结模式时，只需要将鼠标停留在相关段落上，豆包即可针对内容提供AI搜索、翻译、复制等功能，李明针对报告中晦涩难懂的部分进行AI搜索，豆包也迅速帮他解答了疑问。</p><p></p><p><img src="https://static001.geekbang.org/infoq/00/003e8385189e88bf35910d121d06f2e7.png" /></p><p></p><p>豆包伴读采取的是高效率大篇幅处理，同时细节内容“精耕细作”的处理模式，通过copilot式的帮助，让李明快速的了解了整篇报告的基本内容，并迅速抽取了他所需的内容。在这一过程中，他能够高效地把握信息细节，并对内容进行高效处理，只用了十几分钟，李明就快速阅读完了这份报告，迅速投入到下一篇资料中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/ca81ba8cac1a21dc5d37df91eeeef911.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/eae5ba26303a6b64028af9c6cc528186.png" /></p><p></p><p>在这个信息爆炸式的时代，对于复杂内容的阅读-思考-理解是一个“熵增再到熵减”的过程，那么借助豆包伴读这样的工具，将有效地解决信息过载问题，缩短“熵增”，让散乱的思绪与碎片化的知识快速回归秩序。</p><p></p><p></p><h2>“写”的灵感：公关经理的创意发电厂</h2><p></p><p></p><p>资深公关经理张华的公司即将举办一场重要的发布会，但原本的发言人却因不可抗力无法到场，公司不得不紧急更换了发言人。但时间只剩下了几个小时，张华需要根据新发言人的身份重新撰写一篇演讲稿。这场发布会对公司未来的发展意义重大，它不仅将展示最新的产品与技术成果，还是向外界传递公司核心价值与愿景的重要机会。</p><p></p><p>基于丰富的公关经验，张华非常清楚，一份成功的演讲稿既要有深度又要有煽动力，但这需要时间和精力去打磨。然而时间过于紧迫，张华只能打开电脑桌面上的“豆包”，他将希望投向了“AI写作”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e6/e612c5fb8b59812fc5b2db2aca3af22e.png" /></p><p></p><p>万事开头难，他首先要通过AI功能帮自己梳理思路，根据这一次的发布内容，他在豆包的搜索栏中输入了关键词“产品发布会演讲稿”。豆包立即为他展示了一系列相关的文章和资料，其中包含了许多经典案例，这为他提供了初步的素材和灵感。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e4/e4b0d3a01f6999042f950f97e2be9230.png" /></p><p></p><p>接着，张华利用豆包的生成功能，根据搜索到的内容素材和自己的想法，快速生成了一份初步的大纲，根据他的要求，这份大纲并非单纯的产品发布，而是融入了市场洞察、研发故事、困难突破一系列故事线，同时也没有落下任何一个关键信息，开场白、公司介绍、产品特点、市场前景等一应俱全。</p><p></p><p>在调整完大纲后，张华再次对话豆包“根据大纲生成文章”。豆包根据他调整后的大纲，迅速为他生成了一篇初稿。这篇初稿不仅内容丰富、条理清晰，而且语言流畅、易于理解。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fa/fad7a2c045a7dbd87cddc988a8b6fea3.gif" /></p><p></p><p>但这份初稿仍需完善，张华利用豆包的编辑功能对初稿进行了深入的修改和完善，同时搭配豆包自带的“润色”功能，对整篇文章的用词语句进行了调整。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e0/e04d03cbf437d84c64d5b5a69f31206e.gif" /></p><p></p><p>豆包写作真正的妙处就在于，它真正贴合我们的写作习惯，“一蹴而就，不再更改”的AI写作并非其目的，通过选中具体字段和格式进行调整，对内容进行扩写、缩写、改写，通过便捷的工具，张华对整个稿件“缝缝补补”，最终完成了一篇令他基本满意的演讲稿。</p><p></p><p><img src="https://static001.geekbang.org/infoq/67/67de0520174bb86dfe759b5949dfc7d2.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/d0/d0a8c3357e7c367ce0c6041108418032.png" /></p><p></p><p>除了能撰写演讲稿，在“写”的维度上，豆包平台以其多功能性和灵活性，为用户提供了全面的写作辅助工具，可以驾驭各种风格。面对长文写作，豆包可以提供公众号文章、论文和报告等多种风格的创作，并且通过分步策略帮助用户从0到1完成写作。</p><p></p><p>而在公关过程中，也要产出很多适配小红书、微博等社交平台的创意短文案，豆包也提供了多种多样的模版、润色选择，生产能力相当全面，搭配使用者的洞察与输入，它能够帮助应对公关写作中可能出现的各种难题。现如今，豆包已经被张华推荐给了他的同事们，整个公关部门的内容生产的效率也随之提升。</p><p></p><p><img src="https://static001.geekbang.org/infoq/75/750021b8990ff7a6c289e043a3cf4219.png" /></p><p></p><p></p><h2>“绘”的创新：创意总监的全新视觉表达</h2><p></p><p></p><p>在一间创意工作室里，创意总监李浩正在进行着一场深度创意的脑暴——他的客户即将推出一款全新的手机产品，他需要为该产品做一套营销方案，对他来说，不仅仅是一份工作任务，更是一次艺术创作的旅程，而这场旅程的起点便是将那些飘渺的营销理念具象化为打动人心的视觉营销画面。</p><p></p><p>李浩他闭上眼睛，让思绪在脑海中自由翱翔，试图捕捉新产品的每一个细节、每一种情感。随后，他将这些抽象的感受转化为具体的关键词，如“深邃宇宙黑”、“流畅未来线条”、“温馨科技光晕”等，并输入进了豆包电脑版之中，作为新晋的“创意加速器”，他已经使用的得心应手。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d7/d77ba45433a717723737928db940e583.png" /></p><p></p><p>不到一分钟，伴随着大模型算法的快速运转，一系列令人惊叹的概念图瞬间呈现在屏幕上，分析、组合、创造，豆包将他抽象的想法落入每一块色彩、每一个线条，也让他发散的灵感有了落脚之处。</p><p></p><p>对于乙方团队来说，在具体的业务场景中，往往需要多种风格的提案、多种风格的素材来供甲方选择，为此，李浩再次输入了多个产品特点，结合豆包提供的绘图风格，科幻、赛博朋克、未来感、CG画风......短短几十分钟内多种风格的产品创意提案被产出。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5b/5bd31c70031f7380f2fa2505e0e1bf06.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/e7/e7c5362ead4a5ed69384fe3a53c1967f.png" /></p><p></p><p>提效，这是当下创意产业最重要的一点，在以往，李浩需要与同事共同创作几天的内容，现如今只需要几个短短的提示词就可以随时生成，大大加快了他们的创作效率，借助豆包可以迅速生成多种选择，可以为他们的创作提供模板与参考，有效辅助内容的快速生产。</p><p></p><p>过往AI绘图的一个真实问题是，生成图片是“一次性”的，输入修补用的关键词后只能再次生成新图，生成过程有着极高的不稳定性，而豆包的绘图功能可操作性更强，不仅可以在已有原图的基础上通过自然语言进行编辑，还能进行局部区域的重绘，以人工创作的逻辑进行AI创作。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/eaf6cb017c2738622bb7711fc90ba002.gif" /></p><p></p><p>更令李浩惊喜的则是，豆包不同应用场景和个性化需求的满足，除了内容不固定的概念图外，李浩还可以通过豆包生成海报、宣传图、商品图等内容。李浩的公司有大量内容营销的需求，通过豆包，他们可以轻松定制图片风格，快速生成符合营销策略的视觉内容，并将其无缝集成到文档、演示文稿或网页中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/95/9547aecf4f05397aabb5f75ac73121e9.png" /></p><p></p><p>数字化、智能化的营销不仅在于策略，更在于内容的生产，针对大量的内容素材需求，AI工具的意义才真正凸显出来，在豆包的帮助下，李浩和他的同事得以从大量的机械化工作中解放出来，从而更好的打磨创作，生产真正具有美感与艺术感的作品。</p><p></p><h2>写在最后</h2><p></p><p>不难看出，豆包电脑版，以其四大功能模块——精准搜索、智慧阅读、高效写作与创新绘图，正在为职场人士打造了一个全方位的工作提效与创意激发平台。它不仅让信息获取变得精准快捷，更让文档处理与决策制定变得轻松高效。在写作的广阔天地里，豆包以AI之力赋能创意表达，让每一篇文案都能直击人心；而在视觉创意领域，豆包更是将抽象的想象转化为具象的视觉方案，引领职场创意高效生产新的风向标。</p><p></p><p>这一切的背后，是AI技术在职场中的深度应用，其正在真正转化为辅助工作的“生产力工具”，除了提升效率之外，它还将潜移默化的改变职场工作习惯，让人们从机械重复且低效的工作中解放出来，让创造力与思想力发光发热。随着AI技术的不断成熟与发展，未来的职场也将更加智能化、高效化。</p><p></p><p>而对于每一位职场人士而言，拥抱AI技术，合理地使用豆包这样的智能化工具，已经成为自身能力提升的重要一环。伴随个人成长与境遇的改变，我们将会面对各种各样的工作挑战，而拥有一个真正好用、有用且全面的“助手”，无疑更能让我们轻松的应对，甚至主动出击。而从AI技术本身的角度出发，只有不断的实践才能不断催动智能的涌现，协作进步，才是“AI赋能”的真正要义。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Nsx4t2otoIcOC6a8los8</id>
            <title>生成式 AI 落地不再难，六大问题一网打尽！《生成式 AI 商业落地白皮书》为 CXO 答疑解惑</title>
            <link>https://www.infoq.cn/article/Nsx4t2otoIcOC6a8los8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Nsx4t2otoIcOC6a8los8</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Jul 2024 06:48:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 生成式AI, 企业落地, 商业价值, 热门场景
<br>
<br>
总结: 2022年11月30日，ChatGPT的发布掀起了一场新技术和商业浪潮，企业开始探索生成式AI在企业内的应用落地。火山引擎联合InfoQ研究中心和RollingAI撰写了《生成式AI商业落地白皮书》，探究企业CXO层级关心的问题。企业对生成式AI的应用进展、商业价值和热门落地场景有了更清晰的认识，努力实现降本增效、提升服务质量和开拓新商业模式。 </div>
                        <hr>
                    
                    <p>2022&nbsp;年&nbsp;11&nbsp;月&nbsp;30&nbsp;日，ChatGPT&nbsp;的发布，掀起了一场以生成式&nbsp;AI&nbsp;为代表的新技术和商业浪潮。在近&nbsp;2&nbsp;年的探索中，出现了大量生成式AI的产品和原有产品的AI&nbsp;升级，企业也开始由观望到躬身入局，探索生成式&nbsp;AI&nbsp;在企业内的应用落地。在逐渐落地的过程中，有希望也有沮丧，有突破也有瓶颈，有成果也有挫折。</p><p></p><p>因此，为了帮助企业更好地找寻&nbsp;AI&nbsp;应用场景并实现高效落地，火山引擎联合&nbsp;InfoQ&nbsp;研究中心和&nbsp;RollingAI&nbsp;精心撰写了这本<a href="https://www.infoq.cn/minibook/E2hbDbQTdoffVOGc9PSN">《生成式&nbsp;AI&nbsp;商业落地白皮书》</a>"。在报告中，编委会结合问卷调研、专家访谈、实践案例分析，探究了企业&nbsp;CXO&nbsp;层级最想要了解/落地生成式&nbsp;AI&nbsp;的六大关键问题，并期望通过对以上问题的解答，为大家奉上一份专业的企业&nbsp;AI&nbsp;转型指南。</p><p></p><p>受文章篇幅限制，欢迎大家扫描文中的<a href="https://www.infoq.cn/minibook/E2hbDbQTdoffVOGc9PSN">「链接」</a>"，进行完整PDF版报告下载。</p><p></p><h3>问题一：目前，企业采用生成式&nbsp;AI&nbsp;的进展如何？</h3><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/a1/a10bd892224b88b2073a5f5ac2c150ca.png" /></p><p></p><p>在调研中，编委会发现，企业用户正在迅速适应生成式&nbsp;AI&nbsp;新能力。在受访的&nbsp;590&nbsp;名样本中，21.0%&nbsp;的样本所在公司已开始小范围试点应用，26.3%&nbsp;在大范围推广生成式&nbsp;AI&nbsp;应用，更有&nbsp;6.4%&nbsp;已将生成式&nbsp;AI&nbsp;应用整合到整体战略转型阶段。</p><p></p><p>这些数据表明，生成式&nbsp;AI&nbsp;应用已经引起了大多数企业中高层的广泛关注。对于尚未开始普及生成式&nbsp;AI&nbsp;的企业而言，若不迅速跟进，可能面临在技术创新和市场竞争中落后的风险。</p><p></p><p>此外，在投入资源方面，19%&nbsp;的企业则进行了生成式&nbsp;AI&nbsp;的培训或分享，34%的受访企业已经有专门的团队负责生成式AI落地的相关事宜。在这&nbsp;34%&nbsp;的企业中，已经有&nbsp;9%&nbsp;的企业更进一步，为企业生成式&nbsp;AI&nbsp;配备了相应的支出预算。</p><p></p><h3>问题二：企业&nbsp;CXO&nbsp;层级期望通过落地生成式&nbsp;AI，获得哪些商业价值？</h3><p></p><p>但越来越多的企业开始清楚地明白，技术落地应以实际需求为目标，而不是为了技术而技术。这意味着，在探索和落地前，企业&nbsp;CXO&nbsp;层级要对技术落地是为了实际解决什么样的问题，提供什么样的商业价值进行充分思考。</p><p></p><p>在调研中，42%&nbsp;的企业&nbsp;CXO&nbsp;层级认为生成式&nbsp;AI&nbsp;的落地应实现运营成本的降低，32%&nbsp;选择了提升运营效率。这意味着降本增效仍然是目前企业的首要目标。同时，企业&nbsp;CXO&nbsp;层级对生成式&nbsp;AI&nbsp;能带来的实际经济效益充满信心。数据显示，有&nbsp;37%&nbsp;的企业&nbsp;CXO&nbsp;层级表示其企业的生成式&nbsp;AI&nbsp;项目将带来超过&nbsp;10%&nbsp;的成本缩减。效率提升方面，26%的高管预计生成式&nbsp;AI&nbsp;将带来超过&nbsp;10%&nbsp;的效率提升。这意味着，生成式AI不仅有助于降低成本，还能显著提高企业的运营效率，为企业在竞争激烈的市场中提供了强有力的支持。但也存在&nbsp;18%&nbsp;的企业高层尚不确定其生成式&nbsp;AI&nbsp;项目能带来多大程度的运营效率提升和成本降低。</p><p></p><p>除了降本增效之外，为用户提供更高专业和更个性化的服务、采集和分析更多维度的非结构化数据、建立行业的知识库并赋能上下游生态、开辟脑力密集型业务的新商业模式也是企业认为生成式&nbsp;AI&nbsp;应该实现的商业价值。</p><p></p><h3>问题三：目前生成式&nbsp;AI&nbsp;的热门落地场景有哪些？</h3><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/3c/3caa7133e4077884a644849bf0879755.png" /></p><p></p><p>从各行各业的宣传中，编委会已经发现生成式&nbsp;AI&nbsp;正在迅速覆盖各行业和职能领域。这其中，生成式&nbsp;AI&nbsp;在营销和销售领域的应用尤为广泛。63%&nbsp;的企业都在尝试/已经在营销领域应用了生成式&nbsp;AI，这与营销文本、素材、图像，甚至视频的生产这些典型的营销需求，与生成式&nbsp;AI&nbsp;原生能力的高契合度离不开关系。销售领域因为生成式内容和个性化定制的需求巨大，吸引了&nbsp;62%&nbsp;的企业投入生成式&nbsp;AI&nbsp;的研究中。此外，IT&nbsp;领域对生成式&nbsp;AI&nbsp;的适应性也非常强，48%&nbsp;的企业在&nbsp;IT&nbsp;团队中引入生成式&nbsp;AI，这可能得益于&nbsp;IT&nbsp;团队本身较高的智能化人才储备和技术基础，这使得生成式&nbsp;AI&nbsp;的应用更加顺利和高效。</p><p></p><h3>问题四：从投入产出的角度，生成式&nbsp;AI&nbsp;落地的关键场景有哪些？</h3><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/e7/e7de6eb1019fa4e217a90d1466c54513.jpeg" /></p><p></p><p>除了以上的热门领域，仍有很多场景机会正等待生成式&nbsp;AI&nbsp;的技术探索。</p><p></p><p>2024&nbsp;年春季，《Gen-AI&nbsp;220&nbsp;应用全场景地图》在火山引擎&nbsp;FORCE&nbsp;原动力大会上首次亮相，基于全球超过&nbsp;100&nbsp;家企业在&nbsp;AI&nbsp;项目上的落地经验、205&nbsp;家中大型企业&nbsp;Al&nbsp;项目的详尽研究以及超过&nbsp;150&nbsp;名国内外专家的洞见，《地图》精心筛选出涵盖&nbsp;12&nbsp;个行业的&nbsp;220&nbsp;个关键场景，并基于投资权重、收益类别和风险警示对场景进行精确评级。这次，编委会在原有&nbsp;220&nbsp;个场景的基础上新增了&nbsp;20&nbsp;个场景，形成了升级版的《Gen-AI&nbsp;240&nbsp;应用全场景地图》，希望能为企业在&nbsp;AI&nbsp;领域找到最适合的发展路径提供更多参考。</p><p></p><p>此外，为了搭配理解，报告中还从消费零售、金融等八大行业中，选择了&nbsp;16&nbsp;个真实案例，详细阐述了各案例企业面临的挑战，生成式&nbsp;AI&nbsp;升级点和项目效果，期望通过以上真实项目的展示，也为企业&nbsp;CXO&nbsp;层级，提供转型场景的选择和实施路径的灵感。</p><p></p><h3>问题五：企业落地生成式&nbsp;AI&nbsp;时，存在哪些落地挑战？</h3><p></p><p><img src="https://static001.geekbang.org/infoq/1f/1ff1a5bc961da48ab52f7bbfeace44c0.png" /></p><p></p><p>在报告中，编委会将生成式&nbsp;AI&nbsp;的落地挑战分为六大类：1）作为新兴技术，企业应该如何评估其带来的创新价值，以在企业内获取项目资源；2）场景选择难，失败率高的情况下，应该如何选择确定合适的落地场景；3）AI&nbsp;基础设施构建慢，周期长，企业内生成式&nbsp;AI&nbsp;项目如何完成快速启动；4）在生成式&nbsp;AI&nbsp;项目启动前，应怎样做好落地准备工作；5）在项目落地时，如何既系统规划又高效匹配人才；6）在现有的组织中，如何形成自下而上的全民创新环境。报告对以上问题进行了一一解答，并提供了行之有效的解决方案。</p><p></p><h3>问题六：如何对企业&nbsp;AI&nbsp;的成熟度进行合理地衡量和评估？</h3><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/39/3947ffd217ac8a6d7639e0ddc61cff3f.jpeg" /></p><p></p><p>光知道如何应对挑战还不足够，企业应先全面评估自身的&nbsp;AI&nbsp;成熟度，再制定符合自身实际情况的&nbsp;AI&nbsp;发展路线图。因此，报告中提出了一个企业&nbsp;AI&nbsp;成熟度框架。该框架从人才、科技、商业、数字化应用、流程、文化和知识六个维度，对企业的&nbsp;AI&nbsp;成熟度进行了划分和梳理。通过对标该框架，企业可以清晰地了解自己目前在&nbsp;AI&nbsp;应用方面所处的阶段，并参考框架中的关键指标和行动建议，有针对性地提升短板，推动企业在&nbsp;AI&nbsp;领域的持续成长。</p><p></p><p>该框架还为企业提供了一个与同行业企业横向比较的基准，帮助企业精准定位自身优势与不足，为后续的&nbsp;AI&nbsp;战略规划和落地实施提供参考和指引。企业可以根据自身的业务特点、发展阶段和资源禀赋，选择适合自己的&nbsp;AI&nbsp;发展路径，稳步推进&nbsp;AI&nbsp;转型之旅。</p><p></p><p>报告调研样本说明：</p><p>在&nbsp;2024&nbsp;年&nbsp;6&nbsp;月，火山引擎和&nbsp;RollingAI&nbsp;联合&nbsp;InfoQ&nbsp;研究中心，展开了一项关于企业生成式AI应用现状的用户问卷调研，调研共计回收了&nbsp;590&nbsp;份有效问卷，样本涵盖金融、消费零售、汽车、医药大健康、B2B&nbsp;企服、制造、智能终端以及教育和科研共计八大行业，以及产品研发、营销、销售、客户关系、IT、人事/法务等诸多领域。</p><p></p><p>以上便是报告对六大核心问题的解答，文章的结尾，我们应当回归到企业&nbsp;AI&nbsp;转型的初心与使命。技术的发展和应用，最终目的是为了解决实际问题，提升效率，创造价值。生成式&nbsp;AI，作为一种新兴的技术力量，其真正的价值在于它如何帮助企业实现这一目标。因此，在未来的日子里，我们期待看到更多企业能够借助生成式&nbsp;AI&nbsp;技术，实现业务的创新与突破，书写属于自己的&nbsp;AI&nbsp;转型成功故事。让我们携手并进，在&nbsp;AI&nbsp;的助力下，共创企业更加辉煌的未来。</p><p></p><p>欢迎大家扫描文末的<a href="https://www.infoq.cn/minibook/E2hbDbQTdoffVOGc9PSN">「链接」</a>"，进行完整PDF版报告下载。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/IF9azEFriXeOC1JI6AHF</id>
            <title>训练一次经历419次意外故障！英伟达 GPU 也差点玩不转405B 模型，全靠Meta 工程师后天救场！</title>
            <link>https://www.infoq.cn/article/IF9azEFriXeOC1JI6AHF</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IF9azEFriXeOC1JI6AHF</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Jul 2024 06:46:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Meta, Llama 3 405B, GPU, 故障
<br>
<br>
总结: Meta在一份研究报告中揭示了训练Llama 3 405B参数模型的重大挑战，其中GPU故障是主要问题之一，导致训练过程中频繁发生意外中断。尽管存在问题，Llama 3团队通过自动化集群维护和优化策略，仍然实现了超过90%的有效训练时间。同时，Meta还开发了多种工具和策略来应对GPU故障和其他意外中断，以提高训练效率和稳定性。 </div>
                        <hr>
                    
                    <p>整理&nbsp;|&nbsp;华卫</p><p>&nbsp;</p><p>最近，Meta&nbsp;在一份研究报告中揭示了训练&nbsp;&nbsp;Llama&nbsp;3&nbsp;405B&nbsp;参数模型的重大挑战：该系统在包含&nbsp;16384&nbsp;个&nbsp;Nvidia&nbsp;H100&nbsp;GPU&nbsp;的集群上运行，在训练期间平均每三个小时就发生一次故障，&nbsp;54&nbsp;天内经历了&nbsp;419&nbsp;次意外故障。</p><p>&nbsp;</p><p>这些故障中，有一半以上的情况都归因于&nbsp;GPU&nbsp;及其高带宽内存&nbsp;（HBM3）。由于&nbsp;GPU&nbsp;训练任务的规模庞大和高度同步，Llama&nbsp;3很容易发生故障，且单个&nbsp;GPU&nbsp;故障就会中断整个训练过程，导致必须重新启动。</p><p>&nbsp;</p><p>不过，据介绍，尽管存在这些问题，Llama&nbsp;3&nbsp;团队仍在支持自动化集群维护（例如固件和Linux内核升级）的同时，实现了超过90%的有效训练时间（有效训练时间是指实际用于有用训练的时间与经过时间的比例）。</p><p>&nbsp;</p><p>正如一句古老的超级计算谚语所言，“大规模系统唯一可以确定的就是失败。”超级计算机是极其复杂的设备，使用数万个处理器、数十万个其他芯片和数百英里长的电缆。在复杂的超级计算机中，每隔几个小时出现故障是很正常的，而开发人员的主要诀窍就是确保系统在出现这种局部故障时仍能正常运行。</p><p></p><h1>58.7%意外中断源于GPU，三起事件需要显著人工干预</h1><p></p><p>据悉，在为期&nbsp;54&nbsp;天的预训练中，共有&nbsp;466&nbsp;次工作中断。其中，47&nbsp;次是计划内中断，是由于自动化维护造成的，如固件升级或操作员发起的配置更新或数据集更新操作；419&nbsp;次是意外中断，主要源于确认的硬件问题，包括GPU、主机组件故障或疑似与硬件相关的问题，如静默数据损坏和未计划的单个主机维护事件。</p><p>&nbsp;</p><p>GPU问题是最主要的意外中断类别，占所有意外问题的58.7%，包括&nbsp;NVLink&nbsp;等各种GPU&nbsp;故障及HBM3&nbsp;内存故障。这并不奇怪，因为&nbsp;Nvidia&nbsp;的&nbsp;H100&nbsp;GPU&nbsp;消耗约&nbsp;700W&nbsp;并承受大量热应力。尽管出现了大量的故障，但只有三起事件需要显著的人工干预，剩下的问题均能由自动化处理。</p><p>&nbsp;</p><p>其余&nbsp;41.3%&nbsp;的意外中断是由软件错误、网络电缆和网络适配器混合造成的。有趣的是，在此期间只有两个&nbsp;CPU&nbsp;出现故障。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/dc/dcce17485317d645ecd487f3f9c82c10.png" /></p><p></p><p>为期&nbsp;54&nbsp;天的&nbsp;Llama&nbsp;3&nbsp;405B&nbsp;预训练期间，对意外中断的根本原因进行分类。</p><p></p><p>Llama&nbsp;3&nbsp;405B&nbsp;大模型训练团队面临的另一个挑战是数以万计的&nbsp;GPU&nbsp;同时发生功耗变化，给数据中心的电网带来了压力。</p><p>&nbsp;</p><p>在训练过程中，成千上万的GPU可能同时增加或减少功耗，例如等待检查点完成或集体通信结束，或者整个训练任务的启动或关闭。当这种情况发生时，会导致数据中心的功耗瞬时波动达到几十兆瓦的数量级，可能使电网不堪重负。</p><p>&nbsp;</p><p>而这是一个持续存在的挑战，意味着&nbsp;Meta&nbsp;必须确保其数据中心有足够的电力，才能维护405B&nbsp;模型以及未来更大规模Llama模型的正常运转。随着&nbsp;AI&nbsp;模型复杂性的不断增长，所需的计算资源也在增加。</p><p>&nbsp;</p><p></p><h1>实现90%有效训练时间背后的努力</h1><p></p><p>为了提高效率，Meta&nbsp;开发了多种工具和优化策略，包括减少任务启动和检查点时间、广泛使用PyTorch内置的NCCL飞行记录器，以及识别滞后的&nbsp;GPU。其中，NCCLX&nbsp;在故障检测和定位方面发挥了至关重要的作用，尤其是对于&nbsp;NVLink&nbsp;和&nbsp;RoCE&nbsp;相关问题，与&nbsp;PyTorch&nbsp;的集成允许监控和自动超时由&nbsp;NVLink&nbsp;故障引起的通信停顿。&nbsp;</p><p>&nbsp;</p><p>据了解，PyTorch&nbsp;的&nbsp;NCCL&nbsp;飞行记录器可以将集体元数据和堆栈跟踪记录到环形缓冲区中，从而能够在大规模的情况下快速诊断和解决挂起和性能问题，尤其是与&nbsp;NCCLX&nbsp;相关的问题。另外，由于Meta在网络中混合使用了&nbsp;NVLink&nbsp;和&nbsp;RoCE，使得大规模训练中的调试问题变得更加复杂。通过NVLink的数据传输通常通过CUDA内核发出的加载/存储操作完成，而远程&nbsp;GPU&nbsp;或&nbsp;NVLink&nbsp;连接的故障通常表现为&nbsp;CUDA&nbsp;内核内的加载/存储操作停滞，且不会返回明确的错误代码。</p><p>&nbsp;</p><p>NCCLX&nbsp;通过与&nbsp;PyTorch&nbsp;的紧密协同设计提高了故障检测和定位的速度和准确性，允许&nbsp;PyTorch&nbsp;访问&nbsp;NCCLX&nbsp;的内部状态并跟踪相关信息。虽然无法完全防止由于NVLink故障导致的挂起，但系统会监控通信库的状态，并在检测到此类挂起时自动超时。此外，NCCLX&nbsp;还会跟踪每次&nbsp;NCCLX&nbsp;通信的内核和网络活动，并提供故障&nbsp;NCCLX&nbsp;集体的内部状态快照，包括所有等级之间已完成和待完成的数据传输。</p><p>&nbsp;</p><p>有时，硬件问题可能会导致出现仍然运行但速度缓慢的“拖后腿者”，还很难被检测出来。而即使只有一个“拖后腿者”也可能减慢成千上万个其他GPU的运行速度，常常表现为正常但速度缓慢的通信。对此，Meta开发了用于优先处理来自选定进程组的潜在问题通信的工具，从而有效检测并及时解决落后者，确保将速度减慢到最低，保持整体训练效率。&nbsp;</p><p>&nbsp;</p><p>还有一个有趣的观察是，环境因素对大规模训练性能的影响。对于Llama&nbsp;3&nbsp;405B，Meta注意到一天中会有一段时间出现1-2%的吞吐量变化，这种波动是因为中午较高的温度影响了GPU的动态电压和频率调整，从而影响训练性能。但这不是什么大问题，GPU&nbsp;的动态电压和频率缩放通常都会受到温度变化的影响。&nbsp;</p><p>&nbsp;</p><p></p><h1>结语</h1><p></p><p>考虑到一个包含&nbsp;16384&nbsp;个&nbsp;H100&nbsp;GPU&nbsp;的集群在&nbsp;54&nbsp;天内经历了&nbsp;419&nbsp;次意外故障，每&nbsp;24&nbsp;小时&nbsp;7.76&nbsp;次，我们不禁想到，xAI&nbsp;配备了100000&nbsp;个&nbsp;H100&nbsp;GPU的孟菲斯超级计算机集群（Memphis&nbsp;Supercluster）发生故障的频率是多少？</p><p>&nbsp;</p><p>上周，埃隆·马斯克（Elon&nbsp;Musk）在社交平台X上吹嘘自己启动了“世界上最强大的人工智能训练集群”，他将在今年12月之前创建“世界上所有指标最强大的人工智能”。据悉，孟菲斯超级计算机集群已经开始进行训练，采用了液冷散热和单一的&nbsp;RDMA&nbsp;网络互连架构。</p><p>&nbsp;</p><p>按GPU规模比例来看，xAI的孟菲斯超级计算机集群可能会面临指数级更高的故障率，出现故障的组件数量或会增加六倍，这给其未来的&nbsp;AI&nbsp;训练带来了更大的挑战。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.inspire2rise.com/meta-faces-frequent-gpu-failures-llama-3-training.html">https://www.inspire2rise.com/meta-faces-frequent-gpu-failures-llama-3-training.html</a>"</p><p><a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/faulty-nvidia-h100-gpus-and-hbm3-memory-caused-half-of-the-failures-during-llama-3-training-one-failure-every-three-hours-for-metas-16384-gpu-training-cluster">https://www.tomshardware.com/tech-industry/artificial-intelligence/faulty-nvidia-h100-gpus-and-hbm3-memory-caused-half-of-the-failures-during-llama-3-training-one-failure-every-three-hours-for-metas-16384-gpu-training-cluster</a>"</p><p><a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">https://ai.meta.com/research/publications/the-llama-3-herd-of-models/</a>"</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/wav7HmQ6va03Mhq3TMv5</id>
            <title>第十九届全国大学生智能汽车竞赛地平线创意组在武汉理工大学隆重开幕</title>
            <link>https://www.infoq.cn/article/wav7HmQ6va03Mhq3TMv5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/wav7HmQ6va03Mhq3TMv5</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Jul 2024 05:45:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 全国大学生智能汽车竞赛, 智慧医疗赛道, 地平线, 创新举措
<br>
<br>
总结: 本文介绍了第十九届全国大学生智能汽车竞赛地平线创意组智慧医疗赛道全国选拔赛的开幕情况，赛事由中国自动化学会主办，吸引了来自全国各地280支队伍参赛。赛事历史悠久，覆盖范围广泛，是教育部白名单内A类赛事之一。地平线首次参加并开设智慧医疗创意赛道，旨在探索前沿技术，丰富竞赛内容，为学生提供更广阔的实践平台。通过赛事举办，促进了产学研合作，推动了智能汽车技术人才的培养和产学融合。 </div>
                        <hr>
                    
                    <p>7月27日上午，第十九届全国大学生智能汽车竞赛地平线创意组智慧医疗赛道全国选拔赛开幕式隆重举行，大赛由中国自动化学会、第十九届全国大学生智能汽车竞赛组织委员会主办，武汉理工大学、地平线、古月居承办。首年即吸引来自全国各地280支队伍的报名，参赛人数突破2000人，覆盖全国120多所高校。</p><p></p><p>全国大学生智能汽车竞赛是教育部白名单内A类赛事，也是智能汽车领域历史最悠久、覆盖学校最广、影响力最大赛事之一。迄今，在全国数百所高校的支持下，全国大学生智能汽车竞赛已成功举办了十八届，参赛学生总规模超55万人次。</p><p></p><p>2024年，地平线首次参加全国大学生智能汽车竞赛并开设了智慧医疗创意赛道，该赛道是12个赛道中唯一率先应用人机交互技术的组别，旨在通过智能机器人在智慧医疗场景下的应用，探索移动车模的运动控制设计、人工智能视觉应用技术、数字孪生应用以及遥操作在数字环境与物理环境下的融合等前沿技术。这一创新举措不仅丰富了竞赛内容，也为参赛学生提供了更加广阔的实践平台和展示空间，促使产学研合作迈入深水区。</p><p></p><p><img src="https://static001.geekbang.org/infoq/78/784917e85ae13d731228205378ea38d9.jpeg" /></p><p>（启动仪式，从左至右依次为武汉理工大学汽车工程学院院长颜伏伍教授、武汉理工大学校长助理李潮欣教授、地平线机器人事业部品牌运营总监高吟佳、地平线机器人事业部开发者生态总监胡春旭）</p><p></p><p>开幕仪式上，全国大学生智能汽车竞赛组委会秘书长卓晴教授在线上为参赛选手给予鼓励，预祝所有的参赛队伍取得优异成绩，并对地平线创意组给予了肯定，“地平线首次加入全国大学生智能汽车竞赛平台，在认真分析之前已有的创意组赛题内容的基础上，创新赛题内容，认真组织竞赛培训，特别是在比赛器材方面，尽可能的降低参赛的门槛，能够让更多的大学生参与其中。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/a4/a4bc97fec144c4131701656446d2cb20.jpeg" /></p><p>（全国大学生智能汽车竞赛组委会秘书处卓晴教授线上致辞）</p><p></p><p>武汉理工大学校长助理李潮欣教授表示，赛事的举办不仅强化了新时代智能汽车技术人才的全过程培养，更深化了教育链、人才链、产业链的多维度交流，在推动校企融通、产学融合、科教融汇上产生了重要影响。</p><p></p><p><img src="https://static001.geekbang.org/infoq/19/19ae4e3813376f0fff65877e8bb2cac3.jpeg" /></p><p>（武汉理工大学校长助理李潮欣教授致辞）</p><p></p><p>地平线机器人事业部开发者生态总监胡春旭表示，本次赛题设计的初衷是希望能够将智能驾驶、机器人、人工智能等这些科技元素融入在比赛中，给同学们提供一个接触前沿科技、了解行业科技发展的现状，提升专业技能的平台并预祝参赛队员取得好成绩。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cd/cda6d4b88d06941ac4fa9f7dc7174039.jpeg" /></p><p>（地平线机器人事业部开发者生态总监胡春旭致辞）</p><p></p><p>“怕什么真理无穷，进一寸有一寸的欢喜”。作为曾经全国大学生智能汽车竞赛的参赛选手，古月居创始人顾强祝愿大家能够充分展现自己的实力与才华，勇于面对技术挑战，积极探索创新，期待与大家共同亲历智能机器人时代的到来。</p><p></p><p><img src="https://static001.geekbang.org/infoq/de/de7163a97af07a96857853bd4cc8649c.jpeg" /></p><p>（古月居创始人顾强致辞）</p><p></p><p>随着开幕式的圆满结束，第十九届全国大学生智能汽车竞赛地平线创意组智慧医疗赛道全国选拔赛也正式拉开帷幕。本次竞赛，地平线结合产业理解、技术研发、创业生态等优势，与组委会及各院校携手，共同加速高校开发者创新实践工作的建设与发展，促进教学创新和机器人交叉学科创新型人才培养，为产学研合作、产教融合协同育人奠定了坚实基础。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/CyITvvXE2uhrCIpJszxv</id>
            <title>第一个制定了AI议程的奥运会开幕了！谷歌、阿里等厂商的大模型也来“干活”了</title>
            <link>https://www.infoq.cn/article/CyITvvXE2uhrCIpJszxv</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/CyITvvXE2uhrCIpJszxv</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Jul 2024 02:31:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 第一次, 大胆, 创意, 人工智能
<br>
<br>
总结: 2024年巴黎奥运会是一次充满创意和大胆举措的盛会，首次在塞纳河上举办开幕式，也是首届指定了人工智能议程的奥运会。全球技术大厂积极参与其中，通过实时监控、数字孪生概念、AI技术等方式，为奥运会带来全新的体验和观赏方式。同时，大模型厂商也推出了关于奥运会的新功能，让体育迷们可以通过AI技术体验更丰富的观赛体验。 </div>
                        <hr>
                    
                    <p></p><p>今天凌晨，2024年巴黎奥运会终于开幕了。据了解，巴黎奥运会开幕式的时间将持续4小时，大约30万人将参加开幕式，超过10亿人次观看开幕式。</p><p>&nbsp;</p><p>“我的第一个关键词就是‘第一次’，第二个关键词是‘大胆’，第三个关键词是‘创意’。当然，我们想把法国能做到的最好的样子展现给世界。”巴黎奥运会组委会主席托尼·埃斯坦盖说道。除了打破历史常规，将开幕式搬到了巴黎的象征之一塞纳河上之外，巴黎奥运会也是首届指定了人工智能议程的奥运会，意味着本次奥运会有大量的AI元素。</p><p>&nbsp;</p><p>接下来，我们先看看下全球技术大厂是如何参与到这场盛事之中的。</p><p></p><h2>大厂直接参与奥运的“姿势”</h2><p></p><p>&nbsp;</p><p>首先，巴黎2024奥运会的能源消耗将接受实时监控，所捕捉的数据也被用于指导未来的赛事规划。Corna解释称，“我们早在2020年就开始收集各种运营数据，以研究如何提高奥运会的管理效率。”</p><p>&nbsp;</p><p>在规划方面，巴黎奥运会与英特尔一起，使用数字孪生概念为体育场馆构建数字化模型。“这样我们就可以预见多种情况，例如哪里电力供应紧张、哪里需要部署摄像机，以及是否存在任何可及性障碍等——所有这些不再需要工作人员亲临现场。利用这些奥运会场馆的数字孪生副本，我们将有望颠覆奥运会的组织方式。”Corna说道。</p><p>&nbsp;</p><p>另外，法国的跨国IT服务管理公司 Atos 将协调并组织起一支由15家技术合作伙伴联合参与的团队。该团队拥有2000多名专家，各位成员将致力于推动2024年巴黎奥运会及残奥会的全面互联、安全与数字化。</p><p>&nbsp;</p><p>直播转播方面，奥林匹克广播服务公司（OBS）将在2024年巴黎奥运会期间使用AI以改善内部工作流程、增强观众体验、丰富叙事效果并更好地解读赛程中的精彩内容，其中包括与阿里巴巴携手，提供创纪录数量的多摄像机回放系统。该系统能够在云端进行由AI驱动的高质量重建，以提供21项涉及运动及不同项目的三维模型及多视点映射。这将从更多摄像机视角出发，提供引人入胜的精彩回放。</p><p>&nbsp;</p><p>据悉，OBS LiveCloud将成为2024年巴黎奥运会直播信号远程分发的主要方式，目前已预定的远程服务中有三分之二通过云计算。奥运转播云将基于阿里云部署在全球的公共云基础设施，来支撑奥运直播信号从巴黎传输到全球200多个国家和地区，走向数十亿观众。</p><p>&nbsp;</p><p>为了提高广播公司的效率，阿里云在2018年9月与OBS联合推出了OBS Cloud，先后为东京2020年奥运会与北京2022年冬奥会的广播报道提供支持。“OBS Cloud的落地，为媒体版权持有方及主办城市提供了一种无需大量前期投入的替代性方案。现在奥运会的相关内容可以通过云端传输，从而有效减少碳足迹。”OBS CEO Yiannis Exarchos表示。</p><p>&nbsp;</p><p>在2024年巴黎奥运会的赛场上，欧米茄将第31次担任官方计时供应商。OBS将与欧米茄合作释放AI的力量，在奥运会期间提供更快、相关度更高、更富洞见的数据。例如，对跳水、田径和艺术体操等项目的智能频闪分析，将帮助观众更好地了解运动员的动作与生物力学状态。</p><p>&nbsp;</p><p>此外，在跳水项目中，OBS和欧米茄还将运用AI生成的增强数据图形，提供关于每位运动员在空中和入水时表现的一组全新数据。基于AI的运动跟踪技术还将帮助评论员和观众在皮划艇短距离竞速、马拉松、竞走、自行车公路赛（公路赛与计时赛）、山地自行车、马拉松式游泳、划船、帆船及铁人三项赛中跟踪运动员的位置。</p><p>&nbsp;</p><p>作为2024年巴黎奥运会的官方AI平台合作伙伴，英特尔将推出创新的AI体验，协助改变全球粉丝、组织方、运动员和观众们的奥运参与体验。该平台将自动生成精彩集锦，根据媒体版权持有方的偏好自动将14项运动及项目的关键瞬间汇编成定制化的片段，通过个性化内容进一步吸引更多数字与社交媒体受众。</p><p>&nbsp;</p><p>负责自动选取精彩片段的推理引擎将基于英特尔Geti AI软件工具训练的AI模型实现，其专门从事AI支持的视觉内容处理。这些模型还针对每项运动进行了训练，训练素材均检索自庞大的奥运会体育视频归档。</p><p>&nbsp;</p><p>作为奥运会入残奥会的官方无线通信与计算设备合作伙伴，三星将在塞纳河上举行的历史性形式上，为各国船只配备Galaxy S24 Ultra智能手机，并通过由2024年巴黎奥运会官方移动设备提供商Orange支持的独家5G网络分享水面镜头，确保屏幕前的观众也能以沉浸式体验感受这场史无前例的庆典活动。</p><p>&nbsp;</p><p>这项集成技术还将在奥运会帆船比赛中发挥作用，让粉丝们以前所未有的近距离视角体验运动员间的角逐、参与这场激烈的水上项目。</p><p></p><h2>&nbsp;</h2><p></p><p></p><h2>也是大模型们的奥运会</h2><p></p><p>&nbsp;</p><p>当然，大模型厂商也不甘落后，争相发布了关于奥运会的新功能。</p><p>&nbsp;</p><p>7月26日，巴黎奥运会期间，通义App上线赛事百事通、全民云运动、AI运动写真等多款新功能。这些新功能基于通义大模型打造，让国内体育迷们看奥运、聊奥运的同时，也能体验AI技术带来的观赛新体验。</p><p>&nbsp;</p><p>据悉，“赛事百事通”是基于通义大语言模型打造的智能体，无论查询赛事看点，还是了解赛事历史，只需简单提问，通义就能提供详尽且专业的答案。全民云运动、AI运动写真等功能则是基于通义视觉大模型，采用EMO、AnimateAnyone、Photo Studio等技术，用户上传一张照片，选择一款喜欢的运动模版，便能实时生成高还原的数字形象，让普通的图片生成具有表现力的视频。</p><p>&nbsp;</p><p>文心一言联合直播吧发布“热点体育智能体-言宝”，能和用户畅聊奥运八卦和小故事，能做赛事预测等。</p><p>&nbsp;</p><p>商汤AI 智慧篮球产品则将全程参与中国国家篮球队的赛事征程，提供运动数据分析及竞技策略优化支持等。</p><p>&nbsp;</p><p>据介绍，基于商汤与SMT合作开发的InnoMotion赛事转播方案，可利用商汤3D无感知运动捕捉技术，在完全无穿戴设备的情况下，实现多人、大范围、多角度的多元场景覆盖，实时获得空间运动姿态信息。此外，商汤智慧赛事转播技术将用于乒乓球、射箭两个项目的全程赛事转播。。</p><p>&nbsp;</p><p>美国的奥运代表队官方 AI 赞助商为谷歌。从7月26日开始，谷歌将利用 Gemini为奥运赛事转播提供技术支持。据悉，转播内容将包括谷歌地图对凡尔赛宫、罗兰-加洛斯球场和水上运动中心等场馆的 3D 全景，以及每个场馆将举办哪些活动的花絮。这些图像来自过去几年中添加到地图中的沉浸式视图，这些视图以逼真的模型表现某些地标性建筑和感兴趣的区域。</p><p>&nbsp;</p><p>作为推广 Gemini 和谷歌其他人工智能工具的协议的一部分，播音员和评论员将在直播片段中加入谷歌搜索人工智能概述，尝试回答奥运会和残奥会的问题。</p><p>&nbsp;</p><p></p><h2>东道主的 AI 喜悦与烦恼</h2><p></p><p>&nbsp;</p><p>本次奥运会准备期间，国际奥委会（IOC）开创性地于2024年4月正式启动了<a href="https://stillmed.olympics.com/media/Documents/International-Olympic-Committee/AI/Olympic-AI-Agenda.pdf">《奥运会AI议程》</a>"，阐述了AI科技将为体育界带来的预期影响，而巴黎奥运会将成为见证这项议程的首批案例。</p><p>&nbsp;</p><p>《奥运会AI议程》是国际奥委会主席 Thomas Bach领导下发布的“三部曲”战略文件中的第三部，之前的两部是 2014 年 12 月发布的《奥林匹克议程 2020》和 2021 年 3 月发布的《奥林匹克2020+5议程》。</p><p>&nbsp;</p><p>该议程是国际奥委会人工智能工作组审议的成果，该工作组包括来自阿里、德勤、英特尔、欧米茄等企业，其中阿里云创始人王坚为该工作组成员之一。</p><p>&nbsp;</p><p>“我们将在2024年巴黎奥运会上见证一系列开创性的概念。我们目前正采取一种较为审慎的方法，测试并评估如何使用AI来改善奥运会并推动其为未来做好准备。”国际奥委会首席技术官Ilario&nbsp;Corna 此前说道。</p><p>&nbsp;</p><p>国际奥委会在这次巴黎奥运会的不同领域中应用AI科技，其中非常重要的一点就是防止网络滥用。Bach预计，本届奥运会期间将发布约5亿条相关社交媒体消息。</p><p>&nbsp;</p><p>今年早些时候，国际奥委会已经公布了旨在保护运动员免受网暴影响的AI监控系统。该系统将使用AI技术监控数十万个社交媒体账户并标记出辱骂性消息，以帮助相关平台及时介入干预。</p><p>&nbsp;</p><p>实际上，为备战奥运会，法国于 2023 年颁布了第 2023-380 号法律，这是一揽子法律，旨在为 2024 年奥运会提供法律框架。其中包括备受争议的第 7 条，该条款允许法国执法部门及其技术承包商在 2024 年奥运会之前、期间和之后试验智能视频监控，以及第 10 条，该条款明确允许使用人工智能软件审查视频和摄像头。这些法律使法国成为第一个将如此广泛的人工智能监控系统合法化的欧盟国家。</p><p>&nbsp;</p><p>学者、社会团体和公民自由倡导者指出，这些条款与《通用数据保护条例》和欧盟监管人工智能的努力相悖。其中第 7 条明确违反了《通用数据保护条例》保护生物特征数据的规定。</p><p>&nbsp;</p><p>法国官员和科技公司代表则表示，人工智能软件可以实现识别和标记特定类型事件的目标，而无需识别人员、也不会违反《通用数据保护条例》对生物特征数据处理的限制。</p><p>&nbsp;</p><p>此外，Corna 表示作为奥运会永远的核心要素，运动员们将能够在Athlete365平台上体验国际奥委会与英特尔合作提供的全新聊天服务。</p><p>&nbsp;</p><p>“对于获得奥运会参赛资格的运动员，这项服务旨在就社交媒体指南、反兴奋剂规则以及〈奥林匹克宪章〉第50条（允许在奥林匹克舞台上进行反种族主义宣传）等常见问题提供简单快捷的阐释。”他补充道。</p><p>&nbsp;</p><p>AI 还将用于在本届奥运会期间以多种格式及语言制作精彩视频，还将通过高度复杂的首套数据捕捉与能源管理系统，运用AI增强奥运会的可持续性水平。“AI也将为人才识别开辟新的途径，我们将于2025年在全球范围内启动这一项目，履行我们做出的、将体育领域的AI为全体人类所用的承诺。“Bach说道。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://olympics.com/ioc/news/ai-and-tech-innovations-at-paris-2024-a-game-changer-in-sport">https://olympics.com/ioc/news/ai-and-tech-innovations-at-paris-2024-a-game-changer-in-sport</a>"</p><p><a href="https://www.channelnewsasia.com/commentary/paris-olympics-ai-france-security-surveillance-privacy-4487966">https://www.channelnewsasia.com/commentary/paris-olympics-ai-france-security-surveillance-privacy-4487966</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/S55WXzuNfqeQg10CqHBH</id>
            <title>智谱上线视频生成模型：30秒生成6秒时长，免费不限次！B 站也有研发功劳？</title>
            <link>https://www.infoq.cn/article/S55WXzuNfqeQg10CqHBH</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/S55WXzuNfqeQg10CqHBH</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jul 2024 08:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI大模型, 文生视频, 图生视频, 清影（Ying）
<br>
<br>
总结: Sora带来了AI大模型的全新玩法，用户可以通过清影（Ying）生成文生视频和图生视频，包括各种风格和音乐选择。清影（Ying）的付费方案灵活，企业和开发者可以通过API调用体验模型能力。智谱AI的自研技术使得视频生成速度提升6倍，同时得到了合作伙伴的支持和北京市的大力支持。 </div>
                        <hr>
                    
                    <p>整理 | 华卫</p><p>&nbsp;</p><p>Sora毫无疑问带来AI大模型的全新玩法，大模型可基于任意文字生成视频，包括Runway的Gen系列、微软的Nuwa、Meta的Emu、谷歌的Phenaki/VideoPoet、智谱CogVideo等。</p><p>&nbsp;</p><p>7月26日，智谱AI&nbsp;CEO张鹏在智谱&nbsp;Open&nbsp;Day上宣布，AI生成视频模型清影（Ying）正式上线智谱清言，只需要30秒时间就能生成6&nbsp;秒时长、1440x960清晰度的3:2&nbsp;比例视频。当日起，所有C端用户都能通过清影（Ying）体验到AI文生视频、图生视频能力。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/45/455aa57e04e76478b280c88820afa395.png" /></p><p></p><p>PC&nbsp;端链接：<a href="https://chatglm.cn/video">https://chatglm.cn/video</a>"</p><p>移动端链接：<a href="https://chatglm.cn/download?fr=web_home">https://chatglm.cn/download?fr=web_home</a>"</p><p>&nbsp;</p><p>输入一段文字后（俗称Prompt），用户可以选择自己想要生成的风格，包括卡通3D、黑白、油画、电影感等，配上清影自带的音乐，就能够生成充满AI想象力的视频片段。</p><p>&nbsp;</p><p>除了文本生成视频，也可以到清影上玩图片生成视频。图生视频带来了更多的新玩法，包括表情包梗图、广告制作、剧情创作、短视频创作等。同时，基于清影的「老照片动起来」小程序也会同步上线，只需一步上传老照片，就能让凝练在旧时光中的照片灵动起来。</p><p>&nbsp;</p><p>现在，清影（Ying）API&nbsp;已同步上线大模型开放平台bigmodel.cn，企业和开发者通过调用API的方式，体验和使用文生视频以及图生视频的模型能力。</p><p>&nbsp;</p><p>据了解，清影（Ying）的付费方案是：在首发测试期间，所有用户均可免费使用，不限次数。此后，付费5元可解锁一天（24小时）的高速通道权益，付费199元可解锁一年的付费高速通道权益。&nbsp;</p><p>&nbsp;</p><p>清影上线后，我们也第一时间测试了它的实际效果。</p><p>&nbsp;</p><p></p><h2>文生视频</h2><p></p><p>&nbsp;</p><p>先来看看对于可爱动物的视频生成效果。我们输入了以下两个提示词：</p><p>&nbsp;</p><p>prompt1：两只小浣熊打架抢苹果</p><p></p><p></p><p></p><p>prompt2：一只奶牛猫在看猫和老鼠的动画片</p><p></p><p></p><p></p><h2>图生视频</h2><p></p><p>再来看看对于人类的视频生成效果，我们输入了一张“仕女拉小提琴”的图片：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9be858a7370ca4fa85c6369298aa41d4.png" /></p><p></p><p>&nbsp;</p><p>得到的视频如下：</p><p></p><p></p><p></p><h1>背后自研技术，推理速度比前代提升6倍</h1><p></p><p>据介绍，清影（Ying）底座的视频生成模型是CogVideoX，它能将文本、时间、空间三个维度融合起来，参考了Sora的算法设计；它也是一个DiT架构，通过优化，CogVideoX&nbsp;相比前代（CogVideo）推理速度提升了6倍。</p><p>&nbsp;</p><p>并且，智谱自研了一个端到端视频理解模型，用于为海量的视频数据生成详细的、贴合内容的描述，这样可以增强模型的文本理解和指令遵循能力，使得生成的视频更符合用户的输入，能够理解超长复杂prompt指令。</p><p>&nbsp;</p><p>在内容连贯性上，智谱AI自研高效三维变分自编码器结构（3D&nbsp;VAE），将原视频空间压缩至2%大小，配合3D&nbsp;RoPE位置编码模块，更有利于在时间维度上捕捉帧间关系，建立起视频中的长程依赖。</p><p>&nbsp;</p><p>该生成式视频模型的研发中，Scaling&nbsp;Law&nbsp;继续在算法和数据两方面发挥作用。“我们积极在模型层面探索更高效的scaling方式。”张鹏表示：“随着算法、数据不断迭代，相信Scaling&nbsp;Law将继续发挥强大威力。”</p><p>&nbsp;</p><p>bilibili作为合作伙伴也参与并支持清影的研发过程。同时，合作伙伴华策影视也参与了模型共建。</p><p>&nbsp;</p><p>此外，智谱&nbsp;AI&nbsp;生成式视频研发得到北京市的大力支持，海淀区是智谱AI总部所在地，为智谱AI开展大模型研发提供了产业投资、算力补贴、应用场景示范、人才等全方位支持。智谱&nbsp;AI&nbsp;生成式视频研发算力支持则来自于亦庄集群，目前北京亦庄人工智能公共算力平台已建成。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/d3WyubtTr7cHBaVALkUa</id>
            <title>两天内，两大开源模型打擂台：都在卷更小、更便宜、更快、更简洁</title>
            <link>https://www.infoq.cn/article/d3WyubtTr7cHBaVALkUa</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/d3WyubtTr7cHBaVALkUa</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jul 2024 10:22:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: GPT-4, 开源模型, Mistral, AI科技竞赛
<br>
<br>
总结: 两天内，市场上出现了两个 GPT-4 级别的开源模型，开源正在经历高光时刻。Mistral发布了旗舰模型，参数更小但性能不打折，加入AI科技竞赛。模型支持多种语言，适用于合成文本生成、代码生成等任务。模型在编码能力、推理能力、多语言能力等方面与其他领先模型相媲美。 </div>
                        <hr>
                    
                    <p>两天内，市场上就出现了两个 GPT-4 级别的开源模型，这意味着开源正在经历一个高光时刻。</p><p></p><h2>Mistral 发布开源旗舰模型，参数更小但性能不打折</h2><p></p><p>&nbsp;</p><p>对于前沿人工智能模型领域来说，这两天可谓热闹非凡，AI科技竞赛正在以前所未有的速度推进。</p><p>&nbsp;</p><p>继Meta日前发布全新开源Llama 3.1并作为其领先闭源“前沿”模型的替代方案之后，法国AI初创公司Mistral也摩拳擦掌加入战团。这家初创公司宣布推出其旗舰级开源模型的下一代产品，此模型拥有1230亿个参数，代号为Mistral Large 2，并声称在代码生成、数学和推理方面与OpenAI和Meta的最新尖端模型不相上下。</p><p>&nbsp;</p><p>Mistral Large 2 的发布恰逢Meta 发布其最新、最出色的开源模型Llama 3.1 405B的第二天。Mistral 表示，Large 2 提高了开源模型的性能和成本标准，这些优化在一些基准测试中已经体现出来。</p><p>&nbsp;</p><p>需要特别强调的是，Mistral 的模型与大多数其他模型一样，不是传统意义上的开源模型——任何商业应用都需要付费许可。这套模型仅被授权为以“开放”方式用于非商业研究用途，包括开放权重并允许第三方根据自身喜好对其进行微调。对于那些寻求将其用于商业/企业级应用的人来说，他们将需要从 Mistral 获得单独的许可和使用协议。</p><p>&nbsp;</p><p>早在今年2月，Mistral就推出过具有&nbsp;3.2万个token上下文窗口的初版Large模型。当时该公司称这款产品“对于语法和文化背景有着细致入微的理解能力”，因此可以推理并生成不同语言（包括英语、法语、西班牙语、德语和意大利语）与母语水平相当的流利文本。</p><p>&nbsp;</p><p>新版模型在此基础之上将上下文窗口增加至12.8万个token，与OpenAI的GPT-4o和GPT-4o mini以及Meta的Llama 3.1旗鼓相当。</p><p>&nbsp;</p><p>新模型还支持数十种新语言，包括初版已经支持的语言外加葡萄牙语、阿拉伯语、印地语、俄语、汉语、日语和韩语。</p><p>&nbsp;</p><p>Mistral方面表示，这套通用模型非常适合需要强大推理能力或者高度专业化的任务，例如合成文本生成、代码生成以及RAG（检索增强生成）等。</p><p></p><h2>两大最新开源模型PK，谁能更胜一筹？</h2><p></p><p>&nbsp;</p><p>Mistral 在一份新闻稿中表示，Large 2训练过程中重点关注点之一是尽量减少模型的幻觉问题。Mistral公司表示，Large 2 经过训练后，能够更敏锐地做出反应，能够意识到自己不知道的事情，而不是编造看似合理的事情。此外，Mistral 还声称 Large 2 的响应也比领先的 AI 模型更简洁，而领先的 AI 模型往往会喋喋不休。</p><p>&nbsp;</p><p>那么，Large 2与同样强大的Llama 3.1相比，在编码能力、推理能力、指令遵循与对齐、语言多样性方面谁高谁低？</p><p></p><h3>编码能力</h3><p></p><p>&nbsp;</p><p>初版Large模型在编码任务方面表现不佳，Mistral似乎在最新版本中专门利用大量代码进行了训练，最终成功纠正了这个问题。</p><p>&nbsp;</p><p>Mistral表示，根据他们在代码模型Codestral 22B和Codestral Mamba上积累的经验，他们在很大一部分代码上训练了 Mistral Large 2。Mistral Large 2 的表现远远优于之前的 Mistral Large，并且与 GPT-4o、Claude 3 Opus 和 Llama 3 405B 等领先模型相当。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/f6/f6f3708c2f35d8569dc72965ec5d6c84.png" /></p><p></p><h3>推理能力</h3><p></p><p>&nbsp;</p><p>Mistral还投入了大量精力来增强模型的推理能力。训练期间的重点关注领域之一是尽量减少模型产生“幻觉”或产生看似合理但实际上不正确或不相关的信息的倾向。这是通过微调模型来实现的，使其在响应时更加谨慎和敏锐，确保它提供可靠和准确的输出。</p><p>&nbsp;</p><p>此外，新款 Mistral Large 2 经过训练，能够在无法找到解决方案或没有足够的信息来提供自信答案时识别。这种对准确性的承诺体现在流行数学基准测试中模型性能的提高，展示了其增强的推理和解决问题的能力：</p><p></p><p><img src="https://static001.geekbang.org/infoq/bc/bc1cb1716bead3dbc186825db1be8aaf.png" /></p><p></p><p>代码生成基准上的性能准确性（所有模型都通过相同的评估流程进行基准测试）</p><p>&nbsp;</p><p>基准测试与 Llama 3.1 405B：</p><p>&nbsp;</p><p>MMLU：84.0% （Mistral Large 2） vs 88.6% （Llama 3.1 405B）HumanEval： 92% （Mistral Large 2） vs 89% （Llama 3.1 405B）GSM8K： 93% （Mistral Large 2） vs 96.8% （Llama 3.1 405B）</p><p>&nbsp;</p><p>在HumanEval和HumanEval Plus代码生成基准测试当中，其表现优于Claude 3.5 Sonnet与Claude 3 Opus，仅次于GPT-4o。同样的，在以数学为重点的基准测试（GSM8K与Math Instruct）当中，其成绩也移居第二。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/f7/f730bf69d75b919a101dfc496dc4ea1c.png" /></p><p>GSM8K（8 次）和 MATH（0 次，无 CoT）生成基准上的性能准确度（所有模型都通过相同的评估流程进行基准测试）</p><p></p><h3>多语言能力</h3><p></p><p>&nbsp;</p><p>在涵盖不同语言的多语种MMLU基准测试当中，Mistral Large 2的表现与Meta全新的Llama 3.1-405B相当，而且由于体量较小，所以有着更加显著的成本效益。</p><p>&nbsp;</p><p>Large 2支持 80 多种编码语言，包括 Python、Java、C、C++、JavaScript 和 Bash。该公司在博文中解释称，“Mistral Large 2专为单节点推理而设计，而且照顾到长上下文类应用场景——其1230亿参数的规模使其能够在单个节点以大吞吐量方式运行。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/3c/3c14e72b36ee65ce3eba4bb18129a520.png" /></p><p></p><p>MultiPL-E 上的性能准确度（除“论文”行外，所有模型都通过相同的评估流程进行基准测试）</p><p></p><p><img src="https://static001.geekbang.org/infoq/4d/4d470e545ce175a0b22bee309afa9e79.png" /></p><p></p><p>Mistral Large 2模型在多语言MMLU中的表现</p><p></p><h3>指令遵循与对齐</h3><p></p><p></p><p>随着企业越来越地采用AI技术，Mistral还专注于减少Mistral Large模型的幻觉。具体方法就是微调模型，使其在响应时更加谨慎且有选择性。如果没有足够的信息来支持答案，它也会直接告知用户以保持完全透明。</p><p>&nbsp;</p><p>此外，该公司还改进了模型的指令遵循能力，使其能够更好地听众用户指引并处理长时间内的多轮对话。新模型还经过调优以尽量让答案保持简洁明了——这一点在企业环境下同样非常重要。</p><p>&nbsp;</p><p>新款 Mistral Large 2 在遵循精确指令和处理长时间多轮对话方面表现尤为出色。下面是 Mistral Large 2 在MT-Bench、Wild Bench 和 Arena Hard 基准测试中的表现：</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/48/48d8702e2f632bce09cac58127b9be1c.png" /></p><p>在一般对齐基准上的表现（所有模型都通过相同的评估流程进行基准测试）</p><p>&nbsp;</p><p>在某些基准测试中，生成较长的响应往往会提高分数。然而，在许多商业应用中，简洁性至关重要——较短的模型生成有助于加快交互速度，并且推理更具成本效益。这就是为什么Mistral花费了大量精力确保生成尽可能简洁明了。下图报告了 MT Bench 基准测试中不同模型生成的平均长度：</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/fe/fe4bae414a6973c0885547c1f06c4e70.png" /></p><p></p><p>目前，Mistral公司已经通过其API商战平台以及Google Vertex AI、Amazon Bedrock、Azure AI Studio以及IBM WatsonX等云平台开放Mistral Large 2模型访问。用户甚至可以通过Mistral的聊天机器人对新模型进行测试，看看它在现实场景下究竟表现如何。</p><p>&nbsp;</p><p>经过多方面对比，最终得出的结论是：在代码能力数学基础测试中，Mistral Large 2的性能要优于Llama 3.1 405B，语言多样性方面的基准测试中，Mistral Large 2表现略逊于Llama 3.1 405B，在推理方面和指令遵循与对齐方面，Mistral Large 2与Llama 3.1 405B的表现不相上下。</p><p>&nbsp;</p><p>Mistral方面指出，该产品将继续“突破成本效率、速度与性能的极限”，同时为用户提供更多新功能，包括高级函数调用与检索，用以构建起更多高性能AI应用程序。</p><p></p><h2>网友怎么看？</h2><p></p><p>&nbsp;</p><p>两天之内，两家大模型明星公司纷纷推出高端大模型的做法引发业内热议。</p><p>&nbsp;</p><p>有网友评论，Large 2虽然不是完全开源有些令人沮丧，但仍然比完全关闭要好得多。</p><p>&nbsp;</p><p></p><blockquote>我认为Large 2的发布有两大很重要的进步： 第一是幻觉的减少；第二是略大于 100B 是一个不错的规模，因为它显示了 LLama 3.1 的收益递减（这里概括为数据不同，但它显示了趋势）。在我看来，这些研究发布将始终有助于改进其他开源模型。</blockquote><p></p><p>&nbsp;</p><p>仅在Meta发布模型的隔天就推出自家模型，Mistral难免被人猜测是想蹭科技巨头的热度。但也有网友为Mistral辩护称：</p><p>&nbsp;</p><p></p><blockquote>“Mistral绝非想借Large 2模型蹭Meta或者OpenAI掀起的这波AI热度。相反，Mistral一直在技术领域积极行动、筹集资金，并在发布各种任务特定模型（包括编码与数学模型）之余，与行业巨头合作以扩大自身影响力。”</blockquote><p></p><p>&nbsp;</p><p>网友Drew Breunig 强调了目前一个有趣的模式：最好的模型都在向 GPT-4 类能力靠拢，同时在速度和价格上展开竞争——变得更小更快，这适用于专有模型和公开许可模型。</p><p>&nbsp;</p><p></p><blockquote>我们都在将模型变得更小、更便宜、更快、更简洁。当 GPT-5 类模型开始出现时，我们是否会看到能力的大幅飞跃？很难得到肯定的答案。</blockquote><p></p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://techcrunch.com/2024/07/24/mistral-releases-large-2-meta-openai-ai-models/">https://techcrunch.com/2024/07/24/mistral-releases-large-2-meta-openai-ai-models/</a>"</p><p><a href="https://venturebeat.com/ai/mistral-shocks-with-new-open-model-mistral-large-2-taking-on-llama-3-1/">https://venturebeat.com/ai/mistral-shocks-with-new-open-model-mistral-large-2-taking-on-llama-3-1/</a>"</p><p><a href="https://mistral.ai/news/mistral-large-2407/">https://mistral.ai/news/mistral-large-2407/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Uguhq4S40LFGUxGu7I8V</id>
            <title>万字长文分享快手 Kolors 可图大模型应用实践</title>
            <link>https://www.infoq.cn/article/Uguhq4S40LFGUxGu7I8V</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Uguhq4S40LFGUxGu7I8V</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jul 2024 10:16:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 多模态能力, AICon 北京, 大语言模型, 多模态大语言模型
<br>
<br>
总结: 在企业提效方面，多模态能力同样具有重要意义。AICon 北京活动邀请了快手「可图」大模型负责人李岩分享了主题为《快手「可图」文生图大模型应用实践》的演讲内容。另外，AICon 全球人工智能开发与应用大会上海站策划了【多模态大语言模型的前沿应用与创新】专题，包括大语言模型在计算机视觉领域的应用和生成式音频大模型的多模态“产模结合”。文中还介绍了基座模型的发展趋势和设计要点。 </div>
                        <hr>
                    
                    <p>在企业提效方面，多模态能力同样具有重要意义。在 <a href="https://aicon.infoq.cn/2024/beijing/">AICon 北京</a>"站活动中，我们邀请了快手「可图」大模型负责人李岩，他分享了主题为《快手「可图」文生图大模型应用实践》的演讲内容，以下为李岩演讲内容～期待对你有所启发！</p><p></p><p>另外，在 8 月 18-19 日即将举办的<a href="https://aicon.infoq.cn/202408/shanghai/"> AICon 全球人工智能开发与应用大会上海站</a>"，我们也策划了【多模态大语言模型的前沿应用与创新】专题，目前已上线两个议题，字节跳动研究科学家冯佳时将带来《大语言模型在计算机视觉领域的应用》、喜马拉雅珠峰 AI 算法负责人叶剑豪将带来《生成式音频大模型的多模态“产模结合”》，详细信息和更多精彩议题<a href="https://aicon.infoq.cn/2024/shanghai/track">点击这里</a>"查看</p><p></p><p></p><h4>一、基座模型</h4><p></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/14/141d42e478ec5d8fcf83ad32a224903e.png" /></p><p></p><p>过去的一年对于文生图行业而言无疑是充满活力与突破的一年。在这段时间里，文生图行业经历了多次爆发式的增长。通过梳理从 2023 年 3 月份至今超过一年的时间线，可以观察到多个机构相继推出了自己的文生图产品，显示出该行业目前的热度与活跃度。上图列出了几个行业标杆，包括闭源机构 Midjourney，开源机构 stability.ai，以及国内外的互联网巨头，同时也包括快手可图 Kolors 大模型。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f5/f5b76786e4d1c77b9693bdc273742c5c.png" /></p><p></p><p>接下来本文将探讨视觉生成技术的发展趋势。为全面了解技术发展，时间轴被追溯至 2014 年。从上图可以很容易地观察到，在过去的十年中，生成式技术框架逐渐从生成对抗网络（GAN）向扩散模型（Diffusion Model）过渡。尽管这期间出现了一些基于自回归（Auto Regressive）的方法，但它们并未成为行业的主流。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f9/f9ddfd6ab936fc409971591fda0f4ef9.png" /></p><p></p><p>接下来，我们从数据侧、模型侧以及效果侧为读者介绍可图文生图基座模型。</p><p></p><p>数据侧：数据是构建大模型最关键的因素之一。数据的关键点在于：1. 数据量级要大；2. 数据覆盖概念要全，特别是中文概念；3. 图像质量要好；4. 图文相关性要高。上图这里展示的两张图像，来自行业最优秀的图像供应商 Shutterstock，很多企业包括我们在内都求之不得，而 Shutterstock 与 Google 和 OpenAI 等企业签订了战略合作。</p><p></p><p>其实这些数据就满足上述标准，包括艺术感、构图以及清晰度，并且在图文相关性方面表现优异，当然这些高质量版权数据的获取成本也相对较高。接着我们讨论数据安全，在文生图的训练过程中，必须同时确保文本和图像的组合安全，在特定情况下即使文本和图像单独看是安全的，组合起来仍会产生不当的关联含义（大家自行脑补）。推理侧更要保证出图的安全，这要感谢快手多年建立起来的行业领先的全场景风控解决方案，从文本到视觉为模型安全保驾护航。在数据的讨论中，一个经常性的问题是：文生图模型训练的过程中是否会遗忘旧概念？这是视觉生成领域许多研究者共同关心的问题。</p><p></p><p>在这里，本文提出了第一个可图观点：“概念只会被覆盖，不会被遗忘；数据是每个公司的核心资产，也是最有可能建立差异化优势的模块，现在和未来会有大量的数据供应商出现；AIGC 数据对视觉生成模型‘功在当代，罪在千秋’”。下面我们将以一个具体的例子来阐释这点，众所周知，文生图领域有一个被大家广泛使用的开源模型 Stable Diffusion。这个模型是基于西方数据进行训练的，天然具备生成裸体的能力（西方对于色情内容的监管相对宽松）。</p><p></p><p>可图在很早期的实验版本中，曾经做过这样的认知实验，我们利用安全的训练数据（不包含任何裸体数据）在 Stable Diffusion 模型基础上进行微调续训，经过多轮数据迭代后，发现当你使用“裸体”这样的关键词去触发文生图，模型依然能够成功生成裸体图像，这些旧的概念知识来自于模型的初始化参数。所以，模型原有的概念并未被完全遗忘，他们只会被同概念的新数据分布所覆盖。因此从安全的角度看，国产文生图大模型应该尽量放弃西方开源的模型参数。此外，我们还想强调的是，数据不仅是每个公司的核心资产，也是未来建立差异化优势的关键模块，大模型时代任何人都应该保持对数据的敬畏与投入。</p><p></p><p>最后一个数据侧的可图观点是“AIGC 数据对视觉生成模型‘功在当代，罪在千秋’”，许多行业从业者都试图尝试利用 Midjourney 这类标杆数据去训练自己的生成模型，其实可图团队在初期也进行了一些类似的认知实验，我们发现这种做法其实是弊大于利的。使用 AIGC 数据训练模型，确实能在短时间内提升模型的出图效果，因为是立竿见影的，所以我们说“功在当代”，但是，长期依赖 AIGC 数据可能会使模型逐渐失去理解和拟合世界物理规律的能力。因此，可图大模型团队已经完全抛弃了 AIGC 的训练数据。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/40/4083a672691252e3ba6073229a075a17.png" /></p><p></p><p>模型侧：这里我们主要探讨当前主流的两种生成式框架。上图左侧是 Stable Diffusion 的框架，其主干模型是 U-Net，而右侧是随着 Sora 的推出，出现在技术舞台中央的 DiT（Diffusion Transformer）框架。这两种框架都是当前文生图以及文生视频领域的主流框架。在这里，我们给出了第二个可图观点：“至少未来一年内扩散模型仍将是视觉生成任务的主要技术框架，只不过执行扩散的主体结构会逐步进化”。</p><p></p><p>下面列举了一些基座模型的设计要点，包括加去噪理论（如 DDPM、EDM、RF），采样器（如 DDIM、Euler、LMS、DPM-Solver），以及参数规模（1B、3B、5B、10B）。在特征空间的设计上，可以选择像素空间或隐空间。文本表征方面更为复杂，需要考虑是单语种还是多语种，选择合适的文本 Encoder 也是至关重要，即可以是能够刻画图文相关性的 CLIP，也可以是大语言模型 LLM。根据输出图像的策略，模型框架又可以分为一阶段、二阶段或者多阶段出图。过去一年的时间里，可图团队在上述关键技术维度上进行了充分的探索，并且沉淀下来了一套在当前资源下的技术方法论。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/78/78530b1cb1d6ff8ead04d701231a1f25.png" /></p><p></p><p>效果侧：我们简要从模型 GSB（图像生成质量评分）和作品墙来展示可图大模型的能力。从 2023 年 5 月发布的第一个版本开始，直到 2024 年 2 月发布的第五个版本，可图大模型在 GSB 评估上已经超过了 Midjourney-V5。目前内部的测试结果显示，最新版本的可图大模型表现已非常接近 Midjourney-V6 的水平。在智源 FlagEval 文生图模型第三方评测榜单中，可图（Kolors）以主观综合评分 75.23 分的成绩，排名全球第二，仅次于闭源的 DALL-E 3。</p><p></p><p>特别值得一提的是，在主观图像质量方面，可图（Kolors）表现尤为突出，评分排名第一，显著优于其他开源和闭源模型。</p><p></p><p></p><h4>二、效果评估</h4><p></p><p></p><p>第二部分，文生图模型的效果评估。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a9/a9209735bf413f0eac6b1f8ce068dd08.png" /></p><p></p><p>文生图和文生视频的效果评估方法在行业内尚未成熟，这与大语言模型领域的情况略有不同。实际上，大语言模型领域已有多个成熟的评估基准，这些评估基准很多由投资机构设立，以帮助其评估模型的投资价值。相比之下，文生图和文生视频的评估基准则发展缓慢。我们将评估方法分为两类，机器评估与人工评估。人工评估是视觉生成模型非常关键的评估方式，因为最终这些模型还是要服务人类用户。</p><p></p><p>在人工评估中，可图团队采用对战场景中的“Good，Same，Bad”（GSB）作为内部评估指标。评估会关注生成图像在整体观感、图文相关性、图像质量以及图像真实感等维度上的表现，并对每个维度进行 1 到 5 分的离散打分。在评估人员接受了充分培训的前提下，人工评估能够非常准确地反映出生成内容与人类体感的一致性。然而，人工评估的主要缺点是耗时耗人力，具体地，可图大模型当前的评测集可能包含数千条 Prompts，进行一轮完整的人工评估就需要两到三天。因此，虽然人工评估在准确性上具有优势，但为了考虑效率，机器评估同样不可或缺。</p><p></p><p>在机器评估方面，存在一些专门用于衡量生成图像效果的指标，例如使用 CLIP 刻画图文的相关性，使用 FID 刻画图像质量，以及其他一些衡量美学或相关性的指标。可图团队在实验过程中发现，上述学术界使用的传统机器评估指标往往是不稳定的，像 FID 这样依赖参考集的指标，参考集轻微的调整就会导致 FID 大幅度变化。此外每个机器评估指标通常仅能评价图像在某一方面的性能，比如美学、相关性或图像质量。</p><p></p><p>因此，我们将机器评估的价值定位为“发现红线”，也就是说，在值域的极端（高或低），机器评估是数值敏感的，但是中间段数值则是不敏感的。具体地，在模型训练的过程中，如果 CLIP 图文相关性指标突然出现了大幅下降，这可能意味着模型确实出现了严重问题，此时需要算法同学注意并调整模型，而在中间段数值范围内波动的机器评估指标，我们通常无需过分关注。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/5f/5f824eaf546b3cd6d44f6ca884a81561.png" /></p><p></p><p>在这里，我们提出了第三个可图观点：“视觉生成类任务的评估是非常主观的，大部分传统学术界机器评估 Measurement 不稳定、不置信，模型最终是给人用的，所以要对人的偏好进行建模”。在 CVPR 2024 上，可图团队发表了一篇论文 Learning Multi-dimensional Human Preference for Text-to-Image Generation（代码开源、数据开源、模型开源），旨在解决这个问题。这篇工作主要传达了一个信息：传统的学术评估指标在衡量真正用于人类的文生图系统时，大多数指标表现出不稳定性和不适用性。为了有效地结合人工评估的精度和机器评估的效率，这篇工作提出了一种基于人类反馈的奖励模型，用机器模型去建模人类偏好。</p><p></p><p>上图左侧是行业内已经存在的一些文生图评估的开放数据集，这些数据集存在一些局限性，量级不够大且评价维度较为单一。右侧展示的是我们这篇工作中提出的技术框架，其中一路负责描述视觉信息，另一路负责描述文本信息。视觉和文本信息通过两条路径进行交融，拟合人工标准对生成图像的多维度质量打分。</p><p></p><p></p><h4>三、衍生能力</h4><p></p><p></p><p>在我们讨论可图大模型应用之前，先来介绍三个关键的衍生能力。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8d/8d675b4a7be77c92dfefbc5b17dce972.png" /></p><p></p><p>第一个衍生能力：提示词润色能力， 这里我们给出三个示例，1）井底之蛙，如果不加任何提示词润色，可以看到可图大模型仅能绘制出青蛙，但如果加上了提示词润色，则可以明显地看到在青蛙的基础上又多了一层坐井观天的文学意境；2）没有青蛙的荷叶，这是个非常典型的衡量一个文生图模型好坏的陷阱示例，即否定词场景，如果不加任何提示词润色，一般会倾向于画上一个青蛙，但是如果使用提示词润色，则会发现只有荷叶被绘制出来；3）A 股 2500 点保卫战，左边是没有提示词润色的结果，画出了一个战争的场景，右边则经过了提示词润色，画出了 A 股市场面临的巨大压力和股民们焦虑的神态。</p><p></p><p>到这里，我们给出第四个可图观点：“借助大语言模型进行提示词润色，能很好的解决成语、文学概念、否定词、互联网热梗新梗，以及新概念的语义理解与表征问题，同时降低文生图大模型的使用门槛，从‘咒语’到‘人话’”。大家可能都听说过 Midjourney，但真正使用过的人其实相对较少。主要原因有三点：第一，它的使用环境是英文；第二，它的使用方式是命令行；第三，它的使用需要一些特定的“咒语”专业知识，这三点对普通用户其实还是构成了挺大的门槛。可图团队需要解决的正是这些问题，目标是让大家能更方便地使用可图大模型。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/1c/1c3571277873822cdac78ebb9e261df0.png" /></p><p></p><p>第二个衍生能力：文字绘制能力，这是可图在整个行业里做的非常前沿的一个能力。可图的文字绘制能力有两大特点，第一，无 Control 逻辑，现有的一些技术方案在文字绘制时需要先确定字的位置、大小、字体等，然后再绘制文字，而可图无需此类控制逻辑；第二，无特殊提示词激活逻辑，不同于需要特定提示词来激活写字模型的其他方法，可图是一个通用模型，能够在没有特殊提示词的情况下进行文字绘制需求响应。这种全自由度的、开放域的文字符号绘制能力，在市面上比较少见。</p><p></p><p>在这里，我们给出第五个可图观点：“开放域的文字符号绘制是视觉生成领域的‘上乘武功’，对专项数据的要求极高。行业短时间内，文字绘制还只能用于娱乐场景（例如表情包），严肃场景的文字绘制需要阶段性倚赖结构性线索或约束”。我们判断行业短时间内文字绘制还只能应用于娱乐场景，如表情包制作。可图团队内部已经在广泛使用这样的文字表情包，结合上人像 ID 保持能力，可玩性非常高，增加了团队间的互动乐趣，也促进了团队氛围。尽管文字绘制确实带来了一定的娱乐价值，但也必须认识到，当这种能力用于更严肃的业务场景时（如广告海报或商品主图），可能还是需要一定的结构性线索或约束来确保正确性和美学效果。</p><p></p><p></p><p>第三个衍生能力：交互式视觉生成能力，该能力已在内测中，它允许用户通过自然语言与可图大模型交互，实现更加直观和灵活的图像生成。在这里，我们给出第六个可图观点：“操作台式产品（Pro）面向高阶用户，对话框式产品（Lite）面向初阶用户”。例如，“画三个老太太吵架”这一请求，可以通过用户的反馈（如“吵得不够激烈”）来调整生成的图像内容，使其更加符合用户的预期。有了大语言模型的配合，可图模型能呈现出更加夸张的视觉效果。另外一个交互式功能可以支持一些业务，例如，如果用户对一张模特的图片比较满意，但希望更换背景为沙滩，仅需通过语言指令告知系统即可。用户还可以进一步调整服装颜色，甚至为模特添加太阳镜。总结来说，用户可以用自然语言的方式与智能体进行交互，让“小白”用户也能轻松使用可图大模型，而这也是整个行业的一大趋势。</p><p></p><p></p><h4>四、应用实践</h4><p></p><p></p><p>这一部分，我们开始介绍可图大模型的应用实践。2023 年，可图团队的重点主要在基座研发，到了 2024 年，团队重点逐步从基座研发转向业务落地。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/77/77321529d6131cbd53654a05e3127a19.png" /></p><p></p><p>上图最下方是文生图的基座层，其上则是插件层，这些插件对于将基础技术转化为实际应用至关重要。我们将插件大致归纳为三类：1）可控模块，其中强 Control 用于保持图像的空间语义信息，弱 Control 用于维持全局的语义信息；2）时序模块，其中长时序指生成时间较长的视频内容，类似于 Sora；短时序则主要指的是一些 10 秒以内的镜头微动；3）ID 保持模块，这里的 ID 包括 SKU-ID、Face-ID 以及一些其他需要保持的 Appearance-ID。</p><p></p><p>最上面是应用层，应用层的讨论将从娱乐场景过渡至商业场景，也即从小生产场景过渡至大生产场景。这两端的业务形态存在明显差异：娱乐场景下，模型的主要目标是创造趣味性和娱乐性，模型的表现达到 80 分就已足够，用户通常能够接受小范围的不完美，因为主要目的还是娱乐；商业场景则对模型的效果提出了更高的要求，必须做到近乎完美，达到接近 100% 的好用，其中任何的小错误都可能影响到商业价值。</p><p></p><p></p><h5>应用实践一：AI 玩评</h5><p></p><p></p><p></p><p></p><p></p><p>我们介绍的第一个应用是 AI 玩评，大家在快手的评论区就可以体验到，这也是快手 AIGC 在短视频领域的一次原创尝试，第七个可图观点：“AI 玩评是 AIGC 在短视频领域落地的一次原创尝试，一定程度上促进了用户在评论区进行自我表达的积极性”。上图展示了一些相对有趣的案例，有些 AI 生成的评论甚至获得了上千次甚至上万次的点赞，一定程度上补齐了用户在评论区使用图像表达自己的需求。</p><p></p><p></p><h5>应用实践二：AI 人像</h5><p></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/d0/d0dac70310f3615455cbdbf9b2e8b45f.png" /></p><p></p><p>第二个应用是 AI 人像，相信大家都了解“妙鸭”类产品，它们主要基于 Dreambooth 和 LoRA 的框架来实现人像 ID 的保持。虽然“妙鸭”类产品因其技术曾被广泛讨论，但实际使用过程中会存在一些局限。首先，用户需要支付一些费用（建立数字分身真的需要 GPU 训练）；其次，用户需要提供若干张照片（反正程序员的相册里是很难找到自己那么多张照片的）；最后，数字分身的建立需要用户分钟级甚至小时级的等待。以上这些因素还是一定程度地限制了功能的便捷性。可图团队倾向更为高效的解决方案，即 Training-Free 的 ID-Adapter 类 ID 保持方法。</p><p></p><p>我们给出第八个可图观点：“基于 Dreambooth &amp; LoRA 框架的人像 ID 保持方法，在 Training-free 的 ID-Adapter 类方法面前 ROI（Return-on-investment）无优势，特别是对于原始输入人像质量较高的业务场景更是如此”。行业更需要的是一种能够仅通过用户给定的单张图像就能实现 ID 保持的解决方案。在我们上图提供的示例中，最左边的大图像是用户输入的原图，右边则随机展示了使用 Dreambooth 和 LoRA 框架的传统重方案出图和使用 ID-Adapter 类轻方案的出图，可以看到人像 ID 的保持程度都非常高。</p><p></p><p></p><p></p><p></p><p></p><p>接下来，我们还将展示关于人像保持在风格化方面的尝试。上图左边是最近非常流行的 Remini 粘土特效（强 Control），我们给出了静态和动态的效果；右边则是通用垫图的风格化效果（弱 Control），它保持了原图的宏观语义。在这里，我们给出第九个可图观点：“传统基于模板的人像魔表特效研发范式正在被大模型逐步重构，未来的趋势将是统一的视觉生成基座大模型辅以设计师专家级 Prompt 调优”。</p><p></p><p>传统做法中，设计师会依赖固定模板集合，利用用户上传的人脸进行换脸操作，往往局限于五官的替换，且模板数量有限，用户经常面临与他人“撞模板”的尴尬。相比之下，基于生成式大模型的方案，模型每次生成的结果都是独一无二的，甚至同一用户同一张照片的不同生成实例也会有所不同，这就是生成式大模型所带来的新的价值。</p><p></p><p></p><h5>应用实践三：IP 定制</h5><p></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a1/a1ff1842c146761835e0ece14912d8be.png" /></p><p></p><p>第三个应用是 IP 定制，可图团队使用 Dreambooth 方法来支持这一类应用场景。这里，我们给出第十个可图观点：“非真人 IP 形象定制还是需要 Dreambooth 类方法框架，但文字细节还原是技术难点”。虽然在人像保持类的应用中，Dreambooth 的 ROI 可能较低，但在处理 IP 或公仔类的形象时，它显示出较高的实操价值。最近，在快手的司庆活动中，小快和小六的司庆形象均由可图大模型来辅助设计师进行设计。</p><p></p><p></p><h5>应用实践四：图像融合</h5><p></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c9/c93def76e574a0c1e5ed871bc3c10ac9.png" /></p><p></p><p>第四个应用是图像融合。图像融合是将文生图的技术从“基于文本生成图像”扩展到“基于图像生成图像”。用户输入是两张图像，生成的过程主要是利用这两张图像作为条件进行图像生成，保持第一张图像的 ID 信息和第二张图像风格信息。这个功能的用户接受度很高，其实特别是在预测类玩法上，用户往往喜欢多次尝试，直到获得满意的结果。</p><p></p><p></p><h5>应用实践五：AI 扩图</h5><p></p><p></p><p></p><p>第五个应用是 AI 扩图。我们在快手的评论区（“AI 小快”和“AI 玩评”）已经上线了扩图功能。上图最右侧展示的是“AI 小快”的评论区互动，有时会生成一些让用户“哭笑不得”的结果。这里，我们给出第十一个可图观点：“AI 扩图的‘翻红’是典型的‘老树长新芽，枯藤开新花’，在此基础上，引入文本条件的 AI 扩图可玩性更高”。</p><p></p><p></p><h5>应用实践六：直播 +AIGC</h5><p></p><p></p><p></p><p>第六个应用是直播 +AIGC。可图大模型在直播侧的应用可以分为两个主要场景：“直播 AIGC 礼物”和“直播 AIGC 场景”。在直播中，主播往往期望收到独一无二的定制礼物，而粉丝也希望送出和自己相关的专属礼物，AIGC 刚好提供了这样的能力。直播间的背景素材也可以由可图大模型生成，这些 AIGC 背景与直播内容的和谐程度相对较高，违和感也较少。而且直播背景素材是直播成本中很重要的一部分，特别是在剧情演绎类的直播中，AIGC 背景可以即时地、平滑地辅助场景切换，提升看播体验的同时也降低了成本。虽然我们在直播侧的 AIGC 应用已经有了一些进展，但整体还是相对保守。这是因为直播场景对即时 AIGC 内容的安全性要求极高，直播中生成的任何内容（礼物或背景）都会被主播和所有直播间观众看到，如果生成的内容触发了安全问题，后果将非常严重。保险的做法是所有可能出现在直播中的 AIGC 生成内容，都事先生成并进行严格的人工审查，确保其绝对安全后才能使用。</p><p></p><p>这里，我们也给出第十二个可图观点：“直播场景对即时 AIGC 内容的安全性要求较高，‘保守的白库策略’与‘积极的即时机审策略’之间的博弈决定了业务价值的空间，直播 +AIGC 的想象空间巨大”。</p><p></p><p></p><h5>应用实践七：小说漫</h5><p></p><p></p><p></p><p>第七个应用是小说漫，小说作为重要的内容消费品类，可图团队正在探索如何利用 AIGC 技术为小说提供配套的视觉素材，从而生成相应的视频内容。小说漫的工作流程包括：1）小说生成，在没有小说正文的情况下，也可以利用大语言模型根据给定主题进行小说的正文生成；2）角色道具解析 + 分镜拆解，通过大语言模型分析小说片段，确定需要哪些角色、道具以及分镜（包括含角色和不含角色的分镜）；3）角色与道具的生成，利用可图大模型生成所需的角色和道具；4）分镜场景生成，在生成分镜场景时，如果涉及到角色，需要使用预生成的角色图作为垫图，从而确保生成场景中角色的视觉一致性；5）视频渲染，这个阶段包括字幕合成、转场效果、TTS 以及 BGM，进而生成完整的可播放视频。</p><p></p><p></p><h5>应用实践八：AI 商品</h5><p></p><p></p><p></p><p>第八个应用是 AI 商品，介绍之前，我们先给出第十三个可图观点：“2024 年是 AI 电商爆发的一年，新技术的出现会催化出全新的产品与业务形态，然而电商的本质却是‘低价电商’，素材的美化只能提供增量价值，这其中‘白牌’商品素材生成与美化更为刚需，且累积长尾收益也会更大”。在快手平台，很多卖家往往只能提供质量一般的白底商品图，可图大模型可以帮助他们进行效果增强。这里我们展示了运动鞋和水果（冻梨）两个案例，通过大语言模型或多模态大模型理解商品特性，并为其匹配合适的背景描述，进而可以通过局部重绘的方式来重绘商品背景。</p><p></p><p>在冻梨的案例中，模型生成与梨花相关的背景，不仅美化了商品图，还增加了视觉上的关联性。上图第三列展示了利用大语言模型生成商品标题、卖点、特性以及宣传语的能力效果，这些生成的文本内容可以用来制作商品的头图或海报，进一步提升商品的表现能力。最后，通过引入时序模块，我们可以使商品图动起来，利用人类对动态内容的敏感度，提高商品在电商平台上的表现力，帮助商品在竞争激烈的电商环境中脱颖而出。</p><p></p><p></p><h5>应用实践九：AI 模特</h5><p></p><p></p><p></p><p></p><p></p><p>第九个应用是 AI 模特，预计未来在快手平台上会有更多试穿相关的应用场景逐渐出现。这项技术主要为了帮助那些无法负担专业模特费用的小服装或个体经营者。借助这个能力，商家只需提供服装的白底图，可图 AI 模特技术可以生成不同发型、长相、年龄、身材、国籍的模特（这一过程完全通过文本描述来控制，使得定制化模特成为可能）。此外，AI 模特不仅可以进行静态的试穿，展示衣服的外观和搭配效果，同时还可以支持动态试穿效果。</p><p></p><p></p><p></p><p>这里，我们给出第十四个可图观点：“从 2024 年开始，开放域的虚拟试穿才呈现出业务落地的技术可行性，B 端（卖家）关注模特版权与商拍级效果，C 端（买家）关注 SKU 保持的同时还关注身份 ID 的保持，千人千面的商品素材生成从此将成为可能”。</p><p></p><p>其实虚拟试穿概念被定义和讨论已经有很多年了，无论是基于 3D 的方案，还是基于 GAN 的方案，商业级的大规模落地都无法保证用户体验，更多的还是停留在学术 Demo 展示层面。如今，得益于基座视觉生成大模型，我们能够更加精确地处理衣服细节（如品牌商标等），从而使虚拟试穿在商业场景中落地成为可能。此外，随着这种技术的成熟，未来“千人千面”的商品素材生成和推荐也许会成为可能，即使是在浏览同一件服装，不同的消费者看到的模特也可能完全不同。</p><p></p><p></p><h4>五、给国内视觉</h4><p></p><p></p><p>生成同行的几点建议</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/48/48ca69677ab0b556e14565bb8d8c94c9.png" /></p><p></p><p>这张“兄弟连”的图像是可图大模型生成的，它象征着可图团队过去一年在逆境中砥砺前行，走过了炮火与硝烟，最终迎来了朝阳和希望，同时这个画面也象征着中国国产大模型过去一年的艰辛，行业的困难，资源的封锁，都挡不住国产大模型向前的决心。我们也希望用这张图把中国大模型赛道里的兄弟企业团结起来，打破封锁，形成我们自己的特色，建立我们自己的优势。最后，我们结合过去一年的从业经历，为国内视觉生成同行贡献六点建议：</p><p></p><p>未来一年，图像生成与视频生成的基础算法框架会实现和谐且鲁棒的统一。目前，尽管 Sora 类技术已经展示出 DiT 在视频生成和图像生成方面的统一能力，但是一个通用模型同时在两个子问题上的生成效果均达到行业 SOTA（图像生成效果超 Midjourney V6，视频生成效果超 Sora）还是有一定技术挑战的。不过，我们有信心行业可以在一年左右的时间内达到这个目标。应用落地不等基座研发，同时启动，相向而行。很多公司目前都在进行基座模型的研发，但是基座模型研发和业务应用落地同等重要，我们给出的建议是两个事情要同时启动，相向而行，应用落地探索不要被动的依赖自家的基座模型，建议初期借助开源模型探索技术方案，随着自研基座模型效果的迭代，逐步完成平滑切换，这样效率会高很多。请保持对数据，特别是优质数据的敬畏与投入，数据的重要性应该优于资源及人才。大模型时代，数据的重要性是大于资源和人才的。资源方面，虽然中美关系给国内的资源获取造成了比较大的困难，但是我们也应该看到国产芯片最近几年的发展趋势，所以长期看资源是乐观的，当然这也非常需要我们这些行业从业者去相信并且支持我们自己的国产芯片，多给他们一些机会。人才方面，中国大陆的 AI 人才在素质上并不逊色于北美，如果非要说有差距，其实这个差距主要还是来源于经验认知，而大模型时代的认知是需要用真金白银（GPU）去建立的，北美的资源确实存在短时优势，人才也就相应地存在一些认知优势，但是这种优势会随着资源问题的解决而慢慢的消失。模型安全问题应该从堵到疏，尽早干预，源头治理。这个比较直观，生成式模型训练的时候不去干预训练数据，而是在使用的时候寄希望于风控能力，这种临时抱佛脚是非常危险的行为。所以我们呼吁安全问题还是要回到源头进行治理。视觉生成产品定位要考虑清楚用户群体是什么，是专业用户还是小白用户？没有人像使用朋友圈一样每天使用 Midjourney。尽管 Midjourney 非常受欢迎，但其用户规模和日活跃用户数量显示，它并没有像微信朋友圈一样成为大众日常频繁使用的工具。所以，从业者要想清楚我们是要做专业的工具工作台去服务专业人群，还是要做简单易用的日常功能去服务大众，这二者将是完全不同的设计思路。“老业务形态 +AIGC”还是“AIGC 催生全新的业务形态”。以电商场景为例，利用 AIGC 技术将普通商品素材转化为优质商品素材（例如，白底图到 AIGC 背景图），它所能带来的业务价值相对有限，因为电商平台的核心竞争力终究是“价格力”，这就叫“老业务形态 +AIGC”，即业务形态还是原来的样子，大差不差，只不过其中某些元素借助 AIGC 进行了升级。另一种模式是“AIGC 催生全新的业务形态”，比如虚拟试穿，这个能力在 2024 年之前是不成熟的，完全不可用于商业场景，但是随着大模型技术的发展，不可能变成了可能，这将影响整个电商行业的游戏规则。试想一下，以前都是衣服卖出去了才能看到“买家秀”；现在衣服一件没卖，“买家秀”却能出来一大堆，这对于现有的买卖逻辑将会形成什么样的冲击，未来内容电商、货架电商的格局会发生什么变化，我们一起拭目以待。</p><p></p><p></p><h4>六、写在最后</h4><p></p><p></p><p>7 月 6 日，快手高级副总裁、主站业务与社区科学负责人盖坤（于越）在世界人工智能大会（WAIC）上宣布，快手旗下的可图 Kolors 大模型将全面开源。Kolors 支持中英文双语，生成效果比肩 Midjourney-v6 水平，支持长达 256 字符的文本输入，具备英文和中文写字能力。目前，Kolors 已在 Hugging Face 平台和 GitHub 上线，包括模型权重和完整代码，供个人开发者免费使用。</p><p></p><p>官网地址：<a href="https://kwai-kolors.github.io/">https://kwai-kolors.github.io/</a>"Github 项目地址：<a href="https://github.com/Kwai-Kolors/Kolors">https://github.com/Kwai-Kolors/Kolors</a>"Hugging Face 模型地址：<a href="https://huggingface.co/Kwai-Kolors/Kolors">https://huggingface.co/Kwai-Kolors/Kolors</a>"技术报告地址：<a href="https://github.com/Kwai-Kolors/Kolors/blob/master/imgs/Kolors_paper.pdf">https://github.com/Kwai-Kolors/Kolors/blob/master/imgs/Kolors_paper.pdf</a>"</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c1/c1e8bed1e1766db0aa21c853031e83fc.png" /></p><p></p><p>在最近的智源 FlagEval 文生图模型评测榜单中，Kolors 凭借其卓越表现，主观综合评分全球第二，仅次于闭源的 DALL-E 3。尤其在主观图像质量上，Kolors 表现显著优于其他开源和闭源模型，评分排名第一。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/31/31cc9b709a3e50c829436e08f3288b5f.png" /></p><p></p><p>Kolors 开源短短几天，在 Github 已收获 2.5k stars，在 Hugging Face 也登上了模型 Trending 榜榜首，截止本文撰稿前已被下载数万次。目前开源社区反响热烈，已经有开发者提供了加速、ComfyUI 等周边能力。这一系列开源动作，将为开发者提供更加全面和多样的工具资源，进一步丰富文生图领域的开源生态，为探索更多的应用场景和技术创新提供便利，共同推动文生图技术的进步和普及。可图，未来可期。</p><p></p><p>嘉宾介绍：</p><p></p><p>李岩，快手可图大模型负责人，中科院计算所博士，原微信视频号内容理解负责人，主要研究方向为多模态内容理解与生成技术，在人脸识别、图像理解、图像生成、视频生成等领域有 10 年以上的算法研发、业务落地及管理经验。</p><p></p><p>活动推荐：</p><p></p><p>InfoQ 将于 8 月 18 日至 19 日在上海举办 AICon 全球人工智能开发与应用大会，汇聚顶尖企业专家，深入端侧 AI、大模型训练、安全实践、RAG 应用、多模态创新等前沿话题。现在大会已开始正式报名，详情可联系票务经理 13269078023 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/33/338342715ad26d9ffeaf7b0af53008f7.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/vVJP1ah6hjgLf1uayh3M</id>
            <title>不“卷”寻常路，这家全病程管理独角兽让AIGC在医疗领域彻底落地了</title>
            <link>https://www.infoq.cn/article/vVJP1ah6hjgLf1uayh3M</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/vVJP1ah6hjgLf1uayh3M</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jul 2024 09:58:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI超级明星公司, GPT-4, 医疗场景, 电子健康记录
<br>
<br>
总结: AI技术在医学领域的应用经历了多个发展阶段，从影像识别到语言处理。随着GPT-4等大模型的出现，AI在生成文本、理解语义方面取得了显著进步，为医疗领域的电子病历生成、问诊辅助及基于个人EMR、EHR的健康管理提供了可行的路径。微脉作为一家依托“互联网+AI”模式成长起来的数字健康公司，致力于全病程管理服务，通过AI技术提供个性化的健康管理方案，为医疗服务的提升和患者满意度做出贡献。 </div>
                        <hr>
                    
                    <p>去年3月份，AI超级明星公司OpenAI重磅发布了GPT-4大语言模型，它的出现标志着自然语言处理技术的重大突破，也意味着人工智能系统的能力更接近于人类水平。</p><p>&nbsp;</p><p>随后没多久，在芝加哥的一个会议中心，数万名与会者观看了由 GPT-4 支持的新型AI技术（AIGC）在真实的医疗场景中的应用。这项具有划时代意义的AI技术模拟了临床医生如何使用新平台在几秒钟内将医生与患者间的互动转化为临床医学笔记。</p><p>&nbsp;</p><p>它的工作原理如下：医生使用 AI 平台上的移动应用程序记录患者就诊情况。平台实时添加患者信息，识别空白信息并提示医生填写，有效地将患者的口述内容转化为有参考价值的结构化笔记。</p><p>&nbsp;</p><p>问诊结束后，医生在计算机上查看 AI 生成的笔记（这些笔记可以通过语音或打字进行编辑），并将其提交到患者的电子健康记录 (EHR)。这种近乎即时的电子记录方式与传统的医生手动执笔记录和管理患者信息相比更加省时省力。</p><p>&nbsp;</p><p>事实上，AI技术在医学领域的应用经历了多个发展阶段。</p><p>&nbsp;</p><p>早期，被称为“AI四小龙”的企业主要聚焦于影像识别与标注，这一领域的应用已相对成熟并广泛普及。这是因为影像识别本质上可以替代大量初级医生或助手的工作，特别是在影像预读方面，有效减轻了高级医生的负担。</p><p>&nbsp;</p><p>然而，除了影像识别外，医疗领域的AI应用还包括自动生成电子病历、辅助临床决策支持系统（CDSS）等功能，但这些应用尚未得到广泛应用，仍处于试点阶段。这主要归因于AI在解决语音、语义和语言理解方面面临的挑战。与影像识别不同，语言处理的复杂性和多样性使得AI在这一领域的进展相对缓慢。</p><p>&nbsp;</p><p>综合上述技术演进历程，微脉创始人兼CEO裘加林将AI在医疗领域的落地应用划分为三个阶段：可行、可用和都用。每个细分领域的AI落地进程又不尽相同。</p><p>&nbsp;</p><p>目前，医疗影像识别已经实现了普及应用，但在语言处理方面，AI仍处于“可用”但尚未达到“普及”的阶段。随着GPT等大模型的出现，AI在生成文本、理解语义方面取得了显著进步，为医疗领域的电子病历生成、问诊辅助及基于个人EMR（电子病历记录）、EHR（电子健康记录）的健康管理提供了可行的路径。</p><p><img src="https://static001.geekbang.org/infoq/7d/7db7c8473615348caabec09de302c351.jpeg" /></p><p></p><p></p><h3>不“卷”寻常路，让微脉押对了宝</h3><p></p><p>&nbsp;</p><p>在AI技术迅猛发展之际，医疗领域也涌现出一批依托“互联网+AI”模式成长起来的公司，微脉就是这样一家借AI之力迅速腾飞起来的数字健康公司。</p><p>&nbsp;</p><p>自2015年9月9日成立以来，微脉致力于为全人群提供全方位、全周期的医疗健康服务，满足老百姓多层次、多样化、个性化的服务需求。如今的微脉已成为中国最大的全病程管理服务平台，先后获得元璟、源码、经纬、千骥、IDG、百度等一线基金数亿美元投资。作为一家深耕于打造具有中国特色管理式医疗组织（C-MCO）的独角兽企业，目前服务已覆盖全国30个省份，合作医院超2500家，累计服务超10亿人次，近20万名医生在微脉上提供20000余种医疗健康服务SKU。</p><p>&nbsp;</p><p>自2017年以来，互联网医疗领域经历了显著的变化，多种商业模式逐渐收敛为两大主要方向：一是线上售药，二是线上问诊。而微脉却创新性地提出了全病程管理的概念，并专注于联合公立医院开展“以患者为中心”的诊后、术后、检后及院后医疗健康管理。</p><p>&nbsp;</p><p>微脉CEO 裘加林在接受InfoQ采访时透露，之所以选择全病程管理这一赛道，是因为他们观察到彼时这一领域还是一片荒芜。而在患者健康管理、周期性诊疗方面拥有多年经验积累的微脉正好可以弥补这一市场空白。</p><p>&nbsp;</p><p>在众多企业依托先进的技术扎进拥挤的线上售药、线上诊疗、线上挂号等领域时，微脉走了一条差异化发展之路。</p><p>&nbsp;</p><p>全病程管理，在前期执行起来并不难，因为患者在疾病治疗期间已经与医生建立了信任关系，为后续的健康管理奠定了基础，但对患者进行病后健康管理并不是件容易事。</p><p>&nbsp;</p><p>据裘加林介绍，“病后健康管理”的难点并不在于缺乏管理知识或方法，而是缺乏有效的供给和履约能力。传统的健康管理方式需要专业的医生和护士参与，但医疗资源的有限性限制了其大规模应用。AI技术的引入，为解决这一问题提供了可能。AI能够基于现有医学知识，为患者提供个性化的健康管理方案，但关键在于如何将这些方案有效落地执行。</p><p>&nbsp;</p><p></p><h3>AIGC在微脉的落地应用</h3><p></p><p></p><p>裘加林首先从服务类型的角度将这些方案进行了分类，即搜索品、体验品和信任品。搜索品是标准化的产品，用户主要基于价格选择；体验品则需要用户实际体验后才能判断好坏；而医疗服务则属于信任品，用户无法仅凭体验或价格来评判，更多的是基于信任选择。</p><p>&nbsp;</p><p>在信任品属性主导的医疗市场中，即使AI技术已达到可行和可用的阶段，要实现广泛应用仍需较长时间，因为信任的建立需要时间。因此，裘加林认为AI在医疗领域的应用不应仅局限于优化或替代现有存量服务，而应更多地聚焦于创造新的增量服务，通过创新来满足未被满足的需求。</p><p>&nbsp;</p><p>微脉在尝试用AI进行全面健康管理时，正是遵循了这一思路。他们并不寻求替代医生或护士的工作，而是希望通过AI辅助医护人员为患者提供出院后的延续性健康管理服。例如，许多患者在出院后仍有康复需求，但现有的医疗服务往往只关注到出院这一环节，而忽略了后续的康复过程。AI可以作为患者的健康代理人，提供个性化的康复指导和健康管理服务，从而延伸和保障医疗服务的质量。</p><p>这种增量服务的模式不仅不会与现有医疗服务产生冲突，反而能够提升医疗服务的整体效能和患者满意度。同时，由于医疗服务的供给相对有限，AI的介入可以有效扩大医疗服务的覆盖范围，满足更多患者的需求。</p><p>&nbsp;</p><p>在效率和效果方面，虽然具体数据因应用场景而异，但总体而言，AI在医疗领域的应用能够显著提高服务效率，减少人为错误，并为患者提供更加个性化和精准的健康管理方案。</p><p>&nbsp;</p><p>更具体来讲，目前微脉将AI技术，尤其是生成式AI技术应用于五个业务场景中：分别是To C的智能助手CareAI、To B的应答辅助、面向专业健管师的全病程管理方案辅助设计、用户标签健康档案维护以及临床数据研究的智能分析。具体场景案例有以下几方面：</p><p>&nbsp;</p><p>第一个方面，CareAI整合超大规模医学及个案管理数据库，在真实的医疗服务场景中，微脉通过与公立医院合作，共建患者健康档案，在其公众号和患者管理工具中，加入智能健康助手——CareAI，充分发挥其健康管理价值，根据对患者的有效交互内容分析，提供文字、图片、视频等多形态的健康建议。</p><p>&nbsp;</p><p>第二个应用场景是嵌套在了企业办公工具侧边栏，作为问答辅助工具。经过系统化的训练，CareAI能够辅助健康管家、个案管理师对患者常问问题进行答疑，或对当下患者管理服务路径进行任务提示，提升管理服务效率的同时，提高患者体验，降低企业培训成本。</p><p>&nbsp;</p><p>第三个应用场景是在指定新病种新治疗方式的管理方案时，结合微脉精心设计的提示词模板，可以输出管理方案初稿，并且可以不断细化迭代方案，减轻个案管理师的工作量。</p><p>&nbsp;</p><p>基于这些AI应用，微脉在内部效率和患者管理效率上实现了显著提升。裘加林透露：“以前一个个案管理师同时期可管理50-70人，现在这一数字跃升至约500名患者。”</p><p>&nbsp;</p><p>第四个应用场景对于微脉现在所管理的用户提问、提交的图片等信息进行自动数据分析，动态更新健康档案，智能推送医院专科咨询链接或管理服务，实现千人千面的个性化健康管理与精准营销。</p><p>&nbsp;</p><p>最后，微脉AI支持医院或科室的横向课题合作，对沉淀的临床数据进行智能分析和多因素判定，为科研工作提供了强有力的数据支持。</p><p>&nbsp;</p><p>裘加林坦言：“在传统诊疗框架内，患者一旦离开医院，医院便难以维系持续的服务链，受限于时间与空间的限制。而今，CareAI的引入正逐渐扭转这一现状，它无缝连接了医院与患者，跨越了时空界限，构建了一条基于‘信任’的长久纽带。不仅确保了患者能够享受到持续的咨询与个性化精准服务，还实现了其健康档案的实时动态管理。这一变革让医院不仅能够‘认识’患者，更拥有了对患者的‘长期记忆’，在面对紧急情况时，能够迅速响应，提供高效的紧急援助。”</p><p>&nbsp;</p><p>据悉，微脉的CareAI平台已成功携手多家国内顶尖公立医院，共同构建了一套高效协同的医疗服务体系。这一体系旨在为患者打造从预防、咨询、预约到康复的全链条智能化健康管理体验，让患者就医有规划、离院有指导、问诊更精准、入院更流畅。“我们致力于摒弃传统的‘找熟人’模式，迈向一个智能化、高效化、个性化的健康管理新时代。”裘加林满怀信心地展望道。</p><p>&nbsp;</p><p></p><h3>AIGC虽然强大，但仍无法取代医生</h3><p></p><p>&nbsp;</p><p>微脉的AIGC部署实践充分证明了生成式AI在特定场景中的应用不仅是可行的，而且是可用的。但裘加林坦言，AI技术虽然很强大，但其在诊断等核心医疗环节上，虽然可行，但尚未达到完全替代医生的阶段。</p><p>&nbsp;</p><p>因为就生成式AI技术目前的发展来看，它是有能力边界的。这种边界主要取决于其大模型学习和微调的能力。随着AI技术的不断发展，许多大公司已经具备了强大的AI能力。在此基础上，外界更多的是关注如何控制输出的质量，解决AI可能产生的“幻觉”问题，让大模型的输出结果能够达到人们的预期。</p><p>&nbsp;</p><p>为了实现这一目标，微脉采用了多种技术手段，如将大模型与ReRank技术相结合、大模型的嵌套等。通过这些方法，可以针对具体患者的详细情况，如指征、数据、病症和病史等，进行精准的输出控制。当输入足够详细和准确的样本时，AI的输出结果可以是非常精准的，这也就避免了泛化或“幻化”的问题。</p><p>&nbsp;</p><p>在微脉的管理方案中，生成式AI并不是孤立地存在的，而是与个案管理师和医生等人工审核环节相结合，形成了一个金字塔模型。在这个模型中，AI主要承担预处理和初步分析的工作，而医疗助理和医生则对AI的输出进行复核和确认。这样的设置不仅提高了工作效率，还保证了结果的准确性和可靠性。</p><p>&nbsp;</p><p>借助AI快速处理大量数据和信息的能力，为个案管理师和医生提供初步的筛选和分类。医疗助理则根据AI的输出结果进行进一步的审核和整理，确保信息的准确性和完整性。最后，医生会对所有信息进行综合评估，并给出最终的诊断和治疗建议。在这个过程中，每一层级的工作量都在逐步缩减，但整体的工作效率和质量都得到了显著提升。</p><p>&nbsp;</p><p>但不可否认的是，看似无所不能的生成式AI技术，却也有其鞭长莫及时候。</p><p>&nbsp;</p><p>裘加林称：“AI从可行到可用，从可用到都用，前者是质变，后者是量变，质变是技术，量变是观念，技术迭代不难，观念转变需要时间。因此在推广AI在医疗健康服务场景的应用过程中仍面临多方面的挑战”。从GPT开始，AI带给了产业革命性的创新机遇，如诺奖得主Edmund Phelps所言，创新的成功需要四大要素：创新的能力，创新的动力，相应的法律法规支持和对失败的容忍；对于AI在医疗健康领域的应用尤其如此，在创新的能力上，技术人才的短缺是一个不可忽视的问题，AI技术的研发和应用需要高水平的技术团队支持，而这类人才在市场上的竞争非常激烈；创新的动力上，医疗机构和医务人员相对比较保守，对新技术的接受需要时间，特别是对失败的容忍上，医疗健康行业要求精准和严肃，容不得初创产品的边实践边迭代模式，这也对AI产品从可用到都用带来很大挑战。</p><p>&nbsp;</p><p>与单纯通过AIGC研发实现技术变现的企业不同，微脉通过将CareAI直接嵌入到成熟的专科专病全病程管理路径中，来辅助公立医院为患者提供连续性健康管理服务。这一模式对于区域性医疗机构来说，能够有效地将区域内的患者管理好、服务好，最终实现留住优质病源，提升核心竞争力。而微脉也因CareAI的应用实现降本增效，建立起独属于自己的“护城河”。</p><p>&nbsp;</p><p>裘加林的见解深刻地揭示了AI技术在医疗健康服务中的变革力量，以及其如何重塑医疗行业的新生态。微脉作为这一领域的探索者和先行者，正不断推进AI技术的边界，为患者带来更高质量、更人性化的医疗服务。</p><p><img src="https://static001.geekbang.org/infoq/fe/fe5f1416fda58af439ebc8183581d799.jpeg" /></p><p>（图源：微脉云谷中心）</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/xs6m7D3v8HxAsGdsjfGp</id>
            <title>蔚来汽车、哔哩哔哩、京东、携程携手为你分享大模型行业应用踩坑经验 ｜AICon</title>
            <link>https://www.infoq.cn/article/xs6m7D3v8HxAsGdsjfGp</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/xs6m7D3v8HxAsGdsjfGp</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jul 2024 09:37:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: LLM, 大模型场景, 行业应用, 人工智能
<br>
<br>
总结: 在当今快速发展的科技时代，LLM 在处理自然语言理解和生成方面展现出了惊人的能力，为各行各业带来了革命性的变化。大模型场景和行业应用论坛将探讨大语言模型在智能座舱、智能客服、酒店业务以及 B 端营销场景中的应用和实践，为企业提供前所未有的机遇和挑战。 </div>
                        <hr>
                    
                    <p>在当今快速发展的科技时代，LLM 在处理自然语言理解和生成方面展现出了惊人的能力，为各行各业带来了革命性的变化。这些模型不仅在提升用户体验、优化客户服务、增强产品功能等方面发挥着关键作用，同时也为企业提供了前所未有的机遇和挑战。</p><p></p><p>面对这样的趋势，我们在即将举办的<a href="https://aicon.infoq.cn/202408/shanghai/"> AICon 全球人工智能开发与应用大会</a>"（上海站）上，策划了“大模型场景 + 行业应用落地论坛”。论坛特别邀请了阅文集团技术副总经理兼 AIGC 负责人陈炜于担任专题出品人，协助甄选优质话题。</p><p></p><p>陈炜于目前主导阅文集团的人工智能技术研发和应用落地，带领团队研发了阅文妙笔大模型，并在作家辅助创作、角色对话和机器翻译等应用场景中取得了显著成果。</p><p></p><p>在本论坛，我们有幸邀请到了四位行业专家，他们将分别从不同角度，深入探讨大语言模型在智能座舱、智能客服、酒店业务以及 B 端营销场景中的应用和实践。期待你来一起交流。以下是详细介绍：</p><p></p><p></p><h5>精彩推荐一：</h5><p></p><p></p><p>现在 PMF 这一概念有点火热，它指的是产品市场匹配度，如果你想了解下这一方面，或许可以听听蔚来汽车人工智能研发负责人 &amp; 高级总监高杰的分享。</p><p></p><p>高杰拥有有 20 年语⾳处理、⾃然语⾔处理和机器学习的相关⼯作经验。现任蔚来座舱⼈⼯智能研发负责⼈。历任腾讯搜索部⻔语⾳研究员，负责语⾳搜索研发⼯作；曾任微软 STC 语⾳科学家，负责基于分布式计算平台的超⼤规模语⾳识别模型训练系统，语⾳助⼿Cortana 研发⼯作；</p><p></p><p>他将以《大模型在智能座舱中的应用》为主题为你展开分享。通过他的分享，你可以了解到 Agent 原生的架构设计以及 Agent 的大规模落地经验， 也可以了解到如何通过情感智能技术，提升助手的互动体验，使其不仅能够理解用户的需求，更能感知用户的情感状态，从而提供更人性化的服务。</p><p></p><p></p><h5>精彩推荐二：</h5><p></p><p></p><p>大语言模型在对话式交互中具有显著的天然优势，它们展现出了在智能客服领域的巨大应用潜力。然而，这些模型也面临着一些挑战，例如缺乏特定领域的深入知识，以及可能产生误导性信息的问题。如果你想了解这方面的探索实践，欢迎听下哔哩哔哩资深算法工程师冯璠的分享！</p><p></p><p>冯璠在推荐搜索、人机对话系统、AIGC 等领域有丰富的研究与实践经验，目前专注于 B 站大模型对话能力建设及大模型在智能客服的业务应用落地。她将以《哔哩哔哩大模型智能客服创新落地探索》为主题，介绍大语言模型对话式交互在智能客服中的天然优势及应用潜力，分享他们如何通过 RAG 结合领域知识，显著提升意图理解准确性和用户情绪感应能力，从而打造出高效、智能化、优质的交互体验。</p><p></p><p>通过她的分享，你将了解到大模型在智能客服中的新范式、落地挑战和难点，以及具体实践中的创新思路和技术解法。</p><p></p><p>冯璠还将探讨未来的发展方向，分享她们的实战经验和用户真实反馈。这是一次难得的机会，让你深入了解大模型结合业务知识的常见问题、解法及未来趋势。</p><p></p><p></p><h5>精彩推荐三：</h5><p></p><p></p><p>在大模型的实际应用中，我们可能需要充分了解大模型的能力边界，合理拆解复杂问题，才能获得良好的应用效果。</p><p></p><p>我们为你邀请到了携程酒店研发部算法专家李彦达，他在携程参与过多个重要项目，包括房型名称多语言翻译和智能商务服务，目前主要研究如何用大语言模型解决携程酒店的业务问题。他将以《大模型在携程酒店业务中的应用》 为主题，为你详细解读大语言模型在企业级应用中的实际表现和局限。</p><p></p><p>在这次分享中，李彦达将通过携程的两个具体案例，展示如何利用大语言模型解决实际业务挑战。你将了解到在大模型的加持下，房型名称多语言翻译覆盖率如何大幅提升，以及商户服务效率的显著提高。李彦达还将探讨大语言模型的发展与能力边界，深入解析这些应用案例，帮助你理解大模型的能力边界，并探索如何通过合理的问题拆解和应用策略来最大化其价值。</p><p></p><p>通过他的分享，你将学到如何用大语言模型技术解决企业中的复杂业务场景，获取宝贵的实战经验和洞见。</p><p></p><p></p><h5>精彩推荐四：</h5><p></p><p></p><p>在当今的商业环境中，企业正面临着前所未有的挑战和机遇。如何有效利用先进的技术，尤其是大语言模型，来提升营销效率和客户体验，已成为众多企业关注的焦点。</p><p></p><p>京东物流作为行业的先行者，已经在这一领域取得了显著的进展。我们为你邀请到了京东物流算法总监陈兰欢，他在推荐、大模型、NLP 对话等算法技术方面拥有 10 多年经验，现负责京东物流的算法总监。他将以**《大模型在京东物流 B 端营销场景落地应用》**为主题展开分享。</p><p></p><p>陈兰欢拥有超过十年的算法研发经验，特别是在推荐系统、大模型和自然语言处理对话技术方面。他的演讲深入探讨了大语言模型在企业营销应用中的落地和局限，并通过展示如何有效利用 RAG、COT、Prompt 工程、微调、Agent 等大模型技术和框架解决实际业务挑战，同时结合京东物流沉淀的亿级的营销对话语料，分享如何借助大模型大幅减少电销以及销售线下获客的时间和成本投入。</p><p></p><p>通过他的分享学习到如何用大语言模型技术进行企业营销。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b3/b365657ea727bcf6cf7fa5292e1748c0.jpeg" /></p><p></p><p></p><h5>活动推荐：</h5><p></p><p></p><p>InfoQ 将于 8 月 18 日至 19 日在上海举办 AICon 全球人工智能开发与应用大会，汇聚顶尖企业专家，深入端侧 AI、大模型训练、安全实践、RAG 应用、多模态创新等前沿话题。现在大会已开始正式报名，详情可联系票务经理 13269078023 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/2e/2e7902b3dcbcd1a3d526249ea92cb872.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/QQvgNt8NUP5LvyL29iMX</id>
            <title>面壁智能正式推出“智能体互联网” IoA：将异构智能体“孤岛”连接成完整大陆</title>
            <link>https://www.infoq.cn/article/QQvgNt8NUP5LvyL29iMX</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/QQvgNt8NUP5LvyL29iMX</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jul 2024 08:15:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型驱动, 面壁智能, Internet of Agents, 智能体互联网
<br>
<br>
总结: 当前，面临着由大模型驱动的智能体在全球迅猛发展的趋势。面壁智能提出了IoA智能体互联网的概念，通过解决多智能体协作的三重限制，实现了智能体之间的大规模连接协作。IoA平台创新包含四大核心机制，为多任务测试带来明显效果，推动了智能体之间的灵活高效协作。 </div>
                        <hr>
                    
                    <p>当前，由大模型驱动、在广泛任务上实现接近人类表现的自主智能体，正在全球各地迅猛发展。正如互联网把全世界所有信息和人连接在一起，物联网把所有设备连接在一起，一个统一的智能体平台把散落在世界各地的智能体连接起来，面壁智能从去年就开始预见，Internet of Agents （IoA）智能体互联网的趋势。</p><p></p><p>从万物互联进阶"万物智联"。现在，面壁智能跨过了异构智能体之间连接、沟通、高效协作存在的沟壑，正式迈出了导向 IoA 未来的第一步，并且从实际效果看，已可窥见异构智能体之间大规模连接协作的“威力”。</p><p></p><p>➤ &nbsp;IoA 论文地址：🔗 <a href="https://arxiv.org/abs/2407.07061">https://arxiv.org/abs/2407.07061</a>"</p><p></p><p>➤ &nbsp;IoA 开源地址：🔗 <a href="https://github.com/OpenBMB/IoA">https://github.com/OpenBMB/IoA</a>"</p><p></p><p></p><h3>IoA 诞生背景，击破多智能体协作的三重限制</h3><p></p><p></p><p>融合了大模型能力，具有感知记忆、自主规划、调用工具、执行任务能力的 Agent，被称为智能体。这些智能体可能有不同的架构、运行于不同的设备、有不同的能力，同时在数量和功能上飞速演进，但目前单个智能体更多处于“孤岛”的相对隔离状态，智能体之间的互相发现、大规模自由协作，还没有先例。之前，多智能体协作的“工作流”（workflow)，尽管已经显示了巨大的应用潜力，却依然存在着三重限制：</p><p></p><p>只允许接入框架内部定义的智能体。大多数多智能体系统在一个设备上模拟多个智能体。现实场景更迫切的需求，是分布在多个设备和位置的智能体通过“网络”进行协作。大多数多智能体系统的沟通机制单一，或者需要用户进行指定。沟通和多轮的信息交换，非常的不灵活。</p><p></p><p>为了跨过这些障碍，面壁智能联合清华大学 NLP 实验室，正式推出了 LLM 驱动的智能体互联网（Internet of Agents, IoA），这是一个受互联网启发的智能体通信和协作通用框架。简单来说，IoA 创建了一个可以自由注册、互相发现的 Agent 协作平台，并且让智能体之间协作再向上构建，对原来 Agent 协作工作流（Work Flow）进行三个方向扩容，跨设备、更多异质 Agent 开放互联、协作组织方式高度灵活，从而在更高维度上形成智能体互联网 Internet of Agents（IoA）。IoA 一经发布，也获得了全球范围内开发者的持续关注和讨论。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/6c/6c76670f5974b5fee909b00adf234ac5.png" /></p><p></p><p></p><h3>IoA 属于平台创新，包含四大核心机制，多任务测试效果明显</h3><p></p><p></p><p>loA 为异构智能体的协作提供了一个灵活且高效的平台。loA 本质通过引入一个能够集成不同第三方智能体的协议，以及类似即时通讯应用的框架来促进智能体在平台上发现其他智能体并动态组队。IoA 的核心由两个主要组件组成：服务器和客户端。服务器作为中心枢纽，管理智能体注册、发现和消息路由，确保具备不同能力的智能体能够互相发现并发起通信。客户端则作为单个智能体的包装，提供必要的通信功能，并适应指定的沟通协议。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/26/268f09bc215a5899dc0626c40a185869.png" /></p><p></p><p>loA 对于现有 Agent 智能体协作“工作流”和应用方式有三项重要突破：</p><p></p><p>Agent 互联载体，从单设备到多设备</p><p></p><p>大多数多智能体系统在一个设备上模拟多个智能体，这与现实场景相悖。IoA 支持分布在多个设备和位置的智能体通过网络进行协作。</p><p></p><p>在开放性上，从限定框架的“局域网”到自由身份注册的“互联网”</p><p></p><p>大多数多智能体系统只允许接入框架内部定义的智能体，而 IoA 允许开发者通过为现有智能体实现一个 adapter 接入到 IoA 的客户端中并注册到 IoA 服务端，扩展系统内智能体的能力多样性。</p><p></p><p>沟通、协作、组队上，从固定计划到灵活高效</p><p></p><p>大多数多智能体系统的沟通机制单一，或者需要用户进行指定。IoA 将沟通阶段抽象为 5 个阶段：讨论、同步任务分配、异步任务分配、暂停等待任务完成、总结，通过有限状态自动机实现了自主会话流程控制机制，允许智能体根据任务需求和进展自适应调整协作策略。</p><p></p><p>loA 的突破，主要得益于四大核心机制的建立：</p><p></p><p></p><h5>机制一，智能体注册与发现</h5><p></p><p></p><p>为了实现分布式的异构智能体协作，我们借鉴了即时通信软件中用户可以进行注册并被其他用户通过关键词搜索到的机制，提出了智能体注册与发现机制。</p><p></p><p>智能体注册：当一个新智能体加入 IoA 时，他所属的客户端需要向服务器发送注册请求。在注册中，我们要求智能体应提供其能力、技能和专长领域的详细描述。这些描述被存储在服务器的数据层中的智能体仓库模块中。智能体发现：智能体发现功能利用存储在智能体仓库模块中的信息，使智能体可以为特定任务找到合适的协作者。当一个智能体需要组建团队或寻求帮助时，它可以使用服务器的智能体查询模块进行搜索。通过匹配搜索条件和智能体描述，确保相关能力的智能体能够被发现。</p><p></p><p></p><h4>机制二，自主嵌套团队组建</h4><p></p><p></p><p>自主嵌套团队组建机制实现了根据任务需求动态灵活地组合合适的智能体。该机制允许智能体根据任务需求自适应地组建团队，并为复杂、多方面的任务创建嵌套子团队。</p><p></p><p>团队组建过程：当一个智能体被分配任务时，它可以使用服务器提供的智能体查询功能寻找合适的协作者。一旦找到合适的智能体，它会发起一个新的群组聊天，形成团队。嵌套组队：嵌套组队允许形成团队和子团队的层级结构。在任务执行过程中，如果智能体被分配了任务，且他识别到需要额外的专长，它可以再次搜索合适的智能体，并发起一个新的子群组聊天，从而形成树状的团队结构。如下图所示。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/77/77454a0231574f4943374a1eb4a39ab4.png" /></p><p></p><p>嵌套组队机制</p><p></p><p></p><h4>机制三，自主会话流程控制</h4><p></p><p></p><p>有效的通信对于成功的协作至关重要。受言语行为理论（Speech Act Theory）启发，我们在 IoA 中引入了自主会话流程控制机制。该机制使智能体能够协调其通信，并保持结构化对话，提升协作的效率和效果。</p><p></p><p>顺序发言机制：为管理潜在的冲突并确保清晰的沟通，IoA 采用了基本的顺序发言机制。在任何给定时间内，只有一个智能体可以发言，防止混乱并保持通信顺序。尽管简单，但当搭配上下面的有限状态机，仍可以构成灵活但相对可控的自主对话流程。群组聊天状态的有限状态机：如下图所示，我们将会话流程形式化为一个有限状态机，每个状态对应协作过程的不同阶段。通过状态转换，智能体能够根据任务需求和进展灵活调整会话状态。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/9c/9c4905e1cbaaf1f11ce5a62e0f665efe.png" /></p><p></p><p>完成任务过程的有限状态转移示意图</p><p></p><p>通过实现这些关键机制，IoA 实现了智能体之间的结构化、高效的通信和协作。这种方法允许智能体根据协作需求动态调整，促进在复杂多智能体场景中的更有效的问题解决和决策。</p><p></p><p>机制四，任务分配与执行</p><p></p><p>任务分配与执行机制旨在高效地在智能体之间分配工作，并管理简单和复杂任务的执行。该机制与团队组建和会话流程控制机制协同工作，确保协作和任务完成。</p><p></p><p>任务分配：任务分配在群组沟通中进行，分为同步任务分配和异步任务分配两种类型。同步任务分配暂停群组聊天直到分配的任务完成，而异步任务分配不打断正在进行的讨论，允许任务并行执行。任务执行：任务分配后，负责的智能体开始执行。执行过程取决于任务的性质和智能体的能力。集成的第三方智能体通过客户端的智能体集成模块处理任务执行。</p><p></p><p>通过集成任务分配与执行与团队组建和会话流程控制机制，IoA 提供了一种灵活和高效的方法来管理复杂的多智能体协作。该方法允许动态任务分解、专门智能体分配和协调执行，使系统能够有效地解决各种问题。</p><p></p><p>IoA 与其他智能体协作“工作流”关键特性对比；</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/6a/6a482158486ec5cfe710d87ae95cfb41.jpeg" /></p><p></p><p>为了展示 IoA 在整合异构智能体方面的有效性，loA 在多种任务上进行了全面的实验。这些实验旨在展示智能体异质性的不同方面，包括工具多样性、架构多样性、不同的观察和动作空间，以及不同的知识基础。</p><p></p><p></p><h4>异构工具：GAIA 基准测试</h4><p></p><p></p><p>GAIA 是 Meta 提出的一个 Agent 能力 benchmark，包含需要推理能力、网页浏览、代码计算等多方面能力的多样化任务。通过仅接入最基础的 4 个 ReAct 智能体（分别配备有网页浏览器、代码解释器、wikidata 查询工具以及 Youtube 字幕下载器），IoA 在 GAIA 基准测试中的表现显著优于现有方法。尽管仅使用了基本的 ReAct 智能体，IoA 在整体性能上仍然达到最高，并在需要高级推理和复杂协作的更高难度级别中表现尤为突出。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/69/691a02c71ae47342f1dce7c8fa5986f4.png" /></p><p></p><p>（图）GAIA 结果表现</p><p></p><p></p><h4>异构工具：开放式指令基准测试</h4><p></p><p></p><p>GAIA 中大部分包含的是问答题，为了评测 IoA 在更为现实的开放式问题下的表现，我们通过 self-instruct 的方式构建了一个涵盖代码、数学、生活助手以及搜索报告四类任务共 150 条数据。并在 IoA 中接入了 AutoGPT 与 Open Interpreter——两个最知名的智能体——通过 GPT-4 对 IoA 的输出与 AutoGPT、Open Interpreter 两者的输出分别进行对比。</p><p></p><p>实验结果显示，IoA 在协调 AutoGPT 和 Open Interpreter 的协作方面表现卓越，显著优于单独使用这些智能体。IoA 在所有四个任务类别中均表现出色，相比于 AutoGPT 与 Open Interpreter 来说，总体胜率分别为 76.5% 和 63.4%。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a0/a0b7e4e7341c14064771c4332ff05719.png" /></p><p></p><p>（图）接入了 AutoGPT + Open Interpreter 的 IoA 与两者分别的对比</p><p></p><p></p><h4>异构观察和动作空间：具身智能体任务</h4><p></p><p></p><p>为了考察 IoA 在智能体所处环境与动作空间不同的情况下，能否使智能体高效协同完成任务，我们在 Rocobench 上进行了测试，这是一个虚拟具身的 benchmark，在每个任务中，两个或三个具身智能体需要通过沟通协作完成共同的目标。</p><p></p><p>我们将 IoA 与两个基准进行了对比：Central Plan 和 Roco Dialog。结果显示，IoA 在具身 AI 任务中表现出色，成功率显著高于专为此任务设计的 Roco Dialog 框架。在多个任务中，IoA 的成功率甚至超过了拥有完整环境可见性的 Central Plan 基准。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a8/a837aa40fe6ee60f571f82f9a051e27f.png" /></p><p></p><p>（图）具身任务中，IoA 与其他基准的对比</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/05/0560685f2f54236924e6827e818adc59.png" /></p><p></p><p>（图）IoA 完成 Rocobench 任务</p><p></p><p></p><h4>异构知识：检索增强生成</h4><p></p><p></p><p>在多智能体系统中，一个常见情况是不同的智能体具备不同的知识，例如挂载了不同的知识库，或是在不同的领域数据集上进行过训练。为了观察在知识异构的情况下 IoA 是否能够使得不同智能体有效沟通并完成回答，我们提出在 RAG 问答场景下进行测试，我们设置了三种场景：</p><p></p><p>2 个异构智能体场景：分别能够访问 Google 和 Wikipedia2 个同构智能体：两者都能访问两个知识源，用于作为 IoA 在信息完备情况下的对比实验3 个同构智能体：三者都能访问两个知识源，用于衡量 IoA 在知识冗余情况下是否仍有可扩展性</p><p></p><p>实验结果显示，基于 GPT 3.5 的 IoA 在所有数据集上的 RAG 表现能够达到或超越单个 GPT-4 的表现，同时在异构知识场景下，IoA 的表现也较为出色，在两个数据集上超过了之前一个同构的多智能体 RAG 框架。同时，IoA 的同构表现也体现了 IoA 有着较高的能力上限。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/dd/dd49a993607d9d3eeb528438bc8be039.png" /></p><p></p><p>（图）IoA 在 RAG 任务上与其他基线框架的对比</p><p></p><p></p><h3>IoA 的未来远景：异质 Agent 大规模协作成主流，全面变革生活和生产方式，导向未知的“智慧爆炸”</h3><p></p><p></p><p>“智能体互联网”IoA 的灵感，由研究积累迸发，也来源于在自然界。智人作为个体已经拥有非凡智慧，深入实践、彼此充分交流信息、分工协作，带来了各种工具和发明层出不穷，以及自然科学的诞生。某种程度上，现代文明也是智人在“世界网络”交互的结果。</p><p></p><p>同样的道理，目前正在快速发展、散落在全球各地的异质 Agent 智能体连接起来，loA 的诞生将在未来产生何种巨大的影响？</p><p></p><p>首先，在 IoA 上，你可以发现更多更丰富更强大的 Agent，loA 的远景就像互联网是一个看不到边界的数据、信息和资源宝库，处在不断的膨胀、扩充、能力边界延展之中，loA 作为一个 Agent 存在、协作、涌动的海洋，不断地扩充如今 Agent 的能力边界，未来，很可能每个人都会主动或被动的参与其中。</p><p></p><p>其次，Agent 智能体从个体智能，真正迈向群体智能。IoA 创造一个智能体可以互相发现、自由交互的开放空间。以 IoA 为纽带，在万物都是 Agent 的未来，每个物品都通过 Agent 技术内置了对自己功能的智能化理解，这些理解通过互联网联系起来，最终引发人类生活方式的全面变革。</p><p></p><p>最后，由高度智慧的个体进行群体协作，会“涌现”什么，长期变化更是值得探索。当前已有的 Agent 协作网络，仅仅针对某些特定任务效果更好，大规模异质 Agents 协作，从促进单个 Agents 的能力演变、强化到 Agents 网络 IoA 上互相交互。从已知到未知，loA 可能正通向某个未知领域，在未来引发“智慧大爆炸。</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Vgp6Bokh0FqEOcxvOyqI</id>
            <title>智能体是金融AI创新的“敲门砖”吗？</title>
            <link>https://www.infoq.cn/article/Vgp6Bokh0FqEOcxvOyqI</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Vgp6Bokh0FqEOcxvOyqI</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jul 2024 03:11:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 金融行业, 智能体技术, 大模型, AI Agent
<br>
<br>
总结: 金融行业正加速智能化转型，智能体技术和大模型等新技术被广泛应用。智能体技术为金融机构提供了更简单、高效的智能化解决方案，推动行业转型。智能体技术在金融领域展现出独特的应用价值，帮助提供个性化、高效的服务。金融智能体的发展需要依赖智能模型、强大算力和完善工具平台。 </div>
                        <hr>
                    
                    <p>金融行业正在进入智能化转型的加速期，大模型、Agent 等技术也开始被应用在金融业务场景中。但在技术落地的过程中存在着许多实际问题：大模型到底该如何应用于业务之中，又能够如何清晰直接的解决行业问题？</p><p></p><p>智能体技术或许是一个切口，它不仅能够让金融机构以较小的成本“尝鲜”大模型，还能帮助企业探索技术和场景的最佳匹配，从而自上而下的激发金融行业的智能化转型。</p><p></p><p>为了探索智能体技术在金融场景中的实践，7 月，火山引擎携手 AI 应用开发平台扣子、英伟达、凤凰网财经频道、InfoQ 联合举办「金融大模型城市环游智能体专场」，不仅邀请了多位行业大咖到场“智话金融”，还吸引了数百位参会者参与扣子“动手实验营”。</p><p></p><p></p><h2>智能体崛起，引领金融智能化转型新浪潮</h2><p></p><p></p><p>“大模型建设不只是场景那么简单，从算力到模型到应用，它是一个体系化的工程。”在两站“智话金融”分享环节，火山引擎金融行业解决方案负责人王建军分享了自己的观点，他认为，智能体将是一个重要入口，而工具则是非常重要的一环，“今天的 AI 创新比以前容易太多了，在扣子这样的平台，上面有可被快速调用的大模型，并且能通过自然语言的方式跟它交互，能够帮助我们更快应用 AI。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/13/137635901dfc02c87e800a31cf5fefc5.webp" /></p><p></p><p>推动智能体技术的长效发展，需要依赖于更智能的模型、更强大的算力和更完善的工具和平台，火山引擎金融行业解决方案总监周思霁在分享中提到，扣子平台拥有低门槛、个性化、实时数据查询和多模态交互等能力，使得智能体的构建和应用变得更加简单和高效。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7d/7d0023bc7008ec150fdaecd6282ae812.webp" /></p><p></p><p>“ AI Agent 很有可能是通往通用人工智能的必经之路。”清华大学电子工程系长聘教授、教育部长江学者特聘教授、博士生导师谷源涛博士则进一步强调了智能体技术在人工智能演进过程中的重要性。尤其是生成式 AI 的引入，进一步拓宽了智能体的应用范围，当金融行业海量的知识数据填入 AI Agent 与大模型的深度学习，智能涌现也将会同步发生。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ff77db197154459ca466353696fcbde9.webp" /></p><p></p><p>基础设施层的努力同样重要。在分享中，NVIDIA 的两位技术专家 Joey Zhang、申意介绍了企业落地大模型的部署应用。NVIDIA NIM 是 NVIDIA AI Enterprise 的一部分，是一套易于使用的预构建容器工具，目的是帮助企业加速生成式 AI 的部署。它支持各种 AI 模型，可确保利用行业标准 API 在本地或云端进行无缝、可扩展的 AI 推理。</p><p></p><p>金融智能体的核心问题仍旧是落地与应用。</p><p></p><p>在财富管理、风险控制、客户服务等金融领域，智能体技术已经开始展现出其独特的应用价值。“总结内容，总结数据，找数据，码字，最后再成文，这个过程人类不可能用两分钟解决，但是通过现在智能体加上编排平台，两分钟就能编写一篇三五千字的报道”，况客科技管理合伙人兼首席产品官安嘉晨认为，通过智能体的自然语言处理能力和大数据分析能力，金融机构能够提供更加个性化和高效的服务，同时降低运营成本和提高决策质量。</p><p></p><p>如何利用智能体帮助企业经营、助力日常工作，首届扣子 Hackathon 的两位优胜者分享了他们的洞察。小成功 AI 孵化器主理人邓稳分享了他在小店运营中的 Bot 应用，他搭建的“PUA 助理（Planning 想到、Understanding 看到、Action 做到）”，真正帮助他在企业经营中更省心、放心、舒心。</p><p></p><p>元禾辰坤金谷资本执行董事 黄铄宁则从医药投资的角度分享了她的金融 Bot，可以实时有效的助力她了解行业讯息，并提供投资建议。在他们的日常工作中，智能体已经成为不可或缺的一部分。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5a/5ac27714947a858001f69422042b6105.webp" /></p><p></p><p></p><h2>自由开发，开拓金融智能体无限想象</h2><p></p><p></p><p>在智能体专场的动手实验营中，近三百位金融从业者参与到了金融 Bot 的搭建中，进一步探索了金融智能体在多元场景中的应用。在多种多样的金融细分领域以及业务工作的各个环节，金融 Bot 已经可以实现智能交互、业务辅助，甚至能够全流程的提供智能客户服务。</p><p></p><p>深圳站冠军队伍、来自平安保险的“i 人狂喜”团队制作的金融 Bot“让 i 人狂喜的保险规划师”，可以做到一站式咨询、销售、服务。从用户投保前中后整个流程，车险和非车险两大业务模式两大方向展开设计，为整个 Bot 打造通用的业务技能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/60/60e5fa3eae38e9e9381fb1399a100571.webp" /></p><p></p><p>对于业务工作流的加成更能体现智能体的超强能力。北京站优胜队伍，来自中泰证券的“律政先锋”团队打造的“合规案例编写 Bot”，可以结合已有的处罚案例以及相应犯规，帮助证券从业者实时编写合规的案例，在第一步解决风险。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4f/4ff5399b7c97145fb801c1c138f9275e.webp" /></p><p></p><p>除了冠军队伍之外，动手实验营还诞生了许多充满着奇思妙想的 Bot。例如关注儿童理财的金融 Bot“钱多多”，遗产管理相关的金融 Bot“遗产管理与咨询”，基于对客户的洞察的金融 Bot“客户基金情绪洞察”。有些队伍会更聚焦金融业务工作中的重要环节，比如“金融法规小助手”“合同检测助手”“催收质检大师”这样的工具型金融 Bot。</p><p></p><p>有些 Bot 则将创意性、趣味性拉满，比如深圳站的“大 A 心理按摩师”，可以就股市情况提供相应心理辅导；北京站参与路演的 Bot“你，贫穷吗？”可以回答金融、宇宙、财富相关的哲学问题，趣味性十足，引发现场的阵阵讨论。</p><p></p><p><img src="https://static001.geekbang.org/infoq/40/4039846e955bcfcda8aabf610c988e03.webp" /></p><p></p><p></p><h2>走向未来，迎接智能金融时代</h2><p></p><p></p><p>到底什么才是“金融 Bot”？</p><p></p><p>在以往很多从业者的认知中，金融 Bot 的主要应用场景是智能客服、问答机器人。在火山引擎看来，金融 Bot 的概念应该被进一步拓宽，金融行业的各个环节都可以融入智能体技术，Bot 不仅可以提升用户体验，还可以解决金融业务问题的，比如金融业务工作流的优化、金融知识数据管理等。</p><p></p><p>金融 Bot 也能为行业创新场景应用带来更多的思考，银行场景下的本地生活、保险行业的智能核赔、基金行业的智能理财顾问..... 基于金融行业的多元场景，智能体技术或许可以覆盖到更多角落，从而推动业务革新与体验升级，进而带动行业的发展。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e7/e72f231b7cee05c19ffa520a0983a7aa.webp" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/f1/f18a267043adea754875a43eafc6cf00.webp" /></p><p></p><p>火山引擎金融大模型将走过更多城市，金融智能体的演进也将继续。</p><p></p><p>火山引擎金融大模型城市环游·智能体专场北京站</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ljUQSoy19lH1EawhFbI3</id>
            <title>Windows 全球宕机造成百亿损失，肇事者却仅给出 10 美元赔偿？ 微软 Azure CTO 借机力推 Rust 上位！</title>
            <link>https://www.infoq.cn/article/ljUQSoy19lH1EawhFbI3</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ljUQSoy19lH1EawhFbI3</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jul 2024 09:34:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: CrowdStrike, 配置更新, 全球灾难, 道歉礼品卡
<br>
<br>
总结: 上周，网络安全公司 CrowdStrike 因一次配置更新出错，导致全球数百万台采用 Windows 系统的计算机崩溃。CrowdStrike 向合作伙伴发送道歉邮件，并提供Uber Eats礼品卡作为补偿。这一事件引发全球灾难，影响航空、银行和医疗保健等多个行业。CrowdStrike 的错误更新可能导致全球经济损失达到150亿美元左右。CrowdStrike 的道歉举措引发了争议和讨论，同时也引发了关于编程语言选择的讨论。CrowdStrike 事件敲响了警钟，引发了对 Rust 是否优于 C/C++ 的讨论。 </div>
                        <hr>
                    
                    <p>上周，网络安全公司&nbsp;CrowdStrike因一次配置更新出错，导致全球数百万台采用&nbsp;Windows系统的计算机崩溃。此番宕机被广泛视为有史以来影响最大的灾难，导致阿姆斯特丹、柏林、迪拜、伦敦和美国各地的机场航班延误，还导致数家医院停止手术，全球无数企业陷入瘫痪。</p><p></p><p>日前，据几位消息人士透露，他们收到了&nbsp;CrowdStrike&nbsp;发来的一封电子邮件，该公司将向其合作伙伴提供一张Uber&nbsp;Eats&nbsp;礼品卡作为道歉，因为其认识到了“7&nbsp;月&nbsp;19&nbsp;日事故所带来的额外工作”。</p><p></p><p>根据消息人士分享的截图，邮件中写道：“为此，我们衷心感谢并对给您带来的不便表示歉意……为了表达我们的感激之情，您的下一杯咖啡或夜宵由我们请客！”其他人也在&nbsp;X&nbsp;上发布了同一封邮件。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d5/d54f54b8bb9254cfa24ce94850535a1a.png" /></p><p></p><p>7&nbsp;月&nbsp;19&nbsp;日事件发生后&nbsp;CrowdStrike&nbsp;向合作伙伴发送的电子邮件截图。</p><p></p><p>该电子邮件是由&nbsp;CrowdStrike&nbsp;的一个电子邮件地址以该公司首席商务官丹Daniel&nbsp;Bernard的名义发送的。根据&nbsp;X&nbsp;上的一篇帖子，在英国，这张代金券价值&nbsp;7.75&nbsp;英镑，按今天的汇率约合&nbsp;10&nbsp;美元。</p><p></p><p>一些发布礼品卡帖子的人表示，当他们去兑换优惠券时，收到了一条错误消息，称礼品卡“已被发行方取消，不再有效”。但&nbsp;CrowdStrike&nbsp;发言人&nbsp;Kevin&nbsp;Benacci&nbsp;向媒体证实该公司确实发送了礼品卡。</p><p></p><p><img src="https://static001.geekbang.org/infoq/35/3514018e5d4ac9614c8c79e7291ee055.png" /></p><p></p><p>“我们确实将这些发送给了一直在帮助客户渡过难关的队友和合作伙伴。Uber&nbsp;将其标记为欺诈行为，因为使用率很高，”Benacci&nbsp;在一封电子邮件中说道。</p><p></p><p>“CrowdStrike&nbsp;的所有人都明白此事的严重性和影响。”CrowdStrike&nbsp;还公布了其首席执行官George&nbsp;Kurtz以及首席安全官Shawn&nbsp;Henry的道歉信。&nbsp;Henry在领英上写道：“让你们失望了，对此我深感抱歉。”</p><p></p><p>Kurtz&nbsp;在公司网站上发布的一条消息中说道，“没有什么比我们的客户和合作伙伴对&nbsp;CrowdStrike&nbsp;的信任和信心更重要。在解决这一事件的过程中，我承诺将完全透明地说明事情发生的原因以及我们为防止类似事件再次发生而采取的措施。”</p><p></p><p>一名网友打趣道：“CrowdStrike&nbsp;以‘我错了’这种理由向所有人发放&nbsp;UberEats&nbsp;积分，这太&nbsp;Z&nbsp;世代了。”还有一些人嘲笑道，&nbsp;CrowdStrike&nbsp;给出的赔偿数目仅仅“够开一场披萨派对！”</p><p></p><h1>CrowdStrike&nbsp;造成了多少损失</h1><p></p><p>此次&nbsp;CrowdStrike&nbsp;安全软件的错误更新引发的整个故障事件，扰乱了全球的互联网服务，影响了航空、银行和医疗保健等众多行业。</p><p></p><p>据保险公司&nbsp;Parametrix&nbsp;称，银行和医疗保健行业以及主要航空公司预计将受到最严重的打击，全球经济损失总计可能达到&nbsp;150&nbsp;亿美元左右。</p><p></p><p>但根据网络安全公司的条款和条件，CrowdStrike&nbsp;除了简单的退款外，无需支付任何费用，其&nbsp;Falcon&nbsp;安全软件（全球各地的公司和政府机构都在使用）的条款将责任限制在“已支付的费用”内。 这意味着，如果一家公司向&nbsp;CrowdStrike&nbsp;索赔其业务损失或收入，那么它最多能收回的只是它向&nbsp;CrowdStrike&nbsp;支付的金额。</p><p></p><p>为承担处理&nbsp;CrowdStrike&nbsp;故障后果所产生的所有费用——包括雇用&nbsp;IT&nbsp;人员安装另一个更新来修复&nbsp;Windows&nbsp;机器上的问题、员工生产力损失、为客户解决问题以及需要向投资者提交相关证券报告的上市公司可能产生的法律费用，大多数公司将不得不求助于网络保险公司。</p><p></p><p>Parametrix&nbsp;在一份声明中表示，全球保险损失总计可能达到&nbsp;15-30&nbsp;亿美元左右，其中给财富&nbsp;500&nbsp;强公司的总保险损失可能在&nbsp;5.4&nbsp;亿美元至&nbsp;10.8&nbsp;亿美元之间。</p><p></p><p>据悉，现在一些受此次网络故障影响的公司已经在向保险公司寻求赔偿。全球最大的保险经纪公司&nbsp;Marsh&nbsp;的一位高管表示，在此次网络安全危机发生后，已有超过&nbsp;75&nbsp;名客户准备提出索赔。</p><p></p><p>需要注意的是，对于提出索赔的公司来说，赔偿金不会立即到账，企业可能无法收回因网络中断而损失的资金。网络保险风险平台&nbsp;Cyberwrite&nbsp;首席执行官&nbsp;Nir&nbsp;Perry&nbsp;表示，某些网络保险政策包括对非恶意事件的承保，受影响的企业在提出索赔之前必须考虑某些变量，例如免赔额和等待期。</p><p></p><h1>宕机事件敲响警钟：Rust优于C/C++ ？</h1><p></p><p>微软宕机事件发生后，微软Azure部门CTO&nbsp;Mark&nbsp;Russinovich&nbsp;提醒开发者应当关注更好的编码实践，借此提高系统可靠性，最终降低系统崩溃和发生蓝屏死机的可能性。</p><p></p><p>上周六，Russinovich转了一条发布于2022年的推文，称“是时候停止在任何新项目中使用C/C++了，而且在一切非GC（垃圾回收）语言场景下都应使用Rust。出于安全和可靠性的考虑，业界应该正式宣布弃用C/C++这类语言。”虽然没有实证，但人们猜测这条推文应该是跟CrowdStriek引爆全球的更新错误有所关联。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2d/2d16e00134f0be8ed79b1956682fb4b2.jpeg" /></p><p></p><p>引发蓝屏死机的原因多种多样，包括内存错误、驱动程序问题和Windows中的进程问题等等，而这一切都依赖于用C/C++编写的内核。曾在谷歌工作的程序员Zack&nbsp;Vorhies表示，此次中断就是由C/C++代码错误所造成。但谷歌研究员Tavis&nbsp;Ormandy驳斥了这种说法。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/ea6fd2601c9ff28e187b22d3e321307a.jpeg" /></p><p></p><p>Vorhies将大规模宕机归咎于空指针，即代码中指向无效内存位置的特定行。根据他的说法，“空指针来自不具备内存安全特性的C++语言”。Ormandy对Vorhies的观点予以驳斥，CrowdStrike方面则回应称“这与Channel&nbsp;File&nbsp;291或者任何其他Channel&nbsp;File中包含的空字节无关。”</p><p></p><h2>微软的Rust应用史</h2><p></p><p>多年以来，微软一直对Rust表示支持，而且也不断在内部推动代码迁移工作。但该公司也很清楚，从C/C++迁往Rust的工程绝不可能一蹴而就。 Russinovich在最近一条推文中表示，“我们正在努力。Azure中已经包含不少Rust代码，Windows中也有Rust的成果存在。” 微软面向Rust的迁移方法也是经过认真规划的：第一步是创建原型应用程序，证明Rust代码与Windows系统的兼容性。此外，微软还陆续将保护系统硬件的外围应用程序迁移至Rust。</p><p></p><p>据了解，微软在Rust工具开发方面投入了约1000万美元。</p><p></p><h3>Azure中的Rust</h3><p></p><p>Azure&nbsp;作为首选应用目标，微软在其Azure云中广泛应用Rust语言。该公司正在部署一套使用Rust编写的虚拟机管理器，用以管理Azure环境中的Hyper-V。 Rust还在Azure&nbsp;Boost中得到应用，Weston称其为“Azure的未来架构”。 “我们将陆续把Azure主机上的更多性能密集型负载移交至专用卡（例如智能网卡及/或FPGA）来运行。”该公司还希望为Rust建立一套类似于Linux操作系统的长期支持版本。</p><p></p><h3>运用Rust保护硬件设备</h3><p></p><p>微软企业和操作系统安全副总裁Dave&nbsp;Weston表示，微软正在部署基于Rust构建的安全固件实现保护效果的自主开发硬件。</p><p></p><p>该公司的Secured-core计划包括为Surface和Windows&nbsp;PC提供稳定且安全的启动环境。微软方面已经将大量组件从C语言转换为Rust，借此增强系统稳定性并降低系统漏洞暴露在黑客面前的可能性。</p><p></p><p>微软正围绕Rust为其Surface硬件打造安全启动模块。UEFI（统一可扩展固件接口）中包含从系统启动到运行Windows操作系统的固件代码。UEFI代码通常位于主板之上，并在计算机启动的同时接受访问。</p><p></p><p>UEFI固件会被加载在内存当中，而Rust负责提供内存安全机制，以防止系统崩溃或遭到利用。以往不少硬件漏洞和安全问题都与计算机内存有着莫大关联。</p><p></p><p>美国政府下辖的主要安全机构——网络安全与基础设施安全局（CISA）就在去年12月呼吁企业改用内存安全技术。CISA在咨询报告中表示，“除了C/C++之外，大多数现代编程语言都已经具备内存安全属性。内存安全编程语言能够管理计算机内存，确保程序员无法引入内存安全漏洞。”</p><p></p><p>微软还为其安全处理器Pluton开发了一套完全由Rust编写的实时操作系统。Pluton包含一个可信平台模块（TPM），用于存储生物特征数据等关键安全信息。</p><p></p><p>Weston解释称，“微软致力于通过设计切实提高安全水平。这也是我们内部原研安全处理器，而没有坐等行业发展所带来的优势之一。我们将转向Rust……这种语言在安全领域相较传统原生语言有着巨大优势。”</p><p></p><p>参考链接： https://thenewstack.io/microsofts-it-outage-reminder-rust-is-better-than-c-c/</p><p></p><p><a href="https://techcrunch.com/2024/07/24/crowdstrike-offers-a-10-apology-gift-card-to-say-sorry-for-outage/">https://techcrunch.com/2024/07/24/crowdstrike-offers-a-10-apology-gift-card-to-say-sorry-for-outage/</a>"</p><p></p><p><a href="https://www.reuters.com/technology/fortune-500-firms-see-54-bln-crowdstrike-losses-says-insurer-parametrix-2024-07-24/">https://www.reuters.com/technology/fortune-500-firms-see-54-bln-crowdstrike-losses-says-insurer-parametrix-2024-07-24/</a>"</p><p></p><p>https://www.businessinsider.com/businesses-claiming-losses-crowdstrike-outage-insurance-billions-losses-cyber-policies-2024-7</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/GsQawujbvLiyPOFNSg1c</id>
            <title>人力、资金成本大幅下降，最早上车Agent的企业已经开始获益</title>
            <link>https://www.infoq.cn/article/GsQawujbvLiyPOFNSg1c</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/GsQawujbvLiyPOFNSg1c</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jul 2024 09:16:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Agent, AI, OpenAI, 软件生产
<br>
<br>
总结: Agent作为AI技术的一种应用，被认为是新一波人工智能技术浪潮中最先落地的应用之一。各大公司和个人都在积极探索AI Agent的商业化应用，以提高软件生产效率和降低成本。AI Agent具备独立思考、规划和执行任务的能力，可以在各个领域提供帮助，对软件行业和社会产生深远影响。通过AI Agent的应用，企业可以实现数字化生产力的提升，员工能力得到最大化提升，带来更高的企业价值。 </div>
                        <hr>
                    
                    <p>Agent太火了！在生成式AI浪潮之后，Agent被广泛称为是“这一波浪潮中最先落地”的应用。“至少有100个项目正致力于将AI代理商业化，近10万名开发人员正在构建自主Agent。”外媒MattSchlicht曾这样表示。</p><p></p><p>2023年下半年，OpenAI联合创始人，前TeslaAI总监Andrej&nbsp;Karpathy说道：“如果一篇论文提出了某种不同的训练方法，OpenAI内部会嗤之以鼻，认为都是我们玩剩下的。但是当新的AI&nbsp;Agents论文出来的时候，我们会十分认真且兴奋地讨论。普通人、创业者和极客在构建AI&nbsp;Agents方面相比OpenAI这样的公司更有优势。”</p><p></p><p>微软创始人比尔盖茨，也通过个人网站发表了对AI&nbsp;Agent的看法：AI&nbsp;Agent将成为下一个平台，简而言之，AI&nbsp;Agent几乎将在任何活动和生活领域提供帮助，对软件行业和社会产生深远的影响。</p><p></p><p>在InfoQ对<a href="https://www.infoq.cn/video/MC4vmsZemRk0LXC7QqVj?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">数势科技创始人兼CEO黎科峰博士</a>"的采访中，他也透露其公司已经通过Agent赚到了钱，而数势科技是国内最早一批躬身入局的企业。</p><p></p><p>与大语言模型相比，<a href="https://aicon.infoq.cn/2024/shanghai/track/1707">AI&nbsp;Agent</a>"具备通过独立思考、调用工具去逐步完成给定目标的能力。同时，它能让AIGC技术拥有感知、记忆、规划和行动能力，可以跨应用程序做复杂任务的执行。有网友曾给出过一个很好的比喻：“大语言模型只能编个贪吃蛇，而AI&nbsp;Agent可以整出一个王者荣耀。”</p><p></p><p>成熟的AI&nbsp;Agent可以使软件生产大幅降低成本。目前任何一家行业巨头动辄上万甚至十万级员工，有了AI&nbsp;Agent之后研发、交付需要耗费的人力和资金将大幅降低，软件也可以灵活地解决更多长尾需求。AI&nbsp;Agent实现了员工与数字生产力的协作，每个员工都可以有自己的数字助力协作工作，每个员工的能力最大化提升，最终带来企业价值，让企业真正意义上步入数字化生产力时代。</p><p></p><p>毫无疑问，AI&nbsp;Agent正在引领新一波人工智能技术浪潮，相关人才的市场价也在水涨船高。在招聘网站输入“Agent”，可以看到平均年薪基本在60-65万人民币以上，且招聘需求相对旺盛。如果企业或者个人对Agent有兴趣，希望通过此来解决一些问题，可以考虑参与InfoQ&nbsp;x&nbsp;极客时间&nbsp;8月17日在上海举办的【AI大模型实战特训营】。</p><p></p><p>时隔多年，我们的线下内训终于再度回归！首期，我们邀请到了谷歌开发者专家、LangChain开发者、谷歌出海创业加速器导师，同时也是多个畅销训练营的主讲专家彭靖田手把手教学，带着大家一起学习<a href="https://aicon.infoq.cn/2024/shanghai/training/6085">LangChain、Agent、RAG</a>"的相关内容，并且可以通过案例实训让大家快速掌握相关知识，结束之后可以直接将所学内容运用到工作场景中（PS：扫描最后一张海报的二维码可以咨询小助手了解价格及相关内容），直接购票请<a href="https://aicon.infoq.cn/2024/shanghai/apply">点击此处</a>"。</p><p></p><p><img src="https://static001.geekbang.org/infoq/37/373124325e880cd658d25cbd9aadec48.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/0a/0a4633de30387124fe29d90987bcab8f.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/26/26e0988888f76da96199b15fccd669fa.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>