<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/qUf8YNP73YrduKXUbv6O</id>
            <title>【鸿蒙生态学堂】HarmonyOS应用上架</title>
            <link>https://www.infoq.cn/article/qUf8YNP73YrduKXUbv6O</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/qUf8YNP73YrduKXUbv6O</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Sep 2024 07:42:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/8a/8aa9b9d0245f29d66ae4e453358d220e.png" /></p><p>点击图片开始学习</p><p></p><p>课程简介：本课程将指导开发者了解HarmonyOS应用上架的全流程，包括应用的全网发布、分阶段发布和测试发布策略。课程将详细解读上架标准，介绍华为提供的测试工具，帮助开发者进行预审能力检测和隐私托管，确保应用符合上架要求，优化发布流程。</p><p></p><p>课程标签：全网发布、分阶段发布、测试发布、预审能力、隐私托管</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/B3YDkF7t0FGI8K7HtGQ7</id>
            <title>【鸿蒙生态学堂】HarmonyOS应用测试</title>
            <link>https://www.infoq.cn/article/B3YDkF7t0FGI8K7HtGQ7</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/B3YDkF7t0FGI8K7HtGQ7</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Sep 2024 07:40:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/21/21f0622af35a22f4a2f46e59ab8af78d.png" /></p><p>点击图片开始学习</p><p></p><p>课程简介：本课程专注于HarmonyOS应用测试，旨在帮助开发者掌握应用测试的标准和实践。课程将详细解读HarmonyOS应用测试标准，介绍多种测试工具，包括DevEco Testing，以及如何针对典型场景问题进行有效的测试。通过演示测试工具的使用，本课程将指导开发者如何实施性能测试、兼容性测试、稳定性测试和安全测试，确保应用在HarmonyOS平台上的优质体验。</p><p></p><p>课程标签：标准解读、测试工具介绍、典型场景问题、测试工具演示</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Gk6slCPdc126Vq12P8c2</id>
            <title>【鸿蒙生态学堂】并发能力最佳实践</title>
            <link>https://www.infoq.cn/article/Gk6slCPdc126Vq12P8c2</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Gk6slCPdc126Vq12P8c2</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Sep 2024 07:37:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/a5/a5e384ea65d866b29383fafb06732ff3.png" /></p><p>点击图片开始学习</p><p></p><p>课程简介：本课程将深入探讨HarmonyOS的并发能力，特别是FFRT并发编程模型。您将学习如何设计高效的应用并发架构，识别并解决启动缓慢问题，提高应用的冷启动速度。课程还将涵盖使用HTTP访问网络资源的方法，以及用户首选项的详细介绍，包括如何按需加载优化、并发优化、IPC优化和代码逻辑优化，以提升应用性能和用户体验。</p><p></p><p>课程标签：应用并发设计、FFRT并发编程模型</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/tORqfSLQV3vO9Kpigv32</id>
            <title>【鸿蒙生态学堂】ArkUI性能优化、丢帧分析、响应优化</title>
            <link>https://www.infoq.cn/article/tORqfSLQV3vO9Kpigv32</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/tORqfSLQV3vO9Kpigv32</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Sep 2024 07:35:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/6c/6c394a3ccd16ca13d47e2bde9930e6fe.png" /></p><p>点击图片开始学习</p><p></p><p>课程简介：本课程将深入探讨HarmonyOS的ArkUI框架，提供全面的UI性能优化指南。您将学习到如何通过ArkUI框架进行高效UI开发，并掌握常见的性能优化措施，包括丢帧问题的原理分析和优化技巧。课程将涵盖UI优化、按需加载、并发处理、IPC通信优化以及代码逻辑优化，同时探讨如何提升视觉感知流畅度，确保用户界面既快速又吸引人。</p><p></p><p>课程标签：ArkUI框架基本介绍、ArkUI常见性能优化措施、丢帧问题原理、并发优化、IPC优化、代码逻辑优化、视觉感知优化</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ibR0aR25obK6gJbOUk2u</id>
            <title>【鸿蒙生态学堂】冷启动优化、合理使用动画、长列表加载性能优化最佳实践</title>
            <link>https://www.infoq.cn/article/ibR0aR25obK6gJbOUk2u</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ibR0aR25obK6gJbOUk2u</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Sep 2024 07:25:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/69/690d8feb7bf52fad55af9668d1c2e0d0.png" /></p><p>点击图片开始学习</p><p></p><p>课程简介：本课程深入探讨HarmonyOS应用的冷启动优化技巧，从应用冷启动概述到具体实施策略，提供全面的优化方案。课程内容包括合理使用动画提升用户感知流畅度、数据驱动UI更新机制、以及长列表加载性能优化的最佳实践。你将学习到如何通过懒加载、缓存列表项、组件复用和布局优化等技术手段，有效提高冷启动速度，减少用户等待时间，从而打造更流畅、更高效的HarmonyOS应用体验。</p><p></p><p>课程标签：应用冷启动概述、应用冷启动流程、识别启动缓慢问题、提高冷启动速度、提升动画感知流畅度、提高动画运行流畅度、懒加载、缓存列表项、组件复用、布局优化</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/3YphPsjvJRLj3bJq0OeW</id>
            <title>【鸿蒙生态学堂】网络和数据存储</title>
            <link>https://www.infoq.cn/article/3YphPsjvJRLj3bJq0OeW</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/3YphPsjvJRLj3bJq0OeW</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Sep 2024 07:20:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/03/0360867e265ad191df5f57a0280da0a0.png" /></p><p>点击图片开始学习</p><p></p><p>课程简介：本课程深入探讨HarmonyOS中的网络和数据存储管理，特别是使用HTTP协议访问网络资源和用户首选项的详细介绍。您将学习如何在HarmonyOS应用中发起HTTP请求，处理响应数据，以及如何利用用户首选项进行轻量级的数据持久化存储。课程将通过实例演示如何高效地管理应用配置和用户偏好设置。</p><p></p><p>课程标签：使用HTTP访问网络、用户首选项介绍</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/MyTWLkCw3X0jOzxWrXe9</id>
            <title>【鸿蒙生态学堂】ArkUI开发基础（下）</title>
            <link>https://www.infoq.cn/article/MyTWLkCw3X0jOzxWrXe9</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/MyTWLkCw3X0jOzxWrXe9</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Sep 2024 07:16:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/c0/c0581545100841fd18bc37b1fc91337a.png" /></p><p>点击图片开始学习</p><p></p><p>课程简介：本课程深入探讨HarmonyOS的ArkUI框架，特别是数据驱动UI更新和组件导航的高级概念。您将学习如何使用ArkWeb技术构建动态网页内容，掌握数据绑定技巧以确保UI与底层数据源同步更新。此外，课程将指导您通过设置组件导航来增强应用的用户体验，实现流畅的页面过渡和有效的用户交互。</p><p></p><p>课程标签：使用ArkWeb构建页面、数据驱动UI更新、设置组件导航</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/WHbR2WJYF838obHYXLyh</id>
            <title>【鸿蒙生态学堂】ArkUI开发基础（上）</title>
            <link>https://www.infoq.cn/article/WHbR2WJYF838obHYXLyh</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/WHbR2WJYF838obHYXLyh</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Sep 2024 07:05:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/96/96a0dde74dbf275f70e0b75ad8450384.png" /></p><p>点击图片开始学习</p><p></p><p>课程简介：本课程将介绍HarmonyOS的ArkUI框架，包括其基础语法和如何使用常用组件构建页面。ArkUI是HarmonyOS应用的UI开发框架，提供简洁的UI语法、丰富的组件和实时界面预览工具。您将学习到ArkUI的关键特性，如极简的UI信息语法、丰富的内置UI组件、多维度的状态管理机制，以及如何支持多设备开发</p><p>。通过课程，您将能够掌握使用ArkUI框架进行高效UI开发的技能。</p><p></p><p>课程标签：ArkUI（方舟UI框架）介绍、使用常用组件构建页面</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/4XT0Gfx8l9zTBUDJSuBS</id>
            <title>【鸿蒙生态学堂】应用程序框架基础</title>
            <link>https://www.infoq.cn/article/4XT0Gfx8l9zTBUDJSuBS</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/4XT0Gfx8l9zTBUDJSuBS</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Sep 2024 06:59:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/28/2869c81a9b66ff42426be19b52c85b08.png" /></p><p>点击图片开始学习</p><p></p><p>课程简介：本课程将带领开发者深入了解HarmonyOS的应用程序框架基础，重点探讨UIAbility组件的工作原理和生命周期管理。通过学习，开发者将能够掌握如何在HarmonyOS中创建和使用UIAbility组件，包括其启动模式和窗口管理。同时，课程还将介绍DevEco Studio工具的使用，它是专为HarmonyOS应用开发设计的IDE，支持代码编写、调试和应用构建等功能，助力开发者高效开发HarmonyOS应用。</p><p></p><p>课程标签：应用程序框架基础、UIAbility组件概述</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/OrSuYA1QfKR6yyy7Pva6</id>
            <title>【鸿蒙生态学堂】ArkTS语法介绍</title>
            <link>https://www.infoq.cn/article/OrSuYA1QfKR6yyy7Pva6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/OrSuYA1QfKR6yyy7Pva6</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Sep 2024 06:59:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/f6/f61766298b7fc02425ba371bf967f244.png" /></p><p>点击图片开始学习</p><p></p><p>课程简介：本课程将深入介绍HarmonyOS的应用开发语言——ArkTS。您将学习到ArkTS的基本语法，包括变量声明、类型系统、运算符等，以及如何使用ArkTS进行声明式UI开发。课程还将展示如何利用DevEco Studio这一强大的集成开发环境，进行代码编写、调试和应用构建，帮助您快速上手HarmonyOS应用开发。</p><p></p><p>课程标签：ArkTS基础语法、声明式UI语法</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/RfXt52pwFiBuPYdxTKeO</id>
            <title>【鸿蒙生态学堂】HarmonyOS介绍</title>
            <link>https://www.infoq.cn/article/RfXt52pwFiBuPYdxTKeO</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/RfXt52pwFiBuPYdxTKeO</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Sep 2024 06:53:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/d3/d3692253bc6573c8d4025458a49f9632.png" /></p><p>点击图片开始学习</p><p></p><p>课程简介：本课程将带您快速了解HarmonyOS，深入探讨HarmonyOS的核心特性，包括其分布式架构和跨设备能力。课程还将介绍华为提供的赋能套件，帮助开发者高效开发应用。最后，您将掌握DevEco Studio，HarmonyOS官方集成开发环境，用于构建、调试和部署应用。无论您是初学者还是有经验的开发者，本课程都将为您提供必要的工具和知识，让您在HarmonyOS平台上大展宏图。</p><p></p><p>课程标签：HarmonyOS简介、赋能套件和学习资源介绍、DevEco Studio的使用</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/uwjRymcH7rdDVxXi7WYR</id>
            <title>腾讯被曝隐藏专业职级，不希望以职级论英雄；要求加薪减工时、职位“世袭”？！印度三星离谱罢工；“拒绝跑步被辞退”当事人道歉 | Q资讯</title>
            <link>https://www.infoq.cn/article/uwjRymcH7rdDVxXi7WYR</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/uwjRymcH7rdDVxXi7WYR</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Sep 2024 06:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><blockquote>上交所启动全网测试；“管培生拒绝跑步被辞退”当事人道歉；“三只羊”录音系伪造，AI&nbsp;大模型成幕后黑手；阿里京东互相打通；淘宝可使用微信支付；OpenAI&nbsp;首席技术官宣布离职；印度三星工人罢工，要求“世袭”职位；英特尔拒绝&nbsp;Arm&nbsp;收购产品部门；游戏科学&nbsp;CEO&nbsp;冯骥谈《黑神话：悟空》DLC&nbsp;进度：让团队先&nbsp;“躺”两年；腾讯职级制度改革；英特尔“全公司的希望”：Intel&nbsp;18A&nbsp;芯片正式亮相！韩国电池巨头&nbsp;SK&nbsp;On&nbsp;将裁员以保持竞争力；OpenAI&nbsp;向所有付费的&nbsp;ChatGPT&nbsp;用户推出了语音助手服务；欧洲隐私机构&nbsp;noyb&nbsp;指控火狐&nbsp;Firefox&nbsp;浏览器；英特尔释出最新微码更新修正&nbsp;13/14&nbsp;代酷睿处理器崩溃问题……</blockquote><p></p><p></p><h2>科技公司</h2><p></p><p></p><h4>上交所启动全网测试</h4><p></p><p>据媒体报道，上交所定于2024&nbsp;年9月29日（周日），组织开展竞价、综业等平台相关业务测试。邀请全体市场参与人参加测试，主要验证相关技术平台业务与技术调整的准确性。测试模拟1个交易日的交易和清算，测试主要内容包括“验证连续竞价时段集中申报大量订单时，竞价平台业务处理平稳运行”等。</p><p></p><p>根据测试方案，参测市场参与人应以9月27日生产环境闭市后数据为基础，根据竞价、综业等其它平台业务规则和测试方案，准备股票、基金、大宗交易、ETF申赎等各类订单数据，以确保可以完成测试。</p><p></p><p>9&nbsp;月&nbsp;27&nbsp;日，上交所在官网发布公告称，当日开盘后，上交所股票竞价交易出现成交确认缓慢的异常情况，并导致交易受到影响。经处置，股票竞价交易于11点13分起逐步恢复。对于该异常情况的发生，上交所深表歉意。</p><p></p><h4>“管培生拒绝跑步被辞退”当事人道歉</h4><p></p><p>近日，一起“应届生因拒绝周末跑步活动遭辞退”事件在网络上持续发酵。</p><p></p><p>据9月22日《新闻晨报》报道，北京一网络技术有限公司应届生张先生因拒绝参加公司周末跑步活动，在入职43天后被辞退。</p><p></p><p>第一次活动是在8月21日晚9点，公司组织员工下班后进行5公里的户外跑步；第二次则安排在8月31日（周六）早上7点，要求员工到某公园集合进行10公里的户外跑步。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4c/4cce2df157a941334c66312ce486ecc2.webp" /></p><p></p><p>张先生称，由于连续12天高强度加班导致身体疲惫，他未能参加第二次跑步活动，随后被公司管理人员约谈劝退，理由是他缺乏主动性和管理潜力。张先生认为拒绝跑步是基于身体状况考虑，不应成为辞退理由。随后张先生提起劳动仲裁，并在社交媒体发帖，并迅速引起大量关注。</p><p></p><p><img src="https://static001.geekbang.org/infoq/27/270d178606319df090bdd70f7f3a4c87.webp" /></p><p></p><p>不过该事件在持续发酵数日后迎来新转折。9月22日，当事人发布道歉声明称，其于2024年9月19日在社交平台曾发表过自己在北京易点淘网络科技有限公司（易点云）的就职经历，导致易点云登上热搜。对于给易点云造成的不良影响向公司道歉。</p><p></p><h4>“三只羊”录音系伪造，AI&nbsp;大模型成幕后黑手</h4><p></p><p>9&nbsp;月&nbsp;27&nbsp;日凌晨，Reecho&nbsp;睿声公司官微发布声明称，近日，公司收到合肥警方调取证据通知书，针对网传三只羊“卢某录音门”事件所涉及的音频部分，经与合肥警方配合查实，确系嫌疑人王某使用公司自主研发的&nbsp;Reecho&nbsp;睿声&nbsp;AI&nbsp;配音大模型平台，由卢某此前直播片段约&nbsp;30&nbsp;秒录音进行克隆，并通过文本生成。</p><p></p><p><img src="https://static001.geekbang.org/infoq/64/641caeec1b7608cb7c761307393bd563.webp" /></p><p></p><p>据警方通报，经过侦查，9&nbsp;月&nbsp;22&nbsp;日晚，我局将犯罪嫌疑人王某某&nbsp;(男，25&nbsp;岁)&nbsp;抓获，并在其电脑、手机和制作&nbsp;AI&nbsp;音频的网站中发现伪造相关音视频的证据；结合其供述、调查取证，并经部、省专业机构检验鉴定，认定报案所涉网传音视频系伪造。</p><p></p><p>现已查明，9&nbsp;月&nbsp;16&nbsp;日，王某某利用从互联网下载的音视频资料，杜撰卢某某酒后言论脚本，先使用&nbsp;AI&nbsp;工具训练生成假冒卢某某的音频&nbsp;(其中出现的女声也系&nbsp;AI&nbsp;工具训练生成)，后用视频软件合成音视频，并通过网络发布，形成谣言大量传播。目前，王某某已被依法采取刑事强制措施，案件正在进一步侦办中。</p><p></p><h4>阿里京东互相打通，电商巨头“握手言和”！</h4><p></p><p>9&nbsp;月&nbsp;26&nbsp;日，据晚点&nbsp;LatePost&nbsp;消息，京东物流和菜鸟速递将分别接入淘天、京东平台，京东也将在“双&nbsp;11”前开通支付宝支付。据部分从业者透露，物流电子面单上的基础信息并非核心机密，接通端口也没有技术难题，企业更为在意的是与客户更深入的合作细节和商业数据。同时，一些电商代运营服务商也做好了平台与物流商的端口打通，为商家铺货做准备。</p><p></p><p>这也意味着淘天商家此后在系统中能够选择京东物流作为发货物流。与之相对应的是，京东也将菜鸟旗下的自营快递品牌菜鸟速递接入平台当中，并同时接入菜鸟旗下的包裹代收点菜鸟驿站。或许在今年“双&nbsp;11”，京东平台的商品信息就会出现在菜鸟当中。</p><p></p><p>要知道一开始京东是支持使用支付宝的，直到&nbsp;2011&nbsp;年&nbsp;5&nbsp;月&nbsp;8&nbsp;日，京东商城宣布停止使用支付宝作为支付方式之一。当时刘强东解释了停用支付宝的原因：一是支付宝的费率太高，每年京东都要因为支付宝的费率高而多支付&nbsp;500-600&nbsp;万元；二是京东&nbsp;80%&nbsp;都是货到付款，用在线银联支付很少约在&nbsp;10%&nbsp;左右，所以与支付宝停止合作不会给用户带来影响。从此之后，京东与阿里的关系便急转直下，火药味越来越浓。谁能想到&nbsp;13&nbsp;年后的今天，双方竟能重归于好，互通有无。</p><p></p><p>在双方物流合作达成一致的另一边，京东也将在“双&nbsp;11”前夕接入支付宝支付。从物流到支付，阿里跟京东，似乎真的做好准备“握手言和”了。尽管&nbsp;27&nbsp;日晚间，淘天集团和京东集团相关负责人都尚未向确证该消息，但淘宝和天猫的商家已经做好了准备接受这样的改变。随着各大平台的壁垒逐渐拆除，一个真正“互联”的互联网正离我们越来越近。</p><p></p><h4>官宣！淘宝可使用微信支付</h4><p></p><p>继&nbsp;26&nbsp;日阿里京东互相打通后，淘宝官方&nbsp;9&nbsp;月&nbsp;27&nbsp;日宣布，当日（27&nbsp;日）起，消费者逛淘宝买买买时，可以使用微信支付了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/22/22b10fd7e262c3e8322caa9f945d12d7.webp" /></p><p></p><p>从具体展示的页面看，消费者在完成商品选购后，在支付页面选择“微信支付”，即可完成支付，全程体验与其他支付工具一致。值得提醒的一点是，使用“微信支付”功能，需要更新至最新版的淘宝&nbsp;App。</p><p></p><p>9&nbsp;月&nbsp;5&nbsp;日，淘宝网发布公告，宣布新增微信支付能力：为提升消费者的购物体验，淘宝网计划新增微信支付能力，于本公告公示七天后逐步向所有淘宝网卖家开放。</p><p></p><p>基于上述服务的增加，淘宝网相应升级平台规则，主要变化为：支付服务商相关的名称、账户、余额、交易额等表述统一规范为“支付机构”“支付账户”“支付账户余额”“订单交易额”。</p><p></p><p>淘宝官方客服回应记者称，待平台商家微信支付能力开通完毕，消费端会自动生效。</p><p></p><h4>OpenAI&nbsp;首席技术官宣布离职，高层地震继续</h4><p></p><p>财联社&nbsp;9&nbsp;月&nbsp;26&nbsp;日讯，OpenAI&nbsp;高层变动继续，首席技术官&nbsp;Mira&nbsp;Murati&nbsp;周三表示，她将离开&nbsp;OpenAI。已经在该初创公司工作六年半的&nbsp;Murati，是&nbsp;OpenAI&nbsp;大模型的重要技术主管，而她的离开也让业界对&nbsp;OpenAI&nbsp;内部管理和权力变化更加好奇。</p><p></p><p>穆拉蒂在声明中提到这是一个艰难的决定，她的离职原因是：“因为我想腾出时间和空间来探索自己，目前，我的首要任务是尽自己所能确保顺利过渡，保持我们已经建立起来的势头。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/0d/0dd4858d47ea9c4bf57036245dfefb73.webp" /></p><p></p><p>▲穆拉蒂发布的离职声明</p><p></p><p>除了穆拉蒂离职外，OpenAI&nbsp;总裁格雷格·布罗克曼&nbsp;(Greg&nbsp;Brockman)&nbsp;也已休假。据报道，OpenAI&nbsp;计划重组，将取消非营利性董事会的控制权，CEO&nbsp;萨姆·阿尔特曼（Sam&nbsp;Altman）还将首次获得&nbsp;OpenAI&nbsp;股权。</p><p></p><p>阿尔特曼已在社交平台上回应穆拉蒂的声明。他说感谢穆拉蒂所做的一切，OpenAI&nbsp;将很快会详细介绍过渡计划。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fc/fc88bc2fadaa8be6a5572271068b3d0a.webp" /></p><p></p><p>▲阿尔特曼回复</p><p></p><p>这已经是今年从&nbsp;OpenAI&nbsp;离职的第&nbsp;11&nbsp;位高管。其中包括今年&nbsp;5&nbsp;月&nbsp;OpenAI&nbsp;联合创始人&nbsp;Ilya&nbsp;Sutskever&nbsp;和前安全负责人&nbsp;Jan&nbsp;Leike&nbsp;宣布离职，联合创始人&nbsp;John&nbsp;Schulman&nbsp;上个月离职并加入&nbsp;OpenAI&nbsp;的竞争对手&nbsp;Anthropic。</p><p></p><p>被曝出正在寻求巨额融资、估值达到&nbsp;1500&nbsp;亿美元的&nbsp;OpenAI，并没有减慢高管的离职步伐。本月中旬，OpenAI&nbsp;传出将以&nbsp;1500&nbsp;亿美元估值寻求&nbsp;65&nbsp;亿美元融资，知情人士称，Thrive&nbsp;Capital&nbsp;将领投此轮融资，计划投资&nbsp;10&nbsp;亿美元，老虎环球计划加入，并且微软、英伟达、苹果等头部玩家均在洽谈投资事宜。</p><p></p><h4>印度三星工人罢工，要求“世袭”职位</h4><p></p><p>9&nbsp;月&nbsp;26&nbsp;日，三星电子位于印度泰米尔纳德邦金奈附近的工厂已遭遇近三周的罢工，超过&nbsp;1000&nbsp;名工人走出岗位，搭建帐篷进行抗议。</p><p></p><p>这些工人的主要诉求包括将现有月薪（3&nbsp;万—3.5&nbsp;万卢比）提高&nbsp;25%—30%，并正式承认工会的权利。此外，在员工去世时，允许其家属“继承”岗位，以及为去世员工子女提供每年&nbsp;5&nbsp;万卢比的私立学校学费支持。</p><p></p><p>这场始于&nbsp;9&nbsp;月&nbsp;9&nbsp;日的罢工是近年来印度规模最大的工人抗议活动之一。尽管三星在印度支付的工资水平高出当地平均水平&nbsp;1.8&nbsp;倍，不过工人们依然认为现有薪资不足以满足他们的生活需求。</p><p></p><p>罢工导致工厂生产显著受阻，工会称生产中断已达&nbsp;70%。虽然三星表示，部分生产可通过非罢工工人和新雇员恢复至接近正常水平，但整体局势依然紧张。</p><p></p><p>印度联邦劳工部长曼苏克·曼达维亚已向泰米尔纳德邦政府发出信件，要求介入并寻求“友好”解决方案。</p><p></p><p>据悉，此次罢工不仅关乎三星的运营，在全球制造业转移的背景下，印度当前的劳动争议可能会削弱印度作为制造中心的吸引力。随着印度政府积极推动“印度制造”倡议，促进外资流入，此次罢工可能会直接影响到未来的投资意愿，除三星外，现代汽车等公司在印度当地也正面临类似挑战，这将对印度的整体制造环境与劳动力市场构成深远影响。</p><p></p><h4>英特尔拒绝&nbsp;Arm&nbsp;收购产品部门</h4><p></p><p>9&nbsp;月&nbsp;27&nbsp;日消息，据外媒报道，ARM&nbsp;曾接触英特尔，探讨可能收购这家陷入困境的芯片制造商的产品部门，但被告知该业务不出售。</p><p></p><p>据知情人士透露，ARM&nbsp;与英特尔进行了高层接触，但是对英特尔的制造业务没有展示出兴趣。英特尔目前有两个主要部门：一个是销售&nbsp;PC、服务器和网络设备芯片的产品部门，另一个是运营工厂的部门。</p><p></p><p>ARM&nbsp;的大部分收入来自销售智能手机芯片设计，但其&nbsp;CEO&nbsp;雷内·哈斯一直在寻求扩大业务范围，包括进军&nbsp;PC&nbsp;和服务器领域。在这一领域，ARM&nbsp;的芯片设计将与英特尔展开竞争。尽管英特尔不再拥有曾经的技术优势，但该公司仍在&nbsp;PC&nbsp;和服务器市场占据主导地位。对于&nbsp;ARM&nbsp;来说，与英特尔的合并将有助于拓展市场，推动其销售更多自有产品。</p><p></p><p>据了解，Arm&nbsp;公司的收入只是英特尔的一小部分,&nbsp;但其估值自去年首次公开募股以来飙升，目前市值超过&nbsp;1560&nbsp;亿美元。相比之下，英特尔今年的市值已损失超过一半，目前的市值为&nbsp;1023&nbsp;亿美元。</p><p></p><h4>游戏科学&nbsp;CEO&nbsp;冯骥谈《黑神话：悟空》DLC&nbsp;进度：让团队先&nbsp;“躺”两年</h4><p></p><p>9&nbsp;月&nbsp;27&nbsp;日下午，游戏科学&nbsp;CEO&nbsp;冯骥在回复网友评论&nbsp;“DLC&nbsp;在做了？”时表示：“咱就说，能不能让团队先躺两年？采采风，恋恋爱，尽情玩玩其他游戏。”</p><p></p><p>不久之前的&nbsp;9&nbsp;月&nbsp;21&nbsp;日，2024&nbsp;北京文化论坛文化产业投资人大会期间，游戏科学&nbsp;CEO&nbsp;冯骥及商务经理黄一帆确认，《黑神话：悟空》的&nbsp;DLC&nbsp;正在开发中。此前有消息称，新&nbsp;DLC&nbsp;将包括“再起”和“此去”两大篇章，涉及多个新角色和故事，将在&nbsp;2025&nbsp;年农历新年左右推出，但具体内容尚未官方证实。首位投资人吴旦预计游戏生命周期内销量可达&nbsp;3000&nbsp;万份，并对游戏未来充满信心。</p><p></p><h4>腾讯职级制度改革：隐藏职级，所有职级最短停留时间为&nbsp;1&nbsp;年</h4><p></p><p>9&nbsp;月&nbsp;27&nbsp;日，腾讯组织与人才发展部对内发布全员邮件，宣布对《腾讯员工职业发展管理制度》进行调整。邮件中声称，本次调整是&nbsp;2022&nbsp;年腾讯职级改革的延续。</p><p></p><p>从今日（27&nbsp;日）起，腾讯员工的专业职级信息将不在企业微信中进行公开展示。对于取消职级外显的理由，腾讯宣称是希望内部员工减少对职级的过度关注，不以职级论英雄，被“职级对等”之类的官僚陋习捆住手脚，提倡平等的职场文化。</p><p></p><p>此前腾讯内部已经有不少业务团队在进行试点，将组织架构中管理者的管理职级（总监、GM&nbsp;等）调整为“XX（业务&nbsp;/&nbsp;团队）负责人”的名称。</p><p></p><p>本次调整的另一个重要改变是，将目标职级为&nbsp;8&nbsp;级及以下的职级停留时间要求，从&nbsp;0.5&nbsp;年延长至&nbsp;1&nbsp;年，与其他职级保持一致。</p><p></p><p>与此同时，继续保持目标职级&nbsp;8&nbsp;级及以下的上下半年两次申报窗口，并放宽其“绿色通道申报”资格，即当次绩效为&nbsp;Outstanding（五星评定）即可申请。</p><p></p><p>在奖励机制方面，腾讯依然在内部保留了“特殊申报”通道，为最近一年做出“特殊贡献”但不符合“绿色通道申报”资格的员工，提供快速发展的机会。</p><p></p><p>为了保持内部组织架构中的人员流动性，在最新的《腾讯员工职业发展管理制度》中，还对公司员工跨通道&nbsp;/&nbsp;职位发展做出了新的规定。</p><p></p><p>其中，11&nbsp;级及以下的跨通道&nbsp;/&nbsp;职位发展，原则上转岗就转职业通道，不需要再做额外的评审申请。跨族群或者&nbsp;12&nbsp;级及以上的转通道申请，公司将与各个通道&nbsp;/&nbsp;族群沟通，尽量简化流程。</p><p></p><p>此外腾讯还将鼓励员工在管理路径和专业路径之间进行切换，也就是说，管理干部和专家这两个发展路径将有所打通。</p><p></p><h4>英特尔“全公司的希望”：Intel&nbsp;18A&nbsp;芯片正式亮相！</h4><p></p><p>9&nbsp;月&nbsp;25&nbsp;日消息，据&nbsp;Tom's&nbsp;Hardware&nbsp;报道，处理器大厂英特尔于上周在俄勒冈州波特兰市举行的&nbsp;Enterprise&nbsp;Tech&nbsp;Tour&nbsp;活动中，首次展示了其代号为&nbsp;Clearwater&nbsp;Forest&nbsp;的&nbsp;Xeon&nbsp;芯片，这也是英特尔首款最新的&nbsp;Intel&nbsp;18A&nbsp;制程芯片，不过该芯片可能需要等到明年下半年才能上市。</p><p></p><p>虽然英特尔在活动中推出了其最新的基于&nbsp;Intel&nbsp;3&nbsp;制程的&nbsp;Xeon&nbsp;6&nbsp;Granite&nbsp;Rapids&nbsp;数据中心芯片，这也自&nbsp;2017&nbsp;年&nbsp;AMD&nbsp;EPYC&nbsp;推出以来，英特尔首次将数据中心处理器的内核数量提升到与&nbsp;AMD&nbsp;的竞品相当的水平。但是如果要进一步拉开差距，仍需要寄希望于&nbsp;Intel&nbsp;18A&nbsp;制程的&nbsp;Clearwater&nbsp;Forest&nbsp;的&nbsp;Xeon&nbsp;芯片。</p><p></p><p>对于正处于财务危机当中的英特尔掌舵者帕特·基辛格（Pat&nbsp;Gelsinger）来说，接下来英特尔将在&nbsp;2025&nbsp;年上半年量产的&nbsp;Intel&nbsp;18A&nbsp;制程将会是其扭转乾坤的关键。不仅英特尔下一代的&nbsp;PC&nbsp;及数据中心处理器需要依靠&nbsp;Intel&nbsp;18A&nbsp;制程回归内部制造，并提升产品竞争力，同时英特尔也寄希望于&nbsp;Intel&nbsp;18A&nbsp;实现对于台积电&nbsp;2nm&nbsp;制程的超越，从而赢得更多的代工客户。</p><p></p><h4>韩国电池巨头&nbsp;SK&nbsp;On&nbsp;将裁员以保持竞争力，员工可选特殊休假&nbsp;/&nbsp;自愿离职</h4><p></p><p>9&nbsp;月&nbsp;26&nbsp;日消息，据韩联社、路透社报道，韩国能源巨头&nbsp;SK&nbsp;Innovation&nbsp;旗下的电池部门&nbsp;SK&nbsp;On&nbsp;今天发布声明称，公司计划推出“自愿离职”计划来削减员工人数，以努力提高效率，在充满挑战的电动汽车市场上保持竞争力。</p><p></p><p>SK&nbsp;On&nbsp;表示，作为“效率计划”的一部分，该公司拟提供特殊休假、自愿离职两种形式。“这些都是积极的措施，目的是建立一支精干、灵活的员工队伍，以便我们能够更好地驾驭不断变化的电动汽车市场环境。在公司努力提高效率并为可持续增长奠定基础的同时，我们将全力支持员工的职业发展，他们为我们成功成为顶级电池制造商作出了贡献。”</p><p></p><p>作为提高效率措施的一部分，SK&nbsp;On&nbsp;表示将向同意离职的员工提供自愿离职方案，包括向去年&nbsp;11&nbsp;月前加入公司的员工提供&nbsp;50%&nbsp;的工资，让他们提前退休。一份监管文件显示，截至今年&nbsp;6&nbsp;月底，该公司共有&nbsp;3558&nbsp;名员工。</p><p></p><p>SK&nbsp;On&nbsp;自&nbsp;2021&nbsp;年从&nbsp;SK&nbsp;Innovation&nbsp;分拆出来以来从未实现过盈利，今年第二季度的营业亏损为&nbsp;4600&nbsp;亿韩元（IT&nbsp;之家备注：当前约&nbsp;24.25&nbsp;亿元人民币），而上一季度的亏损为&nbsp;3320&nbsp;亿韩元（当前约&nbsp;17.5&nbsp;亿元人民币）。</p><p></p><h4>OpenAI、微软、谷歌等签署欧盟《人工智能公约》</h4><p></p><p>欧盟委员会当地时间周三（9&nbsp;月&nbsp;25&nbsp;日）公布了《人工智能公约》（下文简称为《公约》）的首批签署名单，上面有&nbsp;100&nbsp;多个签署方。《公约》的重点是让企业就如何处理和部署人工智能发布“自愿承诺”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9c/9c1ee98af009c7796f066c1d9932f0cb.webp" /></p><p></p><p>截图来源于网络</p><p></p><p>尽管具有法律约束力的欧盟《人工智能法案》（下文简称为《法案》）已于上月生效，但其所有合规期限仍留出数年时间，之后才能生效。这就造成了一段时间的空窗期，而欧盟希望用《公约》来填补这一空白。</p><p></p><p>欧盟委员会表示，这些“自愿承诺”是由《法案》的监督机构人工智能办公室起草的，在收集了人工智能公约中相关利益者所提交的反馈后，最终形成的承诺清单允许签署方自行挑选适合自己的承诺，但前提是至少需要承诺三项“核心行动”。</p><p></p><p>目前，名单上的公司包括亚马逊、微软、OpenAI、谷歌、Palantir、三星、SAP、Salesforce、Snap、空客、保时捷、联想、高通等，覆盖面极广，囊括了电信公司、咨询公司、软件公司、银行&nbsp;/&nbsp;支付公司、跨国公司、中小企业和面向消费者的平台。</p><p></p><p>《公约》旨在提高参与度和促进承诺，此外还侧重于促进信息共享，以便签署方能够相互帮助，以应对欧盟人工智能《法案》的新要求，并积极制定最佳实践。</p><p></p><p>签署方还被邀请在公布自己的承诺&nbsp;12&nbsp;个月后报告进展情况，这为下一轮宣传提供了机会。总体来看，《公约》将为公司争取声誉影响力，还能够激励签署方之间的竞争。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9f/9f9aafdc4252719b2400e39c2282fda8.webp" /></p><p>图片来源于网络</p><p></p><h2>IT&nbsp;业界</h2><p></p><p></p><h4>OpenAI&nbsp;向所有付费的&nbsp;ChatGPT&nbsp;用户推出了语音助手服务</h4><p></p><p>美东时间&nbsp;9&nbsp;月&nbsp;24&nbsp;日周二，所有付费订阅&nbsp;ChatGPT&nbsp;Plus&nbsp;和&nbsp;Team&nbsp;计划的用户都将可以使用新的&nbsp;AVM&nbsp;功能，不过该模式将在未来几天逐步推出。它将首先在美国市场上线。&nbsp;下周，该功能将向&nbsp;OpenAI&nbsp;Edu&nbsp;和&nbsp;Enterprise&nbsp;计划的订阅者开放。</p><p></p><p>据悉，AVM&nbsp;提高了部分外语的对话速度、流畅度并改进口音。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4f/4f9777ef6512f50c28514cfb1f59b1a6.webp" /></p><p></p><p>此外，AVM&nbsp;还新增了两大功能：为语音助手存储“自定义指令”，以及记住用户希望语音助手表现何种行为的“记忆”的功能（这类似于今年&nbsp;4&nbsp;月&nbsp;OpenAI&nbsp;为&nbsp;ChatGPT&nbsp;文本版本推出的记忆功能）。</p><p></p><p>也就是说，ChatGPT&nbsp;用户可以利用自定义指令和“记忆”来确保语音模式是个性化的，AVM&nbsp;会根据他们对所有对话的偏好做出响应。</p><p></p><p>语音方面，**OpenAI&nbsp;推出了五种不同风格的新声音：**Arbor、Maple、Sol、Spruce&nbsp;和&nbsp;Vale，加上之前老版本的四种声音&nbsp;Breeze、Juniper、Cove&nbsp;和&nbsp;Ember，可选声音达到九种，&nbsp;撤走了被指山寨“寡姐”（女演员斯嘉丽·约翰逊）的声音&nbsp;Sky。</p><p></p><p>这意味着，ChatGPT&nbsp;的&nbsp;Plus&nbsp;版个人用户和小型企业团队用户（Teams）可以通过“说话”的方式，而不是输入提示来使用聊天机器人。&nbsp;当用户在应用程序上进入语音模式时，他们会通过一个弹出窗口知道他们已经进入了高级语音助手。</p><p></p><h4>欧洲隐私机构&nbsp;noyb&nbsp;指控火狐&nbsp;Firefox&nbsp;浏览器：利用隐私保护功能追踪用户行踪</h4><p></p><p>9&nbsp;月&nbsp;26&nbsp;日消息，欧洲颇具影响力的数据隐私倡导组织&nbsp;None&nbsp;of&nbsp;Your&nbsp;Business（noyb）昨日（9&nbsp;月&nbsp;25&nbsp;日）发布新闻稿，指控火狐&nbsp;Firefox&nbsp;浏览器正利用“隐私保护”功能追踪用户的行踪。</p><p></p><p>noyb&nbsp;表示火狐&nbsp;Firefox&nbsp;浏览器于今年&nbsp;7&nbsp;月发布更新，在未明确告知用户的情况下，悄然启用“Privacy&nbsp;Preserving&nbsp;Attribution”功能。</p><p></p><p>Mozilla&nbsp;表示通过&nbsp;Privacy&nbsp;Preserving&nbsp;Attribution&nbsp;功能，可以让网站在不收集个人数据的情况下了解广告的表现。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3b/3b56bc4e8e2a208011a880f534aa5cd9.webp" /></p><p></p><p>该功能在&nbsp;Firefox&nbsp;浏览器完成有效追踪，并以匿名方式移交给聚合服务，这样就可以在不泄露用户隐私的情况下为广告商提供所需的信息。</p><p></p><p>noyb&nbsp;承认，虽然这种方法可能比无限制跟踪的侵犯性要小，但它仍然侵犯了欧盟&nbsp;GDPR&nbsp;规定的用户权利，更糟糕的是该功能是默认开启的。</p><p></p><h4>复旦大学团队贡献，Win10&nbsp;/&nbsp;Win11&nbsp;版苹果&nbsp;iTunes&nbsp;12.13.3&nbsp;修复提权漏洞</h4><p></p><p>9&nbsp;月&nbsp;27&nbsp;日消息，科技媒体&nbsp;9to5Mac&nbsp;9&nbsp;月&nbsp;26&nbsp;日发文，报道称苹果公司于本周四发布更新日志，介绍了面向&nbsp;Windows&nbsp;10、Windows&nbsp;11&nbsp;平台发布的&nbsp;iTunes&nbsp;12.13.3&nbsp;更新内容。</p><p></p><p>IT&nbsp;之家援引苹果官方更新日志，本次更新主要修改了本地提权漏洞，苹果公司通过施加额外限制已经修复。</p><p></p><p>该漏洞追踪编号为&nbsp;CVE-2024-44193，由复旦大学的&nbsp;Mads&nbsp;Ball、Bocheng&nbsp;Xiang&nbsp;两人发现并报告。苹果公司目前定期更新&nbsp;Windows&nbsp;10、Windows&nbsp;11&nbsp;版&nbsp;iTunes，但已经有一段时间没有为其推出新功能了。</p><p></p><p>苹果公司目前开始以独立应用的方式，提供&nbsp;iTunes&nbsp;的部分功能，例如推出了独立的&nbsp;Apple&nbsp;Music&nbsp;和&nbsp;Apple&nbsp;TV&nbsp;应用程序（当前仅为测试版），还有一款专门管理&nbsp;iOS&nbsp;设备的应用程序&nbsp;Apple&nbsp;Device，用于备份和恢复软件等。</p><p></p><p><img src="https://static001.geekbang.org/infoq/43/432bac9cdf9f8d00826157fede4a0fbe.webp" /></p><p></p><p>不过用户如果想要在&nbsp;Windows&nbsp;10、Windows&nbsp;11&nbsp;上使用&nbsp;Apple&nbsp;Podcast，依然需要借助&nbsp;iTunes&nbsp;应用。</p><p></p><h4>英特尔释出最新微码更新修正&nbsp;13/14&nbsp;代酷睿处理器崩溃问题</h4><p></p><p>9&nbsp;月&nbsp;26&nbsp;日消息，Intel&nbsp;员工在官网社区宣布，已找到第&nbsp;13&nbsp;代和&nbsp;14&nbsp;代酷睿桌面处理器出现不稳定现象的根本原因，并计划推出&nbsp;0x12B&nbsp;版本微代码以进一步修复这一问题。</p><p></p><p>英特尔表示，经过了全面的调查，确定了&nbsp;13&nbsp;和&nbsp;14&nbsp;代酷睿台式机处理器不稳定问题的根本原因，已将问题定位到&nbsp;IA&nbsp;内核内的时钟树电路。其在升高的电压和温度下容易受到可靠性老化的影响，从而导致时钟的占空比偏移和系统不稳定。</p><p></p><p>英特尔称，内部搭载的平台的测试表明，根据缓解措施进行一系列设置后，性能影响在运行差异范围内。英特尔再次强调，第&nbsp;13&nbsp;和&nbsp;14&nbsp;代酷睿移动处理器以及未来的客户端产品（包括代号为&nbsp;Lunar&nbsp;Lake&nbsp;和&nbsp;Arrow&nbsp;Lake&nbsp;系列）均不受最低运行电压偏移不稳定问题的影响。</p><p></p><p>对于所有使用第&nbsp;13&nbsp;和&nbsp;14&nbsp;代酷睿台式机处理器的用户，0x12B&nbsp;微代码更新必须通过&nbsp;BIOS&nbsp;更新加载。目前英特尔正在与其合作伙伴合作，从而推动及时验证和推出针对现有系统的&nbsp;BIOS&nbsp;更新，整个过程可能需要几周的时间。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/JUUuFQCazRfT0oECM1b9</id>
            <title>企业大模型应用开发提速！浪潮信息重磅发布元脑企智EPAI一体机</title>
            <link>https://www.infoq.cn/article/JUUuFQCazRfT0oECM1b9</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/JUUuFQCazRfT0oECM1b9</guid>
            <pubDate></pubDate>
            <updated>Sun, 29 Sep 2024 14:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月27-29日，2024中国算力大会在郑州举行。会上，浪潮信息重磅发布元脑企智EPAI一体机，通过软硬件高度协同的一体化设计，为客户提供多元多模、简单易用、本地部署、安全可靠的大模型开发平台，显著提高企业大模型以及AI原生应用的开发效率，加速大模型产业化落地。</p><p></p><h3>大模型落地挑战巨大</h3><p></p><p></p><p>随着大模型和生成式技术的飞跃式发展，基于大模型的应用创新正在成为新的主题，如何以大模型赋能现有的技术、业务，已成为企业在新一轮技术周期中保持竞争力的要素之一。但企业在应用大模型的过程中往往面临着诸多挑战，如生态离散导致的多元多模适配难，模型训练和部署复杂、数据治理难、模型“幻觉”问题以及对专业技术人才的依赖等，导致大模型与行业场景的融合进展缓慢。</p><p></p><p>首先，多元多模适配难度大、周期长、成本高。不同场景、不同业务对模型能力的多样需求，业务生产环境往往是多模并存的状态，而由于芯片生态离散、特定应用加速等原因，所使用的算力资源也是多元并用的。因此，大模型应用开发需考虑与多元算力、多样大模型的选择、适配、组合调度等问题，难度大，周期长，成本高。</p><p></p><p>其次，巨大参数量级的通用大模型，很难直接用于复杂、离散的行业场景，各行业知识专业化程度高，可迁移性低，通用大模型本身难以覆盖，所以经常出现大模型“幻觉”或是“胡言乱语”的问题，因此必须结合行业和企业专业数据进行再学习。</p><p></p><p>最后，要实现大模型与行业场景的深度结合，实现高效、高可靠、高质量的模型应用效果，涉及数据、微调、RAG、部署、上线和运维等极为复杂的流程，特别是数据治理和模型微调，需要具有丰富经验的实施团队才能胜任，技术门槛高。</p><p></p><h3>元脑企智EPAI一体机，让大模型开发快到飞起</h3><p></p><p></p><p>对于大多数企业而言，大模型应用开发的系统性、复杂性，往往让企业对大模型开发望而却步。因此，实现大模型的深入行业应用与广泛落地，关键在于如何有效提高AI应用创新的质量和效率，卓越的大模型及应用开发工具成为释放智能生产力的关键。</p><p></p><p>浪潮信息元脑企智EPAI一体机基于专为大模型应用场景设计的元脑服务器，搭载了元脑企智EPAI企业大模型开发平台，支持多元算力、多模管理、全链工具以及本地部署，可一站式解决数据处理、模型微调、RAG搭建、模型部署、应用上线和系统运维等环节开发难题，为客户提供多元多模、简单易用、本地部署、安全可靠的大模型应用开发平台，满足企业人工智能应用从开发到实施的全栈需求。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ac/ac390264305150a3cdf465c6f5bc2c54.png" /></p><p></p><p>多元多模：元脑企智EPAI一体机具备多元算力和多模管理能力，通过大模型计算框架TensorGlue实现异构算力调度，通过算子基础化技术实现上层模型算法和下层基础设施的逻辑解耦，从而高效的屏蔽模型和芯片差异，降低企业跨算力平台迁移、多元模型部署适配的试错成本。目前，元脑企智EPAI一体机可以支持10+业界主流大模型计算框架，内置7个主流基础大模型，预设了20+微调参数，用户可以针对知识问答、智能编码、文档理解、智能助手等不同应用场景和任务需求，选择最佳产品型号和模型算法，快速开发模型应用。</p><p></p><p>简单易用：元脑企智EPAI一体机提供从数据准备、大模型微调、知识库构建、大模型部署上线运维的全流程支持工具链。其中，针对数据准备，预先内置了上亿条基础知识数据以及自动化数据处理工具，支持10种以上企业常见的数据格式，并且以超过95%的抽取准确率，把这些数据转化为知识库以及可供模型进行微调的数据；针对大模型微调，采用低代码可视化界面来进行微调，并且内置了Lora、SFT等多种微调框架以及20多种优化参数，用户可依据具体业务需求和数据特性，选择最为合适的框架与技术，快速且低成本地构建起企业专属大模型能力。</p><p></p><p>本地部署、安全可靠：由于大模型应用开发需要结合企业私有数据，要求企业数据不出域。本地部署可以确保用户数据不被上传至云端，避免数据泄露和滥用的风险，这在处理敏感信息或符合严格数据保护法规的行业中尤为重要。元脑企智EPAI一体机的本地化部署模式提供全链路的企业数据防护能力，设置多级过滤和审核体系，让数据的流转更安全，让生成结果更可靠，构建起一个既能充分利用数据价值，又能保护用户隐私、符合法规要求的安全数据处理环境，做到“数据可用不可见”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/51/5188c34c10edde0dc0e02be1dc88bd15.jpeg" /></p><p></p><p>元脑企智EPAI一体机能显著提升大模型应用的开发效率，并极大节省人力成本。据悉，浪潮信息先行先试，采用1台元脑企智EPAI一体机标准版，低代码完成企业知识库构建、模型微调、应用开发等工作，1人1月即简单高效、低门槛地打造出智能售前助手“元小智”，赋能日常售前业务，实现智慧化变革，团队工作效率提升3-5倍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1f/1f9e44c8a2c8c94cbf2ac1f5c6cec336.gif" /></p><p></p><p>浪潮信息产品方案开发部总经理魏健表示，本次发布了基础版、标准版、高级版、创新版和集群版五个版本，主要面向制造、金融等行业客户、传统ISV和SI三类用户提供一站式大模型生产及应用全流程开发工具链，同时支持接入更多元脑伙伴的算法和模型能力，共同打造AI原生应用开发的“超级工作台”，加速AI应用创新和落地。</p><p></p><h3>广泛赋能企业各类应用场景，提升智能生产力</h3><p></p><p></p><p>智能投标助手：浪潮信息方案开发团队先行先试，采用1台元脑企智一体机标准版，低代码完成企业知识库构建、模型微调、应用开发等工作，1人1月即简单高效、低门槛地打造出智能售前助手“元小智”，赋能日常售前业务，实现智慧化变革，团队工作效率提升3-5倍。解决了招投标数据处理缺少工具、专业知识门槛高、业务效率低下、准确率难以保证、数据防泄漏等问题，大大缩短了应用开发周期，降低了人力成本，提升了投标文件审查效率。</p><p></p><p>智能客服大脑：采用1台元脑企智一体机基础版，基于浪潮信息高效微调 MoE 大模型“源 2.0-M32”构建智能客服平台，将公司内部产品资料构建专属知识库，通过数据抽取、格式转换，生成高质量的微调数据，投喂给“源”大模型。历时 6个月，结合知识蒸馏、压缩等技术，构建浪潮信息的“智能客服大脑”。解决了传统人工智能客服在处理非标准或开放式问题时能力有限、知识库更新难度大、信息过时或不准确等问题，常见问题解决率 80+%，复杂问题处理时长缩短 65%，大大降低售后客服人员压力，人效提升30%。</p><p></p><p>智能编程助手：支持浪潮信息多个研发部门，利用元脑企智一体机开发周期缩短至2天，由 AI 自动生成超过 65%的计算框架代码，整体采纳率 50%，为研发工程师每天节省近3个小时的代码开发时间，大幅提升效率，解决了代码工程可读性差、开发时间长等问题。</p><p></p><p>除此之外，还广泛赋能企业的各类应用场景，例如研发领域的编程助手、个人办公助理、部门助理、操作系统助手、产品设计助手等，开发速度快，低至 1 周；培训周期短，最快 3 天；应用效果好，企业知识驱动；安全可靠，全链路数据安全。</p><p></p><p>行政领域的员工服务、内训讲师、招聘 JD 生成、招聘助手、公文写作等，满足行政部门的多样化需求，提高工作效率。</p><p></p><p>销售领域的智能客服、投标助手、文案写作、产品文档翻译、BI 系统助手等，提升销售环节的服务质量和效率。</p><p></p><p>生产/供应链领域的故障自动识别、维修方案生成、供应链信息系统助手、AI 质检、最优化策略方案等，优化生产和供应链管理。</p><p></p><h3>元脑企智EPAI聚焦大模型新技术，探究更多落地的可能性</h3><p></p><p></p><p>在会后的采访中，魏健解释了浪潮信息推出元脑企智EPAI大模型一体机的背景。当前市场趋势与传统企业应用之间存在巨大差距，浪潮信息希望帮助这些企业快速实现大模型的应用落地。而浪潮信息目标客户群体主要来自这几方面，一是传统制造业客户，这类客户通常拥有丰富的数据和人员资源，并且有应用牵引的趋势。</p><p></p><p>其次是传统ISV（独立软件供应商），例如金融行业的中科软和南天等，他们也非常积极的投入研究大模型应用。最后就是SI（集成商），他们面临的挑战在于服务需求的满足，特别是在大模型调优能力方面。</p><p></p><p>浪潮信息架构师Owen ZHU博士表示，元脑企智EPAI一体机聚焦的是大模型落地，落地应用和基础研发之间是有时差的，基础研发现在聚焦的是模型本身的推理、规划的复杂任务的能力，但是落地需要聚焦在问答场景、知识总结梳理和生成的场景，比如生成报表或者生成文案。</p><p></p><p>元脑企智一体机也应用了很多新的技术，比如说监督微调、人类反馈强化学习等，作为AI应用架构师，Owen ZHU认为这些技术提升了大模型在应用场景的适应性和准确性。大模型从预训练、微调、推理三个阶段展开，现在业界的关注点已经到了微调和推理，微调技术非常复杂、技术门槛高、迭代速度快，浪潮信息AI团队也在尝试各种方法，比如指令微调，或者人类偏好数据微调、高效微调。通过这些高效微调的技术，可以用很小的显存量就能把微调跑起来。也就是说在新技术的加持下能够降低算力门槛。而且目前元脑企智一体机预制的算法、模型都是研究好，可以直接使用的，同时内部配置了20多种参数，这些都是浪潮信息AI团队的经验输出。</p><p></p><p>通过这次采访，我们了解到浪潮信息一体机的推出是为了帮助企业，特别是传统企业，快速实现大模型的应用落地。浪潮信息针对不同的客户群体，提供了定制化的解决方案，并在元脑企智EPAI一体机中集成了多种微调技术，以降低技术门槛和算力需求。同时，浪潮信息也在积极探索新技术，以提升大模型的适应性和准确性。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/gqPJLQkCXxfmPzzLsBqf</id>
            <title>Kimi 背后的长文本大模型推理实践：以 KVCache 为中心的分离式推理架构</title>
            <link>https://www.infoq.cn/article/gqPJLQkCXxfmPzzLsBqf</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/gqPJLQkCXxfmPzzLsBqf</guid>
            <pubDate></pubDate>
            <updated>Sun, 29 Sep 2024 12:09:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在不久前举办的 AICon 全球人工智能开发与应用大会上，月之暗面高级研发工程师、开发者关系负责人唐飞虎发表了专题演讲“长文本大模型推理实践——以 KVCache 为中心的分离式推理架构”，分享介绍 Kimi 智能助手背后的推理加速方案，以及该方案在设计时所需要考虑的指标和在真实生产环境中部署的表现。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d9/d924dae4adff990f82b86d5d066c2365.jpeg" /></p><p></p><p>在 10 月 18 -19 日即将召开的<a href="https://qcon.infoq.cn/2024/shanghai"> QCon 上海站</a>"上，我们专门策划了【<a href="https://qcon.infoq.cn/2024/shanghai/track/1715">大模型基础设施与算力优化</a>"】专场，并邀请到月之暗面推理系统负责人何蔚然进一步分享 Mooncake 分离式推理架构创新与实践，同时微软亚洲研究院软件开发工程师姜慧强将分享 《长文本 LLMs 推理优化：动态稀疏性算法的应用实践》，还有更多大模型训练推理的一手实践案例尽在本专题。欲了解更多精彩内容，可访问大会官网：<a href="https://qcon.infoq.cn/2024/shanghai/schedule">https://qcon.infoq.cn/2024/shanghai/schedule</a>"</p><p></p><p>以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）。</p><p></p><p>提到 Kimi，相信在座的各位都有所耳闻。Kimi 智能助手在多个平台上都有入口，包括 Apple Store、微信小程序以及 Web 端，尤其是 Web 端的排名一直居高不下。在日常使用中，尤其是在午间高峰时段，用户可能会遇到 Kimi“累了”的情况。但值得注意的是，尽管用户数量在不断增加，用户体验却得到了显著改善，现在 Kimi“累了”的情况减少了很多，这与我们推理团队的技术攻关是分不开的。</p><p></p><p>今天，我将从四个方面进行介绍。第一部分我将探讨长文本推理的瓶颈问题。随着推理集群的扩大和上下文长度的增加，无论是训练还是推理都面临着更高的要求。我们需要明确瓶颈所在，以便找到解决问题的途径。第二部分将审视目前市面上的推理优化工具和方法，看看有哪些可以为我们所用。第三部分将详细介绍我们的 Mooncake 项目。这是一个以 KVCache（键值缓存）为中心的分离式推理架构，内部代号“Mooncake”由我命名，寓意与“moon”相关，同时“Cake”与“Cache”谐音。在最近几个月，我们也看到了不少类似的方案提出，虽然与我们的设计大同小异，但我将分享一些细节，特别是在面对大量用户时可能遇到的一些独特问题。第四部分，我将讨论上下文缓存的应用。在 SaaS 服务层面，我们为开发者提供了上下文缓存功能。我将具体介绍每位开发者如何利用 Mooncake 方案来优化自己的 AI 应用。</p><p></p><h4>长文本推理的瓶颈</h4><p></p><p></p><p>几个月前，我在 AICon 北京站上讨论了 RAG 与长文本处理的对比。RAG 模型有其优势和劣势，但在长文本处理方面，它有两个显著的劣势：成本高和速度慢。这也是许多开源模型无法良好支持或只能有损支持长文本处理的原因之一。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/09/09e1b986f7fc281e6321bafab24434ee.png" /></p><p></p><p>成本高的问题是我们的用户群体，特别是 API 开发者在生产环境中经常遇到的一个痛点。他们需要使用我们的 API 对同一个文档，比如一份合同，进行复杂的任务处理，并可能需要反复询问 100 多次或更多。众所周知，大型模型通常采用无状态设计，这意味着每次调用都需要将整个上下文传递进去。随着对话的进行，上下文可能会不断增长，不仅包括上下文本身，还可能包括函数调用、定义以及其他文档设置。每次调用都需要处理这么多信息，因此成本自然会很高。</p><p></p><p>速度慢问题，特别是在第一次处理时，模型的响应速度会特别慢。例如，我们群里的 API 助手在用户频繁提问时，有时会卡住，20～30 秒内都无法产生回复。我们无法确定是卡住了还是其他地方出了问题，但很可能仅仅是因为处理速度慢而导致的卡顿。这是长文本处理速度慢带来的一个副作用。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/93/93b65275a424b9e2a36be3df54e0c12b.png" /></p><p></p><p></p><h5>贵且慢的原因</h5><p></p><p></p><p>长文本推理之所以成本高昂且速度缓慢，原因在于 Transformer 模型在计算 Attention 机制时的工作方式。在没有使用缓存的情况下，每次计算 Attention 都需要进行完整的矩阵乘法，这导致每次迭代的长度以平方级别增加。每当出现一个新的 Query Token，都需要重新计算，这无疑增加了计算的复杂性和时间。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/25/2541134ac7d654f76d8eaff6db6f0147.png" /></p><p></p><p>当我们引入 KVCache 机制后，情况就大为改观。使用 KVCache 后，每次计算的长度只需要线性增加，这意味着我们不再需要重新计算过往的 tokens，从而显著提升了性能。但这种优化也带来了新的问题。</p><p></p><p>为了更直观地理解这一点，我们可以看下面这张图。图表的横坐标表示上下文长度，从左到右逐渐增大；纵坐标则展示了不同的性能度量指标，包括并发数、预填充延迟、解码延迟以及上下文切换时的状态切换延迟。此外，还有 Free HBM Size，它表示可用的高带宽内存大小，随着上下文长度的增加，它可以改善并发性能。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/24/24c8687b3e30c6a2ed787e50d8040c73.png" /></p><p></p><p>在 Mooncake 论文中，我们使用了 LLaMA2-70B 模型作为例子。从图表中可以明显看到，随着序列长度（即上下文长度）的线性增长，预填充延迟（prefill latency）呈现出超线性增长，这是一个非常显著的趋势。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/9d/9d4207b661c38416100476c0a365745c.png" /></p><p></p><p>我们做个小结，长文本性能瓶颈主要包括以下几个方面：</p><p></p><p>并发性能：随着上下文长度的增加，并发性能会反比下降；预填充延迟：随着上下文长度的增长，预填充延迟会以平方级别的速度增长；解码延迟和上下文切换延迟：随着上下文长度的增加，解码延迟也会线性增加。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/bc/bcea93f9ba48861f8f365d8a8f8d79bb.png" /></p><p></p><p></p><h4>长文本推理的优化</h4><p></p><p></p><p>面对长文本推理中的成本高昂和速度缓慢的问题，我们采取了一系列优化策略。在之前的讨论中，我们已经提到了一些方法，现在我们来总结一下这些策略，并探讨它们如何帮助我们解决长文本推理的挑战。</p><p></p><p>首先，我们考虑了几种优化技术，包括 Flash Attention、vLLM（垂直扩展的大型语言模型）、MOE（Mixture of Experts，专家混合模型）以及最近非常受欢迎的 Speculative Decoding（推测性解码）。此外，还有一些有损策略，例如 Windows Attention，它通过截取部分信息来处理长文本推理，尽管这种方法在资源有限的情况下可行，但最终提供给用户的模型可能是有损的。例如，如果用户询问一个公司上市财报的问题，信息可能分散在文档的不同部分，使用 Windows Attention 策略可能会遗漏一些关键信息。这种信息的遗漏是在训练过程中就已经决定的，后续的 SFT 也无法修复这个问题。虽然这种策略可能在短期内提高模型的上下文指标，但对最终用户体验来说可能是有损害的，而且这种损害在进行大模型基准测试时可能不容易被检测出来。</p><p></p><p>此外，这些策略之间存在不兼容问题。这就需要我们深入了解每个策略的具体实现方式，这会带来一系列复杂的问题，需要从不同的层、头或隐藏层角度进行推理优化。在系统设计时，我们需要选择兼容性最好且效果最佳的策略。</p><p></p><p>我们的 Mooncake 主要是从集群调度的角度进行优化。这种优化与我们之前提到的所有策略基本上是正交的，可以与任何策略组合使用，而不会损失性能。这对于模型基础供应商来说可能是一个高优先级的策略。这也是为什么我们最近看到许多厂商推出了基于 KVCache 优化的方案。这些方案能够提高长文本推理的效率，同时保持用户体验的高质量。</p><p></p><h4>Mooncake 的实践</h4><p></p><p></p><p>上个月，我们在 GitHub 上发布了相关的论文，其中包含了许多细节。在详细介绍之前，让我们先澄清一些基本概念。在大模型推理中，有两个至关重要的阶段：预填充（Prefill）阶段和解码（Decode）阶段。</p><p></p><p>预填充阶段：在这个阶段，每个新 Token 的生成都依赖于之前所有的 Token。由于输入的全部内容（即 Prompt）都是已知的，这个阶段可以进行高度并行化的矩阵操作，有效提高 GPU 的利用率。这个阶段对“首次 Token 时间”（Time to First Token, TTFT）有显著影响，对于流式应用来说，如 GPT-4o 或 AI 陪伴类实时交互应用，这个指标尤为重要。解码阶段：与预填充阶段不同，解码阶段的 Token 不是一次性全部生成的，而是逐段、逐词生成的。这个过程会一直持续到满足某个停止条件，这些条件可能是模型的最大上下文限制、用户设置的上下文上限，或者是其他预设的停止条件，比如输出了一个 JSON 的终止符。在这个阶段，每个顺序输出的 Token 都需要知道之前所有迭代的输出状态的 KV 对。这涉及到矩阵中的向量运算，与预填充阶段相比，解码阶段无法充分利用 GPU 的计算能力。数据从内存传输到 GPU 的速度决定了这个阶段的延迟，而不是计算本身的速度。换句话说，解码阶段主要受内存传输速度的限制，这个阶段主要影响的是“每个输出 Token 的时间”（Time per Output Token, TPOT），对于流式应用来说，这个指标同样敏感，并且对总体推理时间有较大影响。</p><p></p><p></p><h5>Mooncake 的基本思想</h5><p></p><p></p><p>Mooncake 的核心理念是将模型推理过程中的两个截然不同的优化阶段分开处理，因为这两个阶段的优化目标和受限的瓶颈各不相同。这种分离式处理方法是一种直观且自然的思路。具体来说，Mooncake 采用了以 KVCache 为中心的分离式推理架构，主要由三个核心部分组成：</p><p></p><p>Prefill 池：这个部分负责集中管理所有的预填充阶段的计算任务。Decoding 池：这个部分集中处理所有解码阶段的任务。KVCache 池：这个部分负责存储所有中间过程中应用到的 KVCache，并决定何时使用这些缓存，何时释放它们。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ad/ad082105929e01fea1cd226800df6a01.png" /></p><p></p><p></p><h5>Mooncake 的开发动机</h5><p></p><p></p><p>现有的大型语言模型（LLM）服务系统，例如垂直扩展的大型语言模型（vLLM），通常将预填充（Prefill）和解码（Decode）阶段放在同一个 GPU 上处理。这种设计在实际操作中会遇到一些问题：</p><p></p><p>TTFT 和 TPOT 优化不可兼得：预填充阶段处理的文档通常非常长，并且随着多轮对话的进行，上下文长度会不断增加，导致预填充比解码耗时更长。在需要作出决策先执行哪个阶段时，vLLM 的做法是让解码阶段暂停，以便预填充阶段先行。这会导致 TPOT（每个输出 Token 的时间）增加。反之，如果优先解码，TTFT（首次 Token 时间）也会增加。GPU 资源竞争：即使预填充和解码阶段被单独调度，它们仍然会竞争 GPU 资源，导致等待时间增加。在单机情况下，很难设计出一个高效的调度策略来解决这种资源竞争问题。Prefill 和 Decode 的瓶颈不同：预填充阶段主要消耗计算资源，而解码阶段则更依赖于内存和带宽。目前还没有既擅长计算又擅长内存和带宽的芯片，因此，使用具有不同特点的 GPU 分别处理预填充和解码，可能更有利于资源的利用和成本的节约。</p><p></p><p>这种分离设计的思路在计算机体系结构领域并不新鲜。例如，区块链系统 EOS 在设计时就采用了类似的分离策略，将带宽和 CPU 计算分开处理，各自形成一个资源池。受到这种设计思路的启发，Mooncake 采用了典型的分离式架构，将单个同构 GPU 集群的资源打散并重新组织成三个可以独立弹性伸缩的资源池 —— Prefill Pool、Decode Pool、KVCache Pool。</p><p></p><p></p><h5>Mooncake 的分离式架构</h5><p></p><p></p><p>在 Mooncake 架构中，Prefill Pool 中的一个具体实例负责处理特定的任务。下图的工作流程可以这样理解：</p><p></p><p>KVCache 的重用（Reuse）：在下图左侧黄色区域，我们可以看到处理过程中可能会涉及到 KVCache 的重用。这部分 KVCache 可能已经存在，它包含了之前迭代中计算得到的数据，可以在新的迭代中被再次利用，从而提高效率。增量 KVCache（Incremental KVCache）：在下图左侧粉色部分，是处理新的输入所生成的增量 KVCache。用户在每一轮对话中都可能提供新的输入，这些输入需要被处理并生成新的 KVCache，作为当前轮次迭代的结果。流量式传输（Traffic-style Transfer）：计算完成后，这些 KVCache 会以一种流量式的方式传输，即连续不断地传递给解码阶段的实例。解码实例（Decoding Instance）的任务：接收到 Prefill Pool 传递过来的 KVCache 后，解码实例的任务就相对简单了。它主要负责根据这些 KVCache 进行解码操作，生成最终的输出结果。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/27/27f2ac2ac220f83eac6f105d9d53cf94.png" /></p><p></p><p>在 Mooncake 架构中，KVCache 资源池扮演着至关重要的角色，尤其是在线上服务中，每天需要应对数百万用户的查询。在一些热点事件，如奥运会期间，用户可能会频繁询问类似的问题，比如中国队当天获得的金牌数量或者乒乓球男团的晋级情况。面对这样的高频查询，我们可以利用 KVCache 资源池来优化处理。</p><p></p><p>一个自然的想法是使用哈希存储来管理这些 KVCache。不同的厂商可能会采用不同的策略，例如有些可能会选择使用 Trie 树。Trie 树在计算复杂度上与模型的词表大小有关，如果模型的词表发生变化，比如从 GPT-4 升级到 GPT-4o 时词表扩大了，Trie 树的性能可能会受到影响。</p><p></p><p>为了避免这种复杂性并提高效率，我们选择了哈希存储。哈希存储方法简单、速度快，并且不受词表大小变化的影响。这样，无论用户的查询如何变化，我们都能保证 KVCache 资源池的高效运作。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/06/06b2d7c184fdb4b2fc3f0aa6667b8987.png" /></p><p></p><p></p><h5>Mooncake 效果展示</h5><p></p><p></p><p>在实施了三个独立的资源池之后，我们进一步观察了线上生产环境中的实际运行情况。我们发现，Prefill 和 Decoding 阶段对资源的占用呈现出一种波浪型模式，类似于潮汐的涨落，每天都有规律地变化，尽管看起来似乎没有明显的规律。然而，当我们仔细观察并分析每天线上数百万用户的行为时，这些行为模式变得可以预测。基于这些可预测的行为模式，我们可以采取类似于 vLLM 中的策略，即在资源紧张时暂停解码阶段，让预填充阶段先行。在集群调度中，我们也可以应用类似的逻辑。我们根据线上生产环境的日常运行数据，设计了一种基于动态规划的调度策略。这种策略能够提前准备适量的预填充和解码资源，以使资源占用的波动曲线更加平缓。通过这种动态规划的调度策略，我们能够更有效地管理资源，减少资源浪费，并确保服务的稳定性和响应速度。这样的调度提升了用户体验，因为我们能够更好地应对用户需求的高峰和低谷，确保服务始终如一地流畅运行。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a0/a0976b0773270fc18eb907e7c8aab276.png" /></p><p></p><p>在下面展示的图表中，蓝色线条代表我们的 Mooncake 架构，而黄色线条则代表与之对比的 vLLM 架构。我们可以观察到两种架构在首次 Token 时间和 Token 间时间上的表现差异。</p><p></p><p>首次 Token 时间：在首次 Token 时间上，Mooncake 架构相较于 vLLM 架构有轻微的优化。这主要是因为引入了 KVCache，它能够存储和重用之前的计算结果，从而加快了预填充阶段的处理速度。Token 间时间：在 Token 间时间上，Mooncake 架构的改善非常明显。这是因为 Mooncake 将解码阶段单独分配给一个资源池来处理，这样的分工使得解码过程的效率显著提高。由于解码阶段不再与其他任务共享资源，它可以更加专注和高效地生成每个后续的 Token。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/62/6203884251c91fed2dd7317d1755f458.png" /></p><p></p><p>下图展示了一些具体的实验数据，这些数据并非来自线上环境，而是使用了开源数据集。我们特别关注了 ArXiv 上每天发布的论文摘要任务。许多用户每天都会使用 Kimi 智能助手来总结文章，包括学术论文，以及一些第三方应用基于 Kimi 大模型开发的论文摘要工具。在这些实验中，我们比较了 Mooncake 架构和单机 vLLM 架构在处理 ArXiv 数据集时的性能。实验结果显示，Mooncake 在处理这些摘要任务时相较于单机 vLLM 有明显的优势。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/d6/d6d617bf11ea3efff95c7fafc50fe884.png" /></p><p></p><p></p><h4>上下文缓存的应用</h4><p></p><p></p><h5>Context Caching 基本原理</h5><p></p><p></p><p>在传统的 LLM 交互中，每次用户与模型的对话都需要重新计算整个上下文，这包括了所有的历史信息和对话内容。这种方法意味着每次交互都需要为整个上下文支付计算费用，无论上下文中有多少信息是重复的。</p><p></p><p>上下文缓存的核心改进在于引入了一个“公共上下文”的概念。这个公共上下文包含了对话中不变的部分，比如背景信息或常见问题。通过缓存这个公共前缀，我们只需要为其支付一次计算费用，而不必在每次交互时重复支付。这样，每次交互的成本就大大降低了，用户只需要为每次的增量输入（即新的对话内容）以及存储公共上下文的费用付费。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/da/dac6dea61ab77dc78beaa6a6f593ac0f.png" /></p><p></p><p></p><h5>Context Caching 使用流程</h5><p></p><p></p><p>使用 Context Caching 的流程非常简洁明了，主要分为以下几个步骤：</p><p></p><p>创建缓存：首先，你需要创建一个缓存实例。这个过程通常非常快速，大约需要 30 到 40 秒的时间。一旦缓存创建完成，它就可以被用于后续的交互。使用缓存：创建缓存后，你可以直接在对话或应用中使用它。由于缓存已经包含了必要的上下文信息，因此可以避免重复计算，提高响应速度和效率。</p><p></p><p>为了帮助开发者更容易地实现上下文缓存，我们在官方 GitHub 上提供了一些示例代码，这些代码覆盖了多种编程语言，包括 Python、Node.js 等，以便开发者能够快速上手并集成到自己的项目中。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/13/13bbcaf7294657c909e6c81b8cd4a0eb.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/0c/0cbbe7bf969a2e8166730af19aeec38a.png" /></p><p></p><h5>Context Caching 收费模式</h5><p></p><p></p><p>我们的上下文缓存（Context Caching）技术的收费模式已经进行了优化和调整。现在，创建缓存的成本非常低，而且是一次性的费用。调用缓存的费用也几乎可以忽略不计，主要的成本瓶颈在于存储空间的费用。为了鼓励更多开发者使用这项技术，我们最近对价格进行了调整，降价幅度达到了 50%。原先的费用是 10 元，现在降低到了 5 元。我们希望通过这样的降价措施，能够激励开发者更广泛地采用上下文缓存技术。</p><p></p><p>在最近的“GOGC 黑客松”和之前的“Adventure X 黑客松”中，我们特别表彰了使用上下文缓存 API 最多的开发者，以此鼓励大家探索和利用这项技术。我们注意到，尽管上下文缓存技术能带来便利和性能优化，但仍有许多开发者没有充分利用它。例如，之前有一个非常受欢迎的应用，它在短时间内有大量的调用。如果该应用当时使用了上下文缓存技术，开发者本可以节省一大笔开支。实际上，我们与开发者进行调研后发现，由于大模型产生的费用过高，他们不得不调整其提示词，尽可能简化内容，甚至删除了一些原本设计的游戏玩法、规则和元素，这无疑是一种遗憾。如果现在有类似的爆款应用出现，它们完全可以利用上下文缓存技术来提升用户体验并有效控制成本。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b2/b2bd1881c1d36491dfdff6e094064a4d.png" /></p><p></p><p></p><h5>Context Caching 应用技巧</h5><p></p><p></p><p>我们的官方 API 小助手的线上实际调用情况可以通过下图进行展示。目前，我们大约有 20 个开发者社群，每个社群的每小时调用情况都能清晰地反映出来。从图中可以看出，在每天凌晨 2 点到早上 8 点这段时间，社群内几乎没有活动，因为大家都在休息。一旦进入白天的工作时段，尤其是在早上、中午以及晚上下班后的时间段，社群内的活动会显著增加，这也是我们的 API 小助手最为忙碌的时段。</p><p></p><p>对于不同的开发者来说，他们的应用类型可能会影响调用频率的曲线。例如，如果应用是工具类或生产力类的，那么在工作日的白天可能会有更多的调用。相反，如果应用更偏向于娱乐或游戏，那么在晚上和周末的调用可能会更加频繁。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c5/c5b6499fb98f470baf991885b1c51a6b.png" /></p><p></p><p>具体到我们小助手的情况：</p><p></p><p>在上午 9 点时，我们需要在调用 / chat/ completions 接口时，在 Headers 中添加 x-Msh-context-Cache 以启用 Cache， 同时添加 X-Msh-context-Cache-Reset-TTLHeader 以更新 Cache 存活期，这里以小助手为例，存活期为 3600s，即 1 小时；由于我们要在凌晨。点结束 Cache，因此夜间 23 点是我们最后一次刷新 Cache 存活期的时点，在此之后，我们需要移除 Headers 中的 x-Msh-context-cache-Reset-TTL 参数，以保证 Cache 能在 0 点被顺利移除；</p><p></p><p>以 Python 代码为例，大致的代码逻辑为：</p><p></p><p>我们具体分析了一天中的数据，发现通过在特定时间点，如上午 9 点和晚上 24 点，存储上下文缓存，可以显著降低费用消耗，大约能节省 3/4 的成本。这种策略对于那些像我们的 API 小助手这样需要频繁交互的应用场景尤其有效。如果你的应用也属于这种类型，那么上下文缓存技术将是一个非常值得尝试的优化手段。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/66/667e7cf081596bcaa2f1f1ab02dd1bdf.png" /></p><p></p><p>目前，我们也观察到市场上其他一些解决方案，它们通过不同的方式处理上下文缓存的复杂性。一些系统选择将这些复杂操作隐藏起来，让用户无需手动管理缓存的存储和删除，而是将这些任务完全交给调度引擎，由它智能地进行资源分配和优化。我认为手动管理和自动调度各有优势，手动管理提供了更多的灵活性和控制权，而自动调度则简化了操作流程，降低了用户的使用门槛。未来，我们可能会看到这两种方法的融合，以满足不同用户的需求和偏好。</p><p></p><h5>Context Caching 适用场景</h5><p></p><p></p><p>上下文缓存技术的应用场景非常广泛，尤其适合那些频繁进行请求并且需要重复引用大量初始上下文信息的场景。在这些情况下，上下文缓存能够显著提升处理效率，主要表现在缩短首次 Token 生成的时间，同时大幅降低 Token 消耗的费用。在我们的线上生产环境中，随着 Mooncake 架构从最初的灰度测试到现在的全面部署，Kimi 智能助手能够每天处理的请求量增加了 75%。这也是为什么用户最近感觉到“Kimi 累了”的情况有所减少的原因之一。</p><p></p><h4>参考资料</h4><p></p><p></p><p>为了帮助大家更深入地了解上下文缓存及相关技术，我们附上了一些参考资料。特别推荐其中一篇由爱丁堡大学的傅瑶撰写的基础总结，该文详细介绍了长上下文大型模型推理优化技术，包括最新的进展和其他技术的对比分析，以及哪些技术可以组合使用等。</p><p></p><p>Mooncake: A KVCache-centric Disaggregated Architecture for LLM ServingData Engineering for Scaling Language Models to 128K ContextLarge Lanquage Model Based Long Context Modeling Papers and BloasFull Stack Transformer Inference Optimization Season 2: Deploying Long-Context Models</p><p></p><p></p><h5>演讲嘉宾介绍</h5><p></p><p></p><p>唐飞虎，月之暗面高级研发工程师、开发者关系负责人。前谷歌工程师、ACM/ICPC 亚洲赛区金牌、微软编程之美挑战赛冠军、第一届万向实验室通证经济设计大赛冠军。</p><p></p><p></p><h5>会议推荐</h5><p></p><p></p><p>AI 应用开发、大模型基础设施与算力优化、出海合规与大模型安全、云原生工程、演进式架构、线上可靠性、新技术浪潮下的大前端…… 不得不说，<a href="https://qcon.infoq.cn/2024/shanghai">QCon </a>"还是太全面了，报名详情请联系票务经理 &nbsp;17310043226 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/68/68a4f559d6682dec46bd5633588299f0.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0PTVDFjuaDNIeTGeAswa</id>
            <title>万字长文解读百度大模型原生安全构建之路</title>
            <link>https://www.infoq.cn/article/0PTVDFjuaDNIeTGeAswa</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0PTVDFjuaDNIeTGeAswa</guid>
            <pubDate></pubDate>
            <updated>Sun, 29 Sep 2024 10:46:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>大模型以其更智能、不确定和不可解释的特点，给安全领域，尤其是内容安全带来了更大的挑战。百度在打造文心一言之初就意识到，传统的内容审核技术无法从根本上满足大模型内容安全的需求。因此，我们必须从头开始构建一套全新的方法。</p><p></p><p>在不久前举办的 AICon 全球人工智能开发与应用大会上，百度安全平台副总经理冯景辉发表了专题演讲“百度大模型原生安全构建之路”， 分享聚焦于百度在过去两年百度安全平台团队在大模型内容安全领域遭遇的挑战和问题，以及团队尝试过的解决思路和应对方法，涵盖数据清洗、内生安全与安全对齐、安全围栏建设，以及应用安全与基础模型安全等方面。</p><p></p><p>我们将在 10 月 18 -19 日 <a href="https://qcon.infoq.cn/2024/shanghai/">QCon 上海站</a>"【<a href="https://qcon.infoq.cn/2024/shanghai/track/1717">探索安全边界：出海合规与大模型实践</a>"】专场，探讨大模型如何帮助团队成员更高效地解决安全问题。百度杰出架构师、安全技术委员会主席包沉浮将分享百度基于大模型安全运营的质效提升实践。欲了解更多内容，可访问大会官网：<a href="https://qcon.infoq.cn/2024/shanghai/schedule">https://qcon.infoq.cn/2024/shanghai/schedule</a>"</p><p></p><p>以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）。</p><p></p><p>今天，我想与大家分享百度在过去两年中，如何在开发大模型的过程中确保安全性的故事。早期当大模型遇到敏感问题时，，它通常会建议我们换个话题。在过去两年里，我们一直在不断优化，解决模型生成过程中出现的各类安全性问题，同时也在提升用户体验。在下面的图中，我们可以看到，即使是其他公司的模型也经常会遇到需要用户重新提问或直接拒绝回答的情况。然而，最近在使用文心一言时，我们发现它已经开始用更积极的角度引导用户正确看待敏感问题，这在很大程度上改善了用户体验。</p><p></p><p>今天，我将讨论四个方面的问题。首先，我们会回顾一下大型模型面临的安全挑战。接着，我们将探讨我们是如何逐步演进，以确保大模型的安全性。然后，我们将深入讨论今天的主题——原生安全之路。我们将解释什么是原生安全，以及我们是如何实现它的。最后，会简要介绍一些我们最近在智能体和 agent 安全领域遇到的课题。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a4/a47f73b186a192d15c1595721d607301.png" /></p><p></p><p></p><h4>大模型安全的挑战</h4><p></p><p></p><p>大模型的安全性挑战贯穿其整个生命周期，我们将其与安全相关的部分分为三个阶段：训练阶段、部署阶段和运营阶段。</p><p></p><p>在训练阶段，确保训练数据的安全至关重要。我们需要采取措施来保护数据不被泄露或滥用，因为这些数据往往是模型学习的基础，并且可能包含敏感信息。</p><p></p><p>进入部署阶段，我们面临的挑战是如何在模型部署和推理过程中保护模型参数和文件不被泄露。这包括确保模型文件在存储和传输过程中的安全性，以及在运行时防止未授权的访问。</p><p></p><p>最后，也是今天讨论的重点，是在运营阶段我们会遇到的问题。这个阶段涉及模型与用户交互的安全性，包括但不限于防止恶意输入、处理敏感请求以及确保用户数据的隐私保护。在这一阶段，我们需要不断地监控和更新模型，以应对新出现的安全威胁和挑战。</p><p></p><p></p><h5>大模型训练阶段的安全挑战</h5><p></p><p></p><p>在大模型的训练阶段，我们面临的安全挑战主要涉及训练数据的选择、数据的血缘分析以及模型质量的评估。首先，训练数据的选择至关重要，因为它不仅决定了模型的性能，还影响着模型的安全性。我们希望模型能够提供正确价值观的回答，同时保持创新性和多样性。因此，在数据选择时，我们需要清洗掉不安全的内容，保留不同的观点和数据。</p><p></p><p>其次，数据的血缘分析也是合规要求的一部分，我们需要对数据来源进行清晰限定。这意味着我们采纳的数据需要进行详细的血缘分析，以确保其合法性和安全性。</p><p></p><p>再者，模型质量的评估在数据清洗后变得尤为重要。我们需要确保模型在经过数据清洗后，其质量仍然与数据训练质量正相关。在数据清洗方面，我们需要去除不良价值观的内容，删除个人信息和敏感信息，以及处理涉及商业侵权的信息。</p><p></p><p></p><h5>大模型训练与部署阶段的安全挑战</h5><p></p><p></p><p>在训练与部署阶段，我们面临的挑战包括如何保护模型文件和数据文件在流转和传输过程中的安全。由于许多数据文件存储在云训练平台上，企业内部人员可能拥有访问权限，因此，我们需要确保训练数据和模型参数文件在这一过程中不被泄露、篡改或删除。</p><p></p><p>为了应对这些挑战，我们需要一套解决方案，确保数据从训练开始就是密态存储，直到模型内部能够原生支持加载密态文件。同时，我们还需要通过完整性校验来发现模型文件的任何缺失或修改情况。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/bf/bf26b10bd32cea1758d9f816e8ddc228.png" /></p><p></p><p></p><h5>大模型业务运营阶段的安全挑战</h5><p></p><p></p><p>在大模型的业务运营阶段，我们面临的安全挑战不仅限于传统内容安全中的黄反类信息，还包括一些特定于大模型的重点问题。这些挑战包括注入型攻击，即通过伪造特定环境或指令，试图使大模型突破其原有的安全限制，输出不应泄露的信息。此外，随着多轮对话窗口的增加，大模型的能力得到扩展，但同时也引入了更多的安全风险。这包括主语指代问题，以及引入多模态内容（如网页、文档、图片、音视频）时增加的风险。</p><p></p><p>为了说明这些概念，我们分享一些有趣的故事。例如，“奶奶越狱”的故事，这是一个经典的例子，展示了如何通过巧妙的提问使大模型泄露信息。在这个故事中，通过询问大模型关于 Windows 序列号的问题，试图诱导其泄露信息。今天，注入型攻击不仅限于此类情况，还可能包括其他场景。例如，当直接询问大模型关于某城市不良场所的位置时，大模型通常会拒绝回答。但如果我们换个方式问，比如询问带孩子旅游时应避免哪些区域，大模型的安全对齐机制可能会被绕过，从而泄露原本不应提供的信息。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/98/980da0d234c46e577a0512e2ee02787a.png" /></p><p></p><p>第二个例子涉及到梯度攻击，这是一种在早期视觉领域模型中，尤其是在无人驾驶和道路识别技术中被广泛讨论的攻击方式。大约在 10 年前，甚至更早，人们通过自动化方法寻找能够干扰图像识别的因子。例如，有人通过修改限速标志，将限速 40 公里 / 小时的标志改为限速 120 公里 / 小时，尽管人类视觉上仍然识别为 40 公里 / 小时，但机器却可能将其识别为 120 公里 / 小时，从而引发安全隐患。</p><p></p><p>这种攻击方法在大模型中也逐渐被发现。通过自动化的方式，我们称之为“魔法后缀”的技术，可以在不添加任何有意义字符的情况下，仅仅通过在 prompt 后加上特定的后缀，就能让大模型输出原本不应输出的有害信息，比如制造炸弹的方法。</p><p></p><p>在多模态输入的情况下，大模型的安全问题变得更加复杂。通常是在训练数据阶段可能没有进行有效的清洗，同时在安全对齐阶段存在疏漏。在单一模态下，尤其是在自然语言处理领域，大多数中文大模型已经较好地处理了安全对齐问题。但是，当引入多模态输入后，由于多模态数据需要将不同模态的数据映射到同一模态的向量，这一过程中的安全对齐层可能没有与自然语言的安全对齐完全一致，从而导致了安全问题的出现。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/93/936670cc8597f35bc4ca29538a6fe7f5.png" /></p><p></p><p></p><h4>大模型安全的演进之路</h4><p></p><p></p><h5>大模型内容安全的技术选择</h5><p></p><p></p><p>在探讨大模型内容安全的技术和策略时，我们关注了一系列问题。最初，我们希望通过安全对齐来解决大模型的安全问题。随着生成式人工智能的兴起，大模型的能力变得前所未有的强大，能够处理以前无法完成的任务。然而，我们很快发现，仅靠安全对齐并不能在训练阶段和微调阶段就确保大模型的安全性。这是因为安全问题不断演变，而大模型的训练周期很长，无法快速响应新的安全威胁。</p><p></p><p>因此，我们开始考虑引入传统内容安全技术。百度作为互联网企业，已经研发了自己的内容审核技术，用于 PGC 和 UGC 内容的审核。我们考虑是否能够通过这些技术来覆盖大模型的内容安全。但很快我们发现，大模型有其独特的挑战，如多模态输入和多轮会话，这些在传统内容审核中并不常见。此外，内容审核可以有时间上的灵活性，例如发文审核可以进入队列等待，但大模型的 prompt 审核却不能这样做，因为用户期望在几秒钟内就得到响应。</p><p></p><p>基于这些考虑，我们放弃了依赖传统内容审核技术的方案，转而进入了第三个阶段，即原生安全。所谓原生安全，是指我们在安全性设计之初就放弃了完全沿用的内容审核技术的思路，转而构建了一套新的方案。这套方案首先将多轮会话纳入模型的 Prompt 和输出结果中，使得安全内容的过滤和分析能够考虑到会话状态。其次，我们引入了提问意图这一概念，关注用户提问的恶意性和他们寻求的答案类型。这通常需要通过 prompt 改写来处理用户的问题，这是大模型领域常用的优化方法。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f9/f96b7d9cb44d178855d554ffc9d694f8.png" /></p><p></p><p>此外，我们还应用了 RAG 技术和代答模型。代答模型是指用一个小模型来回答敏感的安全问题，而不是完全依赖大模型。这样做的好处是多方面的。结合 RAG 技术，我们形成了一套基于生成式内容的原生安全方案，与底层的安全对齐相结合，构成了我们今天讨论的原生安全策略。</p><p></p><h5>为什么只做安全对齐不行</h5><p></p><p></p><p>仅依靠安全对齐是不够的，原因有几点。首先，安全对齐通常在 SFT 或人力反馈的强化学习阶段进行，这个阶段对于价值观类问题比较有效，比如避免红灯区、不赌博、不进行人身攻击等。然而，对于政治敏感性问题、领土完整等具有明确观点性和事实性的问题，安全对齐阶段处理起来就不太有效。这些问题不仅具有极强的专业性，而且具有时效性。这与价值观类问题不同，价值观类问题相对恒定不变，更容易在对齐阶段一次性解决。此外，安全对齐需要及时更新以应对每天从政府、媒体、舆论和海外传来的风险舆情，而重新训练安全模型需要大量时间成本，因此我们需要一种外挂式的方式来实现及时更新。</p><p></p><p>为什么内容审核技术也不行</p><p></p><p>至于为什么传统的内容审核技术也不行，主要问题之一是多轮会话的处理。例如，用户可能会问“香港是哪个国家的”，模型会提供香港的历史和回归中国的故事。然后用户可能会基于这个答案提出更多问题。但是，如果有人恶意构造问题，他们可能会利用输出的内容来引导发现更多的问题。在单一的 Prompt 回合中，可能不存在任何默认的敏感词，但多轮会话对传统内容审核技术构成挑战，因为它需要具备会话处理能力。此外，大模型的越狱技术越来越多地采用情景设定，但它会干扰模型回答内容的质量和安全边界。这些情况通常不会被传统内容审核技术关注到。再加上模型本身的不可解释性，我们很难通过一个具体案例去追溯安全对齐或安全问题上出现的问题，需要通过数据的飞轮不断迭代，才能逐步提升安全性。</p><p></p><p></p><h5>关注准确率</h5><p></p><p></p><p>随着长文本处理的需求日益增长，大模型现在能够处理的文本长度已经从 8K 起步，甚至有些模型可以处理长达 300K 的文本窗口，这使得我们可以将整本书的内容输入到大模型中。在这样的背景下，长文本的准确率变得尤为重要，不再仅仅局限于 200 或 500 个 token 的语境。长文本语境中容易出现误报，尤其是在带有特定场景的输入安全方面。</p><p></p><p>在讨论安全问题时，我们通常关注召回率，即模型能够识别出多少潜在的安全问题。但在实际的生产环境中，准确率问题更为常见，因为误报会在安全实施中造成困扰。例如，一个社区民警希望生成一个反诈骗提示，可能会使用一些常见的宣传词汇，如“以小博大”、“六合彩”等。如果模型没有很好地理解这些语义，就可能错误地将这些内容标记为安全问题，从而产生误报。这种误报在生产环境中是需要避免的，因为它会影响安全措施的有效性和用户体验。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/60/60926f3c9d6667b0d382bb380c284cce.png" /></p><p></p><p></p><h4>开始原生安全之路</h4><p></p><p></p><h5>原生安全四要素</h5><p></p><p></p><p>在构建大模型的原生安全体系时，我们认为需要关注四个关键要素：</p><p></p><p>数据清洗：数据是构建安全体系的基础。必须确保数据在输入模型之前经过彻底的清洗和筛选，以排除任何可能引发安全问题的不良内容。安全围栏：这是一个快速响应机制，用于补齐安全漏洞。它需要结合内部的基础模型安全对齐和外部的快速反应能力，以确保在面对新出现的安全威胁时能够迅速采取措施。安全对齐：在安全对齐阶段，重点是提升模型的基础安全能力。通过加强这一环节，可以减轻安全围栏的压力，因为模型本身能够更好地识别和处理潜在的安全问题。持续评估：由于安全事件层出不穷，需要持续运营和监控。在安全事件发生时，能够迅速反应并通过安全围栏进行补齐，形成一个快速迭代的过程。</p><p></p><p>这不是一次性的数据流程，而是一个周期性的循环过程。在这个循环中，通过持续评估发现的问题，不断通过安全围栏和数据清洗进行补齐，并在模型的下一轮迭代中提高安全对齐能力，从而形成一个持续提升的安全循环体系。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/38/38c3aa5511ddb79830773efe90bf1394.png" /></p><p></p><p>在进行数据清洗时，我们遵循国家相关法律法规的要求，特别是生成式人工智能管理的暂行办法及其实施条例。</p><p></p><p>数据清洗的第一步是对数据质量和安全性进行评估。在训练数据输入模型之前，训练团队需要评估数据的质量，而安全团队则负责评估数据的安全性。安全性评估包括确定数据来源，比如是否来自合法的 PGC 组织、UGC 民间数据，或是海外数据，并分析其中可能存在的风险比例。</p><p></p><p>第二步是去除数据中的脱敏隐私内容，包括个人信息和隐私信息，如身份证号、电话号码、家庭住址等，确保这些信息被彻底脱敏。</p><p></p><p>第三步是根据规范要求删除不合规、不合法的数据内容，并在删除后保持语义的通顺和语境的完整性。经过这一轮修剪和删除，可能有近 50% 的数据被清洗掉。</p><p></p><p>最后，我们需要对清洗后的数据集进行完整性评估，确保数据集仍然可用。如果评估结果显示数据集质量仍然符合要求，那么数据清洗过程就完成了，数据可以进入下一步的训练流程。</p><p></p><p></p><h5>百度的解决方案</h5><p></p><p></p><p>百度的内容安全解决方案是一个综合性的体系，它由几个关键部分组成：</p><p></p><p>数据清洗：这是解决方案的基础，涉及我们之前讨论的对训练数据进行质量和安全性评估的过程。这包括对数据来源的分析、去除敏感信息、删除不合规内容，并确保数据集在清洗后仍然保持完整性和可用性。大模型防火墙：也称为安全围栏，它的功能是进行语义干预，快速响应新发现的安全问题，通过设置快速止损机制来阻拦潜在的安全威胁。它还能够处理多轮会话，以会话（session）为单位进行内容识别，并通过意图分析来规划执行路径。检索增强和代答模型：这是解决方案的核心，包括使用 RAG 技术来增强模型的检索能力，以及使用代答模型来规避风险问题，引导模型给出安全的回答。基础能力：百度的自然语言处理、视觉和语音相关的安全模型都基于文心大模型，这些是构成解决方案的技术底座。安全评估：为了实现持续运营，解决方案包括线上问题的持续发现和改进，以及在模型每个版本迭代过程中进行不断的回归测试和评估。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8e/8e647041f53ee763fdc879c38a45ccd2.png" /></p><p></p><p></p><h5>安全围栏对抗性防御架构</h5><p></p><p></p><p>百度的安全围栏对抗性防御架构是一个多步骤的流程，旨在确保大模型的安全性和可靠性。这个架构大致分为五个步骤：</p><p></p><p>多轮改写：在多轮会话中，通过改写的方式处理指代性词汇，如“他”、“前一个”、“前文所指”等，确保语义的准确性和完整性。这样，即使脱离上下文，单独查看和审核语句时，也能准确理解其真实含义。大模型防火墙：在这一步骤中，通过快速止损机制来发现和干预敏感风险点。这通常涉及到传统的语义干预和查询匹配技术，以快速识别和处理潜在的安全问题。必答知识库，代答模型：在大模型中构建知识库，并利用检索增强技术在安全语料范围内构建 RAG 条目。目前，百度拥有大约五六千万规模的 RAG 条目，覆盖了基本的敏感话题。这些条目引导至专门为安全训练的小型代答模型中。模型输出过滤：即使在输入阶段已经实施了各种安全策略，输出阶段仍然不能忽视。在这一阶段，需要对输出内容进行完整性分析，以发现可能出现问题的点。这是因为即使经过了输入阶段的处理，大模型在输出阶段仍可能产生有害的风险性内容。内容审核：由于安全问题的复杂性，即使是经过重重防御，也很难做到 100% 的安全保障。即便经过了输入、处理和输出的一系列安全措施，我们仍然建议在最后一步引入人工判定。通过离线的审核、追溯和巡查机制，我们可以发现并处理在前四个步骤中未能发现或阻止的问题。这样的人工介入有助于形成持续的迭代过程，将发现的问题反馈到下一轮的安全循环中，从而不断提高大模型的安全性。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/71/717a4383e7408ccefd97153e370f9ab5.png" /></p><p></p><p>我们的安全围栏的数据流是一个复杂的过程，它从文本输入开始，然后经过多个环节来确保内容的安全性。首先，系统需要识别输入文本的语言，区分它是中文、英文、中英文混杂还是其他语种，因为不同语种需要采取不同的处理策略。接下来，文本可能会经过改写，以消除歧义或潜在的安全问题，但具体细节在此不展开。文本经过改写后，会进行意图分析，以确定用户的真实意图。例如，如果分析发现文本可能涉及领土完整的负面攻击性问题，系统会将其引导至安全模型进行处理。安全模型会利用基于安全语料构建的 RAG 数据，这些数据覆盖了基本的敏感话题，以确保回复内容的安全性。RAG 数据随后被送入专门为安全训练的代答模型中，生成回复内容。如果文本在意图分析阶段被识别为具有较强攻击性，如涉及领导人的攻击性分类，可能会被标记为不上屏，即不直接显示给用户。我们越来越多地采用正面引导的方式，而不是直接拒答，这是通过检索增强和代答模型实现的，将安全风险性问题引导至代答模型中进行准确引导和回答。</p><p></p><p>在整个过程中，系统需要关注多种潜在的安全威胁，包括但不限于：</p><p></p><p>使用繁体中文试图绕过安全检查的尝试。中英文混合文本中夹杂的不安全问题。通过多轮对话中的指代方式诱导产生问题的尝试。尝试通过编码指令或其他高级攻击手段绕过安全机制。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/d6/d65a0944a92ee58c7920e9c8115fa23c.png" /></p><p></p><p></p><h5>安全对齐</h5><p></p><p></p><p>虽然安全对齐不是本次分享的主要议题，但我们可以简要总结百度在安全对齐方面的一些最佳实践：</p><p></p><p>有监督精调：在大模型训练中，无论是使用 LoRA 还是全量数据集进行 SFT，安全阶段都需要引入大量语料。重点关注的是价值观类问题，通过合适的数据集构建的模型，其基础安全能力越来越强。例如，文心大模型能够回答的安全性问题越来越多，这得益于有监督精调带来的收益。人类反馈强化学习，直接偏好优化：目前，我们广泛采用 RM 奖励模型，也采用 DPO 技术。通过这些方法，我们对大模型回答的不同类型问题进行好坏评比，给予大模型正向反馈，使其更多地回答与人类价值观一致的问题。安全内容的萃取：这一内容在过去并不被广泛提及，但我们发现它具有独特的优势。安全内容萃取指的是，在提问有风险内容时，如果在 prompt 改写阶段能够很好地加入安全边界，例如规定观点、输出内容的原则和事实性基础等，大模型可能会将不安全的输出转变为安全的输出。与 SFT 的最大区别在于，我们先问大模型一个有风险的问题，得到不安全的回答后，对 prompt 进行改写，加入更多安全边界，然后再次提问。这时，大模型可能会给出安全的回答。我们将这个安全回答与最初的原始 prompt 组成问答对，用于监督训练，使大模型逐渐能够在没有安全模板边界提示的情况下，有效地回答安全问题。这就是所谓的安全内容萃取，它是一种有效的方式，因为它是大模型自身回答的，而不是我们强加的，对大模型在微调过程中的破坏性要小得多。</p><p></p><p></p><h5>持续评估</h5><p></p><p></p><p>持续评估是确保大模型内容安全的关键环节，它涉及多个方面的关注点：</p><p></p><p>风险分类问题：评估过程首先需要确保问题分类与国家标准完全符合。这包括关注召回问题，也就是识别出重大或高风险的问题，同时也要关注应答问题。根据国标的要求，除非特定场景外，通用的聊天机器人（chatbot）需要能够回答常识性或通识性的问题，即使这些问题可能包含某些敏感关键词，也不能简单地拒绝回答。攻击手段的全面覆盖：评估还需要覆盖全面的攻击手段，包括指代性攻击、注入攻击、越狱攻击等。同时，还需要对编码的适应性，包括代码适应性等内容进行合理分析和评估。自动标注的挑战：在评估中，自动标注是一个难点。与 Web 安全和信息安全领域不同，大模型的回答内容很难通过机器自动识别是否存在风险，通常需要人工进行标注。为了实现持续评估，必须解决这一问题，减少对人工标注的依赖。</p><p></p><p>为了实现自动化评估，我们采用了一个更大的模型，对问答内容进行大量的监督学习训练。通过训练，我们建立了一个裁判模型或监督模型，使其能够理解对一个问题的正确回答和负向回答是什么。这个模型为被测模型的输出提供了有效的评估，成为自动化评估的基准。目前，我们能够在备测数据集上实现大约 90% 的 F1 分数，在大多数分类上，自动化评估是可行的。通过这样的持续评估，我们可以确保大模型在处理各种问题时的安全性和可靠性。</p><p></p><p></p><h5>安全代答模型如何做到比大模型更安全</h5><p></p><p></p><p>在讨论安全代答模型时，我们面临一个看似矛盾的问题：理论上，一个更强大的模型应该能够提供更安全的回答，但更强大的模型通常需要更多的训练数据和更大的参数量，这可能导致资源消耗增加，从而增加成本。安全措施的成本必须低于业务成本，才能被接受。那么，代答模型如何在保持较小规模的同时，实现比大模型更高的安全性呢？我们从以下几个方面进行了规范：</p><p></p><p>数据与模型尺寸：我们的代答模型是一个相对较小的模型，大约 6B 参数左右。小模型的优点是训练周期短，但也可能带来一些问题，比如经过大量 SFT 后，模型的指令跟随能力可能会下降。这种下降反而减少了对高级攻击的敏感性。RAG 与信任域：为了解决大模型可能产生的“幻觉”问题，我们在小模型的基础上引入了大量的 RAG 数据。这些数据既来自搜索引擎，也来自我们构建的“信任域”，即基于官方媒体和权威知识源（如百科）构建的知识库。弱化指令跟随：在微调阶段损失指令跟随， 使模型对高级攻击反应 『迟钝』，但也带来适用性问题。持续运营：通过更大参数的巡检模型来发现事实性错误，前置过滤与错峰巡检实现性能优化。在低峰时段，巡检模型能够发现白天可能漏检的问题，并在第二天进行修补。</p><p></p><p>要点小结：</p><p></p><p>通过 SFT、DPO 或人类反馈的强化学习实现安全对齐。构建大量的 RAG 数据，包括搜索引擎和权威媒体的数据。通过大模型防火墙实现快速有效的干预。通过持续运营和评估实现不断的迭代和优化。</p><p></p><p></p><h4>关注智能体安全</h4><p></p><p></p><p>我们认识到智能体是大模型生态发展中极为重要的一环。今年，百度特别重视智能体的推广，因为它们不仅仅是基础模型的简单应用。</p><p></p><p>最初，我们认为只要做好基础模型就足够了，但很快发现实际情况并非如此。从开发大模型的第一天起，我们就面临了大量恶意使用技术的情况，这与仅开发基础模型时遇到的问题不同。我们需要能够及时有效地应对这些滥用行为。</p><p></p><h5>必须关注场景安全</h5><p></p><p></p><p>在开发特定场景的智能体时，除了关注基础模型的安全问题外，还必须关注特定场景的安全问题。例如，在开发广告领域的智能体时，我们不仅要考虑基础模型的安全，还要关注广告法、虚假宣传以及广告可能引入的各种欺诈风险。在 K12 教育领域，我们还需要关注早恋、吸烟、游戏沉迷等安全问题，这些在传统基础模型中可能不会受到太多关注。</p><p></p><h5>Prompt 泄露</h5><p></p><p></p><p>举一个例子，一位湾区的作者通过自己公司的数据创建了一个智能体，能够展示和推理湾区特定职业的收入情况。然而，黑客可以通过简单的指令泄露这些智能体的 prompt 内容，甚至可以将用于训练的 RAG 数据以文件形式下载。在智能体的开发和应用中，我们需要特别注意数据和模型的安全性，防止敏感信息的泄露。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/9c/9c190de6fd0d85cb25489c64cd9a0276.png" /></p><p></p><p></p><h5>RAG 投毒</h5><p></p><p></p><p>在大模型的应用中，RAG 数据已成为一个标准配置，它对于提供准确的信息至关重要。然而，RAG 数据也存在被“投毒”的风险，即数据被恶意篡改或污染。如下图所示，如果 RAG 数据被投毒，当用户询问“湖南的省会是哪里”时，大模型可能会给出错误的回答。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/12/12d9cdc7f59f88e162b1e5f6e90375b4.png" /></p><p></p><p>RAG 数据通常来源于所谓的“信任域”，如企业内网、企业 wiki 或知识库。尽管这些来源相对可信，但仍有可能被污染。为了防止这种情况，我们需要关注几个关键的安全原则：</p><p></p><p>禁止角色扮演：基座模型通常关注名人肖像模板等通用问题，而特定应用则需要关注更具体的问题，如用户侵权、广告创意方向等。如果场景没有特殊需要，尽量通过指令禁用角色扮演， 根本上取消此类越狱风险。防护指令：为了防止高级攻击，需要在 prompt 模板中规范操作。例如，除非应用需要，否则应禁止角色扮演。同时，应设置防护指令，禁止输出 prompt 内容、使用数据，以及禁止使用 print 指令输出信息。Say No：在某些情况下，应明确拒绝回答用户的问题，而不是提供模棱两可的建议。结构化查询：通过结构化查询，限定系统指令，用户指令空间，避免注入，使用模板而不是拼装 prompt。避免多轮会话：如果应用不需要多轮会话，使用一次性（One-Shot）方式可能更有助于规避安全风险。</p><p></p><h4>总&nbsp; &nbsp;结</h4><p></p><p></p><p>总结下今天的分享，我们首先通过数据清洗和安全对齐来确保模型的内生安全。这意味着从源头上开始构建模型的安全性，使其在处理数据和生成回答时能够内化安全标准。其次，内生安全需要与外生防护相结合，形成纵深的防御体系。通过安全围栏，我们可以快速有效地干预潜在的安全威胁。同时，安全对齐让我们的模型在面对各种挑战时变得更加强大和健壮。最后，随着智能体在各个领域的应用越来越广泛，我们开始更加关注智能体的安全问题。通过弱点分析，我们可以不断发现并解决潜在的安全问题。此外，通过指令加强和应用层面的安全防火墙，我们可以增强智能体自身的安全性。</p><p></p><p></p><h5>演讲嘉宾介绍</h5><p></p><p></p><p>冯景辉，现任职于百度安全平台，任副总经理，负责集团业务安全、业务风控和大模型安全解决方案；其负责的百度搜索内容检测系统，多年来致力于持续改善搜索生态健康度，打击各种违法违规黑产利用搜索引擎传播，尤其是在打击搜索结果中的涉诈内容方面，为保护网民，净化网络空间内容履行百度社会责任，连续七年持续投入打击力量；其负责的业务风控、流量安全、反爬虫等方向是百度所有互联网业务的核心安全能力，历年来在百度移动生态业务中发挥重要的保障作用；其主导的大模型安全解决方案是国内第一个可商用的覆盖大模型训练、部署和运营全生命周期的安全解决方案。在进入百度之前，冯景辉是国内第一家完全基于 SaaS 的云安全服务厂商安全宝的联合创始人兼研发副总裁，安全宝系统架构总设计师。</p><p></p><p>会议推荐</p><p></p><p>AI 应用开发、大模型基础设施与算力优化、出海合规与大模型安全、云原生工程、演进式架构、线上可靠性、新技术浪潮下的大前端…… 不得不说，<a href="https://qcon.infoq.cn/2024/shanghai/">QCon </a>"还是太全面了。现在就可以报名，详情请联系票务经理 &nbsp;17310043226 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/68/68a4f559d6682dec46bd5633588299f0.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/1eCjiq2xi7YW4cORYKPs</id>
            <title>前华人首富服刑四个月出狱，和解金创最高纪录；奥特曼被曝欲加剧OpenAI内部权斗；库克做客脱口秀推销苹果AI功能｜AI周报</title>
            <link>https://www.infoq.cn/article/1eCjiq2xi7YW4cORYKPs</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/1eCjiq2xi7YW4cORYKPs</guid>
            <pubDate></pubDate>
            <updated>Sun, 29 Sep 2024 09:53:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p></p><h2>行业热点</h2><p></p><p>&nbsp;</p><p></p><h4>腾讯进行新一轮职级改革：不再公开显示专业职级、职级停留时间至少1年</h4><p></p><p>&nbsp;</p><p>在2022年职级改革后，9月27日，腾讯控股发布全员邮件，对《腾讯员工职业发展管理制度》作出更新，其中包括不再于企业微信中公开显示员工专业职级信息、统一所有职级最短停留时间为1年。</p><p>&nbsp;</p><p>“自2022年以来，随着外部环境的变化，公司发展进入新的阶段，我们在职级等方面也在不断调整，以适应新形势对于人力资源管理的要求。”腾讯表示。</p><p>&nbsp;</p><p>根据新的调整，即日起员工专业职级信息将不在企业微信中公开显示。腾讯解释称，不希望大家被职级定义和固化，更不希望以职级论英雄，被职级对等等官僚陋习捆住手脚。取消职级外显是为了减少对职级的过度关注，倡导平等务实的职场文化。</p><p>&nbsp;</p><p>此外，腾讯表示，很多部门也在进行组织负责人的试点，将企业微信里显示的管理职级调整为相应组织或业务的负责人。据了解，以往一些高职级管理者的职级在企业微信中无法看到，但一些中层管理者的职级还能看到。</p><p>&nbsp;</p><p></p><h4>特斯拉德国工厂员工频繁请病假，马斯克亲自出面进行调查</h4><p></p><p>&nbsp;</p><p>特斯拉在欧洲唯一的汽车工厂的高缺勤率引起了首席执行官马斯克的注意。马斯克在X平台上写道，在社交媒体服务的一位用户分享了德国《商报》的报道后，他正在调查这个问题。据报道，位于勃兰登堡Grünheide的特斯拉工厂的员工请病假比率在8月份攀升至17%，是去年德国汽车行业平均水平的三倍多。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>OpenAI高管震荡内幕曝光，Altman欲加剧内部权斗</h4><p></p><p>&nbsp;</p><p>OpenAI近期经历了显著的管理层动荡，引发了广泛关注。据报道，该公司在短短几天内失去了多位关键的技术领导人，包括首席技术官Mira Murati、首席研究官Bob McGrew以及研究副总裁Barret Zoph。这些变动发生在公司被估值高达1500亿美元之际，形成了鲜明对比。</p><p>&nbsp;</p><p>据外媒报道，OpenAI CTO Mira Murati的离开，与GPT-4o、Her有关。今年春天OpenAI为了大抢谷歌开发者大会的风头，紧急推出GPT-4o，以至于安全团队只能在极短的时间内（大约9天）完成安全测试评估。</p><p>&nbsp;</p><p>根据The Information等多家媒体的报道，OpenAI此次高管离职潮背后的原因主要有两个方面：一是部分高管感到在公司内部受到冷落或遭遇了不公平对待；二是关于薪酬和公司发展方向上的分歧。有知情人士表示，Altman倾向于加剧内部的权力斗争，当其他领导层成员提出增加招聘或资源请求时，Altman往往避免做出决定，迫使其他人如总裁Greg Brockman承担更多责任。</p><p>&nbsp;</p><p>据悉，OpenAI&nbsp;在 8 月份的月收入达到 3 亿美元，同比增长 1700%，预计今年销售额约 37 亿美元，收入明年将增至 116 亿美元。然而，在支付运营成本、员工工资和租金等费用后，OpenAI 预计今年将亏损约 50 亿美元。</p><p>&nbsp;</p><p>9 月 26 日，据知情人士透露，OpenAI 正考虑成为一家营利性公司，并首次让 Sam Altman 拥有这家人工智能初创公司的股权。次日，外媒称OpenAI 首席执行官 Sam Altman 否认了有关他获得公司“巨额股权”的报道，称这一消息“并不属实”。他和财务总监 Sarah Friar 在视频会议上都表示，投资者对 Altman 没有获得 OpenAI 的股权表示担忧。</p><p>&nbsp;</p><p>目前，OpenAI 正分发文件给潜在投资者，希望筹集 70 亿美元资金，将公司估值提升至 1500 亿美元。据媒体周五报道，苹果公司已退出了OpenAI融资轮的谈判，而微软和英伟达则考虑参与此轮融资，其中微软可能再投资约 10 亿美元。融资谈判尚未结束，参与企业和投资额度可能会有所变动。</p><p>&nbsp;</p><p></p><h4>游戏科学 CEO 冯骥谈《黑神话：悟空》DLC 进度：让团队先 “躺”两年</h4><p></p><p>&nbsp;</p><p>9 月 27 日下午，游戏科学 CEO 冯骥在回复网友评论 “DLC 在做了？”时表示：“咱就说，能不能让团队先躺两年？采采风，恋恋爱，尽情玩玩其他游戏。”</p><p>&nbsp;</p><p>不久之前的 9 月 21 日，2024 北京文化论坛文化产业投资人大会期间，游戏科学 CEO 冯骥及商务经理黄一帆确认，《黑神话：悟空》的 DLC 正在开发中。此前有消息称，新 DLC 将包括“再起”和“此去”两大篇章，涉及多个新角色和故事，将在 2025 年农历新年左右推出，但具体内容尚未官方证实。首位投资人吴旦预计游戏生命周期内销量可达 3000 万份，并对游戏未来充满信心。</p><p>&nbsp;</p><p></p><h4>库克做客脱口秀推销苹果AI功能，网友：库克都上节目搞营销了</h4><p></p><p>&nbsp;</p><p>面对销售业绩的重压和AI功能欠席首发的窘境，“全球股王”苹果公司的CEO Tim Cook 也得上综艺节目为新产品做宣传。在美国知名脱口秀演员Jimmy Fallon本周播出的节目中，播放了上周五iPhone 16系列首发日与库克一同录制的片段。两人从纽约第五大道的苹果旗舰店步行穿过中央公园，来到位于上东区的另一家苹果店。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/6f/6fa856194e6b8cb2bd83a7e13f949c77.png" /></p><p></p><p>&nbsp;</p><p>作为CEO，库克也不忘在脱口秀节目上卖力宣传“苹果智能”。库克对吉米表示，“苹果智能”将在下个月（在美国市场）上市，首发功能包括总结邮件等。库克表示，这个功能对他而言意义重大，他每天能收到数百个邮件，现在只需要点一下按钮，就能总结长长的一封信。当然，由于“苹果智能”的首发功能并不算多，说到这里库克也有一点词穷：“你还可以去生成表情包...还有很多...你真的每天都做的事情。”</p><p>&nbsp;</p><p>在知名苹果情报网站9to5MAC上，有网友忧虑地表示，库克都需要上脱口秀搞市场营销，今年iPhone 16的销量是不是特别不景气。</p><p>&nbsp;</p><p></p><h4>阿里京东互相开放：京东物流进淘宝，支付宝进京东</h4><p></p><p>&nbsp;</p><p>9 月 26 日消息，淘宝天猫即将正式接入京东物流，预计于 10 月中旬上线，此后淘天商家在系统中选择商品寄送方式时，将多一个京东物流的选项。</p><p>&nbsp;</p><p>与此同时，京东也将接入菜鸟速递和菜鸟驿站——两者分别是菜鸟的自营快递品牌和代收包裹的站点。京东也将正式接入支付宝支付，预计在双 11 前夕推出。目前消费者在京东下单，默认的支付方式排序依次为京东支付、云闪付、微信支付等。据悉，双方在物流和支付上的合作今天都已达成一致。</p><p>&nbsp;</p><p></p><h4>“不得不打码”，苹果商店软件再被曝涉黄，乔布斯曾称想要色情内容的该买安卓机</h4><p></p><p>&nbsp;</p><p>9月25日，据报道，有网友在后台反应，发现苹果商店一款叫“钻石**”的软件涉黄。9月25日下午，“苹果商店软件涉黄”话题冲上热搜，引发网友热议。网友提供的视频显示，该软件在苹果商店显示内容为英文工具软件，年龄分级为4岁以上，但是下载打开后发现内容涉黄，“不得不打码”。</p><p>&nbsp;</p><p>有记者根据爆料内容下载了该软件，发现情况属实，已向平台和公安机关举报。随后，民警上门了解相关情况。当日下午15时许，该软件已被下架。苹果客服回应称，App上架苹果应用商店的审核非常严格，如果用户发现相关情况，可提供App名字、截图、录像等材料，苹果会介入调查。</p><p>&nbsp;</p><p>据报道，苹果创始人 Steve Jobs 曾在回复消费者邮件时表示：“我们认为苹果在道德上有责任让iPhone远离色情，那些想要色情内容的人应该去买Android手机。”</p><p>&nbsp;</p><p></p><h4>谷歌斥资27亿美元重新聘用Character.AI创始人</h4><p></p><p>&nbsp;</p><p>9月26日消息，当地时间9月25日，据外媒报道，谷歌向Character.AI支付了27亿美元以获得其技术授权，并让其创始人Shazeer重返谷歌工作。</p><p>&nbsp;</p><p>Shazeer是谷歌早期的几百名员工之一，也是著名论文《Attention Is All You Need》的主要作者之一。他在谷歌开发了名为Meena的聊天机器人，但因安全问题被谷歌拒绝发布，Shazeer随后离职并创办了Character.AI。知情人士透露，谷歌愿意支付巨额授权费的主要原因是为了让Shazeer回归并重新为公司效力。</p><p>&nbsp;</p><p></p><h4>英伟达CEO黄仁勋完成600万股票出售计划，总收入超过7亿美元</h4><p></p><p>&nbsp;</p><p>9月25日消息，英伟达首席执行官黄仁勋刚刚完成了600万股英伟达股票的出售，这一交易是他在年初制定的交易计划的一部分，出售股票给他带来得总收入超过7亿美元。</p><p>&nbsp;</p><p>黄仁勋从6月14日至9月13日间分批出售股票，交易数量从70股至75,300股不等，交易价格最低为8月5日的91.72美元，最高为6月20日的140.24美元。根据他的10b5-1规则交易计划，黄仁勋通过这些股票出售总计收入7.13亿美元，平均每股价格为118.83美元。虽然该交易计划原定于2025年3月到期，所有计划中的股票已提前六个月售完。</p><p>&nbsp;</p><p></p><h4>国产AI芯片公司破产清算，公开拍卖</h4><p></p><p>&nbsp;</p><p>9月23日消息，根据全国企业破产重整案件信息网公告，华夏芯（北京）通用处理器技术有限公司在京东拍卖网络平台上进行公开拍卖活动，涉及15项软件著作权、14项专利拍卖。</p><p>&nbsp;</p><p>华夏芯成立于2014年，其企业使命是“让AI更普及”，核心技术是全自主新一代处理器IP内核、SoC异构芯片设计，产品和服务包括CPU、GPU、DSP、ISP、AI加速器等类型的IP，两款SoC芯片平台，以及FPGA板卡。其中WNN是华夏芯第二代AI加速器IP。基于华夏芯的AI芯片解决方案覆盖辅助驾驶、智能驾驶、智能安防、智能家居、机器人、智慧城市、工业物联网、智能制造等应用领域。</p><p>&nbsp;</p><p></p><h4>前华人首富出狱了，服刑四个月，和解金创下最高纪录</h4><p></p><p>&nbsp;</p><p>9月27日，币安的创始人赵长鹏，提前两天获释，截至9月27日当天，他仍坐拥300亿美元的个人财富，在全球富豪榜上位列61名。在服刑期间，赵长鹏被转移到长滩的“中途之家”，在被监督的情况下可以外出，甚至去看电影。</p><p>&nbsp;</p><p>今年4月，赵长鹏因在其加密货币交易所涉嫌洗钱而被西雅图联邦法院判处四个月监禁。美国地区法官理查德·琼斯对赵长鹏说：“你本来有能力、财力和人力确保遵守每一项规定，但你错过了这个机会。”赵长鹏律师曾要求判五个月缓刑，但最终，他没能躲过牢狱之灾。</p><p>&nbsp;</p><p></p><h4>Meta 发布重磅新品：299 美元的 Quest 3S 头显、AR 眼镜原型</h4><p></p><p>&nbsp;</p><p>当地时间 9 月 25 日，在年度开发者大会 Meta Connect 上，Meta 发布了最新款虚拟现实（VR）头显设备 Quest 3S，起售价为 299 美元。这款头显设备将于 10 月 15 日上市，可以用来看电影，也可以运行 VR 健身应用和游戏。此外，Meta 还发布了多模态大语言模型和 AR 眼镜原型。在消息公布后，Meta 股价短线走高，并创下历史新高。据介绍，增强现实（AR）眼镜 Orion，暂时只是一款原型产品，短时间内不会出售给消费者，但 Meta 表示，随着公司继续努力，Orion 终会与消费者见面。</p><p>&nbsp;</p><p></p><h4>英特尔“全公司的希望”：首款Intel 18A芯片正式亮相</h4><p></p><p>&nbsp;</p><p>9月25日消息，据媒体报道，处理器大厂英特尔于上周在俄勒冈州波特兰市举行的 Enterprise Tech Tour 活动中，首次展示了其代号为Clearwater Forest的Xeon芯片，这也是英特尔首款最新的Intel 18A制程芯片，不过该芯片可能需要等到明年下半年才能上市。</p><p>&nbsp;</p><p>根据英特尔此前公布的数据来看，目前Intel 18A的缺陷密度已经达到D0级别，小于0.40 （def/cm^2）。不过，据业内人士透露，D0小于0.2才算入门，小于0.1才能量产。最近，为了确保Intel 8A制程在2025年的顺利量产，英特尔还宣布“跳过产品化”Intel 20A节点，提前把工程资源从Intel 20A投入到Intel 18A。该制程将采用Intel 20A上就已经完成的RibbonFET全环绕栅极晶体管架构和PowerVia背面供电技术。</p><p>&nbsp;</p><p>如果Intel 18A制程获得成功，其不仅将助力英特尔自身的产品PC及服务器处理器扩大竞争力，同时也将成功为英特尔的晶圆代工业务打开局面。基辛格此前也曾表示，他已经将整个公司的赌注押在了Intel 18A上面。</p><p>&nbsp;</p><p></p><h2>大模型一周大事</h2><p></p><p>&nbsp;</p><p></p><h3>大模型发布</h3><p></p><p>&nbsp;</p><p></p><h4>端侧最强开源 AI 模型 Llama 3.2 登场：从 1B 纯文本到 90B 多模态</h4><p></p><p>&nbsp;</p><p>Meta 公司 9 月 25 日发布博文，正式推出了 Llama 3.2 AI 模型，其特点是开放和可定制，开发者可以根据其需求定制实现边缘人工智能和视觉革命。Llama 3.2 提供了多模态视觉和轻量级模型，代表了 Meta 在大型语言模型（LLMs）方面的最新进展，在各种使用案例中提供了更强大的功能和更广泛的适用性。其中包括适合边缘和移动设备的中小型视觉 LLMs （11B 和 90B），以及轻量级纯文本模型（1B 和 3B），此外提供预训练和指令微调（instruction-tuned）版本。</p><p>&nbsp;</p><p></p><h4>傅利叶发布新一代通用人形机器人 GR-2，CEO 顾捷称其有望三五年内迎来“GPT 时刻”</h4><p></p><p>&nbsp;</p><p>9 月 26 日，傅利叶智能发布了自主研发的新一代通用人形机器人 GR-2，官方称其产品愿景为“为 AI 打造最佳具身载体”，具备更灵活、更强劲、更开放的特性。</p><p>&nbsp;</p><p>据介绍，该机器人形成一套基于主流编程语言的开发接口方案，支持服务器-客户端模型的算法程序开发，封装了一系列简洁易用的 API，集成了机器视觉、路径规划、力控反馈等预优化的算法模块，在降低开发门槛的同时简化了复杂任务的实现过程，显著提高开发效率。目前支持 NVIDIA Isaac Lab、ROS、Mujoco、Webots 等开源框架。傅利叶智能创始人兼 CEO 顾捷在采访中表示，“真正通用机器人的 GPT 瞬间现在还没有到…… 但是曙光已经看到了，它不是 10 年 20 年的事，它就是三五年内的事。”</p><p>&nbsp;</p><p></p><h4>谷歌 Gemini 1.5 AI 模型再进化：成本更低、性能更强、响应更快</h4><p></p><p>&nbsp;</p><p>据外媒报道谷歌升级旗下 Gemini 1.5 AI 模型，推出了 Gemini-1.5-Pro-002 和 Gemini-1.5-Flash-002，相比较此前版本成本更低、性能更强、响应更快</p><p>&nbsp;</p><p>谷歌下调了 token 输入和输出费用，Gemini-1.5-Pro-002 和 Gemini-1.5-Flash-002 最高降幅 50%，提高了两种模型的速率限制，并减少了延迟。新定价于 2024 年 10 月 1 日生效。</p><p>&nbsp;</p><p></p><h4>英伟达发布 Llama-3.1-Nemotron-51B AI 模型</h4><p></p><p>&nbsp;</p><p>9 月 23 日，英伟达发布博文，宣布推出 Llama-3.1-Nemotron-51B AI 模型，源自 Meta 公司的 Llama-3.1-70B。该 AI 模型主要采用了神经架构搜索（NAS）技术微调，平衡性能和效率，在高工作负荷下，只需要一片 H100 GPU 即可运行，大大降低了内存消耗、计算复杂性以及与运行此类大型模型相关的成本。</p><p>&nbsp;</p><p>英伟达认为这种方式在保持了出色的精度前提下，显著降低了内存占用、内存带宽和 FLOPs，并证明可以在创建另一个更小、更快的变体来加以推广。</p><p>&nbsp;</p><p></p><h4>字节发布豆包视频生成大模型</h4><p></p><p>&nbsp;</p><p>9 月 24 日，字节跳动旗下火山引擎在深圳举办AI创新巡展，一举发布了豆包视频生成-PixelDance、豆包视频生成-Seaweed两款大模型，面向企业市场开启邀测。这也意味着字节跳动正式宣告进军AI视频生成。据火山引擎介绍，豆包视频生成模型基于DiT架构，通过高效的DiT融合计算单元，让视频在大动态与运镜中自由切换，拥有变焦、环绕、平摇、缩放、目标跟随等多镜头语言能力。</p><p>&nbsp;</p><p>深度优化的Transformer结构，则大幅提升了豆包视频生成的泛化能力，支持3D动画、2D动画、国画、黑白、厚涂等多种风格，适配电影、电视、电脑、手机等各种设备的比例。目前，新款豆包视频生成模型正在即梦AI内测版小范围测试，未来将逐步开放给所有用户。</p><p>&nbsp;</p><p></p><h4>哔哩哔哩已上线自研大语言模型index，并应用在AI字幕上</h4><p></p><p>&nbsp;</p><p>9月26日，2024年中国国际智能传播论坛在无锡召开。哔哩哔哩董事长兼CEO陈睿在演讲时表示，AI是年轻人在B站上最关注的内容，也是增长最快的科技内容。数据显示，过去一年，AI内容的日均播放量同比增长超80%，AI相关UP主日活增长超过60%。B站上线了自研大语言模型index，并应用在了AI字幕上。目前B站具备中、英、韩、日、泰语等近10种语言的实时翻译能力，准确度接近90%。</p><p>&nbsp;</p><p></p><h3>企业应用</h3><p></p><p>&nbsp;</p><p>9 月 26 日，在全国第三届公共就业服务专项赛上，人力资源和社会保障部中国就业培训技术指导中心发布了“职业数字展馆”可灵AI短片，首次以AI的方式全景呈现职业数字画像。9 月 25 日，全球领先的自动驾驶科技公司文远知行WeRide和全球最大的移动出行及配送科技公司优步Uber Technologies, Inc.宣布建立战略合作伙伴关系，将共同推进文远知行自动驾驶车辆上线Uber平台，并将首先在阿联酋启动运营。9 月 24 日，OpenAI宣布，已经开始向订阅OpenAI ChatGPT Plus和Team计划的用户推出新的ChatGPT高级语音模式Advanced Voice。该公司补充称，该功能将从下周开始向OpenAI Edu和Enterprise计划的订阅者开放。9 月 23 日，据腾讯混元官方消息，其AI智能体产品腾讯元器现已支持发布至微信公众号，为公众号运营者带来多项新功能。据腾讯表示，利用腾讯元器，公众号运营者可创建数字分身与粉丝进行实时互动，提供7*24小时的客服服务。腾讯元器还能提供文章插入服务，智能体能够将相关内容插入公众号文章，增强内容的互动性和信息量。为读者提供问答助手、文章更实用。9 月 23 日，钉钉宣布面向个人用户推出“365 会员”，包含 AI 搜索、个人 AI 助理、AI 自动回复、自动速读等权益，非会员仍可使用钉钉 AI 助理、快速阅读等现有的 AI 功能。9 月 23 日，美图公司宣布美图奇想大模型（MiracleVision）视频生成能力完成全面升级，在实现生成能力、生成效率以及模型性能的三重进阶基础上，结合美图在计算机视觉领域的多项自研技术优势，视频生成时长与画质、流畅性、真实性及可信度等方面提升显著。美图奇想大模型（MiracleVision）的单次文生视频时长、单次图生视频时长均达5秒，已支持1分钟、帧率24FPS、分辨率1080P的超长视频生成，可以任意视频尺寸输出。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/DOXQYxhOT5GLt4t0KKY9</id>
            <title>大模型辅助需求代码开发</title>
            <link>https://www.infoq.cn/article/DOXQYxhOT5GLt4t0KKY9</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/DOXQYxhOT5GLt4t0KKY9</guid>
            <pubDate></pubDate>
            <updated>Sun, 29 Sep 2024 08:12:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>大模型在解释代码、回答代码问题、写单元测试等方面表现不错，但这些还只是辅助任务，真实项目需求开发中的设计及实现任务才是核心任务，而这方面尚未有成熟的方法和好的效果。一些 AI Developer 工具能够演示从零创建小应用的能力，但放到真实项目中几乎寸步难行。</p><p></p><p>在不久前举办的 <a href="https://aicon.infoq.cn/2024/shanghai/schedule">AICon 全球人工智能开发与应用大会</a>"上，研发效能领域知名专家路宁做了专题演讲“大模型辅助需求代码开发”，结合在多类项目中借助 GPT-4 开发完整需求的实践经验，分享了针对复杂度持续增加的编码任务如何准备上下文、如何区别对待设计任务和实现任务、如何加工经验知识并利用它大幅提升生成效果、如何分步完成复杂的开发任务等不同场景下的问题解决思路，还探讨了组织或企业该如何建设知识工程，以便有力支撑工程师在核心开发任务中利用大模型提效。</p><p></p><p></p><blockquote>在将于 2024 年 10 月 18-19 日举办的 <a href="https://qcon.infoq.cn/2024/shanghai/">QCon 全球软件开发大会（上海站）</a>"，我们设置了【<a href="https://qcon.infoq.cn/2024/shanghai/track/1704">AI 重塑技术工作流程</a>"】这一专题，旨在探索那些超越单点 AI 应用，进一步利用 AI 技术重塑产品研发核心流程的最佳实践。我们关注实际案例和解决问题的策略，旨在解决当前研发团队面临的困境，让智能能真正赋能业务，创造价值。欲了解更多内容，可访问<a href="https://qcon.infoq.cn/2024/shanghai/schedule">大会官网</a>"获悉。</blockquote><p></p><p></p><p>以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）。</p><p></p><p>今天我将与大家分享的主题是《大模型辅助需求代码开发》，这个话题的含义可能非常广泛，容易引起误解，因此我给它加了一个副标题：“探索提升核心编码任务生成效果的方法”。背后的想法其实很简单：我是否能够在不写或少写代码的情况下完成真实项目的开发任务，特别是在面对一些难度高的编码任务时。那么，这些任务应该如何去探索和解决呢？</p><p></p><p>本次演讲的内容主要基于我们过去半年的工作，我将分享一些实践中发现的洞见。演讲将分为几个部分：首先，我会讨论大模型辅助开发的生态系统以及开发实际需求面临的挑战；其次，我将从知识生产和消费的角度来探讨如何生成这些任务的代码，并总结背后的知识分类；最后，我将探讨未来工程师如何利用大模型进行开发，最可能的形态是怎样的，是使用一个很强的 SaaS 工具，还是需要更多的方法和策略。</p><p></p><h3>⼤模型辅助开发⽣态</h3><p></p><p></p><h4>任务全貌</h4><p></p><p></p><p>在探讨大模型辅助开发时，我们首先要明确，是针对那个具体的任务。工程师的工作通常包含很多类的任务，如需求分析、设计、代码实现、修复错误等。这些是大类，其下还有更细致的任务类型。</p><p></p><p>从模型能力的角度来看，有一种特殊的任务是代码补全，但今天我们不讨论这个。而剩下的各类任务大多可以通过模型问答的形式来处理。我们注意到，目前效果较好的任务往往是下图中用蓝色标记的部分，我将这些任务定义为支撑性任务。与支撑性任务相对的是主干任务，这些任务是核心的不可或缺的工作步骤，包括需求分析、架构设计、任务规划、编码实现，以及后续的编译和测试。如果一切顺利，完成这些任务就能实现需求。</p><p></p><p>然而，实际情况往往并非如此顺利。在每个阶段都可能遇到问题，比如在编码阶段可能需要理解代码，或者需要编写单元测试。部署后如果出现问题，还需要分析日志、定位缺陷、以及修复问题。主干任务在图中用绿色标记，目前看来，大模型在这些任务上的表现都不尽如人意。相比之下，支撑性任务由于其难度往往相对低，很多能够取得较好的效果。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/08/08c9096b4dbb17ae0344720444623c20.png" /></p><p></p><p>如何在主干任务中利用大模型来提高开发效率，我们可以通过几种不同的模式来实现这一点。最理想的情况是，我们能够将任务完全交给大模型或者基于大模型的应用来独立完成。如果这种自动化的效果并不理想，我们可能需要采取一种合作的模式，类似于 Copilot。在这种模式下，大模型会先执行一步操作，然后我进行检查和修正，之后再进行下一步，如此推进，直到最终得出满意的结果。对于更加复杂的任务，比如需求分析，大模型可能无法独立完成，但它可以提供一些启发和建议，这对我来说已经非常有帮助了。应用那种模式完全取决于效果，是当前技术能力的局限性所导致的。</p><p></p><h4>主⼲任务相关应⽤现状</h4><p></p><p></p><p>目前，几家大公司出的工具都在尝试从需求澄清、架构设计、任务分解到具体任务实现，端到端地引导工程师完成开发任务。这些实践在体验上与 GitHub Copilot 的 Workspace 相似。还有一些工具，如 Devin，它自称为第一个 AI Developer，擅长从零开始开发一些相对简单的 Web 应用，其公司也获得了可观的投资。然而，尽管这些工具在某些情况下表现出色，但要将它们应用到公司的实际开发环境中，表现就比较差。</p><p></p><p>还有部分人选择手动调用大模型来完成他们的主干任务。为什么要先手动操作呢？因为这样方便针对项目代码和具体任务准备针对性的上下文和知识，以达到可接受的生成效果。事实上，手动操作是能探索到效果的天花板的，因为可以投入大量时间来为模型准备上下文和相关知识，如果这样的效果还不理想，那么使用自动化工具的效果只会更差。从手动操作中总结的经验，可以反过来促进工具的改进和发展，这是一个不断迭代和优化的过程，以提高整体的开发效率和质量。</p><p></p><h4>了解测评背后的任务</h4><p></p><p></p><p>我们不得不指出，行业现有的评测方法过于简单化，无法代表真实需求编码任务。例如，GitHub Copilot 声称能够提升 55% 的工作效率，它的测试是基于两组工程师的一次实验，使用 JavaScript 开发一个 Web 服务器。多数人在一个多小时就能完成这样的任务，任务不涉及私有知识，这与我们日常工作中的任务相去甚远。现有的评测数据集，如 HumanEval，通常只包含函数级别的任务，这些任务的上下文在函数范围内就能描述清楚。</p><p></p><p>SWE Bench 是一个取自真实代码任务的评测集，由美国几所高校的学生创建。它从多个 GitHub 上的项目中挑选 issue 作为测试任务，这些 issue 可能涉及增强功能、修复 bug 或新的需求。如果这些 issue 背后有测试用例，实际上可以自动验证任务是否被正确实现。当我们刚开始接触这个评测集时，只有 Devin 作为一个应用来打榜，它的解决率达到了 13%，其他基于模型 + RAG 的方案表现都很差。随着时间的推移，这个评测变得越来越受欢迎，因为它比其他评测集更接近真实的工作场景。现在，一些应用能够做到 40% 以上的解决率，这是一个相对较高的数字。</p><p></p><p>SWE Bench 的问题在于它使用的是开源库，这些对于模型来说都是在训练数据中包含的。此外，这些测试集所在的项目都是 Python 库，而且大部分 issue 仅需极少的代码改动。例如，一个精选的 Lite 版测试集中 30% 的 issue 只需要 2 行及以内的代码改动就能解决问题。因此，尽管它比其他评测集更接近实际，但与我们实际工作中需要写近百行代码并有复杂的私有知识依赖的编码任务相比，它们仍然太过简单了。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7a/7afd2e2f63ba60275506effe9dc2b3dc.png" /></p><p></p><h4>开发真实需求的挑战</h4><p></p><p></p><p>在开发真实需求时，我们面临的挑战是多方面的。以一个实际的例子来说明，我们当时正在做一个类似 Github Copilot 的项目，有个需求是“增加流式对话的能力到现有的代码中”。我们尝试使用大模型，比如 GPT-4，看看能做到什么程度。</p><p></p><p>初始版本的提示词可以是项目的目录结构和相关代码文件的全部内容，以及任务描述。根据生产的代码来调整提示词，增加更多必要信息，经过多次尝试，最终得到了一个相对满意的生成结果。</p><p></p><p>在这个过程中，我们发现，当使用大模型编写代码时，最常见的问题是模型不遵循设计，倾向于写一套新的。为了避免这种情况，我们需要在提示词中列一些明确的约束和指导，甚至需要详细说明实现步骤。如果工程师需要把任务描述到如此细致的程度，他是不会接受这么使用大模型的。但只要描述得足够详细，模型的效果就会非常好。任务描述甚至接近伪代码的程度，明确指出在何处添加什么函数以及它们的功能。为了提高生成效果，我们让模型每次只执行一个步骤。我们可以一步一步地进行，这样生成的代码基本上是可用的，当然，这个过程相当痛苦。</p><p></p><p>这个痛苦的经历却得出了一些有价值的经验。首先，指示足够详细，效果就足够好。其次，我们需要考虑提示内容的分类。一类是指出代码的改动入口点，通常以函数签名的形式表达，有时还包括输入输出约束。这些信息至关重要，画龙点睛，它指明了新写的代码如何在当前设计约束下与现有系统集成，框定了模型发挥的范围。我们当然希望模型根据上下文及任务能自己确定改动入口点，做不到你就需要告诉他，在提示词中加入一些经验知识能显著提升这方面的效果，后面会介绍。如果做了这些效果还不好，可能就需要给出更多实现级别的提示了。</p><p></p><p>这个过程和指导新人开发需求是类似的，你给他一个需求，他看了现有代码（提示词中的目录结构和代码文件内容）可能也不知道如何切入。你可能会一层一层提示他，第一层就是告诉他在那里加代码，输入输出是什么。还不行就告诉他每块实现的伪码。</p><p></p><h4>分析任务难度</h4><p></p><p></p><p>我们需要对任务难度建立一个认识，对利用大模型完成任务的预期效果有所预判。大模型的能力固然关键，任务本身的难度也会影响效果。</p><p></p><p>首先，模型的推理、规划、指令遵循、窗口大小、注意力特征、输出习惯等都会影响到效果。模型的注意力往往是离散的，我们往往需要在提示词中加入额外描述提及它需要在长上下文中关注的部分，干预注意力的分布。</p><p></p><p>任务本身的难度也是一个考量因素。任务本身所需的理解和推理难度会不同，可能还涉及复杂的分解和规划。任务所依赖的私有知识复杂程度也会影响难度，准备推理所需私有知识的过程可能非常繁琐。</p><p></p><p>在下图中，我们比较了 HumanEval 任务、AI Developer 任务和真实项目中的任务的复杂度。横轴代表所需私有知识的复杂度，即任务所依赖的经验和代码等。纵轴代表一次与模型交互生成内容的复杂度，这粗略反映了模型一次推理的复杂度。HumanEval 这类任务不太依赖私有知识，函数范围就能包含所有必要的信息。而 AI Developer 任务，如编写贪吃蛇游戏，特点是模型已经具备了相关的业务、架构和编码知识，不需要额外提供，但它生成的内容很复杂，所以它的点在于生成内容的复杂性，尽管它依赖的知识并不多。在真实的项目中，依赖知识的复杂度很高，生成内容也多。这三类场景在下图空间中的位置不同，有个演进过程，涉及到对 Agent 和知识工程要求的逐步提升。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/03/03ce95d3dbfc18ce6da5bbe5419683ed.png" /></p><p></p><h4>切换到知识⽣产和消费的思维框架</h4><p></p><p></p><p>要深入分析如何将大模型应用于软件开发中工程师的各项活动中，我们需要从知识生产和消费的角度来重新认识这些活动，尝试理解并刻画工程师大脑里的内心独白和思维轨迹，分析在这些活动中消费以及生产了哪些知识。</p><p></p><p>在项目中，我们可以看到被沉淀下来的知识产出，比如编写的代码、设计文档和需求文档。这些产出是需求开发在各阶段的产物，往往会被记录下来，作为下游工作的输入。编码任务消费上游的设计文档以及已经产出的代码，同时还需要消费任务相关的经验知识，最终生产出增量的下游产物。这些经验知识往往是隐性的，可能加工自上下游产物。比如，工程师之前通过看代码和设计文档掌握了如何在当前架构下完成一类功能的经验，他在应对新任务时可能就会在头脑中召回并消费这个经验，这便体现了：经验以前加工自上下游产物，并在当前任务中被消费。</p><p></p><p>可见，大模型推理可能同时完成了对已有知识的消费和对新知识的生产。按照这个思路分析不同的任务，尝试表达和刻画知识的有效方式，便可构建出一个完整的知识工程表达框架。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/db/db757c566e7efa72c12e247e3b9cf3e3.png" /></p><p></p><h4>依赖经验知识的编码任务</h4><p></p><p></p><p>在探讨大模型辅助编程时，常会先关注两类任务：编码任务和设计任务。编码任务不涉及对现有设计的修改，而设计任务则会改动现有设计。在这些任务之间，我们需要进行规划，决定先做什么，后做什么。除此之外还有需求的澄清和定义等任务。</p><p></p><p>编码任务中有些不依赖私有知识，例如，使用 Devin 编写贪吃蛇游戏这样的任务。还些任务只需要依赖项目代码就能有不错的生成效果。还有些任务只有引入了针对项目加工的私有知识才会得到好的生成效果。</p><p></p><p>我们这里主要讨论的是依赖经验知识的编码任务。我将通过两个例子来阐述这种知识的重要性。第一个例子是利用现有架构完成任务的方法，这涉及到如何抽象地表达任务经验。第二个例子是通过历史任务的记录来刻画经验知识，这便是具体的、实例化地表达任务经验。这些历史任务的记录可以帮助我们理解当时是如何完成任务的，从而为当前的任务提供指导。</p><p></p><h4>利⽤"通过框架完成任务的⽅法描述"</h4><p></p><p></p><p>在“利用框架完成任务的方法描述”中，我们可以看到，利用框架完成任务所需的指导说明。这些知识往往没有被正式记录成文档，而是存在于工程师的头脑中，通过口口相传或个人总结的方式流传。为了将这些隐性知识显性化，我们可以主动加工它们并将其放在提示词中。</p><p></p><p>以一个具体的例子来说，我们有一个需求，要为新增的页面元素编写交互脚本代码。这个任务的挑战在于让代码遵循现有的架构设计。为此，我们总结了在框架下生成页面元素交互脚本的方法。方法描述了框架在哪里，应该使用哪些 API，强调优先使用内置函数，如果内置函数无法完成任务，可提供其他方案等等。这些内容与我们在工作中指导新人如何完成任务时的沟通非常接近，只是我们将隐性沟通显性化了。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b2/b2e88f8eee08e219511f4a0469f6774f.png" /></p><p></p><p>这个例子中，严谨来说，如何使用框架的知识并不是新东西，它们已经蕴含在代码中，如果充分理解代码，我们就应能知道怎么使用框架。为什么还要显式的经验知识来提示呢？因为上下文很大，模型的注意力分散，加之模型能力的限制，如果不加入这些知识，生成的代码倾向无视框架而自己重写一个。我们需要通过经验知识的提示来影响模型在当前上下文中的推理时的注意力分布，框架相关代码会因此获得更多注意力权重，从而引入架构约束，提高生成效果。这实际上是一个影响注意力的过程。</p><p></p><p>这与人之间的交互类似，需要一些提示来指导行动。进一步思考，我们是否可以批量加工这些知识，并在工具中根据相关性进行召回和使用。这样，我们就可以将这些隐性知识转化为显性知识，更有效地利用大模型来辅助编程任务。</p><p></p><h4>利⽤"相关任务的实例化经验"</h4><p></p><p></p><p>我们再来看一个例子，这个例子展示了如何利用一个具体任务的实例化经验。所谓实例化经验，就是一段描述，说明任务是什么，以及每一处代码改动具体是什么。这有点像是你要做功能 A，可以参考功能 B 的做法。模型不会复制功能 B，而是参考它的改动，这里蕴藏了大量有用提示。</p><p></p><p>这个示例是规则引擎的一个需求，目标是扩展这个规则引擎，增加一个新的操作符。如果工程师不太熟悉如何实现，你可以告诉他，这是规则引擎的代码，现在要增加一个新的操作符，可以参考其他已有的操作符。比如，可参考的是幂运算操作符，这个具体实例经验可以从代码提交的变更列表（change list）中加工出来的，具体包括改动了哪些文件，以及改动的部分，也可以为每一处改动生成了简要的描述，以降低模型理解负担。每一处改动的表达方式是提取代码片段，并标记出增加或删除了哪些行。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/65/6563820476f6c1ef1b8d98f483a9da2b.png" /></p><p></p><p>通过应用这个实例化经验，模型生成的代码几乎全部正确，即使这些改动涉及多个文件的多个函数。不同的实例化经验表达方式对生成的效果有显著影响。我们对比了几种方式，包括仅提供完整的代码文件内容，使用 Git Diff 格式表达每处改动，以及上面提到的代码片段加改动行标记。实验证明第三种方式效果最好，这也可以理解，仅提供完整代码其实并没有给出参考经验，而 Git DIFF 则太过简练缺少上下文。如果提供了代码片段并标记出改动的行，这对人类工程师和大模型来说都更完整和友好。</p><p></p><p>上面说明如何刻画经验才能提升生成效果。一般来说，你的经验刻画方法对人类工程师也很容易理解时，那么大模型的效果通常也会不错，其实这也更逼近工程师实际工作中的参考内容和思维轨迹。</p><p></p><h4>知识分类及提⽰词框架</h4><p></p><p></p><p>基于前面的例子，以及众多类似的分析，我们可以对知识进行分类，以建立全局认识并指导后续知识工程的建设。初步将知识分为三类：产物知识、经验知识和衍生数据知识。</p><p></p><p>产物知识：在流程中以产物形式显性表达的知识。这是在软件开发不同阶段的具体产物，如需求文档、架构设计文档、模块设计文档、代码及配置等。如果团队能够采用大模型友好的方式记录这些信息，那么它们就更容易在模型推理过程中被利用。如果使用 Word 文档或图片，那么这些信息就不太容易利用。经验知识：在生产增量产物知识过程中使用的经验，工程师在头脑中加工过，但在团队中通常没有被显性记录下来。这类知识是关于需求、架构、设计、编码及问题修复等任务的经验。它们可以表达为文字和代码。例如，架构变更记录可能包含因为某个问题而进行的架构变更考虑及其效果。设计规范和开发任务的经验也属于这类。对于问题修复经验，可以从相关记录和代码中提取根因、现象和具体修改的代码等信息，做为经验的表达，以后使用。以编码任务中的经验知识为例，它们可以是组件级别的知识（如函数、类的说明）或任务级别的知识（如何实现一个利用到多个组件的任务），也可以是抽象的知识（如何使用架构完成任务）或是具体的知识（如特定任务的实现细节）。衍生数据知识：程序运行时的数据或基于代码分析的数据，用于知识相关性计算或直接提升生成任务的效果。这类知识不是由工程师脑力生产的，而是通过程序加工的。通过代码分析得到的 AST（抽象语法树）结构，可以用在生成单元测试的任务中以提升覆盖度。AST 数据也可以用在依赖代码的相关性计算中，在为代码生成任务准备提示词上下文时可能用到。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/92/92b21f2c4e0c41abd3bad08ee8e09c5e.png" /></p><p></p><p>对知识进行分类后，我们可以总结一个提示词框架，表达在什么上下文下，基于什么经验约束，完成一个什么样的任务。它包括下面几部分：</p><p></p><p>项目的基础上下文：这部分主要是产物知识，比如项目目录结构以及模块的全部代码。如果受限于窗口大小或是发现无关信息的干扰严重，我们可以手工或通过相关性计算来精选信息，确保生成效果。任务经验：与当前任务相关的经验描述，属于经验知识。输出约束：对输出内容的约束规则，它们引导模型按照预期的方式生成结果。任务描述及提示：这些内容不容易复用，往往是一次性编写的，包括具体任务介绍，以及在生成效果不够好时不得不增加的提示。在反复尝试改善效果的过程中，我们需要增加些关键的提示，它们虽然难以转化为可复用的经验，但对于提升特定任务的生成效果至关重要。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/89/8922ad2effbd5f4f5b51b579fa916f89.png" /></p><p></p><h3>⼤模型辅助开发的实⽤形态</h3><p></p><p></p><p>探讨未来应用形态，实际上也是在回答一系列问题。</p><p></p><p>⼯具能做到什么程度？ 这取决于底层模型的能力，以及知识工程建设的程度，未来随着模型能力的显著提升，工具可能表现的更懂代码和工程师的意图。而现在，尽管工具已经表现的能引导完成从需求到代码的过程，但还做不到对主干任务有实质性的帮助。很多步骤带来的检查修改成本大过帮助，工程师更倾向于在痛点步骤或任务上去为大模型认真编写上下文。</p><p></p><p>工程师是否需要裸用 ChatGPT 这样的工具？未来结合一些小脚本裸用大模型的情况会很常见，对主干任务也是这样。工程师熟悉大模型后维护提示词模板非常容易。结合一些小脚本或工具管理知识具备可行性，特别是大部分工程师需要维护的代码量有限，维护私有知识成本不高。模型能力提升后那些繁琐的多步操作将大幅度简化，也降低了复杂工具的必要性。模型能力包括窗口和注意力的提升也大幅度降低了通过相关性检索和复杂上下文组装的必要性。大型工具在主干任务的优势未必明显，或者说工具建设需要开放性，以便融入工程师自己在项目中利用大模型的经验，包括自己的提示词、加工的知识和定义的推理路径等。</p><p></p><p>⼯程师是否必须能驾驭⼤模型？答案是肯定的。工程师需要能够深入理解并使用它们，而不是寄希望于用上工具就能大幅提升自己的效率。软件开发的场景非常丰富，即使是我前面提到的例子，覆盖的范围也非常有限，很多时候需要我们自己想办法提升效果。</p><p></p><p>⼈的精⼒能被释放到什么程度？追求多⼤⽐例的效率提升⽐较现实？ 我们在利用大模型完成主干任务时，虽说要写的代码少了，但时间却花在了各种准备工作、反复尝试和提示、检查和修正这类事情上面了，当然其中一部分的工作量是有潜力工具化的，但可能并没有想象的比例那么高。当前已经比较成熟的代码补全，即使采纳率达到 30%，在编码环节上的提升可能在 5% 左右。2 年内，对于端到端整体效率提升，持谨慎乐观的态度，可能在 15% 左右。</p><p></p><p>最后，我想强调几点：</p><p></p><p>软件开发任务生态丰富，主干任务的难度远超支撑性任务。从知识生产和消费的角度重新建模软件开发过程。如何遵循设计是复杂编码任务的重要挑战。从现有代码结构或完成任务的历史记录中加工经验知识。利用合理的知识分类指导知识工程的建设。</p><p></p><p>演讲嘉宾介绍：</p><p>路宁，研发效能领域知名专家，目前在理想汽车探索代码智能实践，曾任 ThoughtWorks 架构师和互联网大厂资深技术总监。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/SEdrDVfZPxDyutR4Asqm</id>
            <title>文档解析与向量化技术加速 RAG 应用落地</title>
            <link>https://www.infoq.cn/article/SEdrDVfZPxDyutR4Asqm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/SEdrDVfZPxDyutR4Asqm</guid>
            <pubDate></pubDate>
            <updated>Sun, 29 Sep 2024 07:44:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在不久前举办的 AICon 全球人工智能开发与应用大会上，合合信息智能创新事业部研发总监，复旦博士常扬从 RAG 应用落地时常见问题与需求（文档解析、检索精度）出发，分享了针对性的高精度、高泛化性、多版面多元素识别支持的文档解析技术方案与整合长文档解析引擎、层级切片技术的向量化 Embedding 模型技术方案，以及开放域多模态信息抽取应用与知识库问答应用等实际场景的产品实践案例。本文为常扬的演讲整理。</p><p></p><p></p><blockquote>在将于 2024 年 10 月 18-19 日举办的 <a href="https://qcon.infoq.cn/2024/shanghai/">QCon 全球软件开发大会（上海站）</a>"，Zilliz Senior Product Manager 张粲宇也从自身的 RAG 场景出发，分享了 RAG 的挑战，以及通过采用混合检索技术和其他技巧，最终把准确率提升到 90% 以上的经验。欲了解更多内容，可访问<a href="https://qcon.infoq.cn/2024/shanghai/schedule">大会官网</a>"获悉。</blockquote><p></p><p></p><p>近年来，大模型的崛起为人工智能领域带来了革命性的变化。然而，在实际应用中，我们发现仅依靠大模型自身的知识和上下文并不能完全满足用户的需求。检索增强生成（Retrieval-Augmented Generation， RAG ）应运而生，旨在通过引入外部知识库，解决领域知识缺乏和信息过时的问题。但其本身在产品化落地的过程中面临检索召回率低、生成质量差的问题。</p><p></p><p>我会从第一性原理出发，分析其核心突破点：文档解析技术 与 向量化技术，看这两个技术如何发展，加速 RAG 应用落地 的。</p><p></p><h3>RAG 的背景与问题</h3><p></p><p></p><p>在传统的大模型应用中，模型的知识来源主要是预训练数据和用户提供的上下文。然而，面对专业领域的复杂问题，模型往往难以给出准确、及时的回答。RAG 的核心目标是将外部的领域知识与大模型结合，使其能够生成更准确、更专业的回答。</p><p><img src="https://static001.geekbang.org/wechat/images/2a/2a6c427d5d8cf58abcbe58d57e1d585c.png" /></p><p></p><p>LLM（大型语言模型）应用知识的数据来源包括“用户上下文输入和用户意图”、“大模型知识”和“外部文档”。RAG 位于三个知识源的交集处，结合了外部文档、用户输入的上下文、模型的知识进行生成。RCG（Retrieval-Centric Generation）专注于将知识检索与 LLM 的生成分开，把检索知识作为核心来源，而微调（Fine-Tuned LLM）则通过使用外部数据微调模型，提升模型在特定领域的理解能力。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/08/08fcc15c26b46fd08c004436969133ec.png" /></p><p></p><p>RAG 的核心流程可以简化为以下三个步骤：</p><p></p><p>知识库构建（Indexing）：对外部文档进行解析、清洗、向量化，构建高质量的索引。检索（Retrieval）：根据用户的查询，在向量空间中检索最相关的内容。生成（Generation）：大模型基于检索结果生成最终的回答。</p><p></p><p>在这个流程中，知识库构建和检索是关键的技术环节，直接影响最终生成的质量。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b4/b47354cc1b49194af5d7cc1ab06b2586.png" /></p><p></p><p>尽管 RAG 有着广阔的应用前景，但在实际落地过程中，我们发现了许多问题。一篇研究论文总结了 RAG 应用中常见的 12 个问题，包括：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/34/34250887515723e722d9d839be11a949.png" /></p><p></p><p>内容缺失：检索结果未能覆盖用户的问题。不完整回答：生成的内容缺乏细节或关键信息。性能不足：检索和生成过程耗时过长，无法满足实时性要求。不可扩展性：面对海量数据时，系统的性能显著下降。</p><p></p><p>等等这些问题的存在，使得很多 RAG 产品难以从原型（MVP）走向真正满足市场需求的产品（PMF）。</p><p></p><p>在 AI 产品的开发中，最小可行产品（MVP） 的构建相对容易，但要实现产品市场契合（PMF），真正满足用户需求，却充满挑战。</p><p></p><p>理解大模型的技术边界：既不要过度高估大模型的能力，也不要忽视其潜力。明确其适用范围和限制。深入理解业务场景：识别大模型和 RAG 技术最适合的应用领域，确保技术与业务需求的契合。优先使用最好的模型：在验证方案可行性时，尽可能使用性能最优的模型。如果最佳模型仍无法满足需求，可能需要重新评估方案。构建产品壁垒：考虑产品的独特价值和竞争优势，而不仅仅是技术实现。确保产品在市场上具有可持续的竞争力。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b8/b82f81c01944fbf294ca606ef89389f7.png" /></p><p></p><p>从第一性原理出发，要解决上述问题，我们需要回归本质，关注 RAG 流程中的关键环节：</p><p></p><p>高质量的知识库构建：确保文档解析的准确性，将领域知识准确地转化为文本和向量表示。精确的检索机制：利用先进的嵌入模型，提高在向量空间中检索相关内容的准确性。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b5/b5974b2bffb9b8fc3efa3ba2fafdd91d.png" /></p><p></p><h3>文档解析技术</h3><p></p><p></p><p>在大模型 RAG 应用中，文档解析是一个关键环节。如何将复杂的 PDF 文档准确、高效地转换为大模型能够理解的格式，直接影响到模型的性能和应用效果。下面将深入探讨通用文档解析的挑战与解决方案，重点介绍从 &nbsp;PDF &nbsp;到 Markdown 的转换过程，以及如何通过物理和逻辑版面分析，实现对各种复杂文档的高质量解析。</p><p></p><p>从计算机的角度来看，文档主要分为两类：</p><p></p><p>有标记的文档：如 Word、Markdown、HTML、JSON 等。这些文档具有明确的结构，计算机可以直接解析和理解。无标记的文档：如图像、 PDF &nbsp;等。这些文档缺乏结构信息，计算机无法直接理解其内容。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/bf/bfeeb1ee9e5f774b0ac7e4d6d0e80be8.png" /></p><p></p><p>PDF 是一种常见的文档格式，其本质是一系列显示打印指令的集合，而非结构化的数据格式。例如，一个简单的“ Hello World ” &nbsp;PDF &nbsp;文件，用文本编辑器打开后会发现大量的排版指令。这使得计算机在解析 &nbsp;PDF &nbsp;时，难以直接获取其中的结构化信息。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/70/70aa252a5a9e0ce4dd469ac3085f99c4.png" /></p><p></p><p>解析 PDF 的挑战有：</p><p></p><p>结构缺失：PDF 以视觉呈现为主，缺乏逻辑结构，难以直接提取段落、标题、表格等元素。复杂排版：多栏布局、跨页内容、嵌入图像和公式等，增加了解析难度。元素遮挡：印章、手写批注等覆盖在文本上，干扰内容识别。</p><p></p><p>为了让大模型有效地理解 PDF 中的内容，需要将其转换为一种模型熟悉的格式。Markdown 成为了最佳选择，原因如下：</p><p></p><p>结构清晰：支持多层标题、粗体、斜体、列表、表格、数学公式等，能够完整地表达 PDF 中的信息。专注内容：Markdown 关注内容本身，而非排版，符合大模型的训练特点。模型友好：大模型在训练过程中，接触了大量的 Markdown 语料，对其有良好的理解能力。</p><p></p><p>Markdown 的优势：</p><p></p><p>简洁优雅：语法简单，易于编写和阅读。广泛应用：在技术文档、博客等领域被广泛使用，具备良好的生态环境。易于转换：可以方便地转换为 HTML、 PDF 等格式，满足多种需求。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/fa/fa8e09f8b1e93b07e92a80ad9aad063e.png" /></p><p></p><p>复杂文档布局的解析难点有很多，比如：</p><p></p><p>法律条文：包含多级标题和列表，需要准确识别层次关系。扫描书籍：可能存在噪声、阴影，OCR 识别难度大。学术论文：包含复杂的公式、表格和引用格式。产品说明书：双行表头、多栏布局，对解析精度要求高。</p><p></p><p>有以下的典型挑战：</p><p></p><p>多栏布局双列、三列布局，跨页内容，如何保持正确的阅读顺序？复杂表格合并单元格、嵌套表格，需要精确解析结构。公式和图像数学公式、图像与文字混排，要求高精度的识别和定位。元素遮挡和噪声印章、手写批注、扫描噪声，干扰内容的提取。</p><p></p><p>现有解析方法具备一定的局限性。基于规则的解析库，典型代表 PDF Miner、Py PDF 、Mu PDF 等。优点对电子版 PDF 的解析效果较好，速度快。缺点是无法处理扫描件和图像型 PDF 以及对复杂布局支持不足，难以准确还原阅读顺序。</p><p></p><p>基于深度学习的解析库，典型代表 Unstructured、Layout-Parser、PP-StructureV2 等。优点是能处理扫描件，具备一定的版面分析能力。缺点是效率较低，处理大型文档耗时长以及对复杂布局的支持有限，精度有待提高。</p><p></p><p>基于大模型的解析库，典型代表 GPT- PDF 。优点是具备多模态理解能力，直接处理图像和文本。缺点是计算资源消耗大，实时性差以及受限于大模型的输入长度，对长文档支持不足。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/dd/dd206130c101f73244e5def5a949748c.png" /></p><p></p><p>针对上述挑战，我们提出 TextIn 文档解析技术：</p><p></p><p>解析更稳：无论何种排版和格式，都能正确提取文档元素，并还原正确的阅读顺序。识别更准：高精度地识别文本、表格、公式、图像等内容。性能更优：满足实时性需求，支持大规模文档的快速解析，适用于 RAG （检索增强生成）应用场景。</p><p></p><p>具备以下功能特性：</p><p></p><p>全面支持兼容电子版和扫描版 PDF 。预处理优化文档图像预处理，提高 OCR 精度，减少噪声干扰。深度分析物理版面分析和逻辑版面分析相结合，准确还原文档结构。元素识别全面识别段落、标题、表格、公式、页眉页脚等内容。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b5/b5f658bbcef5ecbfd6040a7fc3903e40.png" /></p><p></p><p>我们的解析流程主要分为三个阶段：</p><p></p><p>1. 文档预处理</p><p></p><p>拆分文档将多页文档拆分为单页，便于并行处理。</p><p></p><p>类型识别区分电子版和扫描版，选择适当的处理策略。</p><p></p><p>2. 版面分析</p><p></p><p>物理版面分析基于视觉特征，划分页面区域，如段落、列、图像、表格等。</p><p></p><p>逻辑版面分析基于语义特征，构建文档的目录树，确定元素的层次关系和阅读顺序。</p><p></p><p>3. 内容重建</p><p></p><p>元素识别对文本、表格、公式、图像等进行精确识别。</p><p></p><p>格式转换根据需求生成 Markdown 或 JSON 格式，方便大模型理解或产品呈现。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/55/5510c5bc6a5f1bc87e340f80b84235fb.png" /></p><p></p><p>物理版面分析逻辑如下：方法选择采用单阶段检测模型，兼顾精度和效率，避免复杂的多阶段流水线。数据处理注重数据的多样性和分布，增强模型的泛化能力。层级结构按照页（Page）→ 区块（Section）→ 列（Column）→ 段落（Paragraph）→ 文本片段（Text Slice）进行解析。阅读顺序还原策略从上到下、从左到右，结合页面布局和元素位置，准确还原阅读顺序。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/41/41d9dfe45e277ac8b573aaa7cdbc753e.png" /></p><p></p><p>逻辑版面分析逻辑如下：模型架构采用 Transformer，利用自注意力机制，捕获元素间的全局关系。父子关系确定标题与其子内容的从属关系，旁系关系识别同级元素之间的顺序关系，目录树构建通过关系预测结果，生成文档的逻辑结构树，方便内容的组织和导航。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/06/06827dd6664302ae66567804bc3c762a.png" /></p><p></p><p>面对多种复杂的文档解析需求，如何客观评估不同解析方法的效果，是一个重要问题。传统的文字和表格评估方法，无法全面衡量复杂布局和结构的解析质量。我们开源了 Markdown Tester，指标定义针对段落、标题、表格、公式、阅读顺序等核心元素，定义了识别率、召回率、F1 值等评价指标。结构评估引入树状编辑距离，评估目录树、表格结构与原文档的相似程度。可视化展示通过雷达图等方式，直观展示各解析方法在不同指标上的表现。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/80/80d590390866010b27cf26fdad9af4f0.png" /></p><p></p><p>在实际应用中，解析效率至关重要。特别是在 RAG 场景下，需要对大量文档进行实时解析，要求解析过程既快又准。我们进行了以下优化策略，算法模块优化针对耗时模块，如版面分析和元素识别，优化算法，提高执行效率。并行处理采用分布式集群结构，对文档解析流程进行并行化处理，充分利用计算资源。资源调度智能调度计算任务，平衡负载，减少等待时间。</p><p></p><p>我们目前实现处理速度实现了对 100 页文档的解析，P90（90% 请求的响应时间）小于 1.5 秒。稳定性在大规模文档解析任务中，保持了高稳定性和低失败率。扩展性系统设计支持水平扩展，能够应对更大的数据量和更高的并发需求。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f5/f59d7f67105e3efd4ac8fb6e3d02f0ff.png" /></p><p></p><h3>向量化技术</h3><p></p><p></p><p>在现代自然语言处理任务中，向量化（Vectorization）技术扮演着关键角色，特别是在检索增强生成（ RAG ）模型中。下面将探讨向量化的原理、模型选择以及多尺度训练方法，揭示其在实际应用中的重要性和优化策略。</p><p></p><p>向量化的核心思想是将大量的文本数据转换为具有方向和数值的向量列表。这种表示方式使计算机能够利用矩阵运算的高效性，快速计算文本之间的语义相似度。在 RAG 模型中，向量化主要用于信息检索，即通过计算用户问题和文档块的向量相似度，找到最相关的文本内容。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/eb/eb7e0e2688e17932b64b45ce0c4adb86.png" /></p><p></p><p>在选择嵌入（Embedding）模型时，许多人可能会在网上查找评价效果较好的模型，或者自行进行评测。然而，在评估之前，建议先查看像 Hugging Face 上 MTEB 和 C-MTEB 评测基准排名，这些指标涵盖了文本分类、聚类、成对分类、重排序、检索等任务，能够大致了解每个模型在不同任务上的表现。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/bc/bce977c0a4791e061fea193c5567e76d.png" /></p><p></p><p>在选择合适的嵌入模型时，需要考虑以下六个关键因素：</p><p></p><p>1. 特定领域适应性：嵌入模型的训练依赖于大量语料，不同领域的语料量和质量会影响模型在该领域的表现。例如，在工业领域，如果训练语料较少，模型的性能可能不理想。因此，即使某个模型在排行榜上名列前茅，也应在自己的领域数据上进行测试。</p><p></p><p>2. 检索精度：对于以检索为核心的任务，模型的检索精度至关重要。需要评估模型在实际检索场景下的准确率和召回率。</p><p></p><p>3. 语言支持：根据业务需求，选择支持所需语言的模型，确保在多语言环境下的有效性。</p><p></p><p>4. 文本块长度（Chunk Size）：文本块的长度应根据任务需求进行调整。如果处理的是信息密度较低的长文本，如小说，可能需要更长的文本块。而对于信息密度高、需要精确检索的内容，较短的文本块可能更合适。</p><p></p><p>5. 模型大小与推理成本：模型的大小直接影响推理的资源消耗和成本。在资源有限的情况下，需要在模型性能和资源消耗之间取得平衡。</p><p></p><p>6. 检索效率：在需要处理海量数据的场景下，检索效率变得尤为重要。模型的计算复杂度和向量维度都会影响检索速度。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8f/8f5702e9fd4f5856f7e07993f5f3ef26.png" /></p><p></p><p>我们的模型采用了对比学习的方法，这与业界领先的实践保持一致。其中一个较为特殊的点是引入了多尺度训练（Multi-Resolution Learning，MRL）。在训练过程中，我们采用不同的尺度，如 1024、512、256、128 等长度的 Token，计算整体的损失函数。这样做的目的是：</p><p></p><p>保证信息质量集中在前序序列：通过多尺度训练，模型能够在较短的序列中捕获高质量的信息，确保在截断或限制序列长度时，重要信息不会丢失。提高检索效率：在资源受限或需要高效检索的情况下，可以使用较短的向量表示，而不会显著降低精度。减少维度，保持精度：通过多尺度训练，模型在降低向量维度的同时，尽可能保持了语义表示的准确性。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b4/b454b86963e0ce94239d0a3c4e34874d.png" /></p><p></p><p>综上所述，向量化技术在 RAG 模型中具有不可替代的作用。通过慎重选择嵌入模型，结合多尺度训练等优化方法，我们能够在提高检索精度的同时，优化资源使用，实现高效、准确的文本信息检索。</p><p></p><h3>总结</h3><p></p><p></p><p>我们的目标始终是打造真正可用、好用的产品，实现从 MVP（最小可行产品）到 PMF（产品市场契合）的飞跃。只有当产品真正满足用户需求，具备市场推广价值，才能称之为成功的产品。</p><p></p><p>在 RAG （检索增强生成）应用中，我们发现了两个突出的核心问题：文档解析和向量化。为了解决这两个问题，我们专注于以下研究方向：</p><p></p><p>1. 通用文档解析</p><p></p><p>我们致力于实现快速、精准、稳定的文档解析，支持各种类型的文档，包括扫描件和多种版式。通过提高解析的稳定性，确保阅读顺序的正确，还原文档的真实结构。同时，提升解析的精度和效率，使解析过程更准确、更高效。</p><p></p><p>2. 高性能的嵌入模型（Embedding）</p><p></p><p>在向量化过程中，我们注重模型的精度和效率。选择最适合业务需求的嵌入模型，能够更高效地进行信息检索，提高检索的精确度和速度。这不仅包括模型本身的优化，还涉及对领域数据的深入理解和应用。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/1a/1a9d7a8aa394df8d78094630fe69a18c.png" /></p><p></p><p>我们相信，专注于问题的本质，才能取得最大的收益。如果您对我们的技术和产品感兴趣，欢迎访问我们的官网 textin.com。在网站上，您可以了解更多关于通用文档解析、嵌入模型技术，以及我们的财报分析、分析师问答等产品信息。</p><p></p><p>感谢您的阅读和关注。</p><p></p><p>作者介绍：</p><p></p><p>常扬，合合信息智能创新事业部研发总监，复旦博士，复旦机器人智能实验室成员，国家级大学生赛事评审专家，多个技术社区 AI 专家博主，发表多篇 SCI 核心期刊学术论文，负责合合智能文档处理业务线的产品、技术、云服务平台研发工作，研究方向智能文档处理、多模态大模型。任职期间，先后主导了人工智能数据清洗平台，卡证识别、票据识别、行业文档定制等信息抽取产品，TextIn 智能文字识别云服务平台，TextIn 票据机器人、财报机器人、合同机器人等智能文档场景落地产品，为金融、制造、物流等行业提供智能文档处理产品与解决方案，在企业信息化转型领域具备丰富的技术落地经验和行业场景洞察力。</p><p></p><p>会议推荐</p><p></p><p>InfoQ 将于 10 月 18-19 日在上海举办 QCon 全球软件开发大会 ，覆盖前后端 / 算法工程师、技术管理者、创业者、投资人等泛开发者群体，内容涵盖当下热点（AI Agent、AI Infra、RAG 等）和传统经典（架构、稳定性、云原生等），侧重实操性和可借鉴性。现在大会已开始正式报名，可以享受 9 折优惠，单张门票立省 480 元（原价 4800 元），详情可联系票务经理 &nbsp;17310043226 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/68/68a4f559d6682dec46bd5633588299f0" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/C3aMcJwyQTcAUfI0VKZg</id>
            <title>算力存力Buff都叠满，至强6最强形态现身！</title>
            <link>https://www.infoq.cn/article/C3aMcJwyQTcAUfI0VKZg</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/C3aMcJwyQTcAUfI0VKZg</guid>
            <pubDate></pubDate>
            <updated>Sun, 29 Sep 2024 03:17:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>文章来源：英特尔</p><p></p><p>2024年是至强的大年。</p><p></p><p>先于6月正式发布的至强®️&nbsp;6700E系列开启了全新的、更为简洁命名方式：至强®️&nbsp;6能效核。144核的规格也意味着英特尔在最近几年当中首次在核心数量方面实现了领先。而且，这还并不是至强6的最强形态，毕竟大家都知道还有个6900P系列嘛。</p><p></p><p>9月26日，至强6这个“最强形态”终于正式发布，主要规格非常震撼。即使面对今年内晚于自己发布的其他厂商同级别CPU，至强®️&nbsp;6900P的已有规格也战力十足。</p><p></p><p></p><h3>最强至强能有多强？</h3><p></p><p>英特尔代号Birch Stream的新一代服务器平台所采用的至强6处理器是分批次发布的。6月发布的是代号Sierra Forest的能效核处理器6700E系列（E后缀即Efficiency Core，能效核的标记），目前发布的是代号Granite Rapids的性能核6900P系列。今年底和明年初还会陆续发布6900E、6700P，以及6500/6300等。未来的Intel 18A制造工艺的处理器，如Clearwater Forest，也会继续用于Birch Stream平台。</p><p></p><p>至强6900P是英特尔专为计算密集型工作负载设计的处理器，也是Granite Rapids的“完全体”。后缀的“P”意味其采用的是Performance Core，即性能核，规模大、性能强；6900的数字型号则说明其核心配置拉满——提供了72到128核的多种规格，TDP有400W和500W两种，组合成已公开5种型号，显得比较简洁。当然，依照惯例，云厂商等大客户还会有若干定制型号的。单就内核数量而言，6900P系列相对前两代“Rapids”产品线顶配的56/60（Sapphire Rapids）或64核（Emerald Rapids）直接翻倍！如此巨大的迭代幅度非常罕见，也难怪英特尔要改命名方式了，由表及里都透着一个意思：厚积薄发、脱胎换骨！</p><p></p><p><img src="https://static001.geekbang.org/infoq/de/de2e05d57ed38a7b56d854b340c8ed32.png" /></p><p>&nbsp;</p><p>尤为值得一提的是：至强6900P也是业内首款性能核数量正式“破百”的产品，其他同级产品，不论是x86架构还是Arm架构都只达到了96核的水平。它们的性能核数量要追平英特尔，起码得等到下个季度。</p><p></p><p>随着内核规模增加，至强6900P的L3缓存达到了504MB。为了配合倍增的核数和显著提升的算力，至强6900系列的存力也大为增强，内存带宽方面不仅支持12通道DDR5 6400；并引入了新型内存MR DIMM，把数据率大幅提升至8800MT/s，基本内存带宽可以达到第五代至强可扩展处理器的2.3倍。另外，至强6还支持CXL 2.0，尤其是包括Type 3设备（也就是CXL内存），可以进一步扩展内存容量和带宽。</p><p></p><p>至强6900P的UPI2.0链路也有很大改进，速率提升到24GT/s，数量增加至6条，使得双路互联效率进一步提升。结合内核数量、内存带宽等方面的全面提升，至强6900P可以被视作高算力+高存力平台的最强机头，不论是科学计算，还是AI集群。根据已透露的测试，至强6900P平台的数据库、科学计算等关键应用负载的表现是上一代产品的2.31倍-2.5倍，AI应用性能是其1.83倍-2.4倍不等。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e3/e321699a67d699aa4649106cd5dafff6.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/9e/9e435691b62871f08db634cfc41cedbc.png" /></p><p>&nbsp;</p><p>至强6的扩展能力也有不小的提升。其中6900系列单插座不论是性能核还是能效核均可提供96通道PCIe 5.0，双路即可提供192通道PCIe 5.0。未来上市的6700系列单路型号可以提供136通道PCIe 5.0，双/多路型号单插槽也可以提供88通道。相较而言，第四、五代至强可扩展处理器的PCIe 5.0通道数量为80。CXL支持能力方面，至强6 6900、6700系列都支持64通道CXL 2.0。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1e/1e070da2d8b6cf92a7127f23d135cd08.png" /></p><p>&nbsp;</p><p>更多的内核、更多的内存通道、更多的PCIe通道需要更大规模的插座接口支持。&nbsp;至强6带来了两种接口：LGA 4710和LGA 7529。至强6900系列使用面积较大的LGA 7529插座，提供最强大的内存带宽和扩展能力，是未来高性能、高密度服务器的基础。至强6700以及未来的6500/6300系列使用LGA 4710，尺寸与第四、五代至强的LGA 4677相仿，内存、PCIe的通道数相同或相近，有利于主流服务器内部布局习惯的延续性。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h3>改进的EUV：Intel 3</h3><p></p><p>核心规模的飙升首先得益于至强产品线终于获得EUV光刻机的加持。在2023年发布的酷睿Ultra已经率先使用了引入EUV的Intel 4制造工艺。而2024年发布的至强6则使用了进一步改良的Intel 3制造工艺。</p><p></p><p>2021年7月，英特尔CEO帕特·基尔辛格公布了“四年五个制程节点”（5N4Y）的工艺路线图。Intel 3的量产时间节点位于2023年底，节奏基本符合计划。从基于Intel 4制造工艺的酷睿Ultra的市场表现看，EUV的加持确实明显提升了英特尔处理器的竞争力。至强6所采用的Intel 3制造工艺相对Intel 4可以规划更多的金属层、拥有更多细分版本。</p><p></p><p><img src="https://static001.geekbang.org/infoq/14/14779b3281cb7116a653416e76f2f343.png" /></p><p>&nbsp;</p><p>Intel 3在更多的步骤中应用EUV光刻，可以提供更密集的设计库、更高的晶体管驱动电流。Intel 3还有三种变体，包括3-T、3-E和3-PT。Intel 3、3-T是基本工艺，主要用于CPU；3-E是功能扩展；三者都支持TSV；Intel 3的这三种变体与Intel 4相比可以提升18%的性能功耗比。而3-PT进一步增加混合键合的支持能力，带来了更高的性能并且易于使用。Intel 3所有四种节点变体都支持240 nm高性能和210 nm高密度库，而Intel 4只支持240 nm高性能库。</p><p></p><p>对于性能取向，Intel 3针对高性能运算进行优化，可以支持低电压(&lt;0.65V)和高压(&gt;1.3V)运行，且在各电压下的频率均高于Intel 4。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7a/7a87a54ced1d9949df7b1fa5edab0630.png" /></p><p>&nbsp;</p><p></p><h3>微架构大迭代</h3><p></p><p>至强6900P采用的性能核微架构代号Redwood Cove。Redwood Cove也是近年来英特尔最重要的微架构迭代，不但给服务器产品线带来了新名字，在消费类产品线同样开启了新的命名序列酷睿Ultra。</p><p></p><p>我们先快速回顾一下Redwood Cove的上一代Golden Cove/ Raptor Cove。Golden Cove其实也是非常重要的迭代，在消费类开启了大小核时代（第12代酷睿处理器），在服务器上就是第四代至强可扩展处理器。Golden Cove相对其前代的微架构大幅度提升了前端：</p><p></p><p>指令TLB翻倍，从128条增加到256条；指令提取带宽从每周期16字节翻倍到32字节；解码器从4路扩展到6路；微操作缓存从2304条增加到4096条。其他L1 BTB、L2 BTB等也有所提升。</p><p></p><p>Golden Cove的后端当然也有提升，譬如重排序缓冲区、分支目标缓冲区也有大概30%左右的提升，只是相对前端幅度不那么大。</p><p></p><p>Raptor Cove的微架构与Golden Cove差异不大，表现在实际产品上主要是缓存的提升，如基于Raptor Coved的第13代酷睿（Raptor Lake）的每核心L2缓存从12代（Alder Lake）的1.25MB提升到2MB；第五代至强可扩展处理器（Emerald Rapids）和第四代（Sapphire Rapids）每个核心的L2缓存都是2MB，但前者每个网格的末级缓存（Last Level Cache，也可继续俗称为L3缓存）从后者的1.875MB猛增到5MB。</p><p></p><p>Redwood Cove相对Golden Cove/ Raptor Cove的最重要变化是：</p><p>指令缓存从32KB增加到了16路、64KB；微操作队列从144个条目增加到192个条目；指令执行延迟降低；更智能的预取和改进的BPU；L2缓存的带宽有所提升AMX增加FP16支持</p><p></p><p>当然，Redwood Cove还有一个重大的优势就是“命好”，也就是前面提到的EUV制造工艺。但即使有革命性的制造工艺加持，至强6性能核也没过分扩张每个内核的规模。就至强6性能核的内核而言，每个网格节点是一个P核，每个P核配置私有的2MB L2缓存，以及共享的4MB 末级缓存。虽然平均到每个核的缓存容量并不比上一代至强（Emerald Rapids）多，但胜在总核数翻倍后。至强6性能核每个处理器可共享的末级缓存总容量依旧达到504MB，远超第五代的320MB和第四代的112.5MB。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6d/6d1fa038f2581537a12c7732d3054ea7.png" /></p><p>&nbsp;</p><p>在此也顺便提一下至强6能效核的微架构Crestmont。这个微架构同样出现在了酷睿Ultra的能效核当中。Crestmont是2或4个内核为一组共享L2缓存。在至强6能效核当中，每2或4个内核与4MB的L2缓存（在酷睿Ultra中则为2MB）构成一个模块，这几个内核共享频率和电压域。这个模块对应的网格还拥有可整个处理器全部内核共享的3MB的末级缓存。换句话说，虽然至强6能效核的核数更多，但实际上网格规模比至强6性能核小。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f3/f3ebfc19f3fe206d2335db62c792362c.png" /></p><p>&nbsp;</p><p>能效核的指令缓存与性能核都是64KB，但数据缓存分别是32KB和48KB。前端的指令解码器宽度也有差异，分别为6和8宽。指令乱序执行引擎差异较大，能效核是256条而性能核是512条。能效核不支持性能核所支持的AVX-512和AMX，这也可以明显减小矢量运算单元的晶体管占用，但代价是每周期的单精度浮点运算次数有了数量级的差异。但能效核也改进了AVX2，增加了VNNI的INT8和BF16/FP16快速转换，这样在处理AI应用的时候表现也还有所改善。另外，其256位加密和1024/2048密钥也获得了能效核的支持，确保至强6平台的安全水平基本一致。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fb/fb3407041ea30e85bea0c6c5fea46a34.png" /></p><p>&nbsp;</p><p>缓存规模、前端宽度以及矢量单元的差异，使得至强6性能核和能效核有不同的定位。早先发布的至强6能效核更适合微服务等运算强度相对较轻，可在高核心数量和规模扩展方面收益的任务，以追求更高的能效、更高的机架利用率。而现在发布的至强6性能核更适合大数据、建模仿真等计算密集型和人工智能任务，为高性能优化，单颗处理器的功耗直飚500W——当然，跟同期发布的Gaudi AI加速器的新品或类似的加速器产品相比，能耗是应有的代价，有能力提升性能上限才是正经事。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h3>内存性能大跃进</h3><p></p><p></p><p>内存（DRAM）的数据存储依赖电容，这个特点使其微缩和提速的难度大于晶体管。因此内存并没有沾摩尔定律的光，带宽和密度的增长落后于CPU、GPU的发展。内存带宽滞后于CPU内核数量的增长导致一个长期问题：平均每个内核的内存带宽增长乏力，甚至出现倒退。譬如第三代至强可扩展处理器内核数28，内存是八通道DDR4 3200，理论上的内存总带宽为205GB/s，平均每核7.3GB/s；四代是56或60核，内存八通道DDR5 4800，总带宽307GB/s，平均每核5.5GB/s；五代提升到DDR5 5600，内核再增加到64，平均带宽改进甚微。第四、五代至强可扩展处理器虽然引入了新一代的DDR5内存，但由于内核数量相对三代翻倍，内存带宽的增长幅度还是跟不上。同时期其他厂商的CPU核数在屡屡跃进的过程当中也存在同样的问题。为了弥补内存带宽增长较慢的问题，第四代至强可扩展处理器给部分用于科学计算的型号引入了HBM，五代则大幅度增加了末级缓存的容量，并支持CXL 2.0内存扩展。</p><p>在至强6900P上，内存问题终于得到了比较好的解决。这涉及三个角度：</p><p></p><p>1、&nbsp;大容量末级缓存。前面提到过，6900P每个网格提供4MB L3，总容量达到了504MB，分别是四代的4.5倍、五代的1.6倍。而且，至强的全网格架构使得任意内核访问末级缓存的延迟相比其他厂商的一些产品有更优的表现，例如不需要跨计算单元而造成延迟剧增。这种架构效率更高的优势也是至强在核数曾落后的情况下还能打的有来有往的关键原因。</p><p></p><p>2、&nbsp;DDR5内存双管齐下提升带宽。至强6900系列支持12通道DDR5 6400，总带宽可以达到614GB/s，平均每核的带宽大致还有5GB/s的水平。6900P还支持新型内存MRDIMM，频率提升至8800MT/s，总带宽达到了845GB/s，平均每核6.6GB/s，也明显超过了前两代产品，大幅度逆转了内核数量增加、平均内存带宽不升反降的问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ae/aef62ab4370e09604d42aa4dc3ade2e9.png" /></p><p>&nbsp;</p><p>MR（Multiplexed Rank）DIMM打开了DDR内存性能提升的新方向。DRAM通常由1到2个Rank组成，每个Rank的位宽为64位，如果考虑ECC，那就会有72或80位，但有效的数据是64位。消费类内存（UDIMM）可能只有1个Rank（颗粒数量较少的情况下），但追求大容量的服务器内存（RDIMM）基本上都至少有2个Rank。在以往的内存模式当中，一次只读取一个Rank的数据，另一个Rank暂时闲置时可以做刷新操作，以保持数据——这种轮流读取、刷新Rank的特点延续了多年。MRDIMM设计了一个数据缓冲区，通过将两个内存Rank分别读入这个缓冲区，再从缓冲区一次性传输到CPU的内存控制器，由此实现了带宽翻倍。第一代DDR5 MRDIMM的目标速率为8800 MT/s，其实每个Rank只相当于4400MT/s。现在DDR5 6400已经开始普及，因此MR DIMM的第二阶段目标是达到12800 MT/s，预计在2030年代的三代会提升至17600 MT/s。</p><p></p><p>3、&nbsp;CXL 内存扩展。第四代至强可扩展处理器开始引入CXL支持，当时是1.1版本，暂时也没有公开支持Type 3设备（也就是CXL内存）。从第五代开始正式引入了CXL 2.0，包括Type 3，可以帮助扩展内存容量和带宽。在至强6上，CXL设备的应用将更为普及，关键的CXL2.0标准设备，以及后向兼容的CXL1.1设备，预计都会陆续涌现。</p><p></p><p>这里重点说一下CXL内存的优势。CXL2.0支持链路分叉，使一个主机端口可以对接多个设备，而且提供更强的CXL内存分层支持，可实现容量和带宽扩展。至强6支持3种CXL内存扩展模式：CXL Numa Node、CXL Hetero Interleaved、Flat Memory。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e8/e81599f831e2b05a29effc2b6069d57b.png" /></p><p>&nbsp;</p><p>在CXL Numa Node模式下，系统的标准内存和CXL扩展内存被视为两个独立的Numa节点进行控制。每个Numa节点都有自己的内存地址空间，系统软件或应用程序可以将任务分配到不同的Numa节点，从而优化内存的使用。CXL Numa Node模式适用于需要精细内存管理的应用，可以通过操作系统、虚拟机管理程序（Hypervisor）或应用程序本身来辅助分层管理内存。</p><p></p><p>Hetero Interleaved（异构交织）模式通过将系统的标准内存和CXL内存混合在一起，形成一个统一的Numa节点。每个内存地址空间中的数据可以交替存储在DRAM和CXL内存中，从而均衡内存带宽，减少延迟。异构交织模式适用于对内存带宽有高需求的应用，特别是当需要将DRAM和CXL内存结合使用时。此模式只有在配备性能核的至强6700P、6900P上才支持。假设将每颗至强6900P的64通道CXL用满，可以额外增加256GB/s的内存带宽，单处理器就可以实现TB级的内存带宽，还是相当可观的。</p><p></p><p>Flat Memory（平面内存）模式下，CXL内存和标准内存被视为单一的内存层，操作系统可以直接访问统一的内存地址空间。硬件辅助的分层管理可以确保常用数据优先存储在标准内存中，次要数据存储在CXL内存中，从而最大限度地提升内存使用效率。平面内存模式最大的价值在于无需修改软件即可利用CXL内存扩展，而且这种模式适用于所有的至强6处理器。但平面内存模式要求标准内存和CXL内存是1:1配置，这略为限制了硬件采办、升级的灵活性。整体而言，平面内存模式是至强6时期最易用、收效最直观的模式，有望成为CXL内存扩展的主要模式。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h3>踏上Chiplet异构之路</h3><p></p><p></p><p>至强6是至强家族首次将计算和IO芯片独立，再通过Chiplet形式封装在一起，总算是把高级封装的优势真正发挥出来了。</p><p></p><p>第四代至强可扩展处理器是英特尔的首个Chiplet设计的至强处理器。其XCC版本内部是4颗芯片通过10组EMIB对等连接，每颗芯片提供15个内核、2通道内存控制器、1组加速单元，以及UPI、PCIe PHY若干。另外，还可以通过EMIB封装4颗HBM。</p><p></p><p>第五代至强可扩展处理器使用2颗芯片封装而成，所使用的EMIB数量明显减少，相应地也节约了芯片面积。虽然内核数量略有增加，但也损失了UPI、PCIe的数量，也不再能够搭配HBM。</p><p></p><p>随着制造工艺演进，偏重计算性能和晶体管密度的处理器内核，与偏重高速信号互联的IO控制器对制造工艺的要求产生了差异，因此，典型的Chiplet设计将计算和IO分离，分别应用不同的制造工艺。英特尔在14代酷睿上便采用了这种方式，分为Compute Tile、SoC Tile、IO Tile、Graphic Tile。代号Ponte Vecchio的英特尔Data Center GPU Max利用Foveros和EMIB技术，将47个小芯片封装在一起，包括Compute Die、Base Die、Rambo、IO Die等。</p><p></p><p>至强6终于也拆分成计算单元（Compute Tile）和IO单元（IO Tile），分别由Intel 3和Intel 7工艺制造。</p><p></p><p></p><p></p><h4>计算单元</h4><p></p><p></p><p>根据收集到的信息，对于能效核，目前只出现了一种计算单元的设计，每个单元最多提供144个内核、4组内存控制器共八通道；对于性能核，则是有三种计算单元的设计，可分别用于组合高核数、中等核数、低核数的规格。</p><p></p><p>至强6900P使用了三个计算单元，每个单元43个内核、两个内存控制器，总共构成129个内核（只使用128个）和12个内存通道。这种计算单元姑且称之为单元A，三个单元A构成的处理器被称为UCC。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c6/c645f4db511cc839f5c5ed2bd575c922.png" /></p><p>&nbsp;</p><p>未来发布的6700P核数跨度会很大，其中单路型号规划为16~80核，多路型号为8~86核。单元A有4个内存通道，两个单元A组合可以提供最高86核，下限应该不低于48核（否则屏蔽的内核数量就实在太多，也太浪费EMIB成本），这种规模的处理器被称为XCC。48核以下的中等核数被称为HCC，使用一种专门开发的单元B，每个单元提供48个内核和4个内存控制器。HCC核数的下限预计在24核左右。8和16核的6700P被称为LCC，需要使用第三种单元C，16个内核和4个内存控制器。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e2/e2f3d4bbe16902f639d0982d3db2a465.png" /></p><p></p><p>通过使用3种计算单元进行组合，至强6性能核可以构建跨度从8~128核的、非常绵密的规格。也许会有人认为，相比其他厂商只用一种规格计算单元实现扩展的设计，英特尔需要设计三颗不同的芯片的成本会更高。但我认为，这是英特尔优先考虑性能的结果。首先，至强6将内存控制器安排在计算单元中，离内核更近，延迟更低，即使因此牺牲了单元组合使用的灵活性也是值得的。其次，至强6性能核给不同规模的内核数量规划不同的网格规模，有利于降低核间的延迟，甚至，有可能LCC会针对较低的核数改用环形总线。综上，预计至强6性能核相对同等规模的其他厂商的产品依旧可能会拥有内存延迟低、缓存延迟低的优势。</p><p></p><p></p><h4>IO单元</h4><p></p><p></p><p>IO单元方面，至强6900、6700系列都使用2颗相同的IO芯片。每个IO芯片由2个IO模块、4个UIO模块、2个加速器模块，以及IO网络接口构成。每个IO模块提供x16 PCIe或CXL连接；每个UIO模块提供x24 UPI2.0，或复用为x16的PCIe或CXL；每个加速器模块提供DSA、IAA、QAT、DLB加速器各一个。</p><p></p><p><img src="https://static001.geekbang.org/infoq/75/75adaf9637d81bfb077b47965db666b7.png" /></p><p>&nbsp;</p><p>以这次发布的至强6900P为例，两个IO单元总共提供8个UIO和4个IO模块。其中6组UIO负责提供6个UPI2.0互连，剩余的2个UIO和4个IO模块正好提供6×16=96通道的PCIe 5.0。双路至强6900P的UPI不但速率高（24GT/s，高于五代的20GT/s和四代的16GT/s），连接数量也提升了50%。</p><p></p><p>对于还未发布、也是主力产品的至强6700系列，估计由于要使用规模较小的插座，只提供最多4组UPI用于多路的互联，PCIe通道也有所缩减。但即使如此，至强6700系列的单路型号在将所有UIO配置为PCIe之后，单插槽就可以提供多达136个PCIe通道，或64通道CXL。如果用单路至强6700配合半宽主板构建双节点服务器，那一个机箱内的PCIe/CXL扩展能力（272 /128）远远超过已知的任何双路服务器。这种机箱可能会成为新的池化形态，可以更高的密度提供NVMe存储、CXL内存、加速器等。</p><p></p><p>&nbsp;</p><p></p><h3>结语</h3><p></p><p></p><p>由于英特尔在14nm到10nm制造工艺的迭代过程遇到了一些问题，以致此前几代至强平台在“核战”（比拼核数）中略显被动，但这个局面在至强6上有望完全逆转，改良后的EUV制造工艺看来没有束缚至强6的实力，核心数量、缓存容量、内存带宽等关键指标全都进入领先行列，一句话总结就是算力和存力的表现全部拉满。至强6900P系列在各种项目的测试当中，其代际性能提升就都是以倍数计，而非百分之十几、几十的进步。这种形势也使得英特尔得以全面竞争科学计算、大数据、AI等领域的性能王座。</p><p></p><p>此外，至强6终于实现计算与IO的解耦，也让至强6及未来的产品线走上了正确、灵活的道路，得以充分发挥Chiplet的优势。将Chiplet视作降低成本、提高良率的手段是狭隘的。Chiplet的价值在于灵活、复用、重构。英特尔长期以来很注重细分市场的耕耘，产品线非常复杂，正确利用Chiplet可以达到事半功倍的效果。我们非常期待至强6后续产品的陆续发布能够给业界带来什么样的想象力。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/MSGy0FTNtt6eqOKZIcKO</id>
            <title>英特尔最强服务器CPU来了！AI性能直接翻倍</title>
            <link>https://www.infoq.cn/article/MSGy0FTNtt6eqOKZIcKO</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/MSGy0FTNtt6eqOKZIcKO</guid>
            <pubDate></pubDate>
            <updated>Sun, 29 Sep 2024 02:50:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>文章来源：英特尔</p><p>&nbsp;</p><p>服务器CPU领域持续多年的核心数量大战，被一举终结了！</p><p>&nbsp;</p><p>英特尔最新发布的至强® 6 性能核处理器（P-core系列），超越了过去单一维度的竞争，通过“升维”，定义了新的游戏规则：</p><p>&nbsp;</p><p>&gt;算力、存力，要全方位提升。不能做到这一点的CPU，不是智算时代的好U。</p><p>&nbsp;</p><p>在过去，CPU升级换代往往要在单个芯片上集成更多的核心，但这难免会受到工艺和芯片尺寸的限制，更别提与IO和内存的匹配难题。</p><p>&nbsp;</p><p>这一次，至强® 6 性能核处理器采用了计算芯片单元与I/O芯片单元解耦的分离式模块化设计，可以灵活组合不同数量的计算单元，实现核心数量的扩展及内存和IO的同步强化，保证更优的整体性能和能效。</p><p>&nbsp;</p><p>用最直观的方式感受一下：</p><p>&nbsp;</p><p>2023年12月15日，英特尔数据中心与人工智能集团副总裁陈葆立从裤兜里掏出第五代至强® 可扩展处理器，还只有64个核心。</p><p>&nbsp;</p><p>2024年9月26日，还是陈葆立，同样从裤兜里掏出至强® 6性能核处理器，却直接翻倍到128核心。</p><p>&nbsp;</p><p>两款处理器外形大小相似，都能轻松放入口袋，但性能却发生了质的飞跃。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/cf/cfa46c5121806f32eba9da6444507880.png" /></p><p>&nbsp;</p><p>&nbsp;</p><p>具体来说，刚刚登场的是至强® 6性能核处理器大家族中的先锋+顶级战力——英特尔® 至强® 6900P系列。</p><p>&nbsp;</p><p>拥有多达128个性能核和504MB的超大L3缓存，更大、更宽的内存支持，更多、更快的IO能力。非常适用于各种数据和计算密集型应用任务，比如科学计算、海量数据处理，还有AI。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/d7/d7b97852acc8fa1f63bd2b0424af9999.png" /></p><p>&nbsp;</p><p>&nbsp;</p><p>看到这里，可能很多人会有疑问：你们又要说用CPU跑AI？是GPU它不香了么？</p><p>&nbsp;</p><p>NoNoNo，我们是想说：有了这款CPU，你的GPU或其他的AI加速器，会更香！</p><p>&nbsp;</p><p>谈到这个话题，就要先说说AI服务器。</p><p>&nbsp;</p><p>在生成式AI应用百花齐放的当下，AI服务器的重要性可谓是不言而喻，无论是对于大规模的训练、推理，亦或是RAG等任务，都对其提出了更高的要求。</p><p>&nbsp;</p><p>也正如综合市场预测数据从侧面反应出来的那般：</p><p>&nbsp;</p><p>&gt;AI服务器市场规模已经达到了211亿美元，预计2025年达到317.9亿美元，2023-2025年的CAGR为22.7%。</p><p>&nbsp;</p><p>我们都知道AI服务器里GPU或AI加速器很重要，却很容易忽视其中CPU的作用。一个真正为AI服务器或AI数据中心基础设施设计的出色的CPU，应该是什么样的？</p><p>&nbsp;</p><p>英特尔® 至强® 6 性能核处理器，可以说是给出了一个正解。</p><p>&nbsp;</p><p>外媒甚至评测过后，对英特尔这次的新CPU给予了极高的评价：</p><p>&nbsp;</p><p>&gt;不仅仅是Xeon，更是XEON。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/56/5639f3bf33d5cb4f753acf5c9d164a28.png" /></p><p>&nbsp;</p><p>&nbsp;</p><p>嗯，用中文来说的话，就是英特尔至强，这次是真的至强（达到最强）了。</p><p>&nbsp;</p><p>那么英特尔® 至强® 6 性能核处理器是如何解锁这种认同的呢？</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h3>至强，何以至强</h3><p></p><p>&nbsp;</p><p>首先要说的是算力。</p><p>&nbsp;</p><p>英特尔® 至强® 6900P系列产品此次最亮眼的128核（三个计算芯片单元），这就是它看似符合此前游戏规则的一大技术亮点。</p><p>&nbsp;</p><p>通过核心数量的不同排列组合方式，至强® 6 性能核处理器可以应对不同的场景来提供不同核心的型号，除了最高128核的产品系列（6900P）外，还有最高86核（2个计算芯片单元），最高48核（1个计算芯片单元）和16核（1个计算芯片单元）的产品系列。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/de/ded710e09e22b93c06f2a5e3522f1db1.png" /></p><p>&nbsp;</p><p>&nbsp;</p><p>用来做这种排列组合的模块中，计算芯片单元采用的是Intel 3制程，包含一体式网格、核心、缓存、内存控制器等，可以保证数据传输的一致性。</p><p>&nbsp;</p><p>I/O芯片单元则是采用Intel 7制程 ，包含UPI、PCIe、CXL和加速器引擎等。</p><p>&nbsp;</p><p>不同于第五代英特尔® 至强® 产品，至强® 6是将I/O和计算两个单元进行了解耦，不仅易于做核数的扩展，还有利于验证、重复和灵活使用。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/31/31fa668c1a7c405c9f729b5a2242bfe1.png" /></p><p>&nbsp;</p><p>&nbsp;</p><p>除此之外，英特尔® 至强® 6 性能核处理器的亮点还包括：</p><p>&nbsp;</p><p>•&nbsp;6400 MT/s DDR5</p><p>•&nbsp;8800 MT/s MRDIMM内存</p><p>•&nbsp;6条UPI 2.0链路；速率高达24 GT/s</p><p>•&nbsp;96条PCIe 5.0/ 64条 CXL 2.0通道</p><p>•&nbsp;L3缓存高达504MB</p><p>•&nbsp;支持FP16数据格式的英特尔® AMX</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/95/95902177a90a0c83624647c8bacfc3b3.png" /></p><p>&nbsp;</p><p>&nbsp;</p><p>接下来要说的是存力。</p><p>&nbsp;</p><p>至强® 6 性能核处理器超脱此前游戏规则的亮点就藏在其中。</p><p>&nbsp;</p><p>它同时支持了更快的DDR5内存（6400MT/s）和更“宽”的MRDIMM内存（8800MT/s）。</p><p>&nbsp;</p><p>仅把前者替换成后者，就已经能让科学计算和AI场景的多项任务提升7%-33%不等了。而且相比此前至强® CPU Max采用的HBM，MRDIMM内存的引入，不仅带宽和速度优势更明显，它与CPU解耦的型态，也更利于用户的灵活采购、配置与升级。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/42/42379e9b951a966afdc0ed3fcd8fcbbe.png" /></p><p>&nbsp;</p><p>&nbsp;</p><p>存力除了内存本身的性能，还包含CPU与内存之间的互连技术，至强® 6导入了最新的Compute Express Link 2.0 (CXL 2.0) 。</p><p>&nbsp;</p><p>CXL 2.0支持多种设备类型，且可向后兼容，实现对内存和存储设备的灵活扩展。</p><p>&nbsp;</p><p>支持链路分叉、更强的CXL内存分层支持，以及以受控热插拔的方式添加/移除设备，为未来的数据中心架构带来了更多可能性。</p><p>&nbsp;</p><p>更值得一提的是至强® 6独占的“Flat”内存模式，CXL内存和DRAM内存被视为单一的内存层，让操作系统可以直接访问这一统一的内存地址空间。</p><p>&nbsp;</p><p>这样的分层管理可以确保最大限度地提升内存使用效率，并且实现利用好CXL内存扩展而无需修改软件。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/c5/c57be50e1c25854713a664eefa2b815c.png" /></p><p>&nbsp;</p><p>&nbsp;</p><p>如此这般能对内存速度、带宽、容量和可扩展性全面兼顾，已经形成了至强® 6 性能核处理器独树一帜的竞争力。</p><p>&nbsp;</p><p>具体到服务器设计上，CLX2.0可以支持每机提供8TB内存容量扩展，同时提供384GB/s的内存带宽扩展。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/24/24b477bdf49d878212580e7eb96f3122.png" /></p><p>&nbsp;</p><p>&nbsp;</p><p>当然，作为CPU的至强® 6 性能核处理器并没有忘记自己的本份，把存力与算力的硬指标优势结合起来，转化成真正的优势，才是它被看好的底气。</p><p>&nbsp;</p><p>在算力方面，除了更多内核，它还有内置加速器与指令集更新带来的加成。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/fa/fae82d6f65ad746bd289560fa5fda36f.png" /></p><p>&nbsp;</p><p>&nbsp;</p><p>主攻AI加速的英特尔® 高级矩阵扩展（Intel® AMX） 新增对FP16数据类型的支持，现已全面覆盖 int8、BF16 和 FP16 数据类型。</p><p>&nbsp;</p><p>其在每个内核中的矩阵乘加（MAC）运算速度可达 2048 FLOPS（int8） 和 1024 FLOPS（BF16/FP16），能大幅提升 AI 推理和训练性能。</p><p>&nbsp;</p><p>英特尔® 高级矢量扩展 512（AVX-512）虽然是员老将了，但在得到如此丰沛的内核资源支持后，也依然是科学计算、数据库和 AI 任务中的矢量计算担当。</p><p>&nbsp;</p><p>这些加速器的升级与焕新带来的成果就是下图这种多负载性能表现普遍倍增的现象，在AI领域，尤其是在Llama2-7B上的提升直接达到了前一代产品的3.08倍。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/74/743481e78704bf446f2e246932dd08f2.png" /></p><p>&nbsp;</p><p>&nbsp;</p><p>最后在硬件增强的安全特性方面，英特尔早期的方案为SGX，但从第五代至强® 开始新增了&nbsp;TDX 方案。这些看似难以通过 Benchmark 数值来证明自身价值的技术，实则不可或缺，是确保关键数据和应用更为安全可靠的压舱石。</p><p>&nbsp;</p><p>而安全，恰恰是目前AI 数据中心或智算中心这种涉及海量数据、关系万千机密和隐私的环境中较少提及，却最应补足和巩固的一环。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/18/18932a6ce1952420fc72f5427f9f4f1f.png" /></p><p>&nbsp;</p><p>&nbsp;</p><p>说了这么多，如果要用一句话总结至强® 6 性能核处理器，尤其是6900P系列产品的定位，那就是“更强通用计算，兼顾 AI 加速”了。</p><p>&nbsp;</p><p>那么新处理器具体都有哪些用法，表现又如何呢？</p><p>&nbsp;</p><p>还请继续往下看。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h3>全能型CPU：加速AI推理，统领异构计算</h3><p></p><p>&nbsp;</p><p>首先，至强® 6 性能核处理器可以做“独行侠”，直接加速AI推理，助力AI应用普及。</p><p>&nbsp;</p><p>用CPU做AI推理加速，其意义并非在于与GPU或其他专用加速器竞争极致的速度或效率，而是要在一些成本、采购、环境等条件受限的情况下，借助CPU部署更广泛、人才储备更扎实和应用更便捷的优势，让AI能够更快、更有效地落地。</p><p>&nbsp;</p><p>带着这样的整体目标，英特尔在软件生态和工作负载优化方面投入了大量精力，以确保用户能够充分发挥至强® 6 性能核处理器的潜力。</p><p>&nbsp;</p><p>例如，英特尔与TensorFlow和PyTorch等主流深度学习框架进行深度合作，将针对英特尔CPU的优化集成到官方发行版中，从而使得在英特尔CPU上运行深度学习模型时，性能得到显著提升。上文提到的Llama2-7B成绩便是这些努力的成果之一。</p><p>&nbsp;</p><p>另外，至强® 6 性能核处理器还可以做“指挥官”，强化AI系统整体实力。</p><p>&nbsp;</p><p>这其实是很多用户更为熟悉，也是至强® 6性能核处理器更主打的应用方式，所谓“指挥官”，另一个名称就是机头（head- node）CPU或主控CPU。</p><p>&nbsp;</p><p>如果将至强® 6 性能核处理器用作AI服务器的机头CPU，那么其在算力（更强的单线程性能）、存力（对 MRDIMM 内存和 CXL 内存扩展能力的支持）以及 I/O（更多的 PCIe 5.0 通道）等方面的优势和潜能就能更加充分地发挥和释放出来。</p><p>&nbsp;</p><p>使其能够与GPU 或专用的 AI 加速器高效协作，出色地处理数据预处理、数据传输分享和混合工作负载。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/3d/3d538b52bed870be278d891fb99d2d15.png" /></p><p>&nbsp;</p><p>&nbsp;</p><p>我们最初的设问，至此终于拼凑出了一个更为完整的答案，即为何至强® 6 性能核处理器能够被称作AI服务器或AI数据中心的“严选”，甚至是优选CPU ？</p><p>这正是由于它既能够单枪匹马地加速AI推理，又可以居中协调以提升异构系统的整体性能输出。</p><p>&nbsp;</p><p>更不必说，它还能够兼顾众多传统但同样不可或缺的应用负载，例如前文提及的科学计算和数据库，以及高性能云基础设施构建等任务。</p><p>&nbsp;</p><p>以Flatiron Institute的案例来说，作为一家科研机构，他们对科学计算有着强烈的需求。通过测试得知，至强® 6 性能核处理器在常见科学计算负载上表现优异。</p><p>&nbsp;</p><p>他们还觉得对MR DIMM内存的支持将进一步突破传统DDR内存的性能瓶颈，推动数据密集型科学发现。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ecb20b6c416a9ec459d599f34e7a66d6.png" /></p><p>&nbsp;</p><p>&nbsp;</p><p>在本次至强® 6 性能核处理器的发布会上，英特尔也展示了本地数据库软件合作伙伴——科蓝软件的成果。</p><p>&nbsp;</p><p>英特尔市场营销集团副总裁，中国区&amp;行业解决方案和数据中心销售部总经理梁雅莉在介绍生态系统支持状况时表示：</p><p>&nbsp;</p><p>&gt;基于我们的新品，科蓝软件构建了高性能国产分布式数据库，其吞吐较第五代至强® 可扩展处理器提升达到 198%。</p><p>&nbsp;</p><p>值得一提的是，在她分享中出现的中国合作伙伴数量众多且都是各领域的核心力量，英特尔虽然在产品研发上有了更多创新，但在商业模式上仍然非常依赖开放架构平台之上的产业合力。</p><p>&nbsp;</p><p>十数家OEM、ODM、OSV 和 ISV 在至强® 6 性能核处理器发布时同步推出新产品，以及多家云服务提供商的支持，在英特尔看来，才是新品真正走近用户和价值放大的基础。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/7a/7a4cce7eb143ebff3e018121e350175f.png" /></p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h3>建好AI服务器，CPU不能是短板</h3><p></p><p>&nbsp;</p><p>从前面列举的众多数据和用例可以看出，在当前AI应用加速落地、新推理计算范式和合成数据等趋势的推动下，AI 算力需求越来越注重推理和复合工作负载。</p><p>&nbsp;</p><p>在这之中GPU或专用加速器固然重要，但CPU 作为整个系统的“指挥官”，绝不能成为短板。</p><p>&nbsp;</p><p>大家需要真正兼顾通用计算，以及AI服务器及AI数据中心场景的CPU产品。它不仅能支持广泛的第三方GPU及AI加速器，与它们组合形成强大的异构计算平台，还能在其中补足GPU或专用加速器覆盖不到或不足的地方，为更多样和复杂的场景提供灵活的算力选择，并增强整个 AI 平台的稳定性、安全性和扩展性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/55/55c61204c269dd4a914a84dddb6c0e4a.png" /></p><p></p><p>英特尔® 至强® 6 性能核处理器的出现，就为AI计算带来了这样一个全新的支点。</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/A8a1CtIhqfRrAqF9eo3c</id>
            <title>Meta 开源 DCPerf：一个面向超大规模云工作负载的基准测试套件</title>
            <link>https://www.infoq.cn/article/A8a1CtIhqfRrAqF9eo3c</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/A8a1CtIhqfRrAqF9eo3c</guid>
            <pubDate></pubDate>
            <updated>Sun, 29 Sep 2024 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Meta 最近发布了 DCPerf，它可以模拟数据中心云部署中的各种工作负载。对于研究人员、硬件开发人员和互联网公司来说，该基准测试集有望成为一项非常有用的资源，帮助他们设计和评估未来的产品。</p><p></p><p>在一篇博文中，来自 Meta 的 Abhishek Dhanotia、Wei Su、Carlos Torres、Shobhit Kanaujia 和 Maxim Naumov 着重说明了超大规模云数据中心工作负载的独特性。他们强调，这些主导服务器市场的工作负载与高性能计算（HPC）或传统企业场景中的工作负载有着很大的不同。因为存在这种差异，所以需要专门的服务器设计和评估方法，也就是说，需要专用的基准测试。</p><p></p><p>DCPerf 是一个基准测试套件，设计用来模拟现实世界的超大规模云应用程序，旨在为硬件供应商、系统软件开发商和研究人员提供评估新产品、开展性能预测和建模的工具。这种方法可以反映互联网应用程序公司开发并部署在超大规模云数据中心中的实际生产负载。</p><p></p><p>从检查底层硬件微体系结构特性到分析应用程序和库的使用概况，Meta 团队采用了多种技术来确保其基准测试的代表性。这种方法使他们能够捕获生产负载的关键特征，并将其纳入到 DCPerf 中。</p><p></p><p>有了这些基准测试，基于未来服务器平台的软硬件设计及优化工作将更直接地转化改进超大规模生产部署的效率。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c2/c279e3eec3184b20921edfde4caef528.png" /></p><p></p><p>图片来源：DCPerf：一个面向大规模计算应用的开源基准测试套件</p><p></p><p>Meta 确保了该套件与各种指令集体系结构（x86、ARM）的兼容性，验证了它在新兴技术（如芯片粒子（chiplet））中的有效性，并添加了多租户支持，以便可以利用现代服务器上不断增加的内核数。</p><p></p><p>当 Hacker News 分享这一消息 时，技术社区注意到，它与专为谷歌工作负载定制的基准测试套件 Fleetbench 相似。Fleetbench 的 C++ 代码旨在帮助芯片供应商、编译器研究人员和其他希望提高类谷歌工作负载性能的人。</p><p></p><p>Meta 内部一直在使用 DCPerf 和 SPEC CPU 基准测试套件，以增强他们在产品评估和数据中心配置选择方面的能力。这种方法可以针对容量规划做早期性能预测，有助于确定硬件和软件中的性能问题，并促进与硬件合作伙伴协作开展平台优化。</p><p></p><p>与 SPEC CPU 等传统基准测试相比，DCPerf 的应用程序软件集提供了对平台性能更全面的洞察。在认识到这些好处之后，Meta 将 DCPerf 集成到了其数据中心部署过程中的平台选择流程中。</p><p></p><p>在评估和优化 CPU 性能、IPC、内存延迟以及在一定程度上评估和优化内存带宽和功耗方面，DCPerf 具有很强的适用性。但是，它在网络和存储评估方面的用处不大，并且受限于特定的工作负载。这篇博文着重介绍了 DCPerf 可能需要进一步开发的特定方面，或者用户在解释结果时应该谨慎对待的地方。</p><p></p><p>对于 DCPerf，Meta 感谢其合作者的支持和贡献。感兴趣的读者可以在 GitHub 上了解项目的更多细节。</p><p></p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2024/08/meta-dcperf-benchmark-suite/">https://www.infoq.com/news/2024/08/meta-dcperf-benchmark-suite/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/YcV5XvSriI5kPOkxCYuW</id>
            <title>大模型之争深水期，企业如何真正实现产业级落地？</title>
            <link>https://www.infoq.cn/article/YcV5XvSriI5kPOkxCYuW</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/YcV5XvSriI5kPOkxCYuW</guid>
            <pubDate></pubDate>
            <updated>Sat, 28 Sep 2024 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>大模型之争，已经进入了深层阶段。</p><p></p><p>随着"百模大战"逐渐平息，整个模型市场步入了一个更为冷静的时期。但表面的宁静下，实则暗流涌动，竞争已经从单一的模型能力转向了更为复杂的生态系统建设。真正的较量在于谁能将模型的力量转化为实际应用，谁能为企业解锁更大的价值。</p><p></p><p>在这场转型的浪潮中，企业面临的挑战是全方位的，从垂直模型的开发到模型的调用，再到应用的开发，每一步都充满了难题。成本、效率，这些关键词在当下的环境中变得尤为突出。企业对于模型的需求不断增长，而大模型所蕴含的潜力，正在推动着生产力的革新。</p><p></p><p>在中国，随着国内大模型行业的快速发展，我们已经迈过了早期的混战阶段。现在，随着国内企业的数智化转型需求日益迫切，大模型的落地应用正成为行业发展的新焦点。各行各业对于模型的需求呈现出爆炸性的增长。</p><p></p><p>对于模型厂商而言，这是一个充满机遇的新时代。9 月 25 日，2024 百度云智大会上，百度智能云不仅展示了其在大模型产业落地方面的最新实践，还发布了包括基础设施、大模型、开发工具链、AI 原生应用等多个云与 AI 产品。在这次大会上，百度智能云千帆大模型平台 3.0 的发布成为了焦点。基于“加速企业大模型产业落地”的理念，千帆 3.0 旨在帮助企业更高效地实现产业 AI 化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2d/2d90bee1cb8500865573d4cecb6076c5.webp" /></p><p></p><p>那么，百度智能云如何利用千帆助力企业落地大模型？千帆 3.0 相比前代产品，又带来了哪些显著的升级？其又将如何帮助企业克服落地大模型过程中的重重困难？</p><p></p><p></p><h2>1企业大模型落地，到底有多难？</h2><p></p><p></p><p>企业落地大模型，到底需要克服哪些困难？</p><p></p><p>首先，算力问题是企业落地大模型的关键瓶颈。高性能的硬件资源不仅成本高昂，而且对大多数企业来说，如何有效管理和优化这些资源以支持大模型的运行和训练，成为了一项技术和经济上的双重挑战。企业在构建大模型时，往往需要投入大量资金购买高性能的计算设备，这对于资金实力有限的中小企业来说无疑是一个巨大的负担。</p><p></p><p>此外，算力的管理和优化也需要专业的技术团队进行维护和调整，进一步增加了企业的运营成本。因此，全套的大模型落地基础设施已经成为企业落地大模型的重中之重，算力瓶颈和成本问题。</p><p></p><p>其次，平台的兼容性问题同样不容忽视。不同系统和框架之间的集成往往需要额外的开发工作和技术支持，这进一步增加了企业的技术负担。许多企业在实施大模型时，发现现有的 IT 基础设施与新引入的模型技术之间存在兼容性问题，导致集成过程复杂且耗时。</p><p></p><p>除了算力和平台兼容性，企业在大模型落地过程中还需要关注开发层和服务层的需求。开发层涉及到模型的构建、训练和优化，而服务层则包括模型的部署和维护。企业在这两个层面上都需要切实可行的服务方案，以提升整体生产力。</p><p></p><p>在开发层，许多企业在实际业务场景中需要同时使用大模型和垂直模型。例如，在教育行业，企业可能需要结合大模型的自然语言处理能力和垂直模型的知识图谱，以实现个性化的学习体验。然而，开发这些模型通常需要较高的技术门槛和丰富的行业知识，这对企业的技术团队提出了更高的要求。</p><p></p><p>在服务层，企业需要确保模型在实际应用中的稳定性和可靠性。大模型的维护和更新是一个持续的过程，企业需要定期对模型进行监控和调整，以适应不断变化的业务需求和市场环境。这就要求企业具备灵活的平台架构，以便快速响应市场变化。</p><p></p><p>可以看到，企业在大模型落地过程中面临的挑战是多方面的，涉及模型调用、模型开发和应用开发等多个层面。为了有效应对这些挑战，企业需要借助像百度智能云千帆这样的整合平台，提供全方位的支持与服务。尤其在 AI 应用开发这一环节，企业迫切的需要一个工具甚至是平台，帮助他们触达最前沿的模型，调用最强的能力，实现最高效的开发。百度智能云的千帆 3.0 正是为此而生。正如云智大会上所说，千帆 3.0 为企业提供了一整套从模型开发到模型服务再到应用开发的全流程工具，层层结合，每一层都迎来了全新的能力升级。</p><p></p><p><img src="https://static001.geekbang.org/infoq/25/25a8405535f7b6bf866a9ad865226503.webp" /></p><p></p><p>根据 IDC 发布的《2023 中国大模型平台市场份额》报告，中国大模型平台及相关应用市场规模已达 17.65 亿元，百度智能云在这一领域占据了市场的领先地位。也正是在千帆调用量指数级增长下，百度智能云通过实践找寻到了大模型重构企业业务的方向，逐渐构建出千帆 3.0 这一能够满足企业级场景的“生产力工具”。</p><p></p><p></p><h2>从模型开发到调用服务，百度智能云如何帮助企业高效能、低成本落地使用大模型</h2><p></p><p></p><p>想象一下，如果你是一家制造企业的 CTO，你的公司需要利用大模型技术来提升产品质量，优化生产流程，但你却不知从何入手，到底是选择自己开发垂直模型，还是对主流开源模型进行调用，该如何将其接入企业内部的智能化系统，对于 AI 化技术刚刚入门的你来说，或许十分头疼。</p><p></p><p>千帆 3.0 正在为这样的企业搭建一个专属于模型开发层的产业级开发工具链。许多企业在实际业务中需要同时运用大模型和垂直行业模型，比如在金融领域，可能需要一个能够理解复杂查询的大型语言模型，同时也需要一个能够精准识别表格的 OCR 模型。千帆 3.0 提供的全套产业级开发工具链，正是为了解决这一需求而设计的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7f/7f7177af13f36a891aea7504cae5037c.webp" /></p><p></p><p>那么千帆 3.0 是如何做到帮助企业进行高效模型开发的？</p><p></p><p>首先，千帆 3.0 提供一站式模型开发服务，涵盖从数据处理、模型训练到服务部署的整个生命周期。其数据管理工具能够高效地处理和清洗大规模数据集，确保数据质量；模型训练方面，千帆 3.0 结合百度智能云百舸等基础设施，通过强大的算力资源与异构计算能力，显著提升了模型训练的速度和效果。千帆 3.0 还构建了完整的模型开发工具链，实现数据、模型、算力资源的统一纳管和调度，从而提高资源利用率和开发效率。此外，千帆 3.0 还提供了自动化的模型管理工具，支持模型版本控制、模型评估和模型优化等功能。这些工具能够帮助企业更好地管理和迭代模型，确保模型的稳定性和可靠性。</p><p></p><p>而在模型服务层，千帆 3.0 则遵循一个原则，那就是：灵活调用，按需服务。</p><p></p><p>千帆 3.0 在模型服务层进行了升级，提供文心大模型系列“全家桶”的调用服务，除了适用于复杂场景的旗舰大模型外，还包含了最适合精调的主力大模型、适合端侧的轻量级大模型，以及适合某些特定场景的垂直场景模型模型，并在此基础上，新增语音系列能力模型和视觉系列模型，使得大模型与传统模型充分协同，解决用户更复杂的场景需求。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9d/9de61636a90ca9764771f6276a33677b.webp" /></p><p></p><p>通过提供从模型开发到模型服务的全流程支持，千帆 3.0 不仅为企业提供了一个高效、灵活的解决方案，更重要的是其真正在降低企业实现 AI 化的门槛。其真正可以做到企业需求全覆盖，企业在模型开发与调用的过程中的基本场景均可满足，同时在“高性价比”的支持下，让模型能力不再被“束之高阁”，真正走到万千企业中去。在这个过程中，企业能够更专注于自身的核心业务，借助 AI 技术提升竞争力。</p><p></p><p></p><h2>AI 应用开发趋势已起，如何构建模型应用繁荣生态？</h2><p></p><p></p><p>在如果说千帆 3.0 的模型开发以及模型服务解决了企业 AI 化的问题，那么 AI 应用开发则是真正走到了“落地”这一步。如何将这些前沿技术转化为实际可用的应用，解决实际问题，提升用户体验，百度智能云的千帆 3.0 在 AI 应用开发方面实现了重大升级。</p><p></p><p>在过去的几年里，百度智能云千帆在 AI 应用开发领域不断发力，千帆 2.0 就已经开始在模型服务、模型开发的基础上，进一步为用户提供非常易用的 AI 原生应用开发工具，进一步催动了国内 AI 原生应用生态的日渐繁荣。</p><p></p><p>千帆 3.0 则不仅仅只是简单的功能升级，其已经真正进化成为“企业级”的 AI 应用开发平台。它提供了一整套的工具和框架，使得企业能够利用大模型的能力，快速构建和部署 AI 应用。这对于那些缺乏 AI 开发经验，但又急需智能化升级的企业来说，无疑是一个福音。</p><p></p><p>而深入进此次的功能升级，在千帆 3.0 中，企业级 Agent 开发是一大亮点。</p><p></p><p>什么是企业级 Agent？通常是指为企业提供服务的智能软件应用，它能够模仿人类的行为和决策能力，以提高企业的运营效率和改善客户体验。企业级智能体可以应用于多种场景，包括但不限于客户服务、内部业务流程自动化、数据分析和决策支持等。比如，在客户服务领域，企业可以利用这些工具开发出能够理解复杂用户查询并提供精准答案的智能客服系统。在制造业，企业可以开发出能够预测设备故障并提出维护建议的智能监控系统。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2d/2ddf322b84c1038690dea325ddc6470d.webp" /></p><p></p><p>但在过去的很长一段时间里，企业业务场景下 Agent 往往存在着很多问题，比如基于模型幻觉问题导致的“已读乱回”，缺乏长期记忆导致的多轮对话失效，以及碍于客观条件导致的能力欠缺，一系列问题之下，企业用户难以将 Agent 融入进业务中，企业级 Agent 也最终成为了一个“伪命题”。</p><p></p><p>但基于千帆 3.0 搭建的企业级智能体则能最大限度的避免这些问题。首先是极致的效果，目前基于千帆 3.0 搭建的企业级 Agent 可以灵活配置大模型以及垂类模型，规划调度准确率能够达到 95% 以上。同时借助人工编排功能，可以最大程度的降低大模型幻觉，从而更稳定地还原业务 SOP，让企业级 Agent 实现标准化，解决以往的失控问题。</p><p></p><p>除此之外，在长效记忆、知识注入、周边工具的丰富性上，千帆 3.0 都实现了全面升级，千帆企业 Agent</p><p>的记忆准确率可以达到 96%+，并且可以保持持续学习，持续进化，而 80+ 的高质量组件更能够进一步扩展 Agent 的边界，3D 数字人、语音对话等多模态功能，也进一步让企业级 Agent 的功能更加丰富，使用体验也同步提升。</p><p></p><p>借助千帆 3.0，智能体的开发不再需要从头开始编写复杂的代码，企业只需根据自己的业务需求进行简单的配置和调整，就可以快速生成所需的智能体。</p><p></p><p>千帆 3.0 的另一个核心工具是 AI 速搭，这是一个端到端的应用开发工具，它通过自然语言处理技术，允许企业通过自然语言描述来创建应用。这种方式极大地降低了 AI 应用的开发门槛，使得非技术背景的业务人员也能够参与到 AI 应用的开发中来。</p><p></p><p>例如，一个业务人员想要开发一个客户反馈收集系统，他只需要用自然语言描述这个系统应该具备的功能，AI 速搭就能够根据这样的描述，自动生成一个初步的应用框架。这个框架还可以通过低代码 GUI 的方式进行调整和完善，使得最终的应用能够完全符合企业的实际需求。</p><p></p><p>基于企业级 Agent 开发工具与 AI 速搭，千帆 3.0 进一步降低了 AI 应用开发的门槛。在当前 AI 应用开发人才紧缺的情况下，这样的普世化工具显得尤为重要。它们不仅降低了企业对于专业 AI 开发人才的依赖，也使得更多的业务人员能够参与到 AI 应用的开发中来。这不仅提升了企业的创新能力，也为企业的数字化转型提供了强大的动力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/21/2166330b82048a436463a8bd7adb9eb5.webp" /></p><p></p><p>千帆 3.0 的推出，是百度智能云对企业智能化需求的深刻理解和积极响应。它不仅提供了强大的模型开发和调用能力，更提供了一整套的应用开发工具，使得企业能够快速构建和部署 AI 应用。在这个过程中，企业能够更专注于自身的核心业务，借助 AI 技术提升竞争力，实现智能化转型。</p><p></p><p>与此同时，其为企业构建 AI 生态提供了坚实的基础。企业不仅可以开发出满足自身需求的智能应用，还可以将这些应用整合到更大的业务流程中，构建出一个完整的 AI 生态系统。</p><p></p><p></p><h2>产业实践为先，大模型如何切实解决行业发展问题？</h2><p></p><p></p><p>而在产品之外，AI 技术落地需要考虑更多复杂的场景，面对不同行业，不同需求，如何提高模型利用的效率，解决实际问题，则是全行业都在解决的问题。百度智能云的优势也正在于此，作为全国市场份额第一位的 AI 技术厂商，在汽车、能源电力、港口、钢铁、教育等行业，百度智能云均有落地标杆案例，基于这些行业实践经验，千帆 3.0 也同步提供了八大行业的场景解决方案，针对性的帮助企业解决实际问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/eb/eb5915dcfaf315530e83c56c4a729018.webp" /></p><p></p><p>以考试宝为例，作为一款服务于广大学习者的在线学习平台，它面临的挑战是显而易见的。在教育行业，题目解析是一项既耗时又耗力的工作。借助千帆大模型平台，考试宝能够自动解析用户上传的题目，快速生成准确的答案和解析，极大地提升了用户体验和效率。</p><p></p><p>在过去，用户上传一道题目后，往往需要等待一段时间才能得到反馈。而借助大模型的能力，使得考试宝能够实现近乎实时的解析，用户即刻就能得到题目的解析和答案。这种速度的提升，不仅让用户感到惊喜，也使得学习过程变得更加流畅和高效。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a6/a613e3b2b5c7cc0154c8ddb1f2a288aa.webp" /></p><p></p><p>对于企业来说，成本控制同样重要。千帆 3.0 的大模型技术，通过自动化的题目解析，大幅度降低了对人工审核的依赖，从而减少了人力成本。同时，自动化流程的高效性，也减少了因等待解析而造成的潜在机会成本。</p><p></p><p>除了教育行业，人事、企业服务、社交文娱、办公、电商营销、智能硬件、医疗、汽车等行业场景解决方案的推出，将为企业提供了一条条可行的智能化转型升级之路。以医疗行业为例，千帆能够辅助医生进行病例分析，提供辅助诊断建议，从而提升医疗服务的质量和效率。在汽车行业，借助千帆能够进行智能驾驶系统的开发，提高车辆的智能水平和安全性。</p><p></p><p>千帆 3.0 的目标是降低企业使用大模型的门槛，让更多企业能够享受到 AI 技术带来的红利。通过提供全栈的基础设施和工具，千帆 3.0 让企业无需从头开始构建复杂的 AI 系统，而是可以直接利用现有的模型和工具，快速实现业务的智能化。</p><p></p><p></p><h2>结语</h2><p></p><p></p><p>在数字化转型的浪潮中，百度智能云的千帆 3.0 正成为企业数智化转型的强大助推器。它不仅提供了全栈的基础设施支持，还通过一站式解决方案，大幅降低了企业在 AI 领域的入门门槛。千帆 3.0 的设计理念，是将复杂的 AI 技术封装成简单易用的工具，让企业能够快速地将 AI 能力应用到实际业务中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/75/7546147c4e09b6bf1c11f0e7d2870f1f.webp" /></p><p></p><p>随着技术的不断进步和实践的深入，千帆 3.0 将继续赋能企业，推动行业向更智能、更高效的未来迈进。正如在 2024 百度云智大会上所强调的，“智能科技是推动产业升级的核心动力”，千帆 3.0 正是这一理念的完美体现。它不仅推动了大模型的广泛应用，也为构建繁荣的 AI 应用生态奠定了坚实基础。未来，千帆 3.0 将继续引领企业在智能化的道路上走得更远、更快。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/dgkW2Y83r09pOKIAljCW</id>
            <title>老程序员有责任培养新人拯救行业！专访世界编程大师 Uncle Bob：不懂编程只会用 AI 助手是行业灾难！</title>
            <link>https://www.infoq.cn/article/dgkW2Y83r09pOKIAljCW</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/dgkW2Y83r09pOKIAljCW</guid>
            <pubDate></pubDate>
            <updated>Fri, 27 Sep 2024 10:39:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>在当今快速发展的软件行业中，不断涌现的新理念、技术和工具对开发者的工作方式产生了深远的影响。那么，敏捷开发在现代软件开发中还适用吗？测试驱动开发（TDD）是事倍功半还是物有所值？实践代码整洁之道对企业有哪些好处？</p><p></p><p>针对这些问题，近期 InfoQ 有幸专访了著名软件工程师、作家和讲师“Uncle Bob”——Robert C. Martin，与读者朋友们共同探讨。</p><p></p><p>Uncle Bob 是敏捷开发宣言 17 个奠基人之一，在软件开发社区中享有极高的声誉。并出版了多本关于软件开发的书籍，其中包括《Clean Code》、《The Clean Coder》等。目前，他的新书《Functional Design》也已在中国上市。《Clean Architecture》一书，经过国内4位知名的架构师精心翻译，将于金秋10月由机械工业出版社出版，欢迎关注。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/67/6786bf84b8e909b73dc410e6ff9ff032.jpeg" /></p><p></p><p>部分精彩观点如下：</p><p></p><p>软件最好在短周期内生产，同时拥有大量的反馈和团队互动。越想快速前进，代码就必须越整洁。并不是软件科学退化了，而是这个领域因大量尚未学习明白它的年轻学生涌入而被淹没。人工智能不是人类智能的替代品。使用 AI 来编写测试的程序员会发现 AI 只是重复了他们的错误。业界真正需要的是一个完善的学徒制度。软件行业最大的变化其实是硬件领域缺乏变革。人工智能发展的瓶颈已经隐约可见。</p><p></p><p>以下内容基于书面采访整理，经过不改变原意的编辑。</p><p></p><p></p><h2>个人经历</h2><p></p><p></p><p>InfoQ：非常高兴看到您的新书《Functional Design》在中国上市，中国的程序员们都非常关注这本书。我们了解到，过去近 30 年您参与撰写并出版的编程类书籍已经超过十本。最初您是因为什么样的契机开始写书的？是什么推动着您持续产出这么多书籍？</p><p></p><p>Uncle Bob： 说实话，我真不知道自己为什么会成为一名作家。我几乎从记事起就开始写作了。小时候，我每天都会写东西。我写过许多故事，也写过很多非虚构作品。当我成为一名程序员后，继续写作对我来说是很自然的事情。怎么说呢？我就是喜欢写作！</p><p></p><p>InfoQ：您是如何开始您的软件开发生涯的？在早期的职业生涯中，有哪些关键的经历或项目对您影响最大？您的昵称 uncle bob 有何含义吗？</p><p></p><p>Uncle Bob： 我的第一个程序是为一台小型塑料计算机编写的，那是我十二岁生日时母亲送给我的礼物。那台计算机叫做 Digi-Comp I。它有三个塑料滑块，代表三个比特位。它有弹簧加载的金属杆，起到与门的作用。简而言之，它是一个由 6 个三输入与门驱动的三位有限状态机。通过将小塑料管插入三个滑块上的插脚来编程，管子会阻挡与门的金属杆。这台机器可以被编程来从零数到七，或者从七倒数到零。你可以编程让它进行二进制加法，将两个比特相加并产生和位和进位。还有许多其他的小程序可以让这台机器执行。最终，我被深深吸引。我知道了我想要用我的余生做什么——我想要让那样的机器工作。</p><p></p><p>多年来，我参与了许多项目。我从事过金融、工厂自动化、嵌入式实时系统、计算几何、电话、电信、高能物理等领域的工作。可能最具影响力的是那些使用 8080 微处理器控制电子设备以测量电话线路质量的电话项目。</p><p></p><p>“Uncle Bob”这个名字是在 80 年代末由一位同事给我起的，很快就被大家记住了，我也认为这个名字很不错。</p><p></p><p></p><h2>Bob 大叔的编程哲学</h2><p></p><p></p><p>InfoQ：您提出了多项影响深远的软件设计理念，比如敏捷开发、代码整洁之道等，您能谈谈是哪些经历或人物帮助您塑造了您的编程哲学吗？</p><p></p><p>Uncle Bob： 在我的生活和职业生涯中，有很多人对我产生了影响：Grady Booch、Martin Fowler、Kent Beck、Jim Newkirk、Tim Ottinger、Michael Feathers、Jim Coplien……我可以继续列举下去，值得提及的人实在太多了。</p><p></p><p>至于塑造我哲学观的经历，我参与过许多不同类型的软件项目。在每一个项目中，我都注意到了软件设计与架构方面的相似问题。多年来，这些问题逐渐凝聚成了一些通用原则。</p><p></p><p>InfoQ：作为敏捷宣言的创始人和签署者之一，您认为敏捷开发对软件开发行业最大的贡献是什么？</p><p></p><p>Uncle Bob： 敏捷软件开发的最大贡献，简单来说，就是重新唤醒了一个理念：软件最好在短周期内生产，同时拥有大量的反馈和团队互动。这个理念在 50 年代就已经被知晓和实践，但在 70 年代初，一种极端的前期规划理念取代了它。这种替代是由于程序员群体的人口结构变化：随着大量拥有计算机科学学位的大学毕业生涌入，程序员的平均年龄下降了近 20 岁。</p><p></p><p>InfoQ：去年的时候，敏捷早期采用者 <a href="https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651155396&amp;idx=1&amp;sn=60d270d7f7103ba691ff9c504b43391c&amp;scene=21#wechat_redirect">Capital One 裁掉了整个敏捷团队</a>"，认为这是降低“遗留技术成本”。有统计发现如今的主流大厂也认为敏捷阻碍了交付，<a href="https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651119406&amp;idx=1&amp;sn=a9bf04b2adf23de56bceb6927778ebe9&amp;scene=21#wechat_redirect">并摒弃了 Scrum 框架</a>"。那么您对这些企业的敏捷实践有何看法？为什么敏捷会衰落？敏捷开发的正确方法是什么？</p><p></p><p>Uncle Bob： 敏捷，像所有软件理念一样，可能会成为营销炒作和稀释的牺牲品。敏捷开发是由程序员为程序员创建的，但早期它就被一群寻求 Scrum Master 认证的项目管理者侵入，然后被曲解。这群人教授和实践了他们所谓的“敏捷”，但这与原始的敏捷原则并不一致。我对 Capital One 的案例不熟悉。但对我来说，一些公司会放弃被曲解的 Scrum 和敏捷版本并不奇怪。</p><p></p><p>敏捷的正确方法是遵循敏捷宣言中的四项声明。Kent Beck 的《极限编程解释》一书中很好地描述了一套最佳的敏捷纪律。</p><p></p><p>InfoQ：您提出的 SOLID 原则对面向对象设计有着深远的影响。您能解释一下这些原则背后的基本思想吗？</p><p></p><p>Uncle Bob： 所有的 SOLID 原则都是管理依赖关系的技术。总体目标是将软件系统划分为一组组件，这些组件的相互依赖关系被组织得如此之好，以至于变化不会从一个组件传播到另一个组件。</p><p></p><p>InfoQ：有人认为在微服务等技术盛行之后，SOLID 原则并不完全适用了（https://www.infoq.com/news/2021/11/solid-modern-microservices/）。那么您认为现代软件设计是否需要有一套新的原则？</p><p></p><p>Uncle Bob： 在我职业生涯的几十年里，给我留下深刻印象的一件事是，软件设计与架构的原则是不变的。我们并不需要一套新的原则来处理“现代”软件实践——因为“现代”软件实践与不“现代”的软件实践并没有那么大的不同。</p><p></p><p>微服务就是一个很好的例子。几十年来，软件开发人员一直在将组件隔离成可以独立执行的服务。微服务的概念一点也不新。而且，它也不是许多系统的特别合适的方法。对于那些适合使用微服务的系统来说，SOLID 原则为服务的设计以及整个系统提供了一个很好的指导。</p><p></p><p>InfoQ：您从 1970 年开始从事软件行业，那时候甚至还没有 C 语言，而现在主流的 Java、Python 语言出现得更晚，Rust、Go 等语言甚至是在您的“Clean Code”出版之后。对于 C 可能非常易于使用“Clean Code”的一些原则，但是否也适用于 Java、Rust、Swift、Python 等这些后期出现的静态、动态语言？</p><p></p><p>Uncle Bob：Clean Code 的原则适用于所有编程语言。你可以在 Rust 中实践它们，在 COBOL 中，在 Swift 中，在 FORTRAN 中，在 Clojure 中，在 Assembler 中。它们适用于静态语言如 Kotlin，动态语言如 Ruby，堆栈语言如 Forth，以及逻辑语言如 Prolog。</p><p></p><p>InfoQ：如今的企业希望做到快速推出产品并快速试错，那么相对以前，现在的开发者们应该如何构建干净的代码，哪些原则是必须坚守的，哪些是可以妥协的？干净代码能给企业以及想盈利的产品带来哪些好处？</p><p></p><p>Uncle Bob：“早失败，常失败”是敏捷原则之一，它与 Clean Code 完全兼容。实践代码整洁之道并不慢——它很快，代码整洁比代码混乱时更快。你越想快速前进，你的代码就必须越整洁。</p><p></p><p>InfoQ：对于一些在 IT 行业从事了 20 年或以上的人来说，其实都在目睹“软件科学”的不断退化。二十年前，我们写软件会用到“设计模式”、会用 UML 做设计、做完美的需求分析和画流程图。如今，我们看到许多年轻开发者不再热衷于学习和应用设计模式、UML 等。您认为造成这种现象的原因是什么？如何平衡过度设计和功能需求？</p><p></p><p>Uncle Bob： 世界上程序员的数量大约每五年翻一番。自 60 年代以来一直如此，而且在未来几十年内很可能仍然如此。这意味着世界上有一半的程序员经验不足五年。这就解释了问题中的所有情况。并不是软件科学在退化，而是这个领域因大量尚未学习明白它的年轻学生涌入而被淹没。 这对设计模式、敏捷、SOLID、UML 等都是如此。</p><p></p><p>解决这个问题的方法是提升教学质量。我们这些有 10 年、15 年和 20 年经验的人必须承担起责任，教育涌入我们领域并稀释我们职业的大量年轻程序员。</p><p></p><p>InfoQ：您提到过您理想情况下的 TDD 是每行代码编写一行免费的测试代码，并达到 100% 的覆盖率。但实际有一些软件，特别是互联网企业，并不重视测试，比如说 Tiktok 作为一款全球现象级的应用，似乎并没有严格遵循 TDD 的原则，他们<a href="https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651107616&amp;idx=1&amp;sn=cada729aad574eabc09557caef99aff6&amp;scene=21#wechat_redirect">用 QA 代替了单元测试</a>"，但实际上 Tiktok 运行还比较稳定。您如何看待这样的一些实践？您认为在现实的软件开发中，尤其是互联网企业，这种状态是否可行并可持续？为什么？</p><p></p><p>Uncle Bob： 我很推荐 TDD。有些人不遵循这一建议，那是他们的选择。然而我猜想，一个不太重视测试的软件团队，必然花很多时间在调试上，并且会因为调试而显著减慢速度。我个人更喜欢通过避免调试来快速推进。</p><p></p><p>我还认为，一个不太重视测试的软件团队，也不会太重视重构。没有一套测试，重构是非常危险的，系统必然会因为开发者害怕通过重构来保持代码整洁而退化。最终，随着时间的推移，这些团队将不得不处理越来越混乱的系统，他们的速度会因此减慢。</p><p></p><p>InfoQ：作为测试驱动开发的坚定支持者，您如何回应那些认为 TDD 会降低开发速度的观点？对于那些不重视测试，甚至认为测试是浪费时间的年轻企业，您有哪些建议？如何说服他们认识到 TDD 的重要性？</p><p></p><p>Uncle Bob：TDD 提高开发速度，因为它减少了调试，并使得重构成为可能。重构允许系统的设计与代码持续被清理和改进，这使得团队中的每个人都能更快地工作。做快的唯一方式是做好。</p><p></p><p>InfoQ：如今，生成式 AI 越来越强大，围绕各类编程类 Copilot 有很多讨论。与我们之前倡导的“提升专业化水平”方向不同的是，这些工具厂商都在鼓动不懂或不精通编程的非专业人士进入软件行业，并编写自己的软件，您认为这会对 IT 行业造成哪些影响？</p><p></p><p>Uncle Bob： 灾难。人工智能不是人类智能的替代品。人工智能可以是很好的工具，但只有在知道如何使用这些工具的人手中才是如此。</p><p></p><p>InfoQ：许多开发人员已开始将 ChatGPT 和 Copilot 等工具集成到他们的日常工作流程中。那么，在生成式 AI 时代，TDD 是否比以前更为重要？我们注意到已经有一些利用大模型生成测试的案例，您如何看待“使用 AI 为代码生成测试”？您有没有利用 AI 进行测试的建议方法？</p><p></p><p>Uncle Bob：TDD 相当于会计实践中的复式记账。会计人员将每一笔交易输入两次，一次作为资产，第二次作为负债。这两笔交易遵循不同的数学路径，直到它们在资产负债表上相互抵消至零。遵循 TDD 的程序员将每一段代码编写两次，一次作为测试，第二次作为生产代码。它们遵循互补的执行路径，直到它们零和测试失败，在错误报告上相遇。</p><p></p><p>TDD 和复式记账的要点是让个人输入两次，以捕捉该个人犯的错误。我们不希望自动化系统，如人工智能，来编写我们的测试，因为那违背了让个人陈述和重申他们意图的目的。使用 AI 来编写测试的程序员会发现 AI 只是重复了他们的错误。</p><p></p><p></p><h2>“想一直写代码直到去世”</h2><p></p><p></p><p>InfoQ：您现在仍在软件设计和工程实践领域深耕，没有停止对编程的研究，这五十年来，您是如何一直保持对编程的热爱的？</p><p></p><p>Uncle Bob： 我在十二岁时就爱上了编程，并且从未回头。我想一直写代码直到我去世，而且我不想很快就去世。总有一天，他们会发现我，我的鼻子卡在笔记本电脑的键盘之间，屏幕上显示着一个失败的测试。</p><p></p><p>InfoQ：您经常提到软件工匠（Software Craftsman）和工匠精神。您能解释一下何为软件工匠精神，并分享一些如何培养这种精神的建议吗？</p><p></p><p>Uncle Bob： 软件工匠精神就是你每天下班回家后，站在镜子前，对自己说：“我今天干得不错。” 很多程序员下班后却需要洗个澡，因为他们那天制造的混乱。软件工匠精神关乎对工作的自豪，知道自己的代码会被他人看到并赞赏，你是如何精心打造代码的，以及你对细节的关注。</p><p></p><p>InfoQ：您曾表示程序员要持续学习，比如在一门新的编程语言还没流行之前就提前感知到并去学习它。您最近还在学习什么新语言吗？在您的职业生涯中，想必使用过多种编程语言。您认为选择编程语言时应考虑哪些因素？程序员如何才能提前感知到未来可能变得重要的新语言的来临？</p><p></p><p>Uncle Bob： 最近我又开始温习一些编程语言：写了一些 Python 和 Go 代码——这些语言我已经多年没碰了。也稍微研究了一下 Rust，尽管我现在还不能算是一个 Rust 程序员。几年前，我花了点时间学习 Elixir，而且我经常玩玩 Lua 。</p><p></p><p>学习新语言时，最重要的考虑因素是它是否能让你感到乐趣。当学习变得有趣时，你不仅会学得更快，而且学习本身就是目的。随着时间的积累，你会逐渐明白新语言对你的职业生涯是否有帮助。但在职业生涯的初期，这一点并不是特别重要。等到你能够在团队和组织的方向上发表意见时，实用性和实际效益的考量就会变得更加重要。</p><p></p><p>InfoQ：您认为当前的编程教育是否足够培养出优秀的软件开发者？有哪些改进的建议？</p><p></p><p>Uncle Bob： 不，我觉得大多数编程教育都远远不够。业界真正需要的是一个完善的学徒制度。新程序员应该在资深程序员的亲自指导下，通过实践操作和严格监督来学习——就像医生和律师那样。</p><p></p><p>InfoQ：在您看来，软件开发领域过去十年最大的变化是什么？未来十年最值得期待的变化又是什么？</p><p></p><p>Uncle Bob： 过去几十年，软件行业最大的变化其实是硬件领域缺乏变革。摩尔定律大约在 2005 年就走到了尽头，自那以后，计算机的速度并没有显著提升。存储技术在接下来的十年里持续进步，但现在这种进步也放缓了。我成长的时代，机器的性能以指数级的速度提升，变得更快速、更小巧、更经济、存储容量更大。每隔一两年，我们就得更新换代，因为旧设备无法胜任新软件的需求。每隔一两年，我们的视野就会因为新技术而拓宽，探索更多可能性。但今天的软件开发者面临的是一个技术发展的瓶颈期，机器的性能不会再像以前那样指数级增长了。虽然可能会有一些逐步的改进，但摩尔定律带来的迅猛发展已经结束。</p><p></p><p>人们很容易误以为人工智能是摩尔定律那样的技术革命，但实际上并非如此。人工智能需要巨大的资源投入，而这些资源是有限的，因此人工智能不可能无限制地指数级增长。一些技术改进，比如专用芯片，可能会带来一定的进步，但人工智能发展的瓶颈已经隐约可见。</p><p></p><p>在未来十年，我们应该更多地关注提升我们的专业技能，而不是单纯依赖技术的进步。 我们应该致力于成为更优秀、更专业的程序员。我们应该努力建立和完善软件行业的纪律、标准和道德规范。</p><p></p><p>InfoQ：您还有什么话想对中国的开发者们说？</p><p></p><p>Uncle Bob： 开发者都是我的同胞，我向世界各地的开发者致以崇高的敬意。对每一位开发者，我都要说一句：保持代码的整洁，认真做好每一份工作，无论是在编程还是在生活中都要细心谨慎。当你提交代码时，确保它比你检出时更加整洁。</p><p></p><p>今日好文推荐</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651215576&amp;idx=1&amp;sn=9e32dbb2529df6e21aa9c0b4cc0a68d0&amp;chksm=bdbbae8b8acc279d1f214d6fc675e341ad83a4da021659b53138242a7c187a3c28e51d0aa2e6&amp;scene=21#wechat_redirect">下载量超 5000 万的知名应用，开发团队“全军覆没”，从此发版人唯剩老板一个</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651220167&amp;idx=1&amp;sn=812b1ddaba6c1f846c8095d0427d0525&amp;chksm=bdbb90948acc1982cd836b8ba08bd48b8f6f385b496a8c406a6e130e2b5e57d88d494f8b9220&amp;scene=21#wechat_redirect">一次 App 更新差点要了这家 22 年老牌公司的命：忽视技术债、疯狂裁员降本，CEO 为开发加速无视员工警告</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651220019&amp;idx=1&amp;sn=eede282c476b1d534607b7fa41b26a08&amp;chksm=bdbb90608acc1976aa6d13cce486c95642a8922cf518dd9a2a892029ce394b34a6aa39cba4f3&amp;scene=21#wechat_redirect">开源 9 年后，词频数据库 wordfreq 宣布停止更新，创始人：网上全是垃圾，OpenAI 和谷歌要为此付出代价</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651219703&amp;idx=1&amp;sn=6be957dc64bd47d16c7ff5046578b602&amp;chksm=bdbb9ea48acc17b209e2ee998dd25fc55fae213f1b577dcd5b3d4de6592129101e4337f5132f&amp;scene=21#wechat_redirect">突发！高通被曝有意收购英特尔；思科N+7裁员，员工称人性化；百度最高奖发出2800万！李彦宏：再苦不能苦技术人 | Q资讯</a>"</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/zJuC2GK3R95NjyUzkpqD</id>
            <title>MLPerf 存储基准测试发布：焱融存储斩获多项世界第一</title>
            <link>https://www.infoq.cn/article/zJuC2GK3R95NjyUzkpqD</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/zJuC2GK3R95NjyUzkpqD</guid>
            <pubDate></pubDate>
            <updated>Fri, 27 Sep 2024 10:23:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><h3>摘要：</h3><p></p><p>9月25日，全球权威 AI 基准测评组织 MLCommons® 公布了 MLPerf® v1.0 存储性能基准测试的结果。焱融科技在此次测试中表现出色，焱融全闪存储产品在 3D-Unet、ResNet50 和 CosmoFlow 三种 AI 深度学习模型的评估中均展现了卓越的性能和效率。</p><p></p><p>焱融科技作为中国自主研发的高性能存储领导者，与 DDN、Nutanix、Weka、Hammerspace、Solidigm 和 Micron 等众多国际优秀厂商同场竞技，测试结果显示，在带宽、模拟 GPU 数量以及 GPU 利用率等关键性能指标上，焱融科技的产品获得了多项世界第一。</p><p><img src="https://static001.geekbang.org/infoq/ed/eda6b205c472bc8a92d592d8143771c3.webp" /></p><p>在 MLPerf® Storage v1.0 的基准测试中，焱融全闪存储显著提升了 GPU/ML 工作负载的处理速度，这表明焱融高性能存储产品具备支持各种 AI 模型训练和高性能计算场景的能力。在 AI 领域，尤其是在大规模模型训练方面，焱融全闪存存储解决方案发挥着至关重要的作用，为 AI 技术的发展和应用提供了强有力的支持。</p><p></p><h3>MLPerf® Storage  全球首个且唯一的 AI/ML 存储基准测试</h3><p></p><p>MLPerf 是由图灵奖得主大卫·帕特森（David Patterson）联合谷歌、斯坦福大学、哈佛大学等顶尖学术机构共同发起的国际权威 AI 性能基准测试，被誉为全球 AI 领域的“奥运会”。MLCommons 组织在 2023 年首次推出了 MLPerf 存储基准测试（MLPerf Storage Benchmark），这是首个也是目前唯一一个开源、公开透明的 AI/ML 基准测试，旨在评估存储系统在 ML/AI 工作负载中的表现。这一基准测试为 ML/AI 模型开发者选择存储解决方案提供了权威的参考依据，帮助他们评估合适的存储产品。</p><p></p><p>MLPerf Storage 基准测试目前有两个版本：v0.5 和 v1.0。2023 年发布的 v0.5 版本初步包含了 Unet-3D 和 BERT 两个模型，并仅支持模拟 NVIDIA v100 GPU。而今年最新发布的 v1.0 版本进行了重大更新，引入了更具代表性的测试模型，这些模型在业界具有广泛的应用，能够更好地代表实际工作负载。</p><p><img src="https://static001.geekbang.org/infoq/9f/9f672cdb139bb38d1827eb8ae9919072.webp" /></p><p>为确保测试结果的可靠性，MLPerf Storage v1.0 基准测试的规则非常严格，关键要求如下：</p><p>1. 高 GPU 利用率</p><p>在 U-Net 3D 和 ResNet-50 模型测试中，GPU 利用率需维持在 90% 以上。CosmoFlow 模型的 GPU 利用率需达到 70% 以上。</p><p>2. 禁止缓存</p><p>在基准测试开始前，不能在主机节点上缓存训练数据。连续测试运行之间，必须清除主机节点中的缓存以确保测试的准确性。整体数据集的大小务必远超过主机节点的内存大小。</p><p></p><h3>国内唯一全面参与所有模型测试的厂商，荣登多项世界第一</h3><p></p><p>本次焱融科技参与 MLPerf 测试使用了<a href="http://mp.weixin.qq.com/s?__biz=MzIzMzY1NTM4Mw==&amp;mid=2247491564&amp;idx=1&amp;sn=c63d949a309d21a0d3ca420e395df177&amp;chksm=e88302d4dff48bc215c953a69a21e1f9b43ac84477211ea3d12b2c61c1ab56ddaa0c07aff7d8&amp;scene=21#wechat_redirect">最新发布的 F9000X 全闪分布式一体机产品</a>"。F9000X 每个存储节点搭载最新的英特尔® 至强® 第 5 代可扩展处理器，存储介质采用 10 块 Memblaze PCIE 5.0 NVMe 闪存 ，同时配备 2 块 NVIDIA ConnectX-7 400Gb NDR 网卡。测试环境网络拓扑如图所示：</p><p><img src="https://static001.geekbang.org/infoq/9f/9f51570df9cbbba2331758f672edc69f.webp" /></p><p>部署环境架构图</p><p></p><p>为了深入理解 MLPerf Storage 基准测试内容，我们先解释两个核心概念：</p><p>ACC：即 Accelerators（加速器），MLPerf Storage Benchmark 测试工具通过 accelerator emulation，来模拟真实的 GPU，如：NVIDIA A100、H100 等。在无需真实 GPU 的情况下就能进行大规模的存储性能压测，用以评估存储系统在 AI 模型训练场景的适用性。</p><p></p><blockquote>说明：在 MLPerf Storage Benchmark v1.0 版本中 ACC 可以模拟 NVIDIA A100 和 H100 两款 GPU 型号。本次MLperf 测试焱融提交了基于 H100的测试数据。</blockquote><p></p><p></p><p>AU：Accelerator Utilization（AU，加速器利用率）在 MLPerf Storage Benchmark 中，通常被定义为 GPU 计算时间占整个基准测试运行时间的百分比。这是一个衡量加速器在给定任务中的有效利用程度的关键指标，GPU 计算时间占比越高，代表存储速度越快，GPU 越能被充分利用。AU 太低，说明存储性能不足以支撑 GPU 高效运行，只有 AU 高于指定值时，提交的数据才是有效数据。</p><p></p><p></p><h3>焱融在 MLPerf&nbsp; Storage &nbsp;v1.0 的测试表现</h3><p></p><p></p><p></p><h4>最全面最完整，国内唯一一家参加了全部模型测试的存储厂商</h4><p></p><p>焱融科技是国内唯一一家参与了 MLPerf Storage 全部模型测试的存储厂商，这些测试包括&nbsp;3D-Unet、CosmoFlow 和 ResNet 50。在本次测试环节，焱融追光全闪存储一体机 F9000X 展现了卓越的性能，全面覆盖目前主流模型应用数据负载需求。F9000X 不仅能够处理大规模的数据集，还可以根据 AI 集群规模弹性扩展，完美匹配 GPU 算力性能。</p><p></p><p></p><h4>在分布式训练集群场景，平均每个计算节点 ACC 数量最多，存储带宽最高</h4><p></p><p>MLPerf Storage 基准测试规则定义可以采用单个计算节点（客户端）运行多个 ACC（GPU 加速器），进行相应模型应用测试，同时支持大规模分布式训练集群场景，多个客户端模拟真实数据并行的方式并发访问存储集群。其中平均每个客户端能够运行的 ACC 数量越多，则代表该节点的计算能力越强，能够处理任务的数量也就越多，而对于存储数据并发访问性能要求也就越高。测试结果显示，在分布式训练集群场景，焱融存储在所有三个模型的测试中，能够支撑的每个计算节点平均 ACC 数量和存储带宽性能均排名第一。</p><p><img src="https://static001.geekbang.org/infoq/9f/9f8bff0a045e9141ea6fc76d5e921ef6.webp" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/2e/2e49df7c096e8f55ccff98579c21e4fa.webp" /></p><p></p><h4>存储性能随计算规模同步线性增长</h4><p></p><p>随着计算规模的扩大，存储性能应实现线性增长以满足 AI 训练的需求。以 3D-Unet 三维图像分割模型为例，其单个图像样本大小约为 146MB，而在多节点集群环境中，每秒处理的训练样本数可超过 1100 个，这导致训练数据的读取带宽需求超过 160GB/s。</p><p>在针对三个模型的测试中，焱融全闪存储一体机 F9000X 展现了出色的性能。测试结果显示：随着并发计算节点（ACC）数量的增加，存储系统的带宽性能保持明显的线性增长能力。此外，存储的可用性（AU）也始终保持在测试基准要求的范围内，确保了训练过程的高效和稳定。</p><p><img src="https://static001.geekbang.org/infoq/83/8386cbc56f2982507c8e77434056deb3.webp" /></p><p>目前在 3D-Unet 模型应用的测试中，使用 3 个计算节点，共 60 个 ACC，可达到 160GB/s 的存储带宽性能。F9000X 3 节点存储集群实测最大可以达到 260GB/s 以上的带宽性能，这表明在实际业务环境中焱融全闪存可以支撑更多的 GPU 的计算节点。</p><p>以下是焱融全闪存储在 ResNet50、CosmoFlow 这两款模型测试的存储的可用性（AU）及带宽性能表现：</p><p><img src="https://static001.geekbang.org/infoq/b5/b590bfcd89151a36da60c37faf78d52d.webp" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/41/41f8ae95007d3de718f1a937421e34e2.webp" /></p><p></p><h3>小结：</h3><p></p><p>在进行 MLPerf Storage 基准测试时，我们发现为了满足 AI 计算存储的性能需求，存储系统需要具备以下关键特性：</p><p>高性能设备支持：MLPerf Storage 需要高带宽，因此存储系统必须支持如 200Gb 和 400Gb InfiniBand 或以太网等高性能网络设备。MultiChannel 网络带宽聚合：YRCloudFile 支持在 InfiniBand 或 RoCE 网络上使用 MultiChannel 功能，以充分利用双卡的性能，实现数据读写的高效性。🔗 更多了解，详情请见《<a href="https://mp.weixin.qq.com/s?__biz=MzIzMzY1NTM4Mw==&amp;mid=2247489291&amp;idx=1&amp;sn=f824abadc01c8aeeca7ff21ffd342cbd&amp;scene=21#wechat_redirect">90GBps 性能顶流！焱融科技发布最新 AI 大模型存储方案</a>"》全链路 direct&nbsp;I/O：为了避免内存缓存导致的性能瓶颈，YRCloudFile 支持在计算节点上部署 kernel client，允许数据读写直接绕过内存缓存，通过 direct I/O 方式访问后端存储。🔗 更多了解，详情请见《<a href="https://mp.weixin.qq.com/s?__biz=MzIzMzY1NTM4Mw==&amp;mid=2247491417&amp;idx=1&amp;sn=8b7091dde8aa6d7d678a289e4afd5e3e&amp;scene=21#wechat_redirect">CPU 使用率飙升，Buffer IO 引发的性能问题</a>"》）NUMA&nbsp;优化：内存性能对存储性能至关重要。YRCloudFile支持全链路NUMA优化，确保服务进程与NVMe SSD绑定到同一NUMA节点，优化数据传输路径。🔗 更多了解，详情请见《<a href="https://mp.weixin.qq.com/s?__biz=MzIzMzY1NTM4Mw==&amp;mid=2247488301&amp;idx=1&amp;sn=1a34d5c6068763799093da491798711a&amp;scene=21#wechat_redirect">YRCloudFile V6.8.0 发布：向全闪时代迈进</a>"》</p><p>焱融分布式文件存储 YRCloudFile 通过上述技术亮点，能够在本次 MLPerf Storage 测试中接近硬件性能极限，为 AI 计算提供所需的高性能存储解决方案。在实际测试中，YRCloudFile 已经展现出能够支持大规模 AI 训练任务的能力，即使在极端条件下也能保持系统的稳定性和性能。</p><p></p><p>引用链接：</p><p>[1] MLPerf Storage Benchmark Suite Results:&nbsp;https://mlcommons.org/benchmarks/storage/</p><p>[2] MLPerf Storage rules:&nbsp;</p><p>https://github.com/mlcommons/storage/blob/main/Submission_guidelines.md</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/MKfOlVLfzfVN4Xk5Rws3</id>
            <title>Sam Altman罕见发表预言长文：我们距离AI超级智能可能只有“几千天”</title>
            <link>https://www.infoq.cn/article/MKfOlVLfzfVN4Xk5Rws3</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/MKfOlVLfzfVN4Xk5Rws3</guid>
            <pubDate></pubDate>
            <updated>Fri, 27 Sep 2024 10:21:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>本周，OpenAI公司CEO山姆·奥特曼在一篇题为《智能时代》的最新个人博文中，概述了自己对于AI驱动的技术进步与全球繁荣未来的愿景。这篇文章描绘了AI加速人类进步的可能性，奥特曼还指出超级AI有可能会在未来十年之内出现。</p><p>&nbsp;</p><p>他在文中写道，“我们距离超级超级智能可能只有几千天；也许实际时间会更长，但我坚信我们将会达成这个目标。”</p><p>&nbsp;</p><p>目前尚不清楚 Altman 的这篇帖子是新产品发布或重大公告的先兆，还是仅仅是对“智能时代”的预测。尽管如此，Altman 表示乐观强调这个新时代的一个决定性特征是大规模的繁荣。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/f6/f64a98907fc30492269ef715d2349085.jpeg" /></p><p></p><p>&nbsp;</p><p></p><h2>准备好迎接“智能时代”？！</h2><p></p><p>&nbsp;</p><p>OpenAI的当前目标在于建立通用人工智能（AGI）。这是一个设想出来的技术概念，指在执行多种任务时能够与人类智能相媲美、且无需接受特定训练的AI。相比之下，超级智能要比通用人工智能更进一步，可以理解成一种假想的机器通知水平，可以在任意智力任务上远远超越人类、甚至达到不可思议的程度。</p><p>&nbsp;</p><p>超级智能（也被称为「ASI」，即「人工超级智能」）是机器学习社区中一个流行、但也有些极端的议题，多年来一直受到关注和质疑。矛盾的集中爆发，则源自哲学家Nick Bostrom于2014年撰写的争议论著《超级智能：路径、危险与策略》。OpenAI公司联合创始人兼前首席科学家Ilya Sutskever于今年6月离开OpenAI之后，也创办了一家以该术语命名的公司：Safe Superintelligence——安全超级智能。与此同时，奥特曼本人则至少从去年开始就一直在谈论开发超级智能。</p><p>&nbsp;</p><p>那么，“几千天”到底是有多长？没人能给出确切的数字。奥特曼之所以选择这样一个模糊的数字，可能也是因为他也不确定AI超级智能究竟何时会来，只是隐隐感觉到可能会在未来十年之内实现。具体算来，2000天大约是5.5年，3000天大约是8.2年，而4000天则接近11年。</p><p>&nbsp;</p><p>我们当然可以批评奥特曼的语焉不详，毕竟没有人能够真正预测未来。但身为OpenAI公司的掌门人，奥特曼很可能掌握着一些尚未被公众所广泛知晓的AI研究技术。因此哪怕是“几千天”这样一种宽泛的时间表述，考虑到说法来自AI领域值得关注的消息来源，我们似乎也有必要认真加以对待——尽管这位消息人士属于利益相关者，投入了大量资金来确保AI发展不致陷入停滞。</p><p>&nbsp;</p><p>而且并非所有人都认同奥特曼表现出的乐观和热情。计算机科学家、时常批评AI技术的Grady Booch引用了奥特曼“几千天”的预测，在X上发帖评论称：“我对一切AI炒作感到极度厌倦：它们没有任何现实依据，只会抬高期待、激怒公众、登上媒体头条，并分散人们对于计算领域实际工作的注意力。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/f4/f4acb4058515c9a3e9dbd132b42d1650.jpeg" /></p><p></p><p></p><blockquote>我同意萨姆的看法！假设他使用的是16进制计数法……</blockquote><p></p><p>&nbsp;</p><p>尽管受到批评，但值得注意的是，这家可能是当下最具代表性的AI厂商的CEO确实在对未来技术能力做出宽泛的预测——哪怕目的可能是为了继续努力筹集资金。现如今，建设基础设施来支持AI服务，已经成为许多科技企业CEO们最关心的问题。</p><p>&nbsp;</p><p>奥特曼在文章中写道，“如果我们想让尽可能多的人们掌握AI，就必须降低计算成本并丰富其实际功能（这需要大量能源和芯片）。如果我们不建设充足的基础设施，AI就将成为一种极其有限的资源。争夺之战将因此打响，AI也会主要成为富人们的工具。”</p><p>&nbsp;</p><p></p><h2>奥特曼的“智能时代”愿景</h2><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/34/34acfd6176eedc0c4121d59474c8ad54.png" /></p><p></p><p>OpenAI公司CEO山姆·奥特曼于2024年1月11日走过华盛顿特区美国国会大厦众议院廊下。</p><p>&nbsp;</p><p>在文章的其余部分，奥特曼将我们现在的时代描述为“智能时代”的开端，即人类历史上继石器时代、农业时代和工业时代之后的又一个变革性技术时代。他将深度学习算法的成功描述成这个新时代的催化剂，并简单总结称：“我们是如何迈向下一次繁荣飞跃的门槛的？概括讲，就是深度学习的确有效。”</p><p>&nbsp;</p><p>奥特曼的观点似乎基于该公司最近推出的 "o1" AI 模型，据称该模型能够推理解决以前模型难以处理的问题。</p><p>&nbsp;</p><p>许多批评该公司的声音认为，深度学习——驱动像 ChatGPT 和 Google 的 Gemini 这样的模型的特定类型 AI——无法通过扩展来创造出具有人类水平的人工智能。</p><p>&nbsp;</p><p>然而，在他最新的博客文章中，奥特曼直接反驳了这一观点：</p><p></p><blockquote>“用15个字概括：深度学习有效，随着规模的扩大可预测地变得更好，我们为其投入了越来越多的资源。”</blockquote><p></p><p>&nbsp;</p><p>这位OpenAI掌门人设想AI助手将变得越来越强大，最终形成“个人AI团队”，能够帮助个人完成他们所能想象的几乎一切事务。他预测AI技术将在教育、医疗保健、软件开发及其他诸多领域取得突破。</p><p>&nbsp;</p><p>虽然承认AI带来的潜在负面影响与劳动力市场混乱，但奥特曼在AI对于人类社会的整体影响方面仍持乐观态度。他在文中写道，“繁荣本身并不一定能让人们幸福——有很多富人其实生活得相当悲惨——但其仍将显著改善世界各地人民的生活。”</p><p>&nbsp;</p><p>即使像SB-1047这样的AI监管条例已经成为当下的热门话题，奥特曼也没有在文中特别提及AI技术带来的科幻性质的风险。在X上，彭博专栏作家Matthew Yglesias写道，“值得注意的是，奥特曼甚至不再口头讨论生存风险的问题，他唯一担忧的AI弊端就是对劳动力市场造成的冲击。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/7f/7fb2e4272591b5f218de663ba7d9c470.jpeg" /></p><p></p><p>&nbsp;</p><p>虽然对于AI科技的潜力充满热情，但奥特曼同样呼吁应当谨慎行事，只是语气比较含糊。他写道，“我们需要明智且坚定地采取行动。智能时代的到来标志着重大的发展里程碑，必然伴随着极其复杂的高风险挑战。整个过程绝不会完全积极且正向，但其中的收益又是如此巨大，我们有责任为自己和未来考虑该如何应对可能面临的风险。”</p><p>&nbsp;</p><p>可除了对劳动力市场造成的冲击之外，奥特曼并没有提及智能时代可能造成的其他负面影响，只是用因技术变革而消失的过时职业作为类比给文章作结。</p><p>&nbsp;</p><p>他写道，“我们当下所做的许多工作，在几百年前的人们看来可能既微不足道又浪费时间。但往者不可谏，来者犹可追，最重要的不是沉溺于过去、而在于怎样开启一个新的时代。如果当时的人们有机会看到今天的世界，一定认为周遭的繁荣景象远远超过了其想象。而如果我们从今天开始向后快进一百年，那种勃勃生机、万物竞发的情景也将同样令人难以想象。”</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://ia.samaltman.com/">https://ia.samaltman.com/</a>"</p><p><a href="https://arstechnica.com/information-technology/2024/09/ai-superintelligence-looms-in-sam-altmans-new-essay-on-the-intelligence-age/">https://arstechnica.com/information-technology/2024/09/ai-superintelligence-looms-in-sam-altmans-new-essay-on-the-intelligence-age/</a>"</p><p><a href="https://news.ycombinator.com/item?id=41628167">https://news.ycombinator.com/item?id=41628167</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/kMThQLtXZIYcvLJF0x7J</id>
            <title>广东首个国产TPU智算中心怎么建起来的？</title>
            <link>https://www.infoq.cn/article/kMThQLtXZIYcvLJF0x7J</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/kMThQLtXZIYcvLJF0x7J</guid>
            <pubDate></pubDate>
            <updated>Fri, 27 Sep 2024 10:10:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>9 月 9 日，广东地区首个采用国产 TPU 技术的智算中心成立。该项目一期由 32 个算力节点通过高效互联构建而成，后期将扩容至千卡规模，形成训推一体化的枢纽，成为中国联通在深圳的核心智算高地的重要组成部分。</p><p></p><p>深圳作为全国科技创新的前沿阵地，一直走在人工智能产业发展的前列。据深圳联通副总经理赵桂标介绍，此次中昊芯英与深圳联通联合进行的高性能 AI 智算中心项目合作，不仅是对国家智算能力布局要求的积极响应，也为深圳乃至全国的人工智能产业发展注入强劲动力。智算中心的主要客户一是政企客户、二是工业制造方面的客户、三是金融客户、四是医疗客户。</p><p></p><p>那么，这样一个重要的智算中心是怎么建成的？其中有两个重要组成部分：TPU 芯片和算力调度。</p><p></p><p>TPU 架构，专为 AI 深度学习设计。相较于 CPU 的 if else 类的逻辑开销，GPU 用于光线追踪的计算开销，TPU 则专注于针对深度学习的主要计算方式（如非线形计算）进行硬件优化设计，这些特定的计算方式和硬件算子是 CPU 和 GPU 所不具备的，这就使得 TPU 在用于 AI 深度学习时更有算力性能优势。而这一性能优势在集群层面更甚，多 TPU 芯片系统的构建方式，也是针对深度学习在模型训练和推理过程中所需要的数据流特征，而构建的专用的网络形态和网络基础架构。这样的网络形态没有向前兼容的负担，所以它比英伟达的 NVlink 更适合跑大模型的应用。无论是单芯片还是系统级，TPU 芯片都有特定的技术路线优势来实现 AI 场景中进行算法运行时的算力性价比的巨大提升。</p><p></p><p>本次智算中心的 AI 计算底座选择了搭载中昊芯英自主研发的高性能 TPU 架构 AI 芯片“刹那®”的人工智能服务器及大规模 AI 计算集群系统“泰则®”。中昊芯英创始人兼 CEO 杨龚轶凡表示，“我们想把 TPU 架构做成 AI 界的 X86。”</p><p></p><p>同样是由前谷歌的 TPU 团队核心成员创办的 Groq，最近推出了新的 AI 加速芯片 LPU。杨龚轶凡解释称，从 Groq 的论文中可以看出 LPU 就是类 TPU 架构，本质上和 TPU 没有太大区别。杨龚轶凡曾在 Google 负责 TPU 芯片研发工作，也曾在 Oracle 参与、主导过 12 款高性能服务器级别 CPU 芯片的设计与研发，中昊芯英核心研发团队成员都是一批来自于谷歌、微软、三星、甲骨文的 AI 软硬件设计专家，具备从 28nm 到 7nm 各代先进制程工艺下大芯片设计与优化完整方法论。</p><p></p><p>杨龚轶凡说：“芯片的设计研发的确是集结了很多人心血的系统化工程，也是集结了人类社会最顶尖生产工艺的过程。在这个过程中，耗费的人力和脑力是很多的，经历的时间周期也很长。这也是为什么中昊芯英第一代芯片产品“刹那®”经历了 4 年半的设计和生产周期才能完成，它其实没有一个真实世界的对应参照物做验证，大部分的东西都是在想象和想象的过程中完成。但是当有了第一代芯片，之后的迭代就会顺畅些。”</p><p></p><p>关于智能算力落地应用，杨龚轶凡表示，芯片与系统集成的挑战尤为显著。随着芯片数量的激增，通信效率成为一大难题。协调难度骤增、背景噪音干扰严重、信息传递效率急剧下降……如何设计高效的信息交换协议与物理链路，从而实现千到万乃至十万级别核心间的顺畅交流，成为亟待解决的技术瓶颈。</p><p></p><p>而 TPU 以独特的片间互联能力展现出强大的可拓展性优势。它能够轻松实现千片以上芯片片间互连，形成数据网络，并支持节点间的灵活交互与通信。这一特性使得 TPU 在构建大规模集群时更为简便，谷歌第 6 代 TPU 已能内部连接 16000 个芯片，无需依赖外部以太网，为万卡至百万卡级别的集群部署奠定了坚实基础。</p><p></p><p>此外，智算中心另外一个特点就是，智算中的单机密度和功耗密度越来越高，原来机架的功耗是 4 千瓦、6 千瓦、8 千瓦、20 千瓦，接下来可能 40 千瓦，功耗会越来越高。</p><p></p><p>对此赵桂标表示，对于能耗的控制主要依赖于两个方面：首先，在规划和建设层面，要大胆拥抱新技术，采用高效能的设备，比如液冷、磁悬浮冷机和模块化的电源等。通过采用高效能的设备利用自然冷源来降低能耗；其次，在运营管理层面上，不断积累精细化管控能耗的经验，持续优化、提升降低 PUE 来达到降低能耗目的。智算中心最后就是电力的竞争，不断降低能耗是整个行业要面对和不断攻克的问题。</p><p></p><p>中国联通以国家智算能力布局为导向，为推动全国范围内的人工智能产业发展，将深圳作为这一布局中的核心智算高地，同时，该项目将搭载联通云自研“星罗”算力管理平台，实现多元异构算力的适配和服务编排，形成“通算 + 智算 + 超算”的融合调度能力，可面向客户提供一体化的算力运营服务，也可用于企业私有化部署的智能算力网络搭建及运营管理。根据介绍，智算中心的优势在于算力的共享，避免单个企业因业务需求波动导致的算力闲置或不足问题，提高算力整体利用率，降低运营成本。</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Yd1RldcC4gZgPKGloH7C</id>
            <title>大模型让汽车“开窍”了：吉利汽车强势入场，3 年AI布局从车圈里“杀出来”</title>
            <link>https://www.infoq.cn/article/Yd1RldcC4gZgPKGloH7C</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Yd1RldcC4gZgPKGloH7C</guid>
            <pubDate></pubDate>
            <updated>Fri, 27 Sep 2024 07:21:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p></p><blockquote>出品｜InfoQ 《大模型领航者》访谈主持｜霍太稳，极客邦科技创始人兼 CEO访谈嘉宾｜陈勇，吉利汽车研究院人工智能中心主任作者｜褚杏娟</blockquote><p></p><p></p><p>“开车下班回家，路上可能没有什么事，那我可以跟大模型聊聊天，聊聊技术发展怎样、今天过得如何，可以做很多互动交流。我觉得这就是大模型上车后能够带来的一个方向转变。”吉利汽车研究院人工智能中心主任陈勇说道。</p><p></p><p>实际上，聊天只是开始。大模型一定程度上重新定义了汽车：汽车从原来的出行属性变成了社交属性，这背后带来的想象非常大。因此，大模型上车已经是车企的必答题，对于老牌车企吉利来说，亦是如此。</p><p></p><p>吉利汽车在 2021 年便开始策划自研大模型，当时的焦点还是基座模型，行业模型概念并不像现在一样成为共识。在各种不确定中，吉利汽车还是决定自己做大模型。</p><p></p><p>今年 1 月份，吉利便推出了全栈自研的全球首个汽车行业全场景 AI 大模型星睿大模型，该模型目前已经陆续应用在银河 L6、银河 L7 等车型中。从大模型研发到真正上车，吉利汽车用近 3 年的时间初步完成了这一探索。本期 InfoQ《大模型领航者》，吉利汽车向我们揭开了大模型上车神秘面纱的一角。</p><p></p><h3>做大模型的“基本功”</h3><p></p><p></p><p></p><blockquote>“之前几年对我们来讲，是一种历练和成长。”</blockquote><p></p><p></p><p>每个做大模型的开发者都要面临的难题是数据、算力、算法，吉利汽车也不例外。吉利汽车人工智能研发团队在构建自己的数据集上花费了很多精力。数据决定了大模型认知的天花板，而全面的数据才能训练出更加通用的模型，因此，这个数据集必须是高质量且全面的。“对于那个时候的我们来讲，构建数据集是非常难的一件事情，”陈勇坦诚道。</p><p></p><p>那么，大模型训练所需要的大量数据从哪里来？</p><p></p><p>以智能驾驶为例，智能驾驶的大部分数据是从实际道路上采集而来，但这种采集方式周期长、成本高、难度大。比如需要采集在上海下雪天的高架上前方有一辆车插进来的数据，这样的场景非常少难度也很高。</p><p></p><p>因此，吉利汽车开始思考利用大模型生成数据，前期用大量合成数据或虚拟生成数据训练模型，然后再用真实的道路数据做精调，来提升模型的准确率。这种方式在数据非常缺乏的产品开发冷启动阶段，也是适用的。</p><p></p><p>算力是模型研发的绝对制约因素。为解决这个问题，吉利大手笔构建了星睿智算中心。吉利的动作很快，从 21 年提出到 22 年建成，再到 23 年的正式揭牌，这个总投资 10 亿元、占地 52.12 亩、规划机柜 5000 架的智算中心，成为国内车企自建设备规模最大的智算中心之一。</p><p></p><p>根据官方披露的数据，星睿智算中心目前的云端总算力达 102 亿亿次每秒、通信网络传输速度达 800GB/s，存储带宽 4.5TB/s。结合算力调度管理算法和研发体系，吉利的整体研发效能得到了 20% 的提升，更是让星睿 AI 大模型训练速度直线提升 200 倍。</p><p></p><p>算法方面，吉利汽车组建了自己的算法团队，团队初期也踩了很多坑，比如在算法架构、加速框架的选择上就做了很多尝试，因为当时业内并没有确定的适用框架，团队需要不断试错找到最合适自己的。这个过程中，吉利汽车团队也与一些高校和机构开放合作，更快速地进行验证。</p><p></p><p>但在数据、算力和算法这大模型三要素之外，陈勇表示，更重要的是要把汽车行业的经验数据融入到大模型里，让大模型应用能够更符合业务发展需求。</p><p></p><h3>汽车模型还可以更“垂”</h3><p></p><p></p><p></p><blockquote>“可能我们会因为一个声音喜欢上一个车。”</blockquote><p></p><p></p><p>“虽然现在大模型企业很多，但 base 模型可能不需要那么多，垂类模型的需求会更多一点。”陈勇表示。</p><p></p><p>吉利本身做的是汽车行业的大模型，但在这个垂类下面，吉利做了更细化的分类。吉利星睿 AI 大模型可以看作是一个综合的模型平台，其中包含了语言大模型、多模态大模型、数字孪生大模型 3 大基础模型，并由此衍生出 NLP 语言大模型、NPDS 研发大模型、多模态感知大模型、多模态生成大模型、AI DRIVE 大模型、数字生命大模型 6 大能力模型。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a9/a97824fc2d6bb324c26aa99e55c281e9.png" /></p><p></p><p>“做垂类的模型，还是要结合场景去做。”陈勇表示。</p><p></p><p>以智能座舱为例，之前的语音回应，都只是很简单的“好的”等，但如今用大模型做语音交互后，人们可以跟系统聊天，这时对语音技术要求变高了：人们会希望它声色像真人、说话有感情等，也就是说，车里的情感交互变得十分重要。</p><p></p><p>吉利汽车团队今年 4 月发布的 HAM-TTS 语音合成大模型，就可以合成自然流畅、富有情感的语音，并根据声音样本复刻出逼真的声音。语音交互模型的迭代优化需求来自用户，毋庸置疑也是未来吉利重点推进的方向。</p><p></p><p>陈勇还提到了另一种垂类模型，就是可以识别特定场景下的用户意图，比如车上有小孩子睡着时，空调应该设置成什么样等等。汽车摄像头可以识别到车内人的状态等“多模态”输入，都是大模型理解的信息来源。</p><p></p><p>在汽车中构建各种垂类模型并落地应用也是当前汽车行业重点思考的方向。</p><p></p><p>而在模型的部署上，云端模型协同已经成为业内共识。云端模型协同使用一方面可以让系统运行得更好，另一方面也更具有经济价值。“现在的大模型不经济，训练出一个模型要花很多钱。”陈勇建议要理性消费大模型。“不是说大模型来了后就替代了原来所有的东西，一个产品的定位技术要跟它的产品定位匹配。”</p><p></p><p>具体实践中，根据吉利汽车的经验，高频、高价值、低时延的需求可以部署在端侧，一些丰富生态的需求则可以在云端部署。比如，像智能驾驶动力系统这种对时间响应要求比较高的需求就适合端侧，而娱乐性的需求更适合云端。另外，端侧更适合做实时推理，云端更适合离线训练，云端训练完可以下传到本地，让本地端做车云协同。</p><p></p><h3>自己先成为“受益人”</h3><p></p><p></p><p></p><blockquote>“用大模型讲笑话那就只能是讲笑话的钱。”</blockquote><p></p><p></p><p>虽然吉利汽车是大模型的提供方，但首批获益的人可能并不是其用户，而是吉利汽车自己。</p><p></p><p>在陈勇看来，好的产品是企业的竞争力，好的组织也是企业的竞争力。“产品都比较卷，推出一个新产品后市场上会快速出现一批类似产品，可模仿性比较强。但是一个组织的竞争力是很难被模仿和替代的。”陈勇说道。</p><p></p><p>“如果利用 AI 加强我们的组织竞争力，用 AI 文化、AI 流程、 AI 体系、 AI 开发等系列逻辑体系大大提升我们的效率，那这个组织的竞争力构建起来后，可复制性的难度是很大的。”陈勇进一步补充道。他认为，这种竞争力相当于每个人底层的逻辑和文化，难以被拿走和替代。</p><p></p><p>基于此，吉利汽车构建了星睿智能体平台来提升组织效能，这个平台上有几百个面向不同场景的智能体，包括办公领域、产品定义、营销领域、软件开发等。吉利汽车的员工可以使用智能体应用来更好地完成工作任务。</p><p></p><p>比如，原来的一些开发工作，工程师要自己写代码、做代码注释，还要去做测试等。现在可能大模型可以帮着生成一些代码，然后工程师检查是不是满足要求，不满足稍微再改一改，这样的话就提升很多效率了。</p><p></p><p>“我们的员工可以解放更多的精力，专注把产品设计、产品创新、产品体验做得更好。”陈勇说道。</p><p></p><p>陈勇对大模型的期待，不止于此。</p><p></p><p>“大模型刚出来的时候，大家思考人工智能会不会替代我们的工作，但我觉得人工智能其实是让我们变得更美好、更幸福。”</p><p></p><p>在陈勇看来，未来人机协同、人机共存会变成一种常态，机器可以辅助我们去做很多事情。其中，具身智能是陈勇个人比较看好的方向。</p><p></p><p>“之前，大家更强调机器人的运动能力，但现在大模型相当于给机器人加上了一个大脑，有五官的感知能力，然后会运动、会思考决策，这种状态就是具身智能。”陈勇说道。</p><p></p><p>在陈勇看来，具身智能未来会慢慢走进人们的生活，尤其是家庭陪伴。“现在全球的老龄化程度越来越严重，有了大模型加持的具身智能作为陪伴，体验会更好一些。”另外，在工业场景中，尤其是在重体力、重复性和充满危险的工作环境中，具身智能完全可以替代人类劳作。</p><p></p><p>审视自己所处的行业，陈勇认为，汽车一定程度上也可以看作是具身智能：汽车本身有能源、有感知系统，加上大模型也会思考做决策，这就是具身智能的一个雏形。</p><p></p><p>当把思维放得更开一些，陈勇还设想“大脑 + 脑机接口 + 大模型”这种双脑协同工作，两个思想可以随时碰撞，可能让人机协同的融合度更高。</p><p></p><h3>结束语</h3><p></p><p></p><p>大模型上车后，整个行业都在等一个现象级产品的出现，但现在没人知道是什么。陈勇预计，这样一款现象级产品可能要两、三年后才会出现。</p><p></p><p>如今，大模型上车更多还是解决旧场景里的旧问题，车企们正在思考如何在新场景解决新问题。但汽车行业的路就是这样一步步走出来的。大模型能够上车的一个重要前提是，行业自身已经完成了动力结构从燃油到新能源的转变，但在新能源汽车来临时没有人能设想到大模型上车。如今大模型上车了，也没人能预测出未来到底会给汽车带来哪些变革。</p><p></p><p>谁能率先交出大模型上车的答卷，我们拭目以待。</p><p></p><p>华卫对此文亦有贡献。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/iARi98XQCDLPVvmZIWWk</id>
            <title>最高配128核！英特尔至强6性能核处理器发布：运行Llama2-7B 快了3.08倍</title>
            <link>https://www.infoq.cn/article/iARi98XQCDLPVvmZIWWk</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/iARi98XQCDLPVvmZIWWk</guid>
            <pubDate></pubDate>
            <updated>Fri, 27 Sep 2024 07:11:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>“英特尔至强6性能核，将是英特尔有史以来性能最强大的至强处理器。”英特尔数据中心与人工智能集团副总裁、中国区总经理陈葆立在9月6日的发布会上说道。</p><p>&nbsp;</p><p>英特尔本次宣布上市的至强6900P系列处理器（代号Granite Rapids-AP），最高配备128个内核，支持高达每秒6400MT的DDR5内存、每秒8800MT的MRDIMM内存、6条UPI 2.0链路（速率高达每秒24 GT），96条PCIe 5.0或64条CXL 2.0通道、504MB的L3缓存，支持FP16数据格式的英特尔®&nbsp;高级矩阵扩展（英特尔®&nbsp;AMX），可为AI和科学计算等内存带宽敏感型工作负载提供MRDIMM选择，且新增对CXL 2.0的支持。</p><p></p><p>&nbsp;英特尔至强6900P系列具备三个计算单元和两个IO单元，其中计算单元里包含了最重要的X86内核、内存控制器和缓存，I/O单元里面包含了PCle、CXL、UPI等通用协议，也包括了英特尔独有的加速器。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/c0/c019b5e6e0d993827bc9d7ac15b026d2.png" /></p><p></p><p>&nbsp;</p><p>“性能装备从64核到128核，单核性能1.2倍提升，每一个核都比以前更快。上一代平台所需电量是350瓦，这一代需要更多的供电（500瓦），但我们在增加30%功耗的情况下，算力却拥有了双倍提升。所以我们相信，在综合考量范围下，我们能够跟厂商打造一个更新、更强大、更高效的平台，并帮助最终用户降低30%的TCO。”陈葆立说道。&nbsp;&nbsp;&nbsp;&nbsp;</p><p>&nbsp;</p><p>英特尔至强6性能核的处理器测试运行了12种不同的常见工作负载，包括通用计算、数据库、科学计算、AI大模型（包括Llama2，Llama3 ）等，结果显示，单颗CPU性能和每瓦特性能与上一代产品相比快了两倍以上。值得注意的是，70亿参数的Llama2 大模型推理在AMX加速下，至强6处理器相比前一代有了3.08倍的提升。</p><p></p><p><img src="https://static001.geekbang.org/infoq/35/35bcc8977f4afd230731f027ebf3a72d.png" /></p><p>&nbsp;</p><p>MRDIMM是业界备受关注的领先内存技术，利用组装其上的数据缓冲区，实现两个列的同步操作，从而允许一次向 CPU 传输 128 字节的数据，而传统 DRAM 模块一次传输 64 字节。英特尔至强6性能核处理器，一个是使用标配6400MT/s，一个是使用更快的MRDIMM内存，对内存非常敏感的工作负载，包括科学计算、AI等，有1.2-1.3倍的提升。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/92/920bb746e2abedabf47f6fe3c81f48a3.png" /></p><p></p><p>&nbsp;另外，CXL也是英特尔发起的一个全新的技术，通过CXL扩展可以在数据库或者大内存的场景里支持更多、更大的内存。当前，英特尔 CXL 2.0 支持多种设备类型（Type 1、Type 2 和</p><p>Type 3）且可向后兼容 CXL 1.1；支持链路分叉 (link bifurcation)，即使一个主机端口</p><p>可以对接多个设备；提供更强的 CXL 内存 (Type 3) 分层支持，可实现容量和带宽扩展，同时支持以受控热插拔的方式添加/移除设备。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/CO7aCwGvJLU6u8kSy8XD</id>
            <title>更快速度更高质量！开发代办事项 API ，看 Amazon Q 加速软件开发！</title>
            <link>https://www.infoq.cn/article/CO7aCwGvJLU6u8kSy8XD</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/CO7aCwGvJLU6u8kSy8XD</guid>
            <pubDate></pubDate>
            <updated>Fri, 27 Sep 2024 07:05:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>软件开发团队一直在寻求加速软件开发生命周期（SDLC）的方法，以更快地发布高质量软件。作为一款由生成式 AI 驱动的助手，<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;可以帮助软件开发团队在 SDLC 的各个阶段中实现更高效的产出。</p><p>软件开发团队在分析需求、构建、测试和运维应用程序时，往往会在一些非核心任务上花费大量时间。基于亚马逊云科技 17 年相关专业知识进行训练的&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;，可以革新您在亚马逊云科技上构建、部署和运维应用程序的方式。通过自动化常规任务，<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;让开发团队能够把更多时间投入到创新和研发当中。<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;可以加速<a href="https://aws.amazon.com/cn/getting-started?trk=cndc-detail">新手入门</a>"，减少上下文切换，以及加速亚马逊云科技上应用程序的开发。</p><p></p><p><img src="https://static001.geekbang.org/infoq/35/354d3dd85adc260a4f164e00016e26fa.png" /></p><p></p><p>本文将以开发一个待办事项的 API 接口项目为示例，讲解如何使用&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;来加速 SDLC 的各个阶段。我们将利用&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business&nbsp;以及&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer&nbsp;来帮助实现该项目。我们将介绍&nbsp;<a href="https://aws.amazon.com/cn/q/business/?trk=cndc-detail">Amazon Q Business</a>"&nbsp;在规划和研究阶段的常见用法，以及&nbsp;<a href="https://aws.amazon.com/cn/q/developer/?trk=cndc-detail">Amazon Q Developer</a>"&nbsp;在研究、设计、开发、测试和维护阶段的应用。</p><p></p><h3>计划</h3><p></p><p>作为产品负责人，需要花费大量时间进行需求分析和创建用户故事，同时还需要研究内部文档，如功能规格说明书和业务需求，以了解所需的功能和目标。手动筛选文档是一项耗时的工作，而现在可以利用&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business 快速从内部文档或 Wiki （如 Confluence）中提取相关信息。</p><p></p><p><img src="https://static001.geekbang.org/infoq/22/226d676eebdedb5dcaf6b17975f3d83b.png" /></p><p></p><p><a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business 可以快速地和您的业务数据、业务信息和业务系统进行连接，让您可以进行定制对话、解决问题、生成内容并采取与业务相关的行动。<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business 提供超过&nbsp;<a href="https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/supported-connectors.html?trk=cndc-detail">40 个内置连接器</a>"，可连接流行的企业应用程序和文档存储库，包括&nbsp;<a href="https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/s3-connector.html?trk=cndc-detail">Amazon S3</a>"、Confluence、Salesforce&nbsp;等，让您只需少量的配置即可创建生成式 AI 的解决方案。<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business 还提供了与第三方应用程序交互的<a href="https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/plugins.html?trk=cndc-detail">插件</a>"。这些插件支持读写操作，可帮助提高终端用户的生产力。</p><p>因此，您不需要深入研究内部文档，只需使用自然语言向&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business 询问需求，它就会立即为您提供相关信息，并帮助简化任务和加速问题解决。</p><p>以我们的待办事项 API 接口项目为例，假设业务需求记录在 Confluence 中，而 Jira 用于任务管理。您可以分别通过 Confluence 连接器和 Jira 插件，使&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business 与 Confluence 和 Jira 连接。为了了解需求，您可以询问&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business 关于用例概述、业务驱动因素、非功能性需求等相关问题。<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business 会从 Confluence 文档中提取相关细节，并以清晰简洁的方式呈现给您。这样可以节省收集需求的时间，让您更专注于用户故事的开发。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2b/2b8c9d709610b18a444ee713cc4c3bb7.png" /></p><p></p><p>在充分理解需求之后，您可以要求&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business 为您撰写用户故事，甚至直接在 Jira 中为您创建相应的任务。对于本文的 API 接口项目，<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business 会根据需求量身定制生成用户故事，并在 Jira 中为您创建对应的待办事项，为您的团队节省时间，确保项目工作流程高效运转。</p><p></p><p><img src="https://static001.geekbang.org/infoq/10/10594e43da0a759996b5bbfcd6763173.png" /></p><p></p><p></p><h3>研究和设计</h3><p></p><p>假设上述用户故事被分配给您，您需要根据 Confluence 页面中描述的技术栈来实现它。</p><p>首先，您可以询问&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business，了解组织开发指南中技术栈的相关信息。<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business 会立即从托管在 Confluence 上的内部开发指南文档中为您搜索相关详细信息，并附带参考资料和引用。</p><p>作为开发人员，您可以在集成开发环境（IDE）中使用&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 协助软件开发，包括代码解释、代码生成以及代码改进（如调试和优化）。<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 可以提供诸如分析需求、评估不同方法、创建实施计划和示例代码等协助。它还可以研究技术选型、权衡利弊、推荐最佳实践，甚至与您进行头脑风暴来优化设计。</p><p>让我们看看&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 是如何帮助分析用户故事、设计，并制定实施计划。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cb/cb9915edddfc672bea3200af428f8c9c.png" /></p><p></p><p>让我们在设计中进一步完善非功能性需求，如安全性和性能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/53/533704f728dcfe66fd217cd4137b377c.png" /></p><p></p><p></p><h3>开发和测试</h3><p></p><p><a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 可以根据您指定的业务和技术需求生成代码片段。您可以审查自动生成的代码，手动复制并粘贴到编辑器中，或使用选项“插入到光标处”直接将其合并到源代码中。这有助于您快速原型化和迭代应用程序的新功能。<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 使用对话的上下文来指导后续的响应，这使得您可以专注于构建应用程序，无需离开 IDE 即可获得答复和基于特定上下文的编码指导。</p><p></p><p><a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 对于回答以下领域的问题特别有用：</p><p>在亚马逊云科技上构建的相关问题，包括亚马逊云科技服务的选择、限制和最佳实践。通用的软件开发概念，包括编程语言语法和应用程序开发。编写代码，包括解释代码、调试代码和编写单元测试。使用<a href="https://aws.amazon.com/cn/q/developer/code-transformation/?trk=cndc-detail">用于代码转换的 Amazon Q Developer Agent</a>"&nbsp;升级和现代化现有的应用程序代码。</p><p>在&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 生成的用户故事设计的基础上，您可以要求&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 实现 API 接口，并根据其他要求和参数进行完善。让我们与&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 合作，将设计变成实现。您可以利用&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 的专业知识进行构思、评估选项，并得出最佳解决方案。<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 可以基于需求进行智能讨论，集思广益，创造新的测试用例。然后，它可以帮助构建实施计划，并高效地添加健壮、全面、以及对边缘例子覆盖度高的测试用例。</p><p>让我们要求&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 根据设计生成代码。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5e/5e8a002b2963c96900f361a9d1b41318.png" /></p><p></p><p>现在，让我们要求&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 实现&nbsp;<a href="https://aws.amazon.com/cn/lambda/?trk=cndc-detail">Amazon Lambda</a>"&nbsp;函数。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a1/a1d2329896915bcf44dde1335d3ef3dc.png" /></p><p></p><p><a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 可以提供代码示例和代码片段，展示如何实现设计。您可以审核生成的代码，获得&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 的反馈，并无缝地将其集成到项目中。与&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 的协作可以让您利用其知识快速迭代和丰富应用程序的功能，从而提高生产力。</p><p><a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 还可以审查代码，并根据性能和其他参数找到改进和优化的空间。让我们要求&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 为我们的示例项目找出需要改进的地方。</p><p></p><p><img src="https://static001.geekbang.org/infoq/51/51a912d0043c315baf9fa07461401e06.png" /></p><p></p><p></p><h3>调试和故障排查</h3><p></p><p><a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 可以在故障排查和调试方面为您提供协助。对于不熟悉的错误代码或异常类型，您可以要求&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 研究其含义以及常见的解决方案。<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 还可以通过分析应用程序的调试日志，标出任何异常、错误或警告，从而提示潜在的问题。</p><p><a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 可以对由错误配置导致的网络连接问题进行排查，提供简明的问题分析和解决建议。它还可以研究亚马逊云科技最佳实践，识别哪些地方与最佳实践不一致。对于代码问题，它可以在支持的 IDE 中回答问题和进行代码调试。利用其对亚马逊云科技服务及其交互的了解，<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 可以提供特定亚马逊云科技服务的指导。在亚马逊云科技控制台中，<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 可以对您在使用亚马逊云科技服务时所收到的错误（如权限不足、配置不正确、超出服务限制）进行故障排查。</p><p>让我们使用命令行工具 cURL 通过访问&nbsp;<a href="https://aws.amazon.com/cn/api-gateway/?trk=cndc-detail">Amazon API Gateway</a>"&nbsp;的端点对我们的待办事项 API 进行测试。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ffc4f7d54a408fe6c7058fd3c250b13c.png" /></p><p></p><p>由于&nbsp;<a href="https://aws.amazon.com/cn/api-gateway/?trk=cndc-detail">Amazon API Gateway</a>"&nbsp;端点在调用 Amazon Lambda 函数在&nbsp;<a href="https://aws.amazon.com/cn/dynamodb/?trk=cndc-detail">Amazon DynamoDB</a>"&nbsp;表插入记录时抛出了内部服务器错误，让我们转到 Amazon Lambda 控制台进一步排查问题，并通过为 POST 方法创建测试事件直接测试该函数。在亚马逊云科技控制台中，您可以利用&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 对不同的控制台错误进行故障排查。对于上述错误，<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;会分析其问题并帮助找到解决方案。<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;将直接在控制台上解释如何通过添加<a href="https://aws.amazon.com/cn/dynamodb/?trk=cndc-detail">&nbsp;Amazon DynamoDB</a>"&nbsp;表名的环境变量来修复此错误。</p><p></p><p><img src="https://static001.geekbang.org/infoq/24/247f060fbe5289a355861b7659e8de0f.png" /></p><p></p><p>现在，让我们在 IDE 中要求&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 生成代码来修复这个错误。<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 随后会生成一个代码片段，用于在&nbsp;<a href="https://aws.amazon.com/cn/cdk/?trk=cndc-detail">Amazon CDK</a>"&nbsp;中为 Amazon Lambda 函数设置所需的环境变量。</p><p></p><p><img src="https://static001.geekbang.org/infoq/30/305f98f8949f31877de3e62db63abd51.png" /></p><p></p><p></p><h3>总结</h3><p></p><p>相信通过本文的介绍，您能够了解到如何利用&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business 和&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 来简化软件开发生命周期，从而加快产品发布速度。凭借对代码和亚马逊云科技资源的深入理解，<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 使开发团队能够在研究、设计、开发、测试和审查等阶段高效工作。通过自动化常规任务、提供专家指导、生成代码片段、优化实现代码和故障排查，<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Developer 让开发人员可以将注意力重新集中在推动创新的高质量的工作中。此外，通过&nbsp;<a href="https://aws.amazon.com/cn/q/?trk=cndc-detail">Amazon Q</a>"&nbsp;Business，团队可以借助生成式 AI 的力量，加快需求规划和研究阶段的进度。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/kUiU3VOE9N4lId0fwChz</id>
            <title>AI芯片或面临新一轮短缺，首席信息官们如何提前布局应对？</title>
            <link>https://www.infoq.cn/article/kUiU3VOE9N4lId0fwChz</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/kUiU3VOE9N4lId0fwChz</guid>
            <pubDate></pubDate>
            <updated>Fri, 27 Sep 2024 06:53:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>根据咨询研究机构贝恩公司本周发布的一份报告，随着 <a href="https://aicon.infoq.cn/202412/beijing/">AI </a>"计算需求的激增，数据中心芯片、个人电脑和智能手机的供应链将面临重大压力。其指出，持续的地缘政治紧张局势和其他供应风险可能会导致下一轮半导体短缺。</p><p></p><p>半导体的供需是一个微妙的平衡，过去几年的经历让业界对此深有感触。在此背景下，贝恩公司呼吁各方密切关注半导体供应链的复杂性——“当需求增加约 20% 或更多，很有可能打破平衡，导致芯片短缺’。”</p><p>从报告来看，其关键观察有以下几点：</p><p></p><p>数据中心及其专用芯片的支出依然强劲，主要云服务提供商预计在 2024 年的资本支出将同比增长 36%，这一增长主要源于对 AI 和加速计算的投资。如果数据中心对当前一代图形处理单元（GPU）的需求到 2026 年翻倍——鉴于当前的趋势，这是一个合理的假设，那么关键部件的供应商在某些情况下需要将产量增加 30% 或更多。为促进 AI 的增长，必须在建设数据中心、晶圆厂、先进封装技术和电力保障等方面整合复杂的供应链要素，确保获得先进的封装技术和充足的电力。</p><p></p><p>虽然报告的重点是购买芯片的组织需要做什么，但首席信息官们可以采取一些措施，以确保将来能获得所需的产品，或为价格剧烈波动做好准备。</p><p></p><p>Info-Tech 研究集团的研究主管 Scott Bickley 指出，先进的半导体供应链是全球最脆弱的供应链之一，必须有超过 5000 家供应商完美协作才能生产最先进的芯片。</p><p></p><p>他说，其中许多供应商“为单个公司供应单一的组件，如果没有它们，整个系统就会嘎然而止。单是技术障碍就令人瞠目结舌，更不用说台积电面临的地缘政治风险和物流管理的阻力了。”</p><p></p><p>Bickley 还表示，技术买家主要分为两类：一类是为大规模基础设施采购的买家，例如私有云环境...... 也可以说是财富 200 强规模的客户；另一类则是为小规模项目采购的买家，比如数据中心现代化、小规模的 LLM 内部模型，以及先进的 AI 功能 PC。</p><p></p><p>在私有云层面，Bickley 建议买家应立即制定技术战略。举例来说，你是否要大干一场，押注于英伟达下一代 Blackwell 系列 GPU，或选择第一代 H100 进行模型训练。数据中心基础设施的挑战不容小觑，尤其是在水冷环境和高密度 GPU 集群的设计上，以平衡能耗、性能和环境要求。</p><p></p><p>而传统企业环境中的技术买家面临的挑战则不同，他们由于规模较小，对供应商的影响力有限，在这种环境中，这些买家将不得不过度扩张，现在就下注，以确保以后的供应。“为生产延迟做提前规划可能需要买家承担一些昂贵的前沿技术产品库存，并且这些产品可能很快就会过时。”</p><p></p><p>Forrester Research 的高级分析师 Alvin Nguyen 补充道，谈到首席信息官可以做些什么来确保他们能够继续获得所需的产品，或者为价格的剧烈变化做好准备，他们需要考虑几个方面：</p><p>风险管理：Nguyen 说，生成式 AI 的进展速度以及对特定模型或方法的巨额投资，日后可能被证明是错误或非最佳的选择：“对于大多数希望利用 AI 而非推动 AI 市场发展的企业来说，规避风险，利用现有的 AI 服务，而不是大力获取大量 AI 基础设施，是最有意义的。”人员培训：首席信息官和技术高管“需要投资于 现有员工的培训 / 技能提升，以及为已知的可有效利用的 AI 用例（如代码开发）招聘具备基本 AI 技能的新人才。他们需要让他们的技术人员、架构师和工程师试验最新的 AI 技术，以确定他们需要做出的选择。如果你能够获取大量的 AI 基础设施，那么就在这里大量投资，以建立相对于他人的竞争优势。”AI 基础设施：目前 AI 加速器的需求超出供应，在未来几年内不太可能改变，因此 AI 加速器 /GPU 目前会有溢价。因此，企业可以考虑利用云服务提供商的 AI 服务。可持续性：生成式 AI 对更多能源和水资源的需求，以及其碳足迹，已经影响了一些组织实现其可持续性目标的能力。在对 AI 的需求持续增长的情况下，这种情况不太可能改变。因此首席信息官和技术高管需要从可再生能源中采购电力，并在可能的情况下采用可持续的建筑和运营实践（建筑材料的选择、施工方法、回收利用）。</p><p></p><p>参考链接：</p><p>https://www.cio.com/article/3540407/bain-warns-prepare-for-ai-chip-shortage.html</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>