<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/6TBscTYlUozuf1cQpz0R</id>
            <title>AI驱动的算力变革：如何突破智能算力瓶颈？</title>
            <link>https://www.infoq.cn/article/6TBscTYlUozuf1cQpz0R</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/6TBscTYlUozuf1cQpz0R</guid>
            <pubDate></pubDate>
            <updated>Thu, 09 Nov 2023 07:21:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI 大模型热潮, 智能算力, 算力解决方案, 智能算力供应不足
<br>
<br>
总结: AI 大模型的兴起加剧了智能算力的稀缺问题，企业需要寻找更高效的算力解决方案。智能算力供应不足的根本原因是算力需求的爆发，如何解决供给不足的问题成为挑战。为了提高智能算力的利用率，需要加强算力产品的迭代和供应链的稳定，同时推动整个产业链的成熟和创新。 </div>
                        <hr>
                    
                    <p>AI 大模型热潮进一步加速了智能算力的稀缺，在这一背景下，如何找到更高效的算力解决方案成为很多企业需要面对的难题。回顾算力的整个演化趋势，从通用算力到智能算力发生了哪些变化？智能算力紧缺的根本原因是什么？如何更好地解决智能算力供应不足的问题？如何提高智能算力的利用率？近日，InfoQ《极客有约》邀请到了首都在线生态副总裁吴锦晟、中关村科金研发总监王素文，共话《AI 驱动的算力变革：如何突破智能算力瓶颈？》。</p><p>&nbsp;</p><p>以下为访谈实录，完整视频参看：<a href="https://www.infoq.cn/video/2I8SXyLdICQOjje6gkao">https://www.infoq.cn/video/2I8SXyLdICQOjje6gkao</a>"</p><p>&nbsp;</p><p>王素文：欢迎大家来到 InfoQ 极客有约，我是今天的特邀主持人，中关村科金研发总监王素文。本期直播，我们邀请到了首都在线生态副总裁吴锦晟来给我们做分享。我们今天直播的主题是《AI 驱动的算力变革：如何突破智能算力瓶颈？》。首先请吴锦晟与网友们打个招呼。</p><p>&nbsp;</p><p>吴锦晟：非常感谢各位能够抽出时间来参加今天的直播活动，我们将一同深入探讨智能算力领域的重要议题，同时也会探讨整个行业的发展趋势。今天，我们希望大家在这次交流中积极提出问题，共同探讨整个行业未来的发展方向。让我们一起来思考如何推动这个行业的进步。</p><p></p><h2>AI 大模型带来哪些趋势变革？</h2><p></p><p>&nbsp;</p><p>王素文：吴老师在人工智能领域有着丰富的经验。现在让我们回到今天的主题，即算力问题。您在今年 9 月初举办的 QCon 全球软件开发大会·北京站中，分享了《<a href="https://qcon.infoq.cn/202309/beijing/presentation/5422">大模型时代下的个人成长</a>"》主题演讲。事实上，ChatGPT 这种大型模型的智能化应用已经引起了广泛的关注，而且在各个方面都带来了革命性的变化。您能从个人和行业两个角度，谈谈这轮 AI 大模型热潮带来的变化吗？</p><p>&nbsp;</p><p>吴锦晟：我认为可以从两个层面来回答这个问题。</p><p>&nbsp;</p><p>从个人角度来看，我认为有三个方面的变化。首先，对于我的个人工作效率来说，大模型应用对我产生了巨大的提升。以前，我需要处理许多 PPT 或产品文档等工作，但现在我可以委托模型来生成初步的输出，然后基于这个结构化的输出进行进一步的编辑，这显著提高了我的工作效率。其次，在编写代码方面。如果需要生成代码，以前我可能会查阅各种文档，去 Google 或 Stack Overflow 查找解决方法。但现在，我养成了使用大模型工具的习惯，用它来生成一些代码脚本，这极大地减轻了我的负担。此外，许多软件应用，如办公软件 WPS 或 Office，都具有集成的 AI 助手功能，进一步提高了工作效率。大模型在各种应用场景中都发挥了积极作用，未来预计将在更多场景中带来工作效率的改进。</p><p>&nbsp;</p><p>第二个方面是个人职业发展的拓宽。从技术角度来看，过去我们可能只需要学习一门编程语言并编写业务代码，但现在我们需要学习各种框架、多种编程语言，以解决不同业务或场景的问题。我们还需要深入研究深度神经网络和深度学习，以及与人工智能相关的算法和应用，包括模型训练和推理。这些要求使得个人职业发展更加广泛，从程序员逐渐演变为智能化工程师。</p><p>&nbsp;</p><p>第三个方面是生活方式的改变，这是基于工作效率和职业发展的改变。我们开发的产品和应用已经开始改变我们的生活方式，包括家庭、汽车等各种场景，智能化的交互应用为我们的生活带来更多便利和乐趣。因此，大模型的应用对个人和行业层面都产生了深远的影响。</p><p>&nbsp;</p><p>从行业层面来看，我认为也存在三个方向的变化。首先，大模型需要应用场景，因此行业中的创新应用领域受到广泛关注。大模型为各种应用提供了创新的可能性，如文本生成、意图理解以及多模态应用等，这在金融科技、办公软件等领域都有着广泛应用。</p><p>&nbsp;</p><p>其次，产业链升级是不可避免的。大模型的应用涵盖了应用层、算法层和基础设施层，包括云计算、芯片和服务器等，形成了庞大的产业链。这个产业链会随着大模型的发展而不断升级和演进。</p><p>&nbsp;</p><p>第三，行业格局也会发生变化。随着大模型的诞生和应用推进，行业内的竞争格局会发生一些变化。许多大公司都在强调 AI 驱动的发展，这表明 AI 领域是一个新兴的蓝海市场，吸引了许多新的参与者，这将导致行业格局的变化。因此，大 AI 和国内的 AI 大模型热潮将带来多方面的变革，从个人到整个行业都将受益。</p><p>&nbsp;</p><p>王素文：我个人也在使用大模型等工具，确实能够提高个人工作效率，这对我来说非常有帮助。同时，大模型确实为我们的社会带来了巨大的价值和潜力，这是难以估量的。它可以帮助我们解决各种复杂的问题，并极大地推动各行各业的变革，对每个个体都产生深远的影响。它提高了我们的生活质量，推动了整个社会向前发展，并有可能开辟新的领域和可能性。AI 大模型的快速发展也带动了算力需求，加速智能算力的稀缺，这轮 AI 算力需求的爆发给您带来最大的感受是什么？</p><p>&nbsp;</p><p>吴锦晟：我想从三个方面分享我的主要感受。首先，由于我从事公有云服务行业，我最明显的感受是产品不断加速迭代。例如，我们通常的计算产品，如 CPU 核心的云主机，现在逐渐演变为以 GPU 核心为主的云主机，这从底层逻辑计算转向了并行计算，这是一项重大变化。</p><p>&nbsp;</p><p>其次，产品的选择也在发生变化，包括在 CPU 领域，我们有 AMD、Intel 等，还有一些国产 CPU。在 GPU 领域，目前竞争格局主要由 NVIDIA 等主导，但也有许多国内的 GPU 制造商。未来的发展趋势可能会以一云多芯为主，涉及到异构计算平台。异构计算平台的不同层面也需要进行快速迭代，以提供更出色的云服务产品。</p><p>&nbsp;</p><p>第二个方面是供应链问题，特别是关于 NVIDIA 产能和国内政策的影响，导致供应链不稳定。这对整个行业都产生了一定影响，所以供应链问题需要被解决。此外，国内 GPU 制造和英伟达之外的解决方案也需要度过基础瓶颈，以使供应链更加稳定。</p><p>&nbsp;</p><p>最后，整个产业链的每个环节，从芯片到服务器到云，再到 AI 框架层、算子融合和模型训练，甚至应用层，都还不够成熟或不够稳定。由于技术在快速迭代，商业模式目前还不够明确，所以这个领域仍然充满了创新机会。在这个背景下，我们正面临一个非常具有创新潜力的蓝海市场，这是我个人的最大感受。</p><p></p><h2>如何更好地解决智能算力供给不足问题？</h2><p></p><p>&nbsp;</p><p>王素文：我们面临挑战，但同时也蕴藏机遇。特别是对国内企业来说，这是一个创新和创业的机会，我认为这非常有潜力。我个人的感受是，从深入学习期间开始，英伟达等公司抓住了宝贵的机会，一路走到大模型的爆发阶段，因此取得了巨大的成功。您认为过去十年，AI 领域主要的算力载体是否发生了变化？从通用算力到智能算力，算力的演化趋势呈现哪些特点？</p><p>&nbsp;</p><p>吴锦晟：我认为，技术栈正在经历一些分叉。首先，在算力层面，我们看到通用算力正在转向专用算力，也可以称为智能算力。专用算力包括以 GPU 为核心的并行训练加速，例如，英伟达正在研发第二代 DPU，谷歌也有自己的 GPU，还有新型的算力形态，如 NPU 等，用于加速 AI 载体。</p><p>&nbsp;</p><p>第二个趋势是从单点到分布式的发展。在十多年前，我们可能只需要使用 CPU 进行 AI 模型训练，然后逐渐转向 GPU 加 CPU 的方式。当时由于 CPU 和 GPU 的编程方式不同，需要重新编译两次才能在 CPU 和 GPU 上运行，因此在那个时候，AI 模型通常在单台机器上单卡上运行。随着模型参数的增加和模型类型的多样化，从单机单卡逐渐演变成了单机多卡，然后随着 GPU 的崛起，从单机多卡又发展为分布式训练。</p><p>&nbsp;</p><p>第三个趋势是能耗和可持续性。随着训练集群的出现，能耗上升成为一个问题，数据中心需要进行改建和升级以满足能耗要求，这也引发了合规和可持续性的关注。高能耗需要政府批准，因此降低能耗、实现绿色和节能成为趋势。</p><p>&nbsp;</p><p>第四个趋势是软硬结合。从纯硬件走向软硬件结合，尤其是英伟达等公司竞争，软件生态系统变得至关重要。软件工程师和人工智能算法工程师的参与推动了这一趋势。</p><p>&nbsp;</p><p>王素文：有观众提问，得算力者达得大模型吗？</p><p>&nbsp;</p><p>吴锦晟：目前，我认为一个关键因素是模型的参数量以及模型是否具有涌现能力。大模型的核心在于其巨大的参数量，而关键在于这些参数是否能够展现出所谓的涌现能力。要实现这种涌现能力，模型训练过程通常需要一个超大规模的算力集群的支持。因此，在目前这个阶段，算力对于大模型的发展确实至关重要。</p><p>&nbsp;</p><p>王素文：我非常认同吴老师的观点，因为在大模型、大数据、和强大算力这三个核心要素中，算力确实扮演着至关重要的角色。大模型通常是开源可用的，而各行各业都积累了大量的数据，但是只有通过强大的算力，才能够高效地训练出这些大模型，以实现更好的效果。另外，我也关注到信通院去年发布的《中国算力发展指数白皮书》显示，2020 年中国智能计算的算力为算力总规模的 41%，而到 2023 年，占比将迅速攀升到 70%。智能算力已经成为新趋势了吗？当前智能算力主要应用在哪些领域？</p><p>&nbsp;</p><p>吴锦晟：这绝对是一个趋势，而且是与国家战略相一致的趋势。中国政府已经将提升整体算力规模作为重要目标，响应了国家发改委的要求，使智能算力的发展成为现实。此外，智能算力的发展也是为了配合人工智能的快速发展。正如你所说，人工智能需要高效的算力，就像没有高速公路，好车也无法发挥作用。这已经成为一个非常明确的趋势。</p><p>&nbsp;</p><p>智能算力的主要应用领域是人工智能，而在人工智能领域中有许多细分场景，包括智慧城市、自动驾驶、智能制造、金融科技、互动娱乐、电子商务等等。举个例子，智慧城市领域的摄像头需要大量的智能算力来计算车流量、人流密度等，从而改善城市运营。对于一个拥有 2000 万人口的城市，可能需要使用 1000P 的算力来支持这些场景，特别是随着自动驾驶车辆的普及，对算力的需求将进一步增加。未来，各种场景都需要强大的算力支持。</p><p>&nbsp;</p><p>王素文：我认为现在基本上只要涉及到人工智能，都需要算力支持，无论是在训练模型还是进行推理的场景下，都离不开强大的算力。因此，人工智能的需求正在不断增加，越来越旺盛。那么，根据您的观察，造成智能算力紧缺的根本原因是什么？整个产业链如何才能更好地解决智能算力供应不足的问题？</p><p>&nbsp;</p><p>吴锦晟：我认为造成智能算力紧缺的根本原因有三个方面。第一，AI 技术的创新突破出乎意料之外，两年前我们对大型模型或基于神经网络的模型的讨论还处于假设阶段，但现在已经确信这些技术的成功。这种技术创新驱动了对算力和智能算力的需求急剧增长，推动了规模的扩大，这是最关键的原因。</p><p>&nbsp;</p><p>第二，尽管需求不断增长，但供应跟不上，导致供需失衡。供需失衡的背后原因复杂多样。首先，智能算力高度依赖于高性能计算卡，受到相关政策限制，造成了困扰。其次，国内加速芯片的生态系统尚不完善，也导致了短期内的供应不足。</p><p>&nbsp;</p><p>第三，规模的快速增长导致了底层智算中心建设跟不上节奏，能耗受到限制。这三个因素是造成智能算力短期内供应不足的根本原因。</p><p>&nbsp;</p><p>那么，如何更好地解决智能算力供给不足的问题呢？我认为有三个关键方面需要考虑。首先，需要政策的正确引导，加大基础设施建设的力度，以根本性解决算力供需不足的问题。第二，需要提高算力效率，尤其在供需失衡的情况下，通过软件优化和算法优化，提高算力的效率以及算力资源的利用率，以帮助解决问题。第三，关键的一点是技术攻关，国内的技术需要在适当的时间点攻克，快速量产，确保产能能够跟上需求，这也是根本性解决智能算力供给不足问题的关键解决方案。</p><p>&nbsp;</p><p>除了前面提到的三个关键解决方案，我认为还有两个值得关注的方面。第一个关注点是新型的算力计算形式。虽然目前的智能算力领域取得了技术突破，但在技术层面可能还存在一些未来的可能性，例如量子计算等。此外，专用芯片的发展也将带来新的计算方式，结合 GPU 等应用，可能会出现新的解决算力不足问题的计算形式。第二个关注点是培育算力产业生态。目前，国内的 GPU 生态还不够完备，因此需要整个产业链的从业者共同努力，才能够发展和完善这个生态系统。这一方面也非常重要。</p><p>&nbsp;</p><p>王素文：除了供应不足，目前智能算力还存在着资源利用不均衡的问题。大型公司拥有足够的资源来提前储备算力资源，但中小型企业或个人可能很难找到可利用的算力资源。有些个人可能拥有算力资源，但不知道如何共享或者利用这些资源，要想提高智能算力的利用率，技术层面需要做出哪些改进和优化？目前行业有哪些比较成熟的解决方案？</p><p>&nbsp;</p><p>吴锦晟：综合来看，解决智能算力不足问题可以从以下四个主要方面着手：</p><p>&nbsp;</p><p>算法优化：通过算法层面的优化，包括算子层面的矩阵调优和元素级算子的融合等，提高现有算法的效率，同时在大规模训练集群调度层面进行并行策略的调优，以提高算力利用率。这需要大量的研究和技术努力。算力调度：通过动态的算力调度和任务调度，提高算力的利用率。这包括不同颗粒度的调度，在国内已经有一些相关技术努力。硬件加速：利用新型专用芯片和硬件来加速特定领域的 AI 任务。通过专用硬件的加速，可以提高算力的效率。例如，在视频分析领域可以使用专用硬件（如 DPU）来处理。云资源共享：充分利用云计算的特性，建立共享的资源池，以提高资源的复用率和利用率。这可以通过公有云构建的共享资源池来实现，为业务提供更敏捷的资源分配。</p><p>&nbsp;</p><p>需要指出的是，在这些方面已经有一些成熟的解决方案，例如在算法优化层面的训练框架、云计算基于&nbsp;Kubernetes&nbsp;的容器化方案、以及硬件加速方面的 CUDA 等。我们公司已经在建设 GPU 为核心的资源池，以提供可复用的算力资源。这些解决方案可以为解决智能算力不足问题提供帮助。</p><p>&nbsp;</p><p>王素文：绿色计算也是当前大家比较关心的话题，当前绿色计算面临的主要挑战是什么？在智能算力的应用中，如何平衡算力需求和环保要求？</p><p>&nbsp;</p><p>吴锦晟：环保与能耗之间的平衡问题确实是非常重要的。我认为主要有以下三个方面的挑战：</p><p>&nbsp;</p><p>合理的能耗：确保在提高智能算力的同时，能够实现合理的能耗。这需要关注环保方案的成本和效益，以确保能源消耗与投入成本之间的平衡。同时，需要考虑消费者是否愿意分担环保支出的成本，以解决谁来买单的问题。强制监管政策也可以在此方面起到作用，例如对数据中心的 PUE（能源使用效率）进行监管。宏观布局：在国家层面进行数据中心的布局规划是一个重要的考虑因素。将实时计算与底线计算在不同地区分布，可以有助于减少对某一地区的能源消耗压力。这种宏观规划有助于实现地理上的平衡，并减轻某些地区的环境负担。微观层面：在数据中心的微观层面，可以采取低功耗芯片的设计和使用，以减少能源消耗。此外，持续进行绿色能源计算方面的研究和迭代，以寻找更环保的能源来源。这包括太阳能、风能、水能等可再生能源的利用。</p><p>&nbsp;</p><p>综合来看，需要在多个层面采取措施来解决智能算力的环保和能耗问题，包括政策层面的监管、宏观层面的规划，以及微观层面的技术创新。这些努力可以帮助平衡环保和成本之间的关系，同时提高能源利用效率。</p><p>&nbsp;</p><p>王素文：当前算法模型愈加复杂，AI 应用不断延伸，这些都对智能算力的发展提出了更高要求，怎样才能进一步推动智能算力的发展和应用？在技术、人才、产业链建设等方面，有哪些可探索的方向？</p><p>&nbsp;</p><p>吴锦晟：这个问题比较大，我尝试回答，后面我也想听听王素文老师的意见。推动智能算力发展和应用的关键因素有以下几点：</p><p>&nbsp;</p><p>应用场景导向：智能算力的发展必须与实际应用场景的需求相匹配。建立国产联盟或合作方式，可以帮助政府、企业和研究机构更好地理解并应对不同行业中的具体问题，以确保技术的发展具有实际意义。技术多维度深耕：在技术层面，硬件、软件、算法和应用等多个维度都有潜力深入研究和开发。这需要不断的技术创新和专业人才的支持，以满足不断变化的需求。人才培养：当前对于与大模型和智能算力相关的高级人才的紧缺情况。提出了培养体系化的人才的重要性，这包括高校、培训机构和企业内部的培训，以确保新一代的工程师和科学家能够胜任复杂的任务。产业链建设：产业链上下游之间的协作和连接的重要性。建议通过建立生态系统，将不同领域的参与者连接起来，以便共同解决问题和实现创新。</p><p>&nbsp;</p><p>王素文：我们中关村科金作为需求方，也希望在智能算力领域发挥供给端的作用，以降低算力成本、加大供给，并推动各种大模型应用产品的更快推出。这种需求确实非常重要，特别是在当前和未来的技术和市场环境下。有观众提问，现在无人驾驶领域的算力需求是什么样的？</p><p>&nbsp;</p><p>吴锦晟：我认为，在智能算力的发展方面，可以从三个不同的角度来考虑。首先是在无人驾驶领域，特别是车辆端。现在，许多汽车配备了强大的车载芯片，具备处理大量任务的算力能力。这为满足无人驾驶的需求提供了供给方式。</p><p>&nbsp;</p><p>第二个方面是近端计算，也就是边缘计算。未来可能会出现一种称为 AIDN（智能算力分发网络）的技术，类似于现在的 CDN（内容分发网络），但专注于智能算力。这意味着在边缘设备上进行处理，例如在无人驾驶汽车、路上通信设备以及车辆之间的通信中，都需要进行智能算力处理。这是第二个方面。</p><p>&nbsp;</p><p>第三个方面是云端计算，其中数据采集后可以进行在线或离线的业务计算。</p><p>&nbsp;</p><p>在这三个端上，都需要提供智能算力。尽管都提供智能算力，但从无人驾驶需求的角度来看，需要根据不同的场景需求，采用不同的供给方式来满足对延迟和静态计算能力的需求。</p><p>&nbsp;</p><p>王素文：确实，在无人驾驶领域，各个新能源汽车厂商采用了不同的传感器和处理方案。最早的一些方案包括使用激光雷达等传感器技术，这些方案对算力需求也相对较高。然而，现在一些公司，例如特斯拉，采用了计算机视觉（CV）技术，这种方法也需要更多的算力来处理图像和视频数据，因此对智能算力的需求也相对旺盛。这说明了在不同的无人驾驶技术路线上，都需要强大的算力支持。</p><p>&nbsp;</p><p>吴锦晟：是的，使用激光雷达等传感器的处理通常在车辆本身的车端进行，而使用计算机视觉（CV）等视觉处理技术的处理则在近端进行。因为不同的无人驾驶场景具有不同的需求，所以对算力供给的方式也会因情况而异。</p><p></p><h2>技术突破是推动算力发展的核心驱动力</h2><p></p><p>&nbsp;</p><p>王素文：现在企业在构建大模型算力基础设施时通常关注哪些方面？不同行业和应用场景对于算力基础设施的需求和特点是什么？对于那些感兴趣，但还未建设算力基础设施的企业，您会给他们哪些建议？</p><p>&nbsp;</p><p>吴锦晟：在构建大模型算力基础设施时需要精确测算投入和产出比。先回答下需要关注哪些方面：</p><p>&nbsp;</p><p>首先，关注成本是核心，因为企业需要精细化运营。在算力基础设施的构建过程中，必须准确测算投入和产出比（ROI）。</p><p>&nbsp;</p><p>其次，算力规模是一个重要的考虑因素。企业需要决定是一次性投入还是通过云来满足需求的弹性问题，这涉及算力规模的估算。</p><p>&nbsp;</p><p>第三，要考虑算力基础设施的建设周期和供应链问题。这是构建基础设施时必须考虑的因素。</p><p>&nbsp;</p><p>第四，选择适当的技术方案非常重要，因为不同的应用场景需要不同的计算、存储和网络解决方案。</p><p>&nbsp;</p><p>第五，进行可行性研究，考虑性能、安全性、可靠性和可扩展性等多个方面的因素来确定基础设施方案的可行性。</p><p>&nbsp;</p><p>最后，需要理解业务生命周期的理论，这是一个重要的方法论，可以指导企业在业务不同阶段采取不同的策略，包括上云、运营自建和进行生命周期管理。在一个业务的平稳期、生命周期的末端或衰退期，我们通常会考虑进行维护和管理，因为即使业务已经衰退，仍然有用户在使用，我们不能直接关闭它。因此，我们会采取一种语义上的迁移策略，以处理生命周期的尾部工作。这是整个发展过程中的一个重要步骤，与互联网的生命周期理论相似。这个过程确实需要关注如何优化、维护和管理旧业务，以最大程度地提供价值，并且在适当的时候进行有序的退出。这样可以确保资源的有效利用，同时满足用户的需求。</p><p>&nbsp;</p><p>王素文：有观众提问，国内外目前算力产业链现在都存在哪些差距？咱们怎么能才能超越他们？</p><p>&nbsp;</p><p>吴锦晟：首先，我认为算力的核心在于计算芯片。就目前国产 GPU 的发展情况而言，我们在设计能力方面已经不逊色于国外一些大厂，包括 NVIDIA、英特尔。然而，目前最大的问题在于芯片制程方面存在瓶颈。我们缺乏光刻机，而在制程方面与海外厂商相比仍然存在较大差距。最近，我们看到台积电已经不再代工生产国产 GPU，这对国内算力产业链的发展有直接影响。不过，也有一些积极的迹象，例如华为 Mate 60 的发布为我们带来了希望。因此，关键问题在于我们何时能够克服这些挑战，即何时能够迅速缩小与国外的差距，这是一个关键的变数。</p><p>&nbsp;</p><p>第二个关键层面是软件生态。为什么 NVIDIA 能够独领风骚？主要是因为他们以扩大为核心的软件生态做得非常出色。然而，目前国产 GPU 在这方面仍然没有实现行业内的统一，各家仍然各自为战。政策引导、行业倡议，以及国内算力联盟等方式都可能有助于我们解决这个关键的软件生态问题。</p><p>&nbsp;</p><p>第三个关键点是除了计算本身之外，还有其他核心问题，包括大型模型的推理和训练场景对存储的不同需求。如何选择适用于不同场景的存储方案是一个重要问题。此外，通信领域也需要关注，特别是网络方面。我们知道 NVIDIA 已经收购了 IB 公司，这对于通信和集群内网络的供给是一个重要的发展。因此，我们需要联合努力解决这些网络层面的挑战。</p><p>&nbsp;</p><p>王素文：确实，如果我们想要迎头赶上国外，国内的企业需要更加努力，这还包括整个行业和政府层面。我们需要共同努力来建设一个更加健全的生态系统，大家需要全力协作，以实现一些重要突破。最后一个问题，有观点认为“技术突破是算力发展的根本”，您对此怎么看？展望未来，算力发展将呈现怎样的趋势？</p><p>&nbsp;</p><p>吴锦晟：我认为技术突破确实是推动算力发展最核心的驱动力。因为从本质上来说，智能算力的需求呈现出快速增长趋势，所以我们可以将其类比于过去讨论的 CPU 的摩尔定律，现在我们可以谈论新型摩尔定律，这同样对智能算力的发展具有指导性作用。</p><p>&nbsp;</p><p>第二个方面是，我认为智能算力的发展可能会在技术上出现分叉。这个分叉可以在两个层面上发生。首先，大模型的训练层面可能更像高性能计算（HPC）。并非每个公司都需要进行大模型的训练，因此基础大模型的数量可能最终会有限。因此，依托于超大规模训练集群的基础大模型属于高性能计算范畴，它将是科技领域的一个重要存在。</p><p>&nbsp;</p><p>第二个方向是模型应用层。基于大模型进行二次预训练、特征提取和推理等操作需要更多通用性和工程化的智能算力。在这种情况下，我们需要云计算、AIDN（智能算力分发网络）以及边缘计算等解决方案，以推动这一方向的发展。这个趋势对于实现AI算力的广泛应用至关重要。</p><p>&nbsp;</p><p>经常会听到一种说法，即让算力像水电一样随时可用。尽管这句话听起来很容易，但在底层，实际上需要产业从业者共同努力才能实现这个目标。</p><p>&nbsp;</p><p>王素文：我认为现在的算力就像是基础能源一样重要。从这个角度来看，我们需要在社会层面，包括技术、人才以及整个产业链建设方面，增加更多的投入。只有这样，我们才能逐步解决这个问题，特别是在国内算力领域。最后我们再回答一个观众问题，国内光刻机有重大突破了吗？</p><p>&nbsp;</p><p>吴锦晟：这个信息相对较为机密，我也没有相关的详细情况。然而，在 7 纳米制程方面，似乎取得了一些突破，因为通过多次反复的多层光刻，Mate 60 芯片似乎可以生产出来。但是，这个过程中会面临两个问题。首先是良品率可能会有问题，第二是产能跟不上。目前，如何解决这些问题还需要进一步观察。</p><p></p><h3>嘉宾介绍</h3><p></p><p>&nbsp;</p><p>特邀主持：</p><p>&nbsp;</p><p>王素文，中关村科金研发总监。</p><p>&nbsp;</p><p>嘉宾：</p><p>&nbsp;</p><p>吴锦晟，首都在线生态副总裁、TGO 鲲鹏会（上海）学员，技术专家，在政法、钢铁、互联网、人工智能等行业的信息化建设方面有着丰富经验。近几年，致力于智能化应用助力社会变革，并因此对于人工智能与行业应用的复杂关系与互动产生了浓厚兴趣。曾编写《云渲一体技术与应用白皮书》、《Docker 技术入门实践》等多本行业领域的技术书籍。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/bwZ5qPQsZKyRLXkC1Mit</id>
            <title>重塑GitHub、颠覆程序开发：GitHub Universe 2023发布重大更新</title>
            <link>https://www.infoq.cn/article/bwZ5qPQsZKyRLXkC1Mit</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/bwZ5qPQsZKyRLXkC1Mit</guid>
            <pubDate></pubDate>
            <updated>Thu, 09 Nov 2023 06:05:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: GitHub, Copilot, 人工智能, 企业计划
<br>
<br>
总结: GitHub的母公司微软表示，GitHub Copilot软件的付费客户在第三季度增长了40%，超过了100万付费用户。GitHub在年度会议上发布了Copilot企业计划，允许客户根据代码库进行功能定制，并公布了Copilot Chat的上线时间。这一举措被认为是使每个人都能够编写代码的一大步。然而，也有人担心微软的举动可能会破坏GitHub的协作能力。GitHub还计划推出新的企业级Copilot订阅套餐，允许公司利用自有代码库进行底层模型微调，从而获得更个性化的Copilot Chat使用体验。 </div>
                        <hr>
                    
                    <p>GitHub 的东家微软看到了生成式 AI业务的大幅增长，其首席执行官萨蒂亚·纳德拉 (Satya Nadella) 告诉华尔街，GitHub Copilot 软件的付费客户在第三季度比上一季度增长了 40%。纳德拉表示：“我们在超过 37,000 个组织中拥有超过 100 万付费Copilot用户。”</p><p>&nbsp;</p><p>现在，该平台以现有的全球用户群为基础，在正在进行的年度 GitHub 会议——Universe 2023上发布了新的人工智能重大公告：GitHub公布Copilot企业计划，允许客户根据代码库做功能定制，并公布了Copilot Chat的明确推出时间。</p><p>&nbsp;</p><p>GitHub 首席执行官Thomas Dohmke表示，他们正在逐步将 Copilot 与 GitHub 各方面融合，并将其作为一个重要组成部分。可以说，这是GitHub的一次重塑，正如他所说：“就像 GitHub 是在 Git 基础上构建的一样，今天我们正在 Copilot 的基础上重新构建它。”</p><p>&nbsp;</p><p>关于这次的“重建”，一些网友评论说这似乎朝着使每个人都能够编写代码的方向迈出了坚实的一步。但也有人担心微软这个举动会破坏掉GitHub的协作能力，因此有人建议保留Git部分，单独建立一个GitHub Copilot平台。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/3f/3fbdfbe29da62eca24bcaedca669c01b.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>无论如何，今年的开发者工具取得了非常显著的进展，GitHub的意义也不再仅作为一个代码托管平台了。我们还总结了 GitHub Universe 2023 上的重大更新：&nbsp;</p><p>&nbsp;</p><p></p><h2>Copilot Chat将全面上线</h2><p></p><p>&nbsp;</p><p>GitHub早在今年3月就公布了Copilot Chat的相关消息，7月向企业用户交付了beta公测版，并于9月将个人用户也纳入公测范围。下个月（12月），Copilot Chat将全面上线，不过GitHub没有给出通用版本的确切落地日期。</p><p>&nbsp;</p><p>简而言之，Copilot Chat是一款聊天机器人，运行在开发者的集成开发环境（IDE）之内，允许用户就当前正在处理的代码询问相关问题，包括让它们识别特定程序中的bug并提供修复建议，甚至可以就特定代码行做出内联反馈。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/73/73d91365465b27bcb41740dbbd68d703.png" /></p><p></p><p>GitHub Copilot Chat。</p><p>Copilot Chat由最新OpenAI大语言模型（LLM）GPT-4提供支持，并作为标准Copilot订阅套餐的组成部分，个人用户每月10美元，企业用户每月19美元。</p><p>&nbsp;</p><p></p><h2>企业级新套餐</h2><p></p><p>&nbsp;</p><p>GitHub同时表示将推出新的企业级Copilot订阅套餐，每月收费为39美元。Copilot Enterprise将于2024年2月正式发布，将包含现有业务套餐中的所有内容，外加一些值得关注的附加功能——包括允许公司利用自有代码库进行底层模型微调，从而获得更加个性化的Copilot Chat使用体验。</p><p>&nbsp;</p><p>基本使用方式为：公司将Copilot接入自己的代码库，开发者即可获得关于内部私有代码的相关建议。这又与前面提到的Copilot Chat新功能有所关联。对于订阅了Copilot Enterprise的用户来说，Copilot Chat将超越代码编辑器和IDE，一路延伸至GitHub.com，帮助开发人员深入研究自己的代码、文档和PR，提供更为广泛的问题摘要、建议和答案。</p><p>&nbsp;</p><p>GitHub CEO Thomas Dohmke在最新发布的评论博文中表示，“通过将Copilot Chat接入您在GitHub.com上的代码仓库，Copilot Enterprise可以帮助您的开发团队快速厘清代码库、搜索和构建文档、根据内部及私有代码获取建议，并快速审查PR。组织代码库中的集体知识将跃然于您的指尖，开发人员不仅可以加快代码编写速度，更能够以领先于竞争对手的方式部署应用程序、功能和更新。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/c0/c0121c7ac431131f834b9e458f782c2a.png" /></p><p></p><p>Copilot Enterprise：通过“智能操作”生成PR摘要。</p><p>&nbsp;</p><p>其实在此之前，Copilot Chat就已经能够与IDE中的私有工作区配合使用，只不过后者要求用户在本地保存一份代码仓库副本。Copilot Enterprise所做的就是围绕云端代码及相关文档开放各种形式的AI对话，同时允许企业用户微调底层模型，以便Copilot能够更好地补全代码、并回答关于给定代码库提出的具体问题。</p><p>&nbsp;</p><p>GitHub产品管理副总裁Mario Rodriguez在采访中表示，“我们的最终目标就是提供一款对话式、无处不在、个性化且值得依赖的Copilot，这种种诉求就实际转化成了我们现在看到的Copilot Enterprise。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/2e/2e00b810eb4b7e25e51162b94b9c391e.png" /></p><p></p><p>在GitHub Copilot Enterprise中创建定制化模型。</p><p>&nbsp;</p><p>参与这项功能初始测试的，就有GitHub的合作伙伴、芯片巨头AMD公司。该公司表示微调之后的Copilot模型能够支持Verilog等硬件设计语言，这在标准Copilot版本中显然是无法实现的。</p><p>AMD公司软件开发高级总监Alexander Androncik在一份声明中指出，“定制化Copilot模型为众多AMD硬件工程师带来了AI辅助功能，可提供准确且质量卓越的AI建议，同时紧密契合我们的产品设计风格。”</p><p>&nbsp;</p><p>在相关新闻中，GitHub还透露将“在未来几个月内”推动Copilot Chat登陆GitHub移动应用，同时增加对JetBrain IDE套件的支持（当前仅支持VS Code与Visual Studio代码编辑器）。此举明显是在回应广大用户的需求和期盼——“你们既然要求了，我们当然会明确做出回应，”Dohmke表示。</p><p>&nbsp;</p><p></p><h2>进一步扩展Copilot</h2><p></p><p>&nbsp;</p><p>本届GitHub Universe大会上发布的另一份重量级公告，则是Copilot的合作伙伴计划。该计划将推动GitHub与更广泛的开发者社区建立合作，具体将以第三方开发工具厂商构建的插件形式出现，包括正在为Copilot打造集成方案的Daastax、LaunchDarkly、Postman、HashiCorp及Datadog等。</p><p>&nbsp;</p><p>Dohmke强调，“随着这一生态系统的不断扩大，GitHub Copilot能够为开发者分担的工作也将越来越多、用例愈加丰富。从协助提高数据库查询性能、到检查功能标记的状态，再到查看A/B测试结果——所有这一切、乃至更多应用场景将很快成为可能。这都要归功于那些正在为GitHub Copilot持续开发插件的合作伙伴们。”</p><p>&nbsp;</p><p>本次大会公布了包含25家合作厂商的首批名单，GitHub还在积极向更多希望参与进来的公司开放早期访问计划。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/36/360244574f5de55429cefc69c9ca81b8.png" /></p><p></p><p>GitHub Copilot合作伙伴计划：以Datastax为例。</p><p>&nbsp;</p><p>最后一条与Copilot相关的消息，就是GitHub初步介绍了所谓Copilot Workspace，据称它能以自然语言方式帮助开发者在短短几分钟内将设计灵感转化为可运行代码。开发人员首先在Copilot&nbsp;Workspace当中提出问题，之后AI会给出自动生成的计划，指导如何实现变更需求。当然，开发者也可以灵活编辑这些计划，通过“引导”让AI更好地理解问题、提供建议。这项功能预计将在2024年年内落地。</p><p>&nbsp;</p><p>Dohmke表示，“Copilot Workspace的使用感受，类似与合作伙伴进行结对编程。它了解项目中的方方面面，而且会跟随你的指引，依托AI的力量在代码仓库中完成问题回应和PR变更等各种用例。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/05/05e555977b007fd0d9093c819ad5f4e6.png" /></p><p></p><p>Copilot Workspace.</p><p></p><h2>安全保障，以及更多</h2><p></p><p>在安全方面，GitHub还对2020年首次内置在IDE中的功能进行了增强。其中包括secret扫描与代码扫描，向GitHub用户开放漏洞自动智能检测，并发现那些无意中被遗漏在公共代码中的secret（例如密码）。</p><p>&nbsp;</p><p>现在，GitHub还在添加新的AI元素，包括用于代码扫描的“autofix”自动修复功能，可帮助开发人员快速完成安全修正。AI能够根据PR中的CodeQL、JavaScript及TypeScript警报生成相应修复方案。</p><p>&nbsp;</p><p>GitHub产品管理副总裁Asha Chakrabarty在博文中提到，“这些新功能带来的可不只是修复意见，而是精确、可操作的操作指导，能帮助开发者快速了解漏洞情况和修复思路。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/4e/4e515ba4b341fbbfc560a31ba4ca5919.png" /></p><p></p><p>GitHub Copilot中的代码扫描autofix自动修复功能。</p><p>&nbsp;</p><p>开发人员可以通过单击将这些修复直接提交到代码当中，也可以先对修复方案进行编辑修改、之后再合并进代码库。</p><p>&nbsp;</p><p>Chakrabarty总结道，“这项功能的优点，在于它带来了无摩擦的修复体验。用户可以在编码的同时快速修复漏洞，这不仅缩短了修复耗时，而且实际准确性也完全能够达到用户的预期。”</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://github.blog/2023-11-08-universe-2023-copilot-transforms-github-into-the-ai-powered-developer-platform/">https://github.blog/2023-11-08-universe-2023-copilot-transforms-github-into-the-ai-powered-developer-platform/</a>"</p><p><a href="https://techcrunch.com/2023/11/08/github-teases-copilot-enterprise-plan-that-lets-companies-customize-for-their-codebase/">https://techcrunch.com/2023/11/08/github-teases-copilot-enterprise-plan-that-lets-companies-customize-for-their-codebase/</a>"</p><p><a href="https://twitter.com/ashtom/status/1722313836798320715">https://twitter.com/ashtom/status/1722313836798320715</a>"</p><p><a href="https://twitter.com/LinusEkenstam/status/1722320525454676063">https://twitter.com/LinusEkenstam/status/1722320525454676063</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/hsNPu3m4Kd0AsphlwCrV</id>
            <title>要务科技 CEO 石东海，确认担任 QCon 建设具备战略思维和弹性文化的组织专题出品人</title>
            <link>https://www.infoq.cn/article/hsNPu3m4Kd0AsphlwCrV</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/hsNPu3m4Kd0AsphlwCrV</guid>
            <pubDate></pubDate>
            <updated>Wed, 08 Nov 2023 08:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, 建设具备战略思维和弹性文化的组织, GenAI, 石东海
<br>
<br>
总结: QCon 全球软件开发大会将在上海举行，石东海将担任建设具备战略思维和弹性文化的组织的专题出品人。在此次专题中，将探讨团队和领导者需要采取的方法来改善组织，并讨论AI工具如何提高组织的协作弹性。石东海作为技术出身的管理者，将帮助提升专题质量，让学习者了解到领导者需要具备战略思维和弹性文化，以及技术团队需要具备高适应性。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1108&amp;utm_content=shidonghai">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。要务科技 CEO 石东海将担任「<a href="https://qcon.infoq.cn/2023/shanghai/track/1607?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1108&amp;utm_content=shidonghai">建设具备战略思维和弹性文化的组织</a>"」的专题出品人。在此次专题中，你将了解到团队和领导者需要采取什么样的方法，来改善组织。当然，也会提到 GenAI，探讨像提示库这样的 AI 工具如何提高组织的协作弹性。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/track/1607?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1108&amp;utm_content=shidonghai">石东海</a>"，北京邮电大学研究生毕业，曾任职于百度、Intel 等企业，原滴滴品质出行事业群 CTO，普惠出行事业群 CTO，出租车事业部总经理，代驾事业部总经理。现投身于工业互联网产业创业，要务科技 CEO。是技术出身转型为业务一号位的管理者，在技术架构，技术组织发展和业务管理和经营方面有丰富的经验。</p><p></p><p>相信石东海的到来，可以帮助提升此专题的质量，让你学习到组织的领导者需要具备战略思维和弹性文化，才能建立坚实的信任基础，以及有助于团队健康发展和提升效率的战术工具。以及，技术团队需要具备高适应性，才能够在需求变化、故障排除、颠覆性技术落地等情况下持续发展。</p><p></p><p>除上述专题外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1599?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的大前端技术</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！现在购票，享 7 折优惠，立减￥2040！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/sJzsW7aMIglaaKFa9EqX</id>
            <title>被时代选中的智谱AI：成为OpenAI，超越OpenAI</title>
            <link>https://www.infoq.cn/article/sJzsW7aMIglaaKFa9EqX</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/sJzsW7aMIglaaKFa9EqX</guid>
            <pubDate></pubDate>
            <updated>Wed, 08 Nov 2023 07:33:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 追赶 OpenAI, 智谱AI, 清华系, 大模型
<br>
<br>
总结: 智谱AI是一家由清华系创办的创业公司，致力于追赶OpenAI并投身于大模型的研发和应用。他们通过积淀和机遇的结合，将清华大学计算机系知识工程实验室的研究成果商业化，并在2020年决定全面投身大模型领域。与OpenAI相似但不同，智谱AI选择从底层算法原理入手，自主开发训练框架，并致力于构建一个拥有百亿参数的模型，以更低的成本支持多个上层任务。 </div>
                        <hr>
                    
                    <p>“追赶 OpenAI ”，是<a href="https://www.infoq.cn/article/MhabGNAVvf1NgAeZ2oIZ?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">智谱AI</a>" CEO 张鹏对外分享时屡次提到的一句话。坦然面对不如别人需要勇气，但公开承诺要追上行业标杆，则需要实力。那么，才成立四年的智谱AI 凭什么？</p><p></p><h2>积淀与机遇，一个也不能少</h2><p></p><p>&nbsp;</p><p>众所周知，智谱AI是清华系出身的学院派创业公司。</p><p>&nbsp;</p><p>1996年，清华大学计算机系知识工程实验室申请成立，这是人工智能下的一个分支，以机器学习、数据挖掘为主要研究方向。2006年，实验室开始做工程化，并推出了AMiner系统。在这之后的10年里，实验室一直进行工程方面的研究。2016年左右，随着相关技术的成熟，实验室开始进行应用转化。直至2019年，智谱AI成立。</p><p>&nbsp;</p><p>刚成立的智谱一方面延续之前的研究，一方面积极进入市场，将实验室积累的科技成果和产品系统用于实际项目并商业化。如果没有意外，这个路线会持续一段时间。但企业战略方向往往是由技术本身和行业应用领域的热点共同决定的。</p><p>&nbsp;</p><p>2020年成为智谱AI发展的一个关键拐点。</p><p>&nbsp;</p><p>GPT-3的发布给了大家非常明确的信号，即大型模型真正具备了实际可用性。但“要不要跟进大模型”却是一个问题。</p><p>&nbsp;</p><p>创业公司战略做错一次就是致命的，虽然此刻看来当时智谱AI的选择没错，表现之一就是风投态度：此后智谱AI每年都能拿到数亿融资，目前单2023年已累计<a href="https://www.infoq.cn/article/AXXtqD6xU6FghjsNE408?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">融资额达到25亿人民币</a>"。但当时情景下，这依然是一项极其冒险的事情，创始团队无法轻易决定。</p><p>&nbsp;</p><p>那如果跟进大模型呢？智谱AI也并非完全从零开始。创始团队多年积累，大模型可以看作是团队积极学习和扩充高速挖掘的延续。因此，在反复纠结和讨论后，智谱AI终于决定全面投身大模型。</p><p>&nbsp;</p><p>但在通用大模型和行业小模型的选择上，智谱AI虽然有参考 OpenAI，但还是决定坚持走通用大模型这条路。</p><p>&nbsp;</p><p>一是技术方面。张鹏认为，行业模型必须建立在通用模型的基础之上，否则独立发展的行业模型由于商业规模较小，其智能水平将受到明显的限制。此外，行业模型很容易被通用模型的能力快速超越。</p><p>&nbsp;</p><p>将行业模型建立在通用模型之上有好有坏。好处是可以节省基础模型预训练的成本和周期，享受到基础模型本身智能提升好处的同时，降低被通用模型取代的风险。坏处则是通用模型本身在行业场景中可能并不完美，因此需要专业知识积累。就像一个专业学校毕业的研究生要成为行业专家也需要时间来不断积累专业知识和经验。</p><p>&nbsp;</p><p>因此，在张鹏看来，行业模型被看作是在当前技术水平和时间点下为解决行业应用需求而催生的一种形态。虽然这种形态具有历史意义，但从更长远的角度看，它只是一个阶段性的产物。</p><p>&nbsp;</p><p>二是社会方面。模型之所以不能掌握行业专业知识，部分原因是因为行业知识的数据不完整或受到限制。这与过去十多年大数据和人工智能发展面临的问题类似，即存在数据孤岛和数据壁垒。这就导致了模型的能力必须迁就数据。</p><p>&nbsp;</p><p>这个问题的根源不是技术层面的决策，而是与当前社会发展、信息化水平、行业信息化程度、数据安全以及各种制度和机制有关的问题。</p><p>&nbsp;</p><p></p><h2>对标 OpenAI，相似但不同</h2><p></p><p>&nbsp;</p><p>同属通用模型赛道，是外界要拿智谱AI和OpenAI比，还是智谱AI自己要和OpenAI比？实际上，两者都有。国内需要有“自己的OpenAI”，而智谱AI的目标恰好也是OpenAI。</p><p>&nbsp;</p><p>“OpenAI公司一直在领跑，所以最直接的方式是先达到他们的水平。”张鹏说道。在技术选型和解决方案方面，智谱AI选择直接对标OpenAI：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ce/ce729e5998027254c52b49acd5718207.png" /></p><p>但智谱AI并没有完全依赖OpenAI的技术经验。</p><p>&nbsp;</p><p>GPT 的问题是注意力是单向的，无法充分捕捉 NLU 任务中上下文词之间的依赖关系。虽然在GLM模型的早期研发阶段，GPT-3已经非常出色，但智谱AI选择从底层算法原理入手，将自己的理解融入进去，最终需要通过实验和应用来验证。</p><p>&nbsp;</p><p>张鹏及其团队在2017年开始关注预训练模型，那时候大模型还没有出现，市面上主要是一些几千万数量级的、相对较小的模型。</p><p>&nbsp;</p><p>团队发现，当时的模型尽管架构相似，但在算法框架方面存在许多不同，比如encoder-decoder模型、auto-encoding自编码模型、auto-regressive自回归模型等。虽然前人尝试通过多任务学习结合它们的目标来统一不同的框架，但由于自编码和自回归目标在本质上的不同，简单的统一并不能充分继承两个框架的优势。</p><p>&nbsp;</p><p>2021年，智谱AI开始自主开发训练框架，着手训练一个拥有百亿参数的模型，并在年底启动了千亿模型的训练。智谱AI的GLM模型将自回归生成和自回归填空集成，即将 NLU 任务构建为包含任务描述的填空题，这些问题通过自回归生成来回答。通过将这两种模式的优点结合起来，模型在下游任务中能够完成更多任务。因此，这个预训练模型的显著特点是单一模型能够处理多个任务，从而用更低的成本来支持更多上层任务。</p><p><img src="https://static001.geekbang.org/infoq/6e/6e7df84ba92b6929f6ad6cf6535896d3.png" /></p><p></p><p>大模型主要被关注的是性能。这里的性能有两方面：一是各种评估指标上的表现，甚至是人工评估标准，二是推理效率和硬件基础成本。这两个方面的性能都非常重要，前者涉及到了模型的潜在极限水平，后者则涉及到了模型的可用性，即在产业链中使用该模型需要付出什么成本以及预期的回报是多少。</p><p>&nbsp;</p><p>对于GLM模型，智谱AI除了在解决精度、稳定性和效率上进行改进，包括算法层面的修改、算子和加速方法的选择，还有工程层面的决策，如商业集群和网络的选择以及性能优化。</p><p>&nbsp;</p><p>在早期某个阶段，模型训练的质量与数据之间存在密切的关系。为此，智谱AI也花费了一些时间和精力来获取更高质量的数据。</p><p>&nbsp;</p><p>智谱AI内部有一个专门的数据处理团队，进行数据清洗和过滤，将数据进行校准和转化等工作。智谱AI训练大模型的数据主要来自公开数据、团队多年来积累的数据、交换或采购合作伙伴数据。</p><p>&nbsp;</p><p>作为一个中英双语模型，GLM数据处理的复杂性略有增加。在模型训练中，文本需要分割成token，只有一种语言的话，token的数量是固定的，但如果涉及另一种语言，token的数量就会显著增加，整个扩展的词汇表会更大。另外，中英文混合数据的处理也是一个问题，模型需要在中英文上都表现良好，有效地跨语言工作。对此，智谱AI主要在设计训练算法以及损失函数的计算等方面做了些额外工作。</p><p>&nbsp;</p><p>对于“高质量的中文语料相对英文语料较少”的观点，张鹏并不赞同，“中文用户的数量全球最多，互联网用户也最多、活跃度也高，为什么中文数据的质量会有问题呢？”张鹏反问道。</p><p>&nbsp;</p><p>他认为，问题的根本在于数据的封闭和存在获取壁垒。可能有大量的中文用户在互联网上没有贡献高质量的内容，也可能是他们贡献了高质量的内容，但这些内容不是公开可获取的。</p><p>&nbsp;</p><p>智谱AI内部通常采用逐渐改进的方法，更倾向与自己之前的版本或标准版本进行比较，追求模型的性能，特别是某一方面上，能有明显提升。</p><p>&nbsp;</p><p>可以看到，智谱AI的产品更新频率很快。在今年3月首次推出ChatGLM基座模型后，智谱AI又在10月底将其<a href="https://www.infoq.cn/article/D5BW4LdBUGislXBCOFIZ?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">迭代到了第三代</a>"。</p><p>&nbsp;</p><p>ChatGLM3采用了智谱AI独创的多阶段增强预训练方法、集成了自研的 AgentTuning 技术，并瞄向GPT-4V做技术升级。此外，智谱AI还推出了可手机部署的端测模型ChatGLM3-1.5B和3B，支持包括Vivo、小米、三星在内的多种手机以及车载平台，甚至支持移动平台上 CPU 芯片的推理。</p><p><img src="https://static001.geekbang.org/infoq/74/74af9b30795d1619456291febad1c587.png" /></p><p></p><p>此外，对于神经网络算法的核心问题，业内在过去的六七年里一直在寻找更高效的技术架构来解决计算和智能水平问题。这是一个偏向理论和基础性研究的长期工作，智谱AI 更多通过投资或支持清华大学等基础性研究团队和机构，也会参与做前瞻性或预期性的研究工作，参与到这一命题的研发中。</p><p></p><h2>做大模型，没有好走的路</h2><p></p><p>&nbsp;</p><p>在2020年之前，智谱AI主要从事算法研究工作，研究是团队的强项，这部分工作相对容易。但到了2021年，情况有所不同。研究出身的创始成员在如何将研究成果落地上，开始遇到许多问题。</p><p>&nbsp;</p><p>具体来说，团队缺乏处理大规模数据和资源项目的经验，因此，许多事情实际上需要靠智谱AI自己摸索，一边学习一边实践。实际上也是如此，比如智谱AI训练GLM-130B时，整个研发和训练过程总共花费了8-9个月的时间，但最终稳定的训练其实只花费了不到2个月的时间，团队大部分精力都用在了适应性调整和系统调整上。</p><p>&nbsp;</p><p>不仅如此，早期的智谱AI 并没有现在的“吸金”能力，资源缺乏是其起步阶段不得不面对的问题。2021年，智谱AI 决定真正开发一个拥有130亿参数的大模型，这个项目的投资金额已经超过首年合同金额。</p><p>&nbsp;</p><p>如何解决资源困境？用张鹏的话就是到处“化缘”。团队与国家科研机构及超算中心等联系，获得支持、渡过难关。</p><p>&nbsp;</p><p>对内，智谱AI 一直注意在研发过程中合理分配和利用资源。尤其在初期，团队更加节约，租用计算资源后就以最短的时间完成工作，尽量让每一分钱花得物有所值。</p><p>&nbsp;</p><p>团队需要在不浪费资源的前提下，找到训练速度、精度和稳定性的最佳平衡点。这是一项复杂的工作：提高精度可能会使训练过程容易出问题，从而耗费额外时间和资源；反之，如果牺牲精度以保持稳定性，最终的结果可能不如预期。</p><p>&nbsp;</p><p>虽然当时缺乏可供参考的标准，但智谱AI根据一些开源项目和技术报告，设计了适合自己需求的解决方案，包括混合精度、流水线工作方式、加速方法等等。这种自定义的方法帮助智谱AI提高了资源利用率，也还需要一些时间来完善。</p><p>&nbsp;</p><p>众所周知，英伟达的GPU价格上涨，直接导致硬件成本增加。原本100万元的硬件如今需要花费1.5倍甚至1.6～1.7倍的价格来购买，大大提高了研发和应用成本。</p><p>&nbsp;</p><p>在解决硬件成本问题方面，智谱AI 选择用国产芯片替代，对模型做了各种国产GPU等硬件设备的适配。自2022年初，GLM 系列模型已支持在昇腾、神威超算、海光 DCU 架构上进行大规模预训练和推理。张鹏表示，国产芯片虽然在价格和性能方面可能距国外芯片有些距离，但在某些特定应用场景，尤其是在边缘计算等领域是可以满足需求的。</p><p>&nbsp;</p><p>通过高效动态推理和显存优化，智谱AI 表示，对比伯克利大学推出的 vLLM 以及 Hugging Face TGI 的最新版本，自己的推理速度提升了2-3倍，推理成本降低一倍，每千 tokens 仅0.5分。</p><p>&nbsp;</p><p>“一旦你经历过一次，积累了全面的经验，不管是遇到了问题还是进展顺利，你都会从中学到很多。你将不再是一张白纸，而是会根据以往的经验不断改进和完善。所以那个时候的困难主要在于缺乏经验，一旦积累了经验，后续的工作就会变得更容易。”张鹏总结道。</p><p>&nbsp;</p><p></p><h2>商业化？开源？</h2><p></p><p>&nbsp;</p><p>作为一家从研究机构出来的公司，智谱AI要比OpenAI更关注商业化。</p><p>&nbsp;</p><p>OpenAI总部位于美国硅谷，其科技创新生态系统和组织方式与国内有很大的不同。OpenAI更多是依赖资本支持积累大量资源，如微软等大公司提供资源、人才和数据，以快速实现目标。早期的OpenAI拥有足够的资源，因此并不太关心推理成本等问题。当然，OpenAI现在也开始关注加速和优化等方面的平衡问题，并且更多地依赖微软等公司来进行商业化。</p><p>&nbsp;</p><p>而智谱AI则是从成立之初便就在思考商业化的问题，“带着客户入场”也是被资本看好的因素之一。</p><p>&nbsp;</p><p>智谱AI的商业化路径主要面向企业和机构的B端用户。一方面，创始团队在B端的经验比较多。早期在学校的科技情报分析、数据挖掘等研究经历帮助智谱AI接触到了国内的科研机构、科技型企业、互联网企业，甚至一些国际顶尖科技企业，他们也成为智谱AI的首批客户来源。</p><p>&nbsp;</p><p>另一方面，向C端用户收费是比较有挑战的。智谱AI只为C端用户开发了一个免费使用的APP工具。</p><p>&nbsp;</p><p>不过在张鹏看来，无论是ToB还是ToC，两者最终都会融合，即服务企业最终也会影响到终端用户，因此两种选择本质上没有太大的区别，只是路径优先级的不同。</p><p>&nbsp;</p><p>在创业早期，智谱AI不会强迫自己去接复杂的客户需求，因为这些需求很可能让团队陷入其中无法自拔。“更复杂的问题需要暂时搁置、等到能力更成熟时再解决。”智谱AI会坦诚自己的能力在什么水平上，在该水平上可以创造什么样的价值。</p><p>&nbsp;</p><p>智谱AI也不会特别限定目标客户。张鹏表示，这一轮由大型模型引领的AI技术革新比上一代技术强大得多，具有更广泛的通用性，提供了巨大的创新空间，会影响到很多甚至之前意想不到的领域。</p><p>&nbsp;</p><p>张鹏举了一个民航的例子。民航飞行控制行业使用国际标准的数据报文来编制飞行信息，编码方式非常晦涩难懂，专业人士有时也难以理解。为了减少通信数据量和解决带宽等问题，业内通常会压缩数据，在实际使用时再将其还原。之前，企业需要庞大的团队手工编程将这些数据翻译成可读格式，非常繁琐。但将这些数据输入后让AI解释，AI能理解八九不离十。</p><p>&nbsp;</p><p>在IT行业，与商业对应的就是开源。Meta 无意打开了大模型开源的“潘多拉魔盒”，影响了很多大模型厂商对于“封闭还是开放”的选择。</p><p>&nbsp;</p><p>“我认为开源和商业化并不矛盾。事实上，已经有许多成功的开源和商业化项目，如Linux、Hadoop等，这些项目都表现出色，所以这两者并不互斥。”张鹏说道。</p><p>&nbsp;</p><p>目前，智谱AI已经开源了ChatGLM3-6B模型、多模态CogVLM-17B和智能体AgentLM等能力。开源对智谱AI来说主要有两个好处：一方面，开源社区主要依赖社区成员的共同努力和影响，项目开源后可以吸引更多的人使用，从而提高项目的质量和成熟度；另一方面，企业提供中文语境下的模型和技术，能在全球开源项目中发出中国声音，同时也能够学习和借鉴国外的先进技术和经验，这种跨文化的合作和知识共享有助于推动整个领域的发展。</p><p>&nbsp;</p><p>“在相当长的一段时间内，开源和商业化版本会并存，而且它们并不矛盾，而是相互促进、形成良性循环。”张鹏说道，“开源在保障生态多样性方面扮演着重要角色，而商业应用则关注稳定性、安全性和生态的持续性。只要能够建立良性循环，这种并存的格局将持续存在很长时间。”</p><p>&nbsp;</p><p>不过，虽然开源是免费的，但企业商业化还是需要一些成本的，资金能力不同的企业需要在成本和质量之间寻求自己的平衡。厂商则需要为不同预算范围的客户设计不同的解决方案和产品，并考虑不同的定价策略，从而使用户的成本降低。</p><p>&nbsp;</p><p></p><h2>“现在更需要商业化人才”</h2><p></p><p>&nbsp;</p><p>智谱AI和OpenAI的团队构成在某种程度上是相似的，OpenAI研究团队主要来自世界顶级大学，而智谱AI的团队主要来自清华大学。</p><p>&nbsp;</p><p>在智谱AI早期，团队构建比较简单。最初的团队起源于实验室，由一些老师、学生以及工程师组成。研究人员和科学家在实验室里带领学生一起工作，研发新技术。然后，工程师将这些技术转化为系统和应用程序，而少数商业人员与客户互动。初期，商业化工作也由工程师或研究人员来担任，他们在多个领域兼职担任不同的职责。</p><p>&nbsp;</p><p>智谱AI组织架构的发展是渐进式的：从内部研究开始，然后逐渐扩展到工程、系统平台、应用和商业化等领域，各部门之间不是独立的实体，而是相互协作、信息流畅的整体。这种紧密的团队协作方式减少了信息传递的损失，使团队能够更高效地应对快速变化的市场需求。</p><p>&nbsp;</p><p>现在，智谱AI已经有大约400名正式员工，其中约70%从事研发工作。</p><p>&nbsp;</p><p>管理方法上，智谱AI 与一般的互联网企业相似。每个人都有自己的日常任务，但当需要集中精力处理某些事情时，如客户交付、产品开发或技术研究，公司就会从各个团队中选择适合的人负责。</p><p>&nbsp;</p><p>团队的负责人在整个团队中发挥着管理和协调的关键作用，他们的职责包括确保各部门之间的高效协作。比如，在一个重要的商业化项目中，负责人的角色涵盖了项目从研究、开发到最终的市场推广的整个生命周期，这需要团队中的博士研究员、科学家、分级经理、工程师、系统专家和应用程序开发人员等人的共同协作。</p><p>&nbsp;</p><p>同样，在研究性项目中，负责人也需要协调不同层次和专业领域的团队成员，以确保项目的成功。无论是商业项目还是研究项目，都需要各方面的知识和专业技能的有机结合来解决复杂的问题和推动项目取得成功。</p><p>&nbsp;</p><p>随着公司的发展，智谱AI的团队构成也在随之变化。在早期，智谱AI要解决很多研究性问题，因此主要集中在研究团队。发展中期，团队增加了工程方面的人才，以优化模型的研发和训练，需要解决系统和应用相关的问题，并将应用推向市场。现在，智谱AI的团队更加需要商业方面的人才。</p><p>&nbsp;</p><p>“大规模模型的商业化是一个新兴领域，需要面对一些独特的挑战，尤其是在教育客户和应对客户的各种问题时。”张鹏说道。</p><p>&nbsp;</p><p>在张鹏看来，大模型时代的商业化人才需要具备强大的学习能力来快速掌握新技术和概念、需要有一定的技术敏感度、优秀的沟通能力和解决问题的能力，还要有具备市场洞察能力，以便制定有效的推广策略。</p><p>&nbsp;</p><p>对于当下智谱AI的主题是将大型模型产业化并落地应用。这一阶段要求更广泛的技能和角色，技术方面主要包括以下：</p><p>&nbsp;</p><p>数据分析师：整理、分析和处理大量数据，以确保数据的质量和有用性，以供模型的训练和应用。提示词工程师：这是一个新兴的角色，专注与大型模型进行高效沟通，以产生客户所需的数据和回应。这个角色可能不需要深入研究和训练模型，但需要懂得如何有效地使用模型。在特定领域或应用中的专家：能够为各种行业和领域提供个性化解决方案。</p><p>&nbsp;</p><p>“这个时代对IT行业来说既是幸运，也具有挑战。因为技术变化如此之快，你必须保持高效地不断了解和深入研究新技术。今天掌握的知识在短短一个月内可能就会变得过时。”张鹏说道，“持续学习是一项非常重要的任务。”</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>目前，大家对大模型技术的认识参差不齐，这也导致了落地上的一些问题。比如有的客户对这项技术不太了解，不清楚厂商在做什么，因此会根据他们的理解提出很多问题。而也有客户则认为他们非常了解这项技术，因此会期望过高，并设定更高的目标。实际上，大家需要在一个相对合理的范围内达成一致。这也是张鹏最近分享的原因之一。</p><p>&nbsp;</p><p>比尔盖茨曾说：“无论对谁来说，640K内存都足够了”。然而，现在随处可见大内存的手机。未来，对于任何人来说都很难预测。</p><p>&nbsp;</p><p>在张鹏看来，AIGC未来发展会很像云计算的轨迹，成为基础设施，而不是互联网生态下的应用。</p><p>&nbsp;</p><p>“在互联网应用中，有很多并行存在的应用，每个应用专注于特定场景。但基础设施领域的情况不同。基础设施的特点是随着规模的增加变得更加集中，资源的利用率越高、整体性能更高，产出投入比也更高。因此，基础设施需要规模效应，大型模型也具备这种特性。”张鹏解释道。</p><p>&nbsp;</p><p>但在当前的成本和回报条件下，基础的通用模型仍需要足够大的数据、足够低的成本、足够多的计算能力来进行训练。因此，未来可能会出现几家公司将通用模型的智能水平提升到一定程度，其他公司在此基础上做行业模型和应用的情况。</p><p>&nbsp;</p><p>谁能最终成为通用模型的“大家长”？这个问题还需要留给时间来回答。</p><p>&nbsp;</p><p>本文节选自<a href="https://www.infoq.cn/minibook/Ba7kqNscXOQoaZAduZsz">《中国卓越技术团队访谈录&amp;架构师特刊》</a>"</p><p>&nbsp;</p><p>大模型风行一年多，创业新秀们都有哪些故事？实际落地中，软件产品中的AIGC能力又如何？本期《中国卓越技术团队访谈录&amp;架构师特刊》中，LeptonAI、智谱AI、Dify.AI&nbsp;和京东云言犀团队深度分享了他们的创业思路和产品经验，来自网易、百度、广推科技等企业的技术专家，也深入探讨关于AIGC&nbsp;编程、算法及应用等话题。</p><p>&nbsp;</p><p>现在识别图中二维码或点击<a href="https://www.infoq.cn/minibook/Ba7kqNscXOQoaZAduZsz">《中国卓越技术团队访谈录&amp;架构师特刊》</a>"即可下载电子书，查看更多、更详细的精彩内容！</p><p></p><p><img src="https://static001.geekbang.org/infoq/19/19d003c1f43e5a20a7cf23de18f26a99.png" /></p><p></p><p>&nbsp;</p><p>另外，在今年 9 月份的 QCon 全球软件开发大会（北京站）中，张鹏曾作题为《ChatGLM：认知大模型与应用初探》主题演讲，完整幻灯片下载：<a href="https://qcon.infoq.cn/202309/beijing/presentation/5432">https://qcon.infoq.cn/202309/beijing/presentation/5432</a>"&nbsp;</p><p>&nbsp;</p><p>下一站 QCon 也将继续探索GenAI 和通用大模型应用探索、AI Agent 与行业融合应用的前景、面向人工智能时代的架构等方向。想要参加这场技术人的年终盛会？现在报名即可享受 7 折优惠，购票立减 ¥2040，详情可咨询票务经理 18514549229（微信同手机号）。12 月 28-29 日，上海·中优城市万豪酒店，期待见面！</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/tcB7Ql56Q1i0UOmgSywd</id>
            <title>烧钱、裁员、叫停业务，这家曾经的自动驾驶独角兽正经历至暗时刻</title>
            <link>https://www.infoq.cn/article/tcB7Ql56Q1i0UOmgSywd</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/tcB7Ql56Q1i0UOmgSywd</guid>
            <pubDate></pubDate>
            <updated>Wed, 08 Nov 2023 07:09:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 通用汽车, Cruise, 无人驾驶面包车, 停止生产
<br>
<br>
总结: 通用汽车旗下的自动驾驶汽车子公司Cruise宣布停止生产无人驾驶面包车Origin。这一决定是因为公司面临的问题和挑战，包括事故和监管问题。停产决定是为了重新评估和解决这些问题，恢复公众对公司的信任。 </div>
                        <hr>
                    
                    <p></p><h2>通用停止生产Cruise无人驾驶面包车</h2><p></p><p>11月6日，据路透社报道，通用汽车公司旗下自动驾驶汽车子公司 Cruise 宣布停止生产无人驾驶面包车 Origin。Cruise 首席执行官 Kyle Vogt 表示：" 由于很多事情都在变化之中，我们确实与通用汽车一起做出了暂停生产 Origin 的决定。"Vogt 补充道，公司已经生产了数百辆 Origin 汽车，短期内已经够用了。</p><p>&nbsp;</p><p>10月2日，一辆汽车在旧金山一处路口处撞倒了一名女子，她被冲击力抛向Cruise一辆无人驾驶出租车的面前。Cruise汽车碾过了她、短暂刹停了一会，之后又把她拖行了几米才最终停靠在路边。事件给受害者造成了严重伤害。</p><p>&nbsp;</p><p>10月24日，加州机动车辆管理局（California’s Department of Motor Vehicles）暂停了Cruise在加州运营无人驾驶汽车的资格，并指控该公司隐瞒了一起旧金山行人事故的关键视频。在被吊销执照几天后，Cruise主动暂停了整个车队的无人驾驶业务。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/6b/6b9006b8ac2b3fd6d4826cc4719fbc75.png" /></p><p></p><p>&nbsp;事故发生一周后，Cruise公司将其无人驾驶汽车从路面上撤下，集中停放在旧金山一处停车场内。</p><p>&nbsp;</p><p>加利福尼亚州机动车辆管理局（DMV）上周指责Cruise在最初提供给该局的事件视频中，刻意忽略了拖行该女子的片段。车管局称Cruise公司存在“歪曲”行为，并要求对方关闭在加州的无人驾驶汽车业务。</p><p>&nbsp;</p><p>两天之后，Cruise决定主动出击，直接暂停全美范围内的无人驾驶业务，约400辆汽车被撤下公共道路。此后，Cruise董事会聘请了Quinn Emanuel律师事务所来调查此事，包括指导其如何与监管机构、执法部门和媒体斡旋。</p><p>&nbsp;</p><p>Cruise董事会打算认真评估事务所给出的调查结果和行动建议。周一参加Cruise公司会议的两位消息人士表示，专门评估复杂软件系统的咨询公司Exponent也在对此次事故展开单独审查。</p><p>&nbsp;</p><p>五名前任及现任员工、连同多位业务合作伙伴都提到，Cruise员工担心公司目前面对的难题可能并没有简单的解决办法。而同行竞争对手更是担心Cruise惹出的“事端”可能导致整个产业都面临更严格的无人驾驶汽车监管要求。</p><p></p><h2>通用旗下Cruise发展历程：在无人驾驶竞赛中迷失自我</h2><p></p><p>&nbsp;</p><p>公司内部人士普遍将问题归咎于38岁的掌门人Vogt和他旗帜鲜明的“工业党”文化。这种文化将项目速度置于安全之上。在Cruise与最大无人驾驶竞争对手Waymo的竞争当中，Vogt一直希望能够全力推进、占据优势地位。在他看来，Uber与Lyft之间的对抗已经表明，谁能抢先一步、谁就能统治整个市场。</p><p>&nbsp;</p><p>纽约卡多佐法学院研究新兴汽车技术的Matthew Wansley教授认为，“Vogt是个愿意冒险的人，他倾向于迅速采取行动，身上有着典型的硅谷特色。这既解释了Cruise为何能取得成功，也解释了他们怎样一步步身陷困境。”</p><p>&nbsp;</p><p>据参加此次公司会议的两名员工透露，Vogt在周一宣布公司暂停运营时，承认自己清楚什么时候才能恢复运营、甚至有可能要考虑裁员。</p><p>&nbsp;</p><p>Vogt还承认Cruise已经在公众中失去了信任，并简单介绍了一项提高透明度、重视安全问题以恢复公众信任的计划。他任命安全副总裁Louise Zhang为公司临时首席安全官，并表示Zhang将直接向他汇报。据与会者回忆，Vogt表示“信任需要很长的时间才能慢慢建立、但崩溃往往就在瞬息之间。我们需要查清真相，并重新建立起信任。”</p><p>&nbsp;</p><p>Cruise公司拒绝了对Vogt的采访请求。通用汽车也在一份声明中指出，“仍然坚定支持Cruise的商业化目标”，对Cruise的使命和技术抱有信心，并支持其将安全放在首位。</p><p>&nbsp;</p><p>Cruise公司CEO Kyle Vogt表示，无人驾驶汽车比人类驾驶的车辆要安全得多。</p><p>&nbsp;</p><p>Vogt从十几岁起就开始研究自动驾驶汽车。13岁时，他对一辆Power Wheels骑乘玩具进行了编程，让它沿着停车场的黄线行驶。后来，他又在麻省理工学院学习期间参加了由政府资助的自动驾驶汽车比赛。</p><p>&nbsp;</p><p>2013年，他创办了Cruise Automation。这家公司对传统汽车进行改造，为其配备了传感器和计算机，能够在高速公路上实现自动行驶。三年之后，他以10亿美元的价格将Cruise卖给了通用汽车集团。</p><p>&nbsp;</p><p>交易完成之后，通用汽车总裁Dan Ammann接任Cruise公司CEO，Vogt则担任总裁兼首席技术官。</p><p>&nbsp;</p><p>前员工们表示，作为总裁，Vogt建立起Cruise工程团队，并将公司的规模从40人扩大至约2000人。他还主张尽快将产品推向更多市场，并相信公司的动作越快、就能拯救更多的生命。</p><p>&nbsp;</p><p>2021年，Vogt接任Cruise公司CEO。通用汽车CEO Mary T. Barra也开始让Vogt现身母公司的财报电话会议和展示活动。他在会上大肆宣传自动驾驶市场，并预测到2030年Cruise将覆盖100万辆汽车。</p><p>&nbsp;</p><p>Vogt不断敦促自己的公司继续扩张，从在旧金山公共道路上的行驶测试中总结经验。Cruise公司在旧金山的单次乘车平均收费为10.50美元。</p><p>&nbsp;</p><p>前员工提到，去年夏季一辆Cruise汽车与一辆在公交车道上行驶的丰田普锐斯相撞之后，公司里有人建议让车辆暂时避开设有公交车道的路线。但Vogt否定了这个想法，他说Cruise汽车就是要多接触这类道路才能应对其复杂性。该公司随后更改了软件，希望降低引发类似事故的风险。</p><p></p><p>到8月，一辆Cruise无人驾驶汽车又与一辆前往处理紧急情况的旧金山消防车相撞。之后，该公司改变了汽车对警报的检测方式。</p><p>&nbsp;</p><p>事故发生之后，市政官员和活动人士开始向加州政府高血压，要求其放慢无人驾驶车辆的扩张速度。旧金山监事会主席Aaron Peskin还呼吁Cruise提供更多碰撞相关数据，包括计划外停车、交通违规及车辆性能等记录。</p><p>&nbsp;</p><p>Peskin表示，“随着时间推移，Cruise的种种行为正不断消耗人们对它的信任。”</p><p>&nbsp;</p><p>如今，Waymo的估值从最高1750亿美元下跌到300亿美元。</p><p></p><h2>烧钱、裁员、业务停摆，自动驾驶走不出寒冬？</h2><p></p><p>&nbsp;</p><p>由于业务被冻结，人们担心Cruise会给通用汽车带来巨大的财务负担，甚至损害这家汽车巨头的声誉。因为就在加州车管局要求Cruise关闭其无人驾驶业务的几个小时前，通用掌门人Barra女士还告诉投资者，自己这家子公司拥有“光明的发展前景”。</p><p>&nbsp;</p><p>Cruise如今已有一个多星期没有收取车费、接送乘客了。在旧金山、菲尼克斯、达拉斯、休斯顿、迈阿密乃至得州奥斯汀，几百辆白橙相间的雪佛兰Bolt改造车在停车场中堆放静置。如今的尴尬局面，似乎也让Cruise在2025年达成10亿美元收入目标的豪言变成了一句玩笑。</p><p>&nbsp;</p><p>过去一年，通用汽车每个季度平均在Cruise身上烧掉5.88亿美元，较上年同期增长42%。据一位知情人士透露，Cruise运营的每辆雪佛兰Bolt的成本约为15万到20万美元。</p><p>&nbsp;</p><p>在无人驾驶业务被叫停时，Cruise旗下总计400辆无人驾驶汽车中有一半都部署在旧金山。这些车辆需要大量操作人员的支持，平均每车对应1.5位员工。两位知情人士透露，操作人员每隔2.5到3英里就会介入并协助指挥车辆。换句话说，在收到车辆疑似出现问题的移动信号之后，他们需要频繁采取措施来实施远程控制。</p><p>&nbsp;</p><p>Evercore ISI财务分析师Chris McNally表示，为了弥补不断上升的运营成本，通用汽车需要为Cruise业务注入或筹集更多资金。Barra女士也在10月底的分析师电话会议上宣布，通用汽车将在年底之前公开新的融资计划。</p><p>&nbsp;</p><p>Cruise的窘境只是整个自动驾驶行业寒冬的一个缩影。虽然以ChatGPT为代表的大语言模型在各行业逐渐落地，但这一AI大趋势似乎并没有给原本就遇冷的自动驾驶产业带来一丝暖阳。</p><p>&nbsp;</p><p>最近两年，裁员、倒闭、市值暴跌成为了自动驾驶行业的三个“热词”。</p><p>&nbsp;</p><p>英特尔旗下的自动驾驶公司Mobileye在去年将估值一减再减，从最初的500亿美元缩水到300亿美元，最后只以160亿美元的身价上市；谷歌和软银都有所投资的自动驾驶机器人Nuro在不到半年的时间内连续进行了占总员工20%和30%的裁员。</p><p>&nbsp;</p><p>就在上个月，Waymo启动了今年的第三次裁员。Waymo 发言人接受媒体采访时表示，此次裁员是内部重组过程的一部分。</p><p>&nbsp;</p><p>“作为正常业务过程的一部分，少数 Waymo 团队最近对其团队进行了调整，”该发言人拒绝提供受影响员工人数的详细信息，但表示人数很少。</p><p>&nbsp;</p><p>早在今年年初，作为Alphabet集团大规模裁员的一部分，Waymo就已经解雇了数十名员工，3月份，该公司又解雇了一批员工。</p><p>&nbsp;</p><p>据报道，Waymo 今年年初雇用了约 2500 名员工。今年早些时候的两轮裁员中，有超过200人被裁，但此次裁员后剩余的 Waymo 员工人数尚不清楚。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.nytimes.com/2023/11/03/technology/cruise-general-motors-self-driving-cars.html">https://www.nytimes.com/2023/11/03/technology/cruise-general-motors-self-driving-cars.html</a>"</p><p><a href="https://sfstandard.com/2023/10/17/tech-layoffs-waymo-san-francisco-robotaxi/">https://sfstandard.com/2023/10/17/tech-layoffs-waymo-san-francisco-robotaxi/</a>"</p><p><a href="https://theintercept.com/2023/11/06/cruise-self-driving-cars-children/">https://theintercept.com/2023/11/06/cruise-self-driving-cars-children/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/B0JNSlJPFxY5eebdIOZE</id>
            <title>Generative AI 新世界：过去、现在和未来</title>
            <link>https://www.infoq.cn/article/B0JNSlJPFxY5eebdIOZE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/B0JNSlJPFxY5eebdIOZE</guid>
            <pubDate></pubDate>
            <updated>Wed, 08 Nov 2023 06:34:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 机器, 分析人工智能, 生成式人工智能, Transformer
<br>
<br>
总结: 机器不仅可以在分析任务上超越人类，还开始在创造性领域尝试超越人类，这一新领域被称为生成式人工智能。生成式人工智能的发展将重塑各行各业，可能取代某些人类创作工作，或激发出超越人类想象力的全新灵感。Transformer模型是生成式人工智能的重要知识底座，它提高了并行计算效率，引入了注意力机制，使人工智能能够理解单词之间的关系。注意力机制是一种模仿认知注意力的技术，可以增强神经网络对数据中重要部分的关注。 </div>
                        <hr>
                    
                    <p>人类善于分析事物。但是现在看来，机器很有可能做得更好。机器可以不知疲倦夜以继日地分析数据，不断从中找到很多人类场景用例的模式：信用卡欺诈预警、垃圾邮件检测，股票价格预测、以及个性化地推荐商品和视频等等。他们在这些任务上变得越来越聪明了。这被称为 “分析人工智能（Analytical AI）” 或”传统人工智能（Traditional AI）”。</p><p></p><p>但是人类不仅擅长分析事物，还善于创造。我们写诗、设计产品、制作游戏和编写代码。直到公元 2022 年之前，机器还没有机会在创造性工作中与人类竞争，它们只能从事分析和死记硬背的认知劳动。但是现在（是的，就是现在）机器已经开始在创造感性而美好事物的领域尝试超越人类，这个新类别被称为 “生成式人工智能（Generative AI）”。这意味着机器已经开始在创造生成全新的事物，而不是分析已经存在的旧事物。</p><p></p><p>生成式人工智能不仅会变得更快、更便宜，而且在某些情况下比人类手工创造的更好。每个需要人类创作原创作品的行业—从社交媒体到游戏、从广告到建筑、从编码到平面设计、从产品设计到法律、从营销到销售都有待全新重塑。某些功能可能会被生成式人工智能完全取代，或者激发出超越人类想象力的全新灵感。</p><p></p><p></p><blockquote><a href="https://xie.infoq.cn/link?target=https%3A%2F%2Fdev.amazoncloud.cn%2F%3Fsc_medium%3Dregulartraffic%26sc_campaign%3Dcrossplatform%26sc_channel%3DInfoQ">亚马逊云科技开发者社区</a>"为开发者们提供全球的开发技术资源。这里有技术文档、开发案例、技术专栏、培训视频、活动与竞赛等。帮助中国开发者对接世界最前沿技术，观点，和项目，并将中国优秀开发者或技术推荐给全球云社区。如果你还没有关注/收藏，看到这里请一定不要匆匆划过，<a href="https://xie.infoq.cn/link?target=https%3A%2F%2Fdev.amazoncloud.cn%2Fuser%2Fregister%3Fshow%3Dtab1%26from%3Dindex%26sc_medium%3Dregulartraffic%26sc_campaign%3Dcrossplatform%26sc_channel%3DInfoQ">点这里</a>"让它成为你的技术宝库！</blockquote><p></p><p></p><p>新世界正在到来。</p><p></p><h1>Transformer 新世界</h1><p></p><p></p><p>做为一名曾经多次穿越过市场周期的从业者，我亲历过通信行业、IT 行业、移动互联网行业等不同时代的周期，亲身体验过其间的潮起云涌，亲眼目睹过其中的天高云淡，以及最终惨烈竞争后的回归平淡。因此，面对已经开启的 AI 时代周期，与其盲目地跳进去跟随，不如先搞清楚这个新周期的一些底层逻辑，比如说：知识底座。</p><p></p><p>如果说 TCP/IP、HTML 等知识结构是上一个时代的知识底座，那么面对已经开始的 AI 时代，我们每个人是否应该先问自己一个问题：“什么是 AI 时代的知识底座？”</p><p></p><p>从到目前为止 AI 的知识发展看来，也许这个知识底座会是：Transformer。</p><p></p><h2>1 Transformer 概述</h2><p></p><p></p><p>欢迎进入 Transformer 的新世界。</p><p></p><p>在过去的五年中，人工智能世界发生了很多令人欣喜的重大变化。其中许多变化是由一篇名为 “Attention is All You Need” 的论文推动的。这篇发表于 2017 年的论文介绍了一种名为 “Transformer” 的新架构。下图为“Attention is All You Need” 的论文中描述的 Transformer 模型的架构图示。</p><p><img src="https://static001.geekbang.org/infoq/64/646dd7f43d6f56a389c3d3d4011d39cd.png" /></p><p>Source:&nbsp;<a href="https://xie.infoq.cn/link?target=https%3A%2F%2Farxiv.org%2Fpdf%2F1706.03762.pdf%3Ftrk%3Dcndc-detail">https://arxiv.org/pdf/1706.03762.pdf?trk=cndc-detail</a>"</p><p></p><p>概括来说，Transformer 模型为机器学习领域做出了两项贡献。首先，它提高了在人工智能中使用并行计算的效率。其次，它引入了 “注意力（Attention）” 的概念，这使人工智能能够理解单词之间的关系。你所听到的技术，例如 GPT-3、BERT、Sable Diffusion 等，都是 Transformer 架构在不同领域演进的结果。</p><p></p><h2>2 注意力机制（Attention）</h2><p></p><p></p><p>什么是注意力机制？根据该论文中的描述，注意力函数可以描述为将查询和一组键值对映射到输出，其中查询、键、值和输出都是向量。输出是按值的加权总和计算的，其中分配给每个值的权重由查询的兼容性函数与相应键值计算得出。Transformer 使用多头注意力（multi-headed attention），这是对称为缩放点积注意力（scaled dot-product attention）的特定注意力函数的并行计算。如下图所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a2e65bdfe28920f08cf204d601fdcced.png" /></p><p>&nbsp;Source:&nbsp;<a href="https://xie.infoq.cn/link?target=https%3A%2F%2Farxiv.org%2Fpdf%2F1706.03762.pdf%3Ftrk%3Dcndc-detail">https://arxiv.org/pdf/1706.03762.pdf?trk=cndc-detail</a>"</p><p></p><p>上面这段对“注意力机制”的描述还是偏学术化。维基百科上的定义会更通俗易懂些：“注意力机制（英语：attention）是人工神经网络中一种模仿认知注意力的技术。这种机制可以增强神经网络输入数据中某些部分的权重，同时减弱其他部分的权重，以此将网络的关注点聚焦于数据中最重要的一小部分。数据中哪些部分比其他部分更重要取决于上下文。可以通过梯度下降法对注意力机制进行训练 ……”</p><p>可见，注意力机制的灵活性来自于它的“软权重”特性，即这种权重是可以在运行时改变的，而非像通常的权重一样必须在运行时保持固定。</p><p></p><h2>3 Transformer in Chip</h2><p></p><p></p><p>很多人工智能领域的思想领袖和专家，认为 Transformer 架构在未来五年左右并不会有太大变化。这就是为什么你会看到一些芯片制造商在其新芯片（例如 NVIDIA H100）中集成 Transformer Engine 的原因。</p><p></p><p>在 2022 年拉斯维加斯的 re:Invent 2022 中，来自 NVIDIA 的架构师分享了如何在亚马逊云科技上，使用 NVIDIA 新一代芯片做深度学习训练的专题，里面特别提到了 H100 芯片中 Transformer Engine 的设计结构和初衷。对技术架构细节感兴趣的同学，可以通过以下视频深入了解：</p><p></p><p><a href="https://xie.infoq.cn/link?target=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dl8AFfaCkp0E%3Ftrk%3Dcndc-detail">https://www.youtube.com/watch?v=l8AFfaCkp0E?trk=cndc-detail</a>"</p><p><img src="https://static001.geekbang.org/infoq/c4/c4c3c7c5c1c7c0e53154b6ff5698088b.png" /></p><p></p><p></p><h2>4 Transformer 演进时间线</h2><p></p><p></p><p>一个有趣的视角是将各种 Transformer 按照出现的时间顺序排列的图示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e9/e9cdf6f817052122267bd8a610257a40.png" /></p><p>Source: “Transformer models: an introduction and catalog”&nbsp;<a href="https://xie.infoq.cn/link?target=https%3A%2F%2Farxiv.org%2Fabs%2F2302.07730%3Ftrk%3Dcndc-detail">https://arxiv.org/abs/2302.07730?trk=cndc-detail</a>"</p><p></p><p>我听到过的一个比较有趣的视角是：如果您之前对 Transformer 知道得不多，不要恐慌。因为您看到引领这一波生成式人工智能（Generative AI）变革的重要几篇论文的情况：</p><p></p><p>CLIP 论文在 2021 年发表；Stable Diffusion 和 DALL-E-2 在 2022 年才出现；GPT3.5、ChatGPT、Bloom 等在 2022 年底才出现……</p><p>这个新世界的演进才刚刚开始，你还有足够的时间重新开始学习 Transformer！</p><p></p><h1>Generative AI</h1><p></p><p></p><h2>1 为什么现在发生?</h2><p></p><p></p><p>Generative AI 与更广泛的人工智能具有相同的值得人类深入思考问题：“为什么现在发生？” 概括来说，这个答案是我们当下具有：</p><p>更好的模型；更多的数据；更多的计算；</p><p></p><p>Generative AI 的进化速度比我们所能想象的要快得多，为了将当前时刻置于大时代洪流的背景之下，非常值得我们大致地了解下 AI 的发展历史和曾经走过的路。</p><p></p><p>第一波浪潮：小型模型占据了至高无上的地位（2015 年之前）</p><p></p><p>小型模型在理解语言方面被认为是 “最先进的”。这些小型模型擅长分析任务，可用于从交货时间预测到欺诈分类等工作。但是，对于一般用途的生成任务，它们的表现力还不够。生成人类级写作或代码仍然是白日梦。</p><p></p><p>第二波浪潮：规模竞赛（2015 年至今）</p><p></p><p>2017 年发表的里程碑意义的论文（“Attention is All You Need”）描述了一种用于自然语言理解的新神经网络架构，这种架构名为 Transformer，它可以生成高质量的语言模型，同时更具可并行性，并且需要更少的训练时间。这些模型是 few-shot learners 的，因此可以相对容易地针对特定领域进行定制。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e1/e13976c5aa69f65f9b401d3a9156e8b0.png" /></p><p>Source:<a href="https://xie.infoq.cn/link?target=https%3A%2F%2Fwww.science.org%2Fcontent%2Farticle%2Fcomputers-ace-iq-tests-still-make-dumb-mistakes-can-different-tests-help%3Ftrk%3Dcndc-detail">https://www.science.org/content/article/computers-ace-iq-tests-still-make-dumb-mistakes-can-different-tests-help?trk=cndc-detail</a>"</p><p></p><p>随着模型越来越大，它们开始提供人类层面的结果，然后是超人的结果。在 2015 - 2020 年间，用于训练这些模型的计算增加了 6 个数量级，其结果超过了人类在手写、语音和图像识别、阅读理解和语言理解方面的性能基准。GPT-3 模型在这时脱颖而出，该模型的性能比 GPT-2 有了巨大的飞跃，内容涉及从代码生成到写作等多项任务。</p><p>尽管基础研究取得了种种进展，但这些模型并不被人广泛使用。原因是它们庞大且难以运行（需要 GPU 编排等），能够使用这些模型的门槛太高（不可用或仅限封闭 BETA），而且用作云服务的成本也很高。尽管存在这些局限性，但最早的 Generative AI 应用程序开始进入竞争阶段。</p><p></p><p>第三波浪潮：更好、更快、更便宜（2022 年以后）</p><p></p><p>由于像亚马逊云科技这样的云技术公司，一直在推动云计算的普及，计算变得更加便宜。而像 diffusion model 等新技术降低了训练和运行推理所需的成本，研究界因此可以继续开发更好的算法和更大的模型。开发者访问权限从封闭 BETA 扩展到开放 BETA，或者在某些情况下扩展到开源（open-source）。对于一直缺乏 LLM 访问权限的开发人员来说，现在闸门已开放，可供探索和应用程序开发。应用程序开始蓬勃发展。</p><p></p><p>第四波浪潮：杀手级应用程序的出现（现在）</p><p></p><p>随着基础平台层的逐渐巩固，模型不断变得更好/更快/更便宜，模型访问趋向于免费和开源，应用层的创造力爆炸的时机已经成熟。</p><p></p><p>正如十年前的移动互联网爆发的前夜，由于移动通过 GPS、摄像头和移动连接等新场景、新功能释放了新类型的应用程序一样，我们预计这些大型模型将激发新一轮的 Generative AI 应用。我们预计 Generative AI 也将出现杀手级应用程序。</p><p></p><p>Source:<a href="https://xie.infoq.cn/link?target=https%3A%2F%2Fwww.sequoiacap.com%2Farticle%2Fgenerative-ai-a-creative-new-world%2F%3Ftrk%3Dcndc-detail">https://www.sequoiacap.com/article/generative-ai-a-creative-new-world/?trk=cndc-detail</a>"</p><p></p><h2>2 Generative AI: 应用层蓝图构想</h2><p></p><p></p><p>以下是 Generative AI 的应用格局图，描述了为每个类别提供支持的平台层以及将在上面构建的潜在应用程序类型。</p><p><img src="https://static001.geekbang.org/infoq/3d/3d38e1857cdb3133cc7dc57410d810e1.png" /></p><p>Source:<a href="https://xie.infoq.cn/link?target=https%3A%2F%2Fwww.sequoiacap.com%2Farticle%2Fgenerative-ai-a-creative-new-world%2F%3Ftrk%3Dcndc-detail">https://www.sequoiacap.com/article/generative-ai-a-creative-new-world/?trk=cndc-detail</a>"</p><p></p><p>文本是进展最快的领域。</p><p></p><p>代码生成可能会在短期内对开发人员的生产力产生重大影响，如 <a href="https://www.infoq.cn/video/4oajrgIyfmkaaNFi7dJF?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Amazon CodeWhisperer</a>" 所示。</p><p>图像是一种较新的现象。我们看到了不同风格的图像模型的出现，以及用于编辑和修改生成的图像的不同技术。</p><p></p><p>语音合成已经存在了一段时间（例如，你好 Siri！）。就像图像一样，今天的模型也为进一步完善提供了起点。</p><p></p><p>视频和三维模型正在迅速上线。人们对这些模式开启电影、游戏、虚拟现实和实体产品设计等大型创意市场的潜力感到兴奋。</p><p></p><p>其他领域：从音频和音乐到生物学和化学，许多领域都在进行基础模型研发。</p><p></p><p>下图说明了我们如何期望基本模型取得进展以及相关应用成为可能的时间表。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f8/f88ee1a43e3da7420420b13f7d3d63f5.png" /></p><p>Source:<a href="https://xie.infoq.cn/link?target=https%3A%2F%2Fwww.sequoiacap.com%2Farticle%2Fgenerative-ai-a-creative-new-world%2F%3Ftrk%3Dcndc-detail">https://www.sequoiacap.com/article/generative-ai-a-creative-new-world/?trk=cndc-detail</a>"</p><p></p><h2>3 Generative AI: 文字生成图像（Text-to-Image）方向</h2><p></p><p></p><p>回顾过去的一年，有两个 AIGC 方向已经发生了让人惊艳的进步。其中一个方向就是：文字生成图像（Text-to-Image）方向。</p><p></p><p>根据来自亚马逊云科技的官方博客，用户现在可以很方便的在 SageMaker JumpStart 中使用 Stable Diffusion 模型，轻松地生成富有想象力的绘画作品。</p><p></p><p>The following images are in response to the inputs “a photo of an astronaut riding a horse on mars,” “a painting of new york city in impressionist style,” and “dog in a suit.”</p><p></p><p><img src="https://static001.geekbang.org/infoq/53/53dfcc043d338fb96f5d4b625e7df2ef.jpeg?x-oss-process=image%2Fresize%2Cp_80%2Fauto-orient%2C1" /></p><p></p><p>The following images are in response to the inputs: (i) dogs playing poker, (ii) A colorful photo of a castle in the middle of a forest with trees, and (iii) A colorful photo of a castle in the middle of a forest with trees. Negative prompt: Yellow color.</p><p></p><p><img src="https://static001.geekbang.org/infoq/04/04f7213f08a541711f019115c0cf0e60.jpeg?x-oss-process=image%2Fresize%2Cp_80%2Fauto-orient%2C1" /></p><p>Source:<a href="https://xie.infoq.cn/link?target=https%3A%2F%2Faws.amazon.com%2Fcn%2Fblogs%2Fmachine-learning%2Fgenerate-images-from-text-with-the-stable-diffusion-model-on-amazon-sagemaker-jumpstart%2F%3Ftrk%3Dcndc-detail">https://aws.amazon.com/cn/blogs/machine-learning/generate-images-from-text-with-the-stable-diffusion-model-on-amazon-sagemaker-jumpstart/?trk=cndc-detail</a>"</p><p></p><p>关于文字生成图像（Text-to-Image）方向的论文解读、示例代码等我们还会有其他专题深入讨论。</p><p>以上就是关于 Transformer 和 Generative AI 的部分介绍。在下一篇文章中，我们将详细讨论关于 Generative AI 另一个重要的进步方向就是：文字生成（Text Generation）方向。分享这个领域的最新进展，以及亚马逊云科技在为支持这些大型语言模型（LLMs）的编译优化、分布式训练等方面的进展和贡献。</p><p></p><p>作者黄浩文</p><p></p><p>亚马逊云科技资深开发者布道师，专注于 AI/ML、Data Science 等。拥有 20 多年电信、移动互联网以及云计算等行业架构设计、技术及创业管理等丰富经验，曾就职于 Microsoft、Sun Microsystems、中国电信等企业，专注为游戏、电商、媒体和广告等企业客户提供 AI/ML、数据分析和企业数字化转型等解决方案咨询服务。</p><p></p><p>文章来源：<a href="https://xie.infoq.cn/link?target=https%3A%2F%2Fdev.amazoncloud.cn%2Fcolumn%2Farticle%2F6413095e3d950b57b3f9f63d%3Fsc_medium%3Dregulartraffic%26amp%3Bsc_campaign%3Dcrossplatform%26amp%3Bsc_channel%3DInfoQ">https://dev.amazoncloud.cn/column/article/6413095e3d950b57b3f9f63d?sc_medium=regulartraffic&amp;sc_campaign=crossplatform&amp;sc_channel=InfoQ</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/SiIDsk9dX3yQJ3pCO1A8</id>
            <title>零一万物李开复：要做ToC的超级应用，成为AI 2.0时代的微信、抖音</title>
            <link>https://www.infoq.cn/article/SiIDsk9dX3yQJ3pCO1A8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/SiIDsk9dX3yQJ3pCO1A8</guid>
            <pubDate></pubDate>
            <updated>Wed, 08 Nov 2023 06:31:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 零一万物, 大模型, 开源, 李开复
<br>
<br>
总结: 李开复带领的AI 2.0公司零一万物在4个月内发布了首款预训练大模型Yi-34B和Yi-6B，其中Yi-34B是一个双语基础模型，经过340亿个参数训练。零一万物团队认为34B是一个黄金尺寸，既满足了精度要求，又具备高效率的推理成本。李开复表示，零一万物在中英文上是最好的底座，希望更多人选择Yi-34B。零一万物还注重算力和数据问题的解决，通过建立AI基础设施团队和优化算法和模型，成功降低了Yi-34B的训练成本。零一万物还开源了长窗口的base模型，推动了世界技术革命的发展。 </div>
                        <hr>
                    
                    <p>“我们在3月底官宣零一万物，后面团队逐渐到位，6、7月开始写下第一行代码，历时短短4个月时间，今天我们非常自豪地宣布产品亮相。”李开复在另一万物首款大模型发布会上说道。“从创立零一万物第一天开始，我的目标就是做一个世界级公司，能够进入世界的第一梯队。”</p><p>&nbsp;</p><p>自四个月前李开复宣布大模型创业，业内就给予了众多关注。千呼万唤，李开复交出了第一份答卷。11月6日，李开复带队创办的AI 2.0公司零一万物正式开源发布首款预训练大模型Yi-34B和Yi-6B。Yi-34B是一个双语（英语和中文）基础模型，经过340亿个参数训练，明显小于Falcon-180B和Meta LlaMa2-70B等其他开放模型。</p><p>&nbsp;</p><p>更多详情查看：</p><p><a href="https://www.infoq.cn/news/3m7F87QpDVsu8zv68k1b">李开复4个多月后“放大招”：对标OpenAI、谷歌，发布“全球最强”开源大模型</a>"</p><p>&nbsp;</p><p>对于模型尺寸的选择，零一万物团队认为，34B是一个黄金尺寸。虽然6B也能在某些领域，比如客服上可用，但模型毕竟越大越好，但随之而来的就是推理成本和后续训练的系列资源问题。</p><p>&nbsp;</p><p>“34B不会小到没有涌现或者涌现不够，完全达到了涌现的门槛。同时它又没有太大，还是允许高效率地单卡推理，而且不一定需要H和A级别的卡，只要内存足够，4090或3090都是可以使用的。”李开复解释道，“既满足了精度的要求，训练推理成本友好，达到涌现的门槛，是属于非常多的商业应用都可以做的。”</p><p>&nbsp;</p><p>另外，李开复提到，通用模型决定了行业模型的天花板。虽然行业大模型有相当大的价值，但是底座如果不好，也无法完成超过底座的事情，所以选底座就要选表现最好的底座。李开复自信地表示，“今天我们在中英文上就是最好的底座，没有之一，也希望更多人选择Yi-34B。”</p><p></p><h3>如何解决算力和数据问题</h3><p></p><p>&nbsp;</p><p>“模型团队非常重要，但并不是雇50个人、100人就能解决问题，而是需要很强的团队。这通常不是很大的团队，团队做得太大了反而会分散GPU资源。”李开复说道。零一万物认为，人均GPU卡能用到多少决定了模型能力的上线。</p><p>&nbsp;</p><p>零一万物内部建立了一个AI Infrastructure（人工智能基础设施技术，简称“Infra”）的团队，成员来自国内顶级公司、国内外顶级高校和跨国公司，负责大模型的研发。</p><p>&nbsp;</p><p>在预训练阶段，高价值数据是最重要的，为此零一万物在数据处理上投入了非常大的精力。</p><p>&nbsp;</p><p>首先，零一万物通过采购、合法爬虫、开源等渠道获得训练模型数据。面对庞杂、质量不齐的数据，团队会先用AI能力进行系统化筛选，之后再做人工评估，基本会从一百多T数据里留下3T左右，包括一定比例的中英文数据，该数据保留率是其他厂家的1/10左右。</p><p>&nbsp;</p><p>在训练中，Infra团队花了很长时间研究scaling law，即模型的预测能力。“我们不做各种试错，因为GPU资源非常昂贵，所以我们是要把规模化做好，当推到下一个尺寸时不要再摸索和试错了，因为尺寸越大成本越高。“李开复介绍道。</p><p>&nbsp;</p><p>Infra团队表示，整个模型训练过程其实是动力学过程，中间每一步基本上都可以通过数学方式预测出来，而不需要做大量的实验。因此，团队可以将每一千步的误差控制在千分之几范围内。不管是做数据匹配、超参搜索，还是模型结构的试验，这个方法都特别重要。</p><p>&nbsp;</p><p>Infra团队在6B上做各种实验优化算法和模型，并能丝滑地从6B推向34B。借助该能力，Yi-34B的训练成本下降了40%。</p><p>&nbsp;</p><p>“我们将这一整套的训练平台称为科学训模。很多人把训练大模型比做‘炼丹’，也有人说模型训练一下就飞了，因为它没有收敛。我们做的规模预测用数学科学可以推理，小的尺寸如果能成功，大的尺寸也大概率可以成功，我们实验后也成功了。”李开复表示。</p><p>&nbsp;</p><p>关于算力资源，零一万物在很早时候就做了资源规划，现在的算力储备可以支持其用到18个月以后。另外，团队还建立了故障预测与故障解决大模型，利用模型本身为预训练过程中可能出现的问题设计相应的解决方案，以及如何以最低成本解决这个问题。</p><p>&nbsp;</p><p>对于预训练，零一万物技术副总裁及Pretrain负责人黄文灏表示，过程中并没有特别关注指标，因为针对指标做优化也可能出现问题，所以内部会有很多衡量模型能力的方法。比如模型到底压缩了哪些信息和知识是一个值得关注指标，但只要训练数据足够高质量，training dynamics做得足够好，出来的模型效果自然会比较好。</p><p>&nbsp;</p><p>另外，由于要将模型开源，零一万物在训练模型时还注重模型在IQ和EQ方面的均衡性。团队想要模型既可以支持代码推理类任务，也可以支持情感类任务。</p><p></p><h4>开源长窗口通用模型</h4><p></p><p>&nbsp;</p><p>之前的长窗口工作都是闭源的，无论是OpenAI的32K或者Cloud的100K。零一万物发现，开发者有大量基于长窗口模型进行微调的需求，因此这次直接开源了长窗口的base模型，开发者可以根据自己的数据去微调有效的长窗口应用。</p><p>&nbsp;</p><p>一般来说，更长的窗口会带来更多的计算，计算复杂度也会指数级上升，还要解决数据完备度的问题，这些都对计算、显存、内存和通信等都是非常大的技术挑战。另外，随着窗口越来越长，计算所需时间也越来越长，一旦端到端的反馈时间太长也就没有太大的意义了。因此，大部分模型都会限定窗口大小，零一万物限定了在200K以下。</p><p>&nbsp;</p><p>技术团队进行了全栈优化，包括计算跟通信的重叠堆叠技术、序列并行的技术、通信压缩技术，包括里面关键算子的重构等。虽然后续还有进一步拓宽的余地，但考虑到实用性和成本的均衡，团队目前就开源出来现在的长度版本。</p><p>&nbsp;</p><p>李开复表示，开源对推动世界技术革命的发展有着非常重要的意义。“很多人觉得大模型需要超级多的资源，只有OpenAI、微软、谷歌、阿里、百度、腾讯这样的公司才能做，但是任何技术都是需要全球化的参与，那么开源让大家都有机会能够接触到大模型。”</p><p>&nbsp;</p><p>“这两个模型的尺寸其实就是量身定做给开源社区使用的，资源多的可以用34B，但是也不会需要特别不合理的资源，而6B可以让更多的开发者能够使用。”李开复称。</p><p>&nbsp;</p><p>对于未来会不会开源更大模型的问题，零一万物技术副总裁及AI Infra负责人戴宗宏表示，这不取决于零一万物有没有更大的模型，而是取决于开源社区里的普通开发者有没有能力，或者有没有那么多的资源用到这样的大模型。“如果在摩尔定律之下，更便宜的卡可以支撑更大的模型，我们一定会考虑把我们更大的模型开源。”</p><p></p><h3>做ToC的超级应用</h3><p></p><p>&nbsp;</p><p>“我们对于未来的一个愿景就是，大模型时代不仅仅是人类跨向AGI的重要一步，它也是一个巨大的平台机会。”李开复认为，这个机会就是创造超级应用。</p><p>&nbsp;</p><p>李开复解释称，如果说PC时代赋予给开发者用户的机会是computer on every desk，移动互联网带来的机会是随时随地的计算，smartphone on &nbsp;every hand，那么现在的AI 2.0时代带来的巨大机会就是把一个超级大脑对接和赋能给每一个应用，即AI for everyone。</p><p>&nbsp;</p><p>“PC时代，微软Office就是超级应用；移动互联网时代，微信、抖音是相当好的超级应用；AI 2.0时代，毫无疑问最大的商机也会是超级应用，所以这个方向是零一万物努力的目标。过去的两个时代值得借鉴，因为人类历史就是不断重复，每一个时代最大的机会跟上一个时代是可以推延的。”</p><p>&nbsp;</p><p>李开复的考虑是，首先一切的基础是大模型。“我觉得未来的内容应该主要是由AI来创造，人来帮忙，这个才是王道。所以我们Super APP开发第一点就是AI First、AI Native，没有大模型整个产品就不成立。”</p><p>&nbsp;</p><p>其次，商业化非常重要。AI 1.0公司面临的挑战主要就是商业化问题：要么收入没有做好，要么缺乏持续化收入。“字节、阿里、百度、谷歌、Facebook能够成为伟大的公司，就是因为他们的收入是有质量的。”李开复说道，“所以我们做的应用一定是朝着能够快速有收入，而且能够产生非常好的利润、收入是高质量的、可持续的，而不是一次性在某一个公司上打下一个单子。”</p><p>&nbsp;</p><p>李开复表示，AI 2.0时代的超级应用一定是在消费者级别的ToC超级应用。他透露，Super App的雏形将在不久后对外发布。对于这个Super App，团队会从简单的功能开始，然后根据捕捉到的用户需求和技术精髓不断迭代。此外，该应用虽然面向国内，但也会面向国外市场。</p><p>&nbsp;</p><p>“今天创业者最好的机会是在AI 2.0上面开发App，如果找对机会、聪明快速勤奋地迭代，任何一个App都有机会成为Super App，成为AI 2.0时代的微信、抖音。”李开复说道。</p><p></p><h3>未来规划</h3><p></p><p>&nbsp;</p><p>对于未来，零一万物表示，一方面会继续在34B规模上进行一系列开源动作，另一方面会进一步提高模型的智能极限。</p><p>&nbsp;</p><p>“我们已经在训练千亿参数以上模型，但是我们觉得模型参数可以再提高一到两个数量级，达到万亿或者十万亿的规模。数据上，我们现在基于几十T token的高质量数据，未来还可以提高到几百T或者几千T。模型智能还是有很大的发展。”据悉，零一万物现在已经在训练千亿模型，更大模型的所有前置实验也已完成，剩下的就是按部就班地训练。</p><p>&nbsp;</p><p>此外，零一万物已经有了一个超过十人的多模态方面的团队，未来一两个月内也会有相关产品发布。多模态已经纳入公司更长周期的规划中。</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/NjkLsroBG4rfmaAYdu13</id>
            <title>“2023深圳国际金融科技大赛暨微众银行2024校园招聘宣讲会”走进深大：AI、区块链、产品经理的未来在何方？</title>
            <link>https://www.infoq.cn/article/NjkLsroBG4rfmaAYdu13</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NjkLsroBG4rfmaAYdu13</guid>
            <pubDate></pubDate>
            <updated>Wed, 08 Nov 2023 05:58:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 金融科技发展, 人才培养, 深圳国际金融科技大赛, 技术公开课
<br>
<br>
总结: 在金融科技发展中，人才培养是至关重要的一环。为了推进深圳市金融科技人才高地建设工作，并向高校学子提供一个展示自身知识、能力和创意的平台，深圳大学微众银行金融科技学院与微众银行联合举办了“2023 深圳国际金融科技大赛（FinTechathon）——西丽湖金融科技大学生挑战赛”。在大赛中，组委会特别设置了技术公开课，以让同学们更加了解大赛及人工智能、区块链、金融产品经理的发展现状。通过这样的活动，将高校培养成果嫁接至高质量人才输送链路之中，为学生提供实践和就业的机会的同时，将学术成果有效转化为实际产业解决方案。 </div>
                        <hr>
                    
                    <p>在金融科技发展的过程中，人才培养是举足轻重的关键一环。为了推进深圳市金融科技人才高地建设工作，并向高校学子提供一个展示自身知识、能力和创意的平台，深圳大学微众银行金融科技学院与微众银行联合举办了“<a href="https://www.infoq.cn/news/9AYU96ZSPoCZ6kyClK94">2023 深圳国际金融科技大赛（FinTechathon）——西丽湖金融科技大学生挑战赛</a>"”（下文称“大赛”）。</p><p></p><p>在本次大赛赛程中，大赛组委会特别设置了技术公开课以让同学们更加了解大赛及人工智能、区块链、金融产品经理的发展现状。继 10 月 25 日<a href="https://www.infoq.cn/article/ZKeta6LLZD97sHwPv2UC">第一场线上技术公开课</a>"圆满开课，为了让同学们能够更直接、更近距离地了解大赛内容，11 月 3 日，大赛组委会联动微众银行人力资源部门共同走进<a href="https://www.infoq.cn/video/atB6FuOvpIQGQPWLX0Eo">深圳大学</a>"，在线下组织了一场 2023 深圳国际金融科技大赛的第二场技术公开课暨微众银行 2024 校园招聘宣讲会。此次活动将金融科技大赛、企业招聘和高校教育结合在一起，形成了一个良好的产学研合作模式，将高校培养成果嫁接至高质量人才输送链路之中，为学生提供实践和就业的机会的同时，将学术成果有效转化为实际产业解决方案。</p><p></p><p>本次宣讲会邀请到了深大微众金融科技学院党委书记刘山海书记、深大微众金融科技学院院长助理祁涵及微众银行的多位专家来到现场，围绕大赛赛题、赛制和微众银行 2024 校招内容展开宣讲。以下为本期公开课直播精华内容整理：</p><p></p><p></p><h2>一、深大微众金融科技学院院长助理祁涵再次介绍大赛规则及流程</h2><p></p><p></p><p>2023 深圳国际金融科技大赛—— 西丽湖金融科技大学生挑战赛致力于推动国内外高校学生探索金融科技领域的技术应用创新，促进政、学、企三方交流，全面提高学生的创新能力、实践能力和就业竞争力。</p><p>作为 2023 年深圳市金融科技节的重要一环，本届大赛在深圳市地方金融监督管理局、深圳市福田区人民政府、深圳市南山区人民政府战略指导下，由深圳大学、微众银行、深圳香蜜湖国际金融科技研究院等多方联合举办。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b2/b2f51f27c732fb8f013d743bc29fb043.png" /></p><p></p><p>本届大赛将通过初赛、复赛在各赛道分别遴选出 10 支队伍进入决赛角逐，并设置总额超过 69 万人民币的赛事奖金及参赛专属区块链数字证书，以奖励各赛道获得一等奖、二等奖、三等奖的队伍及成员。此外，本次大赛还邀请了学术和企业界的众多资深专家为参赛选手答疑解难——特邀国家统计局原副局长许宪春、微众银行首席智能官杨强、中国人民银行研究局原局长张健华等担任学术顾问，评委嘉宾来自微众银行及国内各大顶尖高校。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f5/f52bd2e8110a8dbb969698a29a8a0175.png" /></p><p></p><p>据祁涵介绍，本届大赛所有参赛队员必须是全日制在校大学生（包括本科生、研究生和博士生），须以团队形式参赛，每支队伍人数 2~5 人，每人只能参加一支队伍，必须要独立完成题目，最后产品的知识产权归参赛选手所有，大赛不收取任何报名费用，决赛期间的队伍食宿及往返交通费用由组委会统一安排。</p><p></p><p></p><h2>二、微众银行区块链高级架构师周禄：区块链赛道高分秘籍</h2><p></p><p></p><p>周禄在宣讲最开始就强调，区块链赛道的参赛项目需要基于 FISCO BCOS 平台及微众区块链系列开源技术设计并开发一个区块链系统，以解决 ESG 相关的某个行业或场景的痛点或问题。具体来说，选手可以将区块链技术应用于大湾区一体化、双碳、乡村振兴、公共服务等 ESG 领域。过往赛事中，有一些作品的创意就非常值得参考。比如 2019 年大赛区块链赛道第一名作品就基于 FISCO BCOS 构建了一个排污权许可区块链交易平台，配合交易纠纷仲裁、黑名单、监督审计等链上机制，健全、活化市场，实现企业、政府、公众在环保排污上的三权制衡、多元共治，以辅助排污政策制定，共建污水治理生态循环。</p><p></p><p><img src="https://static001.geekbang.org/infoq/86/861dcbc86a0af420dde7ecfcff970638.png" /></p><p></p><p>2020 年区块链赛道第二名作品 WeHelp 则基于微众银行社会治理框架“善度”，使用区块链底层平台 FISCO BCOS、分布式身份解决方案 WeIdentity 等区块链技术，加速求救与救援的匹配。项目还采用可共享的分布式账本记录善行，保证数据公信力，解决求助过程中的信任问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b2/b2221f7277edb82f51bb676863e5ccf9.png" /></p><p></p><p>2021 年区块链赛道第三名作品《亿点爱》旨在通过区块链构建公益众筹平台，利用隐私保护技术实现安全可信存储，保护用户个人隐私的同时，有效预防虚假筹款和善款被挪用等问题，以此促进互联网公益行业健康有序地发展。</p><p></p><p>周禄总结了上述获奖作品的特点第一就是应用方向符合 ESG 命题，其次就是充分理解了区块链的特性，并融入应用场景；以上述获奖作品为参考，周禄继续讲解了本届大赛中获得高分的锦囊。</p><p></p><p>参赛作品要赢得评委青睐，首先要选择适合区块链应用场景的正确方向。ESG 指环境、社会和治理，其本质是一种价值观，鼓励企业更多重视财务数据以外的贡献，在保护环境、有利社会和加强治理方面创造价值。而区块链是传递信任的机器，可以大幅降低信任成本；同时区块链可以保护隐私，避免个人数据泄露；区块链还是激励相容的，很容易设计激励机制；区块链的交易记录可全程追溯、可信可验证，这也是它的一大优势。而正是因为具备了这些特性，区块链技术很适合用于 ESG 实践。</p><p></p><p>此外，周禄还为同学们讲解了开发区块链应用的基本步骤。区块链应用的架构包括了用户、分布式应用 DAPP、服务接口 API、智能合约和底层平台，其中 DAPP 可以是命令行、网页、手机或 PC 应用，通过接口和平台通信；服务接口采用通用 JSON 格式 RPC 调用；智能合约采用 Solidity 语言编写；底层平台包含网络、共识、加密和存储等模块。</p><p></p><p>同学们在开发区块链应用时，可以首先使用大赛官方提供的快速建链工具搭建区块链，然后使用业务模版开发 Solidity 合约，并通过交互式控制台的 SDK 部署合约，使用 SDK 开发业务，通过 RPC 协议交互，这里的交互语言没有限制。最后部署业务系统，发起查询和上链交易。这些工作要在团队内分工合作，提高效率。</p><p></p><p>参赛选手要善用各类区块链组件，微众区块链全部开源，提供了众多组件供公众使用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/97/97690f91de649a76fe37f04bb4e68101.png" /></p><p></p><p>大赛官方还提供了大量代码仓库（https://github.com/FISCO-BCOS/FISCO-BCOS），包含很多参考 demo 和开源项目，选手可以直接克隆研究。</p><p></p><p><img src="https://static001.geekbang.org/infoq/68/68870ba93cf861429ca6fa9a514a8e81.png" /></p><p></p><p>官方的黑客松目录收集了基于 FISCO BCOS 开发的，参与各种大赛的优秀案例，包括每个案例的项目介绍、设计文档、源代码等，供选手参考。周禄推荐各位选手充分利用上述资源，在比赛中取得佳绩。</p><p></p><p></p><h2>三、微众银行个人直通银行部室经理金虎光：银行线上场景的交互式智能柜台服务</h2><p></p><p></p><p>本届大赛产品经理赛道的赛题是《银行线上场景的交互式智能柜台服务》，金虎光在解析赛题时表示，参赛选手应当基于对话式交互的应用进行银行产品方案设计，实现对客户全场景的陪伴式交互，为客户提供智能、便捷、懂用户、有温度的线上银行服务（可选择微众银行 APP、微众银行 We2000 小程序或其他银行产品作为产品框架进行设计）。设置该赛题的背景是银行业传统的线下柜台服务帮助实现银行职员与客户之间的互动，可以处理较为复杂和个性化的问题，更容易发展信任关系，但由于网点服务存在地点和工作时间限制服务效率比较低。近年来兴起的银行线上远程服务则希望通过各种技术手段为客户带来随时随地、方便快捷的体验，同时尽可能做到像线下一样可以处理复杂、个性化的问题。从电话银行到网上银行、银行 APP 再到虚拟数字人服务，银行正在努力将线上数字银行打造成新的增长点，提升金融服务体验和质量，提升客户经营质效。</p><p></p><p>如今各家银行的线上服务都已经包括了几乎所有银行服务功能，但随着功能指数级增长，客户的线上交互也变得非常复杂。每家银行都有多个 APP，如何让客户更方便地找到所需功能是银行面临的普遍挑战。金虎光提到，评委们希望看到参赛选手的创新想法，展现出如何在功能、场景、信息繁多的背景下让用户更加便捷地体验线上银行服务。</p><p></p><p>最近火热的生成式 AI 技术可以贯穿从市场、销售到运营、研发、风控的所有银行服务，带来许多创新性的体验。金虎光建议选手可以选择某一个垂直场景或服务，或基于一揽子服务模式来利用生成式 AI 改进交互形式。</p><p></p><p><img src="https://static001.geekbang.org/infoq/11/1128d3ad32ef06751999b20f30492ff8.png" /></p><p></p><p>例如作品可以为投资小白用户基于生成式 AI 技术分析、总结投研报告，辅助投资决策。选手还可以参考智能投顾、智能客服、远程面签等技术和场景拓展思路，发挥创意。例如招行 APP 左上角的小猫就会根据用户行为提供实时建议，APP 中的 AI 小招机器人则利用数字人模式提供了智能财富管理顾问服务；百信银行则探索利用虚拟空间模式服务线上用户等等。</p><p></p><p>微众银行也在做相关探索。例如用户可以在微众银行 APP 中与虚拟小 weiWE 机器人对话获得服务，或者使用微众银行小程序快速完成操作。金虎光建议，参赛同学可以选择微众银行 APP 或小程序，亦或是任何自己熟悉的银行 APP 作为底层框架来设计作品。</p><p></p><p>针对产品经理赛道的参赛规则，金虎光也做了详细解读。本赛道中，评委主要考虑以下四个维度为作品打分：</p><p>创新性：参赛作品具备创意亮点，用新思维解决现有问题，或探索新的行业模式，有望开拓新的产业运作模式，市场空间等。商业价值：参赛作品所面向的场景和用户有一定代表性，且作品能够很好地结合实际应用场景解决所描述场景痛点；参赛作品有良好的社会价值，运作合法合规，或具备一定商业价值、成长性和可持续性，值得规模化推广。完整性及可行性：进行有效的竞品、市场及用户分析，并总结产品相对竞品的优劣势、可借鉴及可创新之处，了解用户需求，针对用户痛点提出对应解决方案；有完整的产品设计方案，包括设计背景、产品流程、功能模块说明等，产品架构设计完整，产品流程可形成闭环。技术先进性：可清晰阐述使用的技术，技术有一定先进或创新性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b9/b9b56b49f06ebd08c59291ac2eb5f881.png" /></p><p></p><p>同时还需要注意的是，参赛作品一定要“有需求、有场景”，同时技术是可落地的，尤其要避免创意大而空的问题，要注重实际的创意内涵。与此同时，因为银行的金融交易非常关注安全性，所以作品一定要考虑必要的安全、核身和信息保护环境，理解银行产品背后的相关设计。</p><p></p><p><img src="https://static001.geekbang.org/infoq/56/56908a6e25ce3a93ec23fa9448cbfc54.png" /></p><p></p><p>金虎光还提醒同学们，参赛作品需提供完整的产品设计文档，建议包括市场及竞品分析、用户调研、产品分析和产品设计说明。初赛需提供产品设计文档（Doc 格式），复赛和决赛还需提供作品展示文档（PPT 格式）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f5/f5f39f7ae9b9a27d9dcdc5174809e6e1.png" /></p><p></p><p></p><h2>四、微众银行人工智能资深研究员、FATE 开源社区技术委员会成员范涛：人工智能赛道讲解</h2><p></p><p></p><p>范涛首先介绍了基于微众人工智能技术开源的 FATE 项目。这是目前国内最大的联邦学习开源项目和社区之一。FATE 不仅提供了底层框架，还提供了很多应用组件。选手须基于该平台构建人工智能产品，可以充分利用 FATE 提供的各类算法和组件。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5d/5ddedbc44d28162945c216cfdc791716.png" /></p><p></p><p>本赛道命题为开放式，作品可基于 AI 联邦学习开源平台 FATE，设计纵向联邦学习、横向联邦学习或者联邦大模型创新性产品或算法，包括并不限于联合风控，联合营销，智能权益定价，数据交易定价等场景。项目须使用 FATE 开源技术实现，选手须将实现代码提交至 Github 供评委考核。评委基于作品的产品实现完备性、创新性和商业价值打分：</p><p>产品实现完备性占 40% 分数，考察作品在技术层面的复杂度和实现完成度。完备性包括可行性分析、方案设计文档、代码实现、测试报告等方面。创新性占 40% 分数，考察作品在设计层面的新颖度。创新性包括但不限于全新的场景痛点，用新的算法或技术手段，有效综合利用多个技术组件从而产生新的效能。商业价值考察作品在人工智能领域中的综合价值。综合价值包括但不限于作品与实际产业的贴合度、是否有效解决真实行业痛点、是否有效地发挥了联邦学习的实用价值等。</p><p></p><p>选手须组队参赛，个人可先报名，由平台协助组队；初赛作品于 11 月 27 日截止提交，包含作品介绍（PPT 格式）、技术文档（Doc 格式）、作品展示材料（包括但不限于作品部分 demo 或演示视频等）。12 月 5 日大赛公布决赛入围名单，16-17 日在深圳举办线下 36 小时封闭马拉松，选手对初赛提交作品进行开发和完善，并做现场路演答辩。</p><p></p><p>除了赛程相关的信息，范涛还向同学们解读了横向联邦学习、纵向联邦学习和联邦大模型三大技术栈：</p><p>横向联邦学习是指每个终端都有一些同质数据，但每一方都有数据隐私保护需求，仅靠自己的数据不足以构建较好的模型，所以希望综合多方数据构建模型，本质上是扩充数据样本来提升模型效果和稳健性的方法，适用于参与者数据特征重叠较多，而样本 ID 重叠较少的情况。</p><p></p><p><img src="https://static001.geekbang.org/infoq/82/821713d23a19373d8feb8a34f18024ff.png" /></p><p></p><p>纵向联邦学习在金融领域应用非常广泛。它通过引用第三方数据与金融数据结合来提升风控、营销等场景的效果，本质上是通过扩充特征维度来提升模型效果，适用于参与者样本重叠较多的情况。</p><p></p><p><img src="https://static001.geekbang.org/infoq/15/157d0219eec8cfabb4b7fa7e65d3c274.png" /></p><p></p><p>大模型技术与联邦迁移学习有很多结合点，后者也是大模型领域的新兴范式。通过联邦迁移学习，大模型可以和本地私有数据结合，成为适合本地数据的中小模型。该课题是本届大赛的新增部分，范涛推荐选手关注。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9e/9ef4d47d97602fb405fcfbfaef48c6a8.png" /></p><p></p><p>范涛最后介绍了 2022、2020、2019 年人工智能赛道名列前茅的一些作品，他希望参赛选手参考这些优秀案例，设计出令人称赞的高分作品：</p><p>2022 年：一等奖作品是面向真人体验感知的系统，通过横向联邦学习场景综合多人信息去做感知应用；二等奖作品是将联邦学习应用于工业智能，如火焰检测、工业设备缺陷检测等场景；三等奖作品是纵向联邦学习应用于电网的场景。2021 年：一等奖作品是心理健康预测监控系统，综合用户的文本、图像、社交媒体数据心理健康行为预测平台；二等奖作品是基于 FATE 构建的联邦营销一站式平台，可以综合多企业数据来做更精准的营销。三等奖作品横向联邦场景来做保险经纪人，通过 F ATE 平台保护用户隐私。2019 年：一等奖作品是横向联邦学习场景，通过车载行为数据对车险定价；二等奖作品基于联邦做了联邦图形预算法，联合多个银行的交易网络，在不泄露隐私数据的前提下预测欺诈用户；三等奖作品是基于联邦学习平台做个人数据定价，形成新的数据交易模式。</p><p></p><p></p><h2>五、写在最后</h2><p></p><p></p><p>除了以上干货内容，深大微众金融科技学院党委书记刘海山书记在本次宣讲会开场时就为同学们加油鼓气。刘书记希望参赛学生能够积极探索金融科技领域的技术应用创新，将创新成果转化为实际应用，向金融科技行业提供更有价值的技术解决方案，为深圳乃至全国的金融科技发展贡献力量。</p><p></p><p>在本次宣讲会的校招环节，微众银行零售存款部总经理邢海鹏整体介绍了微众银行的业务和技术。据其介绍，微众银行是全国首家数字银行，2019 年与深圳大学合作成立了深大微众金融科技学院。微众银行在 IT 方面的投入营收占比超过了 9%，科技人员占比超过 50%，微众银行在 AI、区块链、云计算、大数据等方面都有大量投入，并取得了一系列行业领先的成果。</p><p></p><p>微众银行人力资源部室资深经理杨帆详细介绍了微众银行的人才结构——银行员工平均年龄 33 岁，本科及以上学历超过 99%，人才来源也非常多元化。校招生身份入职的微众银行的同学，可以获得专属“私塾学习计划”，并为校招生设置了一年的培养期。</p><p></p><p>2024 年微众银行校园招聘主要分为技术研发、数据算法、产品业务、综合职能、财富管理五大类别，具体的招聘详情可以根据下列途径进行了解 ↓</p><p></p><p><img src="https://static001.infoq.cn/resource/image/96/fb/96ee25124fde526ded7913138c2199fb.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/zptXlfRaUrtvBoE8bYmu</id>
            <title>阿里云CTO周靖人：API和模型级别开放大模型能力，做to C 产品不是目标</title>
            <link>https://www.infoq.cn/article/zptXlfRaUrtvBoE8bYmu</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/zptXlfRaUrtvBoE8bYmu</guid>
            <pubDate></pubDate>
            <updated>Wed, 08 Nov 2023 04:17:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 阿里云, 大模型通义千问2.0, GPT-3.5, 智能编码助手通义灵码
<br>
<br>
总结: 阿里云发布了千亿级参数大模型通义千问2.0，该模型在综合性能上超过了GPT-3.5，并且正在追赶GPT-4。此外，阿里云还发布了智能编码助手通义灵码等行业应用大模型。阿里云的目标是将模型能力开放给更多的开发者和合作伙伴使用，以支持他们在创业、落地和创新方面的需求。阿里云的大模型策略包括提供先进的AI基础设施、开源模型与产品结合的服务以及简单的API集成。这次AI技术变革是一次技术体系的全面升级，包括系统优化、提升开发效率和打造最好的AI基础设施。云厂商需要懂AI和云计算，以更低的成本提供模型服务。国内模型生态正在快速发展，代表着算力的发展。 </div>
                        <hr>
                    
                    <p>10月31日，阿里云正式发布千亿级参数大模型通义千问2.0。官方数据显示，在10个权威测评中，通义千问2.0综合性能超过GPT-3.5，正在加速追赶GPT-4。此外，阿里云还发布了智能编码助手通义灵码等行业应用大模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cc/cc23cc39efb9acd3e61f3863210bbba1.jpeg" /></p><p>&nbsp;</p><p>针对阿里云的大模型策略，阿里云CTO周靖人在接受记者采访时表示，“我们的目标并不是做toC的产品，而是希望更多地把模型的能力开放出来，让更多开发者、合作伙伴使用。”</p><p>&nbsp;</p><p>据周靖人介绍，阿里云的定位是“服务好各种各样在AI时代的创业者、开发者和企业客户等。通过多层技术能力，包括AI基础设施、模型等能力，最好地支持开发者和客户，帮助他们解决在创业、落地、创新等方面的问题。”</p><p>&nbsp;</p><p>具体来讲，模型创业公司希望使用到最先进的AI基础设施；企业客户希望将开源模型与自己产品做二次结合，这类产品包括通义千问等开源模型、帮助企业做模型定制的阿里云百炼等；还有关注业务系统的开发者，通过简单的API集成到自己业务体系中。</p><p>&nbsp;</p><p>周靖人表示，这次AI技术变革的实质是一次技术体系的全面升级。对云计算来说，主要包括以下纬度：第一，系统优化，即如何利用模型能力优化复杂庞大的分布式系统，让它真正变成一个“自动驾驶的云”；第二，用模型帮助提升开发效率，即让用云这件事本身变得更加智能；第三，以模型为中心打造最好的AI基础设施，提供低成本、一站式的模型训练、微调、推理等服务。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9f/9f74a49d11d71efa482b001ac1473b48.jpeg" /></p><p>&nbsp;</p><p>“而云厂商既要懂AI，又要懂云计算，才能在这次竞争里取得一个重要的战略性优势。”周靖人表示。具体来说，云厂商要做的事情就是让用户以更低的成本使用模型来提供服务。</p><p>&nbsp;</p><p>“今天基础设施的目标，特别是模型推理方面，不单是提升延迟等各个方面的性能，同时还要能够降低使用成本。在这方面，我们还有大量的工作需要做。”周靖人表示，阿里云不是简单开发界面的开放，而是API级别、模型级别的开放。用户可以借这些产品发挥更大的想象空间，做更多业务的创新。</p><p>&nbsp;</p><p>周靖人呼吁，大家要给这个领域一些时间。“毕竟从国内来讲，整个产业的变化是从今年开始的，甚至到了3、4月份，大家才陆陆续续发模型。在这方面，我们的确比海外要晚，海外拥有至少一年的先发优势，甚至更长的时间。”</p><p>&nbsp;</p><p>不过，周靖人表示，国内也在快速地追赶中。短短半年时间内，国内模型生态已经慢慢发展起来了。模型的生态发展起来，一定代表了算力发展得起来。</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0Mh7GIzC3GdmtAQGW0Jw</id>
            <title>火山引擎金融解决方案负责人王建军确认出席 FCon，分享金融数字化升级：让智慧带来生产力</title>
            <link>https://www.infoq.cn/article/0Mh7GIzC3GdmtAQGW0Jw</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0Mh7GIzC3GdmtAQGW0Jw</guid>
            <pubDate></pubDate>
            <updated>Wed, 08 Nov 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: FCon 全球金融科技大会, 火山引擎金融解决方案, 大模型发展及应用现状, 王建军
<br>
<br>
总结: FCon 全球金融科技大会将在上海举行，火山引擎金融解决方案负责人王建军将分享关于金融数字化升级和大模型应用的主题演讲。他将介绍国内外大模型发展及应用现状，以及火山引擎在金融行业的探索实践。此外，大会还将涉及DevOps在金融企业落地实践、金融科技创新应用、金融数据平台建设、金融安全风险管控和数据合规等领域的交流。 </div>
                        <hr>
                    
                    <p><a href="https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle">FCon 全球金融科技大会</a>"，将于 11 月在上海召开。火山引擎金融解决方案负责人王建军将发表题为《<a href="https://fcon.infoq.cn/2023/shanghai/presentation/5606?utm_source=infoqweb&amp;utm_medium=article">金融数字化升级：让智慧带来生产力</a>"》主题分享，介绍国内外大模型发展及应用现状、火山引擎在金融行业的探索实践，以及大模型在未来行业的应用。</p><p></p><p><a href="https://fcon.infoq.cn/2023/shanghai/presentation/5606?utm_source=infoqweb&amp;utm_medium=article">王建军</a>"，10 年以上人工智能及数字化实践经验，2020 年加入字节跳动，曾服务于德勤咨询、第四范式等企业，推动过数十家金融机构的数字化转型和人工智能应用实践。他在本次会议的演讲内容如下：</p><p></p><p>演讲：大模型：让智慧带来生产力</p><p></p><p>通过观察海外大模型领先应用实践，结合国内产业生产效率痛点，明确大模型应用蓝图，及围绕蓝图展开的探索与实践。</p><p></p><p>演讲提纲：</p><p></p><p>海内外大模型风起云涌；大模型能力范畴和典型应用分析；围绕金融行业的探索实践；展望未来。</p><p></p><p>你将获得：</p><p></p><p>○ 了解到国内外大模型发展及应用现状；</p><p>○ 了解火山引擎在金融行业的探索实践；</p><p>○ 共同畅想大模型未来行业应用。</p><p></p><p>除上述演讲外，FCon 上海还将围绕&nbsp;<a href="https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle">DevOps&nbsp;在金融企业落地实践</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle">金融行业大模型应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle">创新的金融科技应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle">金融实时数据平台建设之路</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle">金融安全风险管控</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle">数据要素流通与数据合规</a>"等进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！大会 8 折优惠报名倒计时仅剩 3 天，现在购票立减￥1360。咨询购票请联系：17310043226（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/7wdgYqK7JYVvdEay5hON</id>
            <title>“算”赋千行，“智”启新程，天翼云多项成果惊艳亮相，邀您共鉴！</title>
            <link>https://www.infoq.cn/article/7wdgYqK7JYVvdEay5hON</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/7wdgYqK7JYVvdEay5hON</guid>
            <pubDate></pubDate>
            <updated>Wed, 08 Nov 2023 03:16:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数字化基础设施建设, 人工智能产业, 算力基础设施, 智算需求
<br>
<br>
总结: 近年来，我国数字化基础设施建设不断完善，人工智能产业蓬勃发展，成为当下驱动经济社会转型升级的重要力量。为抢抓人工智能发展的重大机遇，构筑我国人工智能发展先发优势，国家陆续出台了多项政策，将人工智能列为国家战略性新兴产业，鼓励人工智能行业发展与创新。在此背景下，智算作为人工智能时代的关键生产力要素，需求呈爆发式增长。为推进算力基础设施高质量发展，充分发挥算力对数字经济的驱动作用，工业和信息化部、中央网信办、教育部等六部门联合印发《算力基础设施高质量发展行动计划》明确提出，结合人工智能产业发展和业务需求，重点在西部算力枢纽及人工智能发展基础较好地区集约化开展智算中心建设，逐步合理提升智能算力占比。面对激增的智算需求，天翼云作为云服务国家队，从多方位升级算力基础设施，为人工智能产业发展夯实算力底座，加速科技普惠。 </div>
                        <hr>
                    
                    <p> 近年来，我国数字化基础设施建设不断完善，人工智能产业蓬勃发展，成为当下驱动经济社会转型升级的重要力量。为抢抓人工智能发展的重大机遇，构筑我国人工智能发展先发优势，国家陆续出台了多项政策，将人工智能列为国家战略性新兴产业，鼓励人工智能行业发展与创新。</p><p></p><p>在此背景下，智算作为人工智能时代的关键生产力要素，需求呈爆发式增长。为推进算力基础设施高质量发展，充分发挥算力对数字经济的驱动作用，近日，工业和信息化部、中央网信办、教育部等六部门联合印发《算力基础设施高质量发展行动计划》明确提出，结合人工智能产业发展和业务需求，重点在西部算力枢纽及人工智能发展基础较好地区集约化开展智算中心建设，逐步合理提升智能算力占比。根据IDC报告显示，预计到2026年，中国智能算力的年复合平均增长率达到52.3%，是三倍于通用算力规模的增长速度。</p><p></p><p> 面对激增的智算需求，天翼云作为云服务国家队，从多方位升级算力基础设施，为人工智能产业发展夯实算力底座，加速科技普惠。技术方面，天翼云始终坚持科技创新，不断攻克关键核心技术，以云操作系统为核心，从底层基础软硬件技术，到上层高阶云能力，实现了全栈技术的自主可控。基础设施方面，天翼云不断完善“2+4+31+X”云网融合资源布局，构建了“集中化+区域化+属地化+边缘化”的云网基础设施，积极推进算力普惠发展；天翼云建设的新一代智算中心在算力、算效、资源利用率等方面不断追求极致，降低大模型训练、推理、部署、应用门槛。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ce/ce40fa919dc071d54806d97739a0aef3.png" /></p><p></p><p> 人工智能浪潮下，天翼云凭借项目沉淀、技术积累，打造智能计算平台，依托分布式架构云底座和充沛的计算、存储、网络资源，为大模型训练、智能推荐、无人驾驶、生命科学、NLP等业务场景提供智算、超算、通算多样化算力服务，激发数字经济发展新活力。同时，天翼云在通用算力资源全国布局的基础上，科学规划建设智能算力，不断夯实国云智算底座，构建AI时代强大基石。</p><p></p><p> 当今社会，科技高速发展，每一轮技术变革无不渗透在我们的日常生活中。站在大算力、大模型、大数据的“高起点”上，天翼云致力于以科技创新服务千行百业，加速算力普惠民生。为进一步推动人工智能应用落地，加速生产生活方式智慧变革，11月10日—13日，以“数字科技 焕新启航”为主题的2023数字科技生态大会即将启幕。</p><p></p><p> 届时，天翼云将亮相大会主论坛及多个分论坛，重磅发布智算领域科技创新最新成果，并带来云电脑等产品的最新升级，同时在展区，天翼云也将从科技创新、算力底座、产业引领等层面，系统性展示领先的云能力和实践成果，以国云筑基，携手业界共创智算新时代</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/P2agXGEtoLNotk2Eb8xP</id>
            <title>谷歌开源 AI 微调方法： Distilling Step-by-Step</title>
            <link>https://www.infoq.cn/article/P2agXGEtoLNotk2Eb8xP</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/P2agXGEtoLNotk2Eb8xP</guid>
            <pubDate></pubDate>
            <updated>Wed, 08 Nov 2023 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 华盛顿大学, 谷歌研究中心, 逐步蒸馏, 小型微调模型
<br>
<br>
总结: 华盛顿大学和谷歌研究中心的团队开源了一种名为"逐步蒸馏"的技术，用于微调规模较小的语言模型。逐步蒸馏通过使用大型语言模型生成小型微调数据集，训练小模型来预测输出标签并生成对应的理由。这种方法可以在减少训练数据集规模和模型大小的同时，提高小型模型的性能。谷歌使用逐步蒸馏技术在NLP基准测试中取得了良好的表现，仅使用数据集的一小部分数据就能超越大型语言模型的性能。 </div>
                        <hr>
                    
                    <p>华盛顿大学和谷歌研究中心的一个团队最近开源了 <a href="https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html">Distilling Step-by-Step</a>"（逐步蒸馏），一种用于微调规模较小的语言模型的技术。与标准微调相比，逐步蒸馏需要的训练数据更少，并且生成的模型更小，但模型性能却优于参数规模是它 700 倍的小样本提示大型语言模型 （LLM）。</p><p>&nbsp;</p><p>虽然 LLM 一般可以在提示较少的情况下在多种任务上有良好的表现，但由于其内存和算力要求过高，模型的托管是比较有挑战的。规模较小的模型在微调后也可以有良好的表现，但这需要工程师手动创建针对具体任务优化的数据集。逐步蒸馏的关键思想是使用 LLM 自动生成一个小型微调数据集，其中的数据有一个输入和一个输出标签，以及选择这个输出标签的“理由”。微调过程会训练这个小模型来预测输出标签并生成对应的理由。在 NLP 基准上评估时，小型微调模型的性能优于 540B PaLM 模型，同时仅需要这个基准测试的全部微调数据的 80%。据谷歌称：</p><p></p><p></p><blockquote>我们展示了，逐步蒸馏既减少了构建针对特定任务的较小模型所需的训练数据集规模，也减少了实现甚至超越小样本提示 LLM 的性能水平所需的模型大小。总的来说，逐步蒸馏提出了一种可以高效利用资源的范例，可以解决模型大小和所需训练数据之间的权衡问题。</blockquote><p></p><p></p><p>研究表明，增加 LLM 中的参数规模可以提高其性能，目前最先进的模型（例如 PaLM）拥有数百亿个参数。然而，这些大型模型价格昂贵，且难以用于推理，因为它们需要多个并行连接的 GPU 才能把这么多参数保存在内存里。最近的研究开发出了规模稍小的模型（例如 Meta 的 Llama 2），其性能表现差不多，但参数少了一个数量级；然而，这些小一些的模型还是很庞大，需求的算力也很高。</p><p>&nbsp;</p><p>要做出在特定任务上表现良好的小模型的一种方法，是使用针对具体任务收集的数据集来微调小规模语言模型。虽然这个数据集可能相对较小（大约有数千个示例），但其数据收集起来可能还是费时费钱。另一种选择是知识蒸馏，也就是使用大型模型作为较小模型的老师。 InfoQ 最近报道了谷歌开发的一项<a href="https://www.infoq.com/news/2023/01/google-llm-self-improvement/">技术</a>"，使用 PaLM LLM 来创建训练数据集，最后生成的微调模型的性能可与规模大 10 倍的 LLM 相媲美。</p><p>&nbsp;</p><p>逐步蒸馏确实需要微调数据集，但它减少了创建高性能模型所需的数据量。源数据集通过思维链提示输入 PaLM LLM，要求模型给出其答案的理由。输出结果是修正后的微调数据集，其中包含原始输入和答案以及理由。这个较小的目标模型经过微调来执行两项任务：回答原始问题并生成理由。</p><p>&nbsp;</p><p>谷歌使用四个 NLP 基准测试评估了他们的技术，每个基准都包含一个微调数据集。他们使用逐步蒸馏来修正这些数据集，并使用了参数不到 1B 的微调 T5 模型。他们发现，这些模型在仅使用数据集的一小部分数据的情况下，性能就比基线微调模型要好；在某些情况下只要 12.5% 的数据就有这样的表现。他们还发现，他们的 770M 参数模型在 ANLI 基准测试中的性能优于大它 700 倍的 540B 参数 PaLM，同时只需要 80% 的微调数据集数据。</p><p>&nbsp;</p><p>在 X（以前的 Twitter）上关于这项工作的讨论中，人工智能企业家 Otto von Zastrow 写道：</p><p></p><p></p><blockquote>这些结果非常厉害。我会把这种办法叫做合成数据生成，而不是蒸馏，我真的很好奇，如果你根据每个示例问题的合成理由来训练原始的 LLM 会发生什么事情。</blockquote><p></p><p></p><p>逐步蒸馏的源代码和训练数据集可在 <a href="https://github.com/google-research/distilling-step-by-step">GitHub</a>" 上获取。 Google Cloud 的 Vertex AI 平台还提供该算法的非公开预览。</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/10/google-distillation/">https://www.infoq.com/news/2023/10/google-distillation/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/67vMj2F2HTC24fDdE64a</id>
            <title>OpenAI 用45分钟重塑游戏规则！干掉MJ、LangChain，创造“不会编程的应用开发者”新职业</title>
            <link>https://www.infoq.cn/article/67vMj2F2HTC24fDdE64a</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/67vMj2F2HTC24fDdE64a</guid>
            <pubDate></pubDate>
            <updated>Tue, 07 Nov 2023 06:06:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, DevDay, 新产品, GPT-4 Turbo, Assistants API, 多模态功能, ChatGPT, GPT Store
<br>
<br>
总结: OpenAI在DevDay开发者日活动上发布了多款新产品，包括功能更强大且价格更低廉的GPT-4 Turbo模型，允许开发人员构建AI助手应用的Assistants API，以及支持视觉、图像创建和文本转语音等多模态功能。此外，OpenAI还推出了ChatGPT的自定义版本GPTs，并计划推出GPT Store用于分享用户构建的自定义GPT助手。 </div>
                        <hr>
                    
                    <p>北京时间 11 月 7 日凌晨 02:00，OpenAI 的首次 DevDay 开发者日活动正式开始。Sam Altman 用了45 分钟的时间发布了多款新产品。微软 CEO Satya Nadella 还亲自去现场参与了这次发布。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f3/f34366844d84e3463822882a2b0cb0dd.png" /></p><p></p><p>此次，OpenAI分享了数十项新增功能和改进，并降低了平台上多种服务的价格。具体包括：</p><p>&nbsp;</p><p>新的GPT-4 Turbo模型，功能更强大、价格更低廉且支持128K上下文窗口。新的Assistants API，允许开发人员轻松构建具有目标且能够调用模型及工具的AI助手应用。平台提供新的多模态功能，包括视觉、图像创建（DALL-E 3）及文本转语音（TTS）等。</p><p>&nbsp;</p><p>此外，OpenAI还推出了 ChatGPT 的自定义版本 GPTs。OpenAI表示，GPTs 是一种新方式，任何人都无需编码就可以创建 ChatGPT 的定制版本，以便其在日常生活、特定任务、工作或家庭中更有帮助，并与其他人分享该创作。比如，GPTs 能协助用户掌握任何桌面游戏的规则、辅助孩子学习数学或者设计个性贴纸。</p><p>&nbsp;</p><p>目前，ChatGPT Plus 和 Enterprise 用户已经能够尝试包括Canva和Zapier AI Actions在内的 GPTs 示例。</p><p>&nbsp;</p><p>本月晚些时候，OpenAI将推出GPT Store，主要用于分享用户构建的自定义 GPT 助手，开发者可以借此赚钱，使用自己作品的用户数越多收入越高。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/cad7684bc790f29a6d3a57853fc37b75.png" /></p><p></p><p>Sam Altman 展示GPT Store</p><p>&nbsp;</p><p>对于此次大会，网友 altoidsjedi 难掩兴奋：“对于我们当地的 LLM 人员来说，鉴于 Orca / Dolphin / Airboros 等合成的、GPT-4 生成的数据集的成功（特别是在删除拒绝响应之后），这些新的 OpenAI API 工具将不可避免地导致开源合成数据集的增加充满了使用视觉、检索、函数调用等的优秀示例。我们可以使用数据集来微调/教学/提炼到我们的本地人工智能模型中以模拟相同的行为！这是双赢。”网友 Independent_Key1940 则一阵见血地指出，“降低成本才是当前最应该做的事情，这能在很大程度上改变现状。”</p><p>&nbsp;</p><p>此次公布的各项新功能，将于太平洋时间11月6日下午一点起向OpenAI客户开放。下面，我们具体看下OpenAI这次到底为我们带了哪些“惊喜”。</p><p></p><h2>功能增强了，价格低了</h2><p></p><p>&nbsp;</p><p></p><h4>GPT-4 Turbo迎来128K上下文</h4><p></p><p>&nbsp;</p><p>OpenAI于今年3月发布了GPT-4的首个版本，并于7月开放GPT-4的通用版。此次，OpenAI又带来下一代GPT-4 Turbo的预览版本。这也是这次颇受关注的更新之一。</p><p>&nbsp;</p><p>GPT-4 Turbo功能更强，学习内容截止于2023年4月。它拥有128k上下文窗口，因此能够在单一提示词中容纳相当于300多页文本的内容。据悉，与GPT-4相比，GPT-4 Turbo的输入token成本只相当于三分之一、输出token成本则为二分之一。</p><p>&nbsp;</p><p>GPT-4 Turbo现通过API中的gpt-4-1106-preview向所有付费开发者开放，OpenAI计划将在未来几周内发布稳定的生产就绪模型。</p><p>&nbsp;</p><p>除了GPT-4 Turbo之外，OpenAI还为GPT-3.5 Turbo发布了新的版本，默认支持16k上下文窗口。新的3.5 Turbo将支持改进的指令跟踪、JSON模式与并行函数调用。OpenAI的内部评估显示，格式遵循任务（例如生成JSON、XML和YAML）的效果提高了38%。开发人员可以调用API中的gpt-3.5-turbo-1106来访问这个新模型。使用gpt-3.5-turbo名称的应用程序将从12月11日起自动升级至该新模型。</p><p>&nbsp;</p><p>此外，OpenAI还在函数调用更新、改进指令遵循与JSON模式、可重复输出与对数概率方面做了改进。OpenAI还计划在未来几周推出另一项功能，用于为GPT-4 Turbo和GPT-3.5 Turbo返回潜在输出标记的对数概率，这项功能将在搜索体验的自动补全等功能中发挥重要作用。</p><p>&nbsp;</p><p></p><h4>发布 Assistants API</h4><p></p><p>&nbsp;</p><p>OpenAI此次发布了Assistants API，这是帮助开发人员在应用程序当中构建智能体功能的第一步。</p><p>&nbsp;</p><p>助手智能体是一种专用型AI，提供特定指令和额外的专业知识，并可调用模型及工具以执行任务。新的Assistants API提供Code Interpreter代码解释器、Retrieval检索以及函数调用等新功能，可以处理以往用户必须新手完成的大量繁重工作，帮助开发者腾出精力构建高质量的AI应用程序。</p><p>&nbsp;</p><p>此API在设计上充分强调灵活性，用例范围包括基于自然语言的数据分析应用、编码助手、AI驱动的假期规划器、语音控制DJ、智能视觉画布等等。Assistants API与OpenAI的新GPT产品基于相同的功能基础，包括Code Interpreter、Retrieval和函数调用等自定义指令和工具。</p><p>&nbsp;</p><p>该API还引入一项关键变化，即持久且无限长线程，允许开发人员将线程状态管理移交给OpenAI以解决上下文窗口约束。使用Assistants API，用户只需要将每条新消息都添加至现有线程即可。</p><p>&nbsp;</p><p>Assistants还可根据需要调用新工具，包括：</p><p>&nbsp;</p><p>Code Interpreter：在沙盒执行环境中编写并运行Python代码，可以生成图形和图表，并处理包含不同数据和格式的文件。它允许AI助手以迭代方式运行代码，从而解决极具挑战的编码和数学问题。Retrieval：利用模型以外的知识来增强助手，例如专有领域数据、产品信息或用户提供的文档。如此一来，大家无需对文档嵌入进行计算和存储，也无需使用分块和搜索算法。Assistants API将根据OpenAI在ChatGPT中构建知识检索的经验，对各类常用检索方法进行优化。函数调用：使助手能够调用定义的函数，并将函数响应合并至消息当中。</p><p>&nbsp;</p><p>与家族中的其他产品一样，OpenAI表示，永远不会将用户上传至API的数据和文件用于训练自有模型，开发人员还可以根据需求随时删除数据。Assistants API目前处于beta测试阶段，且从即日起面向全体开发者开放。</p><p>&nbsp;</p><p></p><h4>API中的新模式</h4><p></p><p>&nbsp;</p><p>现在，GPT-4 Turbo可接收图像作为Chat Completions聊天补全API中的输入，从而实现标题生成、真实图像分析、阅读带图形的文档等用途。例如，BeMyEyes就使用这项技术帮助盲人或弱视人群完成日常任务，例如识别产品或浏览商店。开发人员可以通过API中的gpt-4-vision-preview来访问此项功能。</p><p>&nbsp;</p><p>OpenAI还计划为GPT-4 Turbo主模型提供视觉支持，这项新功能将被纳入稳定版本，而计费标准则由输入的图像大小决定。例如，将一张1080 x 1080像素的图像上传至GPT-4 Turbo的费用为0.00765美元。</p><p>&nbsp;</p><p>此外，开发人员可以通过Images API将DALL-E 3 直接集成至自己的应用程序和产品当中，具体方式就是用dall-e-3指定模型。据悉，Snap、可口可乐和Shutterstock等公司已经使用DALL-E 3以编程方式为客户及活动生成图像和设计。</p><p>&nbsp;</p><p>与之前版本的DALL-E类似，该API中同样内置有审核功能，可帮助开发人员保护自身免遭滥用。OpenAI还提供不同的格式和质量选项，生成单张图像的起步价格为0.04美元。</p><p>&nbsp;</p><p>开发人员还可以通过文本转语音API将普通文本转换为与真人质量相当的语音。新的TTS模型提供六种预设声音以及两种模型变体：tts-1和tts-1-hd。tts针对实时用例进行了优化，而tts-1-hd则主要面向更高的质量需求。每输入1000字符的起步价格为0.015美元。</p><p></p><h4>模型定制</h4><p></p><p>&nbsp;</p><p>OpenAI表示，正在开发一款用于GPT_4微调的实验性访问程序。初步结果表明，与GPT-3.5微调实现的效果相比，GPT-4微调需要更大的工作量才能对基础模型做出有意义的改进。随着GPT-4微调质量与安全性的提升，已经熟悉GPT-3.5微调开发人员现可尝试在微调控制台中操作GPT-4程序。</p><p>&nbsp;</p><p>对于需要在微调之外更多定制模型特征的用户（主要指拥有超大规模专有数据集、对应数十亿token的场景），OpenAI还启动了模型定制计划，为特定组织提供与OpenAI研究团队合作的机会，共同面向特定领域对GPT-4做定制训练。其中包括修改模型训练流程中的各个步骤，开展额外的特定领域预训练，以及运行针对特定领域定制的强化学习后训练过程。组织将拥有对其定制模型的独家访问权。</p><p>&nbsp;</p><p>根据OpenAI的现有企业隐私政策，自定义模型不会被提供给其他客户或对外开放，也不会被用于训练其他模型。此外，提供给OpenAI用于训练自定义模型的专有数据也不会在任何其他环境中被重复使用。不过，这项计划高度受限且价格昂贵，只面向特定组织开放。</p><p></p><h4>价格更低，限制更少</h4><p></p><p></p><p>OpenAI正在下调各项服务的价格，希望将节约下的成本回馈给用户（以下价格均以1000 token为单位）：</p><p>&nbsp;</p><p>GPT-4 Turbo的输入token价格为GPT-4的三分之一，即0.01美元；输出token为GPT-4的二分之一，即0.03美元。GPT-3.5 Turbo的输入token为此前16k模型的三分之一，即0.001美元；输出token价格为二分之一，即0.002美元。此前使用GPT-3.5 Turbo 4k的开发者输入token价格将下降三分之一，即0.001美元。所有价格下调仅适用于此次推出的新款GPT-3.5 Turbo。经过微调的GPT-3.5 Turbo 4k模型输入token价格降低至四分之一，即0.003美元；输出token下调至1/2.7，即0.006美元。微调版本还通过新的GPT-3.5 Turbo模型，实现了价格与原4k版本相同、但上下文窗口扩大至16k的效果。这些新价格也将适用于微调版gpt-3.5-turbo-0613模型。</p><p></p><p>&nbsp;</p><p>OpenAI还将每位付费GPT-4客户的每分钟token限制扩大了一倍，现在大家可以在速率限制页面查看新的指标。OpenAI还公布了自动速率限额的使用等级，用户可以根据自己的情况查看相应限额，并在账户设置中申请提升限额。</p><p>&nbsp;</p><p>版权保护方面，OpenAI 推出了Copyright Shield，即帮助客户应对关于侵犯版权的法律索赔，并支付由此产生的费用。这项服务将在ChatGPT Enterprise及开发者平台上全面开放。</p><p>&nbsp;</p><p>此外，OpenAI即将发布Whisper large-v3，即开源自动语音识别模型（ASR）的下一版本，其跨语言性能将得到提升。OpenAI还计划在不久之后通过API支持Whisper v3。</p><p>&nbsp;</p><p>OpenAI同时开源了Consistency Decoder，即Stable Diffusion VADE解码器的替代方案。这款解码器针对Stable Diffusion 1.0+ VAE所兼容的一切图像做出优化，在文本、人脸和直线等处理能力上均有显著改进。</p><p></p><h2>ChatGPT周活跃用户破亿，大股东微软CEO现身“带货”Azure</h2><p></p><p>&nbsp;</p><p>在开发者大会上，OpenAI 公司CEO Sam Altman 宣布，ChatGPT 的周活用户数量已经突破 1 亿。自今年 3 月通过 API 发布 ChatGPT 与 Whisper 模型以来，该公司目前已经吸引到超 200 万开发人员，涵盖超 92% 的全球财富 500 强企业。</p><p>&nbsp;</p><p>在发布的近一年之后，ChatGPT 已经被广泛认定为有史以来增速最快的消费级互联网应用，其用户数量估计在短短两个月内就达到 1 亿。相比之下，Facebook 自 2004 年推出以来经过约四年半时间才拥有 1 亿用户，Twitter 达成这个目标用了五年多时间，Instagram 则用了两年多。可以说，ChatGPT 仍是有史以来增长速度最快的服务之一。</p><p>&nbsp;</p><p>今年以来，OpenAI 聊天机器人似乎成为流量密码，谁拥抱它、谁就能获得用户的青睐。早在今年 2 月，Similarweb 就估计该工具已经迈过了单月 1 亿访问者、单日 2500 万访问者的里程碑。但本次大会上的声明尤其值得关注，因为这是 OpenAI 发布的官方数字，而非第三方粗略统计。有评论认为，OpenAI 发布这些数据似乎是为了反驳近期媒体的报道，即自去年 11 月上线以来 ChatGPT 的人气正有所下滑。</p><p>&nbsp;</p><p>此外，微软 CEO&nbsp;Satya Nadella&nbsp;意外现身 OpenAI 开发者大会，并传达了一条明确信息：请与我们携手创造。Satya Nadella&nbsp;向 Sam Altman 强调，“我们的首要任务就是打造出最好的系统，你们可以借此构建起最好的模型，再将其开放给开发人员。”</p><p>&nbsp;</p><p>据报道，微软已经先后向 OpenAI 投资 130 亿美元，并希望吸引更多开发者使用其 Azure 云基础设施提供的计算和存储资源，而非选择亚马逊云科技和 Google Cloud 等竞品。近年来，Azure 已经成为微软的关键业务增长引擎，也帮助该公司重振了自身在开发者心中的品牌形象。</p><p>&nbsp;</p><p>在本次大会上，OpenAI 公布了更加强大的 GPT-4 Turbo 模型，并表示用户还可以借此构建ChatGPT 聊天机器人的定制版本。该公司也带来了价格更低廉的软件付费选项，允许开发者通过微软购买 OpenAI 的编程工具。但无论走微软的渠道还是直接从 OpenAI 处购买，主机均由 Azure 提供。</p><p>&nbsp;</p><p>微软拥有 OpenAI GPT-4 大语言模型的独家许可，该模型能够根据几个提示词就生成与人类质量相当的输出。微软正推出多款基于 GPT-4 模型的产品，包括用于 Office 生产力应用订阅的 AI 插件、以及 Windows 11 中的智能助手。微软表示，其 Bing 搜索引擎凭借今年早些时候引入 Open AI GPT-4 支持的生成式 AI 功能，而在 2009 年首度亮相的十多年之后，终于在今年 3 月突破了 1 亿日活用户这一里程碑。</p><p>&nbsp;</p><p>Satya Nadella&nbsp;指出，使用 OpenAI 构建软件的开发者可以通过 Azure&nbsp;Marketplace“将产品快速投放市场”。这也是 Satya Nadella&nbsp;用于吸引大量开发者使用 Azure 的最新战略。2018 年，微软就曾斥资 75 亿美元收购 GitHub，而 GitHub 正是无数企业用于存储和共享代码的主力开发平台。Satya Nadella&nbsp;表示，微软将向所有与会者开放 GitHub Copilot 企业版，帮助开发人员高效补全源代码行。</p><p>&nbsp;</p><p>微软正努力发挥 OpenAI“大庄家”这一优势地位，希望让Azure成为更多开发者构建AI产品和服务的首选平台。Satya Nadella&nbsp;指出，“我们的使命是帮助世界上的每个人、每家组织取得更大的成就。对我来说，AI 必须通过赋能来发挥它的价值和作用。”</p><p>&nbsp;</p><p>Sam Altman 也从自己的角度努力推广微软产品，并为企业客户勾勒出了关于通用人工智能（AGI）的美好前景。Sam Altman 在发言中强调，“我认为我们与微软有着科技领域最好的合作伙伴关系，很高兴我们能够为 AGI 的实现而共同努力。”关于双方业务安排，Sam Altman 表示“我们双方之间已经建立起合作关系，我们乐于看到微软成功拿下一笔笔订单，微软也真心为 OpenAI 业务的快速发展喝彩。”</p><p>&nbsp;</p><p>根据公司发言人介绍，OpenAI 的首届现场活动吸引到约 900 名与会者。作为 ChatGPT 的缔造者，OpenAI 凭借这款 AI 驱动的聊天机器人在去年年底突然走红，更引发了全球对于生成式AI领域的大量投入。《华尔街日报》9 月曾报道称，OpenAI 正与投资者就出售股票事宜进行谈判，目前该公司的估值已经在 800 亿到 900 亿美元之间。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://openai.com/blog/introducing-gpts">https://openai.com/blog/introducing-gpts</a>"</p><p><a href="https://openai.com/blog/new-models-and-developer-products-announced-at-devday">https://openai.com/blog/new-models-and-developer-products-announced-at-devday</a>"</p><p><a href="https://www.theverge.com/2023/11/6/23948386/chatgpt-active-user-count-openai-developer-conference">https://www.theverge.com/2023/11/6/23948386/chatgpt-active-user-count-openai-developer-conference</a>"</p><p><a href="https://www.cnbc.com/2023/11/06/microsoft-ceo-nadella-makes-surprise-appearance-at-openai-event.html">https://www.cnbc.com/2023/11/06/microsoft-ceo-nadella-makes-surprise-appearance-at-openai-event.html</a>"</p><p><a href="https://www.reddit.com/r/LocalLLaMA/comments/17p9mgc/openai_dev_day_discussion/">https://www.reddit.com/r/LocalLLaMA/comments/17p9mgc/openai_dev_day_discussion/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/dcxN1gUJbMrBcc0d5K86</id>
            <title>火山引擎云计算解决方案负责人吴春龙确认出席 FCon，分享大规模混合部署基础设施探索与实践</title>
            <link>https://www.infoq.cn/article/dcxN1gUJbMrBcc0d5K86</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/dcxN1gUJbMrBcc0d5K86</guid>
            <pubDate></pubDate>
            <updated>Tue, 07 Nov 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: FCon 全球金融科技大会, 大规模混合部署基础设施探索与实践, 吴春龙, 云计算服务
<br>
<br>
总结: FCon 全球金融科技大会将在上海举行，吴春龙将发表题为《大规模混合部署基础设施探索与实践》的主题分享，介绍大规模基础设施的建设、组网和监控运维的实践与方法论。吴春龙是资深云计算专家，致力于为汽车、银行、手机厂商等公司提供安全、稳定、易用、高效的云计算服务。 </div>
                        <hr>
                    
                    <p><a href="https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle">FCon 全球金融科技大会</a>"，将于 11 月在上海召开。火山引擎云计算解决方案负责人吴春龙将发表题为《<a href="https://fcon.infoq.cn/2023/shanghai/presentation/5609?utm_source=infoqweb&amp;utm_medium=article">大规模混合部署基础设施探索与实践</a>"》主题分享，介绍大规模基础设施的建设、组网和监控运维的实践与方法论。</p><p></p><p><a href="https://fcon.infoq.cn/2023/shanghai/presentation/5609?utm_source=infoqweb&amp;utm_medium=article">吴春龙</a>"，资深云计算专家，致力于为汽车、银行、手机厂商等公司提供安全、稳定、易用、高效的云计算服务。他在本次会议的演讲内容如下：</p><p></p><p>演讲：大规模混合部署基础设施探索与实践</p><p></p><p>在大模型时代，AI 智能生产力无处不在，将重新定义数字金融、保险行业，火山引擎提供大规模 AI 基础设施解方案，保障效率、稳定性的同时提供创新的生产力，加速金融行业智能化。</p><p></p><p>演讲提纲：</p><p></p><p>大模型基础设施挑战；大算力基础设施技术探索；大算力基础设施推荐实践；展望技术发展。</p><p></p><p>你将获得：</p><p></p><p>○ 了解大规模基础设施的建设的实践与方法论；</p><p>○ 了解大规模基础设施的组网的实践与方法论；</p><p>○ 了解大规模基础设施的监控运维的实践与方法论。</p><p></p><p>除上述演讲外，FCon 上海还将围绕&nbsp;<a href="https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle">DevOps&nbsp;在金融企业落地实践</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle">金融行业大模型应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle">创新的金融科技应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle">金融实时数据平台建设之路</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle">金融安全风险管控</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle">数据要素流通与数据合规</a>"等进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！现在购票，享 8 折优惠 ，立省 ￥1360！咨询购票请联系：17310043226（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Ife3NEEtk2yYPu7mIL1Q</id>
            <title>腾讯 AI Lab 专家研究员黄国平博士，确认担任 QCon LLM 推理加速和大规模服务专题出品人</title>
            <link>https://www.infoq.cn/article/Ife3NEEtk2yYPu7mIL1Q</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Ife3NEEtk2yYPu7mIL1Q</guid>
            <pubDate></pubDate>
            <updated>Mon, 06 Nov 2023 08:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, LLM 推理加速和大规模服务, 黄国平博士, 交互翻译
<br>
<br>
总结: QCon 全球软件开发大会将在上海举行，黄国平博士将担任「LLM 推理加速和大规模服务」的专题出品人。在此次专题中，将介绍新技术和模型功能，以及如何上线进行服务等前沿话题。黄国平博士是腾讯 AI Lab 专家研究员，专注于交互翻译的研究与应用。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1106&amp;utm_content=huangguoping">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。腾讯 AI Lab 专家研究员黄国平博士将担任「<a href="https://qcon.infoq.cn/2023/shanghai/track/1605?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1106&amp;utm_content=huangguoping">LLM 推理加速和大规模服务</a>"」的专题出品人。在此次专题中，你将了解到来自业内的专家关于新的技术和模型功能，以及如何上线进行服务等较为前沿的话题。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/track/1605?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1106&amp;utm_content=huangguoping">黄国平博士</a>"，腾讯翻译负责人，腾讯 AI Lab 专家研究员。2008 级本科计算机科学与技术专业，后保研到中国科学院自动化研究所模式识别国家重点实验室硕博连读，师从宗成庆研究员，研究方向为机器翻译、自然语言处理。2017 年博士毕业至至今，在腾讯 AI Lab 长期专注于交互翻译的研究与应用。在 ACL、AAAI、IJCAI、EMNLP 等人工智能领域顶级会议与 TASLP 等顶级期刊发表论文 20 余篇。</p><p></p><p>相信黄国平博士的到来，可以帮助提升此专题的质量，让你学习到如何利用新的技术和模型功能来实现推理加速，如何上线进行服务（比如应对攻击和刁难）等关于 LLM 的相关热点，为探索新方向提供了更广阔的思路。</p><p></p><p>除上述专题外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、</p><p><a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1599?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的大前端技术</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！现在购票，享 7 折优惠，立减￥2040！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/EUNuoKDME8GPIzhXLUu4</id>
            <title>vivo发布基于Rust语言的操作系统，全球首款？字节跳动宣布除夕统一放假；大妈招女婿要求大模型从业人员 | Q资讯</title>
            <link>https://www.infoq.cn/article/EUNuoKDME8GPIzhXLUu4</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/EUNuoKDME8GPIzhXLUu4</guid>
            <pubDate></pubDate>
            <updated>Mon, 06 Nov 2023 07:19:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 百度文心一言推出专业版, vivo自研智慧操作系统, 大妈招女婿要求, 马斯克的X公司估值降至190亿美元, 字节跳动宣布除夕统一放假, ChatGPT之父剑桥演讲遭抵制, 谷歌Assistant语音助手团队调整
<br>
<br>
总结: 百度推出文心一言专业版，vivo发布自研智慧操作系统，大妈招女婿要求阿里云从业人员，马斯克的X公司估值下降，字节跳动宣布除夕放假，ChatGPT之父剑桥演讲遭抵制，谷歌Assistant团队裁员。 </div>
                        <hr>
                    
                    <p></p><blockquote>百度文心一言推出专业版，定价为 59.9 元/月；vivo 将发布自研智慧操作系统：基于 Rust 语言编写，全球首款？大妈招女婿要求阿里云大模型从业人员，淘宝的不要；字节跳动宣布除夕统一放假：不占用年假额度；1688和闲鱼升级为淘天集团一级业务，负责人直接向戴珊汇报；世界首个开源贡献榜出炉......</blockquote><p></p><p>&nbsp;</p><p></p><h2>科技企业</h2><p></p><p>&nbsp;</p><p></p><h4>百度文心一言推出专业版，定价为 59.9 元/月</h4><p></p><p>&nbsp;</p><p>百度上线文心一言专业版，单月购买定价为 59.9 元/月，连续包月优惠价 49.9 元/月。此前已经向用户开放的文心一言基础版，仍可免费使用。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ee/eeb88e6a7dab7cfda92b9226a6b542ce.png" /></p><p></p><p>&nbsp;</p><p>据介绍，文心一言会员版基于文心大模型 4.0。基于 4.0 的专业版具备更强的模型能力和图片生成能力。支持各种插件，适合需要使用文心一言进行代码编程、文案撰写、绘画设计等专业工作需求的用户。</p><p>&nbsp;</p><p></p><h4>vivo 将发布自研智慧操作系统：基于 Rust 语言编写，全球首款？</h4><p></p><p>&nbsp;</p><p>11月1日，在 2023 vivo 开发者大会上，vivo 发布了自研操作系统蓝河 (BlueOS)。vivo 表示，蓝河操作系统采用 Rust 编写“系统框架”—— 从源头避免了内存使用不当引起的安全漏洞。据称是行业首家。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b1b3e77af2ccfb0715aa0f581e701cec.png" /></p><p></p><p>&nbsp;</p><p>vivo 还称蓝河操作系统是面向通用人工智能时代的自研智慧操作系统：底层接入了 AI 大模型，支持基于自然交互方式的应用开发。此外，据称蓝河操作系统是基于 Linux/RTOS 的自研架构，因此不兼容 Android 应用。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>大妈招女婿要求阿里云大模型从业人员，淘宝的不要</h4><p></p><p>&nbsp;</p><p>10月31日，在杭州云栖大会现场，坊间传闻有热心人士举牌寻缘：独女，温柔萧山女孩，94年，事业编，体重51，家中有厂，家庭和睦身高1.63，接受姐弟恋。要求男孩96年-98年，在阿里从事大模型工作，算法&gt;后端&gt;前端，淘宝的不要、阿里云优先。不要彩礼，入赘送房一套，通勤车凯迪拉克一辆。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/28/288b60975ac5128e9a74a4ecb0e26821.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/69/69a4dce3f76e63880823e7ed640571f4.jpeg" /></p><p></p><p>&nbsp;</p><p>虽然有人怀疑这只是一种营销手法，但这则花边新闻的热度丝毫未减。一些网友评论称，在他们的朋友圈里，技术PPT分享并不多，而招婿大妈的帖子却刷屏了……</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>马斯克的X公司估值降至190亿美元，不到推特收购价一半</h4><p></p><p>&nbsp;</p><p>据知情人士称，马斯克的社交媒体公司X现在的估值已降至190亿美元，还不到马斯克一年前收购推特价格的一半。</p><p>&nbsp;</p><p>一位知情人士透露，X将以每股45美元的价格向员工发放限制性股票，对该公司的估值约190亿美元，相比马斯克一年前440亿美元的收购价格已缩水55%。</p><p>&nbsp;</p><p>更多阅读：</p><p><a href="https://mp.weixin.qq.com/s/j9PINcFAZy-m1CBlNi-wzA">疯狂马斯克的“极限”计划居然成功了？！“下云”后成本降低 60%，部分功能代码精简 90%，30 天急速迁移服务器</a>"</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>字节跳动宣布除夕统一放假：不占用年假额度</h4><p></p><p>&nbsp;</p><p>10月30日，网传字节跳动宣布除夕统一放假，其办公软件“飞书”日历上，已显示除夕当天为“春节团聚假”。当日，就此传闻求证字节跳动员工，获知该消息属实。前述字节跳动员工表示，公司有关除夕放假的公告发布于10月30日13时许，公告中还指出：“其中除夕当天为公司额外提供的春节团聚假，不占用年假额度。”</p><p>&nbsp;</p><p>10月25日，国务院办公厅发布关于2024年部分节假日安排的通知。其中，春节假期为2月10日至17日放假调休，共8天，2月4日（星期日）、2月18日（星期日）上班，即除夕不放假。通知中提出，鼓励各单位结合带薪年休假等制度落实，安排职工在除夕（2月9日）休息。</p><p>&nbsp;</p><p></p><h4>ChatGPT之父剑桥演讲遭抵制：抗议者挂横幅扔传单</h4><p></p><p>&nbsp;</p><p>当地时间11月1日，山姆·奥特曼代表OpenAI团队接受2023年霍金奖学金并演讲时遭到抵制。数名抗议者悬横幅、扔传单，引来观众嘘声，场面一度十分混乱。而在活动开始前，就有少数抗议者聚集在外面，举着标语要求停止AI竞赛。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/61/614cf1948a12948dffd018c93d8b3abf.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>谷歌Assistant语音助手团队调整，20多名数据科学家被裁</h4><p></p><p>&nbsp;</p><p>11月2日消息，报道称，据内部人士和一份记录公司内部裁员情况的员工文件透露，谷歌已经解雇了一些从事语音助手工作的员工。周一，一些从事 Assistant 项目的员工被告知他们的职位被取消了。据该员工文件称，被裁掉的员工有多达 20 名，他们都是数据科学家。该文件还指出，一些被裁员工上周五刚从 Bard 团队转移到 Assistant 团队。该文件是由员工编制的，汇总了在内部和外部发布的关于裁员的信息。“其中一名被裁员工正在休产假，另一名患有癌症，”文件写道。</p><p>&nbsp;</p><p>被裁员工有 60 天的时间在公司内部找到新的职位，否则他们将不得不离开。谷歌发言人在一份声明中表示：“为了优化我们团队结构，使其更符合我们最高优先级的目标，我们对团队进行了重组，以更好地支持我们的战略业务目标。”</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>1688和闲鱼升级为淘天集团一级业务，负责人直接向戴珊汇报</h4><p></p><p>&nbsp;</p><p>据晚点 LatePost消息，阿里旗下的国内批发电商平台1688和二手电商平台闲鱼的重要性提升，近期升级为淘天集团的一级业务。此前，1688 总裁余涌（花名：朴初）向淘天集团中小企业发展事业部负责人汪海（花名：七公）汇报，闲鱼总经理为丁健（花名：季山），向阿里妈妈负责人刘博（花名：家洛）汇报。业务升级后，余涌和丁健两位负责人将直接向淘天集团CEO戴珊汇报。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>全球通缉！苹果公司一中国籍女程序员醉驾保时捷致使他人死亡</h4><p></p><p>&nbsp;</p><p>上个月的9月30日，美国西雅图贝尔维尤发生了一起严重的车祸事件。事故中，一名在苹果公司工作的中国女程序员涉嫌醉酒和超速驾车，导致了这起严重车祸，副驾驶座上的中国男性乘客当场死亡。</p><p>&nbsp;</p><p>目前，西雅图警方已发布了对涉事女程序员的通缉令，并计划对她提出重罪指控。这起事故引起了广泛关注，据称该女子目前已经潜逃回国。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1afbb5fa6b7b2b271fd9bff80429ad37.jpeg" /></p><p></p><p>图源一亩三分地论坛</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>IT业界</h2><p></p><p>&nbsp;</p><p></p><h4>世界首个开源贡献榜出炉：中国排行第二，谷歌微软阿里巴巴等公司在列</h4><p></p><p>&nbsp;</p><p>10月31日消息，据<a href="https://mp.weixin.qq.com/s/rk8RYcJstd7KYmFYFdoPRA">国际测试委员会Bench Council官方公众号</a>"消息，Bench Council公布了“世界首个开源贡献榜”，号称“只以贡献分高下”。从 Bench Council 披露数据得知，Bench Council 号称邀请了多位独立科学家，从 20 世纪 60 年代至今的开源或对开源产生重要影响的成果中，遴选出了 145 项代表性成果，在确定主要贡献者的基础上产生了开源领域五十年人才榜、机构榜、国家榜。</p><p>&nbsp;</p><p>据悉，总共 264 人进入榜单，共有 24 位华人上榜；美国在国家榜上排行第一，中国排名第二。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/9f/9f4ba91e10479a16480551dacddad3a5.png" /></p><p></p><p>&nbsp;</p><p>国内机构中，涛思数据因时序数据库TDengine上榜，百度因深度学习框架PaddlePaddle上榜，PingCAP因TiDB数据库上榜，中国科学院计算所和北京开源芯片研究院因开源RISC-V处理器香山系列上榜，阿里巴巴和平头哥半导体因开源RISC-V处理器玄铁系列上榜，中国科学院软件所因OpenBLAS线性代数库上榜，上海纽约大学因对深度学习框架MXNet的贡献上榜。</p><p>&nbsp;</p><p></p><h4>谷歌放弃了 Web Environment Integrity API</h4><p></p><p>&nbsp;</p><p>谷歌放弃了倍受争议的 Web Environment Integrity(WEI) API 提议。此前，该提议引发了广泛争议，主要浏览器开发商 Mozilla 等都表达了反对立场。谷歌官方博客表示，他们听取了社区的反馈，Chrome 团队现在不再考虑 WEI 提议，已经递交了 commit 从 Chromium 源代码中移除相关代码。但谷歌并没有完全放弃 WEI，而是决定从 Web 版退缩到 Android 版，宣布开发 Android WebView Media Integrity API，该 API 功能上与 WEI 类似，但针对的是嵌入在 Android 应用中的 WebViews。谷歌仍然要创造出一种 Android DRM API，这一次它不再需要考虑社区意见或反馈。</p><p>&nbsp;</p><p>更多阅读：</p><p><a href="https://mp.weixin.qq.com/s/02r_D7O2I9XtnV_v90aK7A">Web 开放性或遭重大打击！谷歌四名工程师推出 WEI 方案，可让广告拦截变成历史</a>"</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>Linux Kernel 6.6 正式发布</h4><p></p><p>&nbsp;</p><p>10 月 30 日消息，Linus Torvalds 宣布 Linux Kernel 6.6 正式推出，主要引入了几项新功能及驱动程序方面的更新。Linus Torvalds 在 6.6 版本更新公告中表示：“过去的一周非常平静，我没有任何借口再推迟 6.6 版本的发布，所以是时候发布了。除了对 r8152 驱动程序的一些较大修复外，其他都是小修小补。”</p><p>&nbsp;</p><p>Linux Kernel 6.6 主要引入了 EEVDF 调度器，实现了 Shadow Stack 的支持，为 Nouveau DRM 驱动程序添加了 Mesa NVK Vulkan 驱动程序所需的用户空间 API，继续支持即将到来的英特尔和 AMD 平台。</p><p>&nbsp;</p><p>更多信息请参阅 Linux 6.6 功能列表：</p><p><a href="https://lore.kernel.org/lkml/CAHk-=wiZuU984NWVgP4snp8sEt4Ux5Mp_pxAN5MNV9VpcGUo+A@mail.gmail.com/T/#u">https://lore.kernel.org/lkml/CAHk-=wiZuU984NWVgP4snp8sEt4Ux5Mp_pxAN5MNV9VpcGUo+A@mail.gmail.com/T/#u</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/3m7F87QpDVsu8zv68k1b</id>
            <title>李开复4个多月后“放大招”：对标OpenAI、谷歌，发布“全球最强”开源大模型</title>
            <link>https://www.infoq.cn/article/3m7F87QpDVsu8zv68k1b</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/3m7F87QpDVsu8zv68k1b</guid>
            <pubDate></pubDate>
            <updated>Mon, 06 Nov 2023 06:21:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 零一万物, Yi-34B, 大模型, 开源模型
<br>
<br>
总结: 由李开复打造的AI大模型创业公司"零一万物"发布了开源大模型Yi-34B，该模型经过340亿个参数训练，具有全球最强的通用能力、知识推理和阅读理解等多项指标。Yi-34B支持超长上下文窗口，能处理约40万汉字超长文本输入。零一万物通过优化计算能力和降低训练成本，致力于构建最先进的专有模型，以满足市场需求。 </div>
                        <hr>
                    
                    <p>今天，由李开复打造的 AI 大模型创业公司“零一万物”发布了一系列开源大模型：Yi-34B和Yi-6B。</p><p>&nbsp;</p><p>Yi-34B 是一个双语（英语和中文）基础模型，经过 340 亿个参数训练，明显小于 Falcon-180B 和 Meta LlaMa2-70B 等其他开放模型。在发布会中，李开复称其数据采集、算法研究、团队配置均为世界第一梯队，对标OpenAI、谷歌一线大厂，并抱有成为世界第一的初衷和决心。同时，他表示Yi-34B是“全球最强开源模型”，其通用能力、知识推理、阅读理解等多指标均处于全球榜单首位。</p><p>&nbsp;</p><p>零一万物团队也进行了一系列打榜测试，具体成绩包括：</p><p>&nbsp;</p><p>Hugging Face英文测试榜单，以70.72分数位列全球第一；以小博大，作为国产大模型碾压Llama-2 70B和Falcon-180B等一众大模型（参数量仅为后两者的1/2、1/5）；C-Eval中文能力排行榜位居第一，超越了全球所有开源模型；MMLU、BBH等八大综合能力表现全部胜出，Yi-34B在通用能力、知识推理、阅读理解等多项指标评比中“击败全球玩家”；......</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b319194b622b37e2630eb89146346ef7.jpeg" /></p><p></p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/89/89024c95616f3552a4b3a6bce0a13f2e.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>然而，在发布中，也有一点需要指出，那就是Yi系列模型在GSM8k和MBPP的数学以及代码测评方面表现不如GPT模型出色。这是因为团队希望在预训练阶段希望先尽可能保留模型的通用能力，所以训练数据中没有加入过多数学和代码数据。后续他们计划在开源系列中推出专注于代码和数学领域的继续训练模型。</p><p>&nbsp;</p><p></p><h2>200K上下文窗口，能处理40万字文本</h2><p></p><p>&nbsp;</p><p>值得注意的是，此次开源的Yi-34B模型，将发布全球最长、可支持200K 超长上下文窗口（context window）版本，可以处理约40万汉字超长文本输入。这意味着Yi-34B不仅能提供更丰富的语义信息，理解超过1000页的PDF文档，还让很多依赖于向量数据库构建外部知识库的场景，都可以用上下文窗口来进行替代。</p><p>&nbsp;</p><p>相比之下，OpenAI的GPT-4上下文窗口只有32K，文字处理量约2.5万字。今年三月，硅谷知名 AI 2.0 创业公司Anthropic的Claude2-100K 将上下文窗口扩展到了100K规模，零一万物直接加倍，并且是第一家将超长上下文窗口在开源社区开放的大模型公司。</p><p>&nbsp;</p><p>在语言模型中，上下文窗口是大模型综合运算能力的金指标之一，对于理解和生成与特定上下文相关的文本至关重要，拥有更长窗口的语言模型可以处理更丰富的知识库信息，生成更连贯、准确的文本。</p><p>&nbsp;</p><p>此外，在文档摘要、基于文档的问答等下游任务中，长上下文的能力发挥着关键作用，行业应用场景广阔。在法律、财务、传媒、档案整理等诸多垂直场景里，更准确、更连贯、速度更快的长文本窗口功能，可以成为人们更可靠的AI助理，让生产力得到大幅提升。然而，受限于计算复杂度、数据完备度等问题，上下文窗口规模扩充从计算、内存和通信的角度存在各种挑战，因此大多数发布的大型语言模型仅支持几千tokens的上下文长度。为了解决这个限制，零一万物技术团队实施了一系列优化，包括：计算通信重叠、序列并行、通信压缩等。通过这些能力增强，实现了在大规模模型训练中近100倍的能力提升。</p><p>&nbsp;</p><p></p><h2>实现40%训练成本下降</h2><p></p><p>&nbsp;</p><p>AI Infra（AI Infrastructure &nbsp;人工智能基础架构技术）主要涵盖大模型训练和部署提供各种底层技术设施，包括处理器、操作系统、存储系统、网络基础设施、云计算平台等等，是模型训练背后极其关键的“保障技术”，这是大模型行业至今较少受到关注的硬技术领域。</p><p>&nbsp;</p><p>李开复曾经表示，“做过大模型Infra的人比做算法的人才更稀缺”，而超强的Infra 能力是大模型研发的核心护城河之一。在芯片、GPU等算力资源紧缺的当下，安全和稳定成为大模型训练的生命线。零一万物的 Infra 技术通过“高精度”系统、弹性训和接力训等全栈式解决方案，确保训练高效、安全地进行。</p><p>&nbsp;</p><p>凭借其强大的 AI Infra 支撑，零一万物团队表示，Yi-34B模型训练成本实测下降40%，实际训练完成达标时间与预测的时间误差不到一小时，进一步模拟上到千亿规模训练成本可下降多达50%。截至目前，零一万物Infra能力实现故障预测准确率超过90%，故障提前发现率达到99.9%，不需要人工参与的故障自愈率超过95%，有力保障了模型训练的顺畅进行。</p><p>&nbsp;</p><p></p><h2>零一万物背后</h2><p></p><p>&nbsp;</p><p>今年7月，李开复博士正式官宣并上线了其筹组的“AI 2.0”新公司：零一万物。此前李开复曾表示，AI大语言模型是中国不能错过的历史机遇，零一万物就是在今年3月下旬，由他亲自带队孵化的新品牌。</p><p>&nbsp;</p><p>在接受外媒采访时，他谈到了创办零一万物的动机：“我认为需求是创新之母，中国显然存在巨大的需求，”“与其他国际地区不同，中国无法访问OpenAI和谷歌，因为这两家公司没有在中国提供他们的产品。因此，我认为有很多人正在努力为市场创造解决方案。这是刚需。”</p><p>&nbsp;</p><p>众所周知，构建大模型是一项耗资巨大的事业。为了维持现金密集型业务，零一万物从一开始就制定了商业化计划。虽然该公司将继续开源其一些模型，但其目标是构建最先进的专有模型，作为各种商业产品的基础。</p><p>&nbsp;</p><p>李开复表示，他们非常清楚这些大型语言模型需要大量计算，花费巨大。“我们筹集到了大量资金，其中大部分都花在了 GPU 上。”与中国其他LLM玩家一样，零一万物也需要积极储备GPU以应对美国制裁。在发布会中，李开复表示零一万物现在的供应至少足以满足未来 12-18 个月的需求。</p><p>&nbsp;</p><p>美国的制裁也让中国企业注重优化计算能力，<a href="https://techcrunch.com/2023/11/05/valued-at-1b-kai-fu-lees-llm-startup-unveils-open-source-model/">李开复表示</a>"：“借助一支非常高质量的基础设施团队，每1000个GPU，我们也许能够从中挤出2000个GPU的工作负载。”</p><p>&nbsp;</p><p>从一些报道中，我们可以了解到，零一万物员工规模已超过100人，半数是来自国内外大厂的LLM专家。其中，零一万物技术副总裁及AI Alignment负责人是 Google Bard/Assistant 早期核心成员，主导或参与了从 Bert、LaMDA 到大模型在多轮对话、个人助理、AI Agent 等多个方向的研究和工程落地；首席架构师曾在Google Brain与Jeff Dean、Samy Bengio等合作，为TensorFlow的核心创始成员之一。</p><p>&nbsp;</p><p>零一万物的商业化之路很大程度上取决于其为其昂贵的AI模型找到适合的产品市场的能力。“中国在大模型方面并不领先于美国，但毫无疑问，中国可以构建比美国开发商更好的应用程序，这主要是因为过去 12 年左右建立的非凡的移动互联网生态系统，”李开复说道。</p><p>&nbsp;</p><p>李开复表示，这家初创公司的最终目标是成为一个外部开发人员可以轻松构建应用程序的生态系统。“我们的职责不仅仅是推出好的研究模型，更重要的是让应用程序开发变得容易，这样才能有优秀的应用程序，”他说。“归根结底。这是一场生态系统游戏。”&nbsp;</p><p>&nbsp;</p><p>开源地址：</p><p>Hugging Face：<a href="https://huggingface.co/01-ai/Yi-34B">https://huggingface.co/01-ai/Yi-34B</a>"；<a href="https://huggingface.co/01-ai/Yi-6B">https://huggingface.co/01-ai/Yi-6B</a>"</p><p>ModelScope：<a href="https://www.modelscope.cn/models/01ai/Yi-34B/summary">https://www.modelscope.cn/models/01ai/Yi-34B/summary</a>"； <a href="https://www.modelscope.cn/models/01ai/Yi-6B/summary">https://www.modelscope.cn/models/01ai/Yi-6B/summary</a>"</p><p>GitHub：<a href="https://github.com/01-ai/Yi">https://github.com/01-ai/Yi</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/evEep3Yxl4hs8H3g4xSm</id>
            <title>晋升神器：AI 一键生成 PPT，技术好的同时也做好PPT｜InfoQ 用户的双十一福利</title>
            <link>https://www.infoq.cn/article/evEep3Yxl4hs8H3g4xSm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/evEep3Yxl4hs8H3g4xSm</guid>
            <pubDate></pubDate>
            <updated>Mon, 06 Nov 2023 04:49:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 升职加薪, 程序员, PPT, 爱设计 AiPPT
<br>
<br>
总结: 程序员晋升需要展示过去一年的成绩和未来规划，PPT的制作水平成为考验。爱设计AiPPT是一款智能的PPT生成工具，通过人工智能和自然语言处理技术，能够快速生成符合需求的PPT内容。它支持文档上传生成、在线自由编辑、云端存储和兼容.pptx格式等特点，让程序员能够通过高效、智能的PPT呈现自己的思维和观点，提升晋升机会。 </div>
                        <hr>
                    
                    <p>升职加薪自然是每位职场人都渴望的事情，程序员们也不例外。但是，晋升就需要汇报过去一年的主要成绩，并介绍自己接下来的具体规划，这时候就要考验各位的 PPT 水平了。过去几年，程序员的圈子里广泛流传着一句话：技术再好，不如 PPT 做得好！</p><p></p><p><img src="https://static001.geekbang.org/infoq/62/62558b508f91e83607cc773353ba54dd.jpeg" /></p><p></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzIzODQ3NDQ4Mw%3D%3D&amp;chksm=e9398033de4e0925f3cb70287083d06fe9abae4e15a4999ad3e049039603fbaee9e26afe3ddf&amp;idx=1&amp;mid=2247484847&amp;scene=27&amp;sn=e7f72229a61027479fddf81828a9697d&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">PPT </a>"不仅仅是把想说的话呈现出来，最重要的其实是背后的抽象总结能力和逻辑思维能力。其次才是选择合适的框架将内容有序组织起来，最后才是美化。</p><p></p><p>一份制作良好的 PPT，不仅能够清晰地呈现你的思维和观点，更能够让你在众多竞争者中脱颖而出。但是，繁琐的制作过程，费时的排版，以及无法完全达到期望的效果，都是制作 PPT 时所面临的困扰。现在，爱设计 AiPPT 将为广大程序员解决这些问题，让大家在技术好的同时通过有效的内容呈现，获得晋升！</p><p></p><p></p><p></p><p>据悉，<a href="https://mp.weixin.qq.com/mp/wappoc_appmsgcaptcha?poc_token=HNhuSGWjyZFUhGN608tMnwAr-8P6v2nBNFTplteR&amp;target_url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU1NDA4NjU2MA%253D%253D%26chksm%3Dfbeb1b50cc9c9246033e7ad2c008c78f34fdb2d3ee0472f79890f09d9562a7b698ca46972ba1%26idx%3D2%26mid%3D2247574175%26scene%3D27%26sn%3D215f4d6a1899210cba77401d5274a635%26utm_campaign%3Dgeek_search%26utm_content%3Dgeek_search%26utm_medium%3Dgeek_search%26utm_source%3Dgeek_search%26utm_term%3Dgeek_search#wechat_redirect">爱设计</a>" AiPPT 是一款真正智能的 PPT 生成工具。它运用了人工智能技术与自然语言处理两项技术，能够智能理解用户输入的主题，并快速生成符合需求的 PPT 内容。无论是文字、图片、表格，还是图表，都能够以最短的时间为你呈现出一份专业而精美的 PPT。</p><p></p><p>具体来说，爱设计 AiPPT 具有以下特点：</p><p></p><p><a href="https://www.infoq.cn/article/understand-huawei-ai-strategy?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">AI</a>" 一键智能生成。基于人工智能和自然语言处理技术，能智能分析用户输入的主题，并快速生成符合需求的 PPT 内容</p><p></p><p>2.文档上传生成。支持多种文件格式上传（doc、docx、xmind、mm），一键上传，AI 智能排版配色、快速生成 PPT</p><p></p><p>3.在线自由编辑器。支持一键整体更换模板、更换配色，内置上千套定制级 PPT 模板及超 10w+ 素材，只需拖拉拽即可快速修改</p><p></p><p>4.云端存储，跨设备同步。PPT 云端制作在线保存，无需下载，打开网站即随时随地开启创作和演示，跨设备不再是障碍</p><p></p><p>5.兼容.pptx 格式，支持源文件导出。支持 JPG、PNG、PDF、PPT 文件导出，PPT 源文件格式导入导出均无格式错乱问题</p><p></p><p>在流程上，用户只需要在 PC 端登录 aippt.cn，输入你的要求和目标，它就会自动生成脑图，帮助大家想清楚整体规划，这个脑图还可以直接增减修改，确定好内容结构大纲后，就可以自动生成对应的 PPT 文件，还支持更换 PPT 风格等。</p><p></p><p>值此双十一之际，InfoQ 为广大用户推出了专属福利，现在通过专属渠道（链接：<a href="https://shop18793264.m.youzan.com/wscgoods/detail/3nu6184aqcboong">https://shop18793264.m.youzan.com/wscgoods/detail/3nu6184aqcboong</a>"）购买爱设计 AiPPT 一年仅需 89 元！这是一个难得的机会，让你在享受高效、智能的 PPT 生成服务的同时，还能够节省大量的时间和精力。</p><p></p><p>与此同时，InfoQ 联合极客时间推出了《PPT 设计进阶 · 从基础操作到高级创意》的课程，帮助大家基于爱设计 AiPPT 完成 PPT 制作，课程 + 工具打包购买将享受更多优惠，仅需 129 元（链接：<a href="https://shop18793264.m.youzan.com/wscgoods/detail/3nu6184aqcboong">https://shop18793264.m.youzan.com/wscgoods/detail/3nu6184aqcboong</a>"）。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/1c/d0/1c4173161ce3655a5b74a677b0ce16d0.jpg" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/wDflTumrr5pfJReMYRz6</id>
            <title>字节宣布除夕放假、连放9天，不占年假；印度“IT业之父”要求年轻人每周工作70小时；Redis 创始人用 C 语言编写出最小聊天服务器｜AI一周资讯</title>
            <link>https://www.infoq.cn/article/wDflTumrr5pfJReMYRz6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/wDflTumrr5pfJReMYRz6</guid>
            <pubDate></pubDate>
            <updated>Sun, 05 Nov 2023 06:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 聊天服务器, Redis, 创始人, C语言编写
<br>
<br>
总结: Redis创始人Salvatore Sanfilippo使用C语言编写了一个核心代码仅300多行的聊天服务器项目Smallchat。这个项目是他给前端开发朋友的系统编程示例，实现了用户自定义昵称。
 </div>
                        <hr>
                    
                    <p></p><h2>资讯</h2><p></p><p></p><h4>Redis 创始人用 C 语言编写最小聊天服务器 Smallchat，核心代码仅 300 多行</h4><p></p><p></p><p>11 月 2 日消息，知名数据库缓存工具 Redis 的创始人 Salvatore Sanfilippo（网名 antirez）在 GitHub 上传了一个名为 Smallchat 的聊天服务器项目，用 C 语言编写了一个核心代码仅 300 多行的服务。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7a/7ae812c0ab4c9598c5cb3122d70eebe7.png" /></p><p></p><p>在项目介绍中表示，这只是他给几个前端开发朋友的系统编程示例，尽自己所能写出来的最小聊天服务器，核心代码（不算空格和注释）仅有 200 多行，甚至实现了用户自定义昵称。</p><p></p><h4>大疆否认被罚60亿美元：案件仍在审理，未有更新的判决信息</h4><p></p><p></p><p>近日，网传美国将“大疆专利侵权”的处罚金额从此前的2.79亿美元上调至60亿美元（约440亿元人民币），对此大疆相关负责人回应称，该案件在今年四月陪审团的裁决金额是2.789亿美元，案子仍在审理过程中，截至目前并没有更新的判决信息。</p><p></p><p>据此前中证报报道，2021年，美国航空航天企业德事隆公司以侵权专利为由对大疆提起专利诉讼，要求大疆共支付3.67亿美元（约合25亿元人民币）的赔偿。今年4月份经过陪审团审理，大疆侵犯了美国无人机公司德事隆的两项美国专利，需分别赔偿3070万美元、2.482亿美元，合计2.789亿美元（约20亿元人民币）。</p><p></p><h4>vivo自研蓝河操作系统不兼容安卓应用，副总裁称AI大模型投入无上限</h4><p></p><p></p><p>11月1日消息，在2023 vivo开发者大会上，vivo自研蓝河操作系统 BlueOS 发布，将在vivo WATCH 3手表首发搭载。在11月1日下午的论坛上，vivo 副总裁周围在接受媒体采访时明确表示，vivo 自研蓝河操作系统不兼容安卓应用。从2023 vivo开发者大会获悉，蓝河操作系统目前已有支付宝、百度地图、喜马拉雅等 App 接入，并兼容 hapjs 快应用标准。</p><p></p><p>此外，vivo副总裁周围表示，vivo大模型现在每年20-30亿的投入成本，人才和设备各占一半，人才成本平均税后100万元。公司对大模型投入定义为高规格投入，目前没有设置上限。</p><p></p><h4>在线办公巨头WeWork将申请破产，估值曾达470亿美元</h4><p></p><p></p><p>据知情人士透露，在线办公巨头WeWork计划最早于下周申请破产。目前，WeWork正考虑在新泽西州根据《破产法》第十一章申请破产。此前，WeWork未能在10月2日向其债券持有人支付利息，然后获得了30天的宽限期。如果在宽限期内仍然无法支付利息，它将被视为违约。</p><p></p><p>WeWork周二表示，已与债券持有人达成协议，在触发违约之前，公司又获得了七天时间与利益相关方进行谈判。据此前报道，WeWork估值一度达到470亿美元。由于“共享办公”公司WeWork认股权证的交易价格“异常低”，纽约证券交易所已暂停其交易，并将启动将其退市的程序。</p><p></p><h4>Windows 11的市场份额已跃升至26%以上</h4><p></p><p></p><p>StatCounter 发布了月度报告，其中包含有关桌面操作系统、搜索引擎、浏览器等的最新数据。根据 2023 年 10 月的报告，Windows 11 的市场份额显著上升，从 9 月份的 23.64% 攀升至 2023 年 10 月的 26.14%。</p><p></p><p>考虑到自 2023 年 4 月以来，该操作系统的市场份额一直保持相对不变，因此本月看到了这款系统的影响力有了明显的增长。</p><p></p><p>虽然 Windows 11 的市场份额似乎并不令人印象深刻，尤其是在其继任者即将到来之际，但微软对其表现还是相当满意的。最近的一份新报告显示，Windows 11 的月活跃设备数已超过 4 亿，这一数字明显高于微软最初的预期。</p><p></p><p>尽管 Windows 11 在一个月内的数据提高了近 3 个百分点，但仍远低于 Windows 10，后者是数亿人的首选操作系统。StatCounter 表示，Windows 10 的用户数占比为 69.35%，上个月下降了 2.27 个百分点。</p><p></p><h4>英伟达发布大语言模型，辅助芯片设计工作</h4><p></p><p></p><p>近日，英伟达推出了自家最新 430 亿参数大语言模型 ——ChipNeMo。对于它的用途，英伟达在官方披露消息中也是非常的明确，剑指 AI 芯片设计。</p><p></p><p>具体而言，ChipNeMo 可以帮助工作人员完成与芯片设计相关的任务，包括回答有关芯片设计的一般问题、总结 bug 文档，以及为 EDA 工具编写脚本等等。</p><p></p><p>英伟达首席科学家 Bill Dally 对此表示：“以英伟达 H100 Tensor Core GPU 为例，它由数百亿个晶体管组成，在显微镜下看着就像是一个精心规划建设的城市一般”。</p><p></p><p>这些晶体管连接在比人类头发丝还细 10000 倍的“街道”上，需要多个工程团队协作两年多的时间来完成，其间繁琐且庞大的工作量，可见一斑。</p><p></p><h4>微软宣布与西门子联手，将Copilot生成式人工智能引入制造业</h4><p></p><p></p><p>微软（Microsoft）和西门子（Siemens）宣布，双方计划围绕生成式人工智能（AI）及其在全球工业领域的应用深化合作关系。此举有望彻底改变人机协作，两家公司将推出西门子工业副驾（Siemens Industrial Copilot），这是一款共同开发的人工智能助手，旨在提高制造业的生产率。</p><p></p><h2>IT业界热评新闻</h2><p></p><p></p><h4>印度“IT业之父”要求年轻人每周工作70小时：不要从西方学到坏习惯，不帮助国家发展</h4><p></p><p></p><p>10月30日报道，英国首相苏纳克的岳父、财富超40亿美元的“印度IT业之父”穆尔蒂在一条视频中表示，“不知为何，印度的年轻人从西方学到了坏习惯，不帮助国家发展。”穆尔蒂称，2075年印度有望成为“世界第二大经济体”。为与中国等国家竞争，印度需要”意志坚定、纪律严明、工作勤奋”的年轻人。</p><p></p><p>“我要求年轻人必须说，‘这是我的国家，我想每周工作70个小时’。”国际劳工组织数据显示印度是工时最长的国家之一，每人每周平均工作47.7小时。</p><p></p><h4>字节宣布除夕统一放假、连放9天，不占用年假</h4><p></p><p></p><p>10月30日，网传字节跳动宣布除夕统一放假，其办公软件“飞书”日历上，已显示除夕当天为“春节团聚假”。</p><p></p><p>当日，记者就此传闻求证字节跳动员工，获知该消息属实。前述字节跳动员工表示，公司有关除夕放假的公告发布于10月30日13时许，公告中还指出：“其中除夕当天为公司额外提供的春节团聚假，不占用年假额度。”</p><p></p><p>10月25日，国务院办公厅发布关于2024年部分节假日安排的通知。其中，春节假期为2月10日至17日放假调休，共8天，2月4日（星期日）、2月18日（星期日）上班，即除夕不放假。通知中提出，鼓励各单位结合带薪年休假等制度落实，安排职工在除夕（2月9日）休息。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/xjAUuCeD556DgJwUDCOL</id>
            <title>字节跳动飞书技术 Leader 杨晶生，确认担任 QCon AI Agent 与行业融合应用的前景专题出品人</title>
            <link>https://www.infoq.cn/article/xjAUuCeD556DgJwUDCOL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/xjAUuCeD556DgJwUDCOL</guid>
            <pubDate></pubDate>
            <updated>Fri, 03 Nov 2023 07:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, AI Agent, 行业融合应用, 杨晶生
<br>
<br>
总结: QCon 全球软件开发大会将在上海举行，杨晶生将担任“AI Agent 与行业融合应用的前景”专题的出品人。在此次专题中，将介绍AI Agent的定义、应用以及与行业技术融合应用的发展前景。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1103&amp;utm_content=yangjingsheng">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。字节跳动飞书 技术 Leader 杨晶生将担任「<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1103&amp;utm_content=yangjingsheng">AI Agent 与行业融合应用的前景</a>"」的专题出品人。在此次专题中，你将了解到 AI Agent 是什么、AI Agent 的落地应用，以及与已有的行业技术融合应用的发展前景。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1103&amp;utm_content=yangjingsheng">杨晶生</a>"，目前就任于字节跳动，负责飞书音视频和人工智能相关产品的研发工作。曾就任于微软云计算部门和蜻蜓 FM。在十余年的工作中，经历了从服务器研发到云计算的过程，参与了超大规模全球化云服务架构、急速增长的内容平台业务、复杂而稳定性要求极高的企业服务等等项目，在服务高并发性能和稳定性方面有丰富的经验。</p><p></p><p>相信杨晶生的到来，可以帮助提升此专题的质量，让你学习到 AI Agent 能够感知环境、进行决策和执行动作，通常基于机器学习和人工智能技术，具备自主性和自适应性。同时，它已是公认大语言模型落地的有效方式之一，让更多人看清了大语言模型创业的方向，为未来技术融合应用提供了新思路。</p><p></p><p>除上述专题外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1599?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的大前端技术</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！现在购票，享 7 折优惠，立减￥2040！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/UPZWsozGefXCVeZxykpV</id>
            <title>火山引擎云安全解决方案负责人林扬确认出席FCon，分享金融企业如何构建安全云底座与合规能力？</title>
            <link>https://www.infoq.cn/article/UPZWsozGefXCVeZxykpV</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/UPZWsozGefXCVeZxykpV</guid>
            <pubDate></pubDate>
            <updated>Fri, 03 Nov 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: FCon 全球金融科技大会, 林扬, 金融企业如何构建安全云底座与合规能力, 云原生安全
<br>
<br>
总结: FCon 全球金融科技大会将在上海召开，林扬将发表题为《金融企业如何构建安全云底座与合规能力？》的主题分享。他将介绍金融行业安全现状、挑战和发展趋势，以及云原生安全解决方案。此次会议将涉及云原生安全最佳实践、隐私和数据安全合规实践、业内安全助力业务增长的方法，以及大模型安全风险评估要点。 </div>
                        <hr>
                    
                    <p><a href="https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle">FCon 全球金融科技大会</a>"，将于 11 月在上海召开。火山引擎云安全解决方案负责人林扬将发表题为《<a href="https://fcon.infoq.cn/2023/shanghai/presentation/5607?utm_source=infoqweb&amp;utm_medium=article">金融企业如何构建安全云底座与合规能力？</a>"》主题分享，介绍金融行业安全现状，挑战和发展趋势、安全技术框架建议，以及相关案例。</p><p></p><p><a href="https://fcon.infoq.cn/2023/shanghai/presentation/5607?utm_source=infoqweb&amp;utm_medium=article">林扬</a>"，20 年的网络安全从业经验，具备甲乙方安全经历和视角，担任过甲方 CISO，曾担任传统安全公司和互联网云公司解决方案负责人，擅长数字化和上云转型的安全规划和实践，长期深入研究各行业安全需求和问题，是国内行业网络安全攻防推演的专家。他在本次会议的演讲内容如下：</p><p></p><p>演讲：金融企业如何构建安全云底座与合规能力？</p><p></p><p>金融行业信息化和数字化一直走在国内前沿，对于科技风险、安全合规、攻防能力方面要求高，挑战巨大，火山引擎基于自身安全和风控的最佳实践，结合金融行业业务特点，为行业定制适合的安全技术框架和解决方案，云原生安全解决金融行业云基础架构底座安全问题，隐私计算保障金融行业数据共享交换的安全合规和业务价值增长，大模型安全帮助金融行业发现模型安全问题，提供大模型底座和模型算法安全的保障措施。</p><p></p><p>演讲提纲：</p><p></p><p>金融行业安全现状，挑战和发展趋势；金融行业安全技术框架建议；云原生安全：构建金融行业稳定弹性的云安全底座；数据安全：降低合规风险；隐私计算：助力金融行业数据合规互联互通，提升营销增长；大模型安全，帮助金融行业大模型探索提供安全保障；云原生、数据安全成功案例分析。</p><p></p><p>你将获得：</p><p></p><p>○ 了解云原生安全最佳实践和关键点；</p><p>○ 隐私和数据安全合规实践和可落地做法；</p><p>○ 了解业内安全助力业务增长的方法；</p><p>○ 了解大模型安全风险，备案所需的安全评估要点。</p><p></p><p>除上述演讲外，FCon 上海还将围绕&nbsp;<a href="https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle">DevOps&nbsp;在金融企业落地实践</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle">金融行业大模型应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle">创新的金融科技应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle">金融实时数据平台建设之路</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle">金融安全风险管控</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle">数据要素流通与数据合规</a>"等进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！现在购票，享 8 折优惠 ，立省 ￥1360！咨询购票请联系：17310043226（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/zLmWSSGfqzTAz0gzEG8r</id>
            <title>揭秘阿里核心引擎，走近阿里巴巴开源自研搜索引擎 Havenask</title>
            <link>https://www.infoq.cn/article/zLmWSSGfqzTAz0gzEG8r</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/zLmWSSGfqzTAz0gzEG8r</guid>
            <pubDate></pubDate>
            <updated>Thu, 02 Nov 2023 09:18:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 2023云栖大会, Havenask, 开源正式版本, 大规模分布式搜索引擎
<br>
<br>
总结: 2023云栖大会上，阿里巴巴发布了Havenask开源正式版本，Havenask是一款大规模分布式搜索引擎，主要用于智能搜索和海量数据实时检索。该搜索引擎在阿里巴巴内部的多个业务中得到广泛应用，如淘宝、天猫商品搜索，盒马搜索，菜鸟物流订单实时检索等。通过开源，团队希望能吸引更多开发者参与项目研发，共同创造更好的搜索引擎。 </div>
                        <hr>
                    
                    <p></p><h2>2023云栖大会，Havenask&nbsp;发布开源正式版本</h2><p></p><p>Havenask&nbsp;是阿里巴巴自主研发的大规模分布式搜索引擎，主要专注于智能搜索和海量数据实时检索，其核心能力广泛应用于阿里巴巴内部的众多业务，如淘宝、天猫商品搜索，盒马搜索，菜鸟物流订单实时检索等。</p><p></p><p>在11月1日云栖大会上，阿里巴巴智能引擎事业部云服务负责人&amp;&nbsp;Havenask&nbsp;开源项目负责人郭瑞杰博士，进行了&nbsp;Havenask&nbsp;开源正式版本发布演讲，并在演讲中介绍了&nbsp;Havenask&nbsp;最新开源进展与后续计划。</p><p></p><p>阿里巴巴智能引擎事业部高级技术专家徐希杰与魏子珺，则是在开源开放麦中为开发者详细阐述了&nbsp;Havenask&nbsp;正式版本的分布式能力细节与&nbsp;havenask-elasticsearch&nbsp;联邦项目最新进展，并结合各场景案例进行具体能力展示。</p><p></p><p>这是自在2022年云栖大会首发后，Havenask&nbsp;又一次亮相云栖，作为阿里巴巴自主研发的大规模分布式搜索引擎，Havenask&nbsp;承载着几代阿里搜索人的技术沉淀，团队期望在开源之后能有更多开发者加入项目研发中，一同联合共创。</p><p></p><p>本文将具体介绍&nbsp;Havenask&nbsp;的引擎架构、索引类型、查询语法、插件机制与运维管控能力，帮助广大开发者快速了解与上手。</p><p></p><p>Havenask&nbsp;开源项目地址：<a href="https://github.com/alibaba/havenask">https://github.com/alibaba/havenask</a>"</p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/ca1589c74e7f2a16d7763106e8953b51.jpeg" /></p><p>图1&nbsp;郭瑞杰博士在云栖大会《阿里云开源年度发布》中发布&nbsp;Havenask&nbsp;正式版</p><p></p><h2>Havenask&nbsp;简介</h2><p></p><p>Havenask&nbsp;底层全部采用&nbsp;C++&nbsp;实现，并经过多年的优化迭代，与其他的开源搜索引擎相比具有如下特点：</p><p></p><p>高性能：查询性能高，某些场景性能数倍于开源引擎。低成本：支持存算分离，冷热数据隔离等功能，海量数据场景下成本更低。时效性高：数据写入或者更新的时效性可以达到毫秒级。稳定性高：内存控制严格，没有其他开源引擎&nbsp;gc&nbsp;的问题，同时支持多机房互备具有更高的可靠性。索引类型丰富：支持&nbsp;kv、kkv、倒排、正排、摘要、向量多种索引类型。定制能力强：支持分词器、数据处理、query&nbsp;改写、算分、功能函数、等多种插件的定制。支持&nbsp;SQL&nbsp;语法：支持&nbsp;SQL&nbsp;查询，多表&nbsp;join，学习门槛低，业务迁移方便。</p><p></p><h2>Havenask&nbsp;架构</h2><p></p><p>Havenask&nbsp;引擎支持两种工作模式：读写分离模式（全量表模式）与读写统一模式（直写表模式），读写统一与读写分离的主要区别是是否有独立的索引构建服务，下面详细介绍。</p><p></p><h3>读写分离架构</h3><p></p><p><img src="https://static001.geekbang.org/infoq/33/33f354cadf938d9f305d845ffdf35a82.png" /></p><p>图2&nbsp;Havenask&nbsp;读写分离架构</p><p></p><p>Havenask&nbsp;读写分离架构如上图所示，在此模式下索引构建系统与在线检索系统是两个独立的子系统，可以分别进行资源的调整和集群的管理，索引构建系统和在线检索系统通过消息中间件和分布式文件系统进行索引的交互。读写分离架构适用于需要快速导入全量数据，需要定期进行索引重建，数据更新量较大，需要对离线资源进行独立控制等场景。</p><p></p><h4>索引构建系统</h4><p></p><p>索引构建系统是一个分布式的索引构建服务（build&nbsp;service，简称&nbsp;bs），每个索引构建服务都有一个或多个&nbsp;bs&nbsp;admin，bs&nbsp;admin&nbsp;负责管理表的索引构建流程，其中每个表可以有多个索引构建流程同时存在，相互之间无影响。</p><p></p><p>每个索引构建流程都由一组&nbsp;processor、builder、merger&nbsp;任务组成，其中&nbsp;processor&nbsp;负责原始数据的处理，比如分词，builder&nbsp;负责索引的构建，merger&nbsp;负责索引的整理。processor&nbsp;可以自由设置分片数，每个分片负责处理一部分数据，分片越多数据处理能力越强，使用的资源也越多。builder&nbsp;的分片数必须是索引表分片数的倍数，分片数越多索引构建越快。merger&nbsp;的分片数必须是索引表分片数的倍数，分片数越多索引整理越快。processor&nbsp;是一个常驻任务，builder&nbsp;和&nbsp;merger&nbsp;是交替执行的任务。</p><p></p><h4>索引构建流程</h4><p></p><p>读写分离架构下的表的数据索引构建由全量和实时两个阶段组成，同一个表可以同时存在多个索引构建流程，彼此之间无影响。全量阶段索引构建系统会将存在&nbsp;HDFS、MaxCompute&nbsp;或者&nbsp;OSS&nbsp;上的全量原始文件构建成全量索引，并订阅消息中间件回追一部分实时数据（防止全量切上线之后需要很长时间才能追上实时数据）。</p><p></p><p>索引构建过程中，processor&nbsp;会读取原始数据并对齐按照配置的数据处理规则进行处理，处理之后的数据会被发送到消息中间件（swift）。builder&nbsp;订阅消息中间件，将处理之后的文档构建成索引，并将索引产出到分布式文件系统。消息中间件中全量阶段处理的数据被&nbsp;builder&nbsp;全部处理完后，启动&nbsp;merger&nbsp;任务对索引进行整理，产出全量索引。</p><p></p><p>全量阶段完成后，全量索引会被切换到在线系统，同时索引构建也切换到增量阶段。增量阶段&nbsp;processor&nbsp;订阅消息中间件，处理实时数据，并将处理好的数据写回消息中间件。在线系统同时订阅消息中间件，获取处理之后的文件，直接在内存中构建成索引，这样数据就可以实时生效。builder&nbsp;也会订阅消息中间件，获取处理之后的文件，将其构建为增量索引，并由&nbsp;merger&nbsp;对全量索引和增量索引进行索引整理，产出最终可以切换上线的索引。增量索引切换上线之后，在线系统实时中的实时索引会被从内存中清理掉，释放的内存会被用于构建新的实时索引。</p><p></p><h4>在线系统</h4><p></p><p>在线系统用于加载索引并提供检索服务，它是一个支持多分片（shard），多备份（replica）部署的分布式服务，主要由&nbsp;admin，qrs（query&nbsp;result&nbsp;service）和&nbsp;searcher（数据节点）三种角色组成。Admin&nbsp;管理整个集群，实时监控各个节点的健康状态并调度，接收运维命令并向&nbsp;qrs&nbsp;和&nbsp;searcher&nbsp;下发运维指令。Qrs&nbsp;用于&nbsp;query&nbsp;的处理和结果的合并，qrs&nbsp;没有分片的概念，每个&nbsp;qrs&nbsp;节点都是同构的。Searcher&nbsp;节点加载索引，并真正执行查询任务，searcher&nbsp;节点上可以加载一个表的一个分片或者多个分片的数据。</p><p></p><p>在&nbsp;Havenask&nbsp;中用区间&nbsp;[0,&nbsp;65535]&nbsp;表示一份完整的数据，所有的数据分区键经过哈希之后都会映射到&nbsp;[0,&nbsp;65535]&nbsp;这个区间之内。在设置表的分片数之后，每个分片都会对应这个区间的一段范围，比如分片数为2，分成的两个分片对应的区间为&nbsp;[0,&nbsp;32767]&nbsp;和&nbsp;[32768,&nbsp;65535]。Havenask&nbsp;单分片最多可以承载21亿个文档，所以分片数越多，整个集群可以承载的数据量越多，最多支持65536个分片。</p><p></p><p>Qrs&nbsp;和&nbsp;searcher&nbsp;都可以通过扩充备份来提高整个集群每秒&nbsp;query&nbsp;处理能力（qps），不同的是&nbsp;qrs&nbsp;没有分片的概念，扩充备份就是扩&nbsp;qrs&nbsp;节点的个数，searcher&nbsp;是按照分片组织的，在每个&nbsp;searcher&nbsp;加载一个分片前提下，扩&nbsp;searcher&nbsp;的备份数就是扩分片数*备份数个&nbsp;searcher&nbsp;节点。</p><p></p><p>Havenask&nbsp;支持存算分离，searcher&nbsp;上的索引加载采用远端分布式文件系统、本地磁盘和内存三级索引加载策略，索引读取的性能也是按照远端、本地、内存这个顺序有数量级的提高。开发者在使用时可以综合考虑成本与查询耗时的要求，合理的配置索引加载策略。</p><p></p><h3>读写统一架构</h3><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/52/52f6f60c3dd206c897364edaf93fac75.png" /></p><p>图3&nbsp;Havenask&nbsp;读写统一架构</p><p></p><p>Havenask&nbsp;读写统一架构如上图所示，与读写分离架构相比有下面几个不同点：1）读写统一模式没有全量流程，所有的数据都要通过&nbsp;api&nbsp;以实时生效的方式推送到系统中；2）实时数据的推送直接写入到&nbsp;qrs，而不是&nbsp;swift&nbsp;中；3）每个分片对应&nbsp;searcher&nbsp;的&nbsp;leader&nbsp;进行索引整理，并将整理好的索引写入分布式系统中，follower&nbsp;加载整理好的索引。读写统一架构中使用&nbsp;swift&nbsp;进行&nbsp;wal&nbsp;以保证数据的安全，swift&nbsp;做&nbsp;wal&nbsp;的好处是不会随着&nbsp;searher&nbsp;备份的增多导致写的性能下降。读写统一架构比较适合频繁创建索引表、不需要全量数据导入、时效性要求高等场景。</p><p></p><h2>Havenask&nbsp;索引类型</h2><p></p><p>Havenask&nbsp;主要有三种类型的索引：倒排索引，正排索引与摘要索引。</p><p></p><h3>倒排索引</h3><p></p><p>倒排索引存储的是词（term）到包含词的文档（doc）的映射关系，可以通过倒排索引快速的检索到需要的候选文档，如下图所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8b/8b3effd4bff47aaec9fd247dc8d95758.png" /></p><p>图4&nbsp;Havenask&nbsp;倒排索引结构示意图</p><p></p><p>Havenask&nbsp;支持多种类型的倒排索引，比如主键索引，STRING&nbsp;索引（keyword，不分词），多字段&nbsp;PACK&nbsp;索引，数值范围索引，空间索引等，具体每个索引的含义开发者可以参考&nbsp;Havenask&nbsp;的用户手册。</p><p></p><p>向量索引（ANN）是一种特殊类型的倒排索引，它的索引结构与上图的通用倒排索引结构不同，具体的构建方式与索引结构和向量索引选择的算法有关。Havenask&nbsp;支持的向量索引有三种类型，线性索引、HNSW&nbsp;索引和聚类索引，可以满足不同场景下的向量检索需求。</p><p></p><h3>正排索引</h3><p></p><p>正排索引存储的是文档中字段的内容，主要用于对查询到的结果进行过滤、统计、排序等操作。Havenask&nbsp;的正排索引采用的是列存模式，即每个字段单独存储，如下图所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a9/a9837bb013557ea8cf81d2373021dc4f.png" /></p><p>图5&nbsp;Havenask&nbsp;正排索引结构示意图</p><p></p><h3>摘要索引</h3><p></p><p>摘要索引存储的是文档中需要返回的字段，可以对返回的字段进行飘红处理，如果字段内容较长可以使用摘要插件动态截取要返回的内容。Havenask&nbsp;的摘要索引采用的是行存模式，即一个文档的所有字段存在一起，如下图所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7c/7c6ff299924a36a3527d65e3b1533486.png" /></p><p>图6&nbsp;Havenask&nbsp;摘要索引结构示意图</p><p></p><h2>Havenask&nbsp;查询语法</h2><p></p><p>Havenask&nbsp;目前支持&nbsp;SQL&nbsp;查询语法，SQL&nbsp;语法简单易用并且便于扩展。但是具体到检索场景，SQL&nbsp;语法还是有些不足，比如查询时的多索引倒排搜索，粗排与精排的支持等。为了弥补搜索场景下&nbsp;SQL&nbsp;能力的不足，Havenask&nbsp;提供丰富的各种内置函数，比如提供了MATCHINDEX&nbsp;和&nbsp;QUERY&nbsp;等&nbsp;UDF&nbsp;支持倒排索引的查询，并支持自定义&nbsp;UDF。</p><p></p><h2>Havenask&nbsp;插件机制</h2><p></p><p>Havenask&nbsp;支持开发者自定义分析器插件、数据处理插件，各类&nbsp;UDF&nbsp;等以满足不同的业务需求。</p><p></p><p>分析器插件：开发者可以通过分析器插件定制自己的分析器，以满足不同的分词需求。分析器插件作用在数据处理分词阶段和查询时&nbsp;Query&nbsp;分词阶段。</p><p></p><p>数据处理插件：在构建索引之前，需要对文档进行处理（默认的是分词），开发者可以通过定制自己的数据处理插件提前对数据进行处理，比如可以集成一个向量化模型，在数据处理阶段将文本转为向量。</p><p></p><p>UDF：用户自定义函数，在查询时通过&nbsp;UDF&nbsp;可以定制自己的业务逻辑，比如外卖场景下，计算店铺和买家的距离。</p><p></p><p>对于各种定制插件，我们推荐开发者不要将插件代码单独编译成&nbsp;so&nbsp;的形式，而是与&nbsp;Havenask&nbsp;代码一起编成一个统一的&nbsp;binary，通过镜像的方式发布。</p><p></p><h2>Havenask&nbsp;运维管控</h2><p></p><p>Havenask&nbsp;的各个子系统（在线系统，索引构建系统，消息中间件）都有对应的&nbsp;admin&nbsp;角色进行集群的管理，每个&nbsp;admin&nbsp;都提供了运维管控的rpc接口。为了方便大家的使用，我们对这些&nbsp;rpc&nbsp;接口进行了封装，提供了方便使用的命令行工具&nbsp;hape。使用&nbsp;hape，开发者可以方便的启停集群，对表进行各种管理操作，如果需要更精细的运维控制，开发者可以直接调用&nbsp;admin&nbsp;提供的&nbsp;rpc&nbsp;接口。</p><p></p><h2>结语</h2><p></p><p>期望通过介绍，可以帮助开发者更好的了解与上手&nbsp;Havenask，我们欢迎广大开发者加入项目开发，共建高质量的搜索引擎。</p><p></p><p>此外，对于有使用需求的企业级开发者，我们也已在阿里云上提供了基于&nbsp;Havenask&nbsp;打造的全托管、免运维的一站式对话式搜索服务——阿里云&nbsp;OpenSearch，欢迎企业级开发者们试用体验。</p><p></p><p>Havenask&nbsp;开源项目地址：<a href="https://github.com/alibaba/havenask">https://github.com/alibaba/havenask</a>"</p><p></p><p>阿里云&nbsp;OpenSearch&nbsp;官网：<a href="https://www.aliyun.com/product/opensearch">https://www.aliyun.com/product/opensearch</a>"</p><p></p><p>欢迎钉钉扫码加入&nbsp;Havenask&nbsp;开源官方技术交流群：</p><p></p><p><img src="https://static001.geekbang.org/infoq/78/78c5cfa61c64a55cdeb0655ac7eb2849.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/V0ykFE4HYFlbNA0vbcE5</id>
            <title>仅凭7页PPT拿下1亿美元融资、半年后估值超10亿！“欧洲OpenAI”杀疯了</title>
            <link>https://www.infoq.cn/article/V0ykFE4HYFlbNA0vbcE5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/V0ykFE4HYFlbNA0vbcE5</guid>
            <pubDate></pubDate>
            <updated>Thu, 02 Nov 2023 07:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI 初创公司, Mistral AI, 融资, 大语言模型
<br>
<br>
总结: Mistral AI 是一家AI初创公司，通过7页PPT成功融资1亿美元，目前正在寻求3亿美元的新融资。该公司专注于开发大语言模型和各类AI技术，旨在解决现实世界问题。最近，他们发布了号称是“最强7B开源模型”的Mistral 7B，该模型在各项基准测试中表现优秀。 </div>
                        <hr>
                    
                    <p></p><blockquote>这家成立 4 周时就能凭借 7 页 PPT 融到超 1 亿美元的 AI 初创公司，究竟是什么来头？</blockquote><p></p><p></p><h2>AI 初创公司 Mistral 正寻求 3 亿美元新融资</h2><p></p><p>&nbsp;</p><p>据外媒报道，生成式 AI 初创公司 Mistral AI（常自称为“欧洲 OpenAI”）目前正寻求 3 亿美元新融资。如果一切顺利，那么新融资将帮助这家年轻企业估值突破 10 亿美元大关。</p><p>&nbsp;</p><p>据了解，Mistral AI&nbsp;总部位于法国巴黎，由来自 Meta Platforms 和 Alphabet 的几位前研究人员 Arthur Mensch（现任 CEO）、Guillaume Lample 和 Timothee Lacroix 共同创立，公司成立于 2023 年 5 月，专门开发大语言模型及各类 AI 技术。Mistral 这个名号来自北方寒冷的季风，也体现了他们想要在 AI 领域占据一席之地的愿望。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ae/ae7ee9ad22dc1fecd7c677caeb92b068.png" /></p><p></p><p>6 月，Mistral 在拿下 1.13 亿美元巨额种子融资后引发业界轰动，公司估值也瞬间来到 2.6 亿美元。彼时，该公司刚刚成立，员工仅 6 人，还未做出任何产品，仅仅凭借着&nbsp;7 页 PPT 就斩获了巨额融资。</p><p>&nbsp;</p><p>该轮融资由 Lightspeed Venture Partners 牵头，Redpoint、Index Ventures、Xavier Niel、德高控股以及意大利、德国、比利时和英国的其他知名风险投资公司参与。但该公司很快发现这“区区”1亿美元根本不够，要推动后续增长和扩张计划还需要更多资金的支持。</p><p>&nbsp;</p><p>据 The Information 近日报道，熟悉谈判内情的消息人士称，Mistral 正计划从投资者处额外筹集 3 亿美元，而此时距离由 Lightspeed Venture Partners 领投的种子轮融资才刚刚过去四个月。</p><p>&nbsp;</p><p>目前还不清楚 Mistral 已经与哪些风险投资商进行过通气，但根据另一位知情人士透露，生成式 AI 投资领域的重要参与者 Andreessen Horowitz 正在积极寻求向开源大语言模型（LLM）开发者注资的机会。如果能够顺利合作，自然不失为一件美事。</p><p></p><p><img src="https://static001.geekbang.org/infoq/62/621d9243b7262fef809f470c83814e33.png" /></p><p></p><p>Mistral 公司CEO、前 DeepMind 研究科学家 Mensch 表示，这家企业的使命是“打造出能够解决现实世界问题的下一代 AI 系统”。他同时补充称，新一轮融资将用于扩大团队、加快研发工作，以及在欧洲和美国建立新的办事处。</p><p>&nbsp;</p><p>Mistral 敢于开出如此夸张的融资数额，也体现出投资者对于 AI 初创企业不断增长的关注和信心。近年来，AI 初创公司已经筹得海量资金，其中不少企业正在开发前沿 AI 技术，有望彻底颠覆众多传统行业。</p><p>&nbsp;</p><p>但目前 Mistral 仍在起步阶段，能否成为 AI 领域的主要参与者仍然有待观察。尽管如此，该公司强大的初始团队和雄心勃勃的发展目标，已经使其成为当前乃至未来几年中最值得关注的 AI 初创力量之一。</p><p></p><h2>“最强 7B 开源模型”Mistral 7B</h2><p></p><p>&nbsp;</p><p>9 月 27 日，Mistral AI 团队发布了自家首个大模型&nbsp;Mistral 7B，该模型号称是“最强 7B 开源模型”。</p><p>&nbsp;</p><p>据介绍，Mistral 7B 是一套拥有 73 亿参数的大语言模型，采用 Apache 2.0 许可证，以不加限制的方式对外开放以供使用。在所有基准测试中，Mistral 7B 均优于 Llama 2 13B；在多种基准测试中，优于 Llama 1 34B；拥有比肩 CodeLlama 7B 的编码性能，并同时保持着良好的英语能力；使用分组查询注意力（GQA）来加快推理速度；使用滑动窗口注意力（SWA）以较低成本处理更长序列。</p><p>&nbsp;</p><p>GitHub 链接：<a href="https://github.com/mistralai/mistral-src">https://github.com/mistralai/mistral-src</a>"HuggingFace 链接：<a href="https://huggingface.co/mistralai">https://huggingface.co/mistralai</a>"</p><p>&nbsp;</p><p>Mistral 7B基础设施集群由 CoreWeave 提供 24/7 全天候支持，CINECA/EuroHPC 团队及 Leonardo 运营团队提供资源与帮助，FlashAttention、vLLM、xFormers、Skypilot 维护团队提供新功能以及方案集成指导。HuggingFace、AWS、GCP、Azure ML 团队协助实现了 Mistral 7B 的全平台兼容。</p><p>&nbsp;</p><p>Mistral 7B 还能针对任意任务进行轻松微调。Mistral AI 团队将 Mistral 7B 与 Llama 2 系列模型进行了比较，并重新运行了这些模型以验证评估结论是否准确。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8f/8f026b61c7d1d27868ee2ac3a51f7881.png" /></p><p></p><p>Mistral 7B 及各 Llama 模型在不同基准测试中的性能。这里列出的所有指标，均从&nbsp;Mistral AI 团队评估管道中的实际运行中采集而来，从而保证比较的真实性。Mistral 7B 在所有指标上均显著优于 Llama 2 13B，而且与 Llama 34B 基本相当（由于 Llama 2 34B 模型尚未发布，因此这里暂时与 Llama 34B 比较）。Mistral 7B 在编码与推理方面同样性能出众。</p><p>&nbsp;</p><p>本轮基准测试按主题可分为以下几类：</p><p>&nbsp;</p><p>常识推理: Hellaswag、Winogrande、PIQA、SIQA、OpenbookQA、ARC-Easy、ARCChallenge和CommonsenseQA&nbsp;的 0-shot 平均值;世界知识: NaturalQuestions&nbsp;和&nbsp;TriviaQA&nbsp;的 5-shot 平均值;阅读理解: BoolQ和&nbsp;QuAC&nbsp;的 0-shot 平均值;数学: mai@8的8-shot GSM8K 和 ma@4的4-shot MATH 的平均值;编码: 0-shot Humaneval&nbsp;和 3-shot MBPP 的平均值;热门聚合结果: 5-shot MMLU、3-shot BBH 和 3-5-shot AGI Eval (仅限英文多项选择题)。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/0b/0bf70ba717857e0b6dc38131fba14550.png" /></p><p></p><p>在对模型的成本/性能进行比较中，Mistral AI 团队提出了一个有趣的指标，即计算“等效模型大小”。在推理、理解与 STEM 推理（MMLU）方面，Mistral 7B 的性能与体量达到其 3 倍以上的 Llama 2 模型相当，意味着它能显著节约内存容量和数据吞吐量。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d7/d7d7373bf51fa51eb28f95fde31b38da.png" /></p><p></p><p>Mistral 7B 和 Llama 2（7B/13B/70B）的 MMLU 常识推理、世界知识与阅读理解比较结果。Mistral 7B 在绝大多数评估中均显著优于 Llama 2 13B，仅在知识基准测试中与后者处于同一水平（这可能是由于参数规模有限，因此掌握的知识量不足）。</p><p>&nbsp;</p><p>注意：此次评估与 Llama 2 论文之间存在以下区别：</p><p>&nbsp;</p><p>在 MBPP 测试中，这里使用了手工验证的子集。在 TriviaQA 测试中，这里未提供维基百科上下文。</p><p>&nbsp;</p><p>此外，Mistral 7B 使用滑动窗口注意力（SWA）机制，即每个层都关注之前的 4096 个隐藏状态。这里做出的主要改进以及尝试改进的原因，来自 O(sliding_window.seq_len) 的线性计算成本。具体来讲，在对 FlashAttention 和 xFormers 做出改进之后，成功在 16k 序列长度和 4k 上下文窗口下实现了速度倍增。Tri Dao 和 Daniel Haziza 为相关调整做出了贡献。</p><p>&nbsp;</p><p>滑动窗口注意力的原理，是利用&nbsp;Transformer&nbsp;的堆叠层来关注此前超出窗口大小的情形：第 k 层的 token&nbsp;i 关注第 k-1 层的 token&nbsp;[i-sliding_window, i]，后者又关注 [i-2*sliding_window, i]。如此一来，较高层就能访问到距离更“久远”的过往信息。</p><p><img src="https://static001.geekbang.org/infoq/12/12fa10bb65ec4544160b5d63edb4eca1.png" /></p><p></p><p>总之，采取固定注意力范围的最大意义，就是使用轮换缓冲区将缓存限制为&nbsp;sliding_window&nbsp;token 的大小（更多细节请查看参考实现<a href="https://github.com/mistralai/mistral-src">https://github.com/mistralai/mistral-src</a>"）。如此一来，同样在执行 8192 序列长度的推理时，可以节约下 50% 的高速缓存容量且不会影响模型质量。</p><p>&nbsp;</p><p>为了展示 Mistral 7B 模型的泛化能力，研究团队使用 HuggingFace 上的公开指令数据集对其进行了微调。不用问题集“作弊”、也不涉及专有数据，由此产生的 Mistral 7B Instruct 模型在 MT-Bench 测试中获得了优于一切同体量 7B 模型的性能，表现可与 13B 聊天模型相比肩。</p><p></p><p><img src="https://static001.geekbang.org/infoq/36/36c664d9c775b51093068b1f94782de0.png" /></p><p></p><p>快速演示的 Mistral 7B Instruct 模型能够轻松微调，进而带来引人注目的卓越性能。其中不涉及任何协调机制。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.theinformation.com/articles/mistral-a-wannabe-openai-of-europe-seeks-300-million">https://www.theinformation.com/articles/mistral-a-wannabe-openai-of-europe-seeks-300-million</a>"</p><p><a href="https://techstartups.com/2023/10/31/mistral-a-generative-ai-startup-aiming-to-be-europes-openai-seeks-300-million-in-new-funding/">https://techstartups.com/2023/10/31/mistral-a-generative-ai-startup-aiming-to-be-europes-openai-seeks-300-million-in-new-funding/</a>"</p><p><a href="https://mistral.ai/news/announcing-mistral-7b/">https://mistral.ai/news/announcing-mistral-7b/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/61OodB9eIKXe3S52SwkC</id>
            <title>中国电子学会主办 第四届 ATEC 科技精英赛报名启动</title>
            <link>https://www.infoq.cn/article/61OodB9eIKXe3S52SwkC</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/61OodB9eIKXe3S52SwkC</guid>
            <pubDate></pubDate>
            <updated>Thu, 02 Nov 2023 06:49:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: ATEC科技精英赛, 大模型应用与安全, 人工智能, 网络安全
<br>
<br>
总结: 中国电子学会主办的第四届ATEC科技精英赛（ATEC2023）已启动报名。该赛事旨在推动新一代人工智能发展和国家网络空间安全战略，培养人工智能及网络安全领域人才。本届赛事的主题是大模型应用与安全，旨在解决大模型技术的可用性、安全性和有效性等问题。赛事将围绕真实场景命题，培养青年科技人才的综合能力。赛事将于11月30日开始，共设4个赛道，奖金池共计146万元。 </div>
                        <hr>
                    
                    <p>11 月 1 日由中国电子学会主办的第四届 ATEC 科技精英赛（ATEC2023）正式启动报名。</p><p>&nbsp;</p><p><a href="https://www.infoq.cn/article/9I7jYdgQNrr0zBTXyyYV?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">ATEC</a>" 科技精英赛是主要面向中国籍计算机等专业在校学生、人工智能及网络安全行业研究者和从业者的一场高水平的智能科技挑战赛，意在贯彻落实党中央、国务院关于推动新一代人工智能发展的决策部署以及全面贯彻国家网络空间安全战略，构筑我国人工智能及网络安全发展的先发优势，推动人工智能及网络安全领域人才培养。</p><p>&nbsp;</p><p>本届大赛的主题为<a href="https://www.infoq.cn/article/Wigm8Jk2atzDYgo61JJC?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">大模型应用与安全</a>"。中国工程院院士、清华大学<a href="https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA%3D%3D&amp;chksm=fbe9a327cc9e2a31ca257bd26d377944080205ce1c81ba8c3714add4012efa5d07b1819982b3&amp;idx=3&amp;mid=2247490280&amp;scene=27&amp;sn=555602ee062f2055e8039a6289ed9cd7&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">人工智能</a>"研究院院长张尧学将担任 ATEC2023 指导委员会主席。52位来自境内外高校计算机领域的知名学者参与评审。ATEC 前沿科技探索社区作为赛事的承办单位。社区联合发起单位清华大学、浙江大学、西安交通大学、上海交通大学、蚂蚁集团，将联合北京大学、新加坡南洋理工大学等 13 所知名高校共同承担大赛命题、组织保障等相关工作。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f9/f90de9bc661be51213a6348812b72b46.png" /></p><p></p><p>人工智能大模型无疑已成为当下计算机领域最受关注的热点技术问题。但如今，大模型及相关技术的发展尚处于初期阶段，仍面临技术短板、隐私安全等问题。在大模型的落地应用过程中，如何平衡前沿技术探索和工业应用之间的隔阂，成为现实挑战。一方面，大模型开发、训练、运营等成本耗资巨大，有效提升资源利用率，成为大模型应用普及的关键；另一方面，在一些大模型的实际使用过程中，已被发现生成内容存在质量不佳、冗余回答等问题，甚至可能产生输出危害内容的问题，有效风险管理成为大模型合理合法应用的前提。</p><p>&nbsp;</p><p>ATEC2023 赛事将围绕大模型应用落地过程中的“可用性”、“安全性”、“有效性”等维度进行赛题设计，针对老年人科技服务等多个真实场景命题，探索大模型应用过程中的创新思路及解法。ATEC 前沿科技探索计划发起人、清华大学计算机系副系主任徐恪指出，“从预训练语料的安全标准，到内容产出物的安全评估，大模型不仅要服务用户，更要安全地服务好用户。重视和研究大模型在应用层面的安全可用问题，是大模型技术落地应用的必由之路。”</p><p>&nbsp;</p><p>作为业内实战型技术人才培养的旗帜性赛事。每一届 ATEC 科技精英赛都以还原真实工业场景、遴选具有社会价值的技术命题为出发点，遵循“以赛育人”的宗旨。ATEC2023 组织委员会主席、中国电子学会副秘书长曹学勤表示，“科技赛事是科技人才培养的有效途径，每年中国电子学会都将针对不同的技术方向组织各类技术赛事。大模型技术是未来计算机技术发展的关键。大模型技术的应用落地，依赖于人工智能、网络安全等多个技术领域的突破发展。借助大模型主题赛事及围绕真实场景的命题，能够很好地培养青年科技人才的综合能力。”</p><p>&nbsp;</p><p>据悉，本届赛事将于 11 月 30 日正式开始开赛。线上赛共设置 4 大赛道，涵盖大模型的知识引入、工具学习、AI 生成新闻检测、网络安全大模型等考点。大赛全程设有 3 大奖金池，共计奖金 146 万元，用以选拔及表彰领域内的优秀人才。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/wKVrNMN6GeeRaT5J6Z0p</id>
            <title>腾讯信息平台与服务线 CTO、PCG 事业群 AI 与推荐中台负责人徐羽，确认担任 QCon GenAI 和通用大模型应用探索专题出品人</title>
            <link>https://www.infoq.cn/article/wKVrNMN6GeeRaT5J6Z0p</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/wKVrNMN6GeeRaT5J6Z0p</guid>
            <pubDate></pubDate>
            <updated>Thu, 02 Nov 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, 徐羽, LLM, AI
<br>
<br>
总结: QCon 全球软件开发大会将在上海举行，徐羽将担任"GenAI 和通用大模型应用探索"专题的出品人。在此次专题中，将介绍LLM的最新进展、类型、能力、局限性和未来发展趋势。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1102&amp;utm_content=xuyu">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。腾讯信息平台与服务线 CTO、PCG 事业群 AI 与推荐中台负责人徐羽将担任「<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1102&amp;utm_content=xuyu">GenAI 和通用大模型应用探索</a>"」的专题出品人。在此次专题中，你将了解到在 LLM 正在迅速发展背景下的的最新进展，以及 LLM 的类型、能力、局限性和未来的发展趋势。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1102&amp;utm_content=xuyu">徐羽</a>"，2009 年加入腾讯，现任信息平台与服务线 CTO 兼总经理、PCG 推荐 AI 中台负责人。硕士毕业于加拿大滑铁卢大学电子与计算机工程系，加入腾讯前在加拿大黑莓公司工作 6 年，参与 BIS 手机邮件研发工作。从 2009 年开始负责手机 QQ 浏览器从 0 起步到现在亿级 DAU 规模的研发工作，在 2018 年建立和负责 PCG 的推荐 AI 中台，在机器学习平台、NLP、CV 视频理解、推荐算法和推荐架构等方面带领团队支持 QQ 浏览器和 PCG 业务的 AI 落地应用。</p><p></p><p>相信徐羽的到来，可以帮助提升此专题的质量，让你学习到，LLM 研究和开发的最新进展，了解不同类型的 LLM、其能力和局限性，以及 LLM 对未来工作、教育、医疗保健和许多其他领域的潜在影响。</p><p></p><p>除上述专题外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1599?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的大前端技术</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！现在购票，享 7 折优惠，立减￥2040 ！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/16/36/160539957f1fd1f4671722f1cab32a36.jpg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/WL2yVwKEqIutiwppz0wK</id>
            <title>AIGC 编程：代码编程模型的应用与挑战</title>
            <link>https://www.infoq.cn/article/WL2yVwKEqIutiwppz0wK</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/WL2yVwKEqIutiwppz0wK</guid>
            <pubDate></pubDate>
            <updated>Wed, 01 Nov 2023 07:50:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 生成式AI, 大型模型, 编码助手, 低代码平台
<br>
<br>
总结: 本文讨论了网易在生成式AI领域的应用，包括编码助手和低代码平台。大型模型帮助程序员编写代码是一项有价值的技术，但从商业角度来看并不一定有利可图。网易通过自研的方式，利用大型模型来更好地适应自身需求和场景，提供程序员更好的使用体验。在优化方面，网易注重将企业的专有知识融入到模型参数训练中，以使模型能够理解企业的专有领域知识，并通过优化提示工程提供更有信息量的上下文，以产生对程序员有价值的输出。 </div>
                        <hr>
                    
                    <p>嘉宾 | 鱼哲、刘东</p><p>编辑 | Tina</p><p>&nbsp;</p><p>生成式AI已经成为软件行业的一个重要推动力。在过去的一年里，包括网易在内的许多公司都在积极探索如何将这项技术应用到他们的产品中。如今，网易已经推出了多个生成式AI的实际应用产品，包括编码助手、大数据分析产品和低代码平台。在最新一期的“极客有约”对话节目中，鱼哲与网易杭州研究院人工智能部的算法负责人刘东一同探讨了有关大型模型产品的成本、速度和精确度等关键问题。</p><p></p><p>本文经编辑，原视频地址：https://www.infoq.cn/video/Wu1iSPABRu9NTVRrrywi</p><p>&nbsp;</p><p>亮点：</p><p>大型模型帮助程序员编写代码是一项很有价值的技术，但从商业角度来看，它并不一定是一个特别有利可图的生意。微调只是为了有针对性地增强它，使其更好地满足用户指令，所以首先需要基准模型能支持该任务。我们从算法的角度出发，努力构建了一个优秀的领域子模型，以尽量避免通用模型的幻觉问题。我们引入了一个"可信AI"的概念，包括过程可验证、用户可干预和产品可运营三个方向。从能力的角度来看，大语言模型已经展现出强大的表现，但我们需要根据投资回报率（ROI）来判断是否使用这些大型模型。</p><p>&nbsp;</p><p>嘉宾简介：</p><p>鱼哲，Lepton AI 创始团队成员，产品负责人。</p><p>刘东，网易杭州研究院人工智能专家，AI算法团队及产品团队负责人，专注于前沿算法研究与商业化应用。相关技术成果曾获浙江省科技进步奖一等奖。</p><p>&nbsp;</p><p></p><h4>AIGC在软件工程领域的应用方向</h4><p></p><p>&nbsp;</p><p>鱼哲：我个人认为Codex和Copilot等工具具有广阔前景，而AIGC也在广泛推广。刘东老师，您能否简要介绍一下，网易杭州研究院在AIGC技术以及软件工程领域的技术研究方向有哪些？</p><p>&nbsp;</p><p>刘东：关注AIGC，特别是在软件研发领域，我们认为它在各个环节都有实际价值。例如，在需求分析和设计方面，大型模型已经能够提供出色的设计建议。在编码和开发阶段，Copilot已经非常成熟，可以显著提高程序员的效率。在代码调试、分析和优化阶段，大型模型也能提供有益的优化建议，包括检测潜在的错误。甚至在测试阶段，我们也尝试使用大型模型生成测试案例。在运维环节，例如线上日志的实时分析和监控，也可以受益于大型模型的能力，提高效率。</p><p>&nbsp;</p><p>从应用的角度来看，我们目前在编码和开发阶段最快地实现了AIGC的应用。特别是我们内部为研发同事提供了类似Copilot的工具，已经看到效率有所提升。此外，我们还开发了一些外部商业化产品，如在BI产品中引入了对话功能，推出了ChatBI产品，以及在低代码产品中使用大型模型来加速低代码开发效率。</p><p>&nbsp;</p><p>鱼哲：关于Copilot，我之前在GitHub上也尝试过，还尝试过Code Llama。我想代表我们的观众逐一提出一些问题。首先，所有程序员都非常熟悉的一个问题是，我们花费很多时间思考如何分解代码的功能实现，以及编写和调试代码。特别是编写单元测试，在后端开发中经常需要，虽然我们不会讨厌，但有时候确实不太愿意做这项工作。</p><p>&nbsp;</p><p>然而，AI辅助编程作为一个产品，我看到市场上已经有很多竞品，比如GitHub的Copilot、OpenAI的Codex，以及AWS的Code Whisper等。有很多成熟的产品存在。我想问一下，为什么网易杭州研究院选择在这个领域开展工作？此外，微软在收购GitHub后表示支持Copilot业务，但据报道，这一业务目前亏本，因为Copilot对于开发者来说价格相对较高。微软在财报中也提到，他们每月支持一个开发者需要100美元，但实际只收取20美元。在这种情况下，您如何看待网易在这一领域的角色和作用？</p><p>&nbsp;</p><p>刘东：我们考虑这个问题时，有几个方面的考虑。首先，从安全的角度看，每家企业都有一些核心代码不愿意与外部共享，因此希望能够拥有相对可控的服务，并在其中使用大型模型以提高程序员的效率。因此，就可控性和安全性而言，自研可能是一个较好的选择。</p><p>&nbsp;</p><p>其次，每家企业都有大量的特定代码积累，而如何有效地利用这些代码，以在其业务中发挥价值，这也是一个重要问题。像Copilot这样的云服务通常比较通用，很难让企业将其自有代码集成进去并进行优化，以适应企业自身的习惯。</p><p>&nbsp;</p><p>因此，我们通过自研的方式，利用自己的模型来更好地适应网易的需求和场景，以提供程序员更好的使用体验。这是我们的出发点。</p><p>&nbsp;</p><p>至于亏本的问题，我也认为大型模型帮助程序员编写代码是一项很有价值的技术，但从商业角度来看，它并不一定是一个特别有利可图的生意。因为在这一领域，客户通常比较价格敏感，即使收费较低，用户也可能觉得价格昂贵。但从成本角度来看，确实需要较大的投入。因此，我们的考虑是综合各方因素来实施这项工作。</p><p>&nbsp;</p><p>鱼哲：除了安全性问题，您在网易情况可能会注意到一些特别适合的场景，不管是在电商还是游戏等许多场景中。您能否举一个具体的例子，展示我们在什么情境下通过自研的Copilot项目，更好地支持业务方编写其业务代码的案例呢？</p><p>&nbsp;</p><p>刘东：我们进行了大量的定制和优化，比如在游戏运营场景中，游戏经常需要举办各种活动。这些活动的方案通常由策划部门提出，要求程序员按照方案进行实现，但实现后可能代码只用一次，然后就不再使用。这种场景非常常见。</p><p>&nbsp;</p><p>但是，由于许多游戏之间存在相似性，企业的代码库中可能有很多人写过类似的代码，具有很大的参考价值。在这种情况下，如果使用通用的Copilot，它通常无法了解企业专有的代码信息，因此在这种场景下提供的提示效果可能不够理想。但如果我们进行企业定制，就可以通过一些增强的方式，将这些信息集成到提示中，然后将其提供给大型模型，使其能够参考这些代码来提供更好的提示。这样，我们就可以更好地实现降本增效。</p><p>&nbsp;</p><p>鱼哲：您刚才提到了在业务场景中进行了许多优化，特别是在模型层面。您能详细介绍一下优化工作是在模型层面进行的，还是在输送给模型的提示工程这一层进行的？或者说，在模型的不同方面都进行了优化，可以谈谈具体的优化思路是什么吗？</p><p>&nbsp;</p><p>刘东：我们的优化思路主要围绕两个关键点展开，以发挥大型模型的价值。第一个关键点是确保模型本身的强大性，这涉及将企业的专有知识融入到模型参数训练中，以使模型能够理解企业的专有领域知识。</p><p>&nbsp;</p><p>第二个关键点是优化提示工程，即我们如何提供给模型更有信息量的上下文，以便模型更好地理解上下文，产生对程序员有价值的输出。我们发现，仅仅将当前代码片段的上文或下文提供给模型并让其继续生成，效果通常一般。因此，我们考虑了编程过程中的各种信息来源，包括引入的外部第三方库、工程中的其他项目文件、类似的工程项目，甚至程序员在编程过程中浏览和检索网页、查找答案以及执行粘贴和复制等操作。这些行为都是宝贵的提示信息，我们通过将这些信息融入到模型的提示中，帮助模型更好地理解当前的上下文，从而更好地输出对程序员有价值的信息。这些工作使我们的模型能够更好地与业务结合，提供更好的效果。</p><p>&nbsp;</p><p>鱼哲：您提到了模型微调，确实，在Google和其他地方，人们一直在进行对模型的微调。通过微调一个基础模型，将其完全适配到特定任务，这是一个非常有效的方法。许多人认为，只要有基本模型，然后进行一些微调，就可以将其应用到任何任务上，使其成为该任务的专家。您对这个问题是怎么看的？</p><p>&nbsp;</p><p>刘东：如果我要进行微调，内部除了我们自己训练的基础模型，还有许多开源的基准模型可供使用。我们进行了大量的评估，具体思路是，如果要进行微调，首先要分析基准模型是否足够强大，是否在具体任务上已经表现得相当不错，微调只是为了有针对性地增强它，使其更好地满足用户指令。如果基准模型根本不支持该任务，仍然强行进行微调的话，效果可能不太好，或者可能需要寻找一种成本更高的微调方式，类似于继续进行预训练，以将知识融入模型，然后再进行微调，例如LORA微调。我认为LORA微调可能只对现有的基准模型进行提升和补充有意义。当然，如果基准模型本来就不太好，那么可能不会获得太大的收益，或者预期的性能可能不会特别出色。</p><p>&nbsp;</p><p></p><h4>成本问题</h4><p></p><p>&nbsp;</p><p>鱼哲：您提到了成本问题，确实，在特别是语言模型（LM）这个领域，模型的推理成本随着参数数量的增加呈指数级增长。我们了解到，网易内部使用Copilot不仅仅涉及生成代码，还可能涉及解释代码推理方面。用户可能以多种不同的方式使用它。您是使用一个巨大的模型或者一个具有固定参数的模型来支持所有使用方式，还是根据不同的使用方式智能地调整背后模型的大小呢？</p><p>&nbsp;</p><p>刘东：我们选择了后者的方式，即根据不同的使用方式智能地调整背后模型的大小，这是有充分考虑的。从成本和效率的角度考虑，这是一个综合的决策。特别是在编程场景中，代码提示是一个非常高频的任务，因为每输入几个字母，都会触发一次提示请求。在这种情况下，模型需要足够快，因为如果太慢，程序员可能会自己完成输入，这样就不会提供太多价值。</p><p>&nbsp;</p><p>另一方面，这个场景通常涉及到代码生成，相对来说是一个相对固定且不太复杂的任务，与通用任务相比，难度较低。因此，我们更倾向于选择规模较小的模型，以确保效率，并且不会明显降低质量。</p><p>&nbsp;</p><p>对于像代码解释、调试分析或注释生成这样的任务，难度较大，可能需要更大的模型才能实现良好的效果。但好的一点是，这些任务通常不会有太高的使用频率，因此在这种情况下，我们可以选择相对更大的模型，而不需要进行大规模的冗余部署，因为请求量本来就不会太大。这种综合考虑帮助我们更好地控制了成本。</p><p>&nbsp;</p><p>鱼哲：随着大型语言模型的发展，特别是像Copilot这样的方式，例如像LLAMA这种模型，我们是否仍然需要手动编写注释呢？大家讨厌别人不写注释，但自己也不喜欢写。</p><p>&nbsp;</p><p>刘东：写注释与不写注释在很大程度上是一个习惯问题。写注释的主要目的首先是为了给自己提供提示，使代码更容易理解和维护。其次，注释也有助于他人理解代码，尽管注释的覆盖度要求可能并不高，因为大模型可以帮助填充一些细节。然而，写注释的程度可以因程序员而异，有些程序员可能倾向于写详尽的注释，解释每个细节，而有些人可能只写简要的概述性注释。这与个体的写作风格和代码质量意识有关。</p><p>&nbsp;</p><p></p><h4>BI产品和低代码平台</h4><p></p><p>&nbsp;</p><p>鱼哲：在网易杭州研究院，我们不仅在内部广泛应用这些先进的技术，还在一些领域提供对外的技术支持和合作机会。在对外方面有哪些技术合作呢？</p><p>&nbsp;</p><p>刘东：除了为内部提供技术支持，我们还将这些大语言模型的能力整合到商业化产品中，以为客户提供更多的服务。其中，我们的代表性产品之一是BI产品。通过整合大语言模型的能力，为BI产品引入了自然语言交互功能，使用户可以通过自然语言查询所需的数据和报表，这完全是由大语言模型驱动的。</p><p>&nbsp;</p><p>另一个重点领域是低代码平台，我们的CodeWave平台，它通过低代码编程方式，降低了编程的门槛，提高了编程的效率，从而帮助企业节省成本并提高效率。在这个平台中，我们引入了大语言模型的能力，以提高效率和降低编程门槛。这两个领域是我们当前主要投入和发展的方向。</p><p>&nbsp;</p><p>鱼哲：我们还有一个低代码产品，可以介绍下这个产品的使用体验吗？</p><p>&nbsp;</p><p>刘东：Low code 不同于 0 code，简而言之，它是一种基于可拖放的方式进行软件开发的方法。它不要求专业的程序员从头编写代码，也不同于完全无需编码的 0 code 方式。在低代码中，你可能需要配置一些固定的模板，定义数据模型，设计流程结构，还可以使用预定义的组件，通过拖拽的方式连接各种逻辑，最终生成软件产品。</p><p>&nbsp;</p><p>这种方法的核心优势在于相对于传统的完全编码软件开发，用户需求较低，无需像计算机专业的本科毕业生或有丰富经验的人才那样写代码。但与 0 code方法相比，它仍然保持了软件开发的灵活性，因为它可以实现复杂的逻辑。低代码的定位介于传统软件开发和 0 code 之间，兼顾了易用性，同时也能满足一些较为复杂的软件开发需求。</p><p>&nbsp;</p><p>鱼哲：您能简单介绍下这个产品的对外发布节奏吗？我看到官网上有些资料相关。</p><p>&nbsp;</p><p>刘东：我们目前在网易数帆官方网站上提供了一些基础材料和介绍。此外，我们即将在11月2日举行2023网易数字+大会，届时将提供更详细的产品介绍以及有关技术的分享。我们期待在发布会上与大家分享更多信息。</p><p>&nbsp;</p><p></p><h4>AIGC在数据分析应用上的挑战</h4><p></p><p>&nbsp;</p><p>鱼哲：回到刚才提到的ChatBI，我在以前做业务时常常需要与BI同事沟通，例如我想了解最近三个月华北地区哪个行业的客户增长最快，哪个行业的客户有一些困难，以及他们所遇到的产品使用情况。这种情况通常需要等待一两天的时间，不管是BI同事还是我自己去做，都需要花费大量的时间来查看数据地图，查看每个表的结构以及做相关的SQL查询，因为我们需要定义特定的指标，例如复购、沉默和活跃等。这是一个非常复杂的问题，之前尝试了许多模型，但它们存在幻觉的问题，导致了一些错误的结果，这是不能接受的。在BI领域，这个问题是非常严肃的，我们不能容忍存在幻觉的问题。我想了解一下，你们是如何处理这个问题，如何解决这种复杂性挑战的。</p><p>&nbsp;</p><p>刘东：我们面对的确实是一个巨大的挑战，而且我们在这个产品上花费了很长时间，因为BI场景是非常严肃的，它的任务是提供准确的信息。如果我们只是编造数据或者输出不可信的信息，那这个任务基本上就失败了。因此，我们采取了多重方法来尽量避免这个问题。</p><p>&nbsp;</p><p>首先，我们从算法的角度出发，努力构建了一个优秀的领域子模型，以尽量避免通用模型的幻觉问题。我们收集了大量的数据和各行各业的常见数据报表，通过数据增强和训练，使模型的能力更强。这个领域子模型专注于解决数据分析场景，能够通过自然语言输入生成高质量的SQL查询语句。</p><p>&nbsp;</p><p>其次，尽管模型很强大，但我们也意识到大型生成式模型不是100%可控的，因此我们在产品层面进行了多方面的工作。我们引入了一个"可信AI"的概念，包括过程可验证、用户可干预和产品可运营三个方向。</p><p>&nbsp;</p><p>过程可验证：我们不仅仅相信模型生成的SQL查询语句，而是使用一个查询语句解析引擎将其解析为人类可理解的语言，以确保用户了解模型的工作原理。如果发现错误，用户可以立即识别并不信任结果。用户可干预：我们允许用户对模型生成的查询进行干预。用户可以更改条件、操作等，以纠正错误或调整查询。这提供了用户对结果的额外控制。产品可运营：我们希望产品不仅仅是一个静态的工具，而是能够随着用户的使用变得更智能。我们收集用户的行为习惯，正例和反例，不断优化模型。我们也提供产品配置，以使模型理解各行各业的“黑话”和简称。通过不断的运营，使模型越来越智能，适应用户的需求。</p><p>&nbsp;</p><p>这些方法的结合，以及其他细节的优化，使我们的产品更可信、可控，提高了用户的工作效率。</p><p>&nbsp;</p><p>鱼哲：这是否意味着当用户使用产品时，他们需要在某种程度上提前注入表结构的一些信息？或者说，模型能够根据表的结构自行猜测字段的含义？</p><p>&nbsp;</p><p>刘东：大模型是通过自主猜测的。只要提供底层表结构，大模型可以自动获取这些信息，所以用户在一开始使用时通常不需要太多干预。</p><p>&nbsp;</p><p>鱼哲：您刚刚提到的这个反馈收集非常有趣，因为通过良好的RLHF方法，模型的性能可以显著提高。</p><p>&nbsp;</p><p>刘东：是的，必须逐步将其系统运营，使其随着使用而不断智能化，而不是采取一劳永逸的方式。这样做的话，问题通常不会被永久解决。但一旦将其运营起来，将负面案例的反馈馈送给它，它就会不断改进。</p><p>&nbsp;</p><p>鱼哲：刚才有个直播观众的提问：“对于这些垂直领域的模型，你们是在基础大模型的基础上进行微调，还是持续进行预训练，或者是从零开始使用领域样本训练参数较小的模型？”</p><p>&nbsp;</p><p>刘东：我们通常是基于基础的基座模型进行调整。网易内部我们已经进行了基础模型的玉言，这是一个从头开始训练的基座模型。从头开始训练的好处是，我们大致了解未来要覆盖的领域，因此在训练过程中，我们有意地将一些领域相关的数据融入其中。例如，如果要处理编程任务，就会注重将代码相关的数据纳入模型。如果要处理SQL的任务，就会加入一些SQL的数据。这个基座模型相对来说比较通用。然后，我们会在这个基础上为每个领域创建领域特定的子模型，以进行适配。</p><p>&nbsp;</p><p></p><h4>数据的重要性</h4><p></p><p>&nbsp;</p><p>鱼哲：您提到的ChatBI的问题，如果拥有一个基础模型并为其创建功能，同时提供大量数据时，我发现在这个工作中，最大的挑战实际上不在于微调模型，而是在于找到合适的数据，并将其准备成可供模型使用的形式。我认为这是最困难的部分。</p><p>&nbsp;</p><p>在研究一篇论文时，我注意到在数据稀缺的情况下，他们提出了一个新名词叫RLAIF，即通过人工智能来生成强化学习所需的数据，以支持强化学习任务。</p><p>&nbsp;</p><p>对于像ChatBI这样的项目，我认为您需要大量的数据来对基础模型进行调整，而且需要具备高度的语义和推理能力。模型的规模不会小，而随着模型规模的增加，调整参数需要更多的精力、计算资源和数据。</p><p>&nbsp;</p><p>我想了解一下，您是如何在数据准备方面处理这些挑战的？因为实际情况是，在这类项目启动之初，数据通常不够整洁，或者很多人最初并不清楚这些数据可能会有哪些用途。您是如何处理这一问题的呢？</p><p>&nbsp;</p><p>刘东：无论是在ChatBI领域还是在以前的代码自动补全项目中，数据准备工作都是至关重要的，也是相当具有挑战性的任务。我们投入了大量精力来应对这个挑战。</p><p>&nbsp;</p><p>在ChatBI项目，我们获得数据的途径多种多样。首先，我们会在网上寻找一些开源数据。在这个领域，因为传统方法已经发展了相当长的时间，所以存在许多开源的评测数据，以及公开数据表结构的定义。我们可以利用这些表结构，以人工智能的方式自动生成问题和答案，从而使用AI来生成数据，这是一种当前相对流行的方法。此外，我们还投入了大量人力资源来进行数据的搜集和标注工作，将各个来源的数据汇总，综合使用，以满足我们的需求。</p><p>&nbsp;</p><p>鱼哲：我觉得这个趋势在从事NLP领域的同事中也非常明显。人们开始广泛使用语言模型来执行以前需要使用多个专门的小模型来完成的任务。举例来说，以前我们需要训练专门的模型来执行诸如语音到文字转换、地址解析以及标点符号分割等任务。而现在，像您刚才提到的，在生成数据方面也使用了语言模型。这引发了一个问题，即您是否认为大型语言模型会逐渐取代NLP领域中使用的多个小型专家模型呢？</p><p>&nbsp;</p><p>刘东：从能力的角度来看，大语言模型已经展现出强大的表现，但我们需要根据投资回报率（ROI）来判断是否使用这些大型模型。这意味着，尽管它们非常有能力，但我们不必在每个场景中都采用大语言模型。例如，对于一些小型NLP任务，我们可以使用较小的模型，它们成本低廉，在线上表现良好，同时可以满足高并发需求，因此在这些情况下，不必迫切转向大型语言模型。</p><p>&nbsp;</p><p>当然，大语言模型的能力毋庸置疑，它们在某些复杂任务上可能表现更出色。然而，我认为大语言模型与领域专家模型之间不是相互替代的关系，而是可以共存的。在不同的情境中，可以选择使用不同的模型，以便最好地满足特定需求。这种差异化的方法可能是更好的选择。</p><p>&nbsp;</p><p></p><h4>模型选择的问题</h4><p></p><p>&nbsp;</p><p>鱼哲：因为我之前有时也会采取简便的方式，直接使用大型模型，特别是在拥有免费积分的情况下。但随着时间的推移，我发现在考虑长期投资回报时，仍需要寻找传统的常规模型。</p><p>&nbsp;</p><p>刘东：在ChatBI中，除了大型模型之外，有时需要将小型模型和大型模型结合使用。这是因为尽管大型模型拥有出色的能力，但它在成本和执行速度上可能存在一些问题。小型模型则执行速度非常快。在某些任务中，如果你不断地调用大型模型来执行和解析，可能会影响用户体验，因此需要进行综合考虑。</p><p>&nbsp;</p><p>鱼哲：您刚刚也提到了，一方面，生代码生成式大模型或ChatBI生成式大模型等，都在数据收集方面面临巨大挑战。我认为数据是其中的一个技术挑战。除了数据之外，在这个过程中还有哪些方面您认为非常具有挑战性，非常难的呢？</p><p>&nbsp;</p><p>刘东：我认为数据确实是一个巨大的挑战，不仅在收集方面，还在清洗数据方面需要耗费大量精力。清洗数据是非常关键的，因为如果不做好，会直接影响模型的效果。我们在数据清洗方面投入了大量精力，因为高质量的数据是确保模型效果的基本保障，这是第一个挑战。</p><p>&nbsp;</p><p>另一个挑战是一旦大模型的能力达到足够强，如何在实际业务场景中找到合适的应用场景，确保它能够创造价值。这方面也非常具有挑战性，因为大模型面临效率、速度和成本等问题。虽然它在很多场景下效果出色，但用户体验可能难以保证。此外，大模型作为生成模型，不可能百分之百准确。如何找到那些既能容忍错误，同时又能为用户带来实际帮助的场景，让模型成功落地，也是一个非常大的挑战。</p><p>&nbsp;</p><p>总之，技术本身的能力与业务场景的结合是非常关键的，只有找到合适的结合点，大模型的能力才能真正发挥作用，用户才能真正感受到其价值。否则，它将一直停留在演示的层面，其技术的价值和影响力都会受到限制。</p><p>&nbsp;</p><p>鱼哲：有时候出现了上下文的误解。例如，用户可能要求搜索最近三个月内是否有玩过某个游戏，但可能会被错误地理解为最近三年。在ChatBI场景中，我们可能有一个SQL生成工具，但它生成的SQL语句缺少一个关键的"where"子句。现在，关于ChatBI，是它能够接受用户的自然语言查询并自动触发查询任务，还是它只返回SQL代码，用户需要将SQL代码用于传统的数据仓库查询窗口中查询？</p><p>&nbsp;</p><p>刘东：我们的当前设计是完全自动的。当用户提供一个查询后，系统会立即执行，而且像之前提到的那样，各种AI操作都会在执行后进行解释，并展示各种条件。用户可以根据查询结果和这些解释来判断查询的可靠性。</p><p>&nbsp;</p><p></p><h4>大模型产品价值的体现</h4><p></p><p>&nbsp;</p><p>鱼哲：所以用户不仅可以看到生成的代码，还能了解为什么会生成这段代码。此外，生成的数据会以表格的形式展示，用户可以导出数据。产品是否还提供可视化或建模分析能力，还是用户需要自己去处理这些方面的工作？</p><p>&nbsp;</p><p>刘东：我们目前主要提供可视化展示，对于后续的建模分析，我们正在进行研究和探索。</p><p>&nbsp;</p><p>鱼哲：您之前提到技术研究院需要同时具备技术和业务的理解，而最终需要为其结果负责。客户，无论是网易集团内部还是外部，都渴望了解这些生产力工具如何提升效率。这包括自动代码生成、SQL自动生成以及低代码平台等技术，它们都旨在提高生产力。然而，生产力提升在实际中往往是一个具有挑战性的问题。难以证明这些技术是否可以直接提高生产力。关于如何证明生产力提升的问题，您是怎么解决的？</p><p>&nbsp;</p><p>刘东：我们一直在思考和探索这个问题，因为要传达技术的价值，需要从多个角度来考虑如何证明其价值。我们非常关注用户的反馈和实际使用数据，这对于衡量技术的有效性至关重要。</p><p>&nbsp;</p><p>在我们内部使用低代码工具，例如代码补全工具，类似于Copilot工具，我们详细记录用户的使用情况，特别关注用户采纳提示的比例以及AI自动生成代码的比例。这有助于我们了解技术是否真正帮助用户减少编码工作，还是只是一个演示性的工具，用户不太愿意采纳其中的建议。</p><p>&nbsp;</p><p>同样，对于ChatBI和低代码工具，我们也密切关注用户的使用情况。例如，如果没有Chat功能，很多业务人员可能无法自行使用BI工具来查询数据。但如果引入Chat功能，我们关心是否有人在使用，以及他们的使用频率。在低代码工具方面，我们使用自然语言来生成逻辑，然后观察生成情况和占比。这些数据帮助我们衡量技术是否真正提高了生产力，帮助用户在成本降低和效率提高方面取得进展。我们非常关注这些方面的数据。</p><p>&nbsp;</p><p>鱼哲：当我们致力于提高生产力效率时，如AIGC或大型模型的出现与以往的机械发明有很大不同。以前的机械或半自动机械本质上需要人的操作。例如，使用除草器可能需要有人操作设备，而自动化机械则需要人的干预。然而，像您刚才提到的ChatBI，如果今天我可以使用自然语言描述业务需求，然后它可以为我生成正确的SQL查询并检索数据。那么对于那些传统的数据分析专业人员或BI同行，他们的存在可能会面临一些挑战，因为这种技术的出现可能改变了传统的数据分析方法。</p><p>&nbsp;</p><p>刘东：我认为这些工具主要是作为一种助力的角色存在的，而人的价值仍然不可或缺。无论是在BI领域还是在编码领域，它们的核心目的是帮助人们在一些简单重复的工作中提高效率。当拥有这种提高效率的工具时，我认为人们可以解放更多的时间，用于思考业务等更有价值的事情。这包括如何改进业务、更好地理解用户需求，以及如何提供更出色的软件产品。这是一种从不同角度思考问题的方式，而不是完全取代人的角色。因为这些技术目前正在逐渐发展，它们还没有达到100%可信并且能够胜任一切的状态。</p><p>&nbsp;</p><p>鱼哲：我听闻有些公司在内部开展了类似工作遇到了许多障碍。其中一部分障碍来自于开发团队，他们担心这种工具可能会与他们的工作发生冲突，或者一旦工具成熟，公司将不再需要他们。在你们尝试推广这些工具的过程中，是否也遇到了类似的挑战？</p><p>&nbsp;</p><p>刘东：我们目前并没有遇到这类挑战，因为我们进行了一些统计，发现从软件研发的角度来看，程序员实际花在编码上的时间并不占很大比例。编码只是他们工作的一小部分，更多的工作包括需求分析、整体设计以及与其他方面的对接等。编码所占的时间并不是很多。我们的目标并不是取代程序员，而是让他们能够更多地投入需求分析和用户场景理解，以便提高编码的质量和整个软件产品的效果。</p><p>&nbsp;</p><p></p><h4>大模型产品的私有化部署</h4><p></p><p>&nbsp;</p><p>鱼哲：我觉得程序员这个称呼有点狭隘，因为在编写代码时，实际上他们不仅仅在写代码，更多的时候在进行工程工作，也就是做工程师的事情。工程师通常需要将来自外部的各种复杂难以理解的需求和分散的模块整合到一起。在这方面，生成式模型的能力是不可替代的，不论是在短期还是长期。我认为很难通过直接应用一个大型模型来完成需要花费多年时间理解和深入了解的业务。这种情况下，使用生成式模型可能不太适用，因为你需要时间来积累对业务的深刻理解，然后才能进行创新性的工程工作。</p><p>&nbsp;</p><p>你刚刚描述的这些能力听起来确实非常有趣和具有吸引力。然而，我们也明白，许多公司，包括像网易自己开发Copilot时，通常出于安全和特殊场景的考虑，不愿意使用市场上通用的产品。这种担忧在中国和美国的科技公司中都非常普遍，特别是当公司规模较大时，它们通常更倾向于采用私有化部署，无论是在公司自己的数据中心还是在云上的IDC。</p><p>&nbsp;</p><p>在传统金融领域，如银行、保险和证券等领域，这些能力可以显著提高工作效率和效能。然而，由于国家监管要求或公司性质的原因，很多银行和保险公司通常需要确保技术提供的方式支持私有化部署。我们会积极考虑这些需求，以满足不同客户的特殊要求。网易在这方面是如何考虑的？</p><p>&nbsp;</p><p>刘东：我们的模型都是基于自己的基座模型调用的，因此完全可控。我们也提供了私有化部署的能力。除了不断优化模型性能，我们还专门组建了一个工程团队，专注于推理效率的优化、部署方案的设计以及各种硬件适配工作。</p><p>&nbsp;</p><p>在私有化部署方面，我们考虑如何尽量降低用户的成本，因为大型模型的成本相当高。首先，我们根据业务场景找到了适合的模型规模，而不是盲目地追求巨大的规模，这样可以减少硬件集群的复杂性。</p><p>&nbsp;</p><p>其次，我们进行了大量的工程优化。这包括引入业界开源的先进技术，以提高性能。我们还根据模型的特点进行了自定义适配，包括自定义内核等，以提高吞吐量和效率。</p><p>&nbsp;</p><p>此外，我们还考虑了量化加速等操作，以确保资源的可控性。因此，即使使用普通的显卡，我们也可以将大型模型部署上，并为用户提供良好的体验。这些措施都有助于提供高效的私有化部署解决方案。</p><p>&nbsp;</p><p>鱼哲：在私有化部署方面，你们是可以进行多方面的性能优化的吧？比如模型量化、压缩以及重新编写一些核心代码，从而提高性能和效率。在私有化部署的时候，一种方式是用户直接使用基座模型，这是一个即插即用的解决方案。另一种是用户在使用一段时间后，需要进行自定义微调，比如强化学习微调（RLHF）或自适应微调（RLAIF）等，你们是如何解决的？</p><p>&nbsp;</p><p>刘东：我们提供两种服务模式，以满足用户的需求。首先，我们提供一种自助工具模式，类似于业界已有的微调工具和强化学习工具。这些工具包括数据集管理、标注、训练任务和部署任务等功能。用户如果拥有足够的实力和理解相关流程，可以自行使用这些工具来满足其需求。用户可以随时尝试不同的方法，进行A/B测试，以确定哪种方式效果更佳。</p><p>&nbsp;</p><p>其次，对于一些重要客户，我们也提供定制化的服务。在这种情况下，我们的算法专家会与客户合作，共同解决他们特定的问题。我们会与客户密切合作，确保他们获得最佳的解决方案。无论用户自行使用工具还是选择我们的定制化服务，我们都将竭诚为他们提供支持。</p><p>&nbsp;</p><p>鱼哲：我也认为在私有化部署后再进行模型训练是一项颇具挑战的任务。从前我在这个领域有一段时间的从业经验，我深知这种工作需要投入大量时间和精力。此外，私有化部署环境通常存在标准不一致的问题，因此需要耗费额外的时间来解决各种复杂情况。</p><p>&nbsp;</p><p>刘东：每个客户的环境都有所不同，因此我们的目标是将产品尽量标准化，同时确保工具功能完善。这种方法有助于客户自行进行调整、训练和部署，降低了使用成本和门槛。只要他们理解这个过程，几乎都可以通过简单的点击鼠标来完成，而不需要深入编程或处理复杂的问题，这对用户来说更加容易接受和理解。</p><p>&nbsp;</p><p>鱼哲：在应用 AIGC 技术能力来提高软件工程效率时，您认为在业务端的落地过程中，最关键的角色通常会是什么？</p><p>&nbsp;</p><p>刘东：我认为在这个过程的每个环节都非常关键，没有哪一个环节可以忽略。首先，理解场景和找到使大模型落地的有价值的点至关重要。然后，需要探索这些点，以确定大模型的能力是否足够可控和可解决。如果算法可以解决问题，那么从工程角度来看，是否有足够的投资回报、性价比和用户体验也是非常重要的。最后，如何将这一切标准化地交付给用户，确保他们能够持续使用，而不仅仅是一个演示，也是一个巨大的挑战。每个环节都需要表现出色，才能成功完成这项工作。</p><p>&nbsp;</p><p></p><h4>对软件工程未来的看法</h4><p></p><p>&nbsp;</p><p>鱼哲：您怎么看待编程以及软件工程的未来？</p><p>&nbsp;</p><p>刘东：我认为AI和AIGC技术并不是要取代软件工程，而是要为软件工程提供更强的支持。随着数据积累、通信和计算能力的提升，对软件工程的要求变得越来越高，而AI技术可以提高软件工程的生产力。未来，软件工程师的角色可能会发生变化，从以程序员为主导变为人机协作的模式，工程师需要花更多精力学习和应用AI技术，以提高工作效率和生成更好的软件。</p><p>&nbsp;</p><p>鱼哲：这实际上涉及到一个重要的哲学性问题，即通用性与特殊性之间的平衡。在优化时，我们必须在通用性和特殊性之间做出权衡。因为优化通常是为了特定场景和硬件而进行的，它可能会牺牲通用性。您认为，在未来多久内，我们是否会看到通过代码生成或自动化方式，针对特定硬件环境进行优化呢？比如剪枝、量化、压缩和算子的自动生成。</p><p>&nbsp;</p><p>刘东：实现这一愿景需要大量的基础积累和技术成熟度。目前的代码生成技术主要依赖于已有的能力和沉淀的代码，然后通过大型模型的学习和自动生成来解决更多问题。这种自动化生成代码的技术在广泛的硬件和环境中得到应用，可能需要更多的积累和实践。</p><p>&nbsp;</p><p>对于新的硬件和场景，手动优化仍然是必要的，因为了解硬件和业务逻辑、分析性能问题等需要人的专业知识。自动化优化工具需要基于已有的知识和经验，而不是凭空生成优化方案。因此，即使技术不断发展，仍然需要工程师的专业知识来指导和验证自动生成的代码。未来，随着技术的发展和积累，可能会有更多的通用性优化工具出现，但在新的硬件和场景中，人工干预和专业知识仍然是至关重要的。这是一个逐步演进的过程。</p><p>&nbsp;</p><p>鱼哲：这是一个非常有趣的问题，因为实际上几乎所有的模型在实际应用中都需要经过优化才能顺利落地。在许多情况下，尤其是当涉及硬件时，例如服务器端或者嵌入式设备，优化工作变得尤为重要。</p><p>&nbsp;</p><p>举一个例子，最近非常受欢迎的 Vox 模型，它能够根据自然语言指令为机器人生成指令。然而，在将这一模型应用到嵌入式设备时，通常需要进行大量的优化工作。这种优化工作可能包括模型规模的压缩，性能优化，硬件加速以及适用于嵌入式设备的特定算法的选择。对于这种情况，通常需要工程师深入了解硬件的性能特征以及特定领域的需求。</p><p>&nbsp;</p><p></p><h4>活动推荐：</h4><p></p><p>11月2日，2023网易数字+大会将于杭州举办，网易数帆将带来AIGC技术与云原生、大数据、低代码结合的进展，下午的创新技术论坛，还将全面对外分享网易杭州研究院技术创新范式，带来领域大模型技术揭秘、开源实践分享等话题，欢迎扫描下图二维码或点击链接报名围观：i.163yun.com/obq6t7202</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/11/11c17401a1fe96ca78f916813cd03ee1.png" /></p><p></p><p>&nbsp;</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/y1vbSBSYjULcGd4kgLpG</id>
            <title>OpenAI 刚刚又杀死了一批初创公司</title>
            <link>https://www.infoq.cn/article/y1vbSBSYjULcGd4kgLpG</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/y1vbSBSYjULcGd4kgLpG</guid>
            <pubDate></pubDate>
            <updated>Wed, 01 Nov 2023 07:40:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 创业, ChatGPT, PDF, 初创公司
<br>
<br>
总结: OpenAI最近在ChatGPT上引入了新功能，用户可以上传多种类型的文档，包括PDF，并在同一对话中使用不同的工具。这一更新对于一些初创公司来说可能是一个打击，因为他们的业务正是基于ChatGPT无法直接与PDF交互的现状。然而，对于一些已经成功打包了ChatGPT的初创公司来说，他们仍然面临着发展前景和清算的选择。 </div>
                        <hr>
                    
                    <p></p><blockquote>围绕别人家的大模型创业，盈利快，死得也快？</blockquote><p></p><p>&nbsp;</p><p>从目前的情况看，每当OpenAI在ChatGPT上发布新功能时，都会因为对开发类似功能的初创公司造成冲击而受到指责。OpenAI日前刚刚又为ChatGPT引入了两项新功能，其一是“上传多种类型文档”、其二为“无需切换对话即可使用工具”。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/25/2547e3b3e5fdb86572fd275522b04970.png" /></p><p></p><p>&nbsp;</p><p></p><blockquote>ChatGPT/GPT-4迎来重要更新：可上传任意PDF文档并询问其内容。无需切换聊天即可直接使用新工具。</blockquote><p></p><p>&nbsp;</p><p>在此次更新之后，ChatGPT不仅能够直接读取PDF，还可以在同一对话当中支持多种文档类型，包括PDF、图像和CSV等等。以往，用户只能在默认模式下上传图像，但现在已经可以无缝上传文档并立即开始提问，这大大扩展了ChatGPT平台的多样性和可用性。</p><p>&nbsp;</p><p>此外，用户也不再需要指定自己要用的ChatGPT模式。如今，浏览、高级数据分析（原名Code Interpreter代码解释器）和DALL-E 3现可在同一对话中直接使用。GPT将自行确定激活不同模式的适当时机，例如在用户要求创建图像时调用DALL-E。</p><p>&nbsp;</p><p>相信很多朋友都有在浏览器里浏览几百页的PDF文件，想要从中提取有用数据和摘要信息的经历，其过程相当之痛苦。正因为如此，此次更新才在ChatGPT Plus用户当中获得了高度评价。</p><p>&nbsp;</p><p>但这次更新也造成了广泛影响。有声音认为，这种新的“多模态”更新将毁掉至少数十家初创公司，其中耳熟能详的名号包括ChatPDF、AskYourPDF和PDF.ai等等。不少初创公司恰恰就是看准了之前ChatGPT无法与PDF直接交互的现状，才构建起自己的业务体系。既然现在ChatGPT可以操作PDF了，那这些后起之秀还有哪些业务空间可以挖掘？</p><p>&nbsp;</p><p></p><h2>面向AI的“打包初创公司”面临毁灭打击？</h2><p></p><p>&nbsp;</p><p>支付巨头Stripe公司产品负责人Sahar Mor在LinkedIn上写道，“OpenAI刚刚推行的一项举措可能直接消灭数十家AI公司。”他还专门提到了“打包初创公司。”这类企业在本质上就是把ChatGPT等API“打包”起来形成自己的业务，使用聊天机器人的底层技术提供某种原厂商未能直接提供的服务。</p><p>&nbsp;</p><p>但建立AI打包业务的创始人们，最初可能未必是要故意利用ChatGPT的软肋。今年3月，OpenAI宣布AI服务对外开放，欢迎开发者们将ChatGPT整合到自己的应用程序和产品当中。</p><p>也就是说，提供PDF分析功能的初创公司只是打包商里的一部分，还有很多在提供其他各种补充性功能。</p><p>&nbsp;</p><p>目前最具市场影响力的打包厂商当数Jasper AI，该公司在Coatue和Bessemer Venture Partners等大型风险投资公司的支持下，成功在今年开年之际获得15亿美元估值。</p><p>&nbsp;</p><p>他们做对了什么才得到如此夸张的估值？答案很简单，围绕OpenAI的GPT模型开发一套专门针对企业营销团队的“AI领航员”（AI Copilot）。</p><p>&nbsp;</p><p>但根据技术外媒The Information报道，随着该公司内部估值的一路走低，其业务定位似乎也陷入了困境。今年7月，Jasper AI还曾宣布裁员。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/cb/cb1fafa20d1bdcf866fc9ada1a2b772a.jpeg" /></p><p></p><p>&nbsp;</p><p></p><blockquote>Jasper 以 $1.5B 估值筹集了 1.25 亿美元，这也无济于事。也许GPT打包模式并不适合初创公司。</blockquote><p></p><p>&nbsp;</p><p>也许其他使用ChatGPT等工具为用户提供PDF交互功能的初创公司，都将面临类似的悲惨命运。</p><p>&nbsp;</p><p>今年5月，数据科学家Alex Reibman发布了ChatOCR。这是一款ChatGPT插件，能够“从PDF中读取文本，包括扫描和手写内容。”但在上周末的更新之后，他在X上开展了一项民意调查，询问用户“既然现在ChatGPT已经内置了PDF处理功能”，大家还愿不愿意继续使用插件。在210名受访者中，72.4%的人预计插件“使用量将会减少”。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/2d/2d4972ac35c66c649dec1af24f5ce82d.jpeg" /></p><p></p><p></p><blockquote>我们是本次更新的“受害者”之一。我们运行着ChatOCR，ChatGPT商店中众多PDF处理插件之一（我们主打的是OCR）。过去3个月来，我们的月度经常性收入达到3500美元。大家认为，这次更新会对我们造成怎样的影响？</blockquote><p></p><p>&nbsp;</p><p></p><h2>是面临清算还是仍有发展前景？</h2><p></p><p>&nbsp;</p><p>在围绕PDF创业的公司中，PDF.ai是一家能赚钱、自给自足而且利润率可观的企业。PDF.ai公司创始人Damon Chen表示，“我们的目标不是成为又一家独角兽企业，几百万美元的年度经常性收入对我来说已经足够了”。</p><p>&nbsp;</p><p>OpenAI的更新对PDF.ai确实也带来了一定冲击，他承认，体量太小的初创公司终将遭到淘汰，而由风险投资供养的大块头在烧光现金之后也挺不住。</p><p>&nbsp;</p><p>但Chen仍然带有希望：“昨晚我和妻子就ChatGPT更新聊了聊。我问，如果PDF.ai最后没能成功，该怎么办？她轻描淡写地说一个项目的失败不算什么，另起炉灶好了。”</p><p>&nbsp;</p><p>如今距离ChatGPT正式亮相已过去近一年，OpenAI正逐渐为其添加更多新功能。OpenAI的终极目标是实现通用人工智能（AGI），而阅读PDF等进展只是这个庞大目标中的一小部分。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/8f/8f0d4e72b50ef5c1c97b57f25c6635a6.jpeg" /></p><p></p><p>&nbsp;</p><p>正如Tenstorrent公司人工智能总监Shubham Saboo评论的那样，“ChatGPT 的战略：巩固、创新和统治。ChatGPT 会成为终极 AI 超级应用程序，将 Midjourney、PDF Chat、Perplexity AI 和高级数据分析全部结合在一个应用程序中。”</p><p>&nbsp;</p><p>从某种程度来说，只要不具备能与竞争对手拉开显著差距的“护城河”，初创公司就随时面临被劫掠的风险。那么在别人的 API 之上建立自己的业务还有前途吗？</p><p>&nbsp;</p><p>让人意外的是，不少人对此依然表示乐观。支付服务商Stripe公司产品负责人Sahar Mor表示，“打造用户友好的界面和更易用的功能仍有其实际意义，因此针对特定细分市场的垂直初创公司将继续保持其主导地位。真正面临风险的，主要是那些横向延伸的AI初创企业。”</p><p>&nbsp;</p><p>分析服务商Glass Acres创始人Mark Zahm也认为，“只要GPT还存在，GPT打包方案就会伴其成长并蓬勃发展……”AI爱好者Rowan Cheung则在X上分享道，“大家现在怎么不讲那些靠GPT打包方案赚大钱的故事了？”</p><p>&nbsp;</p><p>在他看来，不少初创公司在网络流量上的表现已经超越了价值数十亿美元的传统公司，而且其业务定位均匀分布在打包、微调和专有模型等各个领域。也就是说，部分GPT打包方案的月度访问量，比某些估值数十亿美元的企业还要高。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/3e/3e078f9ac6611e19b134bf9ee94d6dc2.png" /></p><p></p><p>图片来源：<a href="https://www.infoq.cn/article/iLZYudwYlgARoXmjawJb">ChatGPT 已成为 2023 年最大金矿，大家是怎么靠它挣到钱的？</a>"</p><p>&nbsp;</p><p>开发GPT打包方案的初创公司，主要为那些需要整合AI功能的企业提供一种更经济、更高效的选项，避免从头开始构建复杂的模型体系。</p><p>&nbsp;</p><p>除了OpenAI之外，众多初创公司正在不断涌现，而生成式AI业务已经成为其冲击独角兽之路上的一股重要推力。根据风险投资公司Accel最近发布的报告，这些新独角兽企业中有60%属于生成式AI范畴。去年，欧洲和以色列的生成式AI初创公司投资总额接近10亿美元。相比之下，美国生成式AI初创公司同期获得的注资更是超过140亿美元。但正如报告中所强调，这140亿美元资金的具体分配并不均衡，单是OpenAI一家就分走了其中100亿美元。</p><p>&nbsp;</p><p>根据近期报道，部分照片AI应用和AI聊天机器人服务商赚到的绝对利润，反而还高于生成式鼻祖ChatGPT。今年9月，Chat &amp; Ask AI和ChatOn——AI聊天助手都产生了可观的收入，分别达到近338万美元和211万美元。</p><p>&nbsp;</p><p>此外，AI Chatbot——Nova和AI Chatbot: AI Chat Smith也不甘落后，同期收入分别为144万美元和172万美元。而由a16z支持的聊天机器人初创公司Character.ai也在市场上闹出不小的动静，截至今年9月下载量已达239万次。</p><p>&nbsp;</p><p>也就是说，哪怕OpenAI凭借其多模态功能领先了竞争对手十步，各位初创选手也没必要悲观放弃。相反，也许底层核心技术的升级能提供丰富灵感、帮助他们开发出更好的后续产品。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.businessinsider.com/openai-chatgpt-pdfs-ai-startups-wrappers-2023-10">https://www.businessinsider.com/openai-chatgpt-pdfs-ai-startups-wrappers-2023-10</a>"</p><p><a href="https://www.businessinsider.com/openai-chatgpt-pdfs-ai-startups-wrappers-2023-10">https://www.businessinsider.com/openai-chatgpt-pdfs-ai-startups-wrappers-2023-10</a>"</p><p><a href="https://twitter.com/thealexker/status/1680626018522914817">https://twitter.com/thealexker/status/1680626018522914817</a>"</p><p><a href="https://twitter.com/AlexReibman/status/1718848888088793487">https://twitter.com/AlexReibman/status/1718848888088793487</a>"</p><p><a href="https://twitter.com/Saboo_Shubham_/status/1718653456926359855">https://twitter.com/Saboo_Shubham_/status/1718653456926359855</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/fha6fjAkOt5wCiw60B9l</id>
            <title>从互联网到云计算再到 AI 原生，百度智能云数据库的演进</title>
            <link>https://www.infoq.cn/article/fha6fjAkOt5wCiw60B9l</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/fha6fjAkOt5wCiw60B9l</guid>
            <pubDate></pubDate>
            <updated>Wed, 01 Nov 2023 06:23:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据库技术, 云原生, AI技术, 百度智能云
<br>
<br>
总结: 作为计算机系统的核心基础软件之一，数据库技术的发展备受关注。随着云计算技术的发展，云原生和分布式数据库成为主流，具有高可用性、可扩展性和低成本等优势。AI技术的不断发展使得AI与云计算结合成为可能，云原生为数据库提供了基础条件，AI成为云原生数据库持续演进的驱动力。百度智能云在数据库领域不断创新，推出了云原生数据库GaiaDB、云数据库GaiaDB-X和数据传输服务DTS等产品和解决方案，以支持大数据和AI应用在行业中的挑战。百度智能云团队还推出了《百度智能云数据库》系列云智公开课，探讨数据库的创新、变革和应用。 </div>
                        <hr>
                    
                    <p>作为计算机系统的三大核心基础软件之一，数据库技术的发展一直备受关注。随着云计算技术的发展，能够适合更大规模业务场景，有着高可用性、可扩展性和低成本等优势的云原生和分布式数据库逐渐成为主流。</p><p></p><p>同时，AI 技术不断向前发展，尤其是 OpenAI 掀起的这场 AI 变革，使得 AI 与云计算更紧密地结合成为了可能。云原生为数据库面向更大范围的智能化应用提供了基础条件，AI 成为云原生数据库持续演进的牵引力。</p><p></p><p>为了应对大数据和 AI 应用在行业中的挑战，<a href="https://www.infoq.cn/news/kqPbdlvF3Jp55PodQLAo">百度智能云</a>"在 2020 年率先提出“云智一体”战略，以“云计算为基础”支撑产业数字化转型，以“人工智能为引擎”深入产业生产的关键场景，为企业的数字化转型和智能化升级提供新型支持。在云智一体战略的指导下，<a href="https://www.infoq.cn/article/o8abj2wff5yLfGWuB0E1">百度智能云</a>"在数据库领域不断创新，基于百度集团各项业务的多年磨炼，对外推出了云原生数据库 GaiaDB 、云数据库 GaiaDB-X、数据传输服务 DTS 等产品和解决方案。那这些产品和相关方案，都有哪些不一样的地方，又有哪些落地实践？比如：</p><p>在互联网、云计算，以及向 AI 的演进过程中，百度智能云数据库团队是如何进行技术升级，做到支持各类业务场景，满足海量数据规模，业务要求越来越苛刻的场景的。在 AI 时代，它的最新成果和规划又有哪些？相比其他云原生数据库， GaiaDB 是如何诞生于百度集团的内部业务，其产品理念有什么独特的地方，在哪些技术上面形成了优势，可以帮助用户解决什么样的挑战？在金融行业的国产化进程，GaiaDB-X 如何承载核心业务系统，满足金融行业对基础设施的高要求，在金融客户中的成功落地场景又有哪些？完善的数据库服务不止有数据库产品本身。数据库的迁移、同步、集成同样重要。这些工作和数据库本身一样，同样是复杂又至关重要的。百度智能云的数据传输服务 DTS 如何将繁琐复杂的这类业务变得可靠简单，在业务实践中帮助头部客户成功上云？</p><p></p><p>为了帮助大家解决这些问题，<a href="https://www.infoq.cn/article/ACXL3WviaTtr2U0v5NlB">百度智能云</a>"团队特推出《百度智能云数据库》系列云智公开课。前四期课程便将围绕“从互联网到云计算再到 AI 原生，百度智能云数据库的演进”、“高性能和多级高可用，云原生数据库 GaiaDB 架构设计解析”、“面向金融场景的 GaiaDB-X 分布式数据库应用实践”、“一站式数据库上云迁移、同步与集成平台 DTS 的设计和实践”四个主题展开。从 11 月 15 日起，每周三都将有一位百度智能云的大咖与各位一起探讨百度智能云数据库的创新、变革和应用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cf/cf46378ce27ef5e83b5b6abf599ec8d3.jpeg" /></p><p></p><p>第一讲：《从互联网到云计算再到 AI 原生，百度智能云数据库的演进》</p><p>你将获得：</p><p>全局完整地了解数据库行业的历史和发展趋势；了解百度智能云数据库在各个阶段的典型产品、应用和关键技术；了解百度智能云数据库在 AI 原生时代的创新和变革。</p><p>&nbsp;</p><p>第二讲：《高性能和多级高可用，云原生数据库 GaiaDB 架构设计解析》</p><p>你将获得：</p><p>了解云原生数据库的不同技术路线和能力对比；了解相比传统单体数据库，云原生数据库的技术差异和挑战；了解 GaiaDB 在高性能和多级高可用方向上的技术架构。</p><p>&nbsp;</p><p>第三讲：《面向金融场景的 GaiaDB-X 分布式数据库应用实践》</p><p>你将获得：</p><p>了解金融核心系统在构建分布式数据库的技术挑战；了解 GaiaDB-X 数据库的架构及在金融场景的分布式特性；了解 GaiaDB-X 在金融机构的核心系统分布式的落地实践。</p><p>&nbsp;</p><p>第四讲：《一站式数据库上云迁移、同步与集成平台DTS的设计和实践》</p><p>你将获得：</p><p>了解数据库上云迁移、数据库同步 / 集成的业务场景，以及实践中可能遇到的技术挑战；了解百度智能云 DTS 的关键特性、核心技术和实践案例。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/jQTpFmq9SqamtQEwdc0M</id>
            <title>蚂蚁SOFA Stack融合大模型发布升级版 助力机构产研效能提升30%</title>
            <link>https://www.infoq.cn/article/jQTpFmq9SqamtQEwdc0M</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/jQTpFmq9SqamtQEwdc0M</guid>
            <pubDate></pubDate>
            <updated>Wed, 01 Nov 2023 06:14:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 蚂蚁集团, CodeFuse, SOFAStack5.0, 企业研发运维智能助手
<br>
<br>
总结: 蚂蚁集团发布了CodeFuse全面加持的SOFAStack5.0升级版本，为企业提供全方位研发运维智能助手相关能力。这个升级版本将为企业产研效能提升30%，通过智能副驾驶提升日常代码研发、测试、运维过程中的效率和质量。SOFAStack还提供了一系列云原生解决方案，帮助企业在云环境下快速构建、部署和管理应用程序。 </div>
                        <hr>
                    
                    <p>11月1日，在云栖大会上，蚂蚁集团正式发布CodeFuse全面加持的SOFAStack5.0升级版本，向企业提供全方位研发运维智能助手相关能力。这是继蚂蚁集团在外滩大会发布代码大模型CodeFuse之后，首次公布面向行业的商业化产品进展。</p><p>&nbsp;</p><p>“大模型将为研发效能带来颠覆性机遇。”蚂蚁集团数字科技事业群产品总监马振雄在发布会上指出。</p><p>&nbsp;</p><p>记者了解到，目前CodeFuse已经与SOFA产品线全面融合，涵盖设计、研发、测试、运维等领域，形成从领域建模到智能运维的端到端Copilot产品解决方案，预计将为企业产研效能提升30%。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/5c/5cd0d1b716d80689035335361a75486b.png" /></p><p></p><p>具体而言，客户在使用SOFAStack时，相当于为企业开发者配备专属智能副驾驶，和机器人“辅助设计”、“结对编程”、“运维助手”，通过人机交互助手提升日常代码研发、测试、运维过程中的效率和质量。对企业而言，引入智能副驾驶可以显著提升人效质量，降低总体成本。</p><p>&nbsp;</p><p>此外，SOFAStack针对Codefuse大模型提供了多任务微调和高性能推理能力，结合企业专有数据构造更懂客户业务的智能副驾驶。而随着CodeFuse在产品线中不断深度融合，SOFAStack将为企业打造新一代AI云原生PaaS平台，使其在开发运维、数据分析、应用治理、绿色计算方面取得更智能的能力，可以加速响应业务创新和价值交付。</p><p>&nbsp;</p><p>针对当下企业应用上云「更异构、更智能、更经济」的三大需求趋势，马振雄表示，SOFAStack提供了一系列云原生解决方案，帮助企业在云环境下快速构建、部署和管理应用程序。这些解决方案可以满足不同行业和企业的需求，并为企业提供更加灵活和高效的技术支持。例如，针对行业进入多云时代，边缘资源调配、云上云下应用开发等统一管理挑战，其拳头产品MESH升级架构，从原来的经典Sidecar架构开始演变为Node架构，同步进行了性能、服务治理、业务可观测能力等全方位优化。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/7c/7cd485b0811e68275758bb2d760a382d.png" /></p><p></p><p>Forrester报告曾分析指出，以云原生为关键能力的下一代云平台， 不仅可以基于全栈云原生架构灵活适应市场变化，而且可以通过全云开发实践帮助企业在云上快速验证创新思路，还能借助云平台的各类自动化能力降本增效强化韧性。</p><p>&nbsp;</p><p>“服务网格降低了我们的上云门槛。如果做云原生改造，系统的所有代码都要重写一遍，大概需要20&nbsp;个人投入一年时间；使用网格（Mesh）只要&nbsp;5&nbsp;个人两三个月就能上云。”传统金融机构信息科技架构规划负责人在Forrester调研时表示。根据报告测算，三年内有&nbsp;10&nbsp;个单体应用不需要经过云原生改造，即可直接上云后统一治理，总体效率提升为企业带来941万元的收益。</p><p>&nbsp;</p><p>据悉，SOFAStack是国内部署云原生技术最广泛的平台之一，基于支付宝、蚂蚁集团各项业务需求进行研发迭代，并服务于超100家银行迈向云原生转型，已经构建了完整金融级的云原生PaaS解决方案。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/hEPJ8eoTHjSSxGbmaWd8</id>
            <title>计算机软硬件优化首席科学家、高级首席工程师周经森（Kingsum Chow）博士，确认担任 QCon LLM 时代的性能优化专题出品人</title>
            <link>https://www.infoq.cn/article/hEPJ8eoTHjSSxGbmaWd8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/hEPJ8eoTHjSSxGbmaWd8</guid>
            <pubDate></pubDate>
            <updated>Wed, 01 Nov 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, LLM 时代的性能优化, 周经森（Kingsum Chow）, CPU 和 GPU 平台
<br>
<br>
总结: QCon 全球软件开发大会将在上海举行，周经森博士将担任“LLM 时代的性能优化”专题的出品人。该专题将介绍LLM时代的性能分析在CPU和GPU平台上的表现。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1101&amp;utm_content=zhoujingsen">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。计算机软硬件优化首席科学家、高级首席工程师周经森（Kingsum Chow）博士将担任「<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1101&amp;utm_content=zhoujingsen">LLM 时代的性能优化</a>"」的专题出品人。在此次专题中，你将了解到 LLM 时代的性能分析在 CPU 和 GPU 平台上，在不同计算环境下的性能表现。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1101&amp;utm_content=zhoujingsen">周经森（Kingsum Chow）</a>"，计算机软硬件优化首席科学家、高级首席工程师。曾就职于美国英特尔公司和中国阿里巴巴集团，2023 年加入浙江大学软件学院（宁波）。二十年来与十余家世界 500 强高科技企业合作，共同推动了世界软硬件性能优化技术的发展。曾作为项目总监主持备受瞩目的云计算蓝图项目（IntelCloudBlueprint）。该项目由英特尔和甲骨文的首席执行官于 2015 年共同宣布，吸引了超过 4 万名开发者的参与，为云计算行业绘制了全新的技术蓝图，对行业发展产生了深远影响。</p><p></p><p>自 2016 年加入阿里巴巴，为中国的性能优化技术发展做出了巨大贡献。2018 年，其作为唯一一名加入 Java 全球管理组织 JavaCommunityProcess（JCP）最高执行委员会 JCP-EC 的中国企业（阿里巴巴）代表，参与制定了 Java 的全球标准。</p><p></p><p>周博士在 CPU 利用率报告不准确（数据普遍误解）方面发表的研究，引起了业界和学术界的广泛关注。周博士拥有超过 30 年的软硬件协同优化的工业实践经验，培养了大批优秀的系统性能优化人才。至今已获授权中国专利 11 项，美国专利 24 项，发表学术论文 127 篇，在过去 6 年的 QCon 中国大会上发表 2 场主题演讲，出品 2 场软件系统性能优化主题讲座。</p><p></p><p>相信周经森（Kingsum Chow）博士的到来，可以帮助提升此专题的质量，让你学习到， 通过 LLM 在不同计算环境下的性能表现，找到的最佳应用策略和优化方法，这为 LLM 的应用和发展提供了更多的可能性。</p><p></p><p>除上述专题外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1599?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的大前端技术</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！现在购票，享 7 折优惠，立减￥2040 ！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/16/36/160539957f1fd1f4671722f1cab32a36.jpg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>