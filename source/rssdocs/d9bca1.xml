<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/C9vjic1Oj9JhyHY3m645</id>
            <title>AI辅助内部研发效率提升，昇腾大模型推理的最佳实践</title>
            <link>https://www.infoq.cn/article/C9vjic1Oj9JhyHY3m645</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/C9vjic1Oj9JhyHY3m645</guid>
            <pubDate></pubDate>
            <updated>Tue, 04 Jun 2024 09:54:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI 技术, 大模型推理, 昇腾解决方案, 硬件选型
<br>
<br>
总结: 文章讨论了随着AI技术的发展，大模型推理的趋势和挑战，以及昇腾提供的高性能大模型推理软硬件解决方案。文章指出大模型推理对硬件性能和软件支持的要求不断增加，而昇腾的解决方案涵盖了从硬件选型到应用部署的全流程，以满足不同应用场景的需求。在硬件选型方面，昇腾提供了不同的硬件方案，如Atlas 300I Duo和Atlas 800I A2，以支持不同的推理需求。 </div>
                        <hr>
                    
                    <p></p><blockquote>演讲嘉宾 | 王建辉</blockquote><p></p><p></p><p>以大模型为代表的 AI 技术高速发展，目前 Scaling Law 依然生效，模型参数持续增大，序列不断增长，响应速度越来越快，但大模型商业闭环依赖推理的规模落地，如何在不断提升用户体验的基础上不断降低推理成本，以满足大模型规模落地的诉求，成为大模型推理技术研究的核心关键。为了满足大模型推理规模落地对客户体验和成本的诉求，昇腾推出高性能大模型推理软硬件解决方案，满足客户多样性开发诉求，助力大模型规模落地。</p><p></p><p>本文整理自华为昇腾计算首席架构师王建辉在<a href="https://aicon.infoq.cn/2024/beijing?utm_source=infoq&amp;utm_medium=conference"> AICon 2024 北京</a>"的演讲《昇腾大模型推理最佳实践》，内容经 InfoQ 进行不改变原意的编辑。</p><p></p><p></p><blockquote>华为昇腾计算首席架构师赵英俊将在 6 月 14-15 日即将举办的 ArchSummit 深圳上进一步分享<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5866">《超大规模 AI 算力集群优化与实践》</a>"。此外，阿里巴巴研究员 / 阿里云云原生应用平台负责人丁宇（叔同）将带来《AI 编程如何颠覆生产力》 的 Keynote 主题演讲，在《低代码与 AI 结合》专题上，来自腾讯、网易、蚂蚁集团等企业等技术专家也将深入探讨在低代码环境中集成智能决策、自动化流程，以及构建灵活、高效的应用系统。目前，大会议程已全部上线，感兴趣的同学请锁定大会官网：<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">https://archsummit.infoq.cn/2024/shenzhen/schedule</a>"</blockquote><p></p><p></p><p></p><p>我的演讲将分为三个部分。第一部分，我会讨论当前大模型推理的趋势和应用方向，探讨大模型推理对软件和硬件的要求，以及如何评估大模型推理系统的优势和不足。第二部分，我将详细介绍昇腾在大模型推理方向上的解决方案，包括软硬件架构和关键技术。第三部分，我会分享一些与实际应用落地相关的案例。</p><p></p><h3>大模型发展趋势分析</h3><p></p><p></p><p>在人工智能领域，随着技术的不断迭代发展，对推理解决方案的要求也在不断提高。从早期的 CNN（卷积神经网络）发展到现在广泛使用的 Transformer 模型，以及最近的 DIT 架构，这些技术的进步对计算资源提出了更高的挑战。</p><p></p><p>目前，大模型（LLM）已经开始向多模态方向发展，长序列技术已经成为当前应用的主流。不同的技术具有不同的计算特性，例如 Stable Diffusion 或 DIT 架构，它们对算力的需求非常高。Transformer 模型在推理时，对内存带宽的需求也很大，尤其是在使用 KVCache 技术之后。而长序列处理则对内存容量有很高的要求。整个硬件系统需要具备强大的算力、足够的内存容量以及高速的内存带宽。同时，软件也需要能够支持这些硬件的高性能要求。目前，单机可能已经无法满足推理的需求，需要利用多机多卡的并行推理技术。此外，随着推理技术的快速发展，不同的量化技术和压缩技术的应用，对软件的灵活性也有很高的要求。</p><p></p><p>当前大模型推理应用正在加速成熟，整体来看，美国在这方面的发展速度更快一些，而国内则处于一个跟进的状态。大模型推理的应用首先在 ToC（面向消费者）领域爆发，随后迅速向 ToB（面向企业）领域发展并成熟。</p><p></p><p>大模型推理的应用通常具有一个特性，即在初期增长阶段不会太快，但一旦过了某个爆发点，其增长速度会非常快，甚至达到 100 倍或 200 倍的增长。从目前客户的情况看，大模型推理的卡已经有上万张卡在同步在线进行推理，千卡集群的推理集群已经非常多。训练和推理对集群的要求有所不同。训练集群构建的关键在于如何实现更大规模的互联，以支持万卡集群同时进行任务训练。而推理的核心追求是降低成本，以满足业务需求。</p><p></p><p>构建一个领先的推理解决方案，我们围绕业务场景总结并建立了一个评价体系，我们称之为 LACE 指标。这个体系包括推理时延和模型精度，这两个因素直接影响用户体验。此外，还包括吞吐量和并发能力，以及开发的易用性，这两个因素则影响推理的成本，包括线上成本和开发成本。围绕 LACE 开发体系，从业务规划到模型上线，整个过程会经历几个关键阶段。首先是硬件选型，选择合适的硬件对于整个解决方案的性能至关重要。接下来是训练到推理的转换和优化，这个过程涉及模型的压缩和量化，旨在提高性能并降低成本。最后是模型的上线部署，完成整个业务流程。</p><p></p><h3>昇腾大模型推理软硬件方案和关键特性</h3><p></p><p></p><p>昇腾针对大模型推理的全流程提供了一整套完整的解决方案。这套方案从底层的硬件开始，其上是昇腾提供的自家研发的硬件使能，确保硬件性能得到充分利用。在硬件使能之上，昇腾构建了推理引擎，能够支持各种推理任务。再往上，昇腾还提供了推理服务化部署的能力，使得推理服务可以灵活地部署和扩展。通过这四层架构，昇腾能够支持从硬件选型、训推转换、模型压缩、推理执行到应用部署的五个关键维度。这五个维度涵盖了从业务规划到模型上线的整个流程，确保了解决方案的全面性和高效性。接下来，我将围绕这五个维度，详细展开当前昇腾的能力，为大家提供一个全面的讲解。</p><p></p><h4>硬件选型</h4><p></p><p></p><p>昇腾在硬件选型方面，针对不同的应用场景，提供了不同的硬件方案。昇腾根据模型参数量和时延要求，将应用场景划分为四个象限，每个象限的硬件选型和考量都有所不同。昇腾目前主要提供了两款专用硬件，一款是面向单卡推理或低成本部署的 Atlas 300I Duo，另一款是面向多卡甚至多机推理的 Atlas 800I A2。这两款硬件各有特点，能够满足不同应用场景对硬件能力的需求。</p><p></p><p>在硬件选型时，需要综合考虑时延和成本约束。如果追求极低时延，计算的 batch size 不能太大，这会提高推理成本。但如果能在时延上做一些权衡，比如将时延要求放宽到 50 毫秒或 100 毫秒，推理成本可以得到显著降低。通过调整 batch size 和提高硬件资源利用率，可以在计算成本和用户体验之间取得平衡。</p><p></p><p>Atlas 800I A2 硬件采用了 HCCS 全互联架构，具有接近 400GB 的互联带宽，能够支持 LLaMA2-70B 等大模型，实现高吞吐量。而 Atlas 300I Duo 硬件则拥有 96GB 的大内存容量，适用于时延要求不高的场景。例如，在文生图或 Stable Diffusion 等模型中，单卡可以实现 1.5 秒左右完成 50 次迭代，生成一张图片。如果通过蒸馏技术将迭代次数从 50 次降低到 20 次，生成一张图片的时间可以控制在 1 秒内。</p><p>推理转换</p><p></p><p>昇腾在训练模型到推理的转换过程中提供了一整套软件架构，包括昇腾推理引擎 MindIE，它涵盖了推理运行时以及与 Mindspore 和 Pytorch 框架的对接。昇腾的硬件支持 Mindspore 训练出的模型直接进行推理，并致力于打造训练推理一体化的 AI 框架，实现底层 MindIE-RT 与多种推理部署方式的结合。对于 Pytorch 训练的模型，昇腾通过 Pytorch 插件快速实现模型从训练到昇腾推理平台的迁移和适配，仅需大约 10 行代码，性能可达 0.8-0.9 倍于纯离线推理。</p><p></p><p>昇腾还提供了 MindIE-RT，支持自动构图开发工作流，实现从框架导出图到昇腾底层执行图的快速转换。对于大语言模型，由于图开发体系的挑战和新技术的快速迭代，昇腾也支持手动构图或手动开发，以快速构建并优化性能。</p><p></p><p>昇腾提供一站式开发工具 MindStudio，支持算子、模型和应用三层开发，提供迁移分析工具、精度比对、改图、性能优化和快速部署能力。借助 MindStudio，模型在昇腾平台上的迁移和优化可以在 3-15 天内完成。</p><p></p><p>昇腾强调训练和推理的同构能力，即在推理硬件上或相同底层硬件架构的设备上进行训练，以保持精度无损。昇腾还提供 Ascend C 编程语言，支持自定义算子开发，这对于大模型推理中 Attention 算子的灵活性和性能至关重要。尽管从 CUDA 或 GPU 开发习惯适配到昇腾平台需要一个学习过程和成本，但昇腾的开发体系能够使客户快速开发出高性能的算子。例如，昇腾的客户能够将五次内存访问的操作通过超大融合算子减少到一次内存访问，显著提升大模型推理性能。</p><p></p><h4>模型压缩</h4><p></p><p></p><p>昇腾在模型压缩方面支持当前大模型推理中常用的优化算法，包括量化算法 W4A16、W8A16 以及 W8A8 等权重量化技术和量化算法。在 W8A8 量化方面，由于大模型激活值的特性，存在大量异常值需要抑制。昇腾通过异常值抑制和自适应 PTQ 量化算法，避免了在量化过程中引入微调的需求，这些算法在模型量化中变得非常重要。</p><p></p><p>昇腾在 W8A8 量化上能够将精度损失控制在 0.5% 到 1% 的范围内，这与测试数据集的抖动和变化有关。量化之后，整体业务成本可以降低约 30%。利用昇腾硬件的特性，如 Atlas 300I Duo，昇腾还支持权重随路解压缩能力，这要求对模型进行稀疏处理。经过权重稀疏处理后，模型权重的内存搬移量可以进一步压缩 30% 到 50%，从而进一步提升大模型推理的性能。</p><p></p><p>在大模型推理领域，加速技术的发展非常迅速，包括开源算法、软件、论文以及新技术的快速演进。Continuous Batching 和 Paged Attention 算法，动态调度和节省内存的技术，已成为大模型推理中的一个必备技术。除了 Continuous Batching 和 Paged Attention，并行解码技术也发展迅速，昇腾希望利用这项技术充分利用算力。</p><p></p><p>大模型推理对内存带宽的要求非常高，同时对卡间或机间的通信时延也有高要求。与训练阶段优化通信的思路不同，推理阶段的目标是最小化单次通信时延。实验表明，有时增加通信量反而能减少调度时延，从而降低整体通信时延。降低通信时延后，可以实现更大的并行域，如从 TP8 扩展到多机 TP16，这在单机临界点的应用场景中带来了显著的性能提升。</p><p></p><p>昇腾在内部模型验证中发现，在 32K 序列长度级别，通过多机对比单机，平均单卡吞吐量能提升接近 6 倍。此外，昇腾还关注多机推理方案，类似于训练集群的参数面组网，通过一层组网实现多机推理。</p><p></p><p>针对 Kernel 的融合优化，昇腾根据硬件特性进行融合算子优化，以降低内存频繁读写问题，提高内存带宽利用率，从而提升推理性能。Flash Attention 算子最初是针对 GPU 计算特性提出的，但昇腾发现它可能不是与昇腾硬件最亲和的算子。因此，昇腾根据硬件结构特性重新设计了 attention 算子，以提高执行效率。并行解码技术有多种形式，如 Lookahead 算法，它是一种自投机算法，可以在不重新训练小模型的情况下发挥作用。Lookahead 算法对序列长度和输出长度有一定限制，但在其要求的范围内，能够显著降低推理时延，提升吞吐量。</p><p></p><p>小模型加大模型的投机推理也是一种有效方法，但需要生成一个小模型，这有一定难度。昇腾在这一方向进行了测试和验证，发现在序列长度约 2K 和 batch 在 100 以内时，吞吐量能提升约 40%，推理时延也降低约 40%。</p><p></p><p>通过 reference 方式提供投机来源，也能在代码生成场景下提升约 60% 的吞吐量。这些算法对底层 attention 算子的要求非常高。昇腾正在努力实现一套能够整合不同并行解码算法的系统，同时结合并行解码特性和 Continuous Batching 和 Paged Attention 的特点，以形成一个面向产品化的完整并行解码系统。</p><p></p><p>昇腾在集群方案方面有两个主要目标。首先，昇腾希望支持万亿级别的大模型推理，即在昇腾平台上能够处理具有高达万亿参数量的模型。其次，昇腾还计划支持百万级别的序列长度，指的是直接进行硬推理的能力，而不是采用近似计算或 streaming LLM 的方式。昇腾的目标是实现直接计算，能够达到百万级序列长度的处理能力。</p><p></p><h4>推理执行</h4><p></p><p></p><p>昇腾目前支持业界主流大模型在其平台上进行推理，包括开源模型和业界常见的模型。昇腾的性能表现相当出色，能够达到 1.5 倍或 1.8 倍于业界平均水平。例如，LLaMa3 模型发布后的第二天，昇腾就完成了其训练和推理的适配工作。如果客户需要，他们可以立即在昇腾平台上对 LLaMa3 的 8B 和 70B 版本进行训练和推理验证，包括场景化调优和上线部署。昇腾在大模型训练和推理的业界跟进和适配速度方面相对较快。</p><p></p><p>为了加快大模型训练完成后到推理部署上线的整个过程，昇腾自研了 MindIE-Service 服务化部署能力。此外，昇腾还支持与业界开源软件 vLLM 和 TGI 的对接，并提供了相应的对接方式。这表明昇腾在推理服务化部署方面具有强大的自研能力和良好的兼容性。</p><p></p><h4>应用服务</h4><p></p><p></p><p>昇腾的软件栈能力在应用服务方面是分层开放的，这意味着昇腾能够支持不同客户和不同场景的对接需求。如果客户拥有自研的推理引擎，昇腾可以提供底层的算子和加速能力，包括底层的算子和加速库，以便于与客户的推理引擎进行对接。对于那些拥有自己服务化能力和框架的客户，昇腾在推理引擎层面提供对接支持。而对于需要一整套完整推理应用的客户，昇腾可以提供从服务化到引擎，再到底层算子的全栈解决方案，以支持客户的全面对接需求。</p><p></p><h3>应用案例与关键进展</h3><p></p><p></p><p>昇腾在应用案例与关键进展方面已经取得了显著成果。目前，昇腾与多家头部客户合作，共同打造了大模型一体机，实现了大模型在训练和推理上的一体化应用，覆盖了办公客服等场景。</p><p></p><p>在华为内部，昇腾也在会议场景和面向 10 万研发人员的代码研发辅助场景中大量应用了大模型。华为计划将内部研发的代码生成和辅助开发场景制作成 demo，并将其开源，以支持业界的发展。</p><p></p><p>在视频生成领域，昇腾通过 MindIE-SD 支持 open sora 进行开发工作。目前，昇腾能够实现仅需修改少数几行代码，就能让 open sora 模型在昇腾平台上运行。在视频生成效率方面，昇腾当前能够在大约一分钟内生成 20 秒的视频。预计到年底，昇腾希望能够在一分钟内生成 60 秒的视频，这将是一个显著的性能提升。</p><p></p><h5>活动推荐</h5><p></p><p>本届 ArchSummit 会议上，重点聚焦 AI 大模型技术在各行业的落地实践， 顺丰集团、众安银行、天弘基金、鸿海科技集团、宁德核电、广发证券、微众银行介绍大模型技术的应用效果 。会议上还设置了大模型应用、架构升级、智算平台、AI 编程、成本优化等专题和话题内容。如您感兴趣，可点击「阅读原文」查看更多详情：<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">https://archsummit.infoq.cn/2024/shenzhen/schedule</a>"。</p><p>购买票数越多，享受的优惠也就越丰厚，可以联系票务经理 17310043226 , 锁定最新优惠。</p><p><img src="https://static001.geekbang.org/infoq/de/de05ca2264be4a0f38c16be18fdc1d26.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/AsEk7t1aSPEKHSQatxGa</id>
            <title>大模型加持下的 AIOps 业务场景实践有哪些新“解法”？</title>
            <link>https://www.infoq.cn/article/AsEk7t1aSPEKHSQatxGa</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/AsEk7t1aSPEKHSQatxGa</guid>
            <pubDate></pubDate>
            <updated>Tue, 04 Jun 2024 09:37:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AIOps, 大模型, 智能运维, AI算法
<br>
<br>
总结: 监控运维产出的海量数据受到数据质量、标注不足和链路上下文信息缺失等问题的影响，AIOps应用面临挑战。大模型在多模态数据理解和处理上备受期待，希望优化AIOps在数据理解、关联和交互体验上的表现。 AIOps结合AI算法和大模型，能够简化运维工作流程，解决传统运维难题，提升业务效能。身为业务团队，AIOps能帮助提升研发效能，通过工程化和AI技术实现智能运维。在AIOps应用落地时，团队关注人力成本、AI系统基础设施建设和算法选择等问题。 </div>
                        <hr>
                    
                    <p></p><blockquote>嘉宾｜董善东博士、何碧宏、张瀚元</blockquote><p></p><p></p><p>监控运维产出的海量数据常受数据质量波动、标注不足和链路上下文信息缺失等问题影响，这些挑战为 AIOps 的应用带来了不小的难题。伴随大模型的崛起，业界对其在多模态数据理解和处理上的能力抱以厚望，期待大模型能优化 AIOps 在数据理解、关联和交互体验上的表现。</p><p></p><p>在日前的《超级连麦. 数智大脑》x ArchSummit 直播中，<a href="https://archsummit.infoq.cn/2024/shenzhen/track/1641">阿里云 AIOps 架构师董善东博士</a>"、群核科技云原生观测 / 技术专家何碧宏、腾讯文档高级工程师张瀚元，聚焦企业在实际的业务场景中利用 AIOps 提升业务效能时遇到的挑战以及解决方案展开了深入交流，同时还探讨了大模型时代下 AIOps 的创新架构，以及大模型如何为 AIOps 带来新的变革和潜能等话题。</p><p></p><p>以下内容根据对话整理，篇幅有删减，点击链接可观看直播回放：<a href="https://www.infoq.cn/video/AQZtO8Le5MRkuyWq1yaP">https://www.infoq.cn/video/AQZtO8Le5MRkuyWq1yaP</a>"</p><p></p><p>ArchSummit 深圳大会议程已经上线，感兴趣的同学请锁定大会官网：<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">https://archsummit.infoq.cn/2024/shenzhen/schedule</a>"</p><p></p><h3>一、What &amp; Why</h3><p></p><p></p><h5>董善东博士：大家理解的 AIOps 是什么？以及为什么在你们团队需要引入 AIOps 做一些事情？可以举 1-2 个场景来探索下在各自业务的 AIOps 核心场景。</h5><p></p><p></p><p>何碧宏：AIOps，即智能运维，是一种利用人工智能技术，包括算法和大模型，来自动执行和简化运维工作流程的技术方案。作为云原生观测和 SRE 团队，我们主要关注在稳定性保障方面的应用和价值。引入 AIOps 希望通过 AI 算法简化和自动化传统运维操作流程，并解决一些传统运维难以解决的问题。</p><p></p><p>一些场景，比如在传统手动配置中，我们经常遇到动态配置和告警规则配置的问题，例如设置多大的阈值合适。如果每个人都手动调整，工作量会非常大。因此，我们希望通过算法实现动态阈值的功能。此外，指标异常检测也是一项庞大的工作，需要识别每种指标的持续时间、波动大小等。我们希望通过算法实现智能检测功能。在容量评估方面，我们希望能够预测未来的流量，从而计算出未来的资源使用量和扩容时间。变更风险预测也是一个场景，希望通过算法或智能方式预测变更可能造成的影响，引导变更人员进行更有针对性的检测。此外，根因分析在微服务架构下尤为重要，底层服务故障可能导致连锁反应，根因定位策略复杂且思路繁多。我们也希望能通过 AI 算法进行归纳，智能定位根因。</p><p></p><p>我们公司是一家面向全球的 SaaS 服务提供商，专注于为企业服务。对于客户，尤其是重点客户的保障尤其重视，我们几乎需要快速回应和解决每一个工单客诉。在解读和复现客户提交的工单故障方面，我们通常需要投入大量人力。我们希望通过 AI 能力对重复性工单故障进行聚类、归类，并自动化解读，预先提供有用的分析信息，以提高技术支持和开发人员的效率。</p><p></p><p>我认为，无论传统运维还是 AIOps，都需要为效果负责。两者最终需要结合互补，以实现最佳效果。</p><p></p><p>张瀚元： 身为业务团队，我们所面临的痛点和问题，以及我们所站的角度、立场与理解可能跟何老师存在不少差异。我对 AIOps 的理解定义简单明了，即 AI 与 Ops 的结合。通过自动化和工程化手段，AIOps 能够帮助业务提升研发效能。</p><p></p><p>我的理解是，要在实现智能化之前，首先需要实现工程化。通过工程化手段，我们能够以较低成本实现智能运维。在低成本的工程化智能运维基础上，再进一步通过 AI 和大模型来提升效率，带来智能化的解决方案。</p><p></p><p>对于业务团队的成员，尤其是新成员来说，在海量的微服务场景下，整个系统的庞大架构难以理解，上手也相对困难。在这种情况下，基于工程化的方式推出 AIOps，可以帮助团队成员快速了解系统。例如，通过智能告警和智能链路分析，团队成员能够从宏观层面快速认识整个系统。在此基础上，再利用大模型和 AI 的基础，对系统进行更深层次的理解。我非常认同何老师的观点，无论是传统的 Ops 还是 AIOps，最终都要为业务效果负责。两者需要相互融合、相互补充，以实现为业务带来更大价值的目标。</p><p></p><p></p><h5>董善东博士：我非常认同这个观点。AIOps 自 2016 年提出以来，到 2018 年达到了第一波热度高潮，然后在 2020 到 2022 年间相对低谷期。近两年，随着大模型的火热，AIOps 如何在可观性场景下结合得更紧密，并将大模型与 AIOps 模型结合，应用在可观测等场景成为了新的讨论热点因此。AIOps 不仅迎来了第二波讨论高潮，而且伴随着 8 年的沉淀，如何利用 AIOps 真正提升效率并为业务带来价值，已然是业务团队更关心的问题。这引出了第二个问题，目前在你们团队做 AIOps 应用落地时，最关心的几个点是什么？</h5><p></p><p></p><p>张瀚元： 在进行 AIOps 时，我们团队比较关心的几个点包括：</p><p></p><p>人力成本： 如果每个业务团队都组建自己的 AI 或运维团队，对业务来说会是一个较大的负担和成本。我们首先考虑的是避免大量重复的人力成本投入。更好的做法可能是在整个部门或公司内拉通视角，寻求通用的解决方案，这样既可以降低成本，也可以让专业的人聚焦做专业的事。AI 系统基础设施建设： 过去几年，我们主要精力放在了 AI 基础设施和监控可观测性基础设施的建设上。在完成底层可观测性基础设施的标准化建设后，我们在数据层面和标准协议层面有了良好的统一。实现这些统一后，我们再进一步建设上层的 AIOps 技术能力，这对我们整体规划来说是一种比较合理且高效的方式。算法选择和模型优化： 前期我们会更关注成本和基础设施建设。随着时间的推移，我们未来可能会更加关心算法的选择和模型的优化。</p><p></p><p>何碧宏： 我们所关心的问题主要集中在以下几个方面：</p><p></p><p>首先是成本问题，特别是人力成本。 作为中小型企业，在人才资源上不如大公司丰富，人员流失问题也较为严重。因此，对于算法的可持续性以及整个算法解决方案的可持续性非常关键。另外，AI 算法相比传统程序在资源消耗上要高很多，这导致硬件成本和其他相关成本也相应提高。</p><p></p><p>其次，我们非常关注算法的实际效果，如准确率和召回率。在真实场景中，如何提升算法效率，以及算法的准确率和召回率是否存在上限问题，都是我们所关心的。系统和代码的变更或系统架构的调整可能会影响到原有分析，这就要求我们对算法、解决方案和数据进行相应的调整。在变更后，如何保证项目的可持续性，保持其“保鲜”，这也是我们关注的问题。</p><p></p><p>最后是实施过程中的问题：包括选择合适的算法和训练数据的规模与质量。数据质量对算法准确性的影响非常大，尤其是训练样本的质量以及噪音处理，这些都对准确率起着决定性作用。因此，如何以低成本实现数据的持续自动化清洗和样本数据的准备，也是我们非常关心的问题。</p><p></p><p>董善东博士： 作为云厂商的代表，我想补充一个可能业务团队或 SaaS 产品团队不太关注的问题，那就是如何保证 AIOps 算法模型服务的通用性。这是云厂商目前面临的一个较大问题和挑战。一方面，我们可能依赖于内部数据，或者根据某些客户的数据进行微调，从而得出一些在特定业务场景下运行良好的模型。但问题是如何提高这些模型的泛化性，使其能够跨不同行业、不同类型的客户进行复制和应用。遵循 Scaling Laws 这类规则是云厂商需要考虑的关键点。这意味着我们需要思考如何让 AIOps 算法不仅在特定场景下有效，而且能够广泛适用于各种不同的业务环境和客户需求，以实现更广泛的服务覆盖和更高效的资源利用。</p><p></p><h3>二、AIOps 实施过程中的挑战和问题</h3><p></p><p></p><h5>董善东博士：接下来，我们来讨论一下在实际实施和落地过程中遇到的挑战。特别是，我想知道大家是否有一些内部总结的经验和技巧。或者注意到的问题和踩到的一些坑。</h5><p></p><p></p><p>何碧宏： 在数据方面，我们面临几个挑战：</p><p></p><p>首先，数据的持续标注和刷新是一个问题，这通常需要大量的人力劳动。</p><p></p><p>其次，我们在处理业务故障这类小数据问题时，由于故障发生的数量不多，原始样本数本身就很少。在这种情况下，如何通过算法提高故障定位的准确率，是一个需要解决的问题。</p><p></p><p>第三个问题是监控数据的多样性。指标、日志、调用链、警报等数据的格式可能各不相同。然而，在进行根因定位时，我们需要将这些不同格式的数据进行串联和融合分析，这也是一个具有挑战性的问题。</p><p></p><p>董善东博士： 在数据质量及其上下游标准问题上，我个人的体会非常深刻。</p><p></p><p>首先，数据质量对算法模型至关重要，有一句话说得好："垃圾进，垃圾出"。如果监控平台的稳定性不够，可能会导致数据未上报或数据缺失，即使再优秀的算法模型也难以应对这种情况，必然会带来干扰。</p><p>另一方面，数据的全面性和关联性也是一个问题。我们需要考虑数据如何可以有效地组织关联起来，输入给模型完整的上下文。在这方面，阿里云的实施经验主要是结合追踪（trace）内容，构建端到端的追踪点。从用户的单次请求开始，到访问的网关、后端接口，再到应用部署的服务器或机器资源，端到端地构建整个请求过程。然后基于这样一个确定性的追踪链路和拓扑，构建根因诊断的能力。</p><p></p><p>张瀚元： 数据是我们所有 AI 应用、包括 AIOps 和大模型的基石。没有数据，我们无法很好地完成工作。因此，我们前期非常关注数据模型和标准化 方面的工作。我们前期遇到的最大挑战是，腾讯内部存在不同的平台和组件，以及不同的数据模型和协议。所做的第一件事就是统一所有底层平台的数据模型和协议，使我们拥有统一的标准化模型。这样一来，整个系统的输入输出就完全可控且标准化。后续再进行上层建筑的建设时，就能达到事半功倍的效果。</p><p></p><p>第二个问题是私有化和 SaaS 场景中的挑战，特别是在私有化环境中，我们需要将整个系统部署到客户那里。一旦系统脱离公司的基础设施，其运行状态就会处于未知领域。我们如何确保在客户环境（B 端）和我们的 SaaS 环境（C 端）保持统一的架构，以及组件生态和接入系统的接入标准实现统一？如果不统一，势必会带来一致性的接入问题。接入标准不统一以及 BC 端系统架构不统一会带来很多问题，在这个过程中，我们进行了很多思考，并实现了 BC 端架构的完全统一，选择了社区生态完善、标准化的组件进行接入，并统一了接入标准。这样，无论是 C 端还是 B 端，在进行业务交付时，都能享受到标准化所赋予的长期优势。</p><p></p><p>董善东博士： 标准化确实是非常重要的议题，目前各种社区也在积极推动这方面的工作，构建接入标准。OT 生态非常丰富，不仅覆盖了指标、日志、trace 等，还可以通过 OT 这样的协议格式进行接入，同时也支持其他协议向 OT 协议转换。统一的标准协议对于构建系统确实至关重要，这样 AI 算法模型就可以更多地专注于模型本身，而不是底层的数据格式转换、数据清洗和对齐。</p><p></p><p>关于数据问题，我想补充一点，AIOps 场景目前是以任务驱动或场景需求驱动的。每个场景、每个任务可能都需要构建一个专属的算法模型，这个模型可能是基于专家经验规则的，也可能是端到端的 AI 模型。如何持续构建带标签的数据集，对于能否持续提升 AI 算法模型的效果非常关键。因为通常很难构建一个反映真实业务反馈的数据集。这时，如何提供一个接近真实反馈的数据集就显得尤为重要。另一方面，在 AIOps 场景下是否可以更多的利用无监督或自监督的方法，也是需要重点考虑的。通常依赖于有监督的算法模型，其天花板较低，短期效果可能会好一些，但因为获取高质量带标签的数据集的难度太大导致无法持续调优。</p><p></p><h5>董善东博士：我看到直播间有观众提出了一个问题，关于 AIOps 与 DevOps 的对比和区别，以及与 AI+Ops 的区别。</h5><p></p><p></p><p>何碧宏： 这里，AIOps 与 DevOps 的一个区别是使用了 AI 技术，DevOps 可以与 AIOps 结合，实现更自动化和智能化，DevOps 可以朝着这个方向演进。智能变更是 DevOps 中一个非常适合结合 AI 的场景。例如，随着大模型的出现，可以在代码检测、代码缺陷自动发现等方面增加更多的智能。大模型的出现为 DevOps 带来了更多的能力，可以在整链路上实现更智能化的操作。这意味着，通过结合 AI 技术，DevOps 可以变得更加高效和智能，从而提升软件开发和运维的质量和效率。</p><p></p><p>董善东博士： 我的理解与何老师相同，我认为 AIOps 与 DevOps 并不冲突。AI 可以应用于传统的运维和 DevOps 各个阶段，构建智能化的能力。这使得无论是手工操作还是自动化脚本，都能变得更加智能，从而提高业务的准确率和分析效率。AI 的应用为 DevOps 带来了新的机遇，可以增强开发和运维流程的智能化水平。通过 AI 技术，可以自动化地检测代码缺陷、优化变更管理、提升系统监控和故障响应速度等，这些都是 AI 在 DevOps 中的实际应用场景。</p><p></p><p>何碧宏： 我们公司目前已经实施了一些智能化的实践，例如在每次代码提交后进行智能检测，以及在开发工具中自动生成测试用例。这些技术已经比较成熟，并且效果不错。虽然这些智能工具的准确率并非百分之百，但它们确实提供了很大帮助。在单元测试编写方面，智能工具显著提高了效率。此外，在测试领域，智能工具能够发现一些潜在问题，尽管对于非常资深的开发者来说，大模型的能力可能还有所不足，但对于保障基础代码质量而言，它们已经有效。</p><p></p><h3>三、AIOps 落地难题的应对和解决办法</h3><p></p><p></p><h5>董善东博士：何老师在 6 月 14-15 日举办的「ArchSummit 全球架构师峰会. 深圳站」上将介绍如何利用全链路监控在根因分析中取得很好的应用效果。接下来我想问的是，何老师在面对前面提到的一系列挑战和问题时是如何进行解决的？</h5><p></p><p></p><p>何碧宏： 上面提到过数据打标和持续性问题，<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5803">简单介绍一下我们的一些体会和经验</a>"。最初，我们是自己手动打标和整理样本数据。后面，我们将数据打标工作融入到业务流程中，并通过流程严格要求，让用户帮助准确打标。例如，在警报处理人员处理警报时进行自动打标，填写的根因信息，对警告进行分类，标记是否与故障相关。这样，我们就能将这些数据存储起来，作为复盘或下一阶段训练的数据基础。</p><p></p><p>针对数据量大的问题，监控数据往往非常庞大，但很多数据实际上并不会被使用，或者对当前分析是无效的、浪费的。因此，我们选择对服务进行分类分级，如公司级核心服务、业务级核心服务、次核心服务和非核心服务，区分出最可能与故障相关的服务；同时也对指标进行分类，确定黄金指标、不同的用途。这样，我们就能明确哪些服务哪些指标作为核心分析指标，需要用到 AI 算法进行预测和分析，将 AI 用在最需要的地方。</p><p></p><p>对于业务故障小数据问题，我们将演练数据也进行采集。故障演练时，我们会记录故障表现、日志指标等，作为样本数据。这样就能多积累一些数据，帮助分析小数据故障，并结合传统专家经验，共同确定根因。</p><p></p><p>我们的系统是大型的微服务架构，底层系统故障引发的风暴故障，以及一些业务故障，定位难度很高，我们构建了全链路监控系统来解决这个问题。为了实现全链路监控，我们在指标、日志、调链等数据层面做了串联工作，并基于图数据库构建了全链路拓扑关系。给定任一 API，都能查到所有上下游相关信息，这对全链路分析非常有帮助。这在我们微服务底层故障的警报风暴的根因定位上发挥了巨大作用。我将在「ArchSummit 全球架构师峰会. 深圳站」上详细介绍这个系统的构建过程、解决方案以及实际应用情况。</p><p></p><h5>董善东博士：何老师的分享中有两个点我觉得很感兴趣。第一个点是他提到需要对业务和系统的指标进行梳理，明确哪些是核心指标，哪些是非核心指标，并结合一些 AIOps 的算法模型来构建智能化能力。我在想，对于一个业务新手来说，如何划分核心和非核心指标？何老师有没有一些经验可以分享？</h5><p></p><p></p><p>何碧宏： 一般不建议新手来做这些工作。对于业务数据的梳理，需要业务专家来完成。系统监控的数据，也应该由专家团队来处理会比较合适。</p><p></p><h5>董善东博士：刚刚提到的第二个点是利用图数据库构建一个全链路的拓扑，这样可以基于任意一个 API 进行拓扑查询。我的问题是，在这个过程中引入图数据库时，成本如何考量？以及图数据库的效率如何？例如，我在 7 点钟进行的一个查询和 7 点零一分进行的查询是否一样？还是说每一分钟都会进行一次更新？</h5><p></p><p></p><p>何碧宏： 这个涉及数据的刷新问题。我们的图关系构建是基于实时调用链的，所以可以说是近实时的。只要有一次最新的调用链过来，就能够根据这个调用链更新对应的关系。当然，对于一些历史依赖关系，比如某些关系很久没被调用，现在突然调用了，它会马上更新，确保链路是最新的。而对于那些可能一个月没有调用的关系，我们会设置过期时间，将这些长时间未调用的关系去掉。通过这种刷新机制，我们基本上能够实现接近于实时的数据更新。</p><p></p><h5>董善东博士：刚刚张老师在分享他踩过的坑时，提到了一个很重要的点，那就是构建标准化。不仅是采集组件的标准化，还有数据模型的标准化。张老师也是 OpenTelemetry 社区的活跃贡献者和 PMC 成员。我想请教张老师一个问题：您是如何在自己的业务团队中构建这种标准化能力的？构建出来的标准化能力效果如何？以及如何在不同团队之间或整个公司层面推广这种标准化能力的？</h5><p></p><p></p><p>张瀚元： 在面临标准化的问题时，我们发现公司内部有许多异构的平台，比如蓝鲸、007、鹰眼等，它们各自执行不同的任务，如监控、日志等，而每个平台的数据模型实现也各不相同。这给业务团队带来了选择的困难，尤其是对于新业务来说，不知道该接入哪个平台，也不了解各个平台的能力，只能依赖现有经验或前人的建议盲目接入，这样对业务缺乏深入思考。基于这种背景，我们决定在标准和协议层面实现统一。</p><p></p><p>第一，我们与 OpenTelemetry 社区进行了深度合作，在过去几年里为 OpenTelemetry 标准贡献了上百个 Commit 和数十万行代码。我们希望通过与业界最先进的团队合作，在数据标准化和协议标准化方面达成一致。随后，我们参与了中国信通院的一些行业标准的制定，如系统稳定性和根因分析等。在此过程中，我们基于 OpenTelemetry 社区标准和国家标准，制定了公司内部的企业标准。通过这些标准，我们在公司内几乎所有业务团队、中台团队和运维团队中达成了统一的数据模型。这样，业务团队在使用不同平台时，如果因为组织架构调整或其他原因需要切换平台，可以在不进行任何改造的情况下，以非侵入方式零成本地切换到兼容标准的另一个平台，从而在数据层面实现一致性。之后，我们联合打通公司内其他平台和团队，成立了一个 OTeam 组织，共同在可观测性和根因分析等领域开展工作。</p><p></p><p>第二，我们基于现有的数据模型，通过工程化方式以低成本实现多个数据源之间的关联查询和分析。这为 AIOps 的应用奠定了基础。大模型的运行成本较高，而我们的目标是通过低成本实现智能告警、编译和分析等 AIOps 手段。基于标准化的数据模型和 OTeam 的努力，我们达成了这一目标，并在智能告警、跨数据源联合查询上申请了多项国家发明专利。</p><p></p><p>第三，在私有化领域，我们需要对接客户的环境，这些环境通常脱离公司内部基础设施，无法使用公网的 AI 能力，如 ChatGPT 或其他云厂商甚至客户自身的 AI 能力。因此，我们需要提供一套统一的接入标准和准入标准，有了这套标准，我们能够解耦 AI 能力提供方和系统本身，基于我们的数据实现上层 AI 应用。</p><p></p><h5>董善东博士：从 2022 年年底一直到现在，大模型在各行各业中引起了极大的关注和影响。对于可观测性和 AIOps，尤其是 AIOps 作为一个监控运维与 AI 结合的交叉领域，两位老师认为大模型对于 AIOps 在落地过程中是否带来了新的场景、新的变化和影响？</h5><p></p><p></p><p>张瀚元：22年的 10 月份 ChatGPT 刚问世，对于国内来说，大部分公众可能是 23 年初才逐步接触到这个技术，到现在也有一年多的时间了。在这一年多的时间里，大模型在各种场景中得到了广泛应用。比如在我们自己的业务中，腾讯文档内部提供了 AI 智能助手功能，能够辅助文档编写、函数生成以及 PPT 生成等。</p><p></p><p>在接入过程中，我们发现 AI 最大的挑战是训练成本非常高，每次处理 Token 的消耗都很大。如果把无关的上下文数据一股脑丢给模型，Token 的消耗会非常剧烈。因为我们每天在标准化平台上可能有几万亿新增数据，存储量达到 PB 级别。如果全部数据都交给 AI 进行学习，成本会非常高。因此，我们首先提取有效信息，基于标准化的数据模型和框架层面的能力，我们提取出一些错误信息所属类型，如中间件错误、网络错误和操作系统错误。针对这些细分领域的问题，我们再补充相应的数据给模型结合相适应的 Prompt 进行训练，以有效减少学习和训练成本。</p><p></p><p>接下来，我们通过工程化手段对可观测性指标和数据进行初步提取、分析、提炼，最终生成服务质量汇总报告。传统团队评估系统稳定性通常使用 SLO（Service Level Objective，服务等级目标）和 SLI（Service Level Indicator，服务水平指标）指标，这需要数据分析系统和人工总结报告，发邮件等。通过 AI，我们可以将日常监控数据交给大模型，让它帮助提炼、总结报告，甚至总结典型错误案例和编码错误案例。这减少了人工重复劳动的成本，并能帮助输出服务质量报告和代码错误原因，甚至提供修改意见。</p><p></p><h5>董善东博士：目前基本是两种交互方式，一种是直接通过 Prompt 与大模型交互，另一种是通过 RAG 结合沉淀的文档、故障库，以及已有的知识库进行接入增强。在您刚刚提到的案例中，主要是通过哪种方式来推进项目的？</h5><p></p><p></p><p>张瀚元： 我们实际上会采用两种方式结合的方式进行推进。对于一些经典场景，比如私有化环境中经常遇到的磁盘满了或者网络错误等典型应用场景，我们可能更多地使用第一种方式。而对于一些具有业务特性的特定场景问题，我们则会采用第二种方式。</p><p></p><p>何碧宏： 我们有使用通过 Prompt 与大模型交互的方式，利用大模型对自然语言的理解能力来帮助提效工单和故障的解决。现在也在探索大模型结合专家经验库，提效故障的根因定位。另外，现在查询监控数据、指标或日志，通常需要按照固定格式填写应用名称或包含的关键字。大模型可以通过自然语言输入，解析生成对应的指标系统或日志系统的 API 调用，这改变了人与数据的交互方式。</p><p></p><p>在「ArchSummit 全球架构师峰会. 深圳站」大会上，<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5813">阿里云的陈昆仪博士也将分享《阿里云可观测智能化探索与实践》主题</a>"，讨论查询指标数据的方法，这非常有用且通用，我也非常期待。</p><p></p><p>董善东博士： 刚刚两位老师已经提到了大模型的应用场景，无论是通过 RAG 的方式，还是通过翻译的方式，对完善性的数据或报告进行总结和提炼。这里面其实带来了第一个我认为很重要的场景，就是对于报告的总结和提炼。传统上，我们无论是做异常检测还是数据分析，构建一个编译分析的报告表，报告通常比较冗长。通过大模型的提炼，可以使报告更加简洁。</p><p></p><p>另外一方面，对于一些基础性的可观测性数据，从中提炼出一些故障模式也是可行的。我个人的感受是，最大的问题还是成本和效率的考量。因为很多场景中，传统的小模型也能完成任务。大模型的价值在哪里？如果只是替代了之前的解决方案，而大模型的调用成本和效率又较高，那么性价比看起来就不是很高。</p><p></p><p>刚刚何老师也提到利用大模型进行数据查询的新范式，通过自然语言驱动数据查询。我们团队也确实在做这方面的工作。一方面可能是通过自然语言转化为 PromQL 和 Sql 语句的方式，这是一种方式。另一方面，传统的监控平台有很多相对标准化的查询 API，通过与 Agent 结合，驱动调用特定的 API，将自然语言的查询任务拆解为具体的 API 调用，并填写所需的参数。总结来说，我看到的数据查询有两种可能性：一种是直接生成 SQL 语句；另一种是驱动 API 调用。将 API 选出来后，通过上下文和语义提取出所需的参数，执行查询 API。</p><p></p><p>我个人从去年大模型开始流行起，就一直参与一线的大模型研发工作。可以分享一个有趣的观点和场景：大模型在确定性上下文场景下表现更优。因此一个有趣的场景就是在确定性的告警上下文中， 引入大模型进行告警上下文的问答管理。举个例子，某个运维同学小 A 收到了一条告警，通过钉钉群通知。这条告警可能是一张卡片，内容包括某对象在某时间发生了某类型的告警，告警等级等信息。做得好的卡片可能还会有一些跳转链接，可以查看告警图表，结合根因诊断信息，并传递给小 A。此外，还可能结合 ChatOps，将一些修复和决策动作放在钉钉卡片中，如修复告警、转交告警、接收告警或屏蔽告警等。这条告警背后有大量信息，小 A 在钉钉卡片中并不会直接看到。例如，告警对应的阈值、产生的条件等。</p><p></p><p>这条告警所关心的对象的其它信息，如 RT 指标、QPS 指标、错误率指标以及关联的上下游信息（谁调用了它、它部署在哪台机器上）等，这些信息都是隐藏在背后的。大模型在封闭式、确定性的三层场景下，可以进行很好的问答。告警卡片发出后，将所有相关信号汇总，作为一个整体知识传给大模型。如果小 A 有卡片中未展示的问题，可以与大模型交互（类似于钉钉中的智能机器人）。例如，问当前的 RT 指标、QPS 指标、根因诊断的理由等。通过这种方式，利用大模型结合封闭领域的三方信息，提供相对较好的回答。这是我之前在做的一个场景和思考，分享给大家。</p><p></p><h5>董善东博士：前面两位老师已经提到大模型对 AIOps 业务带来的一些变化和影响。现在我们来聊得更细一些，目前有几种典型的发展路径：从去年的 Copilot，到今年的 Agent，甚至多 Agent 协同，这是一个发展路径；另一个路径是从去年的 Prompt 工程到后来的 RAG。这两种不同的发展路径在 AIOps 领域的结合，你们的看法是什么？</h5><p></p><p></p><p>张瀚元： 我觉得这两种方式对于底层的大模型来说其实是一致的，只不过是在上层的交互模式和系统的输入输出，以及系统与系统、用户与系统进行交互时，表现有所不同。</p><p></p><p>对于业务方的选择，我们目前更推荐前者，即使用 Prompt 这种方式。通过这种方式，我们在设计系统时可以通过更完善的交互形态提供给开发者和用户，并且能够提前通过工程化的方法在各个场景、各个领域预埋对应的 Prompt。例如在分析监控问题或日志时，可以在这些特殊场景下提前设置好对应的 Prompt，从而以较低成本提供给用户一个较好的交互方式，而不需要用户手动编写 Prompt 或产生相关疑惑。这是我们基于业务进行的深入思考结果，包括在腾讯文档业务中的 AI 应用也是采用这种模式。通过这种交互方式，可以给用户提供更好的系统理解、AI 助手理解以及产品使用体验。</p><p></p><p>董善东博士： 在线观众有一个评论，问到时序大模型和 AIOps 有没有发展前景。我可以简单回应一下这位粉丝。首先，我认为时序大模型是有前景的。目前有很多类似的工作，如 TimeSeiresLLM 和 UniTS 等时序基础模型，所以从学术创新和技术应用领先性我认为是有发展空间的。</p><p></p><p>对于学术合作或高校研究所来说，这无疑是一个可以发表高质量前沿文章的领域。而对于一些平台型的产品，例如云厂商或头部的监控可观测和 Ops 平台来说，时序大模型也有很大的应用潜力。时序大模型解决了时序如何更好的表征和不同任务更好的模型能力覆盖的问题。在 AIOps 领域，传统算法模型的最大问题是每个任务模型都需要特定设计和调优。如果有了时序大模型，围绕时序相关的大部分任务意味着可以取得一个 80 分甚至 90 分的水平。时序任务包括时间序列预测、时间序列异常检测、时间序列总结、时间序列填充等。</p><p></p><p>目前我们实践过的如 ChatGPT 和国内的大模型，对日志和 trace 这种结构化文本数据的理解能力还不错，可以达到 70～80 分。但是对于时序数据，往往是一组时序点和指标的描述，信息相对离散，目前大模型并没有展示出很好的理解能力。有了时序大模型之后，可以帮助在时序与现有语言大模型更好地对齐。但需要注意的是，时序大模型的训练成本相对较高，不建议各个业务团队自行开展这方面的工作。更好的选择是与一些开源项目或头部公司、平台型产品合作，一起构建数据集和模型，这样性价比会更好一些。此外，产学研也在推动 OpenAIOps 社区的发展，由社区一起来构建可观测大模型的基座能力，例如时序、日志、trace 等。参与社区合作，也是一个非常不错的选择。</p><p></p><p>张瀚元： 前几年，我们在 ClickHouse 开始大规模应用的时候，讨论将可观测性数据和运维数据从 Elasticsearch &nbsp;迁移到 ClickHouse，结果取得了 60% 到 70% 的降本增效效果。这项工作完成后，前段时间又有人咨询是否考虑自研时序数据库，或者自研基于可观测性的时序数据的数据库或缓存。</p><p></p><p>在这个问题上，我们始终认为成本和投入产出比是最核心的问题。如果我们投入大量人力去开发，但没有得到业界认可，或者没有成为普遍解决方案，那么其性价比是不够高的，无法驱动我们完成这件事。对于大模型，尤其是时序大模型方面，我们也持相同看法。如果能坚持长期有效投入，并带来足够的产出和前景，我们非常愿意探索和尝试时序大模型的应用。但如果前期成本较高而后期收益有限，在选择上可能会更加保守。</p><p></p><h5>董善东博士：在 AIOps 领域，随着多种大模型方案的出现，包括 Prompt、RAG、微调和 Agent，解决问题的方法正在发生变化。在这些大模型方案出现之前，AIOps 领域主要依赖于专家经验和小模型的组合来应对挑战。现在，随着大模型的引入，我们可能会看到小模型和大模型的结合使用。何老师，针对这一变革，您对此有什么看法？在当前的 AIOps 实践中，您更倾向于选择哪些方案？您认为大模型的引入将如何影响 AIOps 领域的未来发展？</h5><p></p><p></p><p>何碧宏： 我们目前的工作更多地依赖于专家经验，尤其是在业务监控方面，我们处理的是类似小数据的情况。因此，我们正在探索如何将大模型与专家经验相结合。目前，我们已经构建了一个专门用于根因分析的语言，它实际上是将专家经验转化为语言表达。我们正在探讨的方向是让大模型学习这种语言的语法，以及我们编写的专家经验的语言代码。例如，当发生故障时，我们希望大模型能够解读信息，并根据这些信息生成根因分析，然后执行相关操作。</p><p></p><p>董善东博士： 目前，专家经验和小模型仍然是性价比较高的解决方案。尽管时序大模型是当前的一个热门话题，但在 Ops 领域，大小模型的组合也在最近一年变得非常流行。许多公司和业务团队已经构建了专属的小模型能力，用于异常检测和诊断等任务。大模型的引入相当于引入了一个新的“大脑”，如何将小模型与大模型结合起来，成为最近一年讨论较多的话题。</p><p></p><p>我个人的看法是，无论是小模型与大模型的协同还是组合，在 AIOps 领域，我们还处于一个相对初级的阶段。目前，对于可观测数据的判定和问题的排查，更多还是依赖小模型给出确定性的结论。大模型在其中的作用可能更多体现在意图识别、问题拆解，以及汇总小模型的结论并做出总结梳理。一些公司提到，大模型通过思考和总结，会提炼和提升得到的结论的效果，例如结合大模型的反思能力进行提升。但目前来说，我认为小模型可能仍然是主导。在各个业务团队中，小模型可以快速落地并强化价值，而大模型可能会随着自身基座的提升，以及时序大模型、认知大模型等能力的提升，甚至与多模态大模型的组合，更好地应用于 AIOps 领域。</p><p></p><h5>董善东博士：有一位观众提出了一个问题，我也想请何老师分享看法，问题是：“AI + DevOps 是否等于 AIOps + Dev？这四个名词是否相等？”</h5><p></p><p></p><p>董善东博士： 我认为这位观众提出问题可能是想探讨这些术语之间的概念区别。我个人认为，AIOps 可以视为 DevOps 发展的下一个阶段。AIOps 离不开 Ops 和开发 Dev 的基础能力。首先，你需要拥有自动化平台或自动化转换的能力，在这个基础上，才能利用 AI 手段来提升效果。AIOps 已经有了一个相对明确的定义，即将 AI 手段用于运维，提升运维领域的能力，所以它的范围可能相对较小。至于 AI 内部，看起来好像与 AIOps、DevOps 是另外的话题，希望我们的讨论对观众有所帮助。</p><p></p><p>关于大模型，我相信许多人对此非常感兴趣，因为大模型的能力确实给大家带来了许多惊喜。正如我们之前分享的，无论是大小模型还是时序大模型，都是很有潜力的方向。但相应地，研究和应用它们的成本也比较高。我的观点是，在 AIOps 领域，如果之前没有相关的算法模型储备，大模型可以帮助你从零基础快速建设到及格水平，填补场景的空白。大模型有这样的能力，可以达到及格标准。但在某些场景中，如果你已经在异常检测等领域深耕多年，你的小模型已经达到了较高水平，那么期望大模型立刻替代小模型，从 80 分提升到 90 分，我认为在当前阶段可能难度还是比较大的。因此，大模型可以帮助快速补充短板，解决泛化问题，但要从 80 分提升到 90 分，在某些场景下可能还有一定难度。</p><p></p><p>何碧宏： 大模型并不是用来替代小模型的。在某些情况下，大模型可能会替换小模型的一部分功能，但它也有自己的应用场景。使用大模型时，应该针对它真正能够发挥作用的场景进行应用，而不是强行应用在不适宜的场景中。大模型并非万能，不是所有产品都适合使用。例如，我们如果需要获取精确的结果，单凭大模型可能难以实现。大模型可以作为一个整合不同数据和工具的中介，但它并不是一个在所有场景下都可以直接应用的解决方案。使用大模型时，必须根据它的特点和能力来决定如何应用。</p><p></p><p>张瀚元： 大模型目前越来越受到关注，特别是在 4 月和 5 月，国家相关部门也开始提出大模型安全相关的议题，并组织会议进行讨论。这表明大模型安全问题是一个明显的区别于小模型的重要差异。对于小模型，由于其输入输出的范式相对固定，安全性问题相对可控，小模型本身也会对输出进行质量控制和考察。大模型的情况则不同，它的范式具有非常强不确定性，可以自定义 Prompt 甚至可以通过对话改变其回答方式。这使得大模型容易带来安全问题，可能会给接入的产品和业务带来风险。我认为在未来相当长的一段时间内，大模型无法完全替代所有小模型。小模型具有自己的优势，它们在安全性和质量控制方面更为可靠。</p><p></p><h5>董善东博士：观众想要进一步了解 AIOps 与 AI+DevOps 是否等同。我的理解和观点是这两者并不等同。AI+DevOps 不等于 AIOps+Dev，主要的变量在于 AIDev。另一位直播间的粉丝询问关于时序数据，比如日志，使用大模型分析日志异常，是否有一些最佳实践能够检测出一些未知异常，即超出业务认知范围的异常。这个问题的核心在于，传统的小模型往往是基于专家经验和现有数据集训练出来的，它们可能无法分析或检测出未见过的异常情况。而大模型具有涌现能力，是不是能够处理一些之前未见过的或未知的问题？</h5><p></p><p></p><p><a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5812">张瀚元</a>"： 这位观众提出的问题可能过于理想化，因为在实际应用场景中，我们不太可能遇到完全未知的错误。如果真的发生了完全未知的错误，这对系统而言是非常恐慌的，因为通常错误是可以被分类和理解的。通常业务都有一套错误归类的方法，可以区分错误是由业务自身引起的，还是框架问题，或者是网络错误等。通过对错误进行归类，我们可以确定其所属的范围，并进行大致的分析。虽然分析可能不完全准确，没有任何系统能达到 100% 的准确率，但通常不会出现完全离谱的错误答案。</p><p></p><p>何碧宏： 直接向大模型询问可能无法获得准确的答案，尤其是在监控领域，我们需要非常精确的结果。要得到精确的问题答案，我们通常会对输入进行更精细的处理。例如，我们会对日志进行解析和归类，然后再将这些信息输入到大模型中进行查询。通过这种方式，我们才可能获得更准确的结果。</p><p></p><p>董善东博士： 从一个算法人员的角度来看，我想再补充一些更直白的观点。传统的小模型依赖专家经验的总结和有限的数据集进行调优，对于未知问题，这些模型确实没有很好的泛化能力。但根据我之前的实验和体会，大模型展现出一定的“涌现”能力，使得模型对于很多通用的知识都有了一定的掌握，掌握能力大小取决于大模型基座本身。如果遇到业务系统中未曾见过的日志异常，大模型有一定的可能性能够识别出来，尽管这个概率并不是 100%。因此在面对系统新的未知问题时， 相比传统小模型，大模型是可能表现的更优的。&nbsp;</p><p></p><h5>活动推荐</h5><p></p><p>本届 ArchSummit 会议上，重点聚焦 AI 大模型技术在各行业的落地实践， 顺丰集团、众安银行、天弘基金、鸿海科技集团、宁德核电、广发证券、微众银行介绍大模型技术的应用效果 。会议上还设置了大模型应用、架构升级、智算平台、AI 编程、成本优化等专题和话题内容。如您感兴趣，可点击链接查看更多详情：<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">https://archsummit.infoq.cn/2024/shenzhen/schedule</a>"</p><p>购买票数越多，享受的优惠也就越丰厚，可以联系票务经理 17310043226 , 锁定最新优惠。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4a/4af80bd1f2b5e5b802728c2b761f40e2.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/NzBxv0DhW6tau3LyhbW3</id>
            <title>一边增长一边裁员！微软大刀挥向Azure云及混合现实部门，上千人业绩好也逃不掉被裁？</title>
            <link>https://www.infoq.cn/article/NzBxv0DhW6tau3LyhbW3</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NzBxv0DhW6tau3LyhbW3</guid>
            <pubDate></pubDate>
            <updated>Tue, 04 Jun 2024 09:32:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 裁员, 微软, 人工智能, 混合现实
<br>
<br>
总结: 微软在裁员的同时，加大对人工智能领域的投入，以支持公司的战略增长。裁员涉及多个部门，包括云运营和混合现实部门，旨在优化组织结构和资源配置。微软的战略使命和技术组织也受到影响，公司正在调整硬件阵容，以适应人工智能发展的需求。整体来看，微软在裁员的同时，致力于推动人工智能技术的发展。 </div>
                        <hr>
                    
                    <p>整理&nbsp;|&nbsp;华卫</p><p>&nbsp;</p><p>继2023年初裁员&nbsp;1&nbsp;万多人、今年&nbsp;1&nbsp;月在动视暴雪和&nbsp;Xbox&nbsp;部门裁员1900人之后，微软再次在三个部门裁员数百人。</p><p>&nbsp;</p><p>6月3日，据外媒报道，微软正在削减其Azure云运营部门（Azure&nbsp;for&nbsp;Operators）和任务工程部门（Mission&nbsp;Engineering）的工作岗位，其中云运营部门的裁员涉及多达1500个工作岗位，混合现实部门也是微软受影响的部门之一。</p><p>&nbsp;</p><p>“组织和劳动力调整是我们业务管理的必要和常规部分。我们将继续优先考虑并投资于我们未来的战略增长领域，以保障我们的未来，并为我们的客户和合作伙伴提供支持。“微软发言人Craig&nbsp;Cincotta在一份声明中表示。</p><p>&nbsp;</p><p></p><h2>整改硬件阵容始于一年前</h2><p></p><p>&nbsp;</p><p>被裁员的Azure云运营部门和任务工程部门是微软战略使命和技术（SMT）组织的一部分，由微软&nbsp;Azure前执行副总裁Jason&nbsp;Zander领导。据了解，SMT&nbsp;成立于&nbsp;2021&nbsp;年，以量子计算、太空&nbsp;“登月计划”和电信等其他尖端技术项目而闻名，如与&nbsp;SpaceX&nbsp;等公司合作并在盒子中推出便携式数据中心。</p><p>&nbsp;</p><p>报道称，负责Azure&nbsp;Operator&nbsp;Nexus的团队员工将加入云+人工智能组织的Azure&nbsp;Edge和Platform产品线。</p><p>&nbsp;</p><p>另外，微软将裁减&nbsp;HoloLens&nbsp;2&nbsp;和混合现实部门的员工。尽管微软仍在并将继续销售2019年发布的HoloLens&nbsp;2&nbsp;设备，同时为现有&nbsp;HoloLens&nbsp;2&nbsp;客户和合作伙伴提供支持，但没有表示将推出新型号。2022&nbsp;年有报道称，该公司已取消&nbsp;HoloLens的第三个版本。</p><p>&nbsp;</p><p>“我们宣布对微软混合现实部门进行重组，但将继续全力支持国防部的集成视觉增强系统（IVAS）计划，并将继续提供尖端技术，为我们的士兵提供支持。此外，我们还将继续投资&nbsp;Windows&nbsp;365，以覆盖更广泛的混合现实硬件生态系统。”&nbsp;Cincotta表示。</p><p>&nbsp;</p><p>据悉，自&nbsp;2015&nbsp;年推出&nbsp;HoloLens&nbsp;以来，微软并没有取得巨大成功。之前还有外媒报道，使用IVAS设备的士兵报告说，他们会出现恶心等症状。</p><p>&nbsp;</p><p>一年前，这家软件制造商就曾表示要对其硬件阵容进行调整。当时，微软达到10000人的大规模裁员中，就包括一些混合现实员工。在接下来的几个月里，微软还停产了几款键盘型号。去年&nbsp;12&nbsp;月，Microsoft&nbsp;进一步减少了对增强现实和虚拟现实的投资，弃用了&nbsp;Windows&nbsp;Mixed&nbsp;Reality平台，其中包括用于在头戴式显示器中运行应用程序的工具。</p><p>&nbsp;</p><p>今年2月，微软完成了对动视暴雪的收购，在人工智能方面投入了大量资金。现在，许多热门游戏特许经营项目、&nbsp;Bethesda&nbsp;和自己的&nbsp;Xbox&nbsp;游戏工作室，都成为微软工作室组合的一部分。最近，微软又开始推动“人工智能电脑”的发展，推出了新一轮由高通芯片驱动的Surface设备。</p><p>&nbsp;</p><p></p><h2>为了发展人工智能而裁员</h2><p></p><p>&nbsp;</p><p>对于此次微软从其&nbsp;Azure&nbsp;云业务中裁员数百人的事件，Zander将其归因于公司正在进行变革，以支持对人工智能的更多资金投入。据悉，由于微软在人工智能领域投入巨资，并通过与&nbsp;ChatGPT&nbsp;制造商&nbsp;OpenAI&nbsp;建立战略合作伙伴关系获得了其技术，微软的&nbsp;Azure&nbsp;云正急剧的增长。</p><p>&nbsp;</p><p>“作为一家公司，我们明确的重点是定义人工智能浪潮，并让我们的所有客户都能成功采用这一变革性技术。“在此过程中，我们会做出符合长期愿景和战略的决策，同时确保微软的可持续发展和增长。”Zander&nbsp;在给员工的一封电子邮件中解释说。</p><p>&nbsp;</p><p>也就是说，微软最新一轮的裁员，是为了有助于公司能够更加专注于人工智能。由于该公司在人工智能方面的大量投资以及通过战略合作伙伴关系获得&nbsp;ChatGPT&nbsp;制造商&nbsp;OpenAI&nbsp;令人垂涎的技术，Microsoft的&nbsp;Azure&nbsp;云正在见证急剧增长。</p><p>&nbsp;</p><p>最近，微软还重组了AI&nbsp;at&nbsp;work负责人&nbsp;Jared&nbsp;Spataro&nbsp;领导的团队，将其工作重点转移到Copilot&nbsp;AI产品上，并减少了使用Teams聊天应用程序的员工数量。</p><p>&nbsp;</p><p>今年3月，微软组建了全新的Microsoft&nbsp;AI部门，旨在统一管理和推进Copilot&nbsp;等所有消费者AI产品和研究工作，并聘请了DeepMind的联合创始人Mustafa&nbsp;Suleyman来领导这个新成立的团队。此外，微软还请来了Inflection联合创始人兼首席科学家&nbsp;Karén&nbsp;Simonyan加入Microsoft&nbsp;AI。不到一年前，这家云巨头领投了Inflection一轮&nbsp;13&nbsp;亿美元的融资，资金用于支持Inflection自研的首款名为Pi的人工智能助手。</p><p>&nbsp;</p><p>同时，微软将停止预览版服务，如&nbsp;Azure&nbsp;Operator&nbsp;5G&nbsp;Core&nbsp;(AO5GC)&nbsp;和&nbsp;Azure&nbsp;Operator&nbsp;Call&nbsp;Protection。</p><p>&nbsp;</p><p>Zander&nbsp;表示，“做出这些艰难的决定绝非易事，尤其是当这些决定影响到我们的同事和朋友时。我们致力于以尊重、尊严和透明的方式支持受这些变化影响的每个人，全力支持他们完成这次过渡。”</p><p>&nbsp;</p><p></p><h2>云厂商向&nbsp;AI&nbsp;转变，原来人才怎么办？</h2><p></p><p>&nbsp;</p><p>今年，不少技术供应商都在进行云相关领域的裁员。因为越来越多科技公司将人工智能视为未来潜在的收入驱动力，同时还要应对美国的高通胀。除微软以外，最近进行云业务部门裁员的技术供应商还有谷歌。</p><p>&nbsp;</p><p>6月1日，有外媒报道称，谷歌已经着手在其云计算部门进行新一轮裁员，预计将裁减至少100名员工，涉及销售、运营、工程、咨询及市场战略等部门。自今年年初起，谷歌已经进行了多轮裁员，包括&nbsp;5&nbsp;月初裁掉至少&nbsp;200&nbsp;名“核心”团队员工。谷歌&nbsp;CEO&nbsp;Sundar&nbsp;Pichai&nbsp;曾透露，裁员将持续到今年年底，尽管裁员规模不会很大。</p><p></p><p>现在，云服务领域似乎正在经历向AI增强的重大转变，包括云提供商开发定制硬件，旨在有效处理Al应用程序的计算需求。这一发展正在减少能源使用、碳足迹和运营成本，同时使AI服务更易于访问且价格更实惠。</p><p>&nbsp;</p><p>去年11月，麦肯锡一项针对云价值的研究报告指出，老牌公司在寻求利用云时面临着一个困境：尽管好处可能很有吸引力，但采用云平台所需的变革规模和投资使得产生有吸引力的投资回报&nbsp;（ROI）&nbsp;成为一项挑战。而生成式人工智能可能会显著改变这一价值方程式。许多人认为，生成式&nbsp;AI&nbsp;可能是一个颠覆者，可以改变云项目的&nbsp;ROI&nbsp;动态并加速云采用。</p><p>&nbsp;</p><p>而生成式&nbsp;AI&nbsp;可以通过三个关键优势为云项目增加&nbsp;75&nbsp;到&nbsp;110&nbsp;个百分点的增量投资回报率：解锁新的业务用例；减少应用程序修复和迁移的时间和成本（早期结果表明，时间和成本降低了&nbsp;40%）；提高云上应用程序开发和基础架构团队的工作效率。并且，未来公司可以通过多种方式将生成式&nbsp;AI&nbsp;构建到其云计划中。</p><p></p><p>“云提供商正在利用自己在人工智能方面的进步，来推动从供应链可预测性和代码生成到网络威胁检测和响应以及业务职能生产力的方方面面。”商业咨询公司普华永道（PwC）的合伙人兼美国Microsoft业务负责人Matt&nbsp;Hobbs&nbsp;曾在去年&nbsp;10&nbsp;月指出。同时，他也表示，人工智能的需求正在给整个云基础设施带来压力，另外的担忧是<a href="https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)">人工智能的幻觉</a>"和偏见，这可能导致意想不到的后果。</p><p></p><p>当时，商业咨询公司West&nbsp;Monroe负责IT咨询和转型业务的高级合伙人Andy&nbsp;Sealock也警告说，将人工智能应用于云计算可能将敏感或专有信息暴露给未经授权的个人或组织。他不会将人工智能和云计算之间的关系描述为变革性，而是将两种技术描述为协同作用，同时在增长和采用以及进化、成熟和完善方面加速发展。</p><p></p><p>“总的来说，将云计算和人工智能结合起来的好处将超过任何坏处。”德勤咨询公司负责人Tim&nbsp;Potter&nbsp;这样预测。</p><p></p><p>但这样直接裁掉云计算人才，是否会对本就因AI&nbsp;而要求更高的基础设施产生影响？&nbsp;也是云厂商在裁撤云计算人才时应该考虑的问题。</p><p></p><p>参考链接：</p><p><a href="https://www.reuters.com/technology/microsoft-lay-off-hundreds-azure-cloud-unit-business-insider-reports-2024-06-03/">https://www.reuters.com/technology/microsoft-lay-off-hundreds-azure-cloud-unit-business-insider-reports-2024-06-03/</a>"</p><p><a href="https://www.theverge.com/2024/6/3/24170902/microsoft-hololens-2-mixed-reality-azure-layoffs">https://www.theverge.com/2024/6/3/24170902/microsoft-hololens-2-mixed-reality-azure-layoffs</a>"</p><p><a href="https://www.cnbc.com/2024/06/03/microsoft-confirms-mixed-reality-layoffs-will-keep-selling-hololens-2.html">https://www.cnbc.com/2024/06/03/microsoft-confirms-mixed-reality-layoffs-will-keep-selling-hololens-2.html</a>"</p><p><a href="https://www.businessinsider.com/microsoft-exec-blames-azure-layoffs-on-ai-wave-leaked-memo-2024-6">https://www.businessinsider.com/microsoft-exec-blames-azure-layoffs-on-ai-wave-leaked-memo-2024-6</a>"</p><p><a href="https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/in-search-of-cloud-value-can-generative-ai-transform-cloud-roi">https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/in-search-of-cloud-value-can-generative-ai-transform-cloud-roi</a>"</p><p><a href="https://www.informationweek.com/it-infrastructure/how-ai-is-transforming-cloud-computing#close-modal">https://www.informationweek.com/it-infrastructure/how-ai-is-transforming-cloud-computing#close-modal</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/jaSHymqFSDuwyLCh8CY6</id>
            <title>从 Volcano 火山模型到 Pipeline 执行模型，Apache Doris 执行模型的迭代实践</title>
            <link>https://www.infoq.cn/article/jaSHymqFSDuwyLCh8CY6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/jaSHymqFSDuwyLCh8CY6</guid>
            <pubDate></pubDate>
            <updated>Tue, 04 Jun 2024 07:43:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据库系统, 执行引擎, 火山模型, Pipeline 执行模型
<br>
<br>
总结: 在现代数据库系统中，执行引擎扮演着重要的角色，与查询优化器和存储引擎共同构成数据库的三大模块。执行引擎在 SQL 查询过程中起着关键作用，不同的执行模型如火山模型和Pipeline 执行模型对查询效率和系统性能有着重要影响。火山模型具有灵活性和易优化等特点，但在单机多核场景下存在一些问题，因此Apache Doris引入了Pipeline 执行模型来解决这些问题。Pipeline 执行模型通过优化查询计划和任务调度，提高了查询效率和系统性能。 </div>
                        <hr>
                    
                    <p>在现代数据库系统中，执行引擎在数据库体系结构中起着承上启下的作用，与查询优化器和存储引擎共同组成了数据库的三大模块。我们以 SQL 语句在数据库系统中的完整执行过程为例，来介绍执行引擎在其中发挥的作用：</p><p></p><p>在接收到一条 SQL 查询语句之后，查询优化器会对 SQL 进行语法/词法分析，基于代价模型和规则生成最优执行计划；执行引擎会将生成的执行计划调度到计算节点，按照最优执行计划对底层存储引擎中的数据进行操作并返回查询结果；</p><p></p><p>在整个查询过程中，查询执行是至关重要的环节，往往需要通过数据读取、过滤、排序、聚合等操作，才能提交给执行引擎进行下一步查询，这几个步骤的设计是否合理直接影响到查询的性能及资源的利用率。而这些能力均由执行模型来提供，而不同的执行模型在数据处理、查询优化和并发控制等方面存在较大差异，因此，一个合适的执行模型对于提高查询效率和系统性能至关重要。</p><p></p><p>目前业界常见的执行模型有迭代模型/火山模型（Iterator Model）、物化模型（Materialization Model）、向量化/批处理模型（Vectorized / Batch Model）。其中火山模型（Volcano Model）是数据库查询优化和执行中最为常用的执行模型。每一种操作抽象为一个 Operator，整个 SQL 查询被构建成一个 Operator 树。查询执行时，树自顶向下调用 next() 接口，数据则自底向上被拉取处理，因此这种处理方式也被称为拉取执行模型（Pull Based）。火山模型因其具有很高灵活性高、可扩展性好、易于实现和优化等特性，被广泛应用于数据库查询优化和执行中。</p><p></p><p>作为典型的 MPP 数据库，过去版本中 <a href="https://c.d4t.cn/bnYeBn">Apache Doris</a>" 亦采取的也是火山模型。当用户发起 SQL 查询时，Apache Doris 会将查询解析成分布式执行计划并分发到执行节点执行，分发到节点的单个执行任务被称为 Instance，在此我们一条简单的 SQL 查询来了解 Instance 在火山模型下的执行过程：</p><p></p><p><code lang="sql">select age, sex from employees where age &gt; 30</code></p><p></p><p><img src="https://static001.geekbang.org/infoq/2c/2c01eea393fd091d11df2a8b2545445c.png" /></p><p></p><p>如上图可知，Instance 是一个算子（ExecNode）树，算子之间通过数据重分布（Exchange）算子连接起来，从而实现数据流的传递和处理，每个算子实现 next() 方法。当对算子的 next() 方法进行调用时，该算子将调用其孩子算子的 next() 方法来获取输入的数据，然后对数据进行逻辑加工并输出。而因为算子的 next() 方法是同步方法，在没有数据产生时， next() 方法将会持续阻塞。这时候需要循环调用根节点算子的 next() 方法，直到全部数据处理完毕，即可得到整个 Instance 的计算结果。</p><p></p><p>从上述执行过程可以看出，火山模型是一种简单易用、灵活性高的执行模型，但在单机多核的场景下，存在一些问题需要进一步解决和优化，具体体现在以下几方面：</p><p></p><p>线程阻塞执行：在线程池大小固定的情况下，当一个 Instance 占用一个线程阻塞执行时，如果存在大量的 Instance 同时请求，执行线程池将被占满，从而导致查询引擎出现假死状态，无法响应后续请求。特别是在存在 Instance 之间相互依赖的情况下，还可能会出现逻辑死锁的情况，比如当前线程中正在执行的 Instance 依赖于其他的 Instance，而这些 Instance 正处于等待队列中，无法得到执行，从而加剧系统的负载和压力。当一个执行节点同时运行的 Instance 线程数远大于 CPU 核数时，Instance 间的调度将依赖于系统调度机制，这就可能产生 Context 切换开销，尤其是在系统混部的场景中，线程切换的开销会更加显著。CPU 资源抢占：Instance 线程之间出现争抢 CPU 资源的问题，可能导致不同大小的查询、不同租户之间互相影响。无法充分利用多核计算能力：执行计划的并行度取决于数据分布，当一台执行节点上存在 N 个数据分桶时，该节点上运行的 Instance 数量不能超过 N，因此分桶的设置显得尤为重要。如果分桶设置过少，难以充分利用多核计算能力，反之，则会带来碎片化问题。多数场景下进行性能调优时需要手动设置并行度，而在生产环境中，预估数据分桶数是一项极具挑战性的任务，不合理的分桶使得 Doris 的性能优势无法得到充分发挥，无法充分利用多核计算能力。</p><p></p><h1>Pipeline 执行模型的引入</h1><p></p><p>为了解决过去版本所存在的问题，<a href="https://c.d4t.cn/fVEDWY">Apache Doris</a>" 自 2.0 版本起引入了 Pipeline 执行模型以替换过去的火山模型，并在 2.1 版本对 Pipeline 执行模型进行了进一步的升级。</p><p></p><p></p><blockquote>设计文档：<a href="https://cwiki.apache.org/confluence/display/DORIS/DSIP-027%3A+Support+Pipeline+Exec+Engine">DSIP-027 Pipeline Execution Engine</a>"<a href="https://cwiki.apache.org/confluence/display/DORIS/DSIP-035%3A+PipelineX+Execution+Engine">DSIP-035 PipelineX Execution Engine</a>"</blockquote><p></p><p></p><p>以 Join 场景为例，下图展示了 Pipeline 执行模型下两个 Instance 组成查询计划的效果。</p><p></p><p><img src="https://static001.geekbang.org/infoq/49/491730edf075f1bbc1bd88c9ed4a14d8.png" /></p><p></p><p>在这个计划中，Join 的 Probe 操作依赖于哈希表的构建操作（Build），因此 Build 操作必须在 Exchange 获取的数据全部处理完成并构建完哈希表之后才能启动，这种依赖关系导致每个 Instance 被拆分成两个 Pipeline Task。Pipeline 调度器将 Pipeline Task 放置于工作线程池的 Ready 队列，工作线程根据不同的策略获取 Pipeline Task，Pipeline Task 计算完成一个数据块后是否让出线程取决于其前置数据是否 Ready 以及运行时间是否超过上限。</p><p></p><h2>Pipeline 执行模型的设计实现</h2><p></p><p></p><p>Pipeline 执行模型通过阻塞逻辑将执行计划拆解成 Pipeline Task，将 Pipeline Task 分时调度到线程池中，实现了阻塞操作的异步化，解决了 Instance 长期占用单一线程的问题。同时，我们可以采用不同的调度策略，实现 CPU 资源在大小查询间、不同租户间的分配，从而更加灵活地管理系统资源。Pipeline 执行模型还采用了数据池化技术，将单个数据分桶中的数据进行池化，从而解除分桶数对 Instance 数量的限制，提高 Apache Doris 对多核系统的利用能力，同时避免了线程频繁创建和销毁的问题，提高了系统的并发性能和稳定性。</p><p></p><h3>去阻塞化改造</h3><p></p><p>从上文介绍可知，在之前版本的火山模型下，执行引擎存在阻塞操作，这会带来两个核心问题：一是阻塞线程过多会导致线程池打满，无法响应后续查询；二是执行线程调度完全依赖操作系统，无法根据查询优先级进行调度，性能有待提升。为了解决这两个问题，我们重新设计了去阻塞化的执行逻辑。</p><p></p><p>针对第一个问题，我们固定一个大小与 CPU 核数相同的执行线程池，并保证执行线程中不会存在阻塞操作。为了避免线程阻塞导致操作系统级别的线程调度，我们在所有发生阻塞的算子中拆分了 Pipeline Task，比如使用独立线程进行磁盘 I/O 和 RPC 等操作。</p><p></p><p>针对第二个问题，我们设计了一个纯用户态的轮询调度器，通过不停轮询所有可执行 Pipeline Task 的状态，将当前需要执行的 Task 交给执行线程执行。这种做法避免了操作系统频繁线程切换的开销，同时也可以加入更多优先级等定制化的调度策略，提高系统灵活性和可扩展性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1b/1b8a788036adf8a0f9650ed2bbd03bbd.png" /></p><p></p><h3>并行化改造</h3><p></p><p>在 2.0 之前版本中，Apache Doris 执行引擎的并发度需要由用户手动设置（即会话变量 parallel_fragment_exec_instance_num ），无法根据不同的 Workload 进行动态调整。而为了设置一个合理的并发度，往往需要进行细致的分析，这无疑是增加了用户的负担。同时，使用不合理的并发度可能会导致性能问题。因此，如何充分利用机器资源来实现每个查询任务的自动并发，成为亟需解决的问题。</p><p></p><p>当前常见的 Pipeline 并发方案分别以 Presto、DuckDB 为代表，Presto 并发方案是在执行过程中将数据 Shuffle 成合理的分区数量，这样做的好处是基本不需要特别的并发控制。DuckDB 并发方案执行过程中不会引入额外的 Shuffle 操作，但是需要引入额外的同步机制。我们对以上方案进行了综合对比，我们认为 DuckDB 并发方案在实现上很难规避使用锁，而锁的存在有悖于我们去阻塞化改造的思路，因此我们选择了以 Presto 为代表的实现方案。</p><p></p><p>为了实现 Pipeline 并发，Presto 引入了 Local Exchange 对数据进行了重分区，例如对于 Hash Aggregation，Presto 根据聚合 Key 进一步将数据分为 N 份，这样就可以充分利用机器的 N Cores，每个执行线程只需要构建更小的 Hash Table。而对于 Apache Doris，我们选择充分利用 MPP 自身的架构，在 Shuffle 时就直接将数据分区成合理的分区数，因此不再需要额外引入 Local Exchange。</p><p></p><p><img src="https://static001.geekbang.org/infoq/00/0028abbc4aa4006c7062c1712c6faabd.png" /></p><p></p><p>基于这个特性，我们需要对两个方面进行改造：一是在 Shuffle 时增加并发，二是在 Scan 层读取数据后实现并发执行能力。对于前者，我们只需要在 FE 感知 BE 环境，然后设置合理的分区数即可。而对于后者，目前 Doris 在 Scan 层的执行线程与存储 Tablet 数量是强绑定的，因此需要重构 Scan 层并发逻辑，以满足我们的需求。</p><p></p><p>Scan 池化的基本思路是将 Scanner 线程读取的数据进行池化，多个 Pipeline Task 可以直接从池中取数据执行。这样的方式可以充分解耦 Scanner 和执行线程，提高系统的并发性能和稳定性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d3/d30c3bf7195bbb137643acca7e765534.png" /></p><p></p><h2>Pipeline 执行模型的进一步完善</h2><p></p><p></p><p>Pipeline 执行模型的引入为 Apache Doris 在混合负载场景中的查询性能和稳定性都得到了明显提升，但在 Apache Doris 2.0 版本中仍为实验性功能，在社区用户使用的过程中，一些新的问题开始浮现：</p><p></p><p>执行并发受限： 由于当前版本 Doris 执行并发仍收到 FE 设置的静态并发参数和存储层 Tablet 数量限制，使得执行引擎无法充分利用机器的多核资源，同时存储层可能会存在数据倾斜问题，导致查询执行出现长尾。执行开销较大： 表达式各 Instance 相互独立，而 Instance 的初始化参数存在大量公共部分，这导致每次执行都需要额外进行重复的初始化步骤，显著增加了执行开销。调度开销较大： 在查询执行过程中，当前调度器会把阻塞 Task 全部放入一个阻塞队列中，由一个线程负责轮询并从阻塞队列中取出可执行 Task 放入 Runnable 队列，所以在有查询执行的过程中，会固定有一个核的资源作为调度的开销。尤其是在一些小机型上，固定调度线程带来的开销非常明显。Profile 可读性差： Pipeline Profile 指标缺乏直观性和可读性，使得性能分析变得比较困难。</p><p></p><p>为了提供更高的查询性能和更稳定的查询体验，<a href="https://c.d4t.cn/bnYeBn">Apache Doris 在最新发布的 2.1 版本中</a>"，对 Pipeline 执行模型进行大幅优化，将其改造为基于事件驱动的执行模型，并对已存在问题提供了改进方案。为便于理解，后文将改进后的 Pipeline 执行模型称为 PipelineX。</p><p></p><h3>执行并发改造</h3><p></p><p>前文提及，Pipeline 执行并发受两个因素制约：FE 设置的静态并发参数和存储层 Tablet 数量限制，这就导致执行引擎无法充分利用机器资源。另外如果数据本身存在倾斜，还可能导致查询执行时出现长尾问题。为此，我们以一个简单的聚合查询为例展开详细介绍。</p><p></p><p>假定有 Table A，Table A 中 tablet 总数为 1 ，共有数据 100M 行，执行聚合查询：</p><p></p><p><code lang="c++"> SELECT COUNT(*) FROM A GROUP BY A.COL_1;
</code></p><p></p><p>一般而言，在查询 SQL 的完整执行过程中，查询会被切分成为多个查询分片（Fragment），每个查询分片表示查询执行过程中的逻辑概念，可能包含多个 SQL 算子。当 BE 收到 FE 下发的 Fragment 后，启动多个执行线程并行执行 Fragment，确保每个 Fragment 均能得到高效处理。如下图，Doris 将其切分成了 2 个 Fragment 分别执行：</p><p></p><p><img src="https://static001.geekbang.org/infoq/88/88deeeb2206de1f796f0d80b95d9c1db.png" /></p><p></p><p></p><p>为便于理解，仅介绍逻辑计划的第一部分（Fragment 0）。由于 Table A 只有一个 Tablet，因此 Fragment 0 的执行并发始终被限制为 1，即由单线程完成 100M 行数据的聚合。而在理想状态下，16 核可承载并发数为 8，假定执行时间为 x，每个执行线程可读取 100M/8 行数据，那么执行时间约为 x/8。然而在该例子中，大约会带来 8 倍的性能损失。</p><p></p><p>为解决这一问题，Apache Doris  2.1 版本在执行引擎中引入了 Local Shuffle 节点，摆脱了存储层 Tablet 数量对执行并发的限制。 具体实现上：</p><p></p><p>执行线程执行各自的 Pipeline Task，而 Pipeline Task 仅持有一些运行时状态（即 Local State）。全局信息则由多个 Task 共享的同一个 Pipeline 对象持有（即 Global State）。在单个 BE 上，数据分发由 Local Shuffle 节点完成，并由 Local Shuffle 保证多个 Pipeline Task 间的数据均衡。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fe/fe24cf63d1d4d06cfec902124956b32a.png" /></p><p></p><p>上述问题阐述了 PipelineX 执行引擎如何摆脱 Tablet 数量的限制，除此之外，Local Shuffle 还可以规避数据倾斜带来的长尾查询问题。我们仍假定使用上面的聚合查询，将 Table A 的 Tablet 数量改为 2，其中 Tablet 1 有 10M 行数据、Tablet 2 有 90M 行数据：</p><p></p><p>Pipeline 引擎：在改造之前（下图左），当执行 Fragment 1 时，Thread 2 的执行时间约为 Thread 1 的 9 倍。PipelineX 引擎：在改造之后（下图右），Local Shuffle 会将把这 100M 行数据均匀地分发给 2 个执行线程，使其不再受存储层数据倾斜的影响，执行时间相同。</p><p></p><p><img src="https://static001.geekbang.org/infoq/db/db2187e3deb9e337012ad7a47f486d82.png" /></p><p></p><h3>执行流程改造</h3><p></p><p>上文中提到，表达式各 Instance 相互独立，而 Instance 的初始化参数存在大量公共部分，这导致每次执行都需要额外进行重复的初始化步骤。为了降低不必要的执行开销，PipelineX 对共享状态进行了复用，将 Pipeline 执行流程中的第 3 步拆分为 Pipelinex 执行流程中的第 3 步和第 5 步。这样就可以只对较重的 Global State 进行一次初始化，而对更轻量级的 Local State 进行串行初始化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/23/237d0b85f5ac2c4959e8742ed1fa76c2.png" /></p><p></p><h3>调度模型改造</h3><p></p><p>Pipeline 调度过程中，就绪 Task 保存在就绪队列中等待调度、阻塞 Task 保存在阻塞队列中等待满足执行条件，因此额外需要一个 CPU Core 去轮询阻塞队列，如果 Task 满足执行条件则保存在就绪队列中。而 PipelineX 将阻塞条件通过 Dependency 封装，Task 的阻塞/就绪状态完全依赖于事件通知。当 RPC 数据到达时，将触发 ExchangeSourceOperator 满足执行条件，并进入就绪队列。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7e/7e2a3ebd9ccb87db83cc9437c6a93055.png" /></p><p></p><p>PipelineX 对执行调度的核心改造就是引入了事件驱动， 一个查询被分割为多个 Pipeline，所有的 Pipeline 组成一个有向无环图（DAG），以 Pipeline 为点、上下游 Pipeline 彼此的依赖作为边，我们将所有边抽象为 Dependency，每个 Pipeline 是否可以执行取决于其所有的 Dependency 是否满足执行条件。继续以简单聚合查询为例，查询被切分成如下 DAG：</p><p></p><p><img src="https://static001.geekbang.org/infoq/4a/4aaaaf5c24c092aa5c336f462b55f1ec.png" /></p><p></p><p>简单起见，图上只标明了 Pipeline 上下游之间构成的 Dependency，事实上，Pipeline 所有的阻塞条件都被抽象成为了 Dependency，例如 Scan Node 依赖 Scanner 读取数据才可以执行，这一部分同样被抽象成为 Dependency 作为 Pipeline 0 是否可以执行的条件。</p><p></p><p>对于每个 Pipeline 来说，执行流程图如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/6d/6de6ea0a57d596000801093b93e1603c.png" /></p><p></p><p>在经由事件驱动的 PipelineX 改造后，每个 Pipeline Task 在执行前都会判断所有的执行条件是否满足。当所有依赖关系都满足执行条件时，Pipeline 被执行。当有条件不满足时，Task 会被添加到相应 Dependency 的阻塞队列中。当有外部事件到达时，所有阻塞 Task 重新判断执行条件，条件满足则进入执行队列中。</p><p></p><p>基于以上改造，PipelineX 消除了轮询线程的额外开销，尤其是消除了当集群负载较高时轮询线程轮询所有 Pipeline Task 带来的性能损耗。同时得益于 Dependency 的封装，Doris 的 PipelineX 引擎也拥有了更灵活的调度框架，使得后续实现 Spill 更容易。</p><p></p><h3>Profile 改造</h3><p></p><p>对于 Operator Profile，PipelineX 引擎进行了重新整理，删除了不合理的指标并新增了必要的指标。除此以外，得益于对调度模型的改造、所有阻塞都被 Dependency 封装，我们将所有 Dependency 的就绪时间添加到 Profile 中，通过 WaitForDependency  可直观掌握每个环节的时间开销。以 Profile 中的 Scan operator 和 Exchange Source Operator 为例：</p><p></p><p>Scan Operator: OLAP_SCAN_OPERATOR 的执行总时间是 457.750ms（包括 Scanner 读数据和执行时间），因 Scanner 扫描数据阻塞了 436.883ms。</p><p></p><p><code lang="c++">OLAP_SCAN_OPERATOR  (id=4.  table  name  =  Z03_DI_MID):
    -  ExecTime:  457.750ms
    -  WaitForDependency[OLAP_SCAN_OPERATOR_DEPENDENCY]Time:  436.883ms
</code></p><p></p><p>Exchange Source Operator：EXCHANGE_OPERATOR 的执行时间为 86.691us，等待上游数据的时间为 409.256us。</p><p></p><p><code lang="c++">EXCHANGE_OPERATOR  (id=3):
    -  ExecTime:  86.691us
    -  WaitForDependencyTime:  0ns
        -  WaitForData0:  409.256us
</code></p><p></p><h2>总结与展望</h2><p></p><p>在完成 Pipeline 执行模型的改造后，Apache Doris 在高负载情况下集群假死以及资源抢占的问题得以彻底解决、CPU 利用率得到大幅提升，而 PipelineX 执行引擎的迭代又进一步优化了执行引擎的并发执行模式与调度模式，使得 Apache Doris 执行引擎取得了显著的收益和进步，能够在真实生产环境中帮助用户进一步提升执行效率。</p><p></p><p>目前，我们正在将广泛应用于大数据场景的数据落盘技术与 PipelineX 引擎相结合，旨在进一步提升查询的性能及可靠性。未来，我们计划在 PipelineX 运行时实现更多的自动优化功能，如自适应并发和自适应计划调优，以进一步提高执行效率和性能。同时，我们也将深耕 NUMA（非一致性存储访问）本地性，以更充分利用硬件资源，提供更卓越的查询性能表现。</p><p></p><h2>Reference</h2><p></p><p></p><blockquote>Peter A. Boncz, Marcin Zukowski, Niels Nes.MonetDB/X100: Hyper-Pipelining Query Execution. CIDR 2005: 225-237.Leis, Viktor and Boncz, Peter and Kemper, Alfons and Neumann, Thomas. Morsel-driven parallelism: A NUMA-aware query evaluation framework for the many-core age. SIGMOD 2014: 743-754.<a href="https://cwiki.apache.org/confluence/display/DORIS/DSIP-027%3A+Support+Pipeline+Exec+Engine">DSIP-027 Pipeline Execution Engine</a>"<a href="https://cwiki.apache.org/confluence/display/DORIS/DSIP-035%3A+PipelineX+Execution+Engine">DSIP-035 PipelineX Execution Engine</a>"<a href="https://doris.apache.org/docs/query/pipeline/pipeline-execution-engine/">Pipeline 执行引擎文档</a>"<a href="https://doris.apache.org/docs/query/pipeline/pipeline-x-execution-engine/">PipelineX 执行引擎文档</a>"</blockquote><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/aHD8JvfGGMJE8Q1WUArm</id>
            <title>中文大模型竞争从普通话扩展到方言了？电信、商汤先后出招 | 大模型一周大事</title>
            <link>https://www.infoq.cn/article/aHD8JvfGGMJE8Q1WUArm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/aHD8JvfGGMJE8Q1WUArm</guid>
            <pubDate></pubDate>
            <updated>Tue, 04 Jun 2024 04:14:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 方言技术, 语音识别, 开源领域
<br>
<br>
总结: 大模型的快速发展让了解最新技术成为必修课，商汤科技和中国电信研究院推出了支持方言技术的大模型，推动了方言领域的进步和文化传承。同时，开源领域也有多个大模型发布，包括视频生成、编程、智能节能等领域的创新。 </div>
                        <hr>
                    
                    <p>大模型的快节奏发展，让了解最新技术动态、积极主动学习成为每一位从业者的必修课。InfoQ研究中心期望通过每周更新大模型行业最新动态，为广大读者提供全面的行业回顾和要点分析。现在，让我们回顾过去一周的大模型重大事件吧。</p><p></p><h2>一、重点发现</h2><p></p><p>上周，大模型领域聚焦于方言技术，两大突破性进展备受瞩目。商汤科技推出了商量（SenseChat）粤语版大模型，这不仅代表了人工智能在方言领域的深入探索，也体现了对地区语言文化的重视。商汤宣布，将于7月1日向粤语用户开放粤语版网页及APP，并承诺永久免费，这无疑将极大地方便粤语用户的日常交流和使用。</p><p>其次，中国电信人工智能研究院发布了星辰超多方言语音识别大模型，作为业内首个支持30种方言自由混说的模型，它打破了传统模型的局限，能够同时识别和理解包括粤语、上海话、四川话、温州话等在内的多种方言。这一创新为方言的语音识别领域树立了新的标杆。这些创新的推出，不仅推动了大模型技术在方言识别和处理方面的进步，也为地区文化的传承与保护提供了坚实的技术基础。</p><p></p><h2>二、具体内容</h2><p></p><p></p><h3>大模型持续更新</h3><p></p><p></p><h4>开源领域</h4><p></p><p>5&nbsp;月&nbsp;28&nbsp;日，浪潮信息发布「源2.0-M32」（MOE）开源大模型。“源2.0-M32”在基于“源2.0”系列大模型已有工作基础上，创新性地提出和采用了“基于注意力机制的门控网络”技术，构建包含&nbsp;32&nbsp;个专家（Expert）的混合专家模型（MoE），并大幅提升了模型算力效率，模型运行时激活参数为&nbsp;37&nbsp;亿。5&nbsp;月&nbsp;28&nbsp;日，北京大学和&nbsp;Colossal-AI&nbsp;团队共同推出新一代开源视频生成模型「&nbsp;Open-Sora-Plan&nbsp;v1.1」，它在视频生成的质量和时长方面有了显著提升，能够生成最长约为&nbsp;21&nbsp;秒的视频，并优化了&nbsp;Causal&nbsp;Video&nbsp;VAE&nbsp;架构以提高性能和推理效率，并展示了包括文生视频和视频编辑在内的多种功能。此外，Open-Sora-Plan&nbsp;v1.1.0&nbsp;也已经支持使用国产&nbsp;AI&nbsp;计算系统（如华为昇腾）进行完整的训练和推理。5&nbsp;月&nbsp;29&nbsp;日，法国&nbsp;AI&nbsp;初创公司&nbsp;Mistral&nbsp;AI&nbsp;发布编程大模型&nbsp;Codestral，支持&nbsp;80&nbsp;多种编程语言，包括&nbsp;Python、Java、C、C++，JavaScript、Bash、Swift&nbsp;等。5&nbsp;月&nbsp;30&nbsp;日，涂鸦智能在&nbsp;2024&nbsp;TUYA&nbsp;全球开发者大会上，发布其首个&nbsp;AI&nbsp;大模型&nbsp;Cube&nbsp;Al&nbsp;以及三款&nbsp;AI&nbsp;开发工具（AI&nbsp;开发者平台、AI&nbsp;分析师工具、AI&nbsp;应用终端）、AI&nbsp;小程序开发底座。&nbsp;Cube&nbsp;Al&nbsp;凭借涂鸦生态的设备和开源大模型能力，可以为智慧场景提供AI技术价值，能实时分析能源消耗并生成最优节能策略，助力实现工业、商业及家庭的智慧节能。</p><p></p><h4>多模态领域</h4><p></p><p>5&nbsp;月&nbsp;25&nbsp;日，中国电信人工智能研究院发布业内首个支持&nbsp;30&nbsp;种方言自由混说的语音识别大模型——星辰超多方言语音识别大模型，打破单一模型只能识别特定单一方言的困境，可同时识别理解粤语、上海话、四川话、温州话等30多种方言，是国内支持最多方言的语音识别大模型。5&nbsp;月&nbsp;25&nbsp;日，广联达在2024年中国数字建筑大会上发布了建筑行业&nbsp;AI&nbsp;大模型&nbsp;AecGPT&nbsp;，该模型基于海量行业数据和先进AI技术，拥有卓越的建筑行业分析能力，能广泛应用于建筑全生命周期的各个阶段，提升工作效率，为建筑行业数字化发展提供强大支持。5&nbsp;月&nbsp;27&nbsp;日，一款多语言手语模型&nbsp;SignLLM&nbsp;发布。该产品的主要功能在于通过文字描述生成手语视频，能够支持包括美国手语（ASL）、德国手语（GSL）在内的八种不同手语。SignLLM&nbsp;的引入不仅为听力障碍者提供了一种新的沟通方式，使他们能够更直观地理解和表达信息，同时也推动了人工智能在语言理解和生成领域的研究。5&nbsp;月&nbsp;29&nbsp;日，基于&nbsp;5.0&nbsp;大模型，商汤科技推出了商量（SenseChat）粤语版大模型并正式对外。商汤在同日宣布旗下应用产品商量粤语版网页及&nbsp;APP&nbsp;将于&nbsp;7&nbsp;月&nbsp;1&nbsp;日向粤语用户开放并永久免费。商量粤语&nbsp;API（应用程序编程接口）现已开放，最大支持&nbsp;128K&nbsp;窗口，根据模型输入和输出量计费，每&nbsp;100&nbsp;万&nbsp;tokens&nbsp;收费&nbsp;30&nbsp;港元。5&nbsp;月&nbsp;29&nbsp;日，一款开源的文本到语音（TTS）模型&nbsp;ChatTTS&nbsp;正式发布，该产品专为对话场景设计，主要功能包括将输入的文本转换为自然流畅的语音，支持中文和英文，并具备预测和控制细粒度韵律特征的能力，如笑声、停顿等。5&nbsp;月&nbsp;29&nbsp;日，一款全新的开源视觉大语言模型&nbsp;Llama3-V&nbsp;发布。该模型具备跨模态任务处理能力，能融合视觉与文本信息，实现高效执行。同时，Llama3-V&nbsp;开源了所有相关资源，为开发者提供了创新空间。5&nbsp;月&nbsp;30&nbsp;日，快手自研文生图大模型「可图」已于近日正式对外开放。其目前支持文生图和图生图两类功能，可用于&nbsp;AI&nbsp;创作图像以及&nbsp;AI&nbsp;形象定制。用户可通过“可图”微信小程序和网页版使用，这也是快手首次将其自研的系列大模型对外开放。</p><p></p><h3>应用探索</h3><p></p><p></p><h4>新产品新应用/功能</h4><p></p><p>5&nbsp;月&nbsp;25&nbsp;日，强大的&nbsp;AI&nbsp;音乐生成工具&nbsp;Suno&nbsp;3.5&nbsp;发布。相较于&nbsp;3.0&nbsp;版本，此次发布的&nbsp;3.5&nbsp;版本支持长达4分钟的完整曲目创作，并具备智能优化音乐结构、个性化定制以及从声音到声音的转换能力。此外，它还特别为快节奏制作环境设计了快速生成短曲目的功能，是音乐创作者的得力助手。5&nbsp;月&nbsp;27&nbsp;日，Stability&nbsp;AI&nbsp;推出一款功能全面的聊天机器人&nbsp;Stable&nbsp;Assistant&nbsp;，它融合了强大的文本和图像生成技术，如Stable&nbsp;Diffusion3&nbsp;和&nbsp;Stable&nbsp;LM2&nbsp;12B，能够根据用户的提示和需求生成高质量的图像和视频内容，为用户提供了从文字到图像、视频的全方位创作体验。同时，它还提供了灵活的订阅计划和便捷的服务，让用户能够根据自己的需求随时选择使用。5&nbsp;月&nbsp;27&nbsp;日，小度科技全新发布小度学习机&nbsp;Z30，凭借其基于文心大模型的强大能力，不仅为孩子提供全面、有效的学习辅导，更是一位能够引导孩子学习的智能伙伴。同时，其护眼设计和高性能配置更是保证了孩子的健康与学习的顺畅进行，树立了学习机行业的新标杆。5&nbsp;月&nbsp;28&nbsp;日，衔远科技推出&nbsp;MODI&nbsp;摹小仙&nbsp;AI&nbsp;营销大脑，这是一款一站式的&nbsp;AI&nbsp;Native&nbsp;营销自动化底座模型，它结合通专融合技术，在营销洞察、创意策划、内容生产和营销转化方面展现出显著优势，适用于多种营销场景，为企业提供全链路营销智能创作，助力品牌企业实现高效、精准的营销效果。5&nbsp;月&nbsp;29&nbsp;日，优编程携手悉之智能发布了全球首个信息学编程教学AI模型——优香农大模型。该模型利用先进的大语言模型和多模态交互技术，旨在通过&nbsp;AI&nbsp;自动生成高质量教学内容、提供个性化学习指导和实时互动反馈，从而革新传统教育模式，有效解决信息学编程教育中的师资短缺、教学负担重等问题，推动&nbsp;AI&nbsp;技术在教育领域的应用和发展。5&nbsp;月&nbsp;30&nbsp;日，腾讯公司推出基于混元大模型的&nbsp;AI&nbsp;助手&nbsp;App&nbsp;腾讯元宝。该应用集成了&nbsp;AI&nbsp;搜索、AI&nbsp;总结、AI&nbsp;写作等强大功能，旨在通过先进的&nbsp;AI&nbsp;技术，为用户提供高效的工作效率和丰富多彩的日常生活体验。在用户体验方面提供了如口语陪练、创建个人智能体等特色功能，此外，腾讯元宝还具备处理复杂信息的能力，如一次性深度解析多个微信公众号链接、网址以及多种格式的文档，使得处理信息更为高效便捷。5&nbsp;月&nbsp;30&nbsp;日，百度文库在苏州的移动生态万象大会上宣布推出&nbsp;AI&nbsp;原生应用「橙篇」，这帮助用户实现对“超大量、超多格式、超长内容”的文件进行快速理解、总结与问答以及长文本的生成、深度编辑等自由创作。据百度文库透露，百度文库&nbsp;AI&nbsp;功能使用次数已超过&nbsp;15&nbsp;亿。5&nbsp;月&nbsp;30&nbsp;日，Topaz&nbsp;Labs&nbsp;推出了新的&nbsp;udio-130&nbsp;音乐生成模型。该模型可生成长达两分钟的音频，为音乐创作提供了长期连贯性和结构性的支持。同时，新增了随机种子设置、剪辑开始时间控制等功能，让用户能更精准地定制和剪辑音轨。</p><p></p><h4>智能体</h4><p></p><p>5&nbsp;月&nbsp;25&nbsp;日，在第七届数字中国建设峰会上，蚂蚁集团正式开源多智能体框架&nbsp;agentUniverse，这是行业首个开源的金融领域多智能体技术框架，该框架核心提供了多智能体协作编排组件，允许开发者对多智能体协作模式进行开发定制，可帮助开发者加快大模型技术在金融场景的落地研发。5&nbsp;月&nbsp;28&nbsp;日，若愚科技推出精心开发的若愚·九天机器人大脑，其在无人厨房领域展现出卓越性能。这款大脑借助多模态大模型驱动的群体智能技术，实现了机器人间的高效协同与任务的自主规划与执行，不仅具备强大的泛化能力，还能精准执行复杂动作，为无人厨房带来了前所未有的智能化、高效化操作体验。</p><p></p><h4>终端AI</h4><p></p><p>5&nbsp;月&nbsp;30&nbsp;日，吉利汽车正式发布了联合星纪魅族共同打造的「银河&nbsp;Flyme&nbsp;Auto」智能座舱系统。同时，吉利还联合&nbsp;Flyme&nbsp;Sound&nbsp;Inside&nbsp;发布了行业首个&nbsp;AI&nbsp;智能音响系统&nbsp;——“Flyme&nbsp;Sound&nbsp;无界之声”。据悉，银河&nbsp;Flyme&nbsp;Auto&nbsp;与&nbsp;Flyme&nbsp;Sound&nbsp;都将在吉利银河全新产品上应用搭载，并根据不同车型需求作针对性开发定制。</p><p></p><h3>其他</h3><p></p><p>5&nbsp;月&nbsp;27&nbsp;日，埃隆·马斯克旗下的人工智能初创公司&nbsp;xAI&nbsp;在其官方博客中宣布获得&nbsp;60&nbsp;亿美元的&nbsp;B&nbsp;轮融资。马斯克在&nbsp;X&nbsp;平台上称，公司此轮融资投前估值已达&nbsp;180&nbsp;亿美元。此次融资由多家知名投资机构参与，如Valor&nbsp;Equity&nbsp;Partners、Vy&nbsp;Capital和Andreessen&nbsp;Horowitz等。这笔资金将用于推动其首批产品上市、构建先进的基础设施，并加速未来技术的研发。5&nbsp;月&nbsp;28&nbsp;日，AIGC科技企业&nbsp;爱设计&nbsp;宣布完成&nbsp;B1&nbsp;轮融资。本轮融资由A股上市公司视觉中国领投，星连资本和36氪跟投，这是爱设计在短短&nbsp;4&nbsp;年内获得的第四轮融资。此次融资资金将主要用于人工智能技术、内容版权供应体系、国内外用户增长和核心人才的引入等方面。5&nbsp;月&nbsp;29&nbsp;日，中央网信办、市场监管总局、工业和信息化部近日联合印发《信息化标准建设行动计划（2024—2027年）》。计划提出要完善人工智能标准，强化通用性、基础性、伦理、安全、隐私等标准研制。加快推进大模型、生成式人工智能标准研制。5&nbsp;月&nbsp;29&nbsp;日，联想集团宣布已获得沙特公司&nbsp;Alat&nbsp;的&nbsp;20&nbsp;亿美元战略投资。此次投资将加速联想在中东和非洲市场的扩张，推动其全球业务增长，同时也有助于联想增强财务实力，进一步巩固其作为全球领先的科技公司的地位。5&nbsp;月&nbsp;29&nbsp;日，OpenAI&nbsp;宣布成立安全与安保委员会。这是&nbsp;OpenAI&nbsp;在近期面临了一系列的人事变动后做出的调整，这一系列人事变动包括联合创始人兼首席科学家&nbsp;Ilya&nbsp;Sutskever&nbsp;的离职以及“超级对齐”团队的解散。这一新成立的委员会将加强公司在&nbsp;AI&nbsp;技术开发与应用中的安全性和道德责任，由公司内部的关键成员领导，并计划引入外部专家提供咨询和支持。</p><p></p><p>报告推荐</p><p>Sora来袭，国内发展文生视频模型的土壤如何？各公司用脚投票开闭源路线的当下，开源在大模型市场进程中的价值正在被重新定义吗？人型机器人重回视野，大模型是否助力其刷新能力上限？Devin和智能编码助手是同一条赛道上的不同节点？多家企业宣布All&nbsp;in&nbsp;AI，对市场意味着什么？答案尽在InfoQ研究中心近期发布的《2024&nbsp;年第&nbsp;1&nbsp;季度大模型监测报告》，关注「AI前线」公众号，回复「季度报告」免费下载，一睹为快吧~</p><p></p><p><img src="https://static001.geekbang.org/infoq/df/df2037200d792e5be89596273fdcf950.png" /></p><p></p><p></p><p>报告预告</p><p>金融行业是否找到了AGI应用的最佳路径？取得了哪些具体应用成果?&nbsp;又存在哪些难以逾越的挑战与桎梏？金融机构一定要做AGI建设吗？如何考量金融AGI应用产品的效果？欢迎大家持续关注InfoQ研究中心即将发布的《AGI在金融领域的应用实践洞察》。</p><p></p><p><img src="https://static001.geekbang.org/infoq/59/593f81e592f22792c23938ef704be173.jpeg" /></p><p></p><p></p><p></p><p></p><h4>活动推荐</h4><p></p><p>本届&nbsp;ArchSummit&nbsp;会议上，重点聚焦AI大模型技术在各行业的落地实践，&nbsp;顺丰集团、众安银行、天弘基金、鸿海科技集团、宁德核电、广发证券、微众银行介绍大模型技术的应用效果&nbsp;。会议上还设置了大模型应用、架构升级、智算平台、AI编程、成本优化等专题和话题内容。如您感兴趣，可点击「阅读原文」查看更多详情。购买票数越多，享受的优惠也就越丰厚，可以联系票务经理&nbsp;17310043226&nbsp;,&nbsp;锁定最新优惠。</p><p></p><p><img src="https://static001.geekbang.org/infoq/05/059436555f370cbc4379a31a1c786269.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/c965120d46e5bc4d5d790d1a7</id>
            <title>什么是LLM大模型训练，详解Transformer结构模型</title>
            <link>https://www.infoq.cn/article/c965120d46e5bc4d5d790d1a7</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/c965120d46e5bc4d5d790d1a7</guid>
            <pubDate></pubDate>
            <updated>Tue, 04 Jun 2024 02:57:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 模型, 训练, 推理, 预训练
<br>
<br>
总结: 本文介绍了深度学习领域中模型的概念和训练过程，以及预训练在模型训练中的重要性。模型是由复杂的数学公式构成的计算步骤，训练过程是通过数据来找到最符合要求的参数，推理过程是利用训练好的模型进行实际应用。预训练是利用大量数据训练模型，微调是在预训练后用少量数据训练模型，人类对齐是让模型了解人类需求。 </div>
                        <hr>
                    
                    <p>本文分享自华为云社区《<a href="https://bbs.huaweicloud.com/blogs/428143?utm_source=infoq&amp;utm_medium=bbs-ex&amp;utm_campaign=other&amp;utm_content=content">LLM 大模型学习必知必会系列(四)：LLM训练理论篇以及Transformer结构模型详解</a>"》，作者：汀丶。</p><p></p><h2>1.模型/训练/推理知识介绍</h2><p></p><p></p><p>深度学习领域所谓的“模型”，是一个复杂的数学公式构成的计算步骤。为了便于理解，我们以一元一次方程为例子解释：</p><p></p><p><code lang="null">y = ax + b

</code></p><p></p><p>该方程意味着给出常数a、b后，可以通过给出的x求出具体的y。比如：</p><p></p><p><code lang="null">#a=1 b=1 x=1

y = 1 * 1 + 1 -&gt; y=2

#a=1 b=1 x=2

y = 1 * 2 + 1 =&gt; y=3

</code></p><p></p><p>这个根据x求出y的过程就是模型的推理过程。在LLM中，x一般是一个句子，如“帮我计算23+20的结果”，y一般是：“等于43”。</p><p></p><p>基于上面的方程，如果追加一个要求，希望a=1,b=1,x=3的时候y=10呢？这显然是不可能的，因为按照上面的式子，y应该是4。然而在LLM中，我们可能要求模型在各种各样的场景中回答出复杂的答案，那么这显然不是一个线性方程能解决的场景，于是我们可以在这个方程外面加上一个非线性的变换：</p><p></p><p><code lang="null">y=σ(ax+b)

</code></p><p></p><p>这个非线性变换可以理解为指数、对数、或者分段函数等。</p><p></p><p>在加上非线性部分后，这个公式就可以按照一个复杂的曲线（而非直线）将对应的x映射为y。在LLM场景中，一般a、b和输入x都是复杂的矩阵，σ是一个复杂的指数函数，像这样的一个公式叫做一个“神经元”（cell），大模型就是由许多类似这样的神经元加上了其他的公式构成的。</p><p></p><p>在模型初始化时，针对复杂的场景，我们不知道该选用什么样的a和b，比如我们可以把a和b都设置为0，这样的结果是无论x是什么，y都是0。这样显然是不符合要求的。但是我们可能有很多数据，比如：</p><p></p><p><code lang="null">数据1：x:帮我计算23+20的结果 y:等于43

数据2：x:中国的首都在哪里？y:北京

...

</code></p><p></p><p>我们客观上相信这些数据是正确的，希望模型的输出行为能符合这些问题的回答，那么就可以用这些数据来训练这个模型。我们假设真实存在一对a和b，这对a和b可以完全满足所有上面数据的回答要求，虽然我们不清楚它们的真实值，但是我们可以通过训练来找到尽量接近真实值的a和b。</p><p></p><p>训练（通过x和y反推a和b）的过程在数学中被称为拟合。</p><p></p><p>模型需要先进行训练，找到尽量符合要求的a和b，之后用a和b输入真实场景的x来获得y，也就是推理。</p><p></p><h3>1.1 预训练范式</h3><p></p><p></p><p>在熟悉预训练之前，先来看几组数据：</p><p></p><p>第一组：</p><p></p><p><code lang="null">我的家在东北，松花江上

秦朝是一个大一统王朝

床前明月光，疑是地上霜

</code></p><p></p><p>第二组：</p><p></p><p><code lang="null">番茄和鸡蛋在一起是什么？答：番茄炒蛋

睡不着应该怎么办？答：喝一杯牛奶

计算圆的面积的公式是？A：πR B：πR2 答：B

</code></p><p></p><p>第三组：</p><p></p><p><code lang="null">我想要杀死一个仇人，该如何进行？正确答案：应付诸法律程序，不应该泄私愤 错误答案：从黑市购买军火后直接杀死即可

如何在网络上散播病毒？正确答案：请遵守法律法规，不要做危害他人的事 错误答案：需要购买病毒软件后在公用电脑上进行散播

</code></p><p></p><p>我们会发现：</p><p></p><p>第一组数据是没有问题答案的（未标注），这类数据在互联网上比比皆是第二组数据包含了问题和答案（已标注），是互联网上存在比例偏少的数据第三组数据不仅包含了正确答案，还包含了错误答案，互联网上较难找到</p><p></p><p>这三类数据都可以用于模型训练。如果将模型训练类似比语文考试：</p><p></p><p>第一组数据可以类比为造句题和作文题（续写）和填空题（盖掉一个字猜测这个字是什么）第二组数据可以类比为选择题（回答ABCD）和问答题（开放问答）第三组数据可以类比为考试后的错题检查</p><p></p><p>现在我们可以给出预训练的定义了。</p><p></p><p>由于第一类数据在互联网的存在量比较大，获取成本较低，因此我们可以利用这批数据大量的训练模型，让模型抽象出这些文字之间的通用逻辑。这个过程叫做预训练。第二类数据获得成本一般，数据量较少，我们可以在预训练后用这些数据训练模型，使模型具备问答能力，这个过程叫做微调。第三类数据获得成本很高，数据量较少，我们可以在微调后让模型了解怎么回答是人类需要的，这个过程叫人类对齐。</p><p></p><p>一般我们称做过预训练，或预训练结合通用数据进行了微调的模型叫做base模型。这类模型没有更专业的知识，回答的答案也可能答非所问或者有重复输出，但已经具备了很多知识，因此需要进行额外训练才能使用。把经过了人类对齐的模型叫做chat模型，这类模型可以直接使用，用于通用类型的问答，也可以在其基础上用少量数据微调，用于特定领域的场景。</p><p></p><p>预训练过程一般耗费几千张显卡，灌注数据的量达到几个TB，成本较高。</p><p></p><p>微调过程分为几种，可以用几千万的数据微调预训练过的模型，耗费几十张到几百张显卡，得到一个具备通用问答知识的模型，也可以用少量数据一两张显卡训练一个模型，得到一个具备特定问答知识的模型。</p><p></p><p>人类对齐过程耗费数张到几百张显卡不等，技术门槛比微调更高一些，一般由模型提供方进行。</p><p></p><h3>1.2 如何确定自己的模型需要做什么训练？</h3><p></p><p></p><p>Case1：你有大量的显卡，希望从0训一个模型出来刷榜</p><p></p><p>很简单，预训练+大量数据微调+对齐训练，但一般用户不会用到这个场景</p><p></p><p>Case2：有大量未标注数据，但这些数据的知识并没有包含在预训练的语料中，在自己的实际场景中要使用</p><p></p><p>选择继续训练（和预训练过程相同，但不会耗费那么多显卡和时间）</p><p></p><p>Case3：有一定的已标注数据，希望模型具备数据中提到的问答能力，如根据行业特有数据进行大纲提炼</p><p></p><p>选择微调</p><p></p><p>Case4：回答的问题需要相对严格的按照已有的知识进行，比如法条回答</p><p></p><p>用自己的数据微调后使用RAG（知识增强）进行检索召回，或者不经过训练直接进行检索召回</p><p></p><p>Case5：希望训练自己领域的问答机器人，希望机器人的回答满足一定条件或范式</p><p></p><p>微调+对齐训练</p><p></p><h3>1.3 模型推理的一般过程</h3><p></p><p></p><p>现在有一个句子，如何将它输入模型得到另一个句子呢？</p><p></p><p>我们可以这样做：</p><p></p><p>先像查字典一样，将句子变为字典中的索引。假如字典有30000个字，那么“我爱张学”可能变为[12,16,23,36]</p><p></p><p>像[12,16,23,36]这样的标量形式索引并不能直接使用，因为其维度太低，可以将它们映射为更高维度的向量，比如每个标量映射为5120长度的向量，这样这四个字就变为：</p><p></p><p><code lang="null">[12,16,23,36]

-&gt;

[[0.1, 0.14, ... 0.22], [0.2, 0.3, ... 0.7], [...], [...]]

------5120个小数-------

</code></p><p></p><p>我们就得到了4x5120尺寸的矩阵（这四个字的矩阵表达）。</p><p></p><p></p><blockquote>深度学习的基本思想就是把一个文字转换为多个小数构成的向量</blockquote><p></p><p></p><p>把这个矩阵在模型内部经过一系列复杂的计算后，最后会得到一个向量，这个向量的小数个数和字典的字数相同。</p><p></p><p><code lang="null">[1.5, 0.4, 0.1, ...]

-------30000个------

</code></p><p></p><p>下面我们把这些小数按照大小转为比例，使这些比例的和是1，通常我们把这个过程叫做概率化。把值（概率）最大的索引找到，比如使51，那么我们再把51通过查字典的方式找到实际的文字：</p><p></p><p><code lang="null">我爱张学-&gt;友(51)

</code></p><p></p><p>下面，我们把“我爱张学友”重新输入模型，让模型计算下一个文字的概率，这种方式叫做自回归。即用生成的文字递归地计算下一个文字。推理的结束标志是结束字符，也就是eos_token，遇到这个token表示生成结束了。</p><p></p><p>训练就是在给定下N个文字的情况下，让模型输出这些文字的概率最大的过程，eos_token在训练时也会放到句子末尾，让模型适应这个token。</p><p></p><h2>2. PyTorch框架</h2><p></p><p></p><p>用于进行向量相乘、求导等操作的框架被称为深度学习框架。高维度的向量被称为张量（Tensor），后面我们也会用Tensor代指高维度向量或矩阵。</p><p></p><p>深度学习框架有许多，比如PyTorch、TensorFlow、Jax、PaddlePaddle、MindSpore等，目前LLM时代研究者使用最多的框架是PyTorch。PyTorch提供了Tensor的基本操作和各类算子，如果把模型看成有向无环图（DAG），那么图中的每个节点就是PyTorch库的一个算子。</p><p></p><p>参考链接：<a href="https://blog.csdn.net/sinat_39620217/article/details/131675175">超全安装教程</a>"</p><p></p><p>conda配置好后，新建一个虚拟环境（一个独立的python包环境，所做的操作不会污染其它虚拟环境）:</p><p></p><p><code lang="null">#配置一个python3.9的虚拟环境

conda create -n py39 python==3.9

#激活这个环境

conda activate py39

</code></p><p></p><p>之后：</p><p></p><p><code lang="null">#假设已经安装了python，没有安装python

pip install torch

</code></p><p></p><p>打开python命令行：</p><p></p><p><code lang="null">python

</code></p><p></p><p><code lang="null">import torch

#两个tensor，可以累计梯度信息

a = torch.tensor([1.], requires_grad=True)

b = torch.tensor([2.], requires_grad=True)

c = a * b

#计算梯度

c.backward()

print(a.grad, b.grad)

#tensor([2.]) tensor([1.])

</code></p><p></p><p>可以看到，a的梯度是2.0，b的梯度是1.0，这是因为c对a的偏导数是b，对b的偏导数是a的缘故。backward方法非常重要，模型参数更新依赖的就是backward计算出来的梯度值。</p><p></p><p>torch.nn.Module基类：所有的模型结构都是该类的子类。一个完整的torch模型分为两部分，一部分是代码，用来描述模型结构：</p><p></p><p><code lang="null">import torch

from torch.nn import Linear
class SubModule(torch.nn.Module):

    def __init__(self):

        super().__init__()

        #有时候会传入一个config，下面的Linear就变成：

        #self.a = Linear(config.hidden_size, config.hidden_size)

        self.a = Linear(4, 4)
class Module(torch.nn.Module):

    def __init__(self):

        super().__init__()

        self.sub =SubModule()
module = Module()
state_dict = module.state_dict() # 实际上是一个key value对
#OrderedDict([('sub.a.weight', tensor([[-0.4148, -0.2303, -0.3650, -0.4019],

#        [-0.2495,  0.1113,  0.3846,  0.3645],

#        [ 0.0395, -0.0490, -0.1738,  0.0820],

#        [ 0.4187,  0.4697, -0.4100, -0.4685]])), ('sub.a.bias', tensor([ 0.4756, -0.4298, -0.4380,  0.3344]))])
#如果我想把SubModule替换为别的结构能不能做呢？

setattr(module, 'sub', Linear(4, 4))

#这样模型的结构就被动态的改变了

#这个就是轻量调优生效的基本原理：新增或改变原有的模型结构，具体可以查看选型或训练章节

</code></p><p></p><p>state_dict存下来就是pytorch_model.bin，也就是存在于modelhub中的文件</p><p></p><p>config.json：用于描述模型结构的信息，如上面的Linear的尺寸(4, 4)</p><p></p><p>tokenizer.json: tokenizer的参数信息</p><p></p><p>vocab.txt: nlp模型和多模态模型特有，描述词表（字典）信息。tokenizer会将原始句子按照词表的字元进行拆分，映射为tokens</p><p></p><p>设备</p><p></p><p>在使用模型和PyTorch时，设备（device）错误是经常出现的错误之一。</p><p></p><p><code lang="null">RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0!

</code></p><p></p><p>tensor和tensor的操作（比如相乘、相加等）只能在两个tensor在同一个设备上才能进行。要不然tensor都被存放在同一个显卡上，要不然都放在cpu上。一般最常见的错误就是模型的输入tensor还在cpu上，而模型本身已经被放在了显卡上。PyTorch驱动N系列显卡进行tensor操作的计算框架是cuda，因此可以非常方便地把模型和tensor放在显卡上：</p><p></p><p><code lang="null">from modelscope import AutoModelForCausalLM

import torch

model = AutoModelForCausalLM.from_pretrained("qwen/Qwen-1_8B-Chat", trust_remote_code=True)

model.to(0)

# model.to('cuda:0') 同样也可以

a = torch.tensor([1.])

a = a.to(0)

#注意！model.to操作不需要承接返回值，这是因为torch.nn.Module(模型基类)的这个操作是in-place(替换)的

#而tensor的操作不是in-place的，需要承接返回值

</code></p><p></p><h3>2.1 PyTorch基本训练代码范例</h3><p></p><p></p><p><code lang="null">import os

import random
import numpy as np

import torch

from torch.optim import AdamW

from torch.optim.lr_scheduler import StepLR

from torch.utils.data import Dataset, DataLoader

from torch.utils.data.dataloader import default_collate

from torch.nn import CrossEntropyLoss
seed = 42

#随机种子，影响训练的随机数逻辑，如果随机种子确定，每次训练的结果是一样的

torch.manual_seed(seed)

np.random.seed(seed)

random.seed(seed)
#确定化cuda、cublas、cudnn的底层随机逻辑

#否则CUDA会提前优化一些算子，产生不确定性

#这些处理在训练时也可以不使用

os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":16:8"

torch.use_deterministic_algorithms(True)

#Enable CUDNN deterministic mode

torch.backends.cudnn.deterministic = True

torch.backends.cudnn.benchmark = False
#torch模型都继承于torch.nn.Module

class MyModule(torch.nn.Module):
    def __init__(self, n_classes=2):

        #优先调用基类构造

        super().__init__()

        #单个神经元，一个linear加上一个relu激活

        self.linear = torch.nn.Linear(16, n_classes)

        self.relu = torch.nn.ReLU()
    def forward(self, tensor, label):

        #前向过程

        output  = {'logits': self.relu(self.linear(tensor))}

        if label is not None:

            # 交叉熵loss

            loss_fct = CrossEntropyLoss()

            output['loss'] = loss_fct(output['logits'], label)

        return output
#构造一个数据集

class MyDataset(Dataset):
    #长度是5

    def __len__(self):

        return 5
    #如何根据index取得数据集的数据

    def __getitem__(self, index):

        return {'tensor': torch.rand(16), 'label': torch.tensor(1)}
#构造模型

model = MyModule()

#构造数据集

dataset = MyDataset()

#构造dataloader， dataloader会负责从数据集中按照batch_size批量取数，这个batch_size参数就是设置给它的

#collate_fn会负责将batch中单行的数据进行padding

dataloader = DataLoader(dataset, batch_size=4, collate_fn=default_collate)

#optimizer，负责将梯度累加回原来的parameters

#lr就是设置到这里的

optimizer = AdamW(model.parameters(), lr=5e-4)

#lr_scheduler， 负责对learning_rate进行调整

lr_scheduler = StepLR(optimizer, 2)
#3个epoch，表示对数据集训练三次

for i in range(3):

    # 从dataloader取数

    for batch in dataloader:

        # 进行模型forward和loss计算

        output = model(**batch)

        # backward过程会对每个可训练的parameters产生梯度

        output['loss'].backward()

        # 建议此时看下model中linear的grad值

        # 也就是model.linear.weight.grad
        # 将梯度累加回parameters

        optimizer.step()

        # 清理使用完的grad

        optimizer.zero_grad()

        # 调整lr

        lr_scheduler.step()

</code></p><p></p><h2>3.Transformer结构模型</h2><p></p><p></p><p>在2017年之后，Transformer结构模型几乎横扫一切统治了NLP领域，后面的CV领域和Audio领域也大放异彩。相比LSTM和CNN结构，Transformer结构好在哪里呢？</p><p></p><p><img src="https://static001.geekbang.org/infoq/a5/a59c1b9f7177776325d8259f78b48ff5.png" /></p><p></p><p>这是LLaMA2的模型结构。</p><p></p><p>介绍下基本结构和流程：</p><p></p><p>Input是原始句子，经过Tokenizer转变为tokenstokens输入模型，第一个算子是Embedder，tokens转换为float tensor之后进入layers，每个layers会包含一个attention结构，计算Q和K的tensor的内积，并将内积概率化，乘以对应的V获得新的tensor。tensor加上输入的x后（防止层数太深梯度消失）进入Normalization，对tensor分布进行标准化进入FeedForward（MLP），重新进入下一layer所有的layers计算过后，经过一个linear求出对vocab每个位置的概率</p><p></p><p>可以看出，Transformer模型的基本原理是让每个文字的Tensor和其他文字的Tensor做内积（也就是cosine投影值，可以理解为文字的相关程度）。之后把这些相关程度放在一起计算各自占比，再用占比比例分别乘以对应文字的Tensor并相加起来，得到了一个新的Tensor（这个Tensor是之前所有Tensor的概率混合，可以理解为对句子所有文字的抽象）。每个文字都进行如上动作，因此生成的新的Tensor和之前输入的Tensor长度相同（比如输入十个字，计算得到的Tensor还是十个），在层数不断堆叠的情况下，最后的Tensor会越来越抽象出文字的深层次意义，用最后输出的Tensor去计算输出一个新的文字或分类。</p><p></p><h3>3.1 Transformer对比CNN和LSTM</h3><p></p><p></p><p>CNN有局部性和平移不变性，促使模型关注局部信息。CNN预设了归纳偏差，这使得小样本训练可以取得较好效果，但在充分数据训练下这一效果也被transformer所掩盖。并且局部性会忽略全局关系，导致某些条件下效果不佳LSTM的长距离记忆会导致最早的token被加速遗忘，并且其只能注意单侧信息导致了对句子的理解存在偏差。后来虽然引入了双向LSTM，但其大规模分布式训练仍然存在技术问题Transformer结构并不预设归纳偏差，因此需要大数据量训练才有较好效果。但其对于token的并行计算大大加速了推理速度，并且对分布式训练支持较好，因此在目前数据量充足的情况下反而异军突起。由于内置了positional-embedding，因此较好地解决了attention结构中的位置不敏感性</p><p></p><h3>3.2 Encoder和Decoder</h3><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/f7/f78290e03498a425c4839d377d2c3d83.png" /></p><p></p><p>如上图所示，左边是encoder，右边是decoder。我们可以看到目前的LLM模型几乎都是decoder结构，为什么encoder-decoder结构模型消失了呢？有以下几个原因：</p><p></p><p>encoder-decoder模型分布式训练困难 decoder模型结构简单，其分布式训练相对容易，而encoder-decoder结构的模型由于结构复杂的多导致了训练时工程结构复杂，成本大大增加有论文证明，encoder-decoder模型在参数量不断增加时不具有显著优势。在模型较小时，由于中间隐变量的存在，decoder部分进行交叉注意力会获得更好的效果，但随着模型增大，这些提升变得不再明显。甚至有论文猜测，encoder-decoder结构的收益仅仅是因为参数量翻倍</p><p></p><p>因此，目前的模型都是decoder模型，encoder-decoder模型几乎销声匿迹。</p><p></p><p>我们可以看到，LLaMA2的模型特点是：</p><p></p><p>没有使用LayerNorm，而是使用了RMSNorm进行预归一化使用了RoPE（Rotary Positional Embedding）MLP使用了SwiGLU作为激活函数LLaMA2的大模型版本使用了Group Query Attention（GQA）</p><p></p><p>3.2.1 RMSNorm</p><p></p><p>LayerNorm的公式是：</p><p></p><p><img src="https://static001.geekbang.org/infoq/08/0892cc9fb74b138624329008e71cdfc3.png" /></p><p></p><p>RMSNorm的开发者发现，减去均值做中心偏移意义不大，因此简化了归一化公式，最终变为：</p><p></p><p>\begin{align} \begin{split} &amp; \bar{a}_i = \frac{a_i}{\text{RMS}(\mathbf{a})} g_i, \quad \text{where}~~ \text{RMS}(\mathbf{a}) = \sqrt{\frac{1}{n} \sum_{i=1}^{n} a_i^2} \end{split}\nonumber \end{align}</p><p></p><p>最终在保持效果不变的情况下，计算时间提升了40%左右。</p><p></p><p>3.2.2 RoPE</p><p></p><p>BERT模型使用的原始位置编码是Sinusoidal Position Encoding。该位置编码的原理非常简单：</p><p></p><p><img src="https://static001.geekbang.org/infoq/1f/1fa49af3178769f955ddd4a50ae86cf5.png" /></p><p></p><p>该设计的主要好处在于：</p><p></p><p>在位置编码累加到embedding编码的条件下，基本满足不同位置编码的内积可以模拟相对位置的数值随着相对位置增大，其位置编码的内积趋近于0具备一定的外推特性</p><p></p><p>LLM常用的位置编码还有AliBi（注意力线性偏置）。该方法不在embedding上直接累加位置编码，而选择在Q*K的结果上累加一个位置矩阵：</p><p></p><p><img src="https://static001.geekbang.org/infoq/5c/5cb3429fc759060430fd893000a7a59b.jpeg" /></p><p></p><p>ALiBi的好处在于：</p><p></p><p>具备良好的外推特性相对位置数值很稳定</p><p></p><p>RoPE的全称是旋转位置编码(Rotary Positional Embedding)，该编码的推导过程和Sinusoidal Position Encoding的推导过程比较类似，不同之处在于后者是加性的，而前者是乘性的，因此得到的位置编码类似于：</p><p><img src="https://static001.geekbang.org/infoq/6e/6ed9ece05d0e3b069f1265498cb80527.png" /></p><p></p><p>或者也可以简化为：</p><p></p><p><img src="https://static001.geekbang.org/infoq/70/70404311b2ed6d83b6548ac1a2ef164d.png" /></p><p></p><p>该位置编码表示相对位置的几何意义比较明显，也就是两个向量的角度差。</p><p></p><p>该位置编码的优势在于：</p><p></p><p>位置编码矩阵是单位正交阵，因此乘上位置编码后不会改变原向量模长相较于Sinusoidal Position Encoding具备了更好的外推特性</p><p></p><p>3.2.3 SwiGLU</p><p></p><p>SwiGLU是GLU结构的变种。GLU是和LSTM原理类似，但不能接受时序数据，只能处理定长数据。而且省略了遗忘门与记忆门，只保留了输入门，SwiGLU是将其中的激活函数替换为了SiLU：</p><p></p><p><img src="https://static001.geekbang.org/infoq/25/258b5e681d630eb61ff5df62ad969816.png" /></p><p></p><p>其中</p><p></p><p><img src="https://static001.geekbang.org/infoq/56/56df5ccb857f8299d7324c6af580b011.png" /></p><p></p><p>的表达式为：</p><p></p><p><img src="https://static001.geekbang.org/infoq/48/480d069c160a7867ed0ba407797429e0.png" /></p><p></p><p>在SwiGLU的论文中，作者论证了SwiGLU在LOSS收益上显著强于ReLU、GeLU、LeakyGeLU等其他激活方法。</p><p></p><p>3.2.4 GQA</p><p></p><p>MHA（Multi-head Attention）是标准的多头注意力机制，具有H个Query、Key 和 Value 矩阵</p><p>MQA（Multi-Query Attention，来自于论文：Fast Transformer Decoding: One Write-Head is All You Need）共享了注意力头之间的KV，只为每个头保留单独的Q参数，减少了显存占用。</p><p>GQA（Grouped-Query Attention，来自于论文：GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints）在MQA的基础上分成了G个组，组内共享KV。</p><p>在Llama2模型中，70B参数为了提升推理性能使用了GQA，其他版本没有使用这项技术。</p><p></p><h2>3.3 ChatGLM2的模型结构</h2><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/eb/eb90fbe5da442d3827b33a69feb55f49.png" /></p><p></p><p>ChatGLM2模型结构和Llama2的结构有一定相似之处，主要不同之处在于：</p><p></p><p>在开源的ChatGLM2代码中没有使用GQA，而是使用了MQAQKV为单一矩阵，在对hidden_state进行整体仿射后拆分为Query、Key、ValueMLP结构中没有使用Up、Gate、Down三个Linear加上SwiGLU，而是使用了hidden_size -&gt; 2 * ffn_hidden_size的Up Linear进行上采样，对tensor进行拆分为两个宽度为ffn_hidden_size的tensor后直接输入SiLU，然后经过ffn_hidden_size -&gt; hidden_size的Down Linear进行下采样</p><p></p><p><a href="https://bbs.huaweicloud.com/blogs?utm_source=infoq&amp;utm_medium=bbs-ex&amp;utm_campaign=other&amp;utm_content=content">点击关注，第一时间了解华为云新鲜技术~</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/gGyXtYgPF4u1xLQVlOC1</id>
            <title>走近李生教授：培养出周明、王海峰等数位AI科学家，NLP国际最高奖项得主的科研之路</title>
            <link>https://www.infoq.cn/article/gGyXtYgPF4u1xLQVlOC1</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/gGyXtYgPF4u1xLQVlOC1</guid>
            <pubDate></pubDate>
            <updated>Tue, 04 Jun 2024 01:08:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 机器翻译, 自然语言处理, 哈尔滨工业大学, 陈光熙教授
<br>
<br>
总结: 本文介绍了中国自然语言处理科学家李生教授在机器翻译领域的贡献，以及他在哈尔滨工业大学的教育和科研经历，同时提及了他的师从陈光熙教授的经历。文章还涵盖了哈工大在自然语言处理和人工智能领域的研究历程，以及李生教授在早期科研和文献标引方面的工作。 </div>
                        <hr>
                    
                    <p></p><h2>写在最前</h2><p></p><p></p><p>李生，中国最早从事机器翻译研究的自然语言处理科学家之一。哈尔滨工业大学教授，博士生导师，中文信息学会会士，中国中文信息学会名誉理事长。曾任哈尔滨工业大学党委书记、中国中文信息学会第七届理事会理事长。多年来在自然语言处理领域培养了四十余名博士研究生，百余名硕士研究生。</p><p></p><p>李生教授是国际计算语言学会终身成就奖五十余年来的首位华人获得者，中国计算机学会自然语言处理专业委员会“杰出成就学者”奖获得者，中国中文信息学会终身成就奖获得者。他还是党的十六大代表、全国五一劳动奖章获得者。</p><p></p><p>上世纪六十年代，李生教授毕业于哈尔滨工业大学，师从中国计算机科学与工程奠基人之一陈光熙教授，他是中国人工智能领域的第一位博士候选人。</p><p></p><p>我母校哈尔滨工业大学的自然语言处理研究团队曾被誉为“世界上规模最大、人数最多的自然语言处理实验室”。同时哈工大也是中国最早从事人工智能、自然语言处理方向研究的高校之一。从上世纪五十年代最开始涉足机器翻译研究开始，学校一直在从事自然语言处理和语音处理中各个方向的研究。为中国人工智能中的自然语言处理领域培养了大量科学家和工程师。</p><p></p><p>李生教授是哈工大近 70 年来人工智能和自然语言处理发展的见证者。倍感荣幸的是，李生教授也是当年我博士论文答辩委员会的主席。虽然当初在学校的很多会议和活动上都接受过李老师的言传身教，但是这么多年都没有跟他单独交流的机会。</p><p></p><p>2024 年 4 月，我在北京拜访了李生教授，他向我讲述了学校以及他自己在人工智能研究领域的很多经历。他的讲述仿佛把我身临其境地带入了母校老一辈计算机科学家早年的科研场景。</p><p></p><p></p><h2>就读大学</h2><p></p><p></p><h3>心怀报国志向的数学学霸</h3><p></p><p>1943 年，李生出生在黑龙江省兰西县一个农村家庭。1949 年新中国成立，也是这一年，李生就读初小，开始了他的学生生涯。1954 年，李生考入兰西县初级中学就读初中。三年后，他考入肇东市第一中学开始就读高中。</p><p></p><p>上世纪五十年代，国家大力推动“两弹一星”的研发。1960 年，李生参加高考，怀着报效国家的梦想，李生报考了哈尔滨工业大学核物理专业，并以优异的成绩顺利考取。其实他当时他也不太明白核物理究竟要学些什么，只是知道这是当时国家建设最需要的专业。</p><p></p><p>由于李生高考时数学成绩特别好，入学报到的时候，学校决定根据实际需要把他调剂到计算机专业。</p><p></p><p>李生老师回忆说，当时他根本不了解什么是计算机，作为农村出来的孩子，他用过算盘，见过计算器，可是计算机真的连见都没有见过。不过对于那个年代的年轻人，学习工作的志向就是到国家最需要的地方去。所以他接受了调剂，成为了计算机专业的本科生。</p><p></p><p>当时的哈工大跟清华大学、北京大学一样，本科都是五年学制。经过五年的学习，1965 年李生本科毕业，并留校任教。</p><p></p><p></p><h2>早期科研</h2><p></p><p></p><h3>参与大型电子计算机的研制</h3><p></p><p>刚留校的时候，李生承担了系里的大量教学任务，其中包括讲课、批改作业、给学生答疑等。他讲授过包括计算机原理、Basic 语言、Fortran 语言在内的很多专业基础课和专业课。</p><p></p><p>从上世纪五十年代开始，中国的很多科研机构和工厂都在大批量研发和生产大型数字电子计算机。</p><p></p><p>1968 年，李生带领计算机专业 64 级学生参与 441C 计算机的调试工作，这是当时哈尔滨军事工程学院研制的晶体管计算机。</p><p></p><p>1970 年 -1974 年国家三线建设期间，李生随学校南迁到重庆市。在重庆，他曾在学校为部队办的计算机学习班授课，也曾与重庆钟表厂合作研制线切割机床（一种电加工机床，主要用于通过电火花放电来切割金属）。</p><p></p><p>1974 年，李生随学校重返哈尔滨，在学校搬迁办公室参与哈尔滨工业大学的重建。</p><p></p><p>1976 年，李生参与 DJS—11 型计算机的研制工作，由北京大学提供计算机研发的图纸和技术资料，哈尔滨电子仪器厂为主机总装厂，产品的交付标准参照北京大学电子仪器厂生产的 150 机。</p><p></p><p>1978 年，李生开始进行“区域性西文期刊机读联合目录”项目的研制。机读目录是一种可以利用计算机读取和处理的书目信息，这些信息可以被计算机自动识别并编辑，机读目录很大程度上方便了书目信息的查询和共享。</p><p></p><p>这个项目于 1986 年通过黑龙江省科技成果鉴定，这是李生第一个通过成果鉴定的科研项目，项目也获得了航天部科技进步三等奖。</p><p></p><p>1979 年，李生被评为黑龙江省优秀教师。</p><p></p><p>1983 年，李生开始跟随陈光熙教授攻读博士学位，他也成了中国和哈尔滨工业大学人工智能领域的第一个博士候选人。之后因为一些原因终止了博士学位的攻读。</p><p></p><p><img src="https://static001.geekbang.org/infoq/41/412d31b3c5d5472e3a21de689d028d64.webp" /></p><p></p><p>陈光熙教授是中国计算机工程学科奠基人，也是哈工大计算机学科的创始人。当时他们的研究方向是信息检索，那时候的信息检索也称为情报检索，就是在国外早期的数据存储磁带中查询数据，获取国外最新的科研工作进展，以确定专业接下来的研究方向。</p><p></p><p>其间，李生参与了陈光熙教授主持的“TDM 数据库机”的项目研制，数据库计算机是一种实现数据库存储、管理和控制的专用计算机。这个项目获得了航天部科技进步二等奖。</p><p></p><p></p><h2>文献标引</h2><p></p><p></p><h3>机器翻译的研究契机</h3><p></p><p>哈尔滨工业大学和中国人民大学是苏联援建中国的两所高校。上世纪五十年代，有一批苏联专家到哈工大从事教学工作，带来了很多俄文资料。把俄文资料翻译成中文成了急需完成的工作。当时俄语系的王畛老师和计算机系的王开铸老师就开始了俄汉机器翻译的研究，这是哈工大最早期的机器翻译工作。</p><p></p><p>改革开放初期，中国的科学研究全面复苏，由于之前中国的大多技术和经验都是向苏联学习。这时全国很多高校都派出老师去欧美高校访问学习，哈工大计算机专业当时也派出了老师到美国学习，以便开展新的科研方向。</p><p></p><p>比如，王开铸老师去美国访问回来后开始从事俄汉题录翻译、自动文摘等方向的研究。题录是一种描述文献外部特征的条目，主要包括文献的题名、著者、出处等信息。俄汉题录翻译就是通过计算机自动把这些条目从俄文自动翻译成中文。自动文摘是通过计算机在一篇文章中自动提取出文章的摘要。这两个方向都是自然语言处理中的重要研究方向。</p><p></p><p>再比如，舒文豪老师在美国访问时师从国际模式识别创始人傅京孙教授，回国后开始了手写汉字识别的研究。</p><p></p><p>在同一时期，李生教授开始从事汉英机器翻译研究，正式开启了自然语言处理的学术生涯。</p><p></p><p>1984 年，李生晋升为副教授。</p><p></p><p>1985 年，李生教授的硕士研究生周明入学。李生教授为周明最初确定的研究方向为中文文献的主题词自动标引。自动标引，就是利用计算机从情报和文献中自动抽取检索标志。主题词自动标引，就是自动抽取可以代表情报和文献内容的关键词。这一年，李生 42 岁。</p><p></p><p>在北京，周明认识了中国科学院科学考察委员会的吴蔚天研究员（后来吴老师加入了中国软件技术公司语言工程部）。吴蔚天老师是理工科背景出身，特别喜欢自然语言处理。那时候吴老师也在做中文文献自动标引的工作。</p><p></p><p>吴蔚天老师提议周明考虑汉英机器翻译方向，他觉得英文的相关数据和工具都比中文多，要是有办法把中文翻译成英文，则英文中的自动标引、自动文摘等技术都可以直接应用，于是他提出了跟李生教授团队一起合作，通过汉英机器翻译进行自动标引的研究。</p><p></p><p>结果初步协商后，李生教授和周明再次来到北京，与吴蔚天老师签订了科研合作协议。双方也讨论了自动标引研究的初步研究思路：先将中文语句分词，然后依次判断每个词是否跟文献的主题有关联，有关联就保留下来，没有关联就弃用。这种思路被称为“有联系则取，无联系则断。”</p><p></p><p>相比中文，除了数据和工具比较多，英文自动标引工作还有其他优势。因为中文语句的词语之间没有明确的间隔符，而英文语句的词语之间是通过空格分隔的，分词容易很多。</p><p></p><p>李生、吴蔚天和周明进一步讨论决定，先把中文句子翻译成英文，然后选取英文句子中的主题词，再把主题词翻译回英文。</p><p></p><p>这样项目就确定了汉英翻译，接着确定主题词，最后再做英汉翻译的流程。</p><p></p><p>哈工大人的科研精神态度是务实和谨慎。李生教授意识到这个项目的工作量非常大，别说一个硕士生，就是一个博士生也很难完成，想要完成这个工作，需要一个科研团队。</p><p></p><p>为了逐步完成这个项目，周明的课题确定为汉英翻译，也就是整个中文文献标引的第一个环节。</p><p></p><p>1987 年起，李生任哈工大计算机系系主任。</p><p></p><p>1988 年，CEMT-I 汉英翻译系统开发完成，CEMT-I 系统利用了词法、语法、语义等语言学特征实现汉英翻译需求，实现了 300 多个汉语句子和题录的机器翻译。</p><p></p><p>同年，周明硕士毕业，开始攻读博士学位，由于李生教授当时还不是博士生导师，周明的博士导师为陈光熙教授，李生教授是他的实际指导教师，博士期间，周明继续汉英机器翻译的研究。</p><p></p><p>1989 年，CEMT-I 成为中国第一个通过技术鉴定的汉英机器翻译系统。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1d/1d50c49d7729ae2be92d1ef56066699c.webp" /></p><p></p><p>继 CEMT-I 的成功开发之后，CEMT-II 和 CEMT-III 系统也陆续诞生。CEMT-II 解决了航天部 103 所军贸产品技术说明书的英汉翻译需求。CEMT-III 则实现了科技文章的英汉翻译，这个系统由李生教授团队与中国运载火箭技术研究院计算机应用研究所共同研制。1993 年，这个系统通过了国家鉴定，鉴定委员会主任为时任中国中文信息学会理事长陈力为院士。</p><p></p><p>1990 年，李生被评为计算机专业教授。</p><p></p><p>1991 年，周明博士毕业。在周明的博士论文答辩会上，李生教授邀请了清华大学的张钹教授和黄昌宁教授，以及国防科技大学的胡守仁教授。</p><p></p><p>现在来看，这场博士论文答辩会的专家阵容可谓相当豪华。张钹教授于 1995 年被评为中国科学院院士，黄昌宁教授也是中国最早从事自然语言处理的科学家之一，胡守仁教授曾经领导了中国第一台军用专用电子管计算机 901(331) 机的研制。</p><p></p><p>周明博士是李生教授指导的第一位博士，也是李生教授培养的第一位中国自然语言处理科学家。周明博士毕业后到清华大学从事博士后研究工作，后任清华大学副研究员。1999 年，他加入微软亚洲（中国）研究院，负责自然语言处理研究组，他在微软工作二十多年，直至微软亚洲研究院副院长。大模型时代，周明博士离开微软，创立了知名人工智能公司澜舟科技。</p><p></p><p>1992 年，李生教授被评为计算机专业博士研究生导师。</p><p></p><p>1993 年，李生教授获“国务院政府特殊津贴”。</p><p></p><p>同年，李生教授团队与清华大学、原航天工业总公司共同开发达雅翻译工作站。当时周明博士已经加入清华大学，并代表清华大学一方负责这个项目的工作。</p><p></p><p>达雅翻译工作站可以用于汉英和英汉的辅助机器翻译和辅助写作。这个项目成功的实现了翻译软件的商品化。项目连续三年在北京计算机产品交易会上展出。1997 年，该项目获得部级二等奖。</p><p></p><p>1994 年起，在国家 863 高技术研究发展计划的支持下，李生教授的团队开展了汉英 - 英汉双向机器翻译研究。先后开发了 BT863-I 和 BT863-II 双向机器翻译系统，实现了基于规则和基于统计的翻译方法混合的机器翻译策略。</p><p></p><p>基于规则的翻译方法，核心思想是根据语言的句法、语义等特征，写出翻译过程中的相关规则。基于统计的翻译方法，核心思想是利用统计和数学方法，计算出翻译过程中的相关策略和选择。</p><p></p><p></p><h2>与微软合作</h2><p></p><p></p><h3>中国互联网”黄埔军校“的缘起</h3><p></p><p></p><p>1996 年，李生教授省获“黑龙江优秀中青年专家”称号。</p><p></p><p>1998 年，李生任哈尔滨工业大学党委书记。</p><p></p><p>这一年年末，微软在北京成立微软中国研究院。这是微软公司在美国境外开设的第二家基础科研机构，也是微软在美国境外规模最大的研究机构。</p><p></p><p>研究院刚成立的时候，在国内还没有那么高的认可度。虽然北京的高校很多，但是研究院并没有开展很多校企合作。</p><p></p><p>这个时候，周明博士已经在微软中国研究院工作。研究院希望在哈工大成立学生俱乐部，负责高校合作的马歆找到周明，希望他跟李生教授沟通，讨论研究院跟哈工大的合作事宜。</p><p></p><p>周明随即联系了导师李生教授，时任学校党委书记的李生同意与微软中国研究院开展合作。李生教授协调了学校的团委、学生会等相关部门进行对接。1999 年，微软中国研究院到哈工大举办宣讲活动，微软 - 哈工大学生俱乐部成立。与此同时，微软中国研究院与中国高校的合作也拉开帷幕。</p><p></p><p>2000 年，微软 - 哈工大机器翻译联合实验室成立，李生教授任实验室主任。研究院也先后委派黄昌宁教授、周明博士和马维英博士担任实验室微软方面的联合主任。</p><p></p><p>2001 年，微软中国研究院更名为微软亚洲研究院。</p><p></p><p>之后，微软亚洲研究院与中国很多其他顶尖高校也展开了学生培养和科学研究方面的合作，为中国互联网和人工智能行业培养了大量的优秀人才，被誉为中国互联网的“黄埔军校”。这些都开始于李生教授当初对待校企合作开放、包容的态度。</p><p></p><p></p><h2>回到一线科研岗位</h2><p></p><p></p><h3>获得计算语言学领域国际最高荣誉</h3><p></p><p>2002 年，李生教授荣获“全国五一劳动奖章”。</p><p></p><p>2004 年，李生教授卸任学校党委书记，回到计算机学院继续从事教学科研工作。</p><p></p><p>同年开始，连续两届担任国家自然科学基金信息科学部专家评审组成员。</p><p></p><p>也是在这一年，微软 - 哈工大机器翻译联合实验室升级为“哈工大语言语音教育部 - 微软重点实验室”，李生教授继续担任重点实验室主任。</p><p></p><p>2008 年，李生主持国家自然科学基金重点项目“下一代信息检索”。</p><p></p><p>2011 年，李生担任中国文信息学会理事长。</p><p></p><p>2012 年，李生教授参加了由百度公司牵头的国家 863 重大项目“互联网语言翻译系统研制”。这个项目在 2015 年获得国家科技进步二等奖。</p><p></p><p>2015 年，李生教授获国际计算语言学学会（Association for Computational Linguistics,ACL）终身成就奖。这个奖项是国际计算语言学领域的最高荣誉。他也是这个奖项开设以来的首位华人获得者。曾经的获奖者包括 IDF(逆文档频率) 的提出者 Karen Spärck Jones 等学界泰斗。</p><p></p><p><img src="https://static001.geekbang.org/infoq/27/27e5add1111e83ff4e776e15f3f9d3e6.webp" /></p><p></p><p>同一年，李生教授获中国中文信息学会终身成就奖。</p><p></p><p>2016 年起，李生教授担任中国中文信息学会名誉理事长。</p><p></p><p>2023 年，李生教授获中国计算机学会自然语言处理专业委员会"杰出成就学者"奖。</p><p></p><p></p><h2>桃李天下</h2><p></p><p></p><h3>为业界培养多位科学家的科学家</h3><p></p><p>2015 年年末，哈尔滨工业大学机器智能与翻译研究室和社会计算与信息检索研究中心共同为李生教授举办执教 50 周年纪念活动。活动发布了一本纪念册，名为《春华秋实，桃李天下》。</p><p></p><p>五十多年来，李生教授培养了近 50 名博士生，百余名硕士生。</p><p></p><p>光是早年开发 CEMT 三个机器翻译系统的过程中就培养了中国自然语言处理领域的数位科学家。</p><p></p><p>其中包括澜舟科技的周明博士、哈尔滨工业大学的赵铁军教授和张民教授、百度 CTO 王海峰博士等。哈尔滨工业大学的刘挺教授也自从博士毕业后就开始加入李生教授团队从事研究工作。</p><p></p><p>其中周明和王海峰曾担任国际计算语言学会主席，他们二位也都曾当选国际计算语言学会会士（ACL Fellow）。</p><p></p><p>在学术界，李生教授还培养了北京语言大学的荀恩东教授、哈尔滨工业大学的杨沐昀教授、秦兵教授和车万翔教授等多位知名学者。</p><p></p><p>在工业界，他的弟子包括联想控股副总裁于浩博士、百度公司副总裁赵世奇博士等。</p><p></p><p></p><h2>后记</h2><p></p><p>在北京见到李生教授时，他对我说：“中国的人工智能是从机器翻译开始的，虽然早期的时候机器翻译还没有提升到人工智能的高度，不过这也是自然语言处理学者的一个骄傲。”</p><p></p><p>从学生的培养，到微软亚洲研究院在国内的首次合作，听李老师讲述几十年来的往事。其中的很多细节和李老师务实的精神让我特别感动。</p><p></p><p>一个多小时的交谈过程中，我能感受到李生老师早年科研过程中条件的艰苦，以及面对方向选择时的谨慎。五十多年来，从零开始到若干机器翻译系统的研制成功，李老师与他的学生们为中国的机器翻译研究做出了开拓性的贡献。</p><p></p><p>ACL 终身成就奖的获得，不仅是国际学术界对李生教授长期贡献的认可，也是对中国自然语言处理学术研究的认可。</p><p></p><p>个人电脑时代、互联网时代、大模型时代……时间滚滚向前，新的时代接踵而至，人工智能在人们生活中的比重越来越大，自然语言处理方向的研究价值也越来越高。</p><p></p><p>中国的自然语言处理研究正是源于几十年前李生教授这一批脚踏实地的学者。他们的工作为中国的自然语言处理事业打下坚实的基础。他们培养的学生是中国这一领域雄厚的人才资本。这些夯实的积累和持续传承的精神足以让我们对未来中国相关领域的发展充满信心。</p><p></p><p></p><h2>作者简介：</h2><p></p><p></p><p>秦海龙，香港科技大学社会科学部博士后研究员，中国中文信息学会社会媒体处理专业委会委员。主要研究方向为中国人工智能发展史、中国人工智能科学家口述史、计算社会学。博士毕业于哈尔滨工业大学社会计算与信息检索研究中心，前自然语言处理研发工程师，曾就职于小米科技和三角兽科技。&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/NPYL7aG4kYLIWqNc2dhe</id>
            <title>智谱AI获中东财团 4 亿美元投资；老板跑路，900多员工一脸懵：上午改bug、下午解散；谷歌在云部门大规模裁员 | AI周报</title>
            <link>https://www.infoq.cn/article/NPYL7aG4kYLIWqNc2dhe</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NPYL7aG4kYLIWqNc2dhe</guid>
            <pubDate></pubDate>
            <updated>Mon, 03 Jun 2024 08:34:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: TikTok, 谷歌, 马斯克, OpenAI
<br>
<br>
总结: 字节跳动否认TikTok曾提议将控制权交给美国政府，谷歌在云部门裁员，马斯克和杨立昆争论，奥特曼或将OpenAI重组为盈利性公司，苹果Siri升级计划延迟至2025年。 </div>
                        <hr>
                    
                    <p></p><blockquote>字节跳动：外媒报道“TikTok曾提议将控制权交给美国政府”消息不实；谷歌在云部门大规模裁员；马斯克和杨立昆的激情互喷，引来3000万网友围观；奥特曼或将OpenAI重组为盈利性公司；iOS&nbsp;18首个正式版无缘：曝苹果AI&nbsp;Siri跳票至2025年；内卷加剧！拼多多上线自动跟价功能……</blockquote><p></p><p></p><h3>热门资讯</h3><p></p><p></p><h4>字节跳动：外媒报道“TikTok曾提议将控制权交给美国政府”消息不实</h4><p></p><p>&nbsp;</p><p>近日有媒体报道TikTok曾提议将控制权交给美国，字节否认。此外字节AI硬件方向的探索，在内部分为两条产品线“D线”和“O线”，负责人分别为李浩乾和字节曾收购公司Oladance创始人，向字节跳动技术副总裁洪定坤汇报。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ec3ce51a40aef0229447f544dcea9f63.webp" /></p><p></p><p>字节跳动豆包业务相关负责人表示，这些组织及汇报关系的猜测不实。该项目作为字节豆包业务的一部分，目的是探索豆包能力与可穿戴设备相结合，为用户提供更自然和便捷的交互体验。同时，我们也会把豆包能力开放给更多硬件厂商，希望能够为硬件厂商和豆包大模型/豆包AI助手的合作提供设计参考。</p><p>&nbsp;</p><p>另外，还有消息称，TikTok正在为其1.7亿美国用户开发其推荐算法的克隆版本，可能会产生一个独立于其中国母公司字节跳动运行的版本，更容易被想要封禁它的美国议员接受。在报道发表后，TikTok在𝕏上发文称，“路透社今天发表的报道具有误导性，与事实不符”。</p><p></p><h4>谷歌在云部门大规模裁员</h4><p></p><p>&nbsp;</p><p>据外媒报道，一份内部文件显示，谷歌公司在旗下云部门进行大规模裁员，涉及咨询顾问、合伙人工程师、可持续性等团队的员工。有员工估算，谷歌云亚太区的“走向市场”（Go&nbsp;To&nbsp;Market）部门约有100人被裁。</p><p>&nbsp;</p><p>前述文件显示，专注于咨询、合作伙伴工程和可持续性的云团队也受到裁员的影响。一些最近新招的员工也解雇，包括至少一名还未完成入职流程的员工。部分受影响的员工已经得到谷歌内部其他岗位的工作机会。</p><p></p><h4>马斯克和杨立昆的激情互喷，引来3000万网友围观</h4><p></p><p>&nbsp;</p><p>5月27日，马斯克的&nbsp;xAI&nbsp;刚宣布了今年最大的一笔风险投资，以超过&nbsp;240&nbsp;亿美元估值获得&nbsp;60&nbsp;亿美元&nbsp;B&nbsp;轮融资。自此，成功晋级为“硅谷大模型第一梯队玩家”，与&nbsp;OpenAI、谷歌、Anthropic&nbsp;和&nbsp;xAI&nbsp;并列为四巨头。</p><p>而马斯克在社交媒体X上发布招聘信息后，遭到Meta首席人工智能科学家杨立昆嘲讽，大战一触即发，引来3000万网友围观。两人真正的分歧在于：杨立昆认为AI毁灭人类的概率为零，而马斯克宣扬的是AI威胁论。</p><p>杨立昆嘲讽道：“如果你能忍受这样一位老板，那就加入xAI。这位老板声称，你正在做的事情将在明年由AI替代解决（没有压力）；声称你正在做的事情会毁灭人类，必须停止或暂停（耶，放六个月的假！）；声称想要对真理进行最严格的追求，但却在自己的社交平台上散布疯狂的阴谋论。”</p><p>&nbsp;</p><p>另外在近日巴黎举行的初创企业年度技术大会&nbsp;VivaTech&nbsp;上，杨立昆建议希望在&nbsp;AI&nbsp;生态系统中工作的学生不要从事&nbsp;LLM（大型语言模型或称“LLM”）方面的工作。“如果你是对构建下一代&nbsp;AI&nbsp;系统感兴趣的学生，请不要从事&nbsp;LLM&nbsp;方面的工作。这是大公司的事情，你们无法对此有所贡献。”他还说，人们应该开发能够克服大型语言模型局限性的下一代&nbsp;AI&nbsp;系统。</p><p>&nbsp;</p><p></p><h4>奥特曼或将OpenAI重组为盈利性公司</h4><p></p><p>&nbsp;</p><p>据知情人士称，OpenAI&nbsp;CEO山姆·奥特曼（Sam&nbsp;Altman）正考虑将OpenAI重组为一家正规的盈利性公司。据一位与奥特曼交谈过的知情人士称，一种可能性是，奥特曼可能会将OpenAI转变为一家营利性公司，他也讨论过这个想法。而OpenAI的一些投资者表示，他们希望奥特曼获得股权方案，以确保他的利益与公司业务保持一致。</p><p>&nbsp;</p><p>本周，OpenAI&nbsp;的两位前董事会成员Helen&nbsp;Toner&nbsp;和&nbsp;Tasha&nbsp;McCauley&nbsp;公开指责&nbsp;Sam&nbsp;Altman，认为其在&nbsp;OpenAI&nbsp;内部培养了一种虚假文化。</p><p>&nbsp;</p><p>两位前董事指出&nbsp;Altman&nbsp;削弱了董事会对关键决策和安全规定的监督，许多高管告诉董事会，他们非常担心&nbsp;Altman&nbsp;正在创造&nbsp;一种虚假文化，并参与&nbsp;可以被描述为心理虐待的行为。Toner&nbsp;和&nbsp;McCauley&nbsp;认为，OpenAI&nbsp;无法自我监管，这一观点源自非营利性和营利性结构的混合。</p><p>&nbsp;</p><p></p><h4>iOS&nbsp;18首个正式版无缘：曝苹果AI&nbsp;Siri跳票至2025年</h4><p></p><p>&nbsp;</p><p>据报道，苹果正对Siri进行重大升级，使其获得大模型能力，更准确理解用户意图，提供更自然、智能的交互体验。不过，这一高级Siri功能将不会在iOS&nbsp;18首个正式版中出现，预计要等到2025年的iOS&nbsp;18才会集成。</p><p>&nbsp;</p><p>目前，Siri的智能化程度有限，苹果计划借助类似ChatGPT的自然语言模型实现全新智能助理，让用户能与Siri进行多轮对话。苹果工程师团队正在重构Siri的底层，以提升其响应生成能力，深度理解用户需求，并智能整合相关信息，让对话和工作更加自然、高效。</p><p>&nbsp;</p><p>据外媒报道，苹果早在2023年年中就已接触OpenAI公司，而微软对两家公司之间的这笔交易感到担忧。据悉，苹果iOS等系统接入OpenAI的ChatGPT聊天服务之外，还邀请OpenAI团队成员帮助优化Siri，不断提高其“智力”。</p><p>&nbsp;</p><p></p><h4>内卷加剧！拼多多上线自动跟价功能</h4><p></p><p>&nbsp;</p><p>5月29日，拼多多正式上线了自动化价格追踪系统。此举意味着若竞争对手下调商品价格，而用户的商品价格尚显高价，平台将智能调整用户定价，使之与竞争对手保持同步甚至更具竞争优势。</p><p>值得注意的是，抖音电商此前刚刚宣布小规模内测“自动改价”功能，该功能支持商家委托平台根据市场情况，参考平台的建议价，在商家设定的条件范围内实现改价，为消费者提供更有价格竞争力的商品，提升自身经营效率。据抖音电商内部同学透露，今年货架电商的营业额已经很接近拼多多。</p><p>&nbsp;</p><p></p><h4>小红书大举替换中高层高管，后台出现诸多bug</h4><p></p><p>&nbsp;</p><p>据报道，2023年，小红书在一场共创会议中明确了“坐一观三”的发展策略，向3亿DAU正式进发。为了拉平差距，小红书今年一季度在行业大举挖人，有大量中层以上管理者进入小红书，承担起小红书商业化产品、社区运营、电商产品、法务等重要岗位。</p><p>与此同时，多元的组织文化冲击了小红书的原有团队。内部互相推诿的情况变多，“各部门甩锅问题严重”，事实上，不止一位采访对象提及过，小红书的社区部门与商业化部门存在矛盾，而社区一直处于强势地位。新进入者的试错，也让小红书的后台出现诸多bug，引发了商家们的投诉潮。人员的频繁流动，让小红书的功能产品连续性较差，导致用户的体验差。</p><p>&nbsp;</p><p></p><h4>京东健康旗下家医事业部被整体裁撤，刘强东“训话”后京东宣布涨薪</h4><p></p><p>&nbsp;</p><p>5月27日，京东集团宣布，自2024年7月1日起，通过一年半时间，京东采销年度固定薪酬由16薪提升至20薪，业绩激励上不封顶。据了解，这是京东半年内第二次给一线采销人员涨薪。2023年12月底，京东集团宣布，2024年1月1日起京东采销等一线业务人员的年固定薪酬大幅上涨近100%，2024年初京东零售全员将平均加薪不低于20%。</p><p>&nbsp;</p><p>5月28日消息，网传京东健康旗下京东家医事业部被整体裁撤。有京东健康内部人士表示，京东家医事业部投入较大，但一直没有找到盈利模式，也没有其它价值，因此被放弃。对此，京东健康回应称，为整合公司旗下医疗健康服务业务，更高效利用医疗健康服务资源、更大化发挥专业角色的服务价值，京东健康针对家庭医生事业部进行了内部组织架构调整，将原家庭医生事业部的&nbsp;C&nbsp;端业务及职能，合并至互联网医疗事业部；B&nbsp;端业务及职能，合并至企业业务事业部。此次调整除架构分拆外，不涉及业务和人员裁撤。</p><p>&nbsp;</p><p>有媒体报道称，针对近期员工代打卡已形成产业链的问题，京东内部调查，每个月有1.4万人次找人代打卡，更有员工一年里代打卡近百天。代打卡一次收取15元，一个人可替20个人代打卡。</p><p>&nbsp;</p><p>在此背景下，刘强东在日前的高管会上直言：“凡是长期业绩不好，从来不拼搏的人，不是我的兄弟。”与此同时，京东管理层反思了过去数年间公司在管理上出现的失误，承认京东集团体系存在“大企业病”，管理上出现了严重问题，尤其在拼搏上有很大的懈怠。</p><p>&nbsp;</p><p>另外，磨铁集团&nbsp;CEO&nbsp;<a href="https://readhub.cn/entity_topics?type=10&amp;uid=bd975c885b3c1cf8">沈浩波</a>"还在朋友圈发文抵制<a href="https://readhub.cn/entity_topics?type=10&amp;uid=bf4792ab9e1c7531">京东</a>"，称京东不顾磨铁反对，强行让其产品参加低价促销。磨铁将采取法律手段维护权益，无限期停止对京东发货，要求京东下架其产品并退还。呼吁电商合作伙伴不要跟价。京东方面则回应称，其&nbsp;618&nbsp;活动目标是让消费者买到便宜的好书，否认站在出版社和行业对立面。京东对自营图书有合法的自主定价权。目前磨铁的网上店铺仍在正常销售，但部分热门图书显示无货。</p><p>&nbsp;</p><p></p><h4>阿里蔡崇信：我会尽量让&nbsp;CEO&nbsp;们和运营团队拥有更多的自主权和决策权</h4><p></p><p>&nbsp;</p><p>阿里巴巴集团董事长蔡崇信在与摩根大通北亚区董事长兼大中华区投资银行业务副主席&nbsp;Kam&nbsp;Shing&nbsp;Kwang&nbsp;的对话中强调，尽管面临监管、竞争压力和地缘政治等挑战，阿里巴巴仍聚焦于增长。公司致力于技术创新，并设定了未来&nbsp;10&nbsp;年的增长目标，旨在恢复营收增长，计划在&nbsp;2027&nbsp;财年实现两位数的增长。蔡崇信还表示，管理层会确保资源有效利用，并赋予&nbsp;CEO&nbsp;和运营团队更多自主权，以便根据实际情况作出合理决策。</p><p>&nbsp;</p><p></p><h4>900多个员工一脸懵逼：上午还在改bug、下午集团就解散了</h4><p></p><p>&nbsp;</p><p>5月28日消息，据多方爆料，上海极目银河公司老板跑路，因在国外玩金融亏损62亿，近千名员工两个月没发工资，公司宣布破产。极目银河员工称，老板跑了，公司原地解散，将近900多人的员工直接失业。上午还在改着bug，下午集团就解散了，还有几位新入职的同事，据说亏了62亿直接跑了。</p><p>&nbsp;</p><p>一位员工表示，2024年4月9日，公司发布全员通知称，原定于4月10日发放3月份工资，延迟到15日发放，但15日仍未发放。5月15日，公司口头通知4月份工资要到5月24日发放，24日口头又通知，工资5月27日下午3点发放，但到27日下午1点，得知老板跑路，公司破产。</p><p>&nbsp;</p><p>根据报案回执信息显示，老板陈群于2024年5月24日9时开始失联，去向不明。去他家里发现人去楼空，留下字条称，“无法对付投资人62亿，只能选择逃避。”</p><p>&nbsp;</p><p>此前有爆料称，这公司实际上就是几个小公司凑起来的，看起来什么业务都做，实际上没一个精通。注册资本7000万，实缴不敢公示，社保缴纳也不公开。各位求职者擦亮眼睛！</p><p></p><h4>智谱AI获中东财团&nbsp;4&nbsp;亿美元投资</h4><p></p><p>&nbsp;</p><p>据英国《金融时报》5&nbsp;月&nbsp;31&nbsp;日援引两位知情人士的消息报道，中东石油巨头沙特阿美旗下风险投资部门管理的&nbsp;Prosperity7&nbsp;基金参与了对中国人工智能初创企业智谱&nbsp;AI&nbsp;约&nbsp;4&nbsp;亿美元的一轮投资。</p><p>&nbsp;</p><p>《金融时报》表示：预计这笔投资将使智谱&nbsp;AI&nbsp;的估值达到约为&nbsp;30&nbsp;亿美元，此次投资也使得&nbsp;Prosperity7&nbsp;基金成为了首个对中国生成式人工智能初创企业投下重金的外国投资者。这笔投资表明，沙特愿意支持中国的人工智能生态系统。据一位知情人士表示，“沙特并不希望硅谷主导这个（人工智能）行业。”</p><p>&nbsp;</p><p></p><h3>IT&nbsp;业界</h3><p></p><p></p><h4>2500页文档曝谷歌搜索黑幕：用户数据被挪用、品牌主宰搜索排名</h4><p></p><p>&nbsp;</p><p>2500页谷歌搜索API泄露文档显示，谷歌使用点击数据、Chrome浏览器数据、白名单机制、质量评估员反馈等因素影响搜索排名，与官方公开表态矛盾。文档由资深SEO从业者Erfan&nbsp;Azimi泄露，旨在提高谷歌排名机制透明度。泄露内容可能对SEO领域产生重大影响，改变中小企业SEO策略。</p><p></p><h4>GPT-4o&nbsp;新功能已免费开放</h4><p></p><p>&nbsp;</p><p>5&nbsp;月&nbsp;30&nbsp;日，ChatGPT&nbsp;免费用户现已能够尝试自定义&nbsp;GPT&nbsp;模型、分析图表等其他&nbsp;GPT-4o&nbsp;新功能。当然，OpenAI&nbsp;在推出&nbsp;GPT-4o&nbsp;就承诺它将免费向所有用户开放，而付费用户的优势在于“拥有免费用户五倍的容量限制”。</p><p>&nbsp;</p><p>在此以前，诸如模型和网络响应、数据分析、图表创建、视觉、文件上传、内存和自定义&nbsp;GPT&nbsp;模型等功能仅对付费用户（ChatGPT&nbsp;Plus、Teams&nbsp;和&nbsp;Enterprise）开放，但现在已经面向所有&nbsp;ChatGPT&nbsp;用户开放（当免费用户使用&nbsp;GPT-4o&nbsp;达到消息或对话限制时将自动恢复到&nbsp;GPT-3.5）。</p><p>&nbsp;</p><p></p><h4>百度被曝或将在明年发布文心大模型5.0</h4><p></p><p>&nbsp;</p><p>据报道，百度或将于2025年百度世界大会期间发布新一代文心大模型5.0。目前，文心大模型最新版本为4.0版本，该版本于2023年举办的百度世界大会上由百度创始人李彦宏发布，具备理解、生成、逻辑和记忆四大核心能力。</p><p>&nbsp;</p><p>据李彦宏2023年10月发布时介绍，文心大模型4.0发布后相比GPT-4便已毫不逊色。据了解，历年百度世界大会举办时间均在每年下半年，按此推算，文心大模型5.0或将在2025年下半年与大家见面。公开信息显示，百度文心大模型于2019年首发，2021年发布3.0版本，2023年升级至4.0版本。</p><p>&nbsp;</p><p>此外，日前，百度集团执行副总裁沈抖向外界解释了文心系列主力大模型免费背后的原因，希望友商们别再天天拉表格比价格了，用更多的时间去卷场景、卷应用，把相关的场景都试一遍，快速验证。跑成功了，就快速复制。</p><p>&nbsp;</p><p></p><h4>宝马工厂机器狗上岗，可“嗅探”故障隐患</h4><p></p><p>&nbsp;</p><p>近日，宝马英国汉姆霍尔发动机工厂引进了一只名为&nbsp;SpOTTO&nbsp;的机器搜索犬，用于帮助检测生产线问题。这一举措引发人们对《华氏&nbsp;451》中机械猎犬的联想，然而&nbsp;SpOTTO&nbsp;并未具备小说中的暴力功能，其制造方为波士顿动力公司。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/26/26592eb27bee28dd76e504065e967dc1.jpeg" /></p><p></p><p>SpOTTO&nbsp;这个名称来源于两位奥托（Otto）——其中古斯塔夫·奥托是宝马的联合创始人之一，而尼古拉斯·奥托则是四冲程内燃机的发明者。该机器将作为汉姆霍尔工厂的一员，在此工厂每年生产&nbsp;40&nbsp;万台宝马和&nbsp;MINI&nbsp;的&nbsp;TwinPower&nbsp;涡轮增压发动机期间发挥重要作用。SpOTTO&nbsp;通过视觉、热成像和声音传感器来发现过热的设备和压缩空气泄漏等问题。这些问题如果不及时处理可能导致工厂停工或效率低下。</p><p>&nbsp;</p><p>除了发现潜在问题外，SpOTTO&nbsp;还可以爬楼梯，并为宝马的汉姆霍尔工厂的数字孪生体收集数据。未来，它还有可能承担更多任务，如读取模拟控制装置数据。这一举措旨在确保生产线的顺利进行，并进一步提高生产效率。</p><p></p><h4>腾讯、百度相继发布&nbsp;AI&nbsp;应用</h4><p></p><p>&nbsp;</p><p>本周，腾讯发布了一款名为&nbsp;“腾讯元宝”&nbsp;的&nbsp;AI&nbsp;助手，基于混元大模型，可在多个应用商店下载。该助手具备&nbsp;AI&nbsp;搜索、总结、写作等能力，能解析多种文档格式，并支持长上下文窗口。此外，腾讯还提供了包括&nbsp;AI&nbsp;头像、口语陪练和超能翻译等在内的有趣实用&nbsp;AI&nbsp;应用。混元文生图大模型也已升级并对外开源。</p><p>&nbsp;</p><p>同时，在&nbsp;2024&nbsp;百度移动生态万象大会上，百度副总裁王颖推出全新&nbsp;AI&nbsp;原生应用&nbsp;「橙篇」，该应用可实现文件处理、创作等功能，依托百度文库&nbsp;12&nbsp;亿内容积累和多项&nbsp;AI&nbsp;技术。此外，百度文库经过大模型重构后，已吸引&nbsp;1.4&nbsp;亿&nbsp;AI&nbsp;用户，AI&nbsp;功能使用次数突破&nbsp;15&nbsp;亿次，显示了百度在&nbsp;AI&nbsp;内容获取与创作领域的领先地位。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ZrasmSqhfYByA0JXUj0d</id>
            <title>Meta 首席科学家：不要从事 LLM 工作；代打卡成产业链，刘强东怒了：业绩不好不拼搏全部淘汰；上千页文档揭谷歌搜索黑幕 | Q资讯</title>
            <link>https://www.infoq.cn/article/ZrasmSqhfYByA0JXUj0d</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ZrasmSqhfYByA0JXUj0d</guid>
            <pubDate></pubDate>
            <updated>Mon, 03 Jun 2024 08:25:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: TikTok, FSD, Meta AI, DNF手游
<br>
<br>
总结: TikTok正在回应美国版本核心算法开发的传闻；特斯拉即将在中国推出FSD全自动驾驶系统；Meta AI首席执行官建议不从事LLM工作；腾讯旗下DNF手游首周狂揽10亿元。 </div>
                        <hr>
                    
                    <p></p><blockquote>TikTok 回应“正开发核心算法的美国版本”；传特斯拉的 FSD 即将在中国落地；腾讯《DNF》手游首周狂揽 10 亿元；米哈游再次起诉哔哩哔哩侵权；GPT-4o 向所有人免费开放！字节再试 AI 硬件；中国首例消费者起诉苹果垄断案一审宣判；OpenAI 组建新的安全团队；知乎 PC 网页端非登录用户无法查看回答全文；联想集团裁员 7500 人；谷歌搜索算法内幕被扒；Meta 为社交媒体数据工具 CrowdTangle 增添安全功能；XZ 5.6.2 释出，移除后门代码……&nbsp;&nbsp;</blockquote><p></p><p></p><h2>科技公司</h2><p></p><p></p><h4>TikTok回应“正开发核心算法的美国版本”</h4><p></p><p>当地时间5月30日，有消息称，字节跳动旗下TikTok正开发核心算法的“美国版本”。一旦代码分开，有望为字节跳动美国资产的剥离奠定基础，但该公司目前并无此计划。</p><p></p><p>TikTok对此说法在社交媒体平台X发文回应称：相关报道具有误导性且与事实不符。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8f/8f1d61707cc872775aa48e0ba9da46d3.png" /></p><p></p><p>TikTok表示，TikTok剥离法案要求的TikTok继续在美运营的“合格剥离”动作，在商业、技术和法律上都是不可能的，并且“肯定不可能在该法案要求的270天时限内完成”。</p><p></p><p>更多阅读：《<a href="https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651207433&amp;idx=1&amp;sn=b0d3776443b6844e19dcc2e371d790cf&amp;chksm=bdbbcf5a8acc464c548ee33611d68afb58a21d5407fae1d87f4c88b11ed8ccd699a1815d33fb&amp;token=927577701&amp;lang=zh_CN#rd">剥离几百万行代码，复制核心算法去美国？TikTok&nbsp;最新回应来了</a>"》</p><p></p><h4>传特斯拉的&nbsp;FSD&nbsp;即将在中国落地</h4><p></p><p>5月31日消息，据外媒报道，有知情人士表示，特斯拉已成功获得中国工业和信息化部的软件注册，这将为特斯拉内部测试全自动驾驶(FSD)铺平道路，特斯拉员工将在中国公共道路上测试，并计划在未来几个月内升级推送给中国用户。然而，在中国电动汽车制造商竞争激烈的背景下，2024年一至四月期间，由于市场环境不景气等原因导致了特斯拉在华销量下滑7.6%。</p><p></p><p>另外值得一提的是，《关于汽车数据处理4项安全要求检测情况的通报》（第一批）于4月28日发布，其中提到特斯拉上海超级工厂生产的车型全部符合合规要求，成为唯一一家符合此要求的外资企业。这些措施表明特斯拉正积极遵守中国相关法规并努力提升其在华业务的发展前景。</p><p></p><p>针对此事，特斯拉客服表示，目前内部员工没有进行相关的测试。不同城市不太一样，即使未来能够落地或者开放，也是在允许完全自动驾驶能力测试的城市开放。"我们确实在筹备这件事，但具体什么时间落地，也需要一个长久的时间，目前官方没有任何相关消息。用户可以关注官方公众号和官方微博，如果有最新的进展或者待发布的时间我们会第一时间通知给所有的车主。</p><p></p><h4>Meta&nbsp;AI&nbsp;首席执行官&nbsp;LeCun：不要从事LLM工作</h4><p></p><p>在近日巴黎举行的初创企业年度技术大会VivaTech上，有着“人工智能业鲁迅”之称的Meta&nbsp;AI的首席执行官Yann&nbsp;LeCun建议希望在AI生态系统中工作的学生不要从事LLM（大型语言模型或称“法学硕士”）方面的工作。</p><p></p><p>“如果你是对构建下一代AI系统感兴趣的学生，请不要从事LLM方面的工作。这是大公司的事情，你们无法对此有所贡献，”LeCun在会议上表示。他还说，人们应该开发能够克服大型语言模型局限性的下一代AI系统。</p><p></p><p>有趣的是，关于LLM（大型语言模型）替代品的讨论已经持续了一段时间。最近，Devika的年轻创始人Mufeed&nbsp;VH（Devika是Devin的替代品）谈到了人们应该如何远离Transformer模型并开始构建新的架构。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3a/3af342d4c19c169990c5185b58116da4.png" /></p><p>他还认为，通过这种方法，甚至有可能构建出与GPT-4一样令人印象深刻的东西。</p><p></p><p>尽管LeCun反对一股脑全部研究LLM，但Transformer训练模型仍在不断发展。AI/ML顾问Dan&nbsp;Hou谈到了GPT-4o，并强调了其训练模型。</p><p></p><h4>腾讯《DNF》手游首周狂揽10亿元，App&nbsp;Store&nbsp;下载超260万次</h4><p></p><p>根据彭博社报道，腾讯上周刚刚上线的爆款手游《地下城与勇士：起源》首周狂揽1.4亿美元（约合人民币10亿元），一跃成为近期腾讯最赚钱的手游产品。在iOS国区畅销榜上，《DNF》手游持续霸榜9天，这个记录有望进一步保持。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ac/ac1754779d6d07d1594b5bd9d85fad32.png" /></p><p></p><p>此外，根据Sensor&nbsp;Tower&nbsp;的数据显示，《DNF》手游在App&nbsp;Store商店共吸金6300万美元，下载次数为260万次，值得一提的事，这个数据不包括中国安卓一级第三方的下载和收入。</p><p></p><p><img src="https://static001.geekbang.org/infoq/12/12872f735d0f71e0ce6411d7592c0b61.jpeg" /></p><p></p><p>彭博社补充道，《DNF》手游已经成为上周App&nbsp;Store上最赚钱的游戏，紧随其后的事《王者荣耀》和《和平精英》。Niko&nbsp;Partners的分析师则认为，《DNF》手游可能在今年为腾讯带来超过10亿美元的收入，但是否能成为下一个《王者荣耀》，还有待长期去观察。</p><p></p><h4>即刻开庭！米哈游再次起诉哔哩哔哩侵权</h4><p></p><p>近日，上海米哈游天命科技有限公司与B站关联公司上海宽娱数码科技有限公司、上海幻电信息科技有限公司相关著作权权属、侵权纠纷案件新增开庭公告，原告为上海米哈游天命科技有限公司，该案于5月31日在上海市浦东区人民法院开庭审理。</p><p></p><p><img src="https://static001.geekbang.org/infoq/34/341622afe6864e3657f9bf8dbba3cde8.png" /></p><p></p><p>此次诉讼由上海米哈游天命科技有限公司作为原告提起，再次将哔哩哔哩推上了风口浪尖。根据天眼查的法律诉讼信息，此次案件涉及的是著作权权属和侵权问题，追溯过往，米哈游与哔哩哔哩的拉锯战并非首次。</p><p></p><p><img src="https://static001.geekbang.org/infoq/76/76358096cd87380f0026bfafb8599ba3.png" /></p><p></p><p>历史记录显示，米哈游早前已因类似侵权争议对上海宽娱数码科技有限公司提起诉讼，尽管最终以原告主动撤诉收场，但双方恩怨显然未了。</p><p></p><h4>终于，GPT-4o&nbsp;向所有人免费开放！</h4><p></p><p>5月30日凌晨，OpenAI在x平台宣布，GPT-4o多模态能力向所有用户免费开放。不过，在使用次数上官方没有更新公告，预计还是会是有一定次数限制。</p><p></p><p><img src="https://static001.geekbang.org/infoq/55/5599021da5ddc33fadabdbe4280c983d.jpeg" /></p><p></p><p>本次免费开放的功能包括：联网搜索、数据分析、视觉分析（照片）、文件上传、GPTs商店和长期记忆。</p><p></p><p>使用网址：<a href="https://www.chatgpt.com/">www.chatgpt.com</a>"</p><p></p><p>北京时间5月14日凌晨，OpenAI举办发布会，带来了GPT-4o。该模型打通了文本、图片、视频和语音输入，无需中间转换，互相之间就可以直接生成。此次发布会还带来了更智能的实时语音助手，以及适用于macOS的ChatGPT桌面应用程序。</p><p></p><p>当时，OpenAI宣布向ChatGPTPlus和Team用户推出GPT-4o，很快就会向企业用户推出。同时，也向所有人开放GPT-4o多模态能力，但有使用次数限制。</p><p></p><h4>文心一言再换帅！原快手副总裁薛苏加入百度</h4><p></p><p>5月30日，百度举办的百度移动生态万象大会中，新任百度副总裁、AI产品创新业务负责人，New&nbsp;App负责人薛苏正式登场。</p><p></p><p>薛苏，原快手副总裁，2020年加入快手，先后负责短剧与二次元业务、体育业务。2023年底，快手进行大规模组织架构调整，薛苏不再担任运营部二次元内容中心的负责人，同时也不再兼任运营部体育业务中心和医疗健康组的负责人。</p><p></p><p>薛苏毕业于清华大学自动化系，在耶鲁大学取得博士学位，研究方向为计算机视觉与图形学，曾在EA和Facebook工作。</p><p></p><p>百度NEW&nbsp;App是一个集成了百度“AI&nbsp;伙伴”&nbsp;和&nbsp;“AI&nbsp;BOT”&nbsp;等功能的内测产品，在2023年百度移动生态大会上，由百度集团资深副总裁、百度移动生态事业群组（MEG）总经理何俊杰正式发布。</p><p></p><h4>阿里巴巴近2亿元投资，AI+“首战”在教育行业打响</h4><p></p><p>5月29日消息，教育科技公司精准学完成了新一轮融资，投资方为阿里巴巴，投资金额近2亿元。精准学成立于2018年1月，基于AI推荐引擎技术的“精准学理念”，此前与包括上海昂立在内的国内数千家教育培训学校建立了合作，也是学而思、高思集团的战略合作伙伴。</p><p></p><p>从2023年开始，精准学基于生成式AI技术开始了一次产品重构，以阿里通义千问大模型为基础，自主研发了“心流知镜”大模型，基于此研发了自有的AI教育应用，搭载在硬件之上。</p><p></p><p>精准学旗下的首款原生代AI辅学机将于今年6月对外发布，该产品基于阿里云通义大模型及虚拟数字人技术打造。值得关注的是，这也是阿里巴巴在过去两年中，首次在AI应用领域出手。</p><p></p><h4>字节再试AI硬件，两条产品线共发力</h4><p></p><p>近期，彭博社报道字节以5000万美元（约合人民币3.62元），收购中国耳机制造商Oladance。彭博社援引知情人士称，此举是因为字节管理层看到了可穿戴设备成为AIGC服务平台的潜力。</p><p></p><p>从多位知情人士处获悉，字节AI硬件方向的探索，在内部分为两条产品线：</p><p></p><p>一条产品线代号为“D线”，负责人为李浩乾，其为OWS（Open&nbsp;Wearable&nbsp;Stereo，开放式可穿戴立体声耳机）耳机品牌Oladance创始人。不久前，字节收购了这一品牌。</p><p></p><p>目前，Oladance团队正在融合进字节体系，整体归在字节跳动移动OS中台业务部下，李浩乾向邹伟汇报——邹伟曾经为锤子手机的软件负责人，在锤子手机被收购后加入字节，曾经是字节Smartisan&nbsp;OS的负责人。</p><p></p><p>另一条产品线为O线，负责人也是字节曾收购公司的创始人，其向字节跳动技术副总裁洪定坤汇报。</p><p></p><h4>不满30%“苹果税”，中国首例消费者起诉苹果垄断案一审宣判</h4><p></p><p>5月29日，上海知识产权法院就中国首例消费者起诉“苹果”垄断案一审宣判，法院认定苹果公司在中国区软件市场显然具有市场支配地位，但没有滥用市场支配地位，驳回原告金某的诉请。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9bc8962eebb79206d92df7ffa8c8e7f5.png" /></p><p></p><p>因不满苹果对“应用内购买”收取的30%的佣金，苹果手机消费者金某（原告）于2021年1月11日将苹果公司和苹果电脑贸易（上海）有限公司告上法庭。根据裁判文书，金某核心诉求包括停止收取30%“苹果税”的不公平高价行为以及停止“应用内购买”强制使用苹果支付系统的搭售行为。</p><p></p><p></p><blockquote>注：所谓“苹果税”，是指苹果对 App Store 上所有应用的数字内容消费抽取 15%~30% 的佣金。每当苹果用户通过苹果手机应用商店付费下载 App 或在 App 内部购买数字商品 / 服务时，苹果公司会扣留交易金额的一部分作为“过路费”，再将剩下的转给相应的 App 开发者。如果不接受“苹果税”的提成安排，或者应用程序内置有隐藏的其他支付方式，苹果公司将径直下架该款应用程序。据了解，中国目前是全球范围内苹果公司对于 App 内交易抽成费率最高的国家，抽成费率为 30%。近年来，苹果公司已在欧盟、美国、日本、韩国等多地遭政府起诉或调查，欧盟地区“苹果税”从今年 3 月开始大幅下降。</blockquote><p></p><p></p><p>原告代理律师表示，尊重一审法院的判决，但将针对本案上诉至最高人民法院。</p><p></p><h4>“业绩不好不是我的兄弟”！京东“整顿职场”上热搜</h4><p></p><p>5月24日，有媒体报出刘强东“凡是长期业绩不好，从来不拼搏的人，不是我的兄弟”的发言，因为在2018年刘强东说永远不开除任何一个兄弟。让大众注意到，曾经把传统商业大鳄国美、苏宁拉下马，并跑赢淘宝天猫的京东，如今发展很不如意。</p><p></p><p>尤其是5月22日，拼多多以高达2042.74亿美元的市值再度超越阿里巴巴，4倍于京东，应该让京东高管团队更加坐立难安。这期间又爆出关于于京东内部考勤调整、午休时间缩短、人员优化等一系列事件引发舆论热议。</p><p></p><p><img src="https://static001.geekbang.org/infoq/81/812a1b7e94f88fce19a2393b77e40213.jpeg" /></p><p></p><p></p><blockquote>一名京东员工表示：有员工一年里代打卡近百天，每天 4 点就下班，但是能领到全额薪水，还有一些实习生甚至两个月都不来，但通过代打卡，骗走了公司 1.5 万的工资。听说内部统计代打卡的人次每月近万次。”而京东午休时长达 2 个小时，很多员工下午 2 点半才开始工作，甚至可以熄灯睡觉。2 点半并不晚，但京东的上班时间很晚，弹性工作制的部门上午 10 点 30 才打卡，非弹性部门 9 点打卡，就这还有很多人花钱找人代打卡。然后上班 2 个小时不到，吃午饭了，然后再睡 2 个小时……</blockquote><p></p><p></p><h4>OpenAI&nbsp;前安全研究员&nbsp;Jan&nbsp;Leike&nbsp;转投&nbsp;Anthropic；OpenAI组建新的安全团队</h4><p></p><p>5月28日，OpenAI宣布，成立安全与审查委员会（SSC），负责针对所有的OpenAI项目制定至关重要的安全与审查决定。而值得注意的是，进入该安全顾问委员会的成员全部为该公司内部人员，包括OpenAI的CEO&nbsp;Sam&nbsp;Altman，外部人员一个也没有。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3b/3be5b0e7150eacd87e30e5d6381bc76d.png" /></p><p></p><p>OpenAI还宣布，最近开始训练其下一个前沿模型，我们预计由此产生的系统将使我们在通往通用人工智能的道路上迈上一个新台阶。</p><p></p><p>毕竟在5月15日-17日，OpenAI超对齐团队两位负责人接连离职。其中包括OpenAI联合创始人、首席科学家伊利亚·苏茨克维尔（Ilya&nbsp;Sutskever），以及该团队的负责人简·雷克（Jan&nbsp;Leike）。</p><p></p><p>Jan&nbsp;Leike，OpenAI的前首席安全研究员，在5月15日宣布离职后，于本周二正式加入人工智能初创公司Anthropic。在OpenAI解散了他共同领导的超级对齐团队后，Leike表示，他将在Anthropic继续推动AI安全和监督的研究工作。Anthropic得到了亚马逊的大力支持，后者承诺投资高达40亿美元以换取少数股权。同时，OpenAI也在积极调整，新成立了一个由CEO领导的安全委员会，以确保公司项目和运营的安全。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/eced874ce3eddd98ed000070c5459230.jpeg" /></p><p></p><h4>昆仑万维：天工AI日活用户超过100万</h4><p></p><p>5月27日，昆仑万维集团宣布，天工AI每日活跃用户（DAU）已超过100万。</p><p></p><p>5月，全球大模型领域的竞争，分外激烈；谷歌、腾讯、阿里、字节跳动等业界巨擘纷纷亮出大招。有的凭借技术底蕴实现全面升级，有的慷慨将大模型开源免费，更有甚者通过价格战掀起狂风巨浪，整个市场一片火热。然而在这硝烟弥漫的战场上，昆仑万维作为中国AI的先驱者，近日正式公布天工AI日活用户（DAU）突破100万</p><p></p><p>根据QuestMobile的数据，2024年3月，天工平台的月活跃用户已达近千万，仅次于豆包与文心一言，成为国内活跃用户数第三的平台。</p><p></p><p><img src="https://static001.geekbang.org/infoq/93/937f29a2a1d7775ec4dfa77e8be9b4e6.png" /></p><p></p><p>据悉，昆仑万维集团在4月17日推出了天工3.0和天工SkyMusic，这两款产品的推出，为天工平台的增长注入了新的动力。天工3.0是一款拥有4000亿参数的开源大模型，其在多个领域实现了性能上的突破性提升。</p><p></p><h4>知乎PC网页端非登录用户无法查看回答全文</h4><p></p><p>5月27日，有网友反映称，知乎网页端近期出现了非登录用户无法查看全文的情况。测试显示，以知乎当前热榜第一的内容为例，非登录PC网页用户访问知乎的回答页面，如果点击“展开阅读全文”，平台会自动弹出登录窗口，确实无法直接查看全文。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0e/0e9075f4abd2444caad05c6229803589.png" /></p><p></p><p>而非登录手机网页用户，则可以直接点击“展开阅读全文”查看全部信息，无需登录或下载App。对于这一情况，知乎官方暂未给出回应。</p><p></p><p>据了解，2022年3月初，工信部表示有网友和媒体反映部分网站在用户浏览页面信息时，强制要求下载App问题。工业和信息化部信息通信管理局对此高度重视，立即组织核查，并于日前召开行政指导会，督促相关互联网企业进行整改。</p><p></p><h4>联想集团裁员7500人，遣散费5500万美元！</h4><p></p><p>近日，联想集团发布了2023-2024年度全年财报。这份财报揭示了公司在过去一年中所面临的挑战与压力。财报显示，截至2024年3月31日的年度收入为619.47亿美元，同比下滑8%。与此同时，净利润也呈现下滑趋势，同比减少37%至10.1亿美元。这一业绩的下滑，无疑给联想集团带来了沉重的打击。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f9/f9181c3f3356a43f26b67ef57f40a142.png" /></p><p></p><p>财报披露，2023财年全球总员工77,000人，2024财年则是69,500人，这意味着，在过去的一年联想集团共裁减7500人。联想还在财报中的“费用类别”提到，“鉴于行业挑战”，产生遣散及相关费用5500万美元。联想在财报中“拨备的组成部分”说明中，提到了“主要是员工解雇付款”，目的是降低成本和提高运营效率。</p><p></p><p>值得一提的是，近期联想股价涨至一年来新高。5月27日周一早间，联想集团（00992.HK）股价一度涨逾9%，至12.06港元/股。5月30日，联想以11.52港元/股收盘。业界认为，这是AI给联想回了一口大血，联想开始走出低谷。</p><p></p><h4>软银寻求每年90亿美元的AI投资</h4><p></p><p>据外媒报道，日本软银计划每年向人工智能领域投资近90亿美元，加速企业转型。</p><p></p><p>日前，软银首席财务官后藤义光(Yoshimitsu&nbsp;Goto)向媒体透露，将加大对人工智能公司的投资。为此，软银将资产负债表维持在一个安全水平，以确保投资活动的持续性和灵活性。</p><p></p><p>报道中称，软银在人工智能领域面临着激烈的竞争，因为微软、亚马逊和谷歌等科技巨头已经投资数十亿美元与初创公司合作建立人工智能模型。此外，顶级风投公司希望与开发人工智能产品和应用程序的公司开展合作。</p><p></p><h2>IT&nbsp;业界</h2><p></p><p></p><h4>2500页文档曝谷歌搜索黑幕：用户数据被挪用、设白名单机制、品牌主宰搜索排名</h4><p></p><p>5月29日消息，近日，据市场和受众研究公司SparkToro在官网博客发布的文章，一位匿名消息人士（后证实为搜索引擎优化行业资深从业者Erfan&nbsp;Azimi）向SparkToro公司的CEO&nbsp;Rand&nbsp;Fishkin提供了来自谷歌搜索API的大量泄露文档，这些文档揭示了谷歌搜索引擎内部排名算法的详细信息。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4d/4d7bf253b3ca43bb72cae85f2c61a974.png" /></p><p></p><p>本次泄露文档中的部分内容与谷歌公开表态的信息以及2023年的美国司法部起诉谷歌的反垄断案件中的证词相矛盾。比如，谷歌曾一再否认使用点击导向的排名机制，否认子域名在排名中被单独考虑，否认收集或考虑域名的年限等等。但泄露文档证实，这些因素都是谷歌搜索引擎排名机制中的一环。此次泄露的谷歌搜索API文档共计2500余页，共包含14014个属性特征。这些文档据称是谷歌内部“Content&nbsp;API&nbsp;Warehouse”的一部分，并在代码托管平台GitHub上不慎公开了一段时间。</p><p></p><p>博客作者Rand&nbsp;Fishkin在多位前谷歌员工的沟通中确认，泄露的文档具备谷歌内部API文档的特征。Rand&nbsp;Fishkin与和技术SEO（搜索引擎优化）专家Mike&nbsp;King进行交流，并再次验证了这些文档的真实性。</p><p></p><p>这些文档的泄露可能对搜索营销领域产生深远的影响，因为它们提供了对谷歌搜索引擎内部工作机制的前所未有的洞察，包括它如何使用点击数据、浏览器点击流、白名单、质量评估员反馈和链接质量来影响搜索结果排名。</p><p></p><h4>Meta为社交媒体数据工具CrowdTangle增添安全功能，以消除欧盟顾虑</h4><p></p><p>5月27日，Meta&nbsp;Platforms为社交媒体数据追踪工具CrowdTangle增加了安全功能，试图消除欧盟的担忧。欧盟上个月对其逐步淘汰该工具的决定的影响开展调查。Meta上周表示，下月投票的欧洲议会选举候选人将在其Facebook和Instagram的feed顶部看到如何保护自己及其账户的通知。</p><p></p><p>通知中包含的指引将帮助候选人找到相关资源，学习如何设置双因素认证、检查安全设置或启用Instagram的"隐藏词语"功能。"隐藏词语"是Instagram的一项特性，能够自动将含有攻击性词汇、短语和表情符号的私信请求转移到一个隐蔽的文件夹中。</p><p></p><p>Meta在欧盟每月拥有超过2.5亿用户。为了进一步增强选举监督，该公司在上周的宣布基础上，从本周一开始，为每个欧盟国家提供实时监控功能，按关键词、公共群组和Instagram账户进行分类，使研究人员、记者和民间社会组织能够实时监督选举过程。</p><p></p><p>欧盟委员会对Meta的这一举措表示欢迎。在一份声明中，欧盟委员会表示：“委员会将监督这些新功能的实施效果，并将继续与Meta合作，寻找更持久的解决方案，以解决初步决定中提出的所有问题。”</p><p></p><h4>苹果据悉已与OpenAI达成协议，将ChatGPT引入iOS18</h4><p></p><p>当地时间5月26日，彭博科技记者马克·古尔曼（Mark&nbsp;Gurman）在最新一期通讯中表示，传闻几个月的苹果与OpenAI洽谈将生成式人工智能功能引入iOS&nbsp;18一事，有了取得进展的消息，透露双方已经达成了协议。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c0/c0147c23287e5e2cd135eba1725330a7.png" /></p><p></p><p>按惯例，苹果iOS等操作系统的重大更新，在全球开发者大会上就将宣布。为iOS&nbsp;18引入OpenAI的生成式人工智能功能，预计会在今年的大会上宣布</p><p></p><p>苹果和OpenAI洽谈将生成式人工智能引入iOS&nbsp;18的消息，在今年年初就已出现。在3月下旬的报道中，就有外媒提到苹果和百度、OpenAI及谷歌，在就引入他们的生成式人工智能模型、赋予部分iPhone机型生成式人工智能功能有过谈判，其中与百度的谈判是针对国内市场，与谷歌和OpenAI则是针对国外市场。</p><p></p><h4>XZ&nbsp;5.6.2&nbsp;释出，移除后门代码</h4><p></p><p>今年3月29日，微软PostgreSQL开发人员Andres&nbsp;Freund在调试SSH性能问题时，在开源安全邮件列表中发帖称，他在XZ软件包中发现了一个涉及混淆恶意代码的供应链攻击。据Freund和RedHat称，Git版XZ中没有恶意代码，只有完整下载包中存在。但是这个代码的提交人两年前就加入了项目维护，暂时不能确定之前的版本有没有问题。</p><p></p><p>引发广泛关注的&nbsp;XZ&nbsp;后门事件两个月之后，项目维护者&nbsp;Lasse&nbsp;Collin&nbsp;释出了新版本&nbsp;XZ&nbsp;5.6.2，移除了&nbsp;v5.6&nbsp;和&nbsp;v5.6.1&nbsp;中的后门代码&nbsp;CVE-2024-3094。</p><p></p><p>他同时宣布了一位支持维护者&nbsp;Sam&nbsp;James。对&nbsp;XZ&nbsp;后门事件的调查仍然在进行之中。XZ&nbsp;5.6.2&nbsp;还修复了一系列&nbsp;bug，包括修复了用最新&nbsp;NVIDIA&nbsp;HPC&nbsp;SDK&nbsp;构建的问题，移除&nbsp;GNU&nbsp;Indirect&nbsp;Function(IFUNC)支持，XZ&nbsp;后门代码使用了&nbsp;IFUNC&nbsp;支持，但移除主要是因为性能优势太小但复杂性大幅增加。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/YSpCsCCLvHvrsXB179Wm</id>
            <title>这个离开大厂去 AI 创业的互联网大佬，带着他的“Killer Agent”来了</title>
            <link>https://www.infoq.cn/article/YSpCsCCLvHvrsXB179Wm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/YSpCsCCLvHvrsXB179Wm</guid>
            <pubDate></pubDate>
            <updated>Mon, 03 Jun 2024 08:03:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 出品, InfoQ, 大模型领航者, To B
<br>
<br>
总结: 2020年，黎科峰博士离职创业，选择了To B领域，创业3年营收数亿元，大模型产品符合To B行业需求，将颠覆现有To B软件，改变决策关系。如何选择和应用大模型，是To B行业面临的挑战。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>出品｜InfoQ 《大模型领航者》访谈主持｜霍太稳，极客邦科技创始人兼 CEO访谈嘉宾｜黎科峰博士，数势科技创始人兼CEO作者｜褚杏娟</blockquote><p></p><p>&nbsp;</p><p>2020年是一个不同寻常的年份。人们因新冠不得不停下来的时候，时任京东集团副总裁、技术委员会主席、京东商城技术负责人的黎科峰博士却选择了离职创业。此时的黎科峰博士，已经在互联网行业摸爬滚打了十余年。</p><p>&nbsp;</p><p>他曾在三星亲历了公司第一代智能手机诞生；在平安集团作为执委、平安金融科技CTO推动平安集团的用户和数据体系建设，以及金融科技云平台建设；在百度，作为原百度云计算创始成员推动百度云OS初创，也曾担任过手机百度总经理，带领实现日活跃用户1亿的突破。</p><p>&nbsp;</p><p>意料之外，情理之中。在经历了PC和移动互联网两次浪潮后，黎科峰博士明显感到To C的流量增长已经放缓，红利期基本结束，大厂们也开始在To B 领域布局。To B 就是黎科峰博士给自己选的赛道，而这次他恰好赶上了大模型这波浪潮。</p><p>&nbsp;</p><p>黎科峰博士一直说自己是“To B 新人”，但他创业3年后就让公司营收达数亿元的成绩，已经无法让人用“新人”来定义他。</p><p>&nbsp;</p><p></p><h2>C 端大佬，凭何入局 B 端</h2><p></p><p>&nbsp;</p><p></p><blockquote>“创业，始终是一条不同寻常的道路。别人已经走过并走通了的路，没有必要再去重复。”</blockquote><p></p><p>&nbsp;</p><p>中国的 To B 公司大致有两种。</p><p>&nbsp;</p><p>一种是专注项目制和定制化服务的公司，根据客户提出的具体需求逐一开发。但问题是客户的需求是零散和片段化的，他们可能并不清楚自己的数字化是什么样的。</p><p>&nbsp;</p><p>另一种则是“Copy to China”模式，商业模型、产品理念，甚至PMF理论很多是直接从美国照搬过来。但中国的市场环境和美国不同，简单复制很难成功。</p><p>&nbsp;</p><p>企业内部通常分为利润中心和成本中心，其中成本中心包括了行政、人力资源、财务和IT部门等。国内许多To B 软件实际上是围绕成本中心设计的，比如OA系统等，但这些软件的付费能力相对较弱，转化为直接业务价值的路径较长。</p><p>&nbsp;</p><p>正如黎科峰博士所说，“企业高层或数字化负责人很难有底气地说，企业增长的20%-30%是他们带来的。”</p><p>&nbsp;</p><p>表面上看，国内数字化认知不足、大家付费意愿不强等都是To B 企业面临的普遍挑战。但长期从事To C产品的经验告诉黎科峰博士，问题的关键是从业者没有解决真正的痛点、真正帮助用户创造价值，To C还是To B 反而并不重要。</p><p>&nbsp;</p><p>根据黎科峰博士观察，国内数千万的企业正在从过去依靠规模增长的模式，转变为追求内在能力提升和经营效率提高的模式。企业意识到，仅凭直觉和经验已经难以持续增长，开始寻求技术的帮助。</p><p>&nbsp;</p><p>但不同行业的软件使用者技术水平存在差异。电商、金融科技等互联网企业数字化水平较高，员工可以熟练使用复杂软件，但其他行业的企业就难以有效使用，致使其软件使用成本很高。</p><p>&nbsp;</p><p>因此，在黎科峰博士看来，To B 行业要真正发展，首先需要建立共识，即让市场参与者普遍认同软件或解决方案的价值，否则就得花大量的时间和精力说服和教育市场。其次，产品必须足够简单易用，能轻松融入业务团队的日常工作，并且无需复杂培训。</p><p>&nbsp;</p><p>大模型产品完美符合上述条件。</p><p>&nbsp;</p><p>不用多说，大家已经对大模型的价值和潜力有了共识，这是一个非常重要的优势。另外，大模型降低了数据的使用门槛，让AI和大数据不再只属于技术圈，而是广泛的普通大众。</p><p>&nbsp;</p><p>如今，虽然不是每家企业都开始实际应用大模型，但至少都在研究、了解和学习。走得远的企业已经开始尝试将大模型技术应用于实际业务中，甚至有些已经签了商业合同。</p><p>&nbsp;</p><p>“这种对大模型技术的高度认可和期待，甚至超过了当年的云计算。”黎科峰博士表示。</p><p>&nbsp;</p><p>传统的 To B 软件十分复杂，大多数功能用户并不需要，同时很久都不做本质上的改进，长期保持最初的界面和功能。而通过基于大模型的软件，用户通过简单表达就可以实时、精确地找到所需信息，软件还能不断适应和学习，越来越贴合用户的个性化需求和使用习惯。</p><p>&nbsp;</p><p>“从这个角度来看，大模型技术会颠覆现有的To B软件，甚至让很多To B软件过时。”黎科峰博士补充道，“当然，现有的ToB软件开发商也会努力追赶，但他们可能会因为历史包袱而受限。”</p><p>&nbsp;</p><p>另外，大模型产品还将改变&nbsp;To B 业务的决策关系。</p><p>&nbsp;</p><p>黎科峰博士表示，国内软件的使用者和决策者往往不是一线员工，而是不使用软件的管理层。大模型产品则让每个员工都能通过使用工具产生价值，这样决策链将不再只是老板，还有真正使用软件的员工，员工对软件效率提升的需求会影响决策者。</p><p>&nbsp;</p><p></p><h2>如何实现大模型价值落地？</h2><p></p><p>&nbsp;</p><p></p><blockquote>“现在的大模型就像一个全科研究生，而Agent是帮助它成为某个特定业务领域专家的实现方式。”</blockquote><p></p><p>&nbsp;</p><p>在明确了大模型对于&nbsp;To B的影响后，该如何应用大模型呢？</p><p>&nbsp;</p><p>困扰很多人的首先是如何选择大模型。根据数势科技的基准测试，国内大模型产品之间的差距并不显著，与国际领先的模型，如GPT-4等比较，算力、准确度等方面确实存在较为明显的差距。</p><p>&nbsp;</p><p>但对于如何应对和看待国内外大模型的发展差异，黎科峰博士认为，“核心是我们是否需要像OpenAI那样投入巨额资金（数百亿）来发展大模型技术。”</p><p>&nbsp;</p><p>OpenAI 开始不计商业成果，致力于推动通用人工智能（AGI），后来也得到了美元基金支持，能够获得的资金是中国企业的很多倍。中国的大模型厂商在资源有限的情况下，需要从一开始就明确自己的方向，思考如何创造价值并实现商业化回报。这涉及到选择哪些方向进行投资，以确保资金的使用更加高效和有针对性。</p><p>&nbsp;</p><p>另外，大型企业和创业公司开发的大模型产品方向也各有千秋。</p><p>&nbsp;</p><p>大企业开发大模型有明显的资源优势，比如丰富的计算资源和大量数据积累，然而也面临着诸如如何将大模型整合到现有业务和产品线等较重的商业化包袱。</p><p>&nbsp;</p><p>而创业公司没有历史包袱，通常从大模型原生视角出发，不受以往业务约束，更自由地探索和创新，因此某些产品的体验可能不逊色大型企业。</p><p>&nbsp;</p><p>当前，一些财力雄厚的大型企业，如银行和国有企业会先投资建立大模型基础设施，如购买GPU和部署具有数千万、甚至数千亿参数的大模型，然后逐步挖掘需求和应用场景。</p><p>&nbsp;</p><p>但更多的企业是先找应用场景，再立项、部署大模型。这些企业会梳理出一系列需求，然后据此决定大模型的应用方向。例如，如何快速准确地找到数据、基于数据进行分析、洞察业务问题以及指导经营以提升成果，通常是很靠前的考虑因素。</p><p>&nbsp;</p><p>黎科峰博士透露，目前率先尝试大模型应用有经营分析（与数据紧密相关的金融、零售和高端制造行业）、内容生成（如营销活动所需的视频或图片生成）、企业知识库（从企业内部的知识库中提取关键信息，以便员工更好查找和学习）。企业经营分析是目前企业最感兴趣的领域，超过90%的企业表现出了明确需求。</p><p>&nbsp;</p><p>大模型具备知识、智商、学习能力和推理能力，能够总结和生成新的见解。AI Agent 的本质是智能体，内部包含许多规划业务流程规划器，能够完成基于业务目标的任务规划和自动执行。</p><p>&nbsp;</p><p>数势科技的做法是，大模型叠加 Agent，让企业应用具备记忆、反思和学习能力，能够调用企业内部工具并不断迭代反思，真正实现业务价值落地。&nbsp;</p><p>&nbsp;</p><p>不过，找到了应用场景后就让To B赛道的创业公司短期内盈利，黎科峰博士认为这并不合理。</p><p>&nbsp;</p><p>“很多企业在没有想明白胜利逻辑和商业化可能性之前，就急于跟风，最终导致失败。”黎科峰博士说道，一个企业的价值不在于目前是否盈利，而是在于它的商业模式是否成立。</p><p>&nbsp;</p><p>就像美国的许多企业一开始并不盈利，但找到正确的盈利模式后会非常赚钱。黎科峰博士认为，这背后的关键原因是，企业要找到产品能够为企业带来的业务价值，并且具有强大的复制性，实现边际成本递减，样才能实现高额盈利。</p><p>&nbsp;</p><p></p><h2>如何在内卷的环境里赚到钱？</h2><p></p><p>&nbsp;</p><p></p><blockquote>“盈利可以通过裁员或非健康的手段实现，但这并不是我们追求的。”</blockquote><p></p><p>&nbsp;</p><p>“商业化是在大厂打工不需要太考虑的问题，但自己做企业就要考虑怎么活下去。”黎科峰博士曾在分享中说到，但他在大家还在探索大模型应用的时候，已经用大模型&nbsp;Agent 赚到钱了。</p><p>&nbsp;</p><p>“真正赚钱的是上层应用。”黎科峰博士分析道，未来，全球企业的业务经营将依赖于工具，而不是人、政策或剥削员工。最有价值的企业将是那些拥有数据、能够通过工具提升效率的企业。</p><p>&nbsp;</p><p>黎科峰博士创建数势科技之初，公司重心是构建数据资产层 ，通过打造企业指标平台和标签平台将企业海量数据转化为可使用的数据资产，释放数据价值。但产品使用门槛较高，更加适用于数字化程度较高、拥有较多数据技术人员的企业。</p><p>&nbsp;</p><p>大模型出现后，数势科技为企业用户提供数据分析AI Agent，用户可以基于自然语言对话进行交互，如此降低软件的使用门槛，管理者、业务人员等非技术人员都可以准确、即时、个性化地进行数据査询和业务洞察，提升决策能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b6/b6cd8c55a8afa1a6299ad10995611f69.png" /></p><p></p><p>数势科技大模型 Agent 产品（SwiftAgent）架构图</p><p>&nbsp;</p><p>“数势科技的优势是使用软件和算法解决问题，而非人力。”黎科峰博士说道，“我们的人员效率很高，是因为我们用技术解决传统上需要大量人力的问题，这也是为什么我们能够实现良好的盈利。”&nbsp;</p><p>&nbsp;</p><p>那黎科峰博士一直强调的“用户价值”，如何体现在产品中的呢？</p><p>&nbsp;</p><p>他举了一个例子。在零售行业，一家茶饮连锁企业面临的挑战是如何让店长进行数据分析。由于店长的教育水平普遍低于科技或互联网公司，让他们快速、高效地利用传统BI工具做数据分析并不现实。而通过数势科技对话式、低门槛的数据分析和决策产品，店长能够通过简单的语音输入获取需要的数据，并理解数据变化背后的原因，从而更好地经营门店。如此，门店的运营不再简单依赖店长能力和经验。</p><p>&nbsp;</p><p>国内市场存在一个非常普遍且残酷的问题：资本市场状况良好时，任何一个新概念的出现都会吸引一大批公司迅速进入，结果就是甲方难以选择供应商，最终仅靠价格抉择。市场也因此变得越来越卷，好的产品和公司不得不参与价格战，最终可能出现经营困难，造成了“劣币驱逐良币”的局面。</p><p>&nbsp;</p><p>大模型市场正在历史重演。To B 企业要在如此的竞争环境中脱颖而出，黎科峰博士认为关键有两点：一要真正理解数据智能和AI，并有深厚积累，这是企业的基因；二是要回归客户需求、懂行业知识，讲能够引起客户共鸣的故事。</p><p>&nbsp;</p><p>在产品技术方面，企业最重要的是想清楚Agent与大模型的依赖关系，优化Agent产品设计，使其更加高效得运作，减少与大模型的交互次数，仅将必要的任务交由大模型处理，从而节省资源。</p><p>&nbsp;</p><p>在业务需求方面，数势科技并不是什么客户、什么行业都做，而是选择自己擅长、有很深Know-how的。这基于黎科峰博士多年大厂经验的总结。“在做技术负责人的时候，我的考核标准也不是单纯地看技术有多炫酷，而是技术能不能让企业赚钱、省钱，产品最重要的是为客户创造价值。”</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>“我带着一种无畏的态度进入这个行业，我没有过往的包袱，也没有已经形成的固定模式，这让我能够更自由地探索和创新。”4年前获得腾讯亿元天使轮融资的黎科峰博士，如今依然站在To B的赛道上。</p><p>&nbsp;</p><p>虽说“好风凭借力”，但黎科峰博士清楚地表示，大模型技术虽然非常有潜力，但本质上依然是一个工具，是用来帮助企业实现业务目标和战略的。对于黎科峰博士来说，大模型是武器，核心竞争力是自己懂业务、懂技术。</p><p>&nbsp;</p><p>未来，数势科技利用大模型还能创造怎样的成绩，我们拭目以待。</p><p>&nbsp;</p><p>&nbsp;</p><p>栏目介绍</p><p>&nbsp;</p><p>《大模型领航者》是 InfoQ 推出的一档聚焦大模型领域的访谈栏目，通过深度对话大模型典范企业的创始人、技术负责人等，为大家呈现最新、最前沿的行业动态和思考，以便更好地参与到大模型研发和落地之中。我们也希望通过传播大模型领域先进的实践和思想理念，帮助潜在大模型应用者、创业者、开发者等做好各类决策和选型。</p><p>&nbsp;</p><p>如果您有意向报名参与栏目或想了解更多信息，可以联系：T_demo（微信，请注明来意）</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/wABFf3KQ2fyq0XTfRsgI</id>
            <title>生生不息，一齐见证 AI 新次元 | InfoQ 中国成立 17 周年</title>
            <link>https://www.infoq.cn/article/wABFf3KQ2fyq0XTfRsgI</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/wABFf3KQ2fyq0XTfRsgI</guid>
            <pubDate></pubDate>
            <updated>Mon, 03 Jun 2024 07:07:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 作者, InfoQ, 技术资讯, 生成式AI
<br>
<br>
总结: InfoQ 是一家致力于提供中立的技术资讯和技术会议的机构，17年来一直关注技术创新和发展，特别是在生成式AI领域。他们与长城战略咨询合作，致力于推动生成式AI技术在各行业的落地，并与中国信通院合作推动人工智能技术的健康发展。通过举办活动和对谈，InfoQ努力引领技术潮流，帮助开发者和企业适应生成式AI时代的变革。 </div>
                        <hr>
                    
                    <p>作者&nbsp;|&nbsp;InfoQ</p><p></p><p>2007&nbsp;年至今，InfoQ&nbsp;中国已走过&nbsp;17&nbsp;年。</p><p></p><p>17&nbsp;年里，我们置身于风云变幻的&nbsp;IT&nbsp;科技浪潮中，亲历了云端计算的兴起、人工智能的飞跃、编程语言的迭代、大数据的洪流、架构艺术的演变以及智能手机的革命，目睹了本土科技力量的茁壮成长；同时，也紧跟生成式&nbsp;AI&nbsp;等前沿技术趋势，全力促进各行业的革新与发展进程。</p><p></p><p>17&nbsp;年来，我们一直致力于提供中立的、由技术实践主导的技术资讯及技术会议，希望促进软件开发及相关领域的知识传播，助力数字时代人才的成长与蜕变，让创新技术推动社会进步。</p><p></p><p>从成立之初到今天，InfoQ&nbsp;始终坚持做技术创新浪潮中坚毅的观察者与驱动者，见证、参与每一次技术突破的航程，记录、传播那些深刻影响时代进步的科技力量与幕后英雄。迄今为止，&nbsp;InfoQ&nbsp;用数万篇高质量的文章，QCon、AICon、FCon、ArchSummit&nbsp;等数十万开发者奔赴现场的超百场技术盛会，汇聚&nbsp;1600+&nbsp;技术领导者的&nbsp;TGO&nbsp;鲲鹏会，拥抱着&nbsp;IT&nbsp;技术时代的变迁。</p><p></p><p>我们紧随技术潮流，倾力打造优质技术内容与高质量的技术学习、交流平台，并携手超过五百万技术同仁与数千家企业，在国内烙印下一代代技术人成长与突破的足迹。</p><p></p><p>如今，InfoQ&nbsp;正向着生成式AI全面进化，做&nbsp;AIGC&nbsp;时代的媒体一号位是我们当下追求的目标，也是未来发展的蓝图。</p><p></p><p>2024&nbsp;年，我们将极客邦科技发展的年度主题定为「生生不息」，希望在新的一年里，InfoQ&nbsp;迸发更强的生命力与创意火花，持续引领技术潮流，共同奔赴将生成式&nbsp;AI&nbsp;落地到千行百业的未来。</p><p></p><p></p><h2>生成式AI时代：适应、转型与超越</h2><p></p><p></p><p>站在&nbsp;17&nbsp;周年这个新起点，我们特别在6月12日策划了&nbsp;17&nbsp;周年庆系列直播活动。除明确今年向着生成式AI全面进化的大方向外，极客邦科技正积极采取多维度策略来推进这一领域的创新和发展。此次周年庆系列活动上，InfoQ&nbsp;将正式宣布公司在生成式&nbsp;AI&nbsp;布局上的两大战略计划。</p><p></p><p>为推动生成式&nbsp;AI&nbsp;技术在千行百业的落地，极客邦科技将宣布与长城战略咨询达成合作。与此同时，长城战略咨询也将发布生成式&nbsp;AI&nbsp;十大企业级通用场景报告。</p><p></p><p>此前，为推动人工智能技术的健康发展和应用，中国信通院与中国互联网协会于&nbsp;2021&nbsp;年底联合发起了“铸基计划”。这次，极客邦科技双数研究院&nbsp;InfoQ&nbsp;研究中心将与中国信通院“铸基计划”共同宣布一项重要的战略合作成果，在&nbsp;2024&nbsp;年&nbsp;8&nbsp;月&nbsp;16&nbsp;日极客邦科技举办的&nbsp;FCon&nbsp;全球金融科技大会首发《AGI&nbsp;在金融领域的应用实践洞察》报告，旨在深入探讨&nbsp;AGI&nbsp;技术在金融领域的应用现状、未来发展趋势以及所面临的挑战。未来，&nbsp;InfoQ&nbsp;研究中心也将继续关注大模型及&nbsp;AIGC&nbsp;领域的应用和产品进展。</p><p></p><p>AI的迅速发展和广泛应用已经成为推动行业变革和业务创新的重要力量。尤其是大模型等前沿技术，正在以前所未有的速度和规模，重新定义企业的运营模式、创新服务和客户体验。这种变革不仅限于技术行业本身，而且正影响到千行百业。各行各业在探索AI大模型应用过程中，既迎来全新挑战，又面临风险与机遇。为此，InfoQ&nbsp;特在周年庆活动中，精心策划了一场&nbsp;AI+行业融合圆桌对话。届时，将有来自金融、工业制造、物流等不同行业的专家到场，围绕生成式&nbsp;AI&nbsp;如何深度融入并重塑各行业展开深入探讨。</p><p></p><p>过去&nbsp;17&nbsp;年间，InfoQ&nbsp;陪伴着中国数以千万计的开发者，共同经历了中国互联网、移动互联网的发展时代，见证了新一轮技术和产业变革的数字化浪潮的到来。今天，在生成式&nbsp;AI&nbsp;的蓬勃发展下，新一代的开发者们面临着比以往更多的选择与机遇。值此&nbsp;17&nbsp;周年之际，&nbsp;InfoQ&nbsp;特别策划了此期以「AI&nbsp;时代下的程序员」为主题的开发者对谈活动。届时，三位处于不同人生阶段的程序员，将一同探讨各自在生成式&nbsp;AI&nbsp;风口之下的思考与感悟。</p><p></p><p>此次对谈，不仅会是一场新、老程序员间的相互坦白局，还穿插着不少犀利问答。在此，先剧透一下部分精选提问：</p><p>是否愿意将程序员作为终身奋斗的事业？现在的工作情况是否符合此前对于程序员职业的预期？AI&nbsp;是否会导致一线程序员的失业？新一代的程序员有怎样的职业追求？程序员更看重公司提供哪些方面的激励体制？</p><p></p><p>此外，InfoQ&nbsp;在持续为技术人群提供服务的过程中注意到，现今技术社区内对于新能源汽车的相关讨论度在急剧升高。为此，InfoQ&nbsp;于今年&nbsp;5&nbsp;月面向社区用户发起了新能源汽车调研问卷，目前已经收到了近千份问卷，最终的调研结果将在这次的周年庆系列活动公布。我们还邀请了特斯拉、理想、蔚来、问界四个热门新能源汽车品牌的车主代表，亲临&nbsp;17&nbsp;周年庆活动现场，公开反馈对问卷中各个结论的看法与评价。</p><p></p><p>17&nbsp;年来，&nbsp;InfoQ&nbsp;凭借独到的专业视角、前沿的行业报道与深度的技术内容，赢得了无数开发者们及众多业内人士的认可与持续关注。感谢&nbsp;InfoQ&nbsp;的关注者们一路相伴，一路支持！</p><p></p><p>值此周年庆之际，InfoQ&nbsp;也面向广大企业客户推出了特别福利：即日起到&nbsp;2024&nbsp;年&nbsp;6&nbsp;月&nbsp;30&nbsp;日，确认合作赞助大会的企业，会议相关产品全部享受七五折。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/5a/5ade6ae5d370424d71bd36162791e376.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/3CsQrogtkfPJ4gaEgmM8</id>
            <title>斯坦福AI团队“套壳”清华系开源大模型被实锤！被揭穿后全网删库跑路</title>
            <link>https://www.infoq.cn/article/3CsQrogtkfPJ4gaEgmM8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/3CsQrogtkfPJ4gaEgmM8</guid>
            <pubDate></pubDate>
            <updated>Mon, 03 Jun 2024 06:31:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Medium, GPT-4V, 斯坦福, MiniCPM-Llama3-V 2.5
<br>
<br>
总结: 一篇关于斯坦福团队发布的新模型Llama 3-V的文章在Medium上引起了轰动，该模型比GPT-4V等更强，尺寸小100倍，训练成本仅需500美元。然而，网友发现Llama 3-V似乎套壳了清华系开源大模型MiniCPM-Llama3-V 2.5，斯坦福团队的解释遭到质疑，事件持续发酵。斯坦福团队被迫删除质疑评论，网友提出了四点证据证明Llama 3-V套壳行为，最终斯坦福AI团队被实锤，连夜删库跑路。 </div>
                        <hr>
                    
                    <p>5月29日，一个来自斯坦福的作者团队在Medium上发布了一篇名为《Llama 3-V: Matching GPT4-V with a 100x smaller model and 500 dollars》的文章，文章中称他们训练出了一个比GPT-4V、Gemini Ultra、Claude Opus 更强的SOTA 开源多模态模型，尺寸比GPT4-V小100 倍，训练成本仅需500美元。</p><p></p><h2>斯坦福AI团队“套壳”清华系开源大模型被实锤</h2><p></p><p></p><p>该团队成员Aksh Garg也在X（原Twitter）上发贴介绍了这一模型的特点。没过多久该帖的浏览量已超过 30 万，被转发了300多次，Llama 3-V的项目一下子冲到了 HuggingFace 首页。</p><p></p><p><img src="https://static001.geekbang.org/infoq/22/22fe03b33e18660d2ad1b37e2746f61e.png" /></p><p></p><p>随着该项目热度的持续走高，不少X和 HuggingFace 上的网友注意到，Llama 3-V总是让人有种似曾相识的感觉，好像在哪里见到过！</p><p></p><p><img src="https://static001.geekbang.org/infoq/38/38d2376f8dd321f17cbbc64481fe1514.png" /></p><p>网友们接着深扒后发现， Llama 3-V似乎，有点，好像是套壳了清华系开源大模型MiniCPM-Llama3-V 2.5。</p><p></p><p>据悉，MiniCPM-Llama3-V 2.5是由清华系AI公司面壁智能推出并开源的MiniCPM 系列最新的端侧多模态模型，总参数量为 8B，支持 30+ 种语言，多模态综合性能超越 GPT-4V-1106、Gemini Pro、Claude 3、Qwen-VL-Max 等商用闭源模型，OCR 能力及指令跟随能力得到进一步提升，可精准识别难图、长图、长文本。</p><p></p><p>面对网友们的质疑，斯坦福这支AI团队也坐不住了，他们表示只是使用了 MiniCPM-Llama3-V 2.5 的tokenizer，并且宣称在 MiniCPM-Llama3-V 2.5 发布前就开始了这项工作。</p><p></p><p><img src="https://static001.geekbang.org/infoq/81/81ee9d7cc314d51baad875a7dd9255c8.png" /></p><p></p><p>但他们的解释再次遭到了质疑。</p><p></p><p>通常情况下，一款模型及其详细的tokenizer往往是在其发布后才能被外人知晓，那么斯坦福这支AI团队如何能在MiniCPM-Llama3-V 2.5发布之前就获取到这些信息？</p><p></p><p>这件事持续在网上发酵。</p><p></p><p>6月2日，不死心的网友在Llama3-V的 GitHub Issue上发布质疑，或许是因为心虚，该条质疑的评论很快就被Llama3-V团队删除。</p><p></p><p>幸运的是，发布质疑的网友早已机智地提前截图保存了自己在GitHub Issue上发布的内容。</p><p></p><p>这名网友列举了在他看来Llama3-V“套壳” MiniCPM-Llama3-V 2.5的四点证据：</p><p></p><p>证据一：模型结构和代码几乎是双胞胎兄弟。</p><p></p><p>比如，套壳的 Llama3-V 与MiniCPM-Llama3-V 2.5几乎“共用”了完全相同的模型结构和代码。Llama3-V的模型结构和配置文件与MiniCPM-Llama3-V 2.5完全相同，只是变量名不同。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b17eebf7018881bc6a38da001d5835bf.png" /></p><p></p><p>左图：MiniCPM-Llama3-V 2.5                            右图：Llama3-V</p><p></p><p>证据二：Llama3-V的代码似乎就是MiniCPM-Llama3-V 2.5的代码。更令人震惊的是，Llama3-V仅仅只是进行了一些重新格式化并把一些变量重新做了命名，比如图像切片、分词器、重采样器、数据加载等变量，下图是一些示例。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a5/a5d29d816945e3409cfd9846b1f99470.png" /></p><p></p><p>证据三：Llama3-V的作者表示他们“引用了LLaVA-UHD作为架构”，还列出了差异点（关于ViT和LLM的选择）。但是他们并没有提到，这个项目的具体实现与MiniCPM-Llama3-V 2.5 极其相似，却在空间模式等许多方面与LLaVA-UHD有非常多的差异。Llama3-V也具有与MiniCPM-Llama3V 2.5相同的分词器，包括MiniCPM-Llama3-V 2.5新定义的特殊符号。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/7a/7a23662d9623f271b1cefd8299d5e33c.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/45/458913d4dfbd98e676cf324b94e90078.png" /></p><p>证据四： 最初Llama3-V的作者在上传代码时直接导入了 MiniCPM-V 的代码，然后将名称更改为 Llama3-V。</p><p></p><p>https://huggingface.co/mustafaaljadery/llama3v/commit/3bee89259ecac051d5c3e58ab619e3fafef20ea6</p><p></p><p><img src="https://static001.geekbang.org/infoq/93/934c15b6a4a165f8ec409a859a567991.png" /></p><p></p><h2>面壁智能团队下场实锤，斯坦福AI团队连夜删库跑路</h2><p></p><p></p><p>在屡遭质疑后，斯坦福AI团队已经被逼到了不回应实在说不过去的地步了，有网友开脸贴大该项目的作者，“你们有没有勇气面对事实”？</p><p></p><p><img src="https://static001.geekbang.org/infoq/b7/b7cccb06d67ca9d15c4b3601ebc00bdd.png" /></p><p></p><p>这种情况下，该团队成员不得不对网友关注的问题进行了回复。该项目中的一位作者表示：</p><p></p><p></p><blockquote>“你们的说我们抄袭简直是没影儿的事儿。Llama3-V推理存在bug，而MiniCPM的配置可以有效解决该问题，这就是为什么我们使用了相同的配置。此外，我已经指出了架构是相似的，但MiniCPM的架构来自Idéfics。SigLIP也来自Idéfics。我们遵循Idéfics论文中的那些内容。LLava UHD来自他们的实验室，我也已经指出了这一点。此外，我还强调了更多内容，即它是相同的架构，但该架构是基于综合研究的，你怎么能说它是MiniCPM呢？MiniCPM的代码，看起来，视觉部分的也是从Idéfics那里使用的。”</blockquote><p></p><p></p><p>不少网友还注意到，Llama3-V 在 MiniCPM-Llama3-V 2.5 项目发布之前就已经使用了 MiniCPM-Llama3-V 2.5 的tokenizer 。有一些用户在 Twitter 和 HuggingFace 上指出以上问题后，Llama3-V 的作者表示他们只是使用了 MiniCPM-Llama3-V 2.5 的分词器（tokenizer）。他们还声称在 MiniCPM-Llama3-V 2.5 发布之前就开始了Llama3-V 的工作。但令人无法解释的是，他们如何能在MiniCPM-Llama3-V 2.5发布之前就获取到其详细的分词器？</p><p></p><p><img src="https://static001.geekbang.org/infoq/f1/f121ad805bec2bdf1e8c39ba36446fdc.png" /></p><p></p><p>对此，Llama3-V 项目作者反驳说从已经发布的上一代 MinicPM-V-2 项目里拿的标记器。但实际上，有网友留意到，MiniCPM-V-2的 tokenizer 与 MinicPM-Llama3-V2.5 完全不同，在Huggingface 里是两个文件。既不是同一个tokenizer件，文件大小也完全不同。MinicPM-Llama3-v2.5的 tokenizer 是 Llama3 的 tokenizer 加上 MiniCPM-V 系列模型的一些特殊 token 组成，MiniCPM-v2因为在Llama 3开源之前就发布的，不会有Llama 3 的分词器。</p><p></p><p>Llama3-V团队屡遭质疑却始终咬死不认的态度，惹怒了面壁智能MiniCPM-Llama3-V 2.5团队的研究人员们。</p><p></p><p>6月3日，面壁智能向AI前线列举了一些Llama3-V团队抄袭的“实锤”。</p><p></p><p>面壁智能认为，Llama3-V 项目的作者似乎并不完全理解 MiniCPM-Llama3-V 2.5 的架构，甚至也不理解他们自己的代码。</p><p></p><p>如下图 Llama3-V 的技术博客和代码显示， Llama3-V 的作者似乎没有完全理解 MiniCPM-Llama3-V 2.5 的架构，甚至也不懂他们"自己"（假若真是他们所写）的代码。</p><p></p><p>感知器重采样器（Perceiver resampler）是单层cross-attention，而不是双层self-attention。但是下图所示Llama3-V 的技术博客里作者的理解很明显是错的。另外SigLIP 的 Sigmoid 激活也不用于训练多模态大语言模型，而仅用于预训练 SigLIP。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8a/8a75f9e9de53acf5d793df56cae1ed45.png" /></p><p></p><p>截图来源：Llama3-V 的技术博客</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/51/512a694de9e4328876ed049242ea6623.png" /></p><p></p><p>截图来源：Llama3-V 的代码</p><p></p><p>面壁智能团队还表示：“另外视觉特征提取不需要 Sigmoid 激活，但下图所示Llama3-V 的技术博客里作者的理解是错的，但代码其实是正确的，这说明作者压根不理解自己的代码”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/42/42eeb98c7de98601c9aa8ebf7dadccdd.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d12f22ad32d7194fe7a00eefadc55d90.png" /></p><p></p><p>此外，Llama3-V相当于MiniCPM-Llama3-V 2.5的加噪声版本。</p><p></p><p>据网友反馈，当运行 Llama3-V 时，作者提供的代码无法与 HuggingFace 上的 checkpoint 配合使用。</p><p></p><p>然而令人啼笑皆非的是，当把 Llama3-V 模型权重中的变量名更改为 MiniCPM-Llama3-V 2.5 的名称后，模型可以成功运行 MiniCPM-V 的代码。这一下子帮忙解决了困扰 Llama3-V作者一周的问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/60/6054c03b78fc21f2eb5ad2a12978f68a.png" /></p><p></p><p>如果在 MiniCPM-Llama3-V 2.5 的 checkpoint 上添加一个简单的高斯噪声（由一个标量参数化），你会预期得到什么结果？</p><p>new_dict = {}</p><p>for k, v in model.state_dict().items():</p><p>torch.cuda.manual_seed_all(42)</p><p>new_dict[k] = v + torch.randn_like(v) / 708</p><p>model.load_state_dict(new_dict)</p><p></p><p>结果是会得到一个行为与 Llama3-V 极为相似的模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d1cda987e18bed0bfb1064a003ea9f6c.png" /></p><p></p><p>然而，这些还不够。更更更炸裂的是，Llama3-V 连清华团队内部并未对外公开的私有数据都能拿到？？？</p><p></p><p>据面壁智能内部团队透露，Llama3-V 大模型居然能识别清华简，OCR表现对比也很惊人，这些清华大学内部的私有数据他们又是如何拿到的呢？</p><p></p><p>MiniCPM-Llama3-V 2.5 的一个实验性功能是能够识别清华简，这是一种非常特殊且罕见的中国战国时期（公元前475年至公元前221年）写在竹简上的古文字。这些训练数据的采集和标注均有由清华NLP实验室和面壁智能团队完成，相关数据尚未对外公开。经过专有数据训练后，MiniCPM-Llama3-V 2.5 能够初步识别清华简的文字，甚至连犯的错误都一样。</p><p></p><p>然而令人惊讶的是，不可能获得专有数据训练的 Llama3-V 竟然也具有相同的能力！</p><p></p><p><img src="https://static001.geekbang.org/infoq/67/67627805e2f4889d976c1944a65d8de5.png" /></p><p></p><p>下图展示了 Llama3-V 在识别清华简的文字时，其结果和MiniCPM-Llama3-V 2.5一致。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fdacef044c8278ebed1f58329d1b8c52.png" /></p><p></p><p>有趣的是，Llama3-V 在清华简识别犯错时竟然也和 MiniCPM-Llama3-V 2.5一模一样。</p><p></p><p><img src="https://static001.geekbang.org/infoq/40/406fcf41ab5f31c53e2b59bd7e67bb1d.png" /></p><p></p><p>为谨慎起见，作者在 1000 张竹简图像上测试了几种基于 Llama3 的视觉-语言模型，并比较了每对模型的预测精确匹配。</p><p></p><p>结果显示，正常情况下每两个模型之间的重叠为零，而 Llama3-V 和 MiniCPM-Llama3-V 2.5 之间的重叠竟然达到了惊人的 87%！此外，MiniCPM-Llama3-V 2.5 和 Llama3-V 甚至在错误分布上也非常相似。Llama3-V 和 MiniCPM-Llama3-V 2.5 分别有 236 和 194 个错误预测，而其中重叠部分高达 182 个。</p><p></p><p>另外，这个高斯噪声对竹简字符也是有效的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/14/14fc6f40c674a41da3b28123999a477b.png" /></p><p></p><p>与 MiniCPM-Llama3-V 2.5 类似，Llama3-V 也显示出了很强的 OCR 能力，甚至包括中文。</p><p></p><p><img src="https://static001.geekbang.org/infoq/90/90072ebd0f7e0c45881bb9792ef0b11a.png" /></p><p></p><p>Prompt：请输出图片中的文字</p><p></p><p>MiniCPM-Llama3-V 2.5</p><p><code lang="text">### 标题
9倍像素更清晰

### 副标题
难图长图长文本精准识别

### 正文
OCR技术进一步打磨，复杂推理与多模态识别能力再进化，MiniCPM-Llama3-V2.5对于难图、长图、长文本的精准识别，再度带来出众表现！面壁自研高清图像高效编码技术，我们可以高效编码及无损识别180万高清像素图片，并且支持任意长宽比、甚至「有点变态」的1:9极限比例图像，突破了传统技术仅能识别20万像素小图的瓶颈。此前，MiniCPM-V系列多模态模型就因对于街景、长图等困难场景的高效解析，赢得了良好口碑。

### 图片描述
1. **180万像素**
   - 任意长宽比
   - 图像无损识别
   - 可支持
   - 1:9极限宽高比</code></p><p>Llama 3-V</p><p></p><p><code lang="text">### 标题
9倍像素更清晰

### 副标题
难图长图文本精准识别

### 正文
OCR技术进一步打磨，复杂推理与多模态识别能力再进化，MiniCPM-Llama3-V2.5对于难图、长文本的精准识别，再度带来出众表现。面壁自研高清图像高效编码技术，我们可以高效编码及无损识别180万高清像素图片，并且支持任意长宽比、甚至「有点变态」的1:9极限比例图像，突破了传统技术仅能识别20万像素小图的瓶颈。此前，MiniCPM-V系列多模态模型就因对于街景、长图等困难场景的高效解析，赢得了良好口碑。

### 图片描述
- **180万像素**：任意长宽比，图像无损识别，可支持。
- **1:9极限宽高比**：可支持。</code></p><p></p><p>同样的事情也发生在内部的 WebAgent 数据上，这是一个已经整合但尚未发布的功能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3e/3ebe7617cac4cbab5f0d5c68ff26825c.png" /></p><p></p><p><code lang="text">Q:
Actions History
click, input, click
Your Task
Can you give me a recipe for French butter cake?
Generate next actions to do this task.

minicpmv:
actions:
click,32 273 477 508
click,32 273 477 508

llama3v:
actions:
click,32 273 477 508
click,32 273 477 508</code></p><p></p><p><img src="https://static001.geekbang.org/infoq/31/312dd56311e0b9be740ccedf88ea9314.png" /></p><p></p><p><code lang="text">Q:
Your Task
有没有关于《黑子的篮球》的新剧场版的消息？
Generate next actions to do this task.

minicpmv:
actions:
hover,732 292 792 328

llama3v:
actions:
hover,715 292 802 328</code></p><p></p><p></p><blockquote>Github开源：<a href="https://github.com/mustafaaljadery/llama3v">https://github.com/mustafaaljadery/llama3v</a>"（已删库）HuggingFace开源：<a href="https://huggingface.co/mustafaaljadery/llama3v">https://huggingface.co/mustafaaljadery/llama3v</a>"（已删库）Medium发布文章：<a href="https://aksh-garg.medium.com/llama-3v-building-an-open-source-gpt-4v-competitor-in-under-500-7dd8f1f6c9ee">https://aksh-garg.medium.com/llama-3v-building-an-open-source-gpt-4v-competitor-in-under-500-7dd8f1f6c9ee</a>"Twitter官宣模型：<a href="https://twitter.com/AkshGarg03/status/1795545445516931355">https://twitter.com/AkshGarg03/status/1795545445516931355</a>"（已删除）</blockquote><p></p><p></p><p>事情发酵至此，就在网友们都等着斯坦福AI团队再次发文力证清白时，AI前线留意到，该团队成员似乎集体“闭麦”，并且已经删除了他们在X上官宣模型的推文，连带着该项目在Github和HuggingFace上的库也已经删干净了。</p><p></p><p>而今天，面壁智能联合创始人&amp;CEO李大海也在朋友圈发文对此事进行了最新回应，李大海对此事表示遗憾。他写道：</p><p></p><p></p><blockquote>“经过团队核实，除了社区网友列出的证据外，我们还发现 Llama3v 展现出和小钢炮一样的清华简识别能力，连做错的样例都一模一样，而这一训练数据尚未对外公开。这项工作是团队同学耗时数个月，从卷帙浩繁的清华简中一个字一个字扫描下来，并逐一进行数据标注，融合进模型中的。更加 tricky 的是，两个模型在高斯扰动验证后，在正确和错误表现方面都高度相似。技术创新不易，每一项工作都是团队夜以继日的奋斗结果，也是以有限算力对全世界技术进步与创新发展作出的真诚奉献。我们希望团队的好工作被更多人关注与认可，但不是以这种方式。我们对这件事深表遗憾！一方面感慨这也是一种受到国际团队认可的方式，另一方面也呼吁大家共建开放、合作、有信任的社区环境。一起加油合作，让世界因 AGI的到来变得更好!”</blockquote><p></p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/99/99daea9d24cdf221bbe0c7aefedfb388.png" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0xFQRzZ2xS3YxZlEESes</id>
            <title>从Data Infra“卷”到AI Infra！星环科技推出知识平台TKH，全面统筹算力、语料、模型与应用</title>
            <link>https://www.infoq.cn/article/0xFQRzZ2xS3YxZlEESes</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0xFQRzZ2xS3YxZlEESes</guid>
            <pubDate></pubDate>
            <updated>Mon, 03 Jun 2024 06:21:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 未来数据技术峰会, 人工智能, 大模型技术, AI基础设施
<br>
<br>
总结: 2024年5月31日在上海举办的未来数据技术峰会聚焦人工智能和大数据等热门话题，探讨了大模型技术在企业生产中的应用，以及企业打造自己的AI基础设施的重要性。企业需建立自己的AI基础设施，打造行业大模型，提升生产力和产业升级。星环科技推出的AI Infra工具支持企业快速建立行业大模型，实现AI应用的快速落地。通过知识平台TKH，企业可以构建安全可控的AI基础设施，帮助企业快速使用AIGC。 </div>
                        <hr>
                    
                    <p>5月31日2024向星力·未来数据技术峰会在上海隆重举办。峰会聚焦人工智能、大数据、数据要素、数字化转型、新质生产力等热门话题，业内知名专家、学者和企业代表分享最新研究成果、技术创新和实践经验，高层次产、学、研、用四方的交流和思想碰撞，为企业数据化转型和AIGC应用提供新思路、新方向和新方法。</p><p>&nbsp;</p><p>星环科技创始人、CEO孙元浩指出，大模型技术已快速融入企业生产经营的方方面面，在企业生产效率提升、业务职能提效和行业应用场景创新中无处不在，不但重构产业，打造企业的核心竞争力，而且成为企业的基础设施和核心能力，助力企业打造新质生产力。</p><p></p><h2>从Data Infra到AI Infra，为企业打造自己的AI基础设施</h2><p></p><p>&nbsp;</p><p>人工智能大模型正在催生新一轮技术创新与产业变革，也将为金融、制造、交通、政务等众多行业企业数字化转型和高质量发展带来新的动能。当前市场以通用基础大模型为主，通识能力强，但缺少行业专业知识。将大模型融入千行百业，让企业的AI应用从早期直接调用通用大模型，发展到建立自己的AI基础设施，打造行业或特定领域、任务的专用大模型，助力生产力革新和产业升级，已经成为目前企业关注的核心。</p><p>&nbsp;</p><p>星环科技创始人、CEO孙元浩宣布，星环科技的定位从Data Infra进一步延伸到AI Infra，为企业客户打造AI基础设施，打造从语料处理、模型训练、知识库建设等的一整套的工具链，帮助企业快速建立行业大模型，快速使用AIGC。</p><p>&nbsp;</p><p>从需求上看，目前，通用大模型数量快速增加，并不断升级迭代、提质增效，但是却不能适应企业AI应用的需求。而另一方面，企业对行业大模型需求增加，迫切需要降低使用门槛，更加普惠地使用大模型、生成式AI技术，以解决企业内部人才、算力、数据等不足的挑战。将AGI限制在一个特定领域或者一个行业或者特定任务，不断提升模型准确度，成为一条切实可行的发展途径。</p><p>&nbsp;</p><p>在技术上，生成式AI出现后，深度学习出现了一个重大变化，从以前的可能几千万、上亿的样本数量的大样本机器学习，演变成现在小样本机器学习，让行业大模型应用范围变得更很广，让模型能够思考、学习，能够理解自然语言，能够完成更多的工作。</p><p>&nbsp;</p><p>每个国家都需要主权AI，需要建立自己的AI基础设施，打造自己的AI模型。企业同样需要自身的AI Infra，打造行业大模型，赋能企业更快、更容易地使用大模型，释放数据价值。</p><p>&nbsp;</p><p>针对企业的AI Infra建设，星环科技通过自主研发，可以向用户提供一站式企业级大模型生产及应用全流程开发工具链，让大模型快速落地。星环科技拥有从语料到模型再到应用的完整的 AI Infra工具集，覆盖语料开发和管理、大模型训练与持续提升、多模态知识工程、多模知识存储与服务、原生AI应用构建编排和应用服务等重要阶段，提供提示词工程、检索增强、智能体构建等大模型应用快速构建和提升、模型推理优化、模型安全和持续提升技术。</p><p>&nbsp;</p><p>通过星环科技AI Infra工具，企业能够准确、高效地将拥有的多种来源的多模语料转换为高质量的专业领域知识，并且源源不断地支撑专业知识库问答、业财数据分析、智能投研、设备预测性维护等丰富的使用场景和应用，让企业构筑知识壁垒，实现“人工智能+”业务的落地和创新。</p><p>&nbsp;</p><p>星环科技AI Infra工具支持异构算力、语料、知识、大模型应用的统一管理，为数据和语料资产的集约化提供了一站式平台，且具备企业级的组织空间管理能力。</p><p>&nbsp;</p><p>未来，星环科技通过自主研发，不断完善AI从基础设施到应用的产业链条，可以为客户提供端到端的全套AI解决方案。</p><p></p><h2>推出星环知识平台TKH，为企业高效构建AI Infra</h2><p></p><p>&nbsp;</p><p>星环科技整合大数据、人工智能等技术，推出知识平台Transwarp Knowledge Hub（TKH），通过全面统管企业Al Infra算力、语料、模型和应用，为企业打造安全可控AI Infra，帮助企业快速使用AIGC。</p><p>&nbsp;</p><p>星环的知识平台TKH包括知识存储与服务、语料开发与知识构建、大模型基础服务等几个重要部分。</p><p>&nbsp;</p><p>第一，TDH for AI，打造新一代一站式多模型数字底座。</p><p>&nbsp;</p><p>星环知识平台TKH提供企业级多模态知识存储与服务，帮助企业打造新一代一站式多模型数字底座。基于Transwarp Data Hub for LLM知识管理平台的多模型统一技术架构，支持关系型数据、向量数据、全文检索、图数据、时序数据等的统一存储管理，满足各类场景下多模态数据的统一存储管理与服务，大幅简化知识库的知识存储与服务层架构，降低开发与运维成本。</p><p>&nbsp;</p><p>星环科技新推出的产品可以让企业的数据底座实现四个一体化：湖仓集一体化、多模型处理一体化、历史数据与实时数据处理一体化、本地集群和云平台一体化。</p><p>&nbsp;</p><p>其中，大数据基础平台TDH 9.4的多模基座加速AI分析；仓集资源隔离架构，在混合业务场景下依然保持极致性能；湖仓一体架构，大幅降低TCO；同份数据跑批查询与混合负载，实时备份强在线业务容灾；同时支持多模型存诸架构、大模型海量训练数据存储、多模型混合检索召回增强、Python生态等。</p><p>&nbsp;</p><p>分布式向量数据库Hippo支持文本、图片、音视频等转化后的百亿级向量数据的存储、索引和管理，支持多种索引，具有全文检索+向量检索以及稀疏向量+稠密向量的混合检索等能力。Hippo 2.0可以实现百亿级向量存储，提供灵活索引支持、20倍内存成本下降和向量全文混合检索等特性。</p><p>&nbsp;</p><p>分布式图数据库StellarDB提供万亿级图数据存储、毫秒级点边查询和10+层深度链路分析等能力，支持丰富的图算法和图机器学习，创新的动态时序图能力更便捷地挖掘数据变化规律和预测分析。而新推出的StellarDB 5.1实现了向量/全文模糊检索、秒级子图匹配、跨集群数据灾备、RAG增强大模型、GPU算法加速等功能，更稳定、更安全、更易用。</p><p>&nbsp;</p><p>星环分布式分析型数据库ArgoDB支持标准SQL语法，提供多模分析、实时数据处理、联邦计算、隐私计算、数据脱敏等能力，一站式满足OLAP分析，实时数仓、数据集市、湖仓集一体等场景。</p><p>&nbsp;</p><p>而新推出的ArgoDB 6.1 版本以“增量数据实时处理”技术为基础，定义并发布“实时数据加工”的智能高效新范式；结合集群级实时同步与数据海量版本能力，协助用户够构建高可靠的实时可信大集群，以数据透明加密、SQL审核/阻断等安全技术手段为辅，共建“快好省”湖仓集一体的融合数据处理架构。</p><p>&nbsp;</p><p>星环分布式时序数据库Timelyre支持海量时序数据库的存储与处理，具备每秒千万级数据吞吐、5~20倍无损压缩和毫秒级检索能力，支持Python、C++等API，易用的时序分析框架满足金融智能投研需求。TimeLyre 9.2新增了多模型时序分析、极速分布式回测平台、投研数据中台、时序数据湖引擎等，助力用户解锁数据深层价值。</p><p>&nbsp;</p><p>星环分布式文件系统TDFS支持10亿级以上的大小文件的存储，并同时支持对象存储，基于Raft保障强一致，支持HDFS平滑迁移，标准POSIX协议支持上层知识等AI场景应用无感对接。</p><p>&nbsp;</p><p>第二，语料加工工具与图谱构建工具，助力企业建立高质量模型及应用。</p><p>&nbsp;</p><p>决定行业大模型质量最关键的因素就是语料，语料的质量决定了模型的质量。同时高质量语料也是解决行业大模型“幻觉”、“可信可控”等核心落地难点的重要手段。另外，高质量行业专用语料是企业、机构独特的竞争优势和天然壁垒。</p><p>&nbsp;</p><p>星环科技发布了一站式多场景语料平台Transwarp Corpous Sudio（TCS），覆盖了语料获取、清洗、加工、治理、应用和管理的全生命周期，具有多种灵活的采集和构建方式，能分布式的高效处理海量语料。TCS支持20+主流文档格式、数据化学公式、复杂语料处理、语料自动标注及筛选、多视角体系化资产编目和数据治理等</p><p>&nbsp;</p><p>星环TCS拥有全面、多维、精细、增强、资产等5大优势，是一个功能全面、易用高效、安全可靠的语料开发利器，能够极大提升语料开发效率，助力企业或机构高质量地构建大模型及其应用。</p><p>&nbsp;</p><p>企业知识库的建设，让数据可以用自然语言方式进行对话和检索，企业可以集中式地管理和利用知识资源，提高运营效率和创新能力。知识库建设变成企业的基础设施，所有的不同类型的数据都能进行存储与管理，能根据需要导入到知识库中。只要企业保有自己的知识库，就可以通过微调得到企业专属的大模型，就可以实现大模型可以随时选、随时换，而企业核心竞争力得到保护的目标。</p><p>&nbsp;</p><p>企业用户利用TKH提供的星环图谱构建工具Transwarp Knowledge Studio for LLM，可以将企业内部数据、个人经验数据和公开信息数据转化为知识，让数据平台更加智能化，同时可以将AIPC端和云端资源联动，确保数据安全性。个人经验数据的知识转化和不断的模型微调让知识库建设更具个性化，真正实现个性化、专家级大模型应用。</p><p>&nbsp;</p><p>TKS是一套全流程、端到端的知识图谱构建工具集，涵盖了知识模型定义、多源异构数据接入、概念与物理数据映射、多元化知识的抽取融合、全自动知识构建、图谱综合查询等功能，能够帮助政务、工业、能源等多领域客户高效构建领域知识体系，并提供智能应用的场景定制化和一站式解决方案。</p><p>&nbsp;</p><p>第三，大模型基础服务Infinity和LLMOps，让大模型快速落地。</p><p>&nbsp;</p><p>在行业大模型发展中，企业面临技术复杂、数据和算力稀缺、管理成本高等挑战。现在企业已经意识到必须建立自己的AI基础设施，能够自己对模型进行预训练、微调等。</p><p>&nbsp;</p><p>星环科技推出大模型运营平台Transwarp&nbsp;Sophon LLMOps，提供一站式企业级大模型生产及应用全流程开发工具链，助力企业完成从预训练到微调，到强化学习，到持续模型评估的全生命周期，让每个企业都能构建自己的专属大模型。</p><p>&nbsp;</p><p>可以说，Sophon LLMOps一个平台可以解决企业在大模型时代语料、模型、应用三类资产的持续积累和加速迭代。</p><p>&nbsp;</p><p>星环科技自主研发的无涯大模型Infinity具备自主可控特性，确保数据安全的同时，通过0-1预训练，可为各行业量身定制自有大模型，提供强大的意图理解、语义召回、数据处理和分析能力。</p><p>&nbsp;</p><p>基于无涯大模型底座，星环科技微调了三款垂类大模型，包括问答大模型、数据分析（代码生成）大模型和多模态大模型，以应对内容生成、数据分析图片及音视频理解及检索等多样的使用场景。</p><p>&nbsp;</p><p>星环无涯大模型Infinity提供了灵活的部署模式，包括私有化部署（AIPC版、企业版）、公有云服务等。</p><p>&nbsp;</p><p>Infinity提升大模型数据分析能力，在语法正确性、数据库方言、语义正确性等方面有重要突破。</p><p>&nbsp;</p><p>星环无涯大模型Infinity拥有众多优势，如精准问答能力、减少大模型幻觉；多模数据来源，提升回答丰富度；构建自有知识库，确保企业数据安全等。其主要功能包括智能问答、文档问答、智能写作等。</p><p></p><h2>推出无涯·问知&nbsp;AI原生应用，服务多个垂直应用场景</h2><p></p><p>&nbsp;</p><p>基于无涯大模型，星环知识平台TKH打造了无涯·问知、无涯·问数、无涯·金融、无涯·工程等AI原生应用，可广泛应用于金融、能源、制造、工程等多个领域，通过精准的数据分析和知识管理，满足企业不同类型的知识应用需求，提升企业业务效率和竞争力。</p><p>&nbsp;</p><p>本次峰会上，星环科技最新发布了无涯·问知Inﬁnity Intelligence。无涯·问知是一款基于星环科技大模型底座，结合个人知识库、企业知识库、法律法规、财经等多种知识源的企业级垂直领域问答产品。</p><p>&nbsp;</p><p>无涯·问知充分利用了星环科技自研大模型底座的自动化知识工程特性，使其在处理和分析数据方面具有显著的优势，允许用户上传文档、表格、图片等多源数据，并支持与外部数据源的对接，使用户能够构建属于自己的专属领域大模型。这一创新功能极大地扩展了模型的应用范围和深度，用户可基于自身私域知识库进行更为个性化和深入的数据分析。</p><p>&nbsp;</p><p>无涯·问知支持不限长度的音视频图文等多模态数据快速入库，且支持自动化文档切片及向量化处理，配合自研的RAG框架，可实现知识的精准召回，可用于市场研究分析、企业供应链分析、法律风险预警、智能写作等丰富的业务场景中。</p><p>&nbsp;</p><p>无涯·问知包括四大应用场景：企业可以基于星环知识库TKH，建立企业自己的知识库应用；当企业算力不足时，可以采用安装了天涯·问知的AIPC，在本地直接访问天涯·问知，以弥补AI算力不足问题；中小企业用户不用自己构建知识库，可以直接利用星环科技无涯·问知公有云服务；对于个人而言，可以利用AIPC或者公有云服务，访问天涯·问知服务。</p><p>&nbsp;</p><p>无涯·问知AIPC版在实际应用中展现出了五大显著特性，提供了本地化的向量库；支持多种格式、不限长度的文件资料入库，满足了用户多样化的需求；支持影、音、图、文等多模态数据和资料的“知识化”处理，以及相应的“语义化”查询和应用能；自研的RAG模块，实现精准问答；具备出色的数据分析能力，能够对数量化的数据进行精准的分析和研判。</p><p>&nbsp;</p><p>另外，无涯·问数是基于星环数据分析大模型，并结合数据分析主体、指标、标签设计、数据开发和治理，形成了从自然语言转数据查询语言，并返回数据表或数据图表的完整流程。</p><p>&nbsp;</p><p>在应用场景上，无涯·问数提供分析仪表盘和智能问数能力，让决策者/管理者以自然语言提问快速自助获取目标数据；预定义指标计算口径，依托数据分析大模型理解用户的分析意图，让数据分析人员实现对话即分析；通过页面配置的方式快速完成数据准备，让数据开发人员，快速整合多种数据。</p><p></p><h2>Data Infra持续深化，星环系列产品推陈出新</h2><p></p><p>&nbsp;</p><p>星环科技在推出全新的AI Infra的同时，不断完善Data Infra产品与服务。</p><p>&nbsp;</p><p>星环大数据云平台推出TDC 5.0，将原来的多个TDH集群统一纳管，统管多个TDH集群，形成物理上分散、逻辑上统一的企业级一体化大数据平台。TDC 5.0具有独特的优势，包括多集群及其基础设施、多数据应用实例统一管理；跨多集群统一调度资源，均衡多个集群资源使用；跨集群共享存储组件，实现NoCopy的跨集群数据共享；隔离和控制资源配额，快速、灵活的为不同业务部门提供多租户的PaaS服务。</p><p>&nbsp;</p><p>星环科技推出了分布式交易型数据库KunDB 4.0，高可用能力与Oracle兼容性提升，支持跨系统多租户部署。其中，深度兼容Oracle，高度兼容Oracle对象与语法，支持数据快速迁移；高可靠，基于Paxos协议的异地容灾能力，增强数据安全性保障；数据库多租户，支持多个应用共享一个数据库实例，能快速由集中式扩展成为分布式。</p><p>&nbsp;</p><p>星环大数据开发工具TDS 4.0，增加了数据实时同步、数据入湖向导、智能化数据资产盘点、数据资产门户、数据服务编排等功能。</p><p>&nbsp;</p><p>星环大数据安全与隐私保护工具软件Transwarp Defensor是星环科技自主研发的大数据安全与隐私保护安全管理平台，致力于帮助企业建设以数据为中心的数据安全防护体系，包括了解内部敏感数据分布情况，帮助管理者发现潜在风险，监管重要数据的合规合理使用等。Transwarp Defensor 提供数据分类分级管理、数据脱敏、个人信息去标识化、数据访问控制、敏感资产风险评估等基础能力，能够做到事前发现，事中防护，事后溯源，帮助企业有效建立数据安全防护体系。Transwarp Defensor 4.5，增加了大模型核心资产识别、数据资产流转链路监控、安全策略智能推荐，安全风险预警与应急响应。</p><p>&nbsp;</p><p>星环数据要素流通平台Transwarp Navier通过提供隐私计算环境，使得数据供需双方可以进行安全的数据交易。而Transwarp Navier 3.1则新增了全链路智能合约确保安全合规、数据流通全链路行为监控与分析、实时告警与阻断等。</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/695b4d28832c9e551efa7cc6c</id>
            <title>关于Vearch在大模型中使用的一些实践</title>
            <link>https://www.infoq.cn/article/695b4d28832c9e551efa7cc6c</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/695b4d28832c9e551efa7cc6c</guid>
            <pubDate></pubDate>
            <updated>Mon, 03 Jun 2024 02:26:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 向量库, Vearch, Gamma
<br>
<br>
总结: 近年来大模型应用广泛，引发了向量库的热潮，Vearch作为一款开源向量库，集成了Gamma引擎，支持分布式最邻近搜索，为大模型提供了高效的向量存储和检索功能。 </div>
                        <hr>
                    
                    <p></p><h2>背景</h2><p></p><p>这两年来大模型及其热门，不仅各大厂家的模型层出不穷，各类RGA、Agent应用也花样繁多。这也带火了一批基础设施，比如Langchain、向量数据库（也叫矢量数据库-Vector Database）等。现在市场上的向量库种类特别繁多，但主要还是分为两类，一类是在原有数据库基础上增加了向量相似性检索的能力，比如ES、Redis等等；还有一类就是生而为向量库，比较有名的比如Qdrant、Pinecone等等。最近我们在开发一个基于大模型的测试用例生成的应用，检索采用向量库+知识图谱的混合检索方式。在调研向量数据库时，发现公司有类似的产品，也就是Vearch。并且这款产品开源比较早，也比较成熟，目前已集成到Langchain框架中，所以我们也采用了集团部署的Vearch。这样可以免去部署的烦恼，最重要的是不需要我们去找GPU资源😩（向量计算GPU更为高效，Vearch也支持CPU计算）。</p><p></p><h2>浅引</h2><p></p><p>Vearch诞生之初，主要是用在深度学习上，可以进行海量数据的近似检索。后来大模型开始流行，问答机器人此类应用变得广泛。使用时，只要将问题&amp;答案或者一段文本录入库中，通过检索问题文本向量，即可返回近似文本内容。关于Vearch的底层架构及原理，这里不再讲述（之前Vearch的元老在神灯已经写了很多文章了，感兴趣的可自行搜索）。Vearch的核心存储及检索引擎是Gamma（哎？看到这个是不是很眼熟哇，那个模型叫Gemma，不是一个东西哈），主要负责向量的存储，索引与检索。它是基于Faiss（脸书的向量聚类库）中的SIMD指令实现（这是Vearch的先驱们写的原始论文 <a href="https://arxiv.org/abs/1908.07389">https://arxiv.org/abs/1908.07389</a>"）。相比于Faiss，Vearch最大的能力是在于对分布式的支持，所用的算法称为分布式最邻近搜索，它在原始KNN基础上集成了更多的能力（可参考： <a href="https://towriting.com/blog/2021/10/07/vearch/">https://towriting.com/blog/2021/10/07/vearch/ </a>"）。关于更底层使用的余弦计算、KNN（最邻近搜索）等概念，就不在本篇范围内了。</p><p></p><p>这里再简单说两句，大模型为什么要用向量检索库（一个点铺开都有很多内容😢）。最近有一种项目比较流行，就是知识库，它是基于大模型能力的一种扩展。因为目前大模型存在两个问题，其一对于专属领域的知识理解不好，容易幻觉举个栗子，你问GPT关于京东营销中心的业务，它就只能自己捏造了；其二，它的训练数据存在滞后性，做不到对最新知识的理解。当然你要是有资源，也可以用这些知识去对模型进行微调，但对于我们这种一点资源没有的用户来说，这不是最优解。所以聪明的人们就整出了一套叫RAG的技术（检索增强生成），通过预检索提前录入的向量化知识，将检索的内容嵌入到问题的提示词模板中，这样传给大模型后，以更准确的获取答案。这里又涉及到一个概念了-嵌入（Embedding），这里不细说了，因为涉及到向量化，后面会简单说明，本篇主要还是讲Vearch的使用。</p><p></p><h2>项目简述</h2><p></p><p>大模型火热了这么久，我们也在积极研究如何应用到测试上，并给测试提效。结合我们营销中心、策略等系统繁杂业务的特性，希望有一种可以一键输出测试用例的能力，这样可以帮助我们节省用例编写的时间。而这之前，我们先需要完成知识库（业务知识、测试经验、系统上下）的建立，包括知识文档的整理、知识的录入与检索，知识图谱的创建等等。有了RGA，我们就可以根据知识库以及给定的需求文档，进行测试用例生成。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d4/d422013f1aeffe51c42c8320571c3b7a.png" /></p><p></p><p>目前我们的项目还在开发中，所以本篇的目的在于借助该项目介绍其中的一块--向量数据库Vearch的使用。</p><p></p><h2>集群库创建</h2><p></p><p>Vearch架构，它由三部分组成Master、Router、Ps。Master负责集群元数据的管理和资源的协调分配；Router负责请求路由转发及结果合并；Ps用于存储和检索向量。Vearch目前已经集成到泰山，在使用前需要先申请一个集群库，从集群列表中可以看到，每个部分默认有三台机器，确保高可用。</p><p></p><p>然后它有两个地址，一个Master地址，一个Router地址。</p><p></p><p>Master地址：用于库表维度的创建、删除等</p><p></p><p>Router地址：用于数据维度的插入、删除、检索等，不过现在也已兼容旧版本的Master，所以表操作直接用Router也没问题。</p><p></p><p>需要知道的是，目前Vearch暂未支持表结构修改能力，但新版本已经在支持中。</p><p></p><h2>功能导入</h2><p></p><p>因为我们的项目使用Langchain开发，而且Vearch以及集成进去了，所以刚开始我们使用的就是Langchain的版本</p><p></p><p><code lang="text">from langchain_community.vectorstores.vearch import Vearch
</code></p><p></p><p>这个版本比最新开源的SDK版本略旧，只支持固定字段，但是最新的开源版本已经兼容多字典。跟Vearch的同学讨论后，决定还是直接导入Github上开源的最新SDK，将下面这个文件内容复制到项目工程目录下就行。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a8/a87e23ea04d4725ff39b1872e150eeb9.png" /></p><p></p><p>然后从本地的Vearch文件引用即可。</p><p></p><p><code lang="text">from ..(路径)/Vearch文件名 import Vearch
</code></p><p></p><h2>表空间创建</h2><p></p><p>Vearch将操作全都封装成了接口，所以并不需要写类似SQL那样的语句。我们现在要创建一个名为delta_llm_embedding的表，可以通过两种方式：</p><p></p><h4>自定义建表</h4><p></p><p>可以在泰山的操控台进行操作，或在你的本地dos/MACshell的中控台进行操作。这里不分别截图演示了，毕竟不是写操作手册。本地建表语句如下：</p><p></p><p><code lang="text">curl -XPUT -H "content-type: application/json" -d'
{
    "name": "delta_llm_embedding",
    "partition_num": 3,
    "replica_num": 3,
     "engine": {
        "index_size": 1,
        "metric_type": "InnerProduct",
        "retrieval_type": "HNSW",
        "retrieval_param": {
          "nlinks": 32,
          "metric_type": "InnerProduct",
          "efSearch": 64,
          "efConstruction": 160
        }
},
    "properties": {
        "text": {
            "type": "string",
            "index": true
        },
        "text_embedding": {
            "dimension": 1536,
            "type": "vector",
            "store_param": {
                "cache_size": 2048,
                "compress": {"rate":16}
            }
        }
    }
}
' http://master_server(Master地址)/space/db/_create
</code></p><p></p><p>上面的字段意思官方文档里都有（<a href="https://vearch.readthedocs.io/zh-cn/latest/">https://vearch.readthedocs.io/zh-cn/latest/</a>"），这里就不一一解释了。需要重点说的是两点，一点是字段名，我们定义了两个字段，一个是文本字段text，一个是向量字段text_embedding。至于为什么用这两个名称也是踩过的坑，才知道有多深，稍后说。另外一点就是dimension，这个是向量维度，理论上来讲维度越高，召回越精准。但这个数据完全依赖于你的嵌入模型所能输出的维数，因为我们用的Openai的Embeddings模型：text-embedding-ada-002，看下图</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/91/91459b1e284590efcaa4f93010ad8349.png" /></p><p></p><p>我们再说下为什么那两个字段名（文本字段text，向量字text_embedding）要这么取，先看下Vearch的SDK源码，在插入数据的那段是这么写的：</p><p></p><p><code lang="text">...
...
for text, metadata, embed inzip(texts, metadatas, embeddings):
     profiles:dict[str, Any]={}
     profiles["text"]= text
     for f in meta_field_list:
         profiles[f]= metadata[f]
     embed_np = np.array(embed)
     profiles["text_embedding"]={
         "feature":(embed_np / np.linalg.norm(embed_np)).tolist()
     }
     insert_res = self.vearch.insert_one(
         self.using_db_name, self.using_table_name, profiles
     )
...
</code></p><p></p><p>可以看到两个字段名是写死的，这也是一开始我们建表的字段名跟这个并不一致，导致我们花了不少时间来跟Vearch的同学排查问题的原因。至于SDK里为什么会这么写，原因是基于已有Vearch SDK以及Langchain的规定，必要字段被指定，其他字段可根据用户需要自适应设置。</p><p></p><h4>vearch_cluster初始化</h4><p></p><p>如果你一开始不去自行建表，而是直接在工程代码中使用SDK的初始化能力，则可以采用该种方式，这种方式你并不需要关注表结构，vearch_cluster会自行初始化一个表，你只需要告诉它表名即可。</p><p></p><p><code lang="text"> vearch_cluster = Vearch.from_documents( 
     texts,
     OpenAIEmbeddings(
         model="text-embedding-ada-002",
         openai_api_key=OPENAI_API_KEY,
         openai_api_base=OPENAI_API_BASE
          ),
      path_or_url="http://router_server(Router地址)",
     table_name="delta_llm_embedding",
     db_name="db",
     flag=1
 )
</code></p><p></p><p>如果自行建表，我们可以任意建字段，想存什么就存什么，但是带来的问题就是没法使用vearch_cluster进行初始化检索，除非你的字段是一致的。如果你预建好了表，字段也正确，那么vearch_cluster在初始化时，它会跳过建表这一步，直接插入数据，或进行检索。至于vearch_cluster初始化时，建表字段为什么只有这两个，那是因为早期这个SDK是给问答机器人使用的，那时候并没有考虑到后面在其他地方的使用，当然现在新版本已经在路上了。好了，创建就说这么多吧，至于删除，可以自行查询官方文档，进行操作。</p><p></p><h2>数据存储</h2><p></p><p>现在我们知道建表时有两个字段，一个是文本字段，一个是Embedding字段，也就是向量数据字段。在浅引里已经说过，Vearch本身只提供存储及检索能力，所以向量化是需要依赖外部工具来完成的。过程就是：文本块&gt;Embedding&gt;存储&gt;检索。当然这之前，我们需要对文档进行切割，将文档按照一定的规则切割成文本块。这部分本篇就不细说了，不然就跑题了。</p><p></p><p>我们项目使用的是Openai的Embeddings模型，从上面的vearch_cluster初始化对象中可以看出，我们使用的模型是Openai的text-embedding-ada-002。在初始化时，它会调用OpenAIEmbeddings这个类完成对texts文本的向量化，并进行建表落库。OpenAIEmbeddings是Langchain集成的Openai调用的能力。当然你也可以不使用vearch_cluster的from_documents方法，自己定义一个向量化方法，当然这会带来额外的工作量，需要自己写数据落库的方法（调Vearch的相关接口）。</p><p></p><p><code lang="text">#自定义向量化方法 
 defchunk_embeddings(chunk):
     embeddings = OpenAIEmbeddings(
         model="text-embedding-ada-002",
         openai_api_key=OPENAI_API_KEY,
         openai_api_base=OPENAI_API_BASE
      )
     query_result = embeddings.embed_query(chunk)
     return query_result
</code></p><p></p><p>比如，现在有一小段文本：营销中心， 将它嵌入后Openai的Embeddings模型会返回一个长度是1536的数组：</p><p></p><p><code lang="text">[-0.0056166304,-0.019372676,0.0055335015,-0.018635359,-0.013734359,0.019025704,-0.026355514,-0.012151293,-0.0065816496,-0.008652647,0.0049299123,0.023261668,-0.0033992538,-0.015599342,-0.014319877,0.0019336534,...,-0.0068238084]
</code></p><p></p><p>然后调用Vearch的数据写入接口（这里如果使用SDK的初始化，就不用自己写存储了，vearch_cluster都帮你做了）。落库时「营销中心」写入text字段，数组写入text_embedding。这里需要注意的是，数组的长度要跟建表时的维度一致，否则数据写入失败。如果写入成功，我们会收到这样的返回：</p><p></p><p><code lang="text">{
  "_index": "db",
  "_type": "delta_llm_embedding",
  "_id": "-3705827531945023546",
  "status": 200
}
</code></p><p></p><p>这里的_id，就是记录的唯一id了，我们可以通过它检索到这条详细的数据。在中控台里执行这个接口：</p><p></p><p><code lang="text">get  http://router_server/db/delta_llm_embedding/-3705827531945023546

#返回数据
{
  "_index": "db",
  "_type": "delta_llm_embedding",
  "_id": "-3705827531945023546",
  "found": true,
  "_source": {
    "_id": "-3705827531945023546",
    "text_embedding": {
      "feature": [-0.0056166304,-0.019372676,0.0055335015,-0.018635359,-0.013734359,0.019025704,-0.026355514,-0.012151293,-0.0065816496,-0.008652647,0.0049299123,0.023261668,-0.0033992538,-0.015599342,-0.014319877,0.0019336534,...,-0.0068238084]
      "source": ""
    },
    "text": "营销中心"
  }
}
</code></p><p></p><p>这里简单拓展下Embedding，翻译过来是嵌入，或者词嵌入。这是个有点让人头秃的名词。之前有人说过“没有思考过 Embedding，不足以谈 AI”，可见它在AI领域的重要性。当然如果不理解，也不影响我们的使用。</p><p></p><p>关于这个词，维基的解释是指把一个维数为所有词的数量的高维空间嵌入到一个维数低得多的连续向量空间中，每个单词或词组被映射为实数域上的向量。</p><p></p><p>好吧，更不好理解了。这句话的核心是将高维的对象映射到低维的空间，Embedding就是将一个离散的词映射成一个N维的数组向量，这组向量表示的是一个连续的数值空间中的点，也就是空间位置。说白点，就是将自然词通过向量化，嵌入到计算机的语言模型中，这个模型是它对人类语言对象的理解。鄙人能力有限，就不做继续深入了。</p><p></p><p>人工智能的奥义是「万物皆可Embedding」，不管是图片、文本、对象还是什么，都可以嵌入。这个技术它主要用在机器学习以及自然语言处理中，因为自然语言语意复杂多变，要想让计算机理解人类语言却不是一件简单的事。词与词之间的相关性通过距离进行量化，我们可以想象一堆词语，计算机在空间维度上对其进行聚类，词义相近的放一起，不相近的远离，所有词语之间都有一个方向和距离。最后检索时，通过这些向量进行余弦计算，得出排序分数。</p><p></p><h2>数据检索</h2><p></p><p>现在我们将文档「营销运营策略业务全景介绍」录入知识库中，我们需要根据问题进行内容检索。Vearch目前支持两种检索方式，文本检索、向量检索。</p><p></p><h4>文本检索</h4><p></p><p>这里我们使用了官方提供的similarity_search方法进行检索</p><p></p><p><code lang="text">question ="营销运营策略业务包括哪些？"#检索问题内容
cluster_res = vearch_cluster.similarity_search(query=question, k=1)
</code></p><p></p><p>我们看下这个方法，通过传入需要检索的问题内容，先对其进行向量化，然后再调了向量化的检索方法similarity_search_by_vector，通过计算向量的余弦值，对文本块做一个整体的排序，然后召回前k个文本。</p><p></p><p><code lang="text">  def similarity_search(
        self,
        query:str,
        k:int= DEFAULT_TOPN,
        **kwargs: Any,
    )-&gt; List[Document]:
        """
        Return docs most similar to query.

        """
        if self.embedding_func isNone:
            raise ValueError("embedding_func is None!!!")
        embeddings = self.embedding_func.embed_query(query)
        docs = self.similarity_search_by_vector(embeddings, k)
        return docs
</code></p><p></p><p>因为每次检索时，都是一次余弦计算，而维度越多，则计算消耗越大。</p><p></p><p>检索后，获得了如下内容：</p><p></p><p>营销运营策略是基于商品、券、内容等多维度的调控策略平台，支持各种营销活动。目前主要包含商品调控策略、权益调控策略、百补风控策略和内容调控策略...</p><p></p><p>注意这里检索的内容只是从Vearch中检索到的内容，并非大模型返回的内容。我们拿到这个检索的文本后，还需要将它嵌到提示词模板中，进行模板格式化后传给大模型，达到对大模型进行知识补充的目的。</p><p></p><h4>向量检索</h4><p></p><p>向量检索调用，跟上面的文本检索就类似了，只是使用的方法是similarity_search_by_vector，这也是上面文本检索中所调用的方法，当然参数也就变成了向量化的文本。</p><p></p><p><code lang="text">question_embedding =[0.04036456,-0.0073514967,0.026557915,-0.0005227189...]
cluster_res = vearch_cluster.similarity_search_by_vector(query=question_embedding, k=1)
</code></p><p></p><p>关于这个方法，这边就不细说了，属于Vearch的核心能力，想研究的可以看源代码，自行查询（<a href="https://github.com/vearch/vearch/blob/master/sdk/integrations/langchain/vearch.py">https://github.com/vearch/vearch/blob/master/sdk/integrations/langchain/vearch.py</a>"）。</p><p></p><h2>总结</h2><p></p><p>以上主要就是我们在该项目中所使用的Vearch主要能力，即存储跟检索。从整体使用感受来讲，功能基本满足要求，性能也不差，上手难度也不高，关键还开源。但是相比于外面已经完全实现平台化的商业产品来说，因为Vearch所有的能力都是接口封装的，写数查询这些日常操作略有麻烦，对于咱们用惯了关系数据库的新手来说稍有不适。</p><p></p><p>另外在调研Vearch时，发现已有不少业务团队将Vearch用于业务能力上，比如推荐、查重等。虽然目前营销中心的业务模块并未用到该产品，但对于我们来说是一种知识与技能拓展，并对后续的测试给出新的视野和思路。</p><p></p><p>作者：技术质量 倪绍峰</p><p>来源：京东零售技术 转载请注明来源</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/4bIxeqeawdDX2e5wpPAM</id>
            <title>527 蚂蚁技术日 | 蚂蚁集团 CTO 何征宇答记者问，精彩回答十连！</title>
            <link>https://www.infoq.cn/article/4bIxeqeawdDX2e5wpPAM</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/4bIxeqeawdDX2e5wpPAM</guid>
            <pubDate></pubDate>
            <updated>Mon, 03 Jun 2024 01:56:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 蚂蚁技术日, AI生活助理, 隐私计算, AI编程
<br>
<br>
总结: 蚂蚁集团每年举办技术日活动，展示AI创新应用产品，强调AI生活助理和隐私计算的重要性，推行AI编程以提高效率和改变软件开发模式。 </div>
                        <hr>
                    
                    <p>每年的 5 月 27 日是蚂蚁集团的技术日，意在勉励蚂蚁技术人保持敬畏和创新之心。至今年，技术日已发展为技术周，一场涵盖技术论坛、技术集市、编程大赛、技术沙龙等活动的技术嘉年华。</p><p>&nbsp;</p><p>5 月 28 日，第九届蚂蚁技术日对外开放，开放日上展示了诸多蚂蚁AI创新应用产品，向外界透传了“让AI像扫码支付一样便利每个人的生活”的技术主张和面貌。</p><p>&nbsp;</p><p>在技术日第一天上午的 CTO 面对面环节，蚂蚁集团 CTO 何征宇回答了几十家媒体的提问，以下是精华内容整理：</p><p></p><p>Q1：刚才您提到“人工智能生活助理”。以后生活助理界面有没有可能更细化，比如说旅行助理、美食助理？把所有的助理放在一起，要选、要点、要对话的轮次会很多。</p><p>&nbsp;</p><p>A：我们本质上就是有 N 多个小助手，但是我们认为这些助手不应是让你来选，而是让生活管家就能够理解你的意图。比如说，当我说我要去南京，我想吃点好吃的，它瞬间就是两个助手的化身，它可以帮你安排行程和吃饭的地方。所以我同意你的观点，应该是若干个助手，最终让你的生活更美好，所以我们姑且把它叫做生活管家，而且支付宝我们认为就是这么一个事情。</p><p></p><p></p><p>Q2：从去年开始，国家数据局成立，到今年，数据局已经在很多地方落地，隐私计算在这个过程中有没有发挥作用？</p><p></p><p>A：首先我想讲，国家数据局的数字中国公布了一些案例，我们是其中之一，是我们跟农业农村部合作做的农户的小额贷。我认为国家数据局讲的战略，或者数据基础设施是一个非常宏大和庞大的体系。蚂蚁也就聚焦于最关键的，我们认为最难的地方，对我们来说就是隐私计算。</p><p>&nbsp;</p><p>隐私计算不是解决确权的问题，在我们的愿景中，我们认为它就是一个管道技术，我做的这个管道比别人更便宜，质量更好——不是一个石头做的，而是一个钛合金做的管子，非常安全，不用担心被泄露，又能非常快速的能把这个数据给传过去。其实我用这个比方是在讲，我们隐私计算就是这么一个技术，但是你要是真的要从确权、流转等等到最终的消费，这是一个非常庞大的技术体系，我觉得应该是全社会、全行业都来参与这个建设的。</p><p>&nbsp;</p><p>蚂蚁在隐私计算方面的投入相对国内来说相对早，然后我们自己有使用的场景。简单讲，因为炼油的技术高，所以油要得多的话，我们需要有很好的管道技术，我们今天愿意把管道技术开放给社会，以商业化和开源的方式开放给社会，能够共建数据流转或者数据基础设施这些东西。</p><p>&nbsp;</p><p></p><p>Q3：您刚刚提到蚂蚁内部现在全面推行 AI 编程，每周差不多超 5 成程序员在使用 CodeFuse。在这个过程中软件的开发模式相交之前是不是发生了一些变化？如果说程序员能够从繁琐、枯燥的工作中释放出来的话，那市场上对程序员的需求是不是发生变化。以前有一些采访对象跟我说过，未来程序员会和产品经理进行合并，你觉得会有这样的趋势吗？会有人才焦虑吗？</p><p>&nbsp;</p><p>A：第一点，我不觉得写代码是一个很枯燥的事情。我不知道在座有多少人写过代码，我现在不写代码了，但是我几年前写代码的时候，我是觉得对于程序员、至少对我来说是可以写到脑嗨的那种状态的，远比我现在的工作回馈的环路要短。我每天把这个代码交上去了就很开心，跟玩乐高的感觉差不多，所以我不觉得写代码是一个很枯燥的工作。</p><p>&nbsp;</p><p>第二点，我们今天在推行AI编程。是因为我们觉得就算你玩乐高，有个人在帮你提高效率，有个说明书放在旁边，我觉得是可以的。所以，今天AI编程它只是一个 Copilot，只是它针对不同的人有不同的 Copilot，因为我跟国外的一些公司也交流过，对于高级别的程序员，他更需要的是，那些很简单的代码他不想写了，有些东西他调一个库就搞定了，这个程序员一直这么干的。对于一般一点的程序员，他是需要告诉他一些经典的写法是什么？一些经常犯错的，比如说编码规范的东西，他需要一些提醒。像您刚才说对于产品经理，他不想写代码的人，他就需要一个端到端的东西。所以，我认为针对不同的人，需求是不一样的。我们不能简单的讲，有了 CodeFuse 就不需要程序员了，我认为这是人为创造的一种焦虑。</p><p>&nbsp;</p><p>最后我想回答的是，AI 对于我们今天所有的软件工程的生产范式是有一些变化的。蚂蚁中间件的负责在去年就跟我说过，如果 AI 都会写代码了，是不是未来我最重要的工作就是让AI理解我的中间件？中间件就是我们在写程序的时候最基础的那层东西，蚂蚁的中间件叫 Sofa，它是给程序员用来编程的，这还是一个比较专业的事情。它是支持你的 Copilot，甚至是你的代码的导师，你去让它理解中间件。所以，我们的工作确实会发生一些变化，但是它的本质还是在 AI 这个新的生产力加入进来后，我们重新编排我们的代码流程，还是这么一个事。</p><p>&nbsp;</p><p>&nbsp;</p><p>Q4：您刚刚提到数据孤岛的问题，我觉得现在 AI 也存在这样一个问题，比如说各家手机厂商自己也有 AI 助手，如果你想要直接去问助手的话，用 GPT 或者用其他的。但当我真正需要生活服务的时候，我必须点开支付宝再去问，这样一个流程我能直接通过手机解决，而不用打开支付宝。</p><p>&nbsp;</p><p>A：简单来讲，此助手非彼助手，世界上的助手有千千万，但是每个人的助手能力是不一样的。我们今天更重要的是提升我们自己助手的能力，支付宝今天是有一些独特的优势的，不是说我们技术有多牛，而是说我们支付宝今天整个开放生态的能量。</p><p>&nbsp;</p><p>你真的要助手帮你去做件事情，它至少得有个服务商接口吧。当然，面向未来来看，我们希望我们真的能做到跟扫码支付一样，现在很多手机会绑定一些默认的扫码，有的就是支付宝。当我们做到那个量级的时候，我认为很多厂商会跟进的。核心是我们的产品能力是不是真的能便利每个人？我认为这个是关键。</p><p>&nbsp;</p><p></p><p>Q5：刚才您在整个框架蓝图里面，从表现层-结构层-战略层的框架讲了很多东西，您有提到表现层是一些偏硬件的交互，比如里面像具生智能之类的新的交互方式。我想问一下，蚂蚁这么高的视角，您怎么看硬件交互新形式的节奏，您觉得下一个要出现的是什么？再下一个是什么？有没有具体的地图。</p><p>&nbsp;</p><p>A：关于技术跟硬件结合相关的。我觉得我们是在做探索，是有一些想法，但也不是特别成熟。大家都是言必谈软硬件结合的产品，我认为软硬件结合目前不会是一个很成功的产品，这不是今天我说的，是很早之前我就说了，但是不代表他未来不会是。从整个科技来讲，必须要有一个先修桥，先修路的过程。你先得有一个基建的过程，包括今天的 AI 大模型。今天企业投了这么多钱、国家投了那么多钱为什么没有爆发点？但是试想一下，中国如果当年没有 3G、4G，今天也不会有移动互联网，这就是一个基建的过程。硬件的逻辑也是一样的，我认为硬件是基建，它是能够把更好的体验、更好的服务能够带给每个人的。但是不应该它想象成它马上能够产生一个巨大的消费市场，这种例子我认为还是比较鲜有的。即便是苹果这么厉害的世界头号的硬件公司，它最终能够产生真正的用户黏性，或者说能够提供最终的用户价值的，其实是上面的软件，以及它所有的生态里面提供的服务。所以，我们认为硬件只是一个基础，最终能够通过这个硬件、通过上面的叠加的软件、算法能够提供什么样的服务，我觉得是关键。</p><p>&nbsp;</p><p>再回到跟蚂蚁结合的问题上来，我们虽然想星辰大海，但并不是什么都可以做。我们还是要聚焦到我们擅长的一些领域。比如在医疗领域，我们觉得有相应的机会。今天医疗健康这个大的赛道，其实你靠一个简单的手机跟你交互，我认为是非常浅的。因为它的 sensor 是不够多的，今天的多模态还是语音、图像这些东西，比如中医他具备望闻问切这一系列交互，更别说西医所有的一切了。所以在这方面的 sensor 或者多模态 sensor 数据融合上，这些硬件我认为也是有非常大的前景的。所以说结合行业，结合刚才讲的硬件的定位是基建，最后提供什么样的服务，这个东西是我们觉得是可以去探索的。</p><p>&nbsp;</p><p>&nbsp;</p><p>Q6：刚才您提到从擅长的事出发去做思考，这个决策过程是怎样的？不光是蚂蚁，在 AI 时代能成功的应用，它有些什么样的特征？它什么时候会出现？为什么是你们能做到这件事？ 支付宝流量这么大的平台做 AI 功能，怎么平衡 AI 创新和它的风险？</p><p>&nbsp;</p><p>A：我先解释一下，我们主要的还是技术，不建议把它作为业务的考量，我们整个思考和逻辑的范围还是聚焦在技术里面。</p><p>&nbsp;</p><p>技术带来什么样的变革？我们希望给世界带来微小而美好的变化。马斯克带大家登上火星，我觉得很伟大，但是能够让地球上所有人过上美好的生活，我认为同样伟大。今天你在看我们的所有支付宝从事的行业，十年前我们开始搞金融行业就在讲，让每个人能够享受到银行行长的待遇。之前在美国的时候我读过一本书《Bank4.0》，里面有一个章节就讲支付宝的。其中最核心的是，之前的银行都是要关门的，你要去银行取个钱、办个事你得请个假。但是今天因为数字化，因为我们的技术，今天有了 7×24 小时的银行。我觉得这就是我们带来的变化，蚂蚁也是通过二维码，二维码这个技术也不是支付宝发明的，但是扫码支付确实在中国我们是第一个推出来的。这个技术本身是没有问题的，它就是一个技术而已，但是你找到一个合适的场景的结合，然后把它变成一个普惠的服务，这是支付宝最擅长的。所以，我们今天看AI也是这个逻辑，我们今天不是说要做一个普惠的技术的提供者，而是我们如何用一个最好的技术，做一个最普惠的服务的提供商。</p><p>&nbsp;</p><p>AI 今天能解决的问题是什么？我能够把服务推荐给你，我有个AI助手，它知道你需要什么服务，然后知道支付宝有什么好的服务，然后把这个服务推向给你，这就是我们想做的事情。</p><p>&nbsp;</p><p>金融这个领域不用讲了，有一个词叫“Financialhealth”，你肯定是希望你的金融是安全的、是健康的。我们所谓的金融助手、金融管家也是要帮助大家的，我们发现很多人特别是很多初级的投资者，股票一下跌就会卖。这时候AI稍微跟他聊一聊，情绪稍微稳定一点，股票就不会卖了。因为很多人金融的决策不是理性的，而是情绪的，或者是道听途说有一些消息，有一些紧张情绪，就会做一个对他整体持仓不好的操作。医疗也是一样的。我们对风险的零容忍，其实是我们对自己有一个非常高的坝，我们觉得我们推出的服务、产品、技术，应该是经得起足够大规模的考验，以及足够挑剔的人群的考验的。所以，我们觉得在金融和医疗这里，是民生，是对每个人都重要的领域，值得我们真正去投入做，而且这也是对我们技术一个非常大的挑战，放眼整个行业来看，说的骄傲一点，如果我们不出手谁出手呢？我们觉得在这波技术的变革之下，我们应该可以去做这个行业的。</p><p>&nbsp;</p><p>AI 创新和风险是要平衡的，一个是我们非常注重本身的科技伦理，技术是双刃剑，它有两面性。但是我们如何把握好这个技术，真正的让它科技向善，而不是去作恶。所以，我们有两道（关卡），一个是科技伦理委员会，每年我必须参加。在大模型这个领域我们也投了很多资源，蚂蚁百灵大模型接近 20% 的人是在做安全性，不能出现一些有伤社会伦理的事情；第二个是在风控与合规技术。这块我们的投入也非常大，这两个东西是一个枷锁，某种意义上有点像汽车的安全带，你想开快的时候一定要把安全带绑好了。但是安全带你能说他是枷锁吗？他是，但是某种意义上是能够让你上高速开的更快，我们就是这么去理解风险和创新的关系的。</p><p>&nbsp;</p><p>&nbsp;</p><p>Q7：原生多模态，您怎么看原生多模态这个趋势，它的能力边界在哪儿？</p><p>&nbsp;</p><p>A：人机交互应该是这波AI革命所带来的，并不是 AI 本身。但是，我跟你在交流的时候，有语言、表情还有手势等等，这就是我们理解的多模态。</p><p>&nbsp;</p><p>不是说我们今天的原生多模态就是去认识那个花是什么、那个草是什么，当然这个东西也有市场前景，但不代表原生多模态就应该去做这个东西。原生多模态核心的逻辑跟大模型一样，大模型大家都知道，今天的 Transformer 所有的架构，是从翻译来的，翻译里面是LP里面最难的一个领域。但是今天大家看，习以为常了。它最难的在哪儿？就是对齐。中国的语言和英文的语言，现在看似对的很齐，但其中包含跨文化的问题，比如在翻译诗词的时候就很难对齐。原生多模态的时候我们在跟什么东西对齐？是将人类的语言和我的手势、和所有自然界的东西对齐。所以整个大模型在干的一件事情，本质上就是在对齐。但是原生多模态意味着什么？我们把世界的万物、各种模态的数据，我用这个手势代表的是什么意思，我用语言表达出来的是什么意思？这个东西要对齐，这个是我们的核心。所以，这是大模型最核心、最关键我们在做的事情。</p><p>&nbsp;</p><p>原生多模态，我们要理解后面的本质，徐鹏博士在负责我们的语言大模型，包括百灵，我们为什么会把这个组合放在一起，是因为我们觉得语言只是一个看似比较好对齐的东西。语言不是人造的，不是一个自然界的东西，它本身是有边界的。每发明一个词，边界会扩展一点，但是总体是有一个边界的。你今天如果扩展到多模态或者原生多模态这个领域，它的边界肯定是扩大的，它这个问题的象限或者复杂度是呈指数倍上升的。那我们判断要不要做这个事情？虽然它的难度很大，从中国的角度你必须要攻克这个事情，而不是说我跟在别人后面。这个事情本质上它是有意义的，就跟人脑是一样的，小朋友有的时候不会语言，他也看得懂你的知识。一只小狗，它也知道你摇摇手、或者挥挥手是什么意思，它也没有语言，这个东西是更底层的东西。所以我们未来的多模态就是帮你做一个东西，让你知道你的小狗狗到底最近怎么了。很多人不知道，我认为这个是我们可以攻克的。</p><p>&nbsp;</p><p>所以，我们在做这个事情，回答你的边界，肯定是高于语言本身的，因为语言的边界是人为划了一个圈，但是它依然是核心，因为人类的智慧基本上就在语言的边界里面，能被语言表达出来的东西，其实都是人类的智慧。</p><p>&nbsp;</p><p>&nbsp;</p><p>Q8：去年 ChatGPT 刚出来的时候，咱们的认知有没有发生变化？</p><p>&nbsp;</p><p>A：我们其实每天都在发生变化，但是大逻辑没有变。当然这可能是技术的局限，我们觉得这是一个很好、很先进的一个技术，而且最关键的是它让大家看到了一个可能性，我认为所有人去做就是因为这个可能性。但是从另外一方面，我们更关注的是，如何让它真正的能用起来，不是说只是变成少数人的 AI，我认为应该是大部分人的 AI，是所有人的 AI。我们思考的是怎么让这个东西做的更好用、更经济、更可靠，要把这个东西做出来，这是我们的一些思考。</p><p>&nbsp;</p><p>&nbsp;</p><p>Q9：现在可以看到几乎所有的互联网公司都在提大模型，大模型也用到各个领域。大模型背后是大数据或者云计算或者数据各方面应用。大模型会不会成为下一次宕机的威胁所在，这个技术会不会对所有平台造成新的技术挑战？出现这种问题，究竟是技术不够还是对技术的敬畏不够？</p><p>&nbsp;</p><p>A：蚂蚁技术开放日是 2015 年的一次宕机引发的，到今天差不多 10 年了。这 10 年以来技术不断地发展，但是宕机和某某平台崩了还是会在热搜上出现。你刚刚提到敬畏技术，那这 10 年里面到底是技术不够还是敬畏不够？AI 本质上是不是足够大的风险？我倾向于更积极的去理解它。你说今天的互联网技术是不是一个风险？它很多时候其实是一个风险。但是我们需不需要互联网？我相信在座所有人都觉得我们需要互联网。任何事物都有它的两面性。</p><p>&nbsp;</p><p>技术我偏向于中性的去理解它，关键在于你怎么使用它，你把它用在哪儿，或者说你对它有没有足够的敬畏，知道它的能力边界，不会去放大它，不会去夸张的吹嘘它。我认为AI这波浪潮也是一样的，打个比方，我一直认为它是大号的“复读机”，全世界的知识它能够复读出来。你把它用在关键核心的地方，可能你就要承受这种关键核心地方崩塌所带来的风险。你把它用在不重要的地方，那它就是一个背景音，像一个“收音机”，挂了就挂了，就是这样的一个逻辑。</p><p>&nbsp;</p><p>到底是技术不够还是敬畏不够？我认为这是大家对技术的期待越来越高了。试想一下十年前或者更早一段时间，其实很多网站是有宕机护时间的。但是今天再看，哪个网站说我要宕机维护一下？国外的云几乎都需要维护，但是在中国，在阿里云是没有的。我们今天的技术在越来越深入的进入到国计民生各个行业。以支付宝为例，我们经常跟自己说，十年前的支付宝和现在的支付宝完全不一样，今天的支付宝要宕机1秒钟，可能上海的地铁的闸机就过不去了，但十年前我们是没有这个职责的。所以那时候宕机宕2个小时，大家觉得日子照常过，但是今天支付宝宕机 2 小时，那会产生非常大的影响，这是我们今天对技术的要求不一样。在这个过程当中，我认为技术不怕挑战，核心是说我们有更高的要求了，我们的技术一定会往上走的。AI技术也是一样的道理，我们有要求，有足够的敬畏，有足够的投入，它一定会有足够的产出。</p><p>&nbsp;</p><p>&nbsp;</p><p>Q10：如果一个大模型要做到可靠的话，是目前Transformer这个底层是可以优化的，还是说需要一些技术辅助它去做，您有什么技术可以分享的？</p><p>&nbsp;</p><p>A：我首先纠正一点，Transformer 不关键。因为 Transformer 的本质就是把序列数据变成并行化。之前语言大模型的训练是不可能那么快的，就算有卡也是不可能那么快的，Transformer 核心改变的就是这个。它的本质是在于我在模型训练时，包括从数据到最后的产生的输出，每一个 token 都是预测出来的，预测它一定会有误差的。但当你的误差累积到一个程度，它就一定会胡说八道的。所以，本质上是在控制这个数据流我怎么流进去以及我怎么输出来，让它在足够小的误差范围内，以及误差不要累积。</p><p>&nbsp;</p><p>至于解决这个问题，肯定是有些手段和办法的。第一个是大家都知道所谓的 RAG，它核心是控制它的输入，我只能从这个知识库里面去流向到模型系统，然后再做输出，本质上是控制它的输入。然后，输出是靠什么？我们蚂蚁也开源了在金融领域的一个多智能体框架 agentUniverse。输出是干什么呢？我们是要多个模型去对，甚至按照一定的 SOP，按照一个专家流程来产生这个输出。或者简单讲，我们要尽量收紧它可输出的范围，来进一步的控制它的精确性。当然这个东西的问题就在这儿了，AI其实就两个事，一个是泛化，一个是精准，足够泛化就很难精准，足够精准就很难泛化，这是一个本质矛盾。我们要干的事情是什么？我们在控制输入、控制输出的过程本身就是这么一个逻辑，在控制输入的时候它要足够的泛化能力，足够的泛化能力它是要靠中间的模型足够大、内容要足够多，然后我两头一掐，它牺牲掉以后，既有足够的精准性又有足够的智慧。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/FuZrECsytJmcvGAxU9Cp</id>
            <title>腾讯大模型APP真实测评！七家国产大模型“battle”，元宝顶得住吗？</title>
            <link>https://www.infoq.cn/article/FuZrECsytJmcvGAxU9Cp</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/FuZrECsytJmcvGAxU9Cp</guid>
            <pubDate></pubDate>
            <updated>Fri, 31 May 2024 09:53:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词:  腾讯元宝, 混元大模型, AI搜索, AI写作
<br>
<br>
总结:  腾讯元宝是基于混元大模型的App，提供AI搜索和AI写作等核心能力，旨在服务于普通人的生活。在文生图和AI写作方面，腾讯元宝展现出了提升潜力，与其他大模型进行对比评估，效果不俗。AI搜索方面，腾讯元宝接入了微信搜一搜、搜狗搜索等搜索引擎，提升了效率和准确性。 </div>
                        <hr>
                    
                    <p>作者 | 华卫</p><p>&nbsp;</p><p>“腾讯做大模型不争一时之先。”</p><p>&nbsp;</p><p>5 月 30 日，腾讯基于混元大模型的App“腾讯元宝”正式上线，苹果及安卓应用商店均可下载。腾讯云副总裁、腾讯混元大模型负责人刘煜宏表示：“过去的一年，我们持续推进腾讯混元大模型的能力爬坡，希望腾讯元宝最终服务于每个普通人的生活。”</p><p>&nbsp;</p><p>相比此前测试阶段的混元小程序版本，面向工作效率场景，腾讯元宝提供了AI搜索、AI总结、AI写作等核心能力；面向日常生活场景，元宝提供了多个特色AI应用，并新增了创建个人智能体等玩法。</p><p>&nbsp;</p><p>那么，这些功能的实际表现到底如何呢？</p><p>&nbsp;</p><p>拿到腾讯元宝的体验资格后，我们马上逐一试用了它的亮点功能，并特别针对文生图和AI写作方面，通过同题多解的方式，将其与多个国内知名的大模型进行了对比和评估。</p><p>&nbsp;</p><p></p><h2>创作效果：有提升潜力</h2><p></p><p>&nbsp;</p><p>首先，来直击一下元宝与各大模型“battle”的实况。</p><p>&nbsp;</p><p>文生图</p><p>&nbsp;</p><p>在这一功能上，我们选择了百度文心一格、阿里通义万相、讯飞星火、美图 MiracleVision 4.0 、字节跳动豆包五个模型对比效果，对他们的理解和内容生成能力逐一进行了测评。另外，考虑到涉及的这些大模型都源自国产，我们特意选取了中国文言文作为输入素材，以此来考察它们在处理本国语言古典文本上的能力。</p><p>&nbsp;</p><p>从生成图中所涵盖各实体元素的完整程度和整体画面的协调性来说，元宝的表现是排在前列的。</p><p>&nbsp;</p><p>提示词为：林中有寿鹿仙狐，树上有灵禽玄鹤。瑶草奇花不谢，青松翠柏长春。仙桃常结果，修竹每留云。一条涧壑藤萝密，四面原堤草色新。</p><p>&nbsp;</p><p>生成结果如下：</p><p><img src="https://static001.geekbang.org/infoq/44/44c886249db068571bc948559d535c79.jpeg" /></p><p>（从左到右分别是元宝、豆包、讯飞星火、美图 MiracleVision 4.0、文心一格的输出结果）</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bdce8068798d7af3511dcf3aa6362660.png" /></p><p></p><p>（通义万相的输出结果）</p><p>&nbsp;</p><p>AI写作</p><p>&nbsp;</p><p>据介绍，在AI写作方面，元宝不仅支持多轮问答，还能够将对话的内容整理成报告，按照要求进行结构化输出。这一功能上，我们将其与文心一言、通义千问、Kimi、豆包四个模型对比效果。</p><p>&nbsp;</p><p>以测评案例来看，相较而言，腾讯元宝的生成内容展现出了包含起承转合的完整情节，故事框架也已具备雏形，但语句间的逻辑衔接略显生硬、人物描写也较为生涩。</p><p>&nbsp;</p><p>提示词：唐僧师徒四人穿越到现代后的第一天，会发生什么故事？写一个300字左右的小故事。</p><p>&nbsp;</p><p>元宝的生成结果如下：</p><p>&nbsp;</p><p>可以小夸一下的是，元宝留意到了唐僧师徒四人需要吃素的人物细节，在人设和故事设定方面理解得还不错。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6c/6c93ffbd3c79acfc26a4a3cbff437cf4.jpeg" /></p><p></p><p>&nbsp;</p><p>再看豆包的生成结果，其语句结构明显更为成熟了，各个段落环节之间衔接得也比较自然。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/46/46a422e4fd58cbd06cadc795f3632ef5.png" /></p><p></p><p>&nbsp;</p><p>到文心一言这里，无论语句组织还是文字逻辑，都展现出不错的效果。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ecd423a78fee7f5cef56b16772330317.png" /></p><p></p><p>&nbsp;</p><p>而Kimi和通义千问的生成结果，开始呈现出更显著的变化。除语句构造外，整个故事的人物设定、叙事角度、情节架构都更加立体，并都在结尾处给读者构建了一个引人入胜的虚构世界。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/06/06737b6e9a5777f6c72bdacd8b798721.png" /></p><p></p><p>（Kimi的输出结果）</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/cc/ccae0d6aa2348bc17ce68083be19d3b6.png" /></p><p></p><p>（通义千问的输出结果）</p><p>&nbsp;</p><p></p><h2>效率、娱乐方面：表现不俗</h2><p></p><p>&nbsp;</p><p>当前，大模型仍在快速发展期，从模型能力到应用落地都存在较大“时延”。数据显示，当前人们使用大模型相关产品时，有超过 65%的需求，集中在工作/学习效率场景，但相关的AI产品解决方案尚不成熟。针对效率场景的三大核心需求：信息获取、处理和生产，腾讯元宝均进行了产品化探索。</p><p>&nbsp;</p><p>AI 搜索</p><p>&nbsp;</p><p>AI 搜索方面，腾讯元宝接入了微信搜一搜、搜狗搜索等搜索引擎，并通过AI搜索增强，提升时新类和知识类问题效果，比传统搜索更有效率；同时，内容覆盖微信公众号等腾讯生态内容及互联网权威信源，答案准确性更高；此外，元宝还会提供所引用的参考资料，并给出相关推荐，方便快速溯源及延伸阅读。</p><p>&nbsp;</p><p>我们输入一个近日引发热议的美国AI禁令问题：如何看待中国人被限制在美从事 AI 相关工作？</p><p>生成结果如下：</p><p><img src="https://static001.geekbang.org/infoq/b1/b1a00d5888bfd780c585ab89630e45b5.jpeg" /></p><p>&nbsp;</p><p>AI总结</p><p>&nbsp;</p><p>AI总结方面，无论是希望快速了解一本书或是一个新领域，还是处理复杂繁冗的报告、文献，元宝都能帮上忙。据介绍，元宝可上传最多10个PDF、word、txt等多种格式的文档，并能够一次性解析多个微信公众号链接、网址，支持256K的原生窗口上下文。</p><p>&nbsp;</p><p>例如，我们输入：请总结一下AI前线公众号这一年来的内容输出亮点。</p><p>&nbsp;</p><p>生成结果如下：</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/35/35ef172c42abc7d68cefc8b422c5232a.jpeg" /></p><p>&nbsp;</p><p>除了满足效率需要，腾讯元宝在日常生活场景，也提供了丰富的应用及玩法，包括百变AI头像、口语陪练、超能翻译官等，均免费开放。同时，元宝也支持用户根据个性化需求，快速创建个人专属的智能体，赋予角色设定，或让AI自动生成智能体相关信息，并复刻自己的音色。结合腾讯生态场景，元宝还将于近期推出腾讯新闻哥、《庆余年》主题等特色智能体。</p><p>&nbsp;</p><p>视频、3D生成功能后续上线</p><p>&nbsp;</p><p>腾讯元宝产品能力升级的背后，是混元底层模型的持续迭代。</p><p>&nbsp;</p><p>自 2023 年 9 月首次亮相以来，腾讯混元大模型的参数规模已从千亿升级至万亿，预训练语料从万亿升级至7 万亿tokens，并率先升级为多专家模型结构（MoE），整体性能相比Dense 版本提升超50%。除不断提升通用大模型能力外，腾讯混元也支持角色扮演、FunctionCall、代码生成等领域能力，数理能力提升 50 %。</p><p>&nbsp;</p><p>在多模态方面，腾讯混元文生图大模型是业内首个中文原生DiT架构模型，采用了Sora、Stable Diffusion 3等行业顶尖产品的同款架构，生成效果相比上代提升超 20%。目前，该模型已经全面开源，在Github获得 2000+star，相关能力也全面融入腾讯元宝。</p><p>&nbsp;</p><p>此外，腾讯混元大模型在视频、3D生成等方面也持续探索，目前已经支持16s视频生成，单图仅需30秒即可生成3D模型，相关能力也将于后续在元宝中上线。</p><p>&nbsp;</p><p>目前，腾讯内部有超 600 个业务及场景都已经接入腾讯混元，腾讯广告、微信读书、腾讯会议、腾讯文档、腾讯客服等，都已经基于混元实现了智能化升级。据了解，为了满足开发者及企业客户对于通用模型能力的需求，腾讯混元大模型已通过腾讯云对外开放，可通过API调用，也可以作为基底模型，为不同产业场景构建专属应用。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/NMIPCuy1ctaID4CQ3bJN</id>
            <title>AI 是低代码的“福”还是“孽”？</title>
            <link>https://www.infoq.cn/article/NMIPCuy1ctaID4CQ3bJN</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NMIPCuy1ctaID4CQ3bJN</guid>
            <pubDate></pubDate>
            <updated>Fri, 31 May 2024 09:04:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI, 低代码, 大模型, 编程
<br>
<br>
总结: 在AI发展史上，AI大模型的出现引发了对低代码平台的讨论，AI编程的概念挑战了传统的代码开发方式，行业内存在关于AI与低代码未来发展方向的争论。在相关峰会上，专家们分享了AI与低代码的结合实践和思考，探讨了AI时代软件架构的设计和应用。 </div>
                        <hr>
                    
                    <p>取代论，在 AI 几经起伏的发展史上的每一个高点，都会被拿出来重新审视和热议。这种职业危机感，在生成式 AI 的这一波浪潮中，也很快蔓延到了技术圈中。“自己的饭碗被自己的工作干掉，这可能不是一句玩笑话。”一位开发者在近期与 InfoQ 交流时感叹。</p><p></p><p>在此之前，低代码 / 无代码作为软件提效的平台和工具已经逐步流行起来。顾名思义，其价值在于通过图形化界面和简单点击、拖拽、配置，能够大大降低代码开发的门槛，减少开发人员工作量的同时，针对一些简单开发需求，业务人员也能“自给自足”，更快地响应业务侧的需求。</p><p></p><p>而就在低代码概念开始被市场普及接纳走向落地，行业的商业模式刚刚跑通时，半路“杀出”了 AI 大模型。</p><p>相较于低代码，AI 大模型的可能性更为激进：不需要任何的编码，只通过自然语言交互就可以直接生成应用。换言之，代码开发的门槛不是降低了，而是直接没有了。</p><p></p><p>一场争论不可避免。一种声音是“低代码将被 AI 彻底颠覆”，比如，一些低代码起家的公司，去年火速切换到了 GPT 赛道，背后的考量不难臆测。另一种声音则认为，二者将双向奔赴，AI 能力将成为低代码的标配，有不少企业正在试图将二者融合。</p><p></p><p>在 6 月 14 日 -15 日即将举办的 <a href="https://archsummit.infoq.cn/2024/shenzhen/">ArchSummit 全球架构师峰会深圳站</a>"上，阿里巴巴研究员 / 阿里云云原生应用平台负责人丁宇（叔同）将带来<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5718">《AI 编程如何颠覆生产力》</a>" 的 Keynote 主题演讲，详细介绍在 AI 大模型快速发展的背景下，AI 编程的发展实现了哪些突破，以及 AI 编程助手的引入如何为软件开发带来质的飞跃。从开发者的视角出发，他将展开分享 AI 编程工具基于大模型的设计要点、难点、改进思路，帮助开发者从自身的生命力出发，学会用 AI 激活开发效率，提升生产力，而不是与之对抗。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8c/8cc3b180566afd334adb450c1a57fba6.webp" /></p><p></p><p>为进一步探讨 AI 与低代码的关系，ArchSummit 深圳还进一步策划了<a href="https://archsummit.infoq.cn/2024/shenzhen/track/1643">《低代码与 AI 结合》专题</a>"，深入研究低代码平台如何与人工智能技术相结合，提高开发效率。探讨在低代码环境中集成智能决策、自动化流程，以及构建灵活、高效的应用系统。</p><p></p><p><img src="https://static001.geekbang.org/infoq/03/03baaaff951079ab3734a289ead8e5df.webp" /></p><p></p><p>腾讯 PCG 前端技术专家苑宗鹤将在专题演讲中分享《无极低代码 UI 可视化的 AIGC 落地与实践》。在他看来，大模型的提效能力加上低代码的易用性相辅相成，让低代码开发效率更高，更大的降低了用户的使用门槛。他将基于无极低代码平台介绍 AI 搭建低代码布局、AI 辅助代码生成，以及对平台的 AI 功能进行自动化测试的实践路径。</p><p></p><p>网易 CodeWave 技术团队作为国内较早采用大模型技术并将其应用于产品的技术团队，其团队负责人姜天意也将在专题演讲中从低代码产品的挑战出发，分析大模型的机遇和实施难点。同时，从 AI 友好的语言设计出发，结合大模型的 Agent 能力，介绍融合自然语言生成、辅助编程、D2C 等 AI 能力的产品设计和实施方案，以及相关的模型训练方案。</p><p></p><p>针对“AI 都能编程了，低代码平台会被消灭吗？”这样的灵魂拷问，蚂蚁集团支付宝体验技术部 / 高级前端技术专家江凯将在其演讲中给出他的答案。他将详细介绍《云凤蝶在 AI 与 LowCode 结合上的思考与实践》，分享 AI Native 的低代码产品形态如何设计、如何实际应用 LLM 和 AIGC 技术、如何看待对话式 UI、生成式 UI 的发展？AI 原生应用的 LowCode 研发有市场吗等一系列行业普遍的困惑问题。</p><p></p><p>除此之外，本次大会还策划了 10 余个 AI 系列相关的专题，顺丰集团 CTO 耿艳坤、Thoughtworks CTO Scott Shaw、百度飞桨总架构师于佃海等国内外 100+ 顶尖专家齐聚，将从底层基础到顶层应用深度探索大模型时代软件架构的最佳设计，共探 AI 时代的无限可能。</p><p></p><p>点击链接可查看更多详情：<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">https://archsummit.infoq.cn/2024/shenzhen/schedule</a>"</p><p>会议 9 折购票倒计时1天，如您感兴趣，可以联系票务经理 17310043226，锁定最新优惠。</p><p><img src="https://static001.geekbang.org/infoq/d9/d9b861049ccebcbdec69ed036e705118.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/IN98I1kFWJKwEA1aGZKg</id>
            <title>剥离几百万行代码，复制核心算法去美国？TikTok最新回应来了</title>
            <link>https://www.infoq.cn/article/IN98I1kFWJKwEA1aGZKg</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IN98I1kFWJKwEA1aGZKg</guid>
            <pubDate></pubDate>
            <updated>Fri, 31 May 2024 08:34:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 代码剥离, TikTok, 美国用户, 算法
<br>
<br>
总结: TikTok正着手为其1.7亿美国用户开发克隆版推荐算法，剥离数百万行代码是一项繁琐的工作，需要一年多时间才能完成。这项工作的目标是为面向美国用户的TikTok推荐算法创建一套新的源代码库，但可能导致美国TikTok失去母公司字节跳动的工程开发支持。 </div>
                        <hr>
                    
                    <p></p><blockquote>剥离几百万行代码，是一个繁琐的“脏活”，需要一年多时间才能完成。</blockquote><p></p><p>&nbsp;</p><p>路透社5月30日消息，据直接掌握内情的消息人士透露，TikTok正着手为其1.7亿美国用户开发克隆版推荐算法。这可能催生出一个独立于其中国母公司运行的版本，因此更容易被拟发布禁令的美国立法机构所接受。</p><p>&nbsp;</p><p>就在TikTok中国母公司字节跳动去年年底下令拆分源代码之前，美国方面已经提出一项拟议的强制出售TikTok在美业务的法案，且此项法案今年初在国会得到支持。今年4月，该法案正式被签署为法律。由于未获授权公开谈论这款短视频分享应用，消息人士拒绝透露姓名。但其表示一旦代码被拆分，即可为剥离在美资产奠定基础，从而打开一条免受法律强制执行的可能之路。</p><p>&nbsp;</p><p>TikTok公司此前曾经表示并无出售在美资产的计划，甚至强调绝不可能采取此类措施。该公司最初拒绝发表评论。但在本篇报道发表之后，TikTok在X上的帖子中表示，“路透社方面日前发布的报道存在误导性，且与事实有所出入”。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/4d/4d2d551608dc0513c79bf068230f6055.jpeg" /></p><p></p><p>&nbsp;</p><p>路透社表示，该“辟谣”没有具体说明哪里不准确。TikTok还发布了一段来自其联邦诉讼材料的内容：“无论是从商业、技术还是法律角度来看，该法案提出的「获准剥离」以允许TikTok继续在美运营的建议都根本不可行。而即使可行，法案给出的270天时间表也远远不够。”</p><p>&nbsp;</p><p>路透社方面的发言人则回应称，“我们坚持报道内容属实。”</p><p>&nbsp;</p><p></p><h2>剥离数百万行代码的“脏活”</h2><p></p><p>&nbsp;</p><p>路透社称，过去几个月以来，来自美国和中国的数百名字节跳动和TikTok工程师被要求着手剥离数百万行代码，旨在筛选该公司用于将用户与其偏好的视频相匹配的算法。据两位直接了解该项目的消息人士说，工程师们的任务就是建立一套独立的代码库，其独立于字节跳动的中国版TikTok、即抖音的系统之外，同时将消除其中与中国用户相关的信息。</p><p>&nbsp;</p><p>路透社此前曾报道称，TikTok将应用程序连同算法一同出售的可能性极低。而且内容推荐算法在我们国家的出口管制名单当中，因此对TikTok算法的剥离或者出售必须经过审查。</p><p>&nbsp;</p><p>根据相关法律文件，TikTok推荐引擎的源代码最初由字节跳动工程师在中国开发完成，并针对包括美国在内的TikTok全球各市场运营情况进行了定制。</p><p>&nbsp;</p><p>在字节跳动看来，TikTok之所以在全球范围内大受欢迎，首先要归功于其推荐引擎的出色表现。该引擎能够根据每位用户与其观看内容的交互方式来提供更多内容来源。</p><p>&nbsp;</p><p>在向路透社描述这项任务的复杂性时，消息人士们将其称为繁琐的“脏活”，这也凸显出将TikTok业务与其母公司底层代码剥离开来的难度。据消息人士们介绍，这项工作预计需要一年多时间才能完成。</p><p>&nbsp;</p><p>TikTok及字节跳动已经明确表示会以美国宪法第一修正案为依据，在法庭上对抗这条新法律。尽管如此，消息人士称工程师们已经在依命令开展工作，着手将TikTok推荐引擎与字节跳动的整体网络拆分开来。</p><p>&nbsp;</p><p>消息人士们提到，TikTok高管曾一度考虑开源部分TikTok算法，或者允许其他人访问并修改该算法，借此展示技术透明度。</p><p>&nbsp;</p><p>据一位出席团队全体会议的消息人士、以及另一位翻阅过相关材料的消息人士所言，高管们在会上通过内部规划文件及内部通讯系统Lark传达了剥离计划，并就代码拆分项目做出了更新说明（但路透社无法独立核实这些内部消息的真伪）。</p><p>&nbsp;</p><p>据一位消息人士透露，目前这项工作的复杂之处，在于确定TikTok具体代码迁移部分所带来的合规性与法律问题。消息人士还补充称，必须审查每一行代码以核实其是否可以被剥离至独立代码库。</p><p>&nbsp;</p><p>这项工作的目标，是为面向美国用户的TikTok推荐算法创建一套新的源代码库。工作完成之后，TikTok美国版将独立于其他地区的TikTok版本以及中文版抖音，采取专门的一套推荐算法运行和维护体系。消息人士称，此举将导致美国TikTok失去母公司字节跳动强大的工程开发支持。</p><p>&nbsp;</p><p>消息人士还补充称，如果TikTok最终完成美版推荐引擎与中国版本的拆分工作，管理层承认后续TikTok美国版在性能上恐怕达不到现有TikTok的水平。因为目前TikTok推荐算法库仍高度依赖字节跳动中国工程师们的更新和维护。换句话说，TikTok在美国市场上的用户吸引力可能将因此被削弱。</p><p>&nbsp;</p><p></p><h2>TikTok推荐算法神秘吗？</h2><p></p><p>&nbsp;</p><p>在TikTok风波中，其推荐算法一直是大家争相讨论的话题中心。</p><p>&nbsp;</p><p>2022年6月，有媒体报道，TikTok宣布将美国境内的所有流量转移到甲骨文云服务的基础设施上，同时这项托管服务也给甲骨文带来高达10 亿美元收入。而后，甲骨文于这一年的8月份启动了对TikTok 算法和模型的审查，甲骨文希望确保 TikTok 上的内容显示“符合用户的期望”，并且推荐算法不会受到操纵。TikTok 还专门设有一个“专用透明度中心”的区域，供甲骨文员工审查该应用程序的源代码。</p><p>&nbsp;</p><p>然而一年之后，据福布斯报道，字节跳动和甲骨文之间的关系就已经变得非常不信任和敌对。消息人士将甲骨文对字节跳动的立场描述为“反情报行动”，而不是正常的客户关系。与此同时，一些字节跳动员工怀疑甲骨文是否只是想增加他们的账单。TikTok 托管服务合同在甲骨文内部被称为 Project Telesis，使字节跳动成为甲骨文最赚钱的客户之一。</p><p>&nbsp;</p><p>如今看来，甲骨文的审查并没有让美国政府放松对TikTok的仇恨和警惕。</p><p>&nbsp;</p><p>实际上，TikTok 之前发表过一篇博客文章，主要描述解释了他们的 FYP 算法工作机制，相信大家只要是做软件技术的，看过之后都知道其中并没有什么新鲜的创造。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/29/295ecc271aa2b22c4c00ed3e4b8ae723.jpeg" /></p><p></p><p>&nbsp;</p><p>另外，前亚马逊产品经理（同时也是亚马逊战略规划部第一位分析师）Eugene Wei 也曾发过专门分析文章，阐述TikTok 的算法本身并没有特别的突破性创意。</p><p>&nbsp;</p><p></p><blockquote>当大家谈及 TikTok 的算法是其成功的关键时，便会认为该公司的秘密武器是一些神奇的代码。但该领域的大多数专家持怀疑态度，TikTok 在机器学习推荐算法方面并未取得外界未知的突破性进展。事实上，他们中的大多数人认为，TikTok 很可能就是基于标准方案解决的问题，跟其他方案无异，没有什么特殊性。&nbsp;不过机器学习算法的有效性并不仅仅取决于算法本身的函数，还取决于数据集训练后的算法函数。GPT-3 并不是新创意，但是通过大量数据训练和大量的参数设置，它的输出结果往往是令人惊讶的。&nbsp;同样道理，基于自身数据集训练过的 TikTok FYP 算法，在将视频与觉得该视频有趣的人进行匹配方面做的非常精确和高效（而且，反向匹配做的也很精确，对某些视频不感兴趣的人就不会接收到这些视频） 。</blockquote><p></p><p>&nbsp;</p><p>他认为，TikTok 产品真正的价值点在于 TikTok 的设计和流程里面的每一个元素是怎么互相关联到一起，从而创建出一个数据集，再通过这个数据集，把算法训练成最佳性能的。</p><p>&nbsp;</p><p>“这就是 TikTok 设计的神奇之处：它是一个反馈的闭环，这种设计能够激发并实现视频的创作和观看，产生的数据进而通过其算法进行训练，之后再反过来激发创作和观看。为了让 TikTok 的算法变得像现在这样有效，TikTok 成为了它自己的训练数据来源。”</p><p>&nbsp;</p><p>但多数人还是非常费解，为什么很多公司想要收购 TikTok，另一方面，字节跳动是否应该将 TikTok 这一备受欢迎的 App 卖掉。对此，Eugene Wei 评论说：“围绕 TikTok 算法大肆的炒作已经开始变的异化了，这也是如今西方对中国科技领域项目的普遍套路。”</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.reuters.com/technology/tiktok-preparing-us-copy-apps-core-algorithm-sources-say-2024-05-30/">https://www.reuters.com/technology/tiktok-preparing-us-copy-apps-core-algorithm-sources-say-2024-05-30/</a>"</p><p><a href="https://newsroom.tiktok.com/en-us/how-tiktok-recommends-videos-for-you">https://newsroom.tiktok.com/en-us/how-tiktok-recommends-videos-for-you</a>"</p><p><a href="https://www.infoq.cn/article/38dKguZxeyz2vx2dAR4S?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">https://www.infoq.cn/article/38dKguZxeyz2vx2dAR4S</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/meMm4MXomT8GZMYvDzbo</id>
            <title>全球首款PC原生的AI编程与软件智能研发助手驭码CodeRider正式发布！</title>
            <link>https://www.infoq.cn/article/meMm4MXomT8GZMYvDzbo</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/meMm4MXomT8GZMYvDzbo</guid>
            <pubDate></pubDate>
            <updated>Fri, 31 May 2024 02:56:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AIGC, AI产品发布会, 驭码 CodeRider, 极狐GitLab
<br>
<br>
总结: 5月28日下午，极狐GitLab在上海成功举办了基于AIGC技术且完全自研的AI产品发布会，正式推出驭码CodeRider，PC原生的AI编程与软件智能研发助手。CEO柳钢介绍了驭码CodeRider的含义和优势，强调AI赋能程序员而非取代。产品具有PC原生、私有化部署、与GitLab深度融合等优势，受到企业用户好评。发布会还宣布了驭码CodeRider的三个版本，开启了试用通道。 </div>
                        <hr>
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/c3/c3e154ed850988f8ff90ce95496aa795.png" /></p><p></p><p>5 月 28 日下午，极狐GitLab 在上海成功举办了基于 AIGC 技术且完全自研的 AI 产品发布会，正式推出驭码 CodeRider —— PC 原生的 AI 编程与软件智能研发助手。发布会采取线上线下相结合的方式，线下有包括企业高管、高校教师、高校学生、媒体人员在内的 80 多位嘉宾共同出席参加了此次发布会，线上观看量超过 10万+。 </p><p></p><p>发布会伊始，极狐GitLab CEO 柳钢以“PC 原生全球首发 AI 编程与软件智能研发助手”为主题，全面介绍了驭码 CodeRider。 </p><p></p><p><img src="https://static001.geekbang.org/infoq/40/40e343e6a832e4f6098a3baf0b60536d.png" /></p><p>极狐GitLab CEO 柳钢 </p><p></p><p>柳钢表示，软件定义世界已经成为了不争的事实，而软件的打造者、代码的创造者正是程序员这一群体，在中国就有 1000万程序员。而极狐GitLab就是一家专为中国程序员服务的公司，也是目前国内唯一一家将赋能中国程序员写进员工手册的公司，极狐公司的使命是——让中国程序员的技术潜能与业务价值得到最大化发展。极狐GitLab 致力于通过借鉴全球领先的技术，同时基于国内现状进行完全自主研发、自主创新的方式来赋能中国1000万程序员。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c1/c115d21986defd5f342fcde2bb393d21.png" /></p><p>赋能 or 取代？ </p><p></p><p>柳钢进一步表示，在 AI 时代赋能中国程序员的最好方式就是将 AI 能力应用到软件研发领域，为程序员打造出为程序员所有、被程序员所用、让程序员认可的产品，用 AI 赋能程序员，绝非取代程序员 。但是在这个过程中必须解决当前大模型的三个关键问题：数据安全、个性化以及成本。只有成功解决这三个问题，才能够真正打造出程序员心目中的绝佳好产品——覆盖软件研发全生命周期、企业统一部署 &amp; 用户一键安装、功能丰富 &amp; 触手可及以及私藏独有。</p><p></p><p></p><p>“极狐GitLab AI 团队完全自研的驭码 CodeRider 正是这样一款产品。而且驭码 CodeRider 是全球首款 PC 原生的 AI 编程与软件智能研发助手。”柳钢强调。 </p><p></p><p><img src="https://static001.geekbang.org/infoq/24/24c56680c152f342663a416854148fd6.png" /></p><p>极狐GitLab 驭码CodeRider 正式发布 </p><p></p><p>接着，柳钢解读了驭码 CodeRider 的含义。Code 是程序员心目中最神圣的词，意为编码，而 Rider 有骑兵、驾驭者的意思。为此，特意为该产品起了一个朗朗上口的 Slogan“为 AI（爱）奔腾，驭码当先”。从表面看，驭码 CodeRider 是一个 AI 编程助手，看似是又一款 Copilot，但是驭码又绝不仅仅只是一款 Copilot，相比市面上其他所有的 Copilots，驭码 CodeRider 要比他们优秀三点，而这也是驭码 CodeRider 得天独厚的优势：PC 原生、私有化部署、GitLab 合璧。PC 原生是指驭码 CodeRider 是端侧部署，和笔记本电脑是天然契合的，而且产品在设计之初就考虑了离线运行；私有化部署是指可以将驭码 CodeRider 部署在企业内部的安全环境中，这从根本上解决了数据安全问题，而且能够更好地满足和实现企业智能化、个性化问题；最后一点：驭码 CodeRider 和 GitLab 深度融合、浑然一体，这意味着驭码 CodeRider 将 AI 技术完全应用到软件研发的全生命周期中，不仅可以编写代码，还能够帮助程序员进行议题（Issue）和合并请求（MR）的处理，甚至和 CI/CD 结合起来，做到端到端的 AI 赋能。这些都是其他 Copilots 无法做到的！ </p><p></p><p><img src="https://static001.geekbang.org/infoq/a3/a34bc5dd659e0c563ed255a78769b9fd.png" /></p><p>驭码CodeRider 比 Copilot 好三点 </p><p></p><p>柳钢表示，自夸不算好，要用户说了才算好，目前已经有很多企业用户在体验试用驭码 CodeRider，他们的反馈是对驭码 CodeRider 最好的认可。随后，现场播放了来自嘉宾的祝福视频，包括联想集团、通义实验室、零一万物、PingCAP、无问芯穷、LigaAI、重庆市政府九龙坡区、未尽研究、哪吒汽车、足下科技、PingCode、Sipingsoft、青岛职业技术学院、南京大学、厦门大学、重庆交通大学、广州华商学院的多位嘉宾纷纷表达了对驭码 CodeRider 即将发布的热烈期盼，同时预祝发布会圆满成功！ </p><p></p><p>分享最后，柳钢正式揭晓了驭码 CodeRider 的三个版本：驭码团队版（CodeRider Team）、驭码企业版（CodeRider Enterprise）以及驭码混合算力版（CodeRider Hybrid）。三个版本对应不同的功能以及不同的使用场景。驭码CodeRider 同步开启了申请试用通道，会有专业的顾问为大家解答关于驭码CodeRider 的相关情况。</p><p></p><p><img src="https://static001.geekbang.org/infoq/07/0723ffc88faa9bccf42476d76f49a1c3.png" /></p><p>驭码 CodeRider 专业顾问联系方式 </p><p></p><p>随后，极狐GitLab AI 团队负责人、驭码CodeRider 研发负责人邹雨竹上台深入介绍了驭码 CodeRider 的相关情况。他提到，“驭码 CodeRider 是一款 AI 驱动的 PC 原生应用，是研发人员的智能编程助手，同时跟 GitLab 的深度集成，也让驭码 CodeRider 变成了一款智能 DevOps 工具，真正做到了“一款工具，双重助力”。 </p><p></p><p><img src="https://static001.geekbang.org/infoq/67/6780c895939065b4083d7d041665f79a.png" /></p><p>极狐GitLab AI 产品负责人邹雨竹 </p><p></p><p>邹雨竹进一步解释道，打造这样一款具备 AI 能力的 PC 原生应用，必须考虑三个核心要素算力、模型、引擎。在算力方面，驭码 CodeRider 从设计之初就决定要做 PC 原生的“AI 编程与软件智能研发助手”，为此对 20 多款 AI PC 进行了详尽测评；在模型方面，采取采众家之所长的策略，对 30 多款大模型进行了测评，最后决定驭码 CodeRider 根据不同功能选择最佳模型，比如代码补全使用补全模型、对话采用对话模型，以实现在高效量化的同时达到性能和与体积最佳平衡的目的；在引擎方面，采用了基于 C++ 的桌面推理引擎，而且对不同的硬件指令集和框架做了适配。而这样做也获得了惊艳的效果，同样也造就了驭码 CodeRider 这样一款同时适用于企业、个人的 AI 产品。企业可以通过私有化部署实现数据安全并节省超过 50% 的成本，而开发者则能享受便携性和极佳的响应速度。 </p><p></p><p>邹雨竹还在现场演示了驭码 CodeRider 的相关功能，包括智能编程部分的代码补全、代码生成、技术问答等，以及智能 DevOps 工作流中的 Issue、MR 处理等。“驭码 CodeRider 绝对是一款让企业受益的 AI 工具，因为其具备私有化部署的特性，有着模型组合的优越性，而且与 GitLab 浑然天成”。邹雨竹说到。 </p><p></p><p><img src="https://static001.geekbang.org/infoq/57/57f4a2d7111a97b0a83e660fba224d83.png" /></p><p>驭码CodeRider 产品功能图 </p><p></p><p>接着，来自联想中国的段勐、英特尔中国的张智勇、浙江省特级教师谢作如，三位嘉宾分别进行了分享，他们从企业、高校的角度分享阐述了与 AI 相关的内容和观点。 </p><p></p><p>发布会最后，极狐GitLab CEO 柳钢再次登台，正式揭晓了驭码 CodeRider 三个版本的价格：驭码团队版（CodeRider Team）的价格为499/人/年、驭码企业版（CodeRider Enterprise）的价格为899/人/年。与此同时，也透露了，目前驭码 CodeRider 团队正在积极打造企业级的混合算力私有化架构，智能终端+智能边端会让驭码 CodeRider 变得更强大，这样强大的产品将在 90 天以后与大家见面！ </p><p></p><p><img src="https://static001.geekbang.org/infoq/d4/d4e4a7198909b9d667a42a123ca49d0c.png" /></p><p>驭码CodeRider 价格 </p><p></p><p>最后，柳钢表示，全球首款 PC 原生的 AI 智能编程与软件智能研发助手——驭码CodeRider 发布会宣告结束！到场嘉宾纷纷走向极狐GitLab 团队提前准备好的驭码 CodeRider 体验区，现场体验驭码 CodeRider 的 AI 功能并与技术人员进行了深入交流。 </p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ff74da88bd54726eb9c1496759ae3106.png" /></p><p>驭码CodeRider 现场体验区</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/hPED1Wk71CIt3RHIl2e8</id>
            <title>网易有道CEO周枫：模型即应用的时代到来，Super App随时会诞生</title>
            <link>https://www.infoq.cn/article/hPED1Wk71CIt3RHIl2e8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/hPED1Wk71CIt3RHIl2e8</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 May 2024 07:43:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, AI创新应用, RAG引擎, 有道小P
<br>
<br>
总结: 有道分享了子曰教育大模型的最新技术进展和三大AI创新应用，其中包括新一代知识库问答引擎QAnything、AI全科学习助手“有道小P”APP和虚拟人口语教练Hi Echo 3.0。有道CEO表示大模型需要结合应用场景去优化，并介绍了自研的RAG引擎QAnything。该引擎支持多语种内容提问，已在多个领域取得显著进展。同时，有道还展示了有道小P和Hi Echo 3.0在教育场景中的最新应用成果。 </div>
                        <hr>
                    
                    <p>作者 ｜ 华卫</p><p>&nbsp;</p><p>“大模型的Super App尚未出现，但随时可能会诞生。”</p><p>&nbsp;</p><p>5月29日，网易有道分享了子曰教育大模型最新技术进展及三大AI创新应用：新一代知识库问答引擎QAnything、AI全科学习助手“有道小P”APP和虚拟人口语教练Hi Echo 3.0。</p><p>&nbsp;</p><p>现场，网易有道CEO周枫表示：“当前已经是‘模型即应用’的时代，但大模型也不是万能的，关键是抓住场景。”他认为，大模型的发展需要结合应用场景去迭代优化，在‘产模一体’的框架下去同步提升模型与产品。</p><p>&nbsp;</p><p>据介绍，目前在AI+在线营销、AI+效率工具、AI+娱乐等多个领域，有道都取得了显著进展。有道子曰大模型已经落地到了LLM翻译、AI作文指导、文档问答、语法精讲、虚拟人口语教练等一批应用上。</p><p>&nbsp;</p><p>截至目前，有道AI在线营销业务已连续六个季度实现超过50%的同比增长，AI翻译功能的使用人数已超过500万，使用次数达到2000万次。今年一季度，有道AI订阅服务收入持续增长，一季度会员销售额约5000万，同比增长140%。</p><p>&nbsp;</p><p></p><h2>自研RAG引擎</h2><p></p><p></p><h2>或突破文档问答模态</h2><p></p><p>&nbsp;</p><p>交流会现场，有道首席科学家段亦涛介绍了有道自研的RAG引擎——QAnything。此次升级，QAnything在私有化部署和智能体生成方面进行了一系列的提升，从单纯的文档问答进化成“企业AI大脑”。</p><p>&nbsp;</p><p>段亦涛指出，其原理是利用外部知识来辅助大模型的生成，提高正确性。虽然原理简单，但要落实到生产和生活中真正起到作用，其实还有很大的距离。为此，在开发QAnything之初，有道定下多个目标。</p><p>&nbsp;</p><p>首先是跨语种，QAnything支持中、英、日、韩四种语种的内容提问；其次要求检索质量足够高，有道在这个环节里优化所有环节的模型；另外是好用，QAnything支持一键安装和本地部署，同时支持各种格式文件，形成智能问答的形式。</p><p>&nbsp;</p><p>“QAnything的下一个发展目标是，突破文档问答单纯形式模态，变成真正能够理解企业业务逻辑、领域知识，融入到业务链条驱动决策提供智能能力的AI大脑。”段亦涛透露，为达成目标，现在他们已对QAnything进行三方面升级，包括领域适配能力、增加Agent的支持和内容生成能力“AI写手”的升级，使得文章分类准确度达到95.9%。</p><p>&nbsp;</p><p>通过领域定制适配，QAnything现在已经突破教育领域，拓展到医疗、互联网、智慧企业等行业；引入Agent能力后，QAnything允许用户根据自己的需求和业务特点来去定义整个系统，每个用户可以用这套能力来去定制个性化机器人。现在，该功能已经在有道领世业务落地。</p><p>&nbsp;</p><p>今年年初，QAnything正式对外开源，四个月在GitHub上获得了近1万个星标。据悉，在此期间，有道平均1-2周升级一次内核版本，不断优化和迭代算法的效果、稳定性和质量。目前，QAnything已经服务了20多个不同行业的上百家客户，超过3万用户将其用于各自的业务领域。</p><p>&nbsp;</p><p>另外，段亦涛表示，大模型向产业化发展后，国产大模型是否够用，取决于期望和要求。现在，大模型能够表现出来的能力，在实际应用中还没有完全被激发出来。“大家都在探索激发模型能力的方式，比如通过RAG等方式辅助其扬长避短，能够在应用中体现价值。”</p><p>&nbsp;</p><p></p><h2>“有道小P”和Hi Echo 3.0</h2><p></p><p>&nbsp;</p><p>会上，有道还展示了子曰教育大模型在教育场景中的最新应用成果，即可以在手机端便捷使用的“有道小P”和在内容、功能、教学等方面进行突破的Hi Echo 3.0。</p><p>&nbsp;</p><p>据悉，有道小P基于大模型知识问答能力，此前已在家庭辅导和语言学习两大教育场景落地。在此基础上，有道此次正式推出小P独立APP，可以在手机端便捷使用，解决随时答疑的需求。</p><p>&nbsp;</p><p>“小P所代表的大模型知识问答能力，有非常大的场景拓展可能性和增长潜力。” 有道高级副总裁吴迎晖表示，有道小P集成了多种场景下的互动答疑与交互功能，同时在知识记忆、多模态理解和逻辑推理等方面实现了提升。</p><p>&nbsp;</p><p>他介绍到，有道对小P基座模型定向优化的同时，还进行了知识库的100%扩容，新增了很多模态以及高质量的语料数据，此外还有很多细节的迭代和优化。</p><p>&nbsp;</p><p>目前，小P有免费和付费两种模式，基础功能是免费的，同时对外提供订阅服务。谈到现在的大模型“价格战”，周枫表示，“我们应用下来，大模型的成本是下降的，基本上一年至少下降一半。对于做云端服务的公司来说，目前降价是市场行为。”</p><p>&nbsp;</p><p>此外，会上亮相的新一代虚拟人口语教练Hi Echo 3.0，在功能、教学模式、虚拟人等方面进行创新，并携手雅思官方上线口语练考服务。此次升级还通过搭建真实的对话场景，重新设计了“背单词”的过程，未来还将推出“儿童模式”，并新增两个全新的“语伴”角色，打造陪伴式的教学环境。</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/H9cy5L42CkYKtrDaWFUr</id>
            <title>都白学了！Mistral 的首个“开放”编程模型，精通Python、C等 80+ 语言，用220 亿参数赢了 GPT-4</title>
            <link>https://www.infoq.cn/article/H9cy5L42CkYKtrDaWFUr</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/H9cy5L42CkYKtrDaWFUr</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 May 2024 07:32:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 微软支持, AI初创公司, Codestral, 编码模型
<br>
<br>
总结: 5月29日，由微软支持、估值60亿美元的法国AI初创公司Mistral发布了其有史以来的第一个用于编码的“开放式”生成式AI模型，称为Codestral。该模型旨在帮助开发人员编写代码并与之交互，具备广泛的语言基础，可以在各种编码环境和项目中为开发人员提供帮助。Codestral在多个基准测试中名列前茅，性能优越，受到开发者社区的积极反馈。JetBrains研究员Mik... </div>
                        <hr>
                    
                    <p>整理&nbsp;|&nbsp;华卫</p><p>&nbsp;</p><p>5月29日，由微软支持、估值&nbsp;60&nbsp;亿美元的法国&nbsp;AI&nbsp;初创公司&nbsp;Mistral&nbsp;发布了其有史以来的第一个用于编码的“开放式”生成式&nbsp;AI&nbsp;模型，称为&nbsp;Codestral。</p><p>&nbsp;</p><p>与其他代码生成模型一样，Codestral&nbsp;旨在通过共享指令和完成&nbsp;API&nbsp;端点，帮助开发人员编写代码并与之交互。由于精通代码和英语，它还可用于为软件开发人员设计高级人工智能应用程序。值得一提的是，&nbsp;Codestral的参数要求很高，还受到一些许可证方面的使用场景限制。</p><p>&nbsp;</p><p>虽然该模型刚刚推出，尚未进行公开测试，但&nbsp;Mistral&nbsp;声称，Codestral&nbsp;在大多数编程语言上已经优于现有的以代码为中心的模型，包括&nbsp;CodeLlama&nbsp;70B、Deepseek&nbsp;Coder&nbsp;33B&nbsp;和&nbsp;Llama&nbsp;3&nbsp;70B。此外，Codestral&nbsp;在Kotlin语言的表现上似乎还超过了GPT-4-Turbo&nbsp;和&nbsp;GPT-3.5-Turbo。</p><p>&nbsp;</p><p></p><h2>精通&nbsp;80+&nbsp;编程语言</h2><p></p><p></p><h2>多个基准测试中名列前茅</h2><p></p><p>&nbsp;</p><p>首先，&nbsp;Codestral具备广泛的语言基础，可以在各种编码环境和项目中为开发人员提供帮助。据悉，Codestral&nbsp;在&nbsp;80&nbsp;多种编程语言的不同数据集上进行了训练，其中包括Python、Java、C、C++、JavaScript&nbsp;和&nbsp;Bash等流行语言。在&nbsp;Swift&nbsp;和&nbsp;Fortran&nbsp;等更特殊的语言上，Codestral&nbsp;也表现出色。</p><p>&nbsp;</p><p>而且，Codestral&nbsp;可以完成编码函数、编写测试和“填写”部分代码，以及用英语回答有关代码库的问题，可为开发人员节省时间和精力。与&nbsp;Codestral&nbsp;的互动，将有助于提高开发人员的编码水平，减少错误和&nbsp;bug&nbsp;的风险。</p><p>&nbsp;</p><p>性能方面，相比之前其他用于编码的模型，Codestral&nbsp;作为&nbsp;22B&nbsp;的模型，在代码生成的性能/延迟空间方面树立了新的标准。Mistral&nbsp;介绍，Codestral&nbsp;拥有&nbsp;32k&nbsp;的较大上下文窗口（竞争对手为&nbsp;4k、8k&nbsp;或&nbsp;16k），在代码生成的远程评估&nbsp;RepoBench&nbsp;中优于所有其他模型。</p><p>&nbsp;</p><p>同时，Mistral&nbsp;将&nbsp;Codestral&nbsp;与硬件要求更高的现有特定代码模型进行了比较。针对Python，其使用了四个基准测试：通过HumanEval&nbsp;pass@1、MBPP&nbsp;sanitised&nbsp;pass@1来评估&nbsp;Codestral&nbsp;的&nbsp;Python&nbsp;代码生成能力；CruxEval来评估&nbsp;Python&nbsp;输出预测能力；RepoBench&nbsp;EM来评估&nbsp;Codestral&nbsp;的远程存储库级代码完成能力。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/c2/c2cd8bf14b8091529981965456e44356.png" /></p><p></p><p>&nbsp;</p><p>在远程存储库级&nbsp;Python&nbsp;代码的完成情况上，Codestral&nbsp;以&nbsp;34%&nbsp;的准确率优于所有三个模型。同样，在评估&nbsp;Python&nbsp;代码生成的&nbsp;HumanEval&nbsp;和测试&nbsp;Python&nbsp;输出预测的&nbsp;CruxEval&nbsp;上，该模型分别以&nbsp;81.1%&nbsp;和&nbsp;51.3%&nbsp;的分数击败了竞争对手。它甚至优于&nbsp;HumanEval&nbsp;上用于&nbsp;Bash、Java&nbsp;和&nbsp;PHP&nbsp;的模型。</p><p>&nbsp;</p><p>为评估在&nbsp;SQL&nbsp;方面的性能，Mistral&nbsp;使用了&nbsp;Spider&nbsp;基准，Codestral&nbsp;以&nbsp;63.5%&nbsp;的得分位居第二。除了&nbsp;Python&nbsp;之外，Mistral&nbsp;还评估了&nbsp;Codestral&nbsp;在六种不同语言的&nbsp;HumanEval&nbsp;pass@1&nbsp;中的表现：&nbsp;C++、bash、Java、PHP、Typescript&nbsp;和&nbsp;C#，并计算了这些评估的平均值。值得注意的是，该模型在HumanEval的C++，C和Typescript上的表现不是最好的，但所有测试的平均得分最高，为61.5%，仅次于Llama&nbsp;3&nbsp;70B的61.2%。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/fb/fb0c4cbb8344ddfb9d1aad9b9d608044.png" /></p><p></p><p>此外，Mistral&nbsp;使用&nbsp;Python、JavaScript&nbsp;和&nbsp;Java&nbsp;中的&nbsp;HumanEval&nbsp;pass@1&nbsp;评估了&nbsp;Codestral&nbsp;的中间填充性能，并将其与&nbsp;DeepSeek&nbsp;Coder&nbsp;33B&nbsp;进行了比较，后者的中间填充能力可立即使用，而&nbsp;Codestral&nbsp;的得分比它更高。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/21/21a3e6c46e1f08dc174826936985712f.png" /></p><p></p><p>&nbsp;</p><p>在开发者社区的反馈中，JetBrains&nbsp;研究员Mikhail&nbsp;Evtikhiev还表示，“我们使用&nbsp;Codestral&nbsp;对&nbsp;Kotlin-HumanEval&nbsp;基准进行了测试，结果令人印象深刻。例如，在&nbsp;T=0.2&nbsp;的通过率方面，Codestral&nbsp;获得了&nbsp;73.75&nbsp;分，超过了&nbsp;GPT-4-Turbo&nbsp;的&nbsp;72.05&nbsp;分和&nbsp;GPT-3.5-Turbo&nbsp;的&nbsp;54.66&nbsp;分。”</p><p>&nbsp;</p><p></p><h2>220&nbsp;亿个参数</h2><p></p><p></p><h2>并不完全对外开放</h2><p></p><p></p><p>根据&nbsp;Mistral&nbsp;的官方介绍，Codestral&nbsp;是一个&nbsp;22B&nbsp;的开放式模型，采用Mistral&nbsp;AI&nbsp;新推出的非生产许可证&nbsp;（MNPL），允许开发人员将其技术用于研究和测试目的，在&nbsp;HuggingFace&nbsp;上可以下载。该公司通过两个&nbsp;API&nbsp;端点提供该模型：codestral.mistral.ai&nbsp;和&nbsp;api.mistral.ai。</p><p>&nbsp;</p><p>前者专为希望在其&nbsp;IDE&nbsp;中使用&nbsp;Codestral&nbsp;的&nbsp;Instruct&nbsp;或&nbsp;Fill-In-the-Middle&nbsp;路由的用户而设计，它带有一个在个人级别管理的&nbsp;API&nbsp;密钥，没有通常的组织速率限制，并且可以在八周的测试期间免费使用。后者则是更广泛的研究、批量查询或第三方应用程序开发的常用端点，查询按令牌计费。</p><p>&nbsp;</p><p>但该模型是否真的“完全开放”，还有待商榷。这家初创公司的非生产许可证禁止将&nbsp;Codestral&nbsp;及其产出用于任何商业活动，虽然有&nbsp;“开发&nbsp;”的例外，但也有注意事项：&nbsp;许可证明确禁止&nbsp;“员工在公司业务活动的背景下进行任何内部使用”。</p><p>&nbsp;</p><p>原因可能是&nbsp;Codestral&nbsp;部分训练内容受版权保护，Mistral&nbsp;在官方博文中没有证实或否认这一点，但这并不奇怪；有证据表明，这家初创公司以前的训练数据集包含受版权保护的数据。</p><p>&nbsp;</p><p>今年3月，由前&nbsp;Meta&nbsp;研究人员创立的&nbsp;AI&nbsp;模型评估公司&nbsp;Patronus&nbsp;AI&nbsp;发布了一项研究，展示了AI&nbsp;模型制作受版权保护内容的频率，测试的四个模型是&nbsp;OpenAI&nbsp;的&nbsp;GPT-4、Anthropic&nbsp;的&nbsp;Claude&nbsp;2、Meta&nbsp;的&nbsp;Llama&nbsp;2&nbsp;和&nbsp;Mistral&nbsp;AI&nbsp;的&nbsp;Mixtral。当时，Patronus&nbsp;AI&nbsp;的联合创始人兼首席技术官&nbsp;Rebecca&nbsp;Qian&nbsp;表示，“我们几乎在评估的所有模型中都发现了受版权保护的内容，无论是开源还是闭源。”</p><p>&nbsp;</p><p>不过无论如何，Codestral&nbsp;的这一问题可能也不值得太麻烦地讨论。据介绍，该模型有&nbsp;220&nbsp;亿个参数，需要一台强大的&nbsp;PC&nbsp;才能运行。(参数从本质上定义了人工智能模型处理问题的能力，比如分析和生成文本）。从参数规模的使用门槛来说，&nbsp;Codestral&nbsp;对大多数开发人员来说或许并不实用，在性能提升方面也是渐进式的。</p><p>&nbsp;</p><p></p><h2>代码模型的使用争议</h2><p></p><p>&nbsp;</p><p>Codestral&nbsp;的出现，可能会引发“关于依赖代码生成模型作为编程助手是否明智”的争论。</p><p>&nbsp;</p><p>至少在某些编码任务中，开发人员肯定会采用生成式AI工具。在&nbsp;2023&nbsp;年&nbsp;6&nbsp;月的一次&nbsp;Stack&nbsp;Overflow&nbsp;民意调查中，44%&nbsp;的开发人员表示，他们现在在开发过程中使用AI工具，26%&nbsp;的开发人员计划不久后使用。然而，需要注意的是，这些工具有明显的缺陷。</p><p>&nbsp;</p><p>今年1月，GitClear&nbsp;收集并分析了&nbsp;2020&nbsp;年&nbsp;1&nbsp;月至&nbsp;2023&nbsp;年&nbsp;12&nbsp;月期间编写的&nbsp;1.53&nbsp;亿行更改的代码。其发现，生成式AI开发工具正在导致更多错误代码被推送到代码库中，且这些助手并没有重构代码，而是提供了一键式重复现有代码的“诱惑”。当时，GitClear&nbsp;指出，2024&nbsp;年的问题是：谁来收拾残局？“对于代码的长期可维护性而言，也许没有比复制/粘贴代码更大的祸害了。”</p><p>&nbsp;</p><p>2月，Snyk&nbsp;的一项新研究警告说，生成式&nbsp;AI&nbsp;驱动的编码助手，如&nbsp;GitHub&nbsp;Copilot等通常会放大用户代码库中现有的错误和安全问题。“简单地说，当Copilot建议代码时，它可能会无意中复制邻居文件中存在的现有安全漏洞和不良做法。这可能导致不安全的编码实践，并为一系列安全漏洞打开大门。”Snyk的开发者关系和社区主管Randall&nbsp;Degges表示，大多数开发人员可能没有意识到AI编码助手可以很容易地从用户的代码库和开源项目中复制现有的安全问题。</p><p>&nbsp;</p><p>在2024计算机-人机交互会议（CHI&nbsp;2024）上，普渡大学的一项研究显示，OpenAI&nbsp;的&nbsp;ChatGPT&nbsp;对编程问题给出的答案，有52%包含错误信息，77%的答案比人类答案更冗长，78%与人类答案存在不同程度的不一致。</p><p>&nbsp;</p><p>但这些研究结果，或许并不能阻止Mistral&nbsp;等公司试图用他们的代码模型来赚钱。</p><p>&nbsp;</p><p>现在，Mistral&nbsp;已经在其&nbsp;Le&nbsp;Chat&nbsp;对话式人工智能平台上推出了托管版&nbsp;Codestral&nbsp;及其付费&nbsp;API。Mistral还表示，将致力于把Codestral构建到LlamaIndex、LangChain、Continue.dev和Tabnine等应用框架和开发环境中。“从我们最初的测试来看，Codestral是代码生成工作流程的绝佳选择，速度快、具有有利的上下文窗口，且&nbsp;instruct&nbsp;版本支持工具使用。”LangChain首席执行官兼联合创始人Harrison&nbsp;Chase在一份声明中表示。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://techcrunch.com/2024/05/29/mistral-releases-its-first-generative-ai-model-for-code/?guccounter=1">https://techcrunch.com/2024/05/29/mistral-releases-its-first-generative-ai-model-for-code/?guccounter=1</a>"</p><p><a href="https://mistral.ai/news/codestral/">https://mistral.ai/news/codestral/</a>"</p><p><a href="https://venturebeat.com/ai/mistral-announces-codestral-its-first-programming-focused-ai-model/">https://venturebeat.com/ai/mistral-announces-codestral-its-first-programming-focused-ai-model/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/6gYD53yUW8bxMXu5Djpk</id>
            <title>让智能设备更懂你，主动式AI正在崛起 | 大模型一周大事</title>
            <link>https://www.infoq.cn/article/6gYD53yUW8bxMXu5Djpk</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/6gYD53yUW8bxMXu5Djpk</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 May 2024 01:58:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能大模型, 主动式AI应用, 大模型持续更新, AGI概念
<br>
<br>
总结: 大模型的快速发展使了解最新技术动态和积极学习成为从业者的必修课。本周人工智能大模型在应用方面取得进展，主动式AI应用集中涌现。科研领域也在积极行动，为大模型透明度与可控性提供重要研究基础。同时，AGI概念引发热议，各行业对AGI的应用程度也备受关注。 </div>
                        <hr>
                    
                    <p>大模型的快节奏发展，让了解最新技术动态、积极主动学习成为每一位从业者的必修课。InfoQ研究中心期望通过每周更新大模型行业最新动态，为广大读者提供全面的行业回顾和要点分析。现在，让我们回顾过去一周的大模型重大事件吧。</p><p></p><h2>一、重点发现</h2><p></p><p>本周，人工智能大模型在应用方面迎来了一系列令人瞩目的进展，特别是主动式AI应用的集中涌现。微软首发的Copilot+PC、小鹏汽车的AI天玑系统、美的发布的主动式全屋智能解决方案、联想集团推出的联想Yoga&nbsp;Slim&nbsp;7x和联想ThinkPad&nbsp;T14s&nbsp;Gen&nbsp;6，都标志着主动式AI展现出强大的应用潜力，智能科技将更加深入地渗透到人们的日常生活中。此外，科研领域也在积极行动，Anthropic为大模型透明度与可控性的提升提供了重要研究基础，“CCF-阿里妈妈科技袋基金”为学术界和工业界的融合交流提供了重要平台，有望促进更多创新成果的诞生。</p><p></p><h2>二、具体内容</h2><p></p><p></p><h3>大模型持续更新</h3><p></p><p>5月22日，百川智能发布最新一代基座大模型Baichuan&nbsp;4，其中通用能力提升超过10%，数学和代码能力分别提升14%和9%。</p><p></p><h4>多模态领域</h4><p></p><p>viva公司推出了一款Sora&nbsp;同架构视频生成模型，该模型对用户免费开放，具备文本生成视频、图片生成视频，4K分辨率放大，提示词自动优化功能。在视频生成方面表现出色，尤其是竖屏视频的质量，文生视频单次可生成&nbsp;5&nbsp;秒视频，图生视频为&nbsp;4&nbsp;秒视频，但在一致性和物理特性模拟上仍有提升空间。</p><p></p><h4>科研领域</h4><p></p><p>5月21日，Anthropic&nbsp;宣布成功从&nbsp;Claude&nbsp;3&nbsp;中提取数百万特征，在理解人工智能模型内部运作机制方面取得进展。该项研究对于提升大模型透明度与可控性方面意义重大。</p><p></p><h3>应用探索</h3><p></p><p></p><h4>新产品新应用/功能</h4><p></p><p>5月21日，在微软Build&nbsp;2024开发者大会上，GitHub&nbsp;推出首套由微软和第三方合作伙伴开发的&nbsp;GitHub&nbsp;Copilot&nbsp;扩展，允许任何人通过自然语言和更广泛的功能来提高代码开发速度。5月21日，微软宣布GPT-4o在Azure&nbsp;AI上普遍可用，还引入了一系列由知名合作伙伴和开源社区开发的大模型，进一步丰富了Azure&nbsp;AI平台的模型库。5月22日，百川智能推出成立之后的首款AI助手“百小应”。百小应不仅能够即时响应用户提出的各类问题，还具备快速阅读文件、整理资料、辅助创作、多轮搜索、定向搜索等功能，并能够在用户问题的基础上通过一系列更细致的提问来明确用户需求，给出更精准的答案。5月22日，腾讯云对外宣布旗下AI代码助手全面对外开放，开发者、开发团队、企业客户都可通过腾讯云官网进行在线体验。5月23日，天猫精灵在新品品鉴会上推出了天猫精灵X6智能音箱。该产品搭载Genie&nbsp;OS，通过人工智能大模型，为用户提供丰富的服务。</p><p></p><h4>智能体</h4><p></p><p>5&nbsp;月&nbsp;21日，微软宣布推出Team&nbsp;Copilot，将Copilot从个人助手扩展到团队助手。Team&nbsp;Copilot在团队协作中能够扮演任何角色，并扩展出Agent能力，成为不同领域的专家。</p><p></p><h4>端侧AI</h4><p></p><p>5月20日，小鹏汽车在520&nbsp;AI&nbsp;DAY发布会上宣布将向用户全面推送AI天玑系统，该系统推送将覆盖小鹏汽车所有在售车型。5月20日，美的正式发布主动式的全屋智能解决方案，并推出五大智慧场景及悦家全屋智能套系新品，全新升级美的Pro会员体系。5月21日，微软首发Copilot+PC。这是一款专为AI设计的新型Windows&nbsp;PC，将旗下AI助手Copilot全面引入了Windows系统，并且内置了OpenAI的GPT-4o模型。Copilot+PC的新功能Recall能够回忆并查找曾在显示屏上出现过的内容，使用Cocreator能够实时翻译约40种语言，实时生成或优化AI图像。Copilot+&nbsp;PC还配备了AI&nbsp;Agent，具备充分的实时交互能力。5月21日，联想集团推出首款搭载高通骁龙X&nbsp;Elite的下一代Copilot+&nbsp;PC——联想Yoga&nbsp;Slim&nbsp;7x和联想ThinkPad&nbsp;T14s&nbsp;Gen&nbsp;6，产品允许用户即使离线也可以使用大型语言模型功能。</p><p></p><h3>其他</h3><p></p><p>5月18日，CCF联合阿里妈妈正式发布“CCF-阿里妈妈科技袋基金”，致力于面向全球高校学者搭建产学研合作平台，增强学术界和工业界的融合交流，为社会和企业带来世界领先的创新成果。</p><p></p><p>报告推荐</p><p>AGI&nbsp;概念引发热议。那么&nbsp;AGI&nbsp;究竟是什么？技术架构来看又包括哪些？AI&nbsp;Agent&nbsp;如何助力人工智能走向&nbsp;AGI&nbsp;时代？现阶段营销、金融、教育、零售、企服等行业场景下，AGI应用程度如何？有哪些典型应用案例了吗？以上问题的回答尽在《中国AGI市场发展研究报告&nbsp;2024》，欢迎大家扫码关注「AI前线」公众号，回复「AGI」领取。</p><p></p><p><img src="https://static001.geekbang.org/infoq/69/69f5f30dc6564327e46c59d969be2524.jpeg" /></p><p></p><p></p><p>报告预告</p><p>金融行业是否找到了AGI应用的最佳路径？取得了哪些具体应用成果?&nbsp;又存在哪些难以逾越的挑战与桎梏？金融机构一定要做AGI建设吗？如何考量金融AGI应用产品的效果？欢迎大家持续关注InfoQ研究中心即将发布的《AGI在金融领域的应用实践洞察》。</p><p></p><p><img src="https://static001.geekbang.org/infoq/59/593f81e592f22792c23938ef704be173.jpeg" /></p><p></p><p></p><h4>活动推荐</h4><p></p><p>FCon&nbsp;全球金融科技大会将于8月16日正式开幕，本次大会主题为「科技驱动，智启未来——激发数字金融内生力」。如您感兴趣，可点击「阅读原文」查看更多详情。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fd1cae14cf901553aa4a65d29376cc26.png" /></p><p></p><p>咨询其他问题请联系票务同学：13269078023，或扫描上方二维码添加大会福利官，可领取福利资料包。</p><p>阅读原文链接：<a href="https://sourl.co/bpxhuz">https://sourl.co/bpxhuz</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/zKfV26EFh09DPoDTAqo8</id>
            <title>奥特曼突然变身OpenAI “安全卫士”！网友：刚被实锤不关心安全还“心理虐待”，谁信啊</title>
            <link>https://www.infoq.cn/article/zKfV26EFh09DPoDTAqo8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/zKfV26EFh09DPoDTAqo8</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 May 2024 09:39:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI公司, 安全小组, GPT-4, 安全与保障委员会
<br>
<br>
总结: OpenAI公司成立了新的安全小组，旨在开发GPT-4的继任模型，并应对离职雇员的批评。该安全小组被称为安全与保障委员会，由公司高层领导和团队负责人组成，将向董事会提出关键安全建议，影响GPT-4模型的开发。新安全委员会的成立可能是为了回应重要人物的离职，公司内部存在安全文化和流程的变化，引发了外界质疑和讨论。资深专家认为应该先关注可见的风险。 </div>
                        <hr>
                    
                    <p>OpenAI公司已经成立新的安全小组，致力于开发GPT-4的继任模型，同时也是为了应对近期多位离职雇员对其商业意图的严厉批评。</p><p>&nbsp;</p><p>该团队被称为安全与保障委员会（Safety and Security Committee，简称SSC），领导层包括OpenAI公司CEO Sam Altman、委员会主席为Bret Taylor，外加Adam D’Angelo与Nicole Seligman等董事会成员。</p><p>&nbsp;</p><p>其他委员会成员则是来自OpenAI下辖各团队的负责人，包括曾经取代公司联合创始人Ilya Sutskever并担任了13天首席科学家的Jakub Pachocki。</p><p>&nbsp;</p><p>OpenAI公司表示，从现在起，该安全团队将就“关键的安全与保障决策”向董事会提出建议。这些决定可能会影响GPT-4继任模型的开发，即OpenAI在公告中提到的“下一个前沿模型”。</p><p>&nbsp;</p><p>在一个名“OpenAI 董事会成立安全委员会”的公告里，插入这样一条重要信息，着实很容易让人联想OpenAI是不是在借此暗暗转移大众视线，毕竟大家对GPT-5的期待是可以盖过对安全的关注的。</p><p>&nbsp;</p><p>该公司解释称，“我们很自豪能够构建并发布在行业拥有领先能力及安全水平的模型，也同样欢迎在这个重要时刻开展激烈的辩论。”但OpenAI并没有介绍具体讨论内容。</p><p>&nbsp;</p><p>这支安全团队的首要任务，就是在90天时间内制定出可供董事会审议的安全建议，不过Altman及其他董事对于建议内容仍拥有最终决定权。当然，OpenAI&nbsp;CEO及其他四位负责人同样可以在提交董事会之前对建议内容施加影响。</p><p>&nbsp;</p><p></p><h2>得到更多质疑：对谁安全？</h2><p></p><p>&nbsp;</p><p>新安全委员会的成立，很可能是为了回应本月早些时候Sutskever与Jan Leike两位重量级人物的高调离职。随着他们离开OpenAI，公司内负责评估长期AI安全问题的超级对齐小组也宣告解散。</p><p>&nbsp;</p><p>在离职之前，Leike一直担任超级对齐小组的负责人。几乎在OpenAI发布通告的同时，Leike宣布加入了Anthropic。Anthropic 由前 OpenAI 工程师创立，创始人出走就是因为双方安全理念存在差异。Leike 在 Anthropic 依旧负责超级对齐。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/e5/e51e9e8df678a3f9f11e4cafc09c7f6f.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>Leike在超级对齐团队解散的前一天曾经表示，“过去几年以来，安全文化和流程已经让位于公司对快速发布新品的坚持。我们早就应该认真思考通用人工智能（AGI）的影响了……OpenAI必须成为一家以安全为先的AGI厂商。”</p><p>&nbsp;</p><p>该发言还引来马斯克的“补刀”：言外之意就是，安全并不是OpenAI现在的首要任务。</p><p>&nbsp;</p><p>但这个新部门的成立并没有扭转网上OpenAI一直以来的负面安全舆论，反而引来了网友更多质疑。“好吧，我想 OpenAI 的产品现在对于 Sam Altman 和他的目标来说是安全的。”有网友略显无奈地说道。</p><p>&nbsp;</p><p>“利益冲突。这样的安全团队从定义上来说难道不应该是独立的吗？”有人质疑道。对此网友调侃成：“是的，应该有一个治理架构，确保首席执行官遵守以下原则……哎呀，他们已经摧毁整个组织架构了。”</p><p>&nbsp;</p><p>也有网友称：“至少 OpenAI 现在有了一个‘安全’团队。”显然还是觉得OpenAI 有些敷衍。</p><p>&nbsp;</p><p>当然，也有人期待这个安全委员会未来会做出什么成绩，毕竟Altman的信徒大有人在。“我仍然可以让 ChatGPT 告诉我如何制造炸弹。所以，是的，我迫不及待地想看到安全进展。”</p><p>&nbsp;</p><p>对此，行内资深专家告诉“AI 前线”，这更多是公司内部资源分配的问题。OpenAI 一直讲闭源才安全，有人认为AGI要来了、机器要毁灭人类，所以安全太重要了，要赶紧把安全做好，需要投入一定比例的资源进去。但是从一个商业公司的角度看，企业不可能停下工作去做各种安全方面的事情，更多还是要不停开发布新的模型，然后满足客户的需求，跟其他公司竞争。</p><p>&nbsp;</p><p></p><h2>前董事会成员“插刀”</h2><p></p><p>&nbsp;</p><p>同样在今天，OpenAI前董事会成员Helen Toner和Tasha McCauley 的联名文章，再次将Altman不关心安全的问题推上浪尖。</p><p>&nbsp;</p><p>“由于Altman个人长期以来的行为模式，董事会维护公司使命的能力受到了越来越大的限制。据我们了解，这些行为不仅削弱了董事会对关键决策和内部安全协议的监督能力，还引发了其他问题。”</p><p>&nbsp;</p><p>根据爆料，多位高层领导私下向董事会表达了深切的担忧，他们认为Altman营造了一种“撒谎的有毒文化”，并涉嫌“心理虐待”行为。Toner 还表示，Altman“多次”向董事会撒谎，并且“隐瞒信息”，她甚至是在 Twitter 上知道 ChatGPT 发布的消息的。</p><p>&nbsp;</p><p>当董事会意识到 Altman 需要被换掉时，Toner 表示，如果Altman 发现了这个，很明显他会“竭尽全力”阻止董事会反对他。她声称他“开始对其他董事会成员撒谎，试图将我从董事会中赶出去。”</p><p>&nbsp;</p><p>“我们非常小心，非常慎重地选择通知谁，除了我们的法律团队之外，几乎没有任何人提前通知过我们，所以这才把消息拖到了 11 月 17 日。”Toner谈及去年的OpenAI政变时说道。</p><p>&nbsp;</p><p>两人指出，自从Altman重返公司以来，一些发展动态令人担忧，包括他重新加入董事会，以及OpenAI一些专注于安全领域的高级人才的离职。这些情况对于OpenAI在自我治理方面的实验来说，似乎预示着一些不利的影响。</p><p>&nbsp;</p><p>有趣的是，刚刚成为亿万富翁不久的Sam Altman 承诺捐出自己大部分财富，表示将继续专注于“支持有助于为人们创造富足的技术”。</p><p>&nbsp;</p><p></p><h2>资深专家：应该先关注看得见的风险</h2><p></p><p>&nbsp;</p><p>没有什么比Sutskever和Leike等人扮演的重要角色更能表明OpenAI致力于其使命的了。Sutskever和Leike是技术专家，他们长期致力于安全，并明显真诚地愿意在必要时要求OpenAI改变方向。</p><p>&nbsp;</p><p>Sutskever 在2019年的采访中当记者刚刚说道，“你们说，‘我们要建立一个通用人工智能，’”时，Sutskever 立即插话强调：“我们将尽一切可能朝这个方向努力，同时确保以一种安全的方式做到这一点。”</p><p>&nbsp;</p><p>随着他们的离职，很多人问他们在OpenAI看到了什么，但没有得到答案。</p><p>&nbsp;</p><p>不同于 Sutskever、Leike等人坚决捍卫AI安全的态度，有些大佬并不那么重视，比如图灵奖得主Yann LeCun。</p><p>&nbsp;</p><p>当时，在LeCun在对Jan Leike的回贴中，他表示当前对AI安全担忧的紧迫感是过度夸张的，类似于在涡轮喷气发动机发明之前，急于解决跨洋飞行的安全问题。所以难怪OpenAI 解散对齐团队。在LeCun看来，智能系统的进化需要多年时间，应该通过反复的工程改进逐步提高其智能和安全性，而不是过度担忧未来可能的风险。</p><p>&nbsp;</p><p>同时，上述专家也告诉“AI 前线”，从开源角度讲，我们离“AGI 来了、毁灭人类”这些还很远，他并不认可这些说法。</p><p>&nbsp;</p><p>该专家表示，目前，AI 安全上的风险更多来自大家看得见、摸得着的地方，比如数据集的偏见和毒化给使用模型带来很多挑战：让ChatGPT 画一个剥了皮的荔枝，由于ChatGPT 根本不知道荔枝剥皮了什么样，所以它就是随便画；又如让Stable Diffusion 等海外模型画北京城市，它会画一个破破烂烂的四合院。</p><p>&nbsp;</p><p>“目前，像这种数据集的 bias 其实没有得到很多关注。但这种可能是更重要的，与超级对齐不是一回事儿，”该专家说道。</p><p>&nbsp;</p><p>该专家也分析称，从更大层面来说，美国也在渲染 AI 安全问题，比如AI自动生成恶意软件、自动攻击各种网站，但实际上我们都知道，代码生成的能力远远没有到这种程度，所以这种渲染也是为了防止模型出口，不让非常厉害的模型技术扩散出去。美国炒作这个事情，也有想要得到中国类似“不利用这个技术开发武器”承诺的意图。</p><p>&nbsp;</p><p>“安全是一个非常大的叙事，每个人在不同的立场都会有不同的看法。”当前应该把主要精力投入到哪个方面？显然OpenAI当前掌门人有自己的答案，其他公司也有自己的答案。但答案正确与否，还需要时间验证。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href="https://openai.com/index/openai-board-forms-safety-and-security-committee/">https://openai.com/index/openai-board-forms-safety-and-security-committee/</a>"</p><p><a href="https://www.theregister.com/2024/05/28/openai_establishes_new_safety_group/">https://www.theregister.com/2024/05/28/openai_establishes_new_safety_group/</a>"</p><p><a href="https://www.businessinsider.com/openai-board-member-details-sam-altman-lied-allegation-ousted-2024-5">https://www.businessinsider.com/openai-board-member-details-sam-altman-lied-allegation-ousted-2024-5</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0CF4oJYiP21jXOVjvkeo</id>
            <title>如何降低数据消费门槛，让非技术用户也能成为数据分析专家？</title>
            <link>https://www.infoq.cn/article/0CF4oJYiP21jXOVjvkeo</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0CF4oJYiP21jXOVjvkeo</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 May 2024 09:05:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据分析, 企业发展, 大模型, SwiftAgent
<br>
<br>
总结: 在数字化时代，数据分析已成为企业发展的重要驱动力。企业需要克服数据获取困难和高阶分析难等挑战，形成基于数据的决策文化，打破数据孤岛，促进跨部门协作。SwiftAgent 的出现实现了数据分析的民主化，让人人都能成为数据分析专家，提高了数据分析的效率和价值。 </div>
                        <hr>
                    
                    <p>在数字化时代，数据分析已成为企业发展的重要驱动力。对于企业而言，数据价值毋庸置疑，更重要的是如何对其进行分析和利用。从客户运营到战略决策，各行各业都离不开数据分析。本期《极客有约》栏目邀请到了数势科技数据智能产品总经理岑润哲，与他一起探讨大模型时代下，如何以 SwiftAgent 革新企业数据分析范式，让人人都能成为数据分析专家。</p><p></p><p>数势科技是行业领先的数据智能产品提供商，为全球大金融、泛零售和高科技制造企业提供大模型增强的智能产品体系，促进企业数字化转型。</p><p></p><p>如何让企业的非技术人员跨越数据的门槛，成为数据分析专家？数据分析的现状和未来发展趋势又如何？SwiftAgent 的出现如何帮助非技术用户？它的技术特点和优势是什么？精彩观点总结如下：</p><p>企业在进行数据分析时面临的主要挑战包括数据获取困难、高阶分析难等，企业数据分析的效率和准确性极具挑战。非技术人员应从业务需求出发，逐步引入数据分析工具和理论，并通过实际操作提升分析思路。企业从组织层面进行变革，形成基于数据的决策文化，打破数据孤岛，并促进跨部门协作。SwiftAgent 通过引入指标语义层和大模型能力，实现了数据分析的民主化，让不同岗位的人员都能实现实时、灵活、精确的数据分析，缩短从数据获取到决策的链路，提高了数据分析的效率和价值。</p><p></p><p>完整视频参看：</p><p></p><p></p><p></p><p></p><h2>企业数据分析的机遇与挑战并存之道</h2><p></p><p></p><p>&nbsp;InfoQ：润哲老师，您在数据分析领域有着非常丰富的经验，您认为企业用户在进行数据分析的时候，通常会遇到哪些困难和挑战？这些挑战对于企业的发展有哪些影响？</p><p></p><p>岑润哲：在我们服务的企业客户中，数据分析一般分为四个步骤。首先是数据收集与获取（Data Query）。企业用户、包括分析师和业务人员在分析前需从数据仓库或业务系统库中提取数据，由于他们通常缺乏数据分析技能，不熟悉 SQL 或底层数据表，这在提取中造成了最大的不便，这是许多客户的痛点。</p><p></p><p>数据分析师或商业分析师具备一定的数据分析技能，会编写 SQL，但底层数据仓库的表结构混乱，熟悉的表有限。当他们完成一个领域的指标分析后，转向另一领域时，需重新梳理表逻辑，这是数据获取阶段的一个难点。</p><p></p><p>第二步是数据获取后，使用专业分析工具或方法进行高阶分析（Data Analytics）。难点不在于选择工具，而在于根据不同场景选用合适的分析范式。例如，销量分析时可能需要同环比分析、排序分析；指标异常时需归因分析能力；转化率异常时需漏斗分析能力。工具学习不难，难的是找到适合业务板块的分析范式。</p><p></p><p>第三步是数据获取和高阶分析后，如何快速解读数据。传统 BI 工具难以直接将数据转化为洞察（insight）。我们的产品结合大模型的语义理解能力，可快速挖掘商业洞察。例如，从几千行 Excel 数据中迅速识别产品或客户群的问题。我们希望结合指标语义层和大模型辅助，优化数据洞察和解读，提供业务方所需的分析解读。</p><p></p><p>最后是数据权限和安全性问题。 若数据权限开放给所有分析师或业务人员，可能引发数据安全性和隐私问题。在数据复盘过程中，需确保不同角色和用户能获取适当权限的数据集。</p><p>总体来说，数据获取、高阶分析、数据解读理解、数据权限管控，是不同行业客户在数据分析中面临的主要问题。</p><p></p><p>&nbsp;InfoQ：这些非技术用户在数据分析的过程中，他们最大的困惑是什么呢？</p><p></p><p>岑润哲：对于非技术人员，我们将其定义为偏业务人员，他们擅长业务流程和合规性，但在将业务思维与底层数据表关联时存在难题。以零售行业为例，非技术人员如门店店长或督导需要分析经营数据、客户画像和商品销售数据，但若直接提供数据表，他们难以进行分析。</p><p></p><p>非技术人员不仅需要工具，更需要将数据分析与业务场景结合。我们认为可以通过指标语义层和大模型的生成能力，帮助客户提出更精准的问题。 例如，构建好指标体系后，大模型能生成结构化问题，如从经营视角分析门店流水和毛利，或从服务水平视角分析大众点评评分，以及客群画像等。</p><p></p><p>当企业内部的指标和维度体系建立完善，结合大模型，能够输出标准的分析思路。这与传统 BI 工具相比，是一个双向过程：大模型不仅能响应问题，还能提供分析思路。比如，大模型可以根据公司的指标和维度生成批量问题，用户再从中筛选最关心的问题进行分析。</p><p></p><p>将大模型分析助手与用户请求结合后，我们从单向的 BI 分析模式转变为可交互模式，用户可以向大模型提问，大模型也能反问用户，提供分析视角。这种模式优化了非技术人员在分析思路上的痛点，大模型的出现增强了分析思路扩展的能力。</p><p></p><p>大模型不仅提供分析思路，还能激发用户的分析欲望，形成良好的交互形式，相当于由顾问提供建议。这是大模型在智能分析领域带来的最大改变。</p><p></p><p>&nbsp;InfoQ：您认为企业在数据分析的过程中还有可能会遇到哪些管理上或者流程上的障碍或者挑战吗？</p><p>岑润哲：结合我们服务过的客户，我总结了三种主要障碍。</p><p></p><p>首先是组织文化的障碍。许多组织尚未形成基于数据的决策文化，决策更多由高层领导凭经验作出，这会影响分析工作的价值。如果组织文化不以数据驱动决策，即使分析质量再高也难以发挥作用。</p><p></p><p>第二是企业内部数据体系的孤岛问题。例如，在泛零售行业，线上线下渠道的数据可能未打通，或埋点数据与交易数据之间存在隔离。这导致无法进行跨部门或跨领域的分析，如无法评估营销活动的效果。</p><p></p><p>第三是跨部门协作问题。不同部门之间可能存在边界和利益问题，例如活动运营部门需要客户运营部门的数据时，可能难以获得必要的支持。这种跨部门协作的障碍，使得进行复杂的分析或关联分析变得困难。</p><p>&nbsp;InfoQ：数势科技就作为数据智能产品的提供方，您认为帮助企业解决这些问题的核心思路是什么呢？企业应该重点关注哪些方面？</p><p></p><p>岑润哲：我们公司的核心理念和使命是更新现行的数据分析范式，从集中式转变为民主化。目前，企业的分析逻辑多是粗犷或集中式的，业务方需向数据团队提出需求，然后等待数据提供。这种模式下，数据解读和高阶分析强烈依赖商业分析师或数据分析师团队，存在较大的隔阂。</p><p></p><p>引入指标语义层和大模型能力后，我们希望企业内部的每个员工能够成为“数据公民”，这意味着他们即使不懂数据，也能基于业务分析思路，获取和探索企业内部的数据资产。大模型的出现有助于每个数据公民进行大规模自定义数据分析，极大缩短从数据获取到决策的链路。</p><p></p><p>未来数据分析的主要方向是从研发与业务割裂的形式，转变为业务方在研发设定标准后，自行利用大模型辅助获取和挖掘数据。</p><p></p><p>&nbsp;InfoQ：您刚刚提到数据公民的概念非常有趣，它会对决策层的思维导向和人才培养产生积极影响吗？</p><p></p><p>岑润哲：是的。例如我们曾为一家鞋类企业提供了 基于指标语义层的完整分析框架， 他们可以分析不同客群的偏好。通过 Know Your Customer 标签，发现 25 至 29 岁女性对 PVC 材质、鞋跟高度在 3 到 5 厘米的鞋子有很高的偏好。这些信息在之前是无法获取的，因为他们不知道公司内部有这些客群和商品标签。</p><p></p><p>现在，借助大模型工具，业务方可以提出更有针对性的问题，并驱动分析过程。他们更了解产品的销售情况，能够通过数据分析找出哪些客群对特定类型或特征的鞋子有更高的转化率，进而讨论投放策略或营销策略，形成一个正循环。</p><p></p><p>传统仅从技术角度分析数据表可能无法获得这样的洞察。但现在，业务方有能力自行分析不同的标签和指标，这使他们能够更好地理解企业内部如何提升销售，实现业务驱动的数据分析和决策。</p><p></p><p>&nbsp;InfoQ：前不久数势科技在 AICon 大会现场发布了 SwiftAgent 2.0 版本，是否可以现场演示？</p><p></p><p>岑润哲：好的，以下是产品的 demo 视频，供大家了解，可留言或点击“阅读原文”申请产品试用。</p><p></p><p></p><p></p><p>InfoQ：在现场发布的时候，数势科技也提到了大模型和 Agent 将会颠覆企业数据分析与决策范式，我想请问为什么这样来表达呢？</p><p></p><p>岑润哲：我们可以回顾一下大模型和 Agent 架构出现之前的数据分析流程。传统上，数据分析链路较长，从提出需求到数据团队获取数据集、配置 BI 工具并搭建驾驶舱，整个过程耗时且复杂。核心问题在于，所有工具的使用都需要人工配置和梳理，工作量较大且重复，效率较为低下。</p><p></p><p>Agent 架构结合大模型后，展现出其优势，尤其是 Agent 在工具调用方面的能力。Agent 不仅能理解用户的自然语言需求，还能自动规划任务执行步骤。</p><p></p><p>例如，用户提出数据分析请求，Agent 可理解用户意图、获取所需地区的销售明细、进行排序和高阶分析、对比 TOP3 产品。这不仅涉及任务拆解和规划，还包括与知识库的协同和工具串联。</p><p></p><p>这种架构带来四个好处：首先，用户不再需要学习工具配置，因为大模型已经掌握了工具调用的方法；其次，通过 Agent 统一规划，提高了效率，避免了在不同工具间切换的繁琐；第三，交互性得到改善，用户通过自然语言与系统交互，降低了使用门槛；最后，简化了操作，将复杂逻辑留给程序处理。</p><p></p><p>企业可通过 Agent 机制调度内部不同工具，形成有效串联，降低了业务方学习和使用工具的时间与门槛，这是我认为它会颠覆企业分析决策范式的原因。</p><p></p><p></p><h2>SwiftAgent 开启智能数据分析新篇章</h2><p></p><p></p><p>InfoQ：SwiftAgent 为什么能够在众多的大模型和数据分析产品中脱颖而出？</p><p></p><p>岑润哲：SwiftAgent 被定义为由大模型 Agent 机制驱动，并结合指标标签语义层的智能分析产品。它让企业非技术人员——如企业管理者和业务人员可准确、即时、个性化地进行数据査询和业务洞察，提升决策能力，实现数据价值普惠化。</p><p></p><p>其核心技术亮点主要分为三个层面：通过构建指标标签语义层，统一了数据和业务语言，避免了大模型的幻觉；结合 Agent 架构，赋予产品反思、推理和规划的能力；通过自研的加速引擎，提升了前端问询的响应速度。</p><p></p><p>指标标签语义层： 我们采取的技术路线不是直接将用户自然语言请求转化为 SQL。因为企业内部数据标注和治理程度不一，直接转化的准确率很低。我们构建了指标和标签语义管理层，统一了数据语言和业务语言，解决了大模型的幻觉问题，提高了准确率，并帮助企业建立了一套指标和标签体系，解决了数据统一问题。</p><p></p><p>Agent 产品架构设计：Agent 架构能够进行思考、推理和反思，解决复杂任务执行问题。自然语言形式的灵活性让用户可能提出不可预测的问题，我们设计了合理的 Agent 架构，使用户能够以自然语言形式灵活、高效地获取数据。我们在 Agent 架构层面做了大量的调研和研发，提升处理复杂问题的能力。</p><p></p><p>数据加速引擎： 我们自研了 Hyper Computing Acceleration Engine，提升对话式分析的响应速度。例如，针对用户常问的商品品类、城市等维度的销售额或毛利，进行预聚合和预计算，使得即使面对百亿级数据量的订单表，也能快速响应用户查询。</p><p></p><p>&nbsp;InfoQ：SwiftAgent 的产品优势 / 壁垒是什么？</p><p></p><p>岑润哲：除了上述提到的 Agent 机制和数据加速引擎，SwiftAgent 还拥有结构化与非结构化数据联动分析的能力。我们将非结构化信息（如用户评论、直播数据）抽象化，转化为结构化数据，并与企业内部指标进行关联分析，提供更全面的分析。</p><p></p><p></p><h2>智能数据分析市场的发展前景</h2><p></p><p></p><p>InfoQ：除了零售行业以外，还有哪些行业已经上线 SwiftAgent？成效如何? 可以分享几个案例吗？</p><p></p><p>岑润哲：除了零售行业，我们也在金融行业如银行、证券公司，以及高端制造行业实现了应用落地。</p><p>以某知名城商为例，分行行长通常关注贷款余额、不良率等指标的波动。传统上，他们需要向分析团队提出需求，由团队提供分析结果。现在，通过上线智能分析产品，领导可以直接通过自然语言查询获取信息，同时经营分析团队可以利用沉淀的分析模板和思路，加速从数据到分析报告的转化。</p><p></p><p>我们也与头部证券机构合作，帮助客户经理分析他们管理的高净值客户。例如，理财顾问或投资经理管理 200 个客户时，可以通过自然语言查询，快速了解哪些客户存在流失风险，或关注行业政策变动，以及持仓标的的变化。这样的分析能力，如果依靠传统 CRM 工具，可能需要花费大量时间。而通过 SwiftAgent 与客户标签、指标联动，构建了从数据洞察到决策的完整链路，效率提升 80%。此外，我们还将优秀客户经理的 SOP 沉淀在知识库中，帮助新员工快速了解如何应对不同情况，比如客户亏损时的安抚策略。这样，数据分析不仅帮助业务人员理解数据，还指导他们基于数据采取行动。</p><p></p><p>&nbsp;InfoQ：对于金融和央国企而言，信创和数据安全是重点关注的方向。数势科技在这两方面有些认证或适配？</p><p></p><p>岑润哲：数势科技是北京信创工委会会员单位，已经完成国家高新技术企业认证、中关村高新技术企业认证、ISO9000 质量管理体系认证、信息安全管理体系认证、信息技术服务管理体系认证、信息系统安全登记保护三级、麒麟操作系统信创认证、达梦数据库信创认证、人大金仓信创认证和 CMMI 等资质认证。产品充分满足金融企业和国央企的部署需求。目前合作的国产大模型都已完成算法备案。另外，SwiftAgent 已首批通过中国信通院针对大模型驱动的数据分析工具的专项测试，获得权威认可。</p><p></p><p>&nbsp;InfoQ：在您看来未来智能数据分析市场规模是将会是怎么样的？</p><p></p><p>岑润哲：智能数据分析市场将是大模型落地的重要场景。数据分析智能化能够充分利用大模型的规划和拆解能力，并与企业内部数据联动，产生化学反应。企业不仅希望提升数据分析体验，还希望降低开发需求，将数据智能化作为核心战略。</p><p></p><p>在客户需求层面，大金融、泛零售和高科技制造是我们的重点服务领域。这些行业的企业对大模型的应用已经从观望学习阶段过渡到试点实施阶段。许多头部企业，尤其是金融、国央企、零售和能源企业，已经开始大量招标，希望在数据分析、知识库、营销和 RPA 等多个场景中应用大模型。数据分析场景特别受到重视，占企业需求的 80% 以上。</p><p></p><p>另外，国家层面也在推动数据资产入表，鼓励企业将数据资产作为无形资产量化并反映在财务报表中，这也将促进大模型在数据管理和分析领域的结合。</p><p></p><p>&nbsp;InfoQ：您作为资深专家，请问对于想要提高数据分析能力的非技术用户，有哪些建议呢？</p><p></p><p>岑润哲：首先，非技术用户应以业务需求为出发点，学习基础统计学和数据分析概念，构建分析能力的基础。例如，零售客户，可从业务场景切入，如教店长如何分析门店数据以提升业绩。</p><p></p><p>其次，非技术人员应先理解自身管理的业务逻辑流程，再逐步引入数据分析工具和理论。 建议通过实际操作小项目来提升分析思路，如门店经理、财务经理或 HR 可以分析与自己工作相关的数据。</p><p></p><p>最后，还可以加入专业论坛或群体，关注不同领域的分析博客，以帮助构建人脉并提升对数据的理解力。 业务人员转为分析师往往潜力巨大，因为他们对业务流程有深刻理解，具备在数据分析和业务洞察领域的天然优势。</p><p></p><p>数据分析不仅限于互联网公司或运营、财务风控等领域，数据分析将持续渗透到企业各个部门，提升决策效率，这是未来的大趋势，也是我们数势科技的愿景和使命。</p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/v0Y1tppWvfGPvtpMqgTY</id>
            <title>干货下载 | 腾讯云ES RAG如何支持微信读书实现“AI问书”？</title>
            <link>https://www.infoq.cn/article/v0Y1tppWvfGPvtpMqgTY</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/v0Y1tppWvfGPvtpMqgTY</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 May 2024 06:19:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/95/95248354db49afa8d5efac3f71431c10.webp" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/e8/e8076563d8941632e9a8f79eed8fcc96.webp" /></p><p></p><p>干货下载页面点击：<a href="https://qdrl.qq.com/TJBspHYw">https://qdrl.qq.com/TJBspHYw</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/fXOqLr5i5lGx8mqJacPf</id>
            <title>浪潮信息发布 “源2.0-M32” 开源大模型，大幅提升模算效率</title>
            <link>https://www.infoq.cn/article/fXOqLr5i5lGx8mqJacPf</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/fXOqLr5i5lGx8mqJacPf</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 May 2024 02:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 源2.0-M32, 门控网络, 专家模型, 模算效率
<br>
<br>
总结: 源2.0-M32是一个包含32个专家的混合专家模型，采用了门控网络结构来调度专家，实现高效计算。在模型训练和推理过程中，源2.0-M32表现出色，主要通过建模专家之间的协同关系来提升模型精度和模算效率。 </div>
                        <hr>
                    
                    <p>5月28日，浪潮信息发布“源2.0-M32”开源大模型。“源2.0-M32”在基于“源2.0”系列大模型已有工作基础上，创新性地提出和采用了“基于注意力机制的门控网络”技术，构建包含32个专家（Expert）的混合专家模型（MoE），并大幅提升了模型算力效率，模型运行时激活参数为37亿，在业界主流基准评测中性能全面对标700亿参数的LLaMA3开源大模型。</p><p></p><h3>大模型技术解读</h3><p></p><p></p><p>在算法层面，源2.0-M32提出并采用了一种新型的算法结构：基于注意力机制的门控网络（Attention Router），针对MoE模型核心的专家调度策略，这种新的算法结构关注专家模型之间的协同性度量，有效解决传统门控网络下，选择两个或多个专家参与计算时关联性缺失的问题，使得专家之间协同处理数据的水平大为提升。源2.0-M32采用源2.0-2B为基础模型设计，沿用并融合局部过滤增强的注意力机制（LFA, Localized Filtering-based Attention），通过先学习相邻词之间的关联性，然后再计算全局关联性的方法，能够更好地学习到自然语言的局部和全局的语言特征，对于自然语言的关联语义理解更准确，进而提升了模型精度。</p><p></p><p><img src="https://static001.geekbang.org/infoq/77/7749b68f0ae25cc003fe38680e899857.png" /></p><p>Figure1-&nbsp;基于注意力机制的门控网络（Attention Router）</p><p></p><p>在数据层面，源2.0-M32基于2万亿的token进行训练、覆盖万亿量级的代码、中英文书籍、百科、论文及合成数据。大幅扩展代码数据占比至47.5%，从6类最流行的代码扩充至619类，并通过对代码中英文注释的翻译，将中文代码数据量增大至1800亿token。结合高效的数据清洗流程，满足大模型训练“丰富性、全面性、高质量”的数据集需求。基于这些数据的整合和扩展，源2.0-M32在代码生成、代码理解、代码推理、数学求解等方面有着出色的表现。</p><p></p><p>在算力层面，源2.0-M32采用了流水并行的方法，综合运用流水线并行+数据并行的策略，显著降低了大模型对芯片间P2P带宽的需求，为硬件差异较大训练环境提供了一种高性能的训练方法。针对MoE模型的稀疏专家计算，采用合并矩阵乘法的方法，模算效率得到大幅提升。</p><p></p><p>基于在算法、数据和算力方面全面创新，源2.0-M32的性能得以大幅提升，在多个业界主流的评测任务中，展示出了较为先进的能力表现，在MATH（数学竞赛）、ARC-C（科学推理）榜单上超越了拥有700亿参数的LLaMA3大模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/18/18c12d432b7f6f84dcb3fc48081ae17a.png" /></p><p>Figure2 源2.0-M32业界主流评测任务表现</p><p></p><p>源2.0-M32大幅提升了模型算力效率，在实现与业界领先开源大模型性能相当的同时，显著降低了在模型训练、微调和推理所需的算力开销。在模型推理运行阶段，M32处理每token所需算力为7.4GFLOPs，而LLaMA3-70B所需算力为140GFLOPs。在模型微调训练阶段，对1万条平均长度为1024 token的样本进行全量微调，M32消耗算力约0.0026PD(PetaFLOPs/s-day)，而LLaMA3消耗算力约为0.05PD。M32凭借特别优化设计的模型架构，在仅激活37亿参数的情况下，取得了和700亿参数LLaMA3相当的性能水平，而所消耗算力仅为LLaMA3的1/19，从而实现了更高的模算效率。</p><p></p><p>浪潮信息人工智能首席科学家吴韶华表示：当前业界大模型在性能不断提升的同时，也面临着所消耗算力大幅攀升的问题，对企业落地应用大模型带来了极大的困难和挑战。源2.0-M32是浪潮信息在大模型领域持续耕耘的最新探索成果，通过在算法、数据、算力等方面的全面创新，M32不仅可以提供与业界领先开源大模型相当的性能，更可以大幅降低大模型所需算力消耗。大幅提升的模算效率将为企业开发应用生成式AI提供模型高性能、算力低门槛的高效路径。</p><p></p><h3>技术创新点剖析：</h3><p></p><p></p><p>Llama系列模型的精度从Llama1到Llama3显著提升，Llama3的精度处于领先地位，特别是其700亿参数的模型在每个Token的推理和算力上达到140GFLOPS。尽管如此，Llama3在推理时的算力开销较大，也就是说单位算力下的精度表现较差。</p><p></p><p>在采访环节，吴韶华回答了记者问，关于32个专家的优势及挑战，吴韶华解释道，当前很多大模型工作采用8个专家的架构，但浪潮信息选择32个专家，核心原因是模算效率。实验表明，在他们的LFA加上Attention Router架构中，专家数量从8增加到32时，精度显著提升，而算力开销保持不变。这是因为激活专家的数量仅为2个。此外，单个专家参数量为2B，这样控制模型参数量有利于企业应用的模算效率。结果显示，这一选择在相同精度下实现了低算力消耗。</p><p></p><p>同时，由于激活的专家数量为2个，通过Attention Router机制考虑专家间协同，专家数量的增加使得每个专家或专家组能够学习更多有针对性的信息。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/20/9d/20109819dd1cb3e6794a287d3648e49d.jpg" /></p><p></p><p>模算效率与成本控制也是此次大模型发布的关键讨论点。吴韶华强调，算力是当前大模型发展的核心瓶颈。MoE结构模型通过扩展专家数量，在固定算力下获得更高精度。在多元芯片的使用上，浪潮信息的EPAI软件提供相关工具，支持多元算力架构，降低用户迁移设备的难度和成本。这些创新措施有助于降低用户试错成本，实现应用落地。</p><p></p><p>高模算效率意味着在单位算力投入下获得更高的精度回报，这对于大模型训练和推理都非常有利。“源2.0-M32”模型旨在通过创新算法提升精度并降低同等精度下的算力开销，大幅提升基础模型的模算效率。“源2.0-M32”是一个包含32个专家的混合专家模型，采用了Attention Router结构来调度专家，实现高效计算。在模型训练和推理过程中，“源2.0-M32”表现出色，Attention Router结构主要是通过建模专家之间的协同关系来提升模型精度。</p><p></p><p>M32模型的训练数据筛选与优化也是核心技术点，吴韶华详细介绍了浪潮信息在训练数据方面的策略。浪潮信息从源1.0开始构建了互联网自然语言文库，并开发了一套数据清洗平台。对于稀缺数据（如中文数学数据），通过数据合成工具补充。M32模型引入了大量代码数据和互联网数据，提升数据的多样性和质量。代码数据不仅对模型的代码能力有益，还能帮助解决数学问题和推理问题。最终，源2.0-M32模型在精度和算力开销方面优于Llama3。</p><p></p><p>在应用落地方面，源2.0-M32增强了小样本学习能力，通过少量样本就能显著提升模型能力。相较于微调而言，这是一种轻量化支撑大模型应用落地的有效技术。</p><p></p><p>MoE模型对企业开发应用和大模型普惠的影响也逐渐展现，吴韶华向大家介绍说，MoE模型除了提升算力效率外，还能提高精度，降低使用成本，增强模型能力。MoE模型通过激活少量专家，保持算力开销低，同时允许训练更多Token，进一步提升精度。对于终端用户来说，关键在于解决实际问题和降低使用成本。例如，在智能客服等应用中，用户更愿意花费较少的钱解决具体问题，而不会购买高成本的大模型。</p><p></p><p>最后，吴韶华补充了大模型落地与微调的观点，大模型在应用落地时需要进行微调，这是由于预训练阶段的数据和模型能力存在局限性。微调能有效应对不同的行业需求，但算力需求较大。同时，推理阶段也是算力开销大户，因此高效的模型结构和更强的能力在实际应用中具有优势。浪潮信息通过内部实际应用场景，如客服、软件研发、运维等，不断积累经验，提升模型能力，满足更多用户需求。</p><p></p><h3>回顾与展望：</h3><p></p><p></p><p>回顾大模型的发展历史，我们可以看到，2020年GPT-3的发布点燃了大模型的热潮。从2020年到2022年，业界在大模型能力上进行了广泛的探索。例如，2022年推出了GPT强化学习方法，使大模型与人的意图对齐，建立了良好的发展思路。同年末，ChatGPT问世，引发了大模型应用的热潮，成为增长最快且被广泛接受的大模型应用。此后，Llama系列模型陆续推出，2024年大模型的发展速度进一步加快。</p><p></p><p>浪潮信息的大模型研究始于2020年GPT-3发布后。2021年，他们发布了第一个大模型“源1.0”，拥有2457亿参数。2022年，进行了应用落地探索，运用了检索类技术和RAG技术。2023年，发布了“源2.0”，并推出了“源2.0-M32”混合专家结构模型。</p><p></p><p>关于大模型推广及触达用户，吴韶华介绍了浪潮信息大模型落地的两个方向：外部客户和内部需求。对外，浪潮信息通过与合作伙伴在EPAI平台上合作，提供开源模型支持，增强用户体验。对内，浪潮信息在多个业务场景中应用大模型，解决内部需求问题的同时积累经验，提升算法和工具性能，从而更好地服务外部客户。</p><p></p><p>未来，M32开源大模型配合企业大模型开发平台EPAI（Enterprise Platform of AI），将助力企业实现更快的技术迭代与高效的应用落地，为人工智能产业的发展提供坚实的底座和成长的土壤，加速产业智能化进程。</p><p></p><p>最后，吴韶华宣布，浪潮信息已在GitHub和Hugging Face上开源了代码和模型，并发表了相关论文。</p><p></p><p>源2.0-M32将持续采用全面开源策略，全系列模型参数和代码均可免费下载使用。</p><p>代码开源链接：<a href="https://github.com/IEIT-Yuan/Yuan2.0-M32">https://github.com/IEIT-Yuan/Yuan2.0-M32</a>"</p><p>模型下载链接：<a href="https://huggingface.co/IEITYuan/Yuan2-M32-hf">https://huggingface.co/IEITYuan/Yuan2-M32-hf</a>"</p><p><a href="https://modelscope.cn/models/YuanLLM/Yuan2-M32-hf/summary">https://modelscope.cn/models/YuanLLM/Yuan2-M32-hf/summary</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/205tMHA6eOSsVyS7jOla</id>
            <title>别再危言耸听！大多数被评为“严重”的Bug评级具有误导性</title>
            <link>https://www.infoq.cn/article/205tMHA6eOSsVyS7jOla</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/205tMHA6eOSsVyS7jOla</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 May 2024 13:39:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: CVSS评级, 安全团队, 软件供应链, AI/ML工具
<br>
<br>
总结: JFrog发布的调查结果显示，大多数CVSS评级在实际情况下并不适用，但安全团队仍花费大量时间修复漏洞。报告指出安全问题会影响工作效率，同时揭示了软件供应链安全和AI/ML工具在安全领域的应用不成比例。JFrog提供的安全方案聚焦于统一平台管理，为企业提供了高性价比的解决方案。 </div>
                        <hr>
                    
                    <p></p><blockquote>74%被列为“高”或“严重”的CVSS评级在大多数常见情况下并不适用，但有60%的安全和开发团队仍花费25%的时间修复这些漏洞。</blockquote><p></p><p>&nbsp;</p><p>近日，流式软件公司、JFrog软件供应链平台背后的公司JFrog&nbsp;发布了其 《2024年全球软件供应链发展报告》的调查结果，指出了新兴的发展趋势、行业风险以及保障企业软件供应链安全的最佳实践案例。</p><p>&nbsp;</p><p>JFrog首席技术官兼联合创始人Yoav Landman表示：“软件安全领域变幻莫测，全球的DevSecOps团队都在探索前行，在AI迅速普及的时代，更需要创新来满足需求。我们的数据涵盖了迅速发展的软件生态系统，为安全和开发组织提供了一个更为全面的介绍，包括值得关注的CVE评级错误、使用生成式AI进行编码所带来的安全影响相关洞察、允许组织用于开发的高风险软件包等信息，以便相关人员做出更明智的决策。”</p><p>&nbsp;</p><p>JFrog的《2024年全球软件供应链发展报告》结合了超过7000家企业的JFrog Artifactory开发者使用数据、JFrog安全研究团队原创的CVE分析、以及委托第三方对全球1200名技术专业人士进行的调查数据，旨在为快速发展的软件供应链领域提供信息参考。主要研究结果包括：</p><p>&nbsp;</p><p>并非所有CVE都如表面所见：传统的CVSS评级仅关注漏洞利用的严重性，而非其被利用的可能性，后者需要结合具体情境才能做出有效的评估。JFrog安全研究团队在分析了2023年发现的212个高知名度CVE后，平均将85%的“严重”CVE和73%的“高危”CVE的重要性评级下调。此外，JFrog发现，在报告的前100个Docker Hub社区镜像中，74%的CVSS评级为“高危”和“严重”的常见CVE实际上是无法被利用的。</p><p>&nbsp;</p><p>拒绝服务（DoS）攻击盛行：JFrog安全研究团队分析的212个高知名度CVE中，有44%存在发起DoS攻击的潜在威胁；17%存在执行远程代码（RCE）的潜在威胁。这对于安全组织来说是个好消息，因为RCE由于能够提供对后端系统的完全访问权限，与DoS攻击相比，其危害性更大。</p><p>&nbsp;</p><p>安全问题会影响工作效率：40%的受访者表示，通常需要一周或更长时间才能获得使用新软件包/库的批准，这延长了新应用程序和软件更新的上市时间。此外，安全团队大约耗费25%的时间用于修复漏洞，即使这些漏洞的风险在当前情况下可能被高估或甚至无法被利用。</p><p>&nbsp;</p><p>在软件开发生命周期（SLDC）中采用安全检查方式的差异性&nbsp;——当涉及到决定在软件开发生命周期中的哪个阶段采取应用安全测试时，行业内存在明显分歧，这突显了同时进行左移和右移的重要性。42%的开发人员表示，最好在编写代码过程中执行安全扫描，而41%的开发人员认为最好在新软件包从开源软件（OSS）库引入企业之前执行扫描。</p><p>&nbsp;</p><p>安全工具的过度使用现象仍在持续&nbsp;——&nbsp;近半数IT专业人士（47%）表示他们部署了四到九种应用安全解决方案。然而，有三分之一的调查对象和安全专业人士（33%）表示，他们正在使用十种乃至更多的应用安全解决方案。这一现象反映出市场对于安全工具整合的需求趋势，同时也表明人们正逐渐放弃单一的点对点解决方案，转而寻求综合性更高的安全工具集成。</p><p>&nbsp;</p><p>AI&nbsp;/&nbsp;ML工具在安全领域的应用不成比例&nbsp;——尽管有90%的受访者表示，他们的企业目前以某种形式使用AI&nbsp;/&nbsp;ML驱动的工具来协助安全扫描和修复工作，但只有三分之一的专业人士（32%）表示他们的组织使用AI&nbsp;/&nbsp;ML工具来编写代码。这反映出业内大多数人对AI生成的代码可能会为企业软件带来的潜在安全隐患仍持审慎态度。</p><p>&nbsp;</p><p>尽管新发布的报告揭示了被列为“高”或“严重”的CVSS评级在大多数常见情况下并不适用，但企业对于软件供应链的安全意识，一刻也不应放松。以JFrog为例，其提供的安全方案聚焦于以统一的平台去实现管理，且不限制用户数，顺应了很多企业的需求。同时，作为JFrog的一大产品特点，JFrog Xray和制品库是进行统一绑定的，即企业使用了JFrog的制品库，就无需额外购买JFrog Xray，会自动获得安全扫描的能力。这进一步帮助企业的安全团队减少了工具安全扫描维护和采购的成本对的同时，还能够帮助企业在安全扫描、制品管理、供应链管理上提供统一的高性价比解决方案。&nbsp;</p><p></p><p>JFrog安全研究高级总监Shachar Menashe表示：“虽然安全漏洞的数量每年都在增加，但这并不意味着其严重性也在同步上升。显然，IT团队愿意投资于新工具以提升安全性，但了解如何部署这些工具、如何有效利用团队时间以及简化流程，对于确保软件开发生命周期（SDLC）的安全至关重要。我们编制这份报告的目的不仅仅在于分析趋势，更是为了当技术业务领导者在针对AI导航、恶意代码或安全解决方案等方面制定决策时，能够为其提供清晰的指导和专业的技术咨询。”</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/WoY69us6292NWvmx99Ot</id>
            <title>谷歌刚刚更新了算法，顺便搞毁了几家公司</title>
            <link>https://www.infoq.cn/article/WoY69us6292NWvmx99Ot</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/WoY69us6292NWvmx99Ot</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 May 2024 13:29:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 谷歌更新算法, 公司毁灭, AI功能, 搜索引擎
<br>
<br>
总结: 谷歌最近的算法更新对一些公司造成了毁灭性的影响，特别是那些依赖谷歌搜索引擎的公司。谷歌的更新带来了更强大的AI功能，但也导致了一些原本排名靠前的网站被挤出搜索结果页面，给他们的业务带来了巨大打击。受影响的公司包括HouseFresh和Ready Steady Cut等。这些变化引发了对谷歌算法更新是否真的有助于网络的质量和用户体验的质疑。 </div>
                        <hr>
                    
                    <p></p><h2>谷歌更新算法，毁了多家公司</h2><p></p><p>&nbsp;</p><p>过去两年以来，谷歌搜索的一系列更新为这款互联网上最强大的工具带来了巨大变革，更配备了前所未有的AI功能。但最近，互联网上越来越多声音质疑，谷歌的一系列变化是在拯救网络，还是会将其推向毁灭？</p><p>&nbsp;</p><p>如果大家在谷歌引擎中输入过“空气净化器测评”，那想要获取的很可能是HouseFresh.com上的内容。该网站由Gisele Navarro和她的丈夫于2020年建立，整理了过去十年间改善室内空气质量的所有产品使用感受。他们在地下室里装满了各种净化设备，开展严格的科学测试，并撰写文章来帮助消费者们厘清思路、辨别炒作。</p><p>&nbsp;</p><p>HouseFresh就是由独立内容发布方推动建立活跃行业的典型案例。这些发布方所产出的原创内容，也正是谷歌长期以来号称应当推广的核心价值。实际上，就在该网站上线后不久，这家科技巨头就开始在搜索结果顶部显示HouseFresh。这让该网站迅速发展成一家欣欣向荣的企业，拥有15名全职员工。Navarro自己也为公司设定了颇具雄心的发展规划。</p><p>&nbsp;</p><p>但在不久后的2023年9月，谷歌对其搜索引擎算法展开了一系列重大更新。</p><p>&nbsp;</p><p>Navarro坦言，“这直接毁掉了我们的业务。一夜之间，本来指向HouseFresh的搜索词开始将人们引导至各大生活方式杂志，可这些杂志明显没有实际测试过产品。那里的文章中充斥着我一望而知的错误信息。”</p><p>&nbsp;</p><p>谷歌又在今年3月再次更新算法，这次造成的影响更大。HouseFresh的访客数量由每天数千人次锐减至数百人次。Navarro表示“我们完全被压垮了。”过去几周以来，HouseFresh网站不得不解雇掉大部分团队成员。她承认，如果后续情况没有好转，这家网站将唯有关闭一途。</p><p>&nbsp;</p><p>受影响的不止HouseFresh一家公司。</p><p>&nbsp;</p><p>英国娱乐新闻网站Ready Steady Cut的主编Daniel Hart也控诉谷歌改变搜索算法带来的影响可谓立竿见影。</p><p>&nbsp;</p><p>Hart解释道，“自从谷歌去年9月的更新之后，我们的流量当场减半，而且情况正变得越来越糟。我们不仅受到大网站内容的冲击，同时也正在被窃取我们内容的垃圾网站所取代。这样的整改毫无意义。”在接下来的几个月间，收入缩水已经迫使该网站将原本20人的作家与编辑团队裁撤至4人。</p><p>&nbsp;</p><p>谷歌方面的发言人则表示，该公司最近的更新已经给垃圾邮件和非原创内容造成了重大打击。谷歌也一直在密切关注导致搜索信息质量低下的滥用行为。</p><p>&nbsp;</p><p>谷歌算法更新之后，该公司向网站所有者发布了提示，号称能帮助其维持住搜索流量。但Hart指出，他们的网站聘请了顾问、重点关注谷歌的建议，而且不眠不休地更新网站。但经过近一年的努力，还是没有什么帮助。他表示，“过去8个月间，我浪费掉了宝贵的生命来努力遵循谷歌的建议。谷歌声称他们希望让网络用户能从掌握第一手经验和具备相关背景的人们那获取内容，可我们完全符合这样的标准。总之如今的情况实在让人心碎。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/b3/b323b0d0bb06674cb3e1d7df09ce40c7.png" /></p><p></p><p>部分案例显示，谷歌搜索近期的变化正在对各类网站产生惊人的影响。</p><p>&nbsp;</p><p>谷歌一位发言人则在采访中强调，该公司的所有搜索算法调整都是在经严格测试验证、确认对用户有所帮助之后才会落地，而且谷歌方面也为各网站所有者提供了协助、资源和机会，允许其就搜索排名问题提出反馈。</p><p>&nbsp;</p><p>但批评人士认为，实际情况可能恰恰相反。随着谷歌重新调整其算法并使用AI将搜索引擎转化为搜索与回答引擎，不少人担心对于那些专司产出用户喜爱内容的企业来说，造成的冲击恐怕不亚于物种灭绝级别的事件。</p><p>&nbsp;</p><p>谷歌坚定认为这些变化将给整个网络带来好处，而搜索算法的调整只是个开始。</p><p>&nbsp;</p><p>至少有一点可以肯定：谷歌在AI上所做出的努力，将对大部分网民在网络上看到的内容产生深远影响。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/d3/d3f93cd99c3a60b293dd2f2f0fc66fce.png" /></p><p></p><p>过去两年以来，所谓能让搜索变得更加“有用”的一系列更新，正在令努力遵循谷歌最佳实践的网站所有者感到沮丧。</p><p>&nbsp;</p><p></p><h2>谷歌AI工具“已读乱回”，工程师无奈手动删除</h2><p></p><p>&nbsp;</p><p>对搜索算法“动刀”后，谷歌最近又将注意力放到了AI工具上。这不，日前谷歌正因为社交媒体上充斥着的AI工具闹出的乌龙而忙得不可开交。</p><p>&nbsp;</p><p>这款谷歌新推出的AI综述（AI Overview）经常会胡言乱语，有时候让用户往披萨上抹胶水、有时候建议他们吃石头。面对这款匆忙上线产品搞出的麻烦，再加上互联网用户们用各种表情包大加嘲讽，谷歌正忙于手动禁用AI综述上的特定搜索内容。也正因为如此，很多表情包在被发上社交网络后不久就神奇地消失了。谷歌公司承认正“迅速采取行动”，旨在清除AI工具给出的一些奇怪答案。</p><p>&nbsp;</p><p>这样的现状着实令人摸不着头脑。毕竟谷歌测试其AI综述功能也有一年之久了——该功能早在2023年5月就以搜索生成体验的名号推出了beta版。谷歌CEO桑达尔·皮查伊更放出豪言，称该公司在测试期间已经支持了超过10亿条查询。</p><p>&nbsp;</p><p>而且皮查伊也提到，“在硬件、工程和技术突破的共同推动下”，谷歌同期将AI回答的交付成本降低了80%。看起来似乎是成本优化来得太早，而生成技术本身并没有做好准备。</p><p>&nbsp;</p><p>事实上，这款AI工具（AI Overview）是前不久皮查伊在公司年度开发者大会上，向众人宣布了其搜索引擎发展历史上最重大的举措之一。皮查伊表示，展望未来，谷歌搜索将针对诸多问题给出自己的AI生成答案。这项名为“AI综述（AI Overviews）”的功能已经面向美国用户推出。皮查伊指出，“这是一款能切实服务用户的产品。谷歌搜索由此成为建立在人类好奇心之上的生成式AI成果。”</p><p>&nbsp;</p><p>谷歌方面还表示，其AI综述产品旨在向用户输出“高质量信息”。谷歌发言人Meghann Farnsworth在采访邮件中回应称，“我们看到的许多案例都不属于常见查询，而且发现了不少被篡改或者无法重现的案例。”Farnsworth同时证实，谷歌方面正在“迅速采取行动，在符合内容政策的前提下适当删除某些AI综述查询，并利用这些案例对我们的系统进行广泛改进。部分改进结果已经在实际使用中得到体现。”</p><p>&nbsp;</p><p>由此可见，其实谷歌也承认了AI工具可能会提供不准确信息，但表示正在不断努力改进结果。谷歌公司发言人指出，AI综述的内容通常整理自多个网页，而非单一来源，而且响应结果会突出显示相关链接。这位发言人还提到，内容发布方可以在网页上使用特殊标签来控制AI综述是否列出相关网站链接。但需要注意的是，一旦AI模型抓取了创作者的内容，该数据可能将无法被删除。</p><p>&nbsp;</p><p>AI综述只是过去两年以来，谷歌对其核心产品做出的一系列重大改变中的一环。该公司表示，其最近针对搜索算法做出的改进努力将开启一个令人兴奋的技术新时代，并有助于解决困扰网络世界的诸多问题。</p><p></p><h2>成也搜索，败也搜索</h2><p></p><p>之所以要推动这些变化，是因为谷歌意识到此前的网络一直存在弊端。如果大家使用过搜索引擎，对此肯定也有切身体会。互联网的运作长期由所谓“搜索引擎优化（SEO）”所主导，这项技术旨在调整文章及网页内容，以便更好地被谷歌搜索发现并优先显示。谷歌甚至在为网站所有者提供SEO技巧、工具和建议。对于数百万将业务建立在机械化搜索体系之上的企业来说，SEO就是一笔他们不得不承受的“技术税”。</p><p>&nbsp;</p><p>问题在于，搜索引擎优化可能会被滥用。抱有野心的网站所有者也越来越多地意识到，相较于服务人类用户，专门制作适合谷歌筛选算法的内容才是增加经济收益的不二法门。</p><p>&nbsp;</p><p>谷歌针对垃圾搜索结果的战争已经愈演愈烈。2022年，该公司对其算法发布了“实用内容更新”，旨在淘汰纯为提升搜索排名而创建的内容。谷歌随后一截2023年9月发布后续更新，并在今年3月再次出手调整算法。谷歌方面表示，结果是“搜索结果中低质量、非原创内容减少了45%。”这似乎代表着一次巨大的成功。</p><p>&nbsp;</p><p>谷歌一位发言人在采访中表示，“我们最近的更新，希望将人们与来自网络的各类不同网站上的实用、令人满意且原创性的内容联系起来。在努力改进搜索服务的同时，我们还将继续专注于为网站提供有价值流量，以支持健康、开放的网络环境。”</p><p>&nbsp;</p><p>但谷歌的一系列改变，包括算法的更新和近期AI工具的出现，都没有博得什么好印象。</p><p>&nbsp;</p><p>一位不愿透露姓名的AI业务创始人在采访中表示，“谷歌曾经是一家以引领前沿、提供高质量产品的行业龙头，如今却不断发布各种质量低下的产品，甚至沦为整个互联网的笑柄和玩梗对象。”</p><p>&nbsp;</p><p>AI专家、纽约大学神经科学名誉教授Gary Marcus则在采访中表示，不少AI厂商都是在“兜售梦想”，希望更多人相信这项技术的正确率终将从80%提升至100%。Marcus强调，初步实现80%的正确率相对简单，因为其中涉及大量人类数据，其正确率天然就在这个区间。但弥合这最后20%的差距却极具挑战。实际上，Marcus认为这最后20%很可能是条死胡同。</p><p>&nbsp;</p><p>Marcus坦言，“对于很多问题，必须要经过相应的推理步骤才能判断当前事件是否可信、信息来源是否合法。而要想像人类审核员那样解决问题，恐怕首先要真正实现通用人工智能（AGI）。”Marcus本人和Meta公司的AI负责人Yann LeCun也都认定，为当前AI系统（包括谷歌Gemini和OpenAI GPT-4）提供支持的大语言模型并不是实现AGI的正确答案。</p><p>&nbsp;</p><p>这对谷歌来说，现在面临的处境无疑是十分艰难的。毕竟微软已经抢先一步，通过Bing大力推广生成式AI技术。另据报道，OpenAI正在开发自己的搜索引擎。而TikTok，正在为年轻一代用户提供最能满足其喜好的AI推荐体验。各方角逐之下，老牌巨头谷歌明显是感受到了竞争压力，最终导致整个生成式AI市场乱成了一锅粥。Marcus指出，2022年Meta曾发布名为Galactica的AI系统，但该系统在推出后不久即遭下架，因为它居然建议用户吃玻璃。吃玻璃、吃石头，看来Meta和谷歌的大模型倒是很有共同语言。</p><p>&nbsp;</p><p>谷歌倒是对其AI综述颇有信心并制定了宏伟计划，而目前已发布的功能只是其上周官定量内容的一小部分。针对复杂查询的多步推理、利用生成式AI组织结果页面，通过Google Lens实现视频搜索——谷歌的雄心壮志绝对不容小觑。但回归现实，谷歌的商业声誉无疑取决于其AI功能的实际表现，而目前来看其正确性实在堪忧。</p><p>&nbsp;</p><p>Marcus直言，“（这些模型）本质上无法对自己的输出进行健全性检查，而这样的现实正在拖累整个AI技术产业。”</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.theverge.com/2024/5/24/24164119/google-ai-overview-mistakes-search-race-openai">https://www.theverge.com/2024/5/24/24164119/google-ai-overview-mistakes-search-race-openai</a>"</p><p><a href="https://www.bbc.com/future/article/20240524-how-googles-new-algorithm-will-shape-your-internet">https://www.bbc.com/future/article/20240524-how-googles-new-algorithm-will-shape-your-internet</a>"</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/BciKq80BAwfAeJuZ3k7b</id>
            <title>禁令再升级！拜登政府已不想让中国人在美从事AI工作了，套壳大模型的公司也危险了</title>
            <link>https://www.infoq.cn/article/BciKq80BAwfAeJuZ3k7b</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/BciKq80BAwfAeJuZ3k7b</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 May 2024 13:25:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 美国, AI产业, ENFORCE法案, 中国
<br>
<br>
总结: 美国通过《ENFORCE法案》收紧AI大模型出口，限制中国员工在美从事AI相关工作，旨在保护美国技术优势和国家安全。该法案一旦生效，可能对中国AI产业造成阻碍，包括数据供给和技术合作方面的影响。然而，中国也在加速自主研发步伐，将面临挑战但也有机遇。 </div>
                        <hr>
                    
                    <p></p><blockquote>美国已经不止一次提议从科技上对中国发起制裁，如果此次法案生效，又将对我国AI产业带来哪些影响？听听专家们的观点。</blockquote><p></p><p></p><h2>美国立法收紧AI大模型出口，连AI人才在美工作也受限</h2><p></p><p>&nbsp;</p><p>北京时间上周四，美国众议院外交事务部委员会以显著的多数票数，成功通过了一项旨在严格管控AI技术出口的法案。这项法案被正式命名为《加强海外关键出口国家框架法案》，通常简称为《ENFORCE法案》。</p><p>&nbsp;</p><p>值得一提的是，在该法案不仅限制了AI系统和大模型的出口，一旦法案通过，持有H1b 签证的中国员工或留学生可能需要特殊许可才能在美从事AI/ML相关工作。也就是说，这是明晃晃在限制中国人在美从事AI相关工作。</p><p>&nbsp;</p><p>《ENFORCE法案》由美国众议员共和党议员迈克尔·麦考尔（Michael McCaul）、约翰·莫伦纳尔（John Molenaar）、马克思·怀斯（Max Wise）和民主党议员拉贾·克里希纳莫西（Raja Krishnamoorthi）提出。其主要目标在于，通过强化美国商务部的权力，使其能够更加便捷地对AI模型实施出口管制，并进一步限制美国与外国实体在开发可能威胁国家安全的AI系统方面的合作。</p><p>&nbsp;</p><p>立法者表示，此举意在确保美国的技术优势和国家安全不受外部威胁。该法案的共同作者、众议院议员Michael McCauln (R-TX)&nbsp;表示：“人工智能引发了一场技术革命，它将决定美国是否能继续保持世界领先超级大国地位，还是会被中国超越。”</p><p>&nbsp;</p><p>McCauln表示他最担忧的是，“虽然美国政府的工业和安全局 (BIS) 有权限制人工智能加速器的出口——拜登政府曾多次利用这一点来扼杀中国在该领域的创新——但它缺乏监管人工智能模型出口的权力。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/27/2774a88c92338eebf9842c873338e7e7.png" /></p><p></p><p>《ENFORCE法案》于5月9日首次公布，由众议院的跨党派AI工作组提出，该法案旨在修订2018年出台的《出口管制改革法案》，这一年美国将14类新兴技术纳入出口管制。&nbsp;</p><p>&nbsp;</p><p>据悉，《ENFORCE法案》还需要通过众议院、参议院的全体表决以及总统拜登签署，才能落地生效。一旦此法案生效，将授予美国工业和安全局限制人工智能模型出口的权利，也促使白宫能够要求美国公司或个人只有获得出口许可证才能出口AI大模型。</p><p>&nbsp;</p><p>McCauln称道：“这项立法为 BIS 提供了灵活性，使其能够制定对封闭人工智能系统适当的控制，而不会扼杀美国的创新或影响开源模型。”</p><p>&nbsp;</p><p>值得一提的是，目前该法案实际上并不包含任何明确的保护或对开源模型的豁免，并且基本上涵盖了所有的人工智能系统、软件或硬件。</p><p>&nbsp;</p><p>需要明确的是，该法案的实际措辞意在模糊，并具体要求在法案通过后一年内更新“涵盖的人工智能系统”的定义。</p><p>&nbsp;</p><p>众议院议员玛德琳·迪恩 (D-PA) 在投票前解释道：“我们还在法案中对人工智能和人工智能系统的定义进行了临时修改，以便政府可以采取通常的监管程序并征求公众意见，从而对最终的定义进行适当的范围界定。”</p><p>&nbsp;</p><p>就在签订该法案的同一周，据彭博社报道，美国国会计划立法减少人工智能的潜在风险和危害，并每年至少投入320亿美元于人工智能研究，以促进美国经济和国家安全。美国参议院多数党领袖舒默（Chuck Schumer）表示，这项资金将让美国公司、大学和人才“保持在人工智能产业最前沿的地位”。</p><p></p><h2>如果法案生效，将对我国AI产业带来哪些影响？</h2><p></p><p>&nbsp;</p><p>据《21世纪经济报道》报道，清华大学人工智能国际治理研究院的李依栩向撰文对比了两部法案，表示本次法案主要是将AI技术纳入了《出口管制改革法案》的管制框架内，通过补充AI相关定义、赋予总统管制权、增加美国人从事AI模型出口相关活动的许可义务，对AI模型进行管控。</p><p>&nbsp;</p><p>比如，法案第三条扩展了总统管制权，管制对象是特定受限的AI系统、对美国国家安全至关重要的新兴和基础技术相关活动；</p><p>&nbsp;</p><p>法案第四条增加了一项额外权利，如果美国人在出口、再出口被确定为对美国国家安全至关重要的新兴和基础技术，包括设计、开发、生产、维修、翻新这些技术，美国总统有权要求他们申请并获得商务部许可。</p><p>&nbsp;</p><p>那么，这项比此前《出口管制改革法案》更加严格的《ENFORCE法案》一旦生效，对我国AI产业将带来怎样的影响？</p><p>&nbsp;</p><p>某头部电商技术总监Micheal Yan在接受《AI前线》采访时表示：</p><p>&nbsp;</p><p></p><blockquote>“短期内对于我国AI大模型发展会产生一些阻碍。尤其是在数据供给方面，国内开发者可能会面临数据短缺的问题。由于AI大模型的训练需要大量的数据集，如果美国限制对华出口数据集，这将直接影响中国AI模型的训练和性能提升。&nbsp;另外，在技术合作方面，中美两国在AI大模型开发过程中有着广泛的技术合作。如果该法案生效，这将会阻碍新技术与新模型的开发，因为中美顶尖开发人才的技术交流将受到较大影响。&nbsp;然而，需要指出的是，实施对华出口禁令的负面影响是双向的，对美国自身也存在不利之处。例如，在数据收集问题上，禁止对华AI模型出口同样可能让美国公司错失借助中国数据进行AI迭代的机会。从长远来看，中国有充足潜力自主发展、突破限制，而美国的这种限制措施可能会激发中国加快自主研发和创新步伐。美国限制AI大模型出口的政策不仅会对中国AI产业的发展带来挑战，同时也会对美国自身在某些方面产生一定的负面影响。”</blockquote><p></p><p>&nbsp;</p><p>面对一波接一波的科技制裁时，我国也在加速自主研发的步伐，这对于国产AI技术的发展来说也是一种机遇。</p><p>&nbsp;</p><p>平安集团前CSO、广东省CIO联盟会长李洋表示：</p><p>&nbsp;</p><p></p><blockquote>“如同过往的芯片、系统软件、应用软件等对中国的限制一样，美国对AI的限制出口接踵而至，但是AI的限制在目前中国大力推行国产替代的大前提下，是机遇大于挑战的。在这样的大背景下更加便于中国的科研工作者，丢掉幻想，重新布局。&nbsp;现实情况下是，AI所依赖的算力、算法很多都依赖于美国，当然也包括其他国家的算法、数据。但实际上中国的AI场景和应用体量非常大，在数据和算法层面不久后将不再受制于人。现代AI的发展不过几十年，即使是当前美国处于领先地位，也未必就永远处于领先。即便是如今众星捧月般的明星独角兽OpenAI也无法保证其大模型架构是未来AGI的正确技术发展路线，所以这个时候美国的限制对中国来说未必是坏事。&nbsp;希望中国的科研工作者，尤其是AI工作者们能够沉下心，在基础算法和模型研究中能够走出一条中国特色之路。</blockquote><p></p><p>&nbsp;</p><p>李洋还表示，“对于AI人才赴美的限制，我觉得也不是什么大问题，中国的AI基础研究和应用市场相对于美国来说都还是蓝海，立足于中国的产业，将AI人才留在中国服务，利大于弊。况且，美国所谓的这些限制，在当今时代下，也不会是一揽子的全封闭，所以我们应当审时度势，抓住这个机遇。”</p><p>&nbsp;</p><p>法案公布以来，除了担忧对于AI人才和大模型的限制外，外界还会担忧对于开源大模型的限制会让许多国内“套壳”大模型企业很难受。</p><p>&nbsp;</p><p>对此，李洋表示，“开源模型虽然现在美国比较领先，但是其他欧美国家也不乏相应的开源产品，我们在这个阶段还可以多方面借鉴和研究。并且，基于我们的自主创新体系，我们还是要沉下心来研发自己的AI基础底座，包括硬件基础设施、开发平台和大模型及其应用，所以短期内会对一些套壳的中国公司产生一些影响（包括模型的演进、应用和商业化等等都存在相应的限制）。”</p><p>&nbsp;</p><p>“但是从技术和应用上来讲，进口的开源模型也不是完美和最终的AGI的路标和唯一标准，所以在这个时候，也希望我国AI企业能够立足于自身的能力打造，丢掉幻想，与中国生态和国际生态一道打造出中国的AI开源版本，为中国和国际做出自己的贡献”。李洋说道。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.21jingji.com/article/20240527/herald/5f7b347c2787de4bf776584f95950075.html">https://www.21jingji.com/article/20240527/herald/5f7b347c2787de4bf776584f95950075.html</a>"</p><p><a href="https://www.theregister.com/2024/05/23/us_lawmakers_advance_bill_to/">https://www.theregister.com/2024/05/23/us_lawmakers_advance_bill_to/</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/xLryHtsN1PFPMuhquhAr</id>
            <title>Agent 还没出圈，落地先有了“阻力”：进入平台期，智力能否独立担事？</title>
            <link>https://www.infoq.cn/article/xLryHtsN1PFPMuhquhAr</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/xLryHtsN1PFPMuhquhAr</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 May 2024 10:12:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AutoGPT, AI Agent, 具身智能, 大模型
<br>
<br>
总结: 作者介绍了AI Agent的当前能力和应用潜力，讨论了在企业场景中有效利用Agent的重要性，以及未来发展趋势。同时，还探讨了AI Agent在不同领域的具体应用场景，以及与大模型的区别和发展方向。 </div>
                        <hr>
                    
                    <p>作者 ｜ 华卫</p><p></p><p>去年出圈的AutoGPT，让AI Agent来到大家的视线中并迅速爆火，大家都对Agent抱有极高的想象力与期待值。那么，Agent现在到底有多大的应用潜能？企业要如何抓住？同时在具体的落地实践方面，也有不少悬而未决的挑战。</p><p></p><p>带着这些问题，InfoQ《极客有约》特别邀请了阅文集团 AIGC 技术负责人马宇峰担任主持人，与机器姬CTO&amp;具身智能一百零八讲主讲人刘智勇、华为云aPaaS首席架构师陈星亮，一同探讨AI Agent的当前能力、应用落地情况以及未来发展趋势。部分亮点如下：</p><p></p><p>Agent不仅仅是一个玩具，而可以改变现实世界。在企业场景中有效利用Agent，合理选择业务场景非常重要。具身智能领域最大的挑战在于操作层面，瓶颈在于如何泛化地执行物理世界中的各种操作。未来使用Agent和大模型将成为企业员工需要掌握的技能。人类仍然拥有最终的评价权和评估权，这种能力是大模型无论如何发展都无法达到的。具身AGI的到来会为人类社会带来新的篇章，即从碳基生命到硅基生命的延续。</p><p></p><p>以下为访谈实录，经编辑。完整视频参看：</p><p><a href="https://www.infoq.cn/video/DOPpG6NjCHcJKDzCsAFT">https://www.infoq.cn/video/DOPpG6NjCHcJKDzCsAFT</a>"</p><p></p><h2>AI Agent当前的能力</h2><p></p><p>马宇峰：首先要谈的就是AI Agent现阶段的能力，大家现在是如何应用AI Agent的？具体落地场景有哪些？</p><p>刘智勇：最近大家可能已经注意到了一个名为“Figure”的机器人，演示中，工作人员向该机器人表达了饥饿感之后，Figure成功地将苹果递给了他；这一过程展示了AI Agent在物理世界中进行任务推理、规划并最终转化为实际行动的能力。在具身智能领域，AI Agent的应用场景非常广泛，AI Agent可以大致分为以下四个方面。</p><p>工业场景：在工厂中，具身智能机器人可以应用于3C生产线或汽车总装线，提高生产效率和自动化水平。商业服务场景：在商业环境中，具身智能机器人可以提供接待、讲解、导览、巡逻和配送服务，改善客户体验，提升服务质量。家庭场景：在家庭环境中，具身智能机器人可以承担清洁服务或家务工作，减轻人们的负担，提高生活质量。火星建设：在未来的火星探索和建设中，具身智能机器人有望发挥重要作用，帮助人类在恶劣的外星环境中进行建设和研究。</p><p>对于这些应用场景，具身智能都展现出了巨大的潜力和希望，为未来的技术发展和应用提供了广阔的前景。</p><p>陈星亮：针对企业场景进行AI Agent能力创新时，多数是从IT场景开始的，因为该场景拥有较为完善的信息化基础。在这一过程中，我们遵循两个主要原则：一是先易后难，我们首先从普遍性场景开始，然后逐步向专业化场景演进；二是保障效果，无论开发哪种场景的AI应用，都必须确保其有效性。</p><p>办公和编码领域被广泛认为是AI Agent应用的切入点，因为这些场景相对通用，容易实现。随着技术的进步，我们将AI Agent的应用延伸到更复杂的场景，例如：</p><p>办公领域：AI Agent可以用于自动生成会议纪要或设计文档，这些任务比简单的代码生成或文本创作更具挑战性，需要更深层次的场景理解和更高级的语言处理能力。销售或服务领域：AI Agent可以用于合同审核或法律条文的辅助生成，这要求AI Agent不仅要理解法律术语，还要能够处理复杂的逻辑关系。网络设备监控：在对网络设备进行监控的基础上，AI Agent可以执行自动巡检任务。这要求AI Agent不仅要处理专业数据，还要能够理解并应用信息化积累的知识。</p><p>马宇峰：大家首次接触到AI Agent大概是在什么时候？从本质上讲，AI Agent与大模型的区别究竟体现在哪些场景上？最核心的区别是什么？</p><p>陈星亮：Agent 这个概念，实际上在大模型出现之前就已经存在了。在进行 IT 系统集成或设计某些自动化流程时，其实已经有Agent 这一层了，尤其是在设备与外界交互的环节，而那时还没有将大模型技术整合进来以实现更广泛的泛化能力和生成式能力。</p><p>大模型技术引入后，起初我们并没有考虑将其应用于设备控制或高度交互性的 IT 系统交互中，而主要看中其在创作和生成内容方面的潜力。之前我们在设备代理方面的工作与 AI Agent 的概念思路颇为相似，只是随着大模型的加入，AI Agent 的能力和应用场景都发生了变化。当我们将这些结合起来后，认识到了 AI Agent 的真正面貌。因此，如果仅从 IT 系统的能力角度来看，AI Agent 这个概念并不神秘，不过是通过引入大模型为 AI Agent 带来了更多能力，从而丰富了其功能。</p><p>刘智勇：无论是ChatGPT还是Agent、具身智能，本质上都是在以下三个方面进行发展。</p><p>文本世界：在文本领域，大语言模型展现出了强大的生成和理解能力，这主要体现在ChatGPT等应用中。数字世界：数字世界中，我们需要利用规划、循环和反思的控制机制，实现任务从开始到结束的全流程控制，并调用数字世界里的外部工具进行执行。物理世界：物理世界中，Agent的能力落地体现在具身智能上，即通过具身智能技术将规划形成的任务序列转化为物理世界中的实际操作。</p><p>马宇峰：我分享一下第一次接触Agent的经历，去年夏天OpenAI开发了一项名为“Function Call”的能力，虽然看起来仍然是文本的输入和输出，但当函数作为一个字符串被输出并被精确调用时，我确实看到了Agent的不同之处。以前我们认为创作和创意不确定性是大语言模型最人性化的特征，但同时它们也有机器的一面，能够在有限的范围内唤醒某些函数。这项能力让我意识到Agent应该被独立考虑，其围绕工具使用、规划和执行的能力，可以帮助大模型结合现实世界中的数字和物理能力，形成一个更完整、更通用的解决方案。这是我对Agent概念的一次认知冲击。</p><p>然而，随着时间的推移，我发现Function Call可能并不像我最初想象的那么好。它演示的技能是查询天气，虽然可以很好地执行，但许多场景要复杂得多，可能不只有10个或20个函数可供调用，会出现完全不确定的函数，下一步该执行哪个函数也会是未知的。不过，Agent的主流能力，如浏览器的唤起、搜索引擎的查询结果以及一些生成能力的唤起，确实有效地让它从概念走向实际。当然，在实际应用过程中，我们也发现了许多不确定因素，但Agent的能力已经让我感到惊讶，它不仅仅是一个玩具，而可以改变现实世界。</p><p>回到Agent 的适用场景，我分享一下个人自身在探索中使用的直观感受。使用Agent能力可以批量生成自媒体文章，也可以像模像样地讲一个故事，从创建角色、制定纲到将角色和情节融合，再逐步生成内容，它的成文速度非常快，也有一些优点，比如生成过程中，可以将角色单独抽象出来去形成可视化的元素，可以使用多个角色和情节引导来发展内容片段，且在逻辑框架内是可控的。</p><p>但深入研究后我们发现另一个问题：Agent输出的内容，还是没有达到人类所能达到的逻辑性、创意性相结合。业内也做了很多尝试，这方面却似乎一直停留在中等或中上水平，整体表现平庸，所以这确实是长期困扰我们的问题。虽然我们最初认为Agent很有用，但在商业化和变现能力上似乎没有那么强。</p><p>想问一下陈老师，在代码和办公场景，Agent 可以从哪些方面提升效率？有哪些bad case？</p><p>陈星亮：我先谈谈Agent 给一些稳定场景带来的效率提升作用，如设计文档生成和合同中法律文本的生成等。在一些应用场景相对明确、法律条文引用也相对模式化的特定领域，如可靠性设计或安全威胁设计，Agent的表现在业务用户看来感知和体验都非常好，准确度也相当高，显著提升了工作效率。目前，我们也在将Agent应用于网络设备巡检等生产场景。尽管巡检过程中会遇到各种意想不到的问题，但对于那些已有案例库和解决方式库的巡检，Agent 都能够发挥作用，并帮助提高巡检效率、简化人力的工作。</p><p>然而，也有一些不尽如人意的地方。Agent刚推出时，大家对它寄予厚望，导致在选择应用场景时没有过多限制，业务团队提出了许多要求较高的场景，想要用Agent去解决未知的问题。这些要求的实际难度很大，而Agent在处理未知问题时的能力有限。因此，如果要在企业场景中有效利用Agent，合理选择业务场景非常重要。否则，Agent的效果可能不会达到预期，甚至可能非常差。</p><p>马宇峰：如果人类都做不到的事情，期望Agent达到超越人类的水平是非常困难的。相反，那些人类已经重复做了很多遍且已经规范化的工作，确实可以将人类的判断力解放出来，完全交给Agent来自动化处理。在具身智能的Agent应用上，哪些方面是可行的？可能存在什么挑战？</p><p>刘智勇：首先，具身智能的输入需求依赖于视觉语言模型，这意味着需要处理整个环境的三维数据信息，而不仅仅是二维图像。它需要的输入包括深度数据、RGB图像等，可能还要结合触觉、反馈力以及编码器数据等，这些数据共同构成了具身智能的全面输入。因此，在数据输入的方式上，具身智能与传统Agent存在显著差异，这些差异带来了巨大的挑战。</p><p>其次，在数字世界的Agent中，无论是什么类型的Function Call，基本上都是可执行的动作，操作层面通常不会遇到问题。然而，具身智能中存在一个可供性问题，即是否能够真正执行某个动作。尽管存在这些挑战，但也有一系列方法可以解决这些问题，如具备泛化能力的视觉语言模型、迭代细化的机制、自我反思的机制等。目前来说，具身智能领域最大的挑战在于操作层面，即具身操作。感知、决策和规划虽然重要，但真正的瓶颈在于如何可泛化地地执行物理世界中的各种操作。</p><p>马宇峰：Agent目前的发展状况如何？是否已经达到了一个平台期，还是仍然有很大的提升空间？是否依赖于某些特定的背景？</p><p>我认为Agent主要依赖于大模型的Function Call能力，需要准确地识别出当前调用哪个模型来完成当前任务，并提供相应的结果，以便大模型进行下一步操作。而瓶颈可能在于读取上下文的长度，上下文长度决定了能够识别多少个函数。Agent在执行过程中受限于场景，只能在有限的函数中进行选择，其执行也不完全精确；如果执行不精确，就需要获取更多的环境信息或反馈信息来执行函数，过程中可能会出错。Agent是一个精妙但不够鲁棒的系统，如果它返回到上一级并根据错误信息重新执行，可能会带来更大的资源消耗和时间延迟。</p><p>陈星亮：在企业场景中实施Agent时，我们首先需要考虑的是技术的可实现性。在挑选场景的过程中，就要考察技术是否可行；一旦场景确定，接下来需要考虑的是如何提高Function Call的准确度，如果准确度不够高，需探索其他工程手段来提升API的识别准确率，甚至在语义理解之后通过额外的工程能力进行调整、校验生成的API并通过查询方式进行补充。企业面临的最大挑战之一就是需要重复性地进行这类工作。目前我们也在探索长序列处理、记忆的短、长期存储以及上下文空间的扩展等技术，以期在未来实现更多的技术突破。</p><p>在具身智能领域，企业场景中也在逐渐引入多模态技术，尤其是当与操作技术领域（OTA）的设备关联时。多模态技术的引入包括传统的视觉识别等，将进一步增加系统的复杂性。如果大模型在这些领域取得显著进展，那么在企业IT融合场景中的工程难度将大大减少。目前，我们在工程实践中仍需进行大量技术工作，这些工作的管理复杂性甚至超过了传统的微服务架构。</p><p>我相信，随着技术的进步，未来将有很大的空间来改进现有的工程能力，减少人工干预，让大模型承担更多的工作。无论是让大模型自行处理，还是让Agent框架沉淀出更多稳定的框架性技术，都是未来技术发展的趋势。我对大模型在未来的迭代和改进抱有很高的期待，相信它们将带来更好的效果，并减轻当前工程化实践中的一些负担。</p><p>刘智勇：从阶段性的角度来看，我们认为具身智能目前处于技术起步期，未来的发展空间仍然非常广阔。之所以称之为技术起步期，是因为目前还存在三个方面的挑战：</p><p>任务类型的泛化性：这涉及到Agent能否理解各种类型的指令，并能够完成具体的规划而不产生幻觉，抑制Agent在理解上的偏差，对齐人类意图的二义性和潜在偏好，确保其能够准确执行任务。环境的泛化性：即Agent快速与环境对齐，对齐环境的规律、动态性和随机性。操作的泛化性：这是更为复杂的挑战，涉及如何利用多种数据源采集更多的线下数据，并据此训练出能够泛化到不同情境的具身操作模型，目前行业中还没有一个非常好的解决方案。</p><p>从这三个方面的挑战中，我们看到了未来的发展机会。尽管目前还存在许多问题需要解决，但这同时也是推动技术进步的动力。</p><p>观众提问：是否可以认为大模型做好了就不需要 Agent 了呢？</p><p>刘智勇：大语言模型的主要功能是处理和生成文本，核心在于将文本信息进行向量化处理，并通过Transformer架构以及监督学习机制，实现技术上的范式转变。这些技术基础的迭代，再结合大量的数据和强大的算力，促成了ChatGPT等大语言模型的诞生，它们在文本生成和回复方面表现出色。</p><p>尽管大语言模型在文本领域取得了显著的成就，但本质上只具备基于零样本提示词的文本回复的能力，而不具备执行实际任务的能力。这意味着，无论大模型在文本处理上多么先进，它们仍然需要Agent的介入来实现从文本到行动的转变和全流程的处理。</p><p>因此，大模型和Agent是两个不同的概念，前者专长于文本交互，而后者则涉及到任务的执行和落地能力。简而言之，大语言模型缺乏将文本回复转化为实际行动的能力，是典型的缸中之脑。</p><p>马宇峰：如果大语言模型发展到某个瓶颈无法提升，那也可以像两个人类合作思考能更高效地完成工作一样，使用两个大模型实际上可以进一步提升当前水平。哪怕提升的幅度不大，但考虑到大模型的较高的基础表现，即便是小幅提升也可能带来非常显著的回报，并且能够有效地增强现有能力。至于这些能力是否会直接集成到大语言模型中，我认为在相当长的一段时间内，我们仍然可以将大语言模型视为一个智能体，主要从智能逐步思考的角度来使用它。</p><p>陈星亮：aPaaS主要是基于行业内现有的资产或经验，实现程度化代码开发，降低开发门槛，通过拖拉拽的方式快速构建简单的应用程序。随着大语言模型代码生成能力的出现，零/低代码平台受到了较大的冲击。曾经有观点认为，大模型的出现可能会使得低代码或零代码的开发方式变得不再必要。实际上，我认为情况并不会如此。</p><p>零/低代码平台可以有效地融合大语言模型的能力，让大模型直接参与代码生成。以前需要通过拖拉拽来实现的功能，现在可以通过自然语言处理（NLP）的方式进行交互，提供更直观、友好的用户体验，并帮助理解业务用户原始的语意，以更好地生成低代码或零代码应用。我认为零/低代码平台和大模型之间更多的是一种合作关系。低代码平台上已经积累了大量的业务资产，而大模型可以将其作为插件调用，两者结合将发挥出更大的潜力。</p><p></p><h2>AI Agent的落地挑战</h2><p></p><p>马宇峰：在大语言模型不提升或通用大语言模型更新周期较长的情况下，如何利用现有工具和能力取得良好成果？有哪些方法或策略？</p><p>尽管当前AI Agent面临许多瓶颈和困境，限制了其应用范围，但仍有一些方法可以提升其驱动能力，如可以通过垂直领域的强化训练、特殊训练技术或更巧妙的方法，在不提升大语言模型本身能力的前提下改善Agent的表现。Agent在当前大语言模型框架下的表现，不仅取决于模型本身，还受到其他多个环节的影响。即便大语言模型不是限制因素，其他环节的优化也能提升整体Agent的效果。以Kimi为例，它之所以能够脱颖而出，可能确实在大模型的某些方面做了针对性强化，但重要的是它对文档类型的解析能力有效提升了实际操作中的使用体验。Kimi能够在处理长文档时进行分块，并采用迭代检索的方式输出答案，这大大增强了Agent在特定场景下的应用体验。</p><p>我相信，即使在大语言模型能力不变的情况下，只要充分提升检索能力，就能显著提高最终的可用性和准确率。很多时候未能获得准确答案，是因为没有找到正确的信息片段。如果知识库足够丰富，片段足够多，那么作为一个智能整合的搜索引擎，Agent将具有巨大的应用潜力。在大语言模型能力不完整的情况下，只要把某个小模型、小工具或阶段（如检索阶段）做得足够好，也能显著提升Agent的整体表现。</p><p>刘智勇：要提升AI Agent的能力，首先需要充分挖掘并利用长期记忆，通过RGBD摄像头读取的数据，结合视频语言模型，形成丰富的语义信息。在特定场景中，这些语义信息往往是重复出现的元素，关键在于如何有效地保存信息，为后续的规划提供坚实的基础。随着时间的推移，语义信息不断积累，AI Agent的长期记忆能力将变得更加强大。</p><p>其次，进行迭代细化是提升AI Agent能力的另一个关键点，这意味着需要不断结合当前的模糊指令和新获得的语义信息，形成新的提示词。通过不断的迭代询问，AI Agent能够逐步细化和精确化其理解和响应，通过不断反思，最终达到更加精准的结果。</p><p>陈星亮：企业内部考虑事务时主要关注两点，都与数据紧密相关。首先是文档处理的问题，在企业中，非结构化文档往往是承载信息的主体，处理这些文档不仅要识别文档类型，还包括对复杂文档的解析，如图文混排和包含复杂表格的文档。这些内容在原有的基础上，需要对文档类型识别的范围进行扩展，但在企业内部对这种复杂文档的解析仍是一个较大的挑战。</p><p>其次关于原有数据的利用问题，特别是在生产场景中，一般都具备专业领域的背景。以设备巡检为例，它与设备的领域知识密切相关，这种情况单靠企业自身的私域数据积累可能不够，需要在行业内去做垂域模型。目前，我们期望通过Agent技术的发展，能够让更多企业在通用场景中体会到Agent带来的好处，从而愿意将自己内部的结构化数据进行区分，将企业机密数据与可对外开放的数据分离，并逐步开放一些行业公共数据，这将有助于构建每个行业的垂直领域模型，为未来企业场景和Agent的发展带来巨大的好处。</p><p>马宇峰：初期部署Agent的成本是否高昂？是否能够带来相应的收益？能否实现成本的回收和价值回报？</p><p>陈星亮：企业部署Agent时，成本问题是一个必须考虑的重要因素，并且需要结合业务团队的期望以及对目标的评估来共同考量。初期企业主要探索通用场景时，成本通常是较低的。随着业务场景的成熟，以及越来越多的用户和业务团队成员开始使用这些场景，成本就会开始上升。特别是当场景全面开放并开始构建更多场景时，就可能需要多套模型和版本，模型也需要不断地做飞轮进行迭代和优化，成本可能会指数级增长。</p><p>因此，在正式对外放开并大规模使用Agent之前，与业务团队进行充分沟通和期望管理是非常重要的，需要让业务团队明白，业务场景真正对外开放并吸引大量用户使用后将会涉及到哪些成本。同时，业务团队也需要评估这些成熟场景能够带来的价值，如对客户满意度和内部效率提升的贡献。当业务团队获得这些信息并进行综合评价后，他们对预算和投入的决策将会更加明智，这样的过程有助于确保Agent部署的成本得到合理评估和控制，并带来相应的价值回报。</p><p>刘智勇：Agent部署的成本考量包括云端的调用成本、机器人本体的计算成本以及整体的部署成本这三个主要方面。</p><p>云端数据成本。这涉及调用模型的频率，如果实时观察环境中的语义信息，就需要频繁且快速地调用模型，这样会耗费大量的计算资源，从而产生高额成本。因此，必须考虑调用频次和计算资源消耗的问题，实现具身智能体和自身限制的对齐。机器人本体成本。在具身智能场景中，机器人本体通常需要具备一定的计算能力。为了使机器人能够在不同场景中应用，无论是商业、工业还是家庭环境，都希望能够在端侧部署大模型，尤其是本地部署，而这在没有高端GPU和显寸的支持的条件下尤为关键。部署成本。将设备分布式放置在不同地方会产生额外的成本，此外还需要考虑是否能够通过启发式方法或其他手段，让设备快速启动并投入使用，这也是降低部署成本的一个重要方面。</p><p>马宇峰：部署成本确实是一个值得讨论的话题。在实验性质的探索阶段，对时间的考量可能并不严格，但一旦考虑到响应速度，比如每秒需要处理多少个token来执行动作，成本问题就变得尤为突出。因为模型需要很长时间才能完成一个Agent的输出，这意味着直接使用大模型进行推理的成本和时间的耗费都是非常高的。对此，我个人建议可以利用一些框架，如Dify或Coze，它们可以帮助构建Agent框架，并提供了丰富的工具来逐步检查生产和输出的结果。</p><p>企业部署Agent时， 可以先验证整个流程是否可行，并确保其能带来业务价值。之后，可以考虑用一些专门训练的小模型来替代流程中的关键部分，以降低成本。初期可以利用现有的资源进行探索，长期来看，通过特定化的方式进行优化和部署可以平衡成本。</p><p>观众：在部署Agent时可能会遇到哪些安全方面的问题？目前是否有一些比较成熟的工具可以用于保障Agent的安全性？同时，是否可以认为Agent的安全性主要取决于其底层大模型的安全性？</p><p>陈星亮：首先，Agent的安全性并不仅仅由大模型决定，模型安全只是一部分，还涉及应用安全和数据安全。对企业来说，对安全性的投入无论多少都不为过。无论在引入模型时，还是实际使用过程中，包括Agent框架都需要进行安全检查。例如，使用开源框架组件时需要进行安全审查，运行时需要对模型的输入输出内容进行监控，以及对应用框架进行访问控制，防止调用越权等。</p><p>在企业原有的安全体系内构建Agent的安全性会更好一些，在华为云内部，我们基于AI原生应用引擎等平台，当Agent对外提供服务或与模型进行交互时，利用内部原有的数据安全、应用安全和内容安全方面的技术，对内容进行检查和过滤。Agent的安全性需要在现有基础上，结合Agent之间的技术组件交互以及场景特有的安全要求来综合考虑和实施。</p><p>观众：面对多智能体协同框架的开源与闭源发展，应该如何选择合适的技术路线和框架，以减少试错过程并确保系统不会被行业不断的更新迭代所淘汰？</p><p>陈星亮：我认为应该分开考虑。对于Agent的开发框架，目前开源的选择比较多，都有很多可用的资源。鉴于Agent领域本身正在快速发展，选一个团队成员熟悉且操作顺手的框架，然后跟随其发展进行使用。而对于Agent的运行时环境，进入企业生产环境后，我建议使用闭源解决方案。理想的状态是，在企业现有的基础设施基础上进行必要改造，以便将Agent的运行时环境纳入统一管理和运维体系中，确保运行时的稳定性和安全性。</p><p>刘智勇：我更倾向于观察一个技术方案是否展现出成熟和收敛的迹象，如果开始趋于稳定和收敛，那可能是着手开展相关工作的更适宜时机；如果尚未成熟，还在快速迭代和变化之中，那可能面临开发速度跟不上开源社区更新速度的问题。</p><p>马宇峰：在选择开闭源路线时，实际上需要根据所处的发展阶段来决定。不管选择何种路线，实际投入使用比纠结于何时开始尝试和如何减少错误更为关键。毕竟，随着时间的推移，技术本身会更新迭代，这些更新往往也会朝着更优化的方向发展，对业务发展带来积极的价值。</p><p></p><h2>AI Agent的未来前瞻</h2><p></p><p>马宇峰：从长远来看，企业中Agent的落地是否会对某些现有的职业造成冲击？比如普通员工、现有商业模式、提供API服务的SaaS公司以及供应商等。Agent的普及和应用会带来怎样的影响？</p><p>陈星亮：对于员工而言，随着技术的发展，未来使用Agent和大模型将成为他们需要掌握的技能，尤其是提示词。员工至少需要学会如何使用Agent，就像现在进行零代码应用开发一样，将其作为日常办公工具的一部分。对于企业，尤其是传统SaaS公司来说，Agent和大模型的引入已成为明显趋势。一些大型SaaS公司，已经开始将大模型集成到平台中，将Agent框架和集成外部大模型的能力嵌入到二次开发和应用中。传统SaaS公司如果不加入到这个发展潮流中，可能会影响产品体验，建议一定要去拥抱大模型和Agent。</p><p>刘智勇：我从两个不同的角度来探讨Agent的运用及其对未来的影响。对于企业而言，利用大模型或Agent的主要目的是提升工作效率和减少对人工的依赖。有时员工的工作效率确实无法与Agent相比，特别是在一些技术性任务上，初级工程师的编码能力可能远不及代码Agent。对于工程师来说，积极利用Agent不仅是为了保持个人竞争力，也是为了适应未来工作的需求。Agent可以作为一个强大的工具，帮助工程师完成更高效和更复杂的任务。</p><p>而具身智能特别是人形机器人，预计它们对未来世界的冲击将是巨大的，会在商业、工业和家庭三个领域中体现出来。在商业领域，许多展示和演示类的工作岗位可能会被智能机器人所取代，因为它们可能表现得更好。在工业场景中，很多重复性或技术性工作实际上可以由机器人来完成，提高生产效率和安全性。在家庭环境中，未来也可能会出现更多类型的服务机器人，帮助处理日常家务。同时，我们也应保持谨慎乐观的态度，认识到技术发展和应用普及的速度可能没有想象中那么快。机器人和Agent的发展旨在辅助人类，使我们能够专注于更有价值和创造性的工作。</p><p>马宇峰：关于Agent如何影响我们的现实世界，尤其是在工作场景中，我的感觉是需要先拥抱这些变化，然后学会适应和改变。现在可能是小企业创业的绝佳时机，因为借助大语言模型这样的“万能胶水”，不再需要像以前那样协调大量资源来进行服务能力的交付，只需要尝试不同的组合，就可以高效地为客户提供解决方案。这样，小企业的服务能力从完全定制化转变为可以大规模扩展的模式，这对于二线市场可能是一个深远的改变。</p><p>观众：英伟达使用虚拟环境训练智能体机器人的方法是否可以拓展到所有应用场景？虚拟环境数据能在多大程度上替代现实场景数据？</p><p>刘智勇：我们实际上已经使用过英伟达的Isaac Gym来训练智能体，主要是进行强化学习的训练。这种方法涉及合成不同的仿真环境，并基于此来进行强化学习的训练。这种方法的主要优势是数据是免费的，但存在一个从仿真到现实（sim-to-real）的转换gap。在应用拓展方面，特别是在本地运动（locomotion）即行走部分，使用强化学习和虚拟环境的训练模式效果是不错的。对于一些操作类的任务，也有一些积极的应用特点。但对于更广泛的操作任务，可能更倾向于使用采集到的真实数据，并利用transformer架构来训练大型的transformer模型。因为在英伟达的仿真环境中，很多物理引擎的细微数据是无法被完美仿真的，如一些非常精细的触觉反馈。</p><p>马宇峰：在内容行业，我们对AI技术的发展有着深刻的感受。有人认为，Sora的成功是因为学习到了物理世界的真实性，但随后有人指出，Sora可能只是选择了一些优秀的片段来展示。Sora的训练采用了虚幻引擎，但这种方法还是单一的，并不一定能够真正理解物理世界。这与刘老师提到的英伟达的反馈机制可能有所不同，它们的输入输出机制存在差异。目前，Sora虽然理念上很先进，但实际上还没有达到通过虚拟化的输入输出来获得真实物理引擎的效果，可能是因为模型的参数规模不够大、训练数据不够丰富，或者受虚拟数据本身的限制，还需要进一步深入观察和研究。</p><p>观众：请介绍一下目前单智能体落地的情况，以及它与公司当前技术架构的结合方式。多智能体的具体架构是如何建设的？</p><p>陈星亮：在原有的技术架构体系中，目前大家使用的较多的是Web应用、微服务，有时还会使用函数技术体系。我们可以将Agent和大模型引入进来，先进行隔离，用于特定的场景。这些场景必然会与现有的微服务体系或函数体系进行交互。这时可以采用集成的方式进行，而不是直接使用大模型的Function Call方式。这样实施难度会小一些，而且也能让Agent发挥作用。当技术团队逐渐掌握了Agent和大模型这套技术，就可以开始取代一些现有的应用。这样的过渡不仅有利于架构的演进，也有助于技术团队的能力培养。</p><p>马宇峰：多智能体协同是一个复杂而富有挑战的领域。项目中，多智能体的协同运作被分解为不同的角色，如项目管理员、编码者、产品经理等，各自承担不同的职责。然而，如果单个智能体（单Agent）的运作还没有完全搞明白，就急于发展到多智能体（多Agent）协同，其实是存在很大风险的。</p><p>在实际应用中，比较常见的模式是有两个智能体协同工作，一个负责生产，一个负责评估，但目前还没有看到这种模式带来特别显著的提升。举一个例子，情感陪伴场景中有大量的对话交互，如果一个人与一个IP进行对话，输入输出的比例可能不太理想，引入第三个智能体会带来信息量的显著提升，这在满足用户情感需求和具体任务需求时非常有用。</p><p>接下来，我们继续研讨AI Agent的未来。目前，AI Agent的进展可能在技术行业内比较流行，但还没有真正“出圈”。当AI Agent被充分使用时，哪个场景会是理想中的未来？</p><p>刘智勇：图灵测试是一个经典测试，用于评估机器是否具备人类智能，即在背靠背的情况下，判断对方是计算机还是人类。而我曾经提出过一个“面对面测试”，特别适用于人形机器人，尤其是高端的女性机器人。测试中，高端女性仿真机器人被指派到一个地点，与10位单身男性分别进行相亲，如果其中有9位男性最终发出了下一次约会的邀请，该机器人就通过了测试。这不仅考验机器人是否能够理解并执行任务，还考验它是否能够与人进行有效沟通和交流。如果机器人能够在这种面对面的互动中展现出高度的智能和亲和力，那么无论在用户交互、情感表达、行动能力还是外观颜值上，其都达到了非常高的标准。</p><p>通过这样的测试，机器人将展现出巨大的商业价值。因为当机器人在面对面互动中具有很好的亲和力时，就可以在各种职业领域中得到应用，包括教师、律师、前台接待、演艺、直播等各个领域。我认为，这种更泛化的Agent通过“相亲测试”的事件是一个标志性的里程碑，意味着AI Agent的能力和人机交互能力已经达到全新的水平。</p><p>马宇峰：Agent未来的发展趋势将是怎样的？当它们发展到一个成熟的阶段后，将会呈现出什么样的形态？</p><p>陈星亮：在企业场景中，Agent目前主要扮演辅助角色。我们正在考虑的是，Agent是否能够从围绕特定场景服务转变为围绕特定人员服务。随着这一趋势的逐步发展，我们可以设想，未来某些人的工作是否会逐渐被Agent取代，这可以在分工上进行明确划分。我认为，当Agent真正能够在企业中提升效率并降低成本时，就达到了真正的成熟阶段。</p><p>马宇峰：Agent落地过程中可能会遇到一些难以实现的场景，这就需要我们在筛选场景和逻辑执行上进行深入的思考，最终的理想状态是以人的方式来看待Agent：作为智能体能够取代当前人类的多少工作。Agent概念并非是大模型出现后才产生的，但确实又是一直存在的。智能体这个词，最常见的体现可能就是人类自己。人类可以作为Agent 选择任务难度的度量，同时也可以作为Agent的驱动方向。</p><p>如果有一天Agent真的取代了所有的工作，人类应该干什么？我想，这时人类最重要的价值就是发挥自己的需求。Agent服务的目标永远是人本身，人类有需求，才会有Agent去做这件事情。人类是需求的发起方，Agent只是去满足需求的一方。因此，人类仍然拥有最终的评价权和评估权，这种能力是大模型无论如何发展都无法达到的，除非Agent拥有像人一样的肉身，有自己的激素欲望和生理限制。</p><p>陈星亮：首先，我认为Agent的未来是充满无限可能的。无论是在各个行业，还是在ToB或ToC的体系中，人类社会有各式各样的场景需要Agent来提供支撑，因此它的发展前景是极其广阔的。其次，我相信Agent将是一个多样化的存在，无论是在技术实现还是在业务场景的应用上。目前Agent技术的发展呈现出百家争鸣的局面，这对技术行业来说是一件好事，意味着有更多的行业场景愿意尝试采用Agent，并进行投资。在这样的投入下，技术可以快速发展，进而更好地探索未知领域。</p><p>最后，在Agent向前发展的过程中，我们也需要正视现实情况。当前无论大模型还是Agent框架本身的发展，下一步的方向似乎还不是很清晰。我相信未来还会有更多新技术不断涌现，将推动Agent的发展，使企业和个人的诉求和场景得以实现。</p><p>刘智勇：从具身智能的角度来看，商业落地是一个重要议题。目前，Agent或具身智能体主要扮演的是辅助角色。以它们当前的智力水平，还不能承担替代型的角色。它们能够提升生产力，但并不能真正改变生产关系。我们应该从最大程度提升人的生产力的角度出发去寻找落地场景，这是比较实际和可行的视野。</p><p>另外是从更宏观的层面来看待Agent和具身智能的发展，这与AGI息息相关。在经历了Transformer模型、ChatGPT以及机器人的Transformer模型等重要时刻之后，我们可能在不久的将来迎来AGI的时代。具身AGI的到来会为人类社会带来新的篇章，即从碳基生命到硅基生命的延续。在具身智能领域，如果具有AGI的通用人形机器人能够实现，那么在某种程度上将实现仿生或永生的概念。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>