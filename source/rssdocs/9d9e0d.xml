<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>AI探索站 - 即刻圈子</title>
        <link>https://m.okjike.com/topics/63579abb6724cc583b9bba9a</link>
        
        <item>
            <id>https://m.okjike.com/originalPosts/65d70a02164d89e601e3f89d</id>
            <title>下一个全民级的内容平台需要满足两个特点（其实有更多，先不提了）： - 不能是纯被动、低参与、低沉浸的。 这类型的内容实质上在被动消费固定信息，在脑机接口这...</title>
            <link>https://m.okjike.com/originalPosts/65d70a02164d89e601e3f89d</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/65d70a02164d89e601e3f89d</guid>
            <pubDate></pubDate>
            <updated>Thu, 22 Feb 2024 08:46:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    下一个全民级的内容平台需要满足两个特点（其实有更多，先不提了）：<br /><br />- 不能是纯被动、低参与、低沉浸的。<br /><br />这类型的内容实质上在被动消费固定信息，在脑机接口这样的传输渠道成熟之前，视频形式在单位时间内传递信息的密度已经到了极限，对人眼和耳朵两个信息采集器官的利用效率到了极致<br /><br />没有新的内容范式就不存在平台的机会，最终都是给抖音Tiktok扩大供给<br /><br />- 不能像游戏一样强制性的高参与高投入。<br /><br />消费成本太高会使得用户对内容消费体验的预期持续抬高，整个游戏工业界都在卷头部精品，这条路的长期趋势是往头号玩家那样终极单品内容的方向走，极致的OGC<br /><br />目前基于LLM的产品探索，进度最靠前的大概还是C.AI，但这种需要用户自我创作自我驱动的体验形式跟大量游戏殊途同归<br /><br />如果内容不成立，就是AI native的小众垂类游戏，内容若成立，最终会逐渐融入现存的各种游戏类型，技术被用于更好的诠释成熟的内容形态<br /><br />这就是为什么我认为在探索AI to C的过程中，持续在“对话”类产品上投入资源是个错误。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/65d60636164d89e601d0dba5</id>
            <title>又看到了一个宝藏文档，这个飞书文档整理了 Sora 模型发布到现在的官方 85 个 demo 视频，其中官网 48 个，Tiktok 8 个，𝕏 (Twitter) 29 个，有原始链接、pro...</title>
            <link>https://m.okjike.com/originalPosts/65d60636164d89e601d0dba5</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/65d60636164d89e601d0dba5</guid>
            <pubDate></pubDate>
            <updated>Wed, 21 Feb 2024 14:18:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <img src="https://cdnv2.ruguoapp.com/icons/link.png" /><br />                    <a href="https://ooglx41xh8.feishu.cn/wiki/Saj3wfzqfiwqCsk3Mh6cHOKGnOy?table=tblK5hmgah9T2jov&amp;view=vewbPpW6ja">https://ooglx41xh8.feishu.cn/wiki/Saj3wfzqfiwqCsk3Mh6cHOKGnOy?table=tblK5hmgah9T2jov&amp;view=vewbPpW6ja</a><br />                <br />又看到了一个宝藏文档，这个飞书文档整理了 Sora 模型发布到现在的官方 85 个 demo 视频，其中官网 48 个，Tiktok 8 个，𝕏 (Twitter) 29 个，有原始链接、prompt（如果有）、发布时间。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/65d5fb22a922aa28d0ac43be</id>
            <title>💎 重磅：Google 刚刚发布了一个超强的开源模型 Gemma 这是 AI 开源模型生态的一大步。 Google DeepMind CEO Demis 表示：「我们长期以来一直支持负责任的开放...</title>
            <link>https://m.okjike.com/originalPosts/65d5fb22a922aa28d0ac43be</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/65d5fb22a922aa28d0ac43be</guid>
            <pubDate></pubDate>
            <updated>Wed, 21 Feb 2024 13:31:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    💎  重磅：Google 刚刚发布了一个超强的开源模型 Gemma <br /><br />这是 AI 开源模型生态的一大步。 Google DeepMind CEO Demis 表示：「我们长期以来一直支持负责任的开放源代码和科学，这可以推动快速的研究进展，因此我们很自豪地发布了 Gemma：一套轻量级的开放模型，其大小在同类产品中名列前茅，其灵感来源于双子座所使用的相同技术。 」<br /><br />✨ Gemma 开放模型的特点<br /><br />- Gemma 是一个轻量级、最先进的开放式模型系列，采用了与Gemini相同的研究和技术。<br /><br />- Gemma 由 Google DeepMind 和 Google 的其他团队共同开发，其灵感来源于双子座，名字反映了拉丁语 gemma，意为「宝石」。<br /><br />- 两种尺寸：Gemma 2B 和 Gemma 7B。每种规模都发布了预训练和指令调整变体。（Hans 注，在图2中，你能看到 7B 在多项核心指标， 大幅领先Llama-2。）<br /><br />- 经过预训练和指令调整的 Gemma 模型可在你的笔记本电脑、工作站或Google Cloud上运行，并可在 Vertex AI 和谷歌 Kubernetes Engine 上轻松部署。<br /><br />💰 使用、研究和AI 伦理方面的补充：<br /><br />- 允许所有组织（无论规模大小）以负责任的方式进行商业使用和分发。<br /><br />- Gemma 专为推动人工智能创新的开发人员和研究人员组成的开放社区而打造。<br /><br />- 现在就可以开始使用 Gemma，免费访问 Kaggle，免费使用 Colab 笔记本，首次使用 Google Cloud 的用户还可获得 300 美元的积分。<br /><br />- 研究人员还可以申请高达 500,000 美元的 Google Cloud 信用额度来加速他们的项目。<br /><br />🔗 详情部署和使用，现在就可以访问：<br /><br /> http://ai.google.dev/gemma<br /><img src="https://cdnv2.ruguoapp.com/Fpm4U8RDmTMZykT9gO6u29zB0lUXv3.png" /><br /><img src="https://cdnv2.ruguoapp.com/Frnu4saaLO-xHHwnFcEwzN4jlA1sv3.png" /><br /><img src="https://cdnv2.ruguoapp.com/Fkkk6FiHn68H6RDt1dtBQ6K2e2Spv3.png" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/65d5ed6f36246663244c7a5a</id>
            <title>最近联系我司（Moonshot AI，月之暗面）找工作的朋友巨多，已经有点回复不过来了... 这里是 JD 汇总，大家可以直接戳开看看，有需要可以再联系我！ https://moon...</title>
            <link>https://m.okjike.com/originalPosts/65d5ed6f36246663244c7a5a</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/65d5ed6f36246663244c7a5a</guid>
            <pubDate></pubDate>
            <updated>Wed, 21 Feb 2024 12:32:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    最近联系我司（Moonshot AI，月之暗面）找工作的朋友巨多，已经有点回复不过来了...<br /><br />这里是 JD 汇总，大家可以直接戳开看看，有需要可以再联系我！<br /><br />https://moonshot.jobs.feishu.cn/s/iNx6qtbH
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/65d5c59e9185c305d19ea2e5</id>
            <title>从 2 月 15 日 OpenAI 发布 Sora 模型到现在，官方一共发布了 85 个 demo 视频，其中官网 48 个，Tiktok 8 个，𝕏 (Twitter) 29 个。 这 85 个视频被我全部存...</title>
            <link>https://m.okjike.com/originalPosts/65d5c59e9185c305d19ea2e5</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/65d5c59e9185c305d19ea2e5</guid>
            <pubDate></pubDate>
            <updated>Wed, 21 Feb 2024 09:42:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    从 2 月 15 日 OpenAI 发布 Sora 模型到现在，官方一共发布了 85 个 demo 视频，其中官网 48 个，Tiktok 8 个，𝕏 (Twitter) 29 个。<br /><br />这 85 个视频被我全部存档在了这里：https://ooglx41xh8.feishu.cn/wiki/Saj3wfzqfiwqCsk3Mh6cHOKGnOy?table=tblK5hmgah9T2jov&amp;view=vewbPpW6ja<br /><br />包含每个视频的 prompt、prompt 中文翻译、原始链接、发布时间。由于 Sora 尚未正式对公众开放使用，可以粗略地认为，除了官方发布的视频之外，其余宣称是「Sora 生成」的视频大概率为伪造。在 Sora 正式对公众开放之前，这个数据库会尽可能保持更新。<br /><img src="https://cdnv2.ruguoapp.com/lh_m_Rin1OJ5Fly7KoQX-EQruVMav3.png" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://mp.weixin.qq.com/s/CEekD2YDy8uhxD2vLAm3KQ</id>
            <title>最近几天一直在研究Sora，然后为了方便我就写了一个自学的文档。 这本《橘匪🍊的sora自学手册》都是我这几天精心整理的sora学习资源，总共有5000多字。 这本手...</title>
            <link>https://mp.weixin.qq.com/s/CEekD2YDy8uhxD2vLAm3KQ</link>
            <guid isPermaLink="false">https://mp.weixin.qq.com/s/CEekD2YDy8uhxD2vLAm3KQ</guid>
            <pubDate></pubDate>
            <updated>Tue, 20 Feb 2024 06:10:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <img src="https://mmbiz.qpic.cn/mmbiz_jpg/JFXicHO83uibuTyFKBry94Mk0SDl4LxWHwLibxo6CrzkTib3uqM0ictt7aQBEW5aFjPUEVSIDGywZtf1DZIPAeQMZxg/0?wx_fmt=jpeg" /><br />                    <a href="https://mp.weixin.qq.com/s/CEekD2YDy8uhxD2vLAm3KQ">耗时3天，我写了一本5000字的《sora自学手册》！</a><br />                <br />最近几天一直在研究Sora，然后为了方便我就写了一个自学的文档。<br /><br />这本《橘匪🍊的sora自学手册》都是我这几天精心整理的sora学习资源，总共有5000多字。<br /><br />这本手册的内容包括——<br /><br />1、Sora基础介绍‍<br /><br />‍‍‍‍‍‍2、Sora生成的AI视频及提示词合集‍<br /><br />3、Sora学习资源汇总<br /><br />4、Sora的10个变现思路<br /><br /> 有需要的可以直接查阅
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/65d2f64a38849f879f82011f</id>
            <title>Meta 发布了一个可以利用 AI 自动剪辑视频的 Agents LAVE。 这玩意再加上 Sora 这样的视频生成模型，一些简单的短视频以及广告视频基本上就不需要人工介入了，大...</title>
            <link>https://m.okjike.com/originalPosts/65d2f64a38849f879f82011f</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/65d2f64a38849f879f82011f</guid>
            <pubDate></pubDate>
            <updated>Mon, 19 Feb 2024 06:33:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    Meta 发布了一个可以利用 AI 自动剪辑视频的 Agents LAVE。<br />这玩意再加上 Sora 这样的视频生成模型，一些简单的短视频以及广告视频基本上就不需要人工介入了，大家以后刷的估计都是生成出来的视频了，想要啥有啥。<br /><br />我下面会简单介绍一下这个剪辑工具的界面组成和 Agents 设计：<br /><br />-----------工具界面及交互（图 1）-----------<br /><br />A 区域主要是输入提示词以及展示 LLM 详细的剪辑逻辑。<br /><br />B 区域是素材库，你可以鼠标 Hover 后获得 LLM 帮你总结的这段视频的内容，不需要播放查看， AI 也会自动生成的素材标题。<br /><br />E 区域就是传统的视频时间轴，AI 剪辑的视频就在这里，你也可以手动调整。<br /><br />-----------Agents 设计（图 2）-----------<br /><br />1️⃣系统提示前言：<br /><br />角色分配：一个开场段指示Agents担任视频编辑助理，负责根据用户命令生成行动计划。<br /><br />动作描述：在角色分配之后，描述了Agents可以执行的一系列动作。每个动作对应于LAVE支持的编辑功能。详细说明了每个动作的功能和用例，帮助Agents选择适当的响应以满足用户的命令。<br /><br />格式指导：最后，指导Agents以一致的格式输出行动计划：首先确定用户的编辑目标，然后列出逐步计划，列举建议的行动以实现该目标。<br /><br />其他系统提示：<br /><br />在前言之后，附加了最近的对话历史，以及最新的用户输入。这种组合形成了发送给LLM以生成行动计划的完整提示。<br /><br />2️⃣制定行动计划后，将其提交给用户进行批准：<br /><br />与批量批准不同，每个行动都由用户依次批准。这种方法允许用户执行一个行动，观察其结果，然后决定是否继续进行下一个行动。LAVE从行动计划中解析每个行动描述，并将其转化为相应的后端函数调用。<br /><br />3️⃣LAVE支持五种LLM功能：<br /><br />1）素材概览，2）创意头脑风暴，3）视频检索，4）故事板，5）剪辑修剪。前四种功能可通过Agents访问，而剪辑修剪可通过双击编辑时间轴上的剪辑时出现的窗口进行。<br /><br />其中，基于语言的视频检索是通过向量存储数据库实现的，而其余功能则是通过LLM提示工程实现的。所有功能都是基于自动生成的语言构建的。<br /><br />生成视觉叙述：以每秒一帧的速率对视频帧进行采样。然后使用建立在Vicuna-V1-13B 的LLaMA-V1-13B模型 的fine-tuned检查点LLaVA v1.0对每帧进行标题标注。<br /><br />检索功能利用向量存储：通过使用OpenAI的text-embedding-ada-002将每个视频的视觉叙述（标题和摘要）进行嵌入。<br /><br />将视频整合成共同的主题：提供用户视频收藏中主题的摘要。提示包括一个功能指令，然后是画廊视频的视觉叙述。然后将此提示发送到LLM以生成概览，随后在聊天界面中呈现给用户进行审阅。<br /><br />基于用户的所有视频进行视频编辑创意：提示结构以功能指令开头。如果提供了创意指导，会在提示中包含用户的创意指导，以引导头脑风暴。<br /><br />根据用户提供的叙述在序列中剪辑视频片段：与以前的功能不同，它只影响时间轴上的视频。与头脑风暴类似，系统会检查用户提供的叙述中是否有任何创意指导。<br /><br />4️⃣LAVE应用构建：<br /><br />LAVE系统实现为全栈Web应用程序。前端UI采用React.js开发，而后端服务器采用Flask。对于LLM推理，主要使用OpenAI的最新GPT-4模型。然而，为了将行动计划映射到功能，使用了gpt-4-0613检查点，专门针对函数调用的使用进行了微调。<br /><br />论文地址：https://arxiv.org/pdf/2402.10294.pdf<br /><img src="https://cdnv2.ruguoapp.com/Ft3iUSHncXz4a-QlNo7D-4ZLNCfKv3.png" /><br /><img src="https://cdnv2.ruguoapp.com/FvNz621Wn5SiZy30-CbYIsD88hsJv3.png" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/65d2eba837f7165b2163744a</id>
            <title>后续面对sora或者类sora的强大工具，我们怎么思考/怎么使用/或者要求我们具备什么样的能力？ 昨天上午写prompt时产生的想法 在使用时，我会反倒觉得不应该把他们...</title>
            <link>https://m.okjike.com/originalPosts/65d2eba837f7165b2163744a</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/65d2eba837f7165b2163744a</guid>
            <pubDate></pubDate>
            <updated>Mon, 19 Feb 2024 05:48:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    后续面对sora或者类sora的强大工具，我们怎么思考/怎么使用/或者要求我们具备什么样的能力？<br /><br />昨天上午写prompt时产生的想法<br /><br />在使用时，我会反倒觉得不应该把他们当作纯工具：他们应该是共同协作完成目标项目的partner，要让他们参与创作其中。<br /><br />如果是纯工具心态，你对自己的能力要求是我怎么才能更好地驾驭它、使用它，我如何讲好一个故事告诉他去执行；<br /><br />但Gen-AI是可以理解学习的，反倒在使用时应该适当留白，少点约束，即很核心的是如何平衡规范性与创造性
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/65d2d2b7a922aa28d06b592e</id>
            <title>上午看 Sora 的几点收获： 🟣 Sora完全站在了Openai成功产品的肩膀上。 chatGPT背后是个大语言模型，把一个句子拆成若干个token，可能是一个单词、一个词组、...</title>
            <link>https://m.okjike.com/originalPosts/65d2d2b7a922aa28d06b592e</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/65d2d2b7a922aa28d06b592e</guid>
            <pubDate></pubDate>
            <updated>Mon, 19 Feb 2024 04:01:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    上午看 Sora 的几点收获：<br /><br /> 🟣 Sora完全站在了Openai成功产品的肩膀上。<br /><br />chatGPT背后是个大语言模型，把一个句子拆成若干个token，可能是一个单词、一个词组、一个短句，通过海量数据训练，推测下一个最大概率的token（生成文字）。<br /><br />Sora模型，同样是把海量视频拆成一个个分块，配合GPT强大的语言能力，给视频分块增加和扩充文字描述。<br />当海量的训练视频都用这种分块统一拆分学习后，用户输入新指令，就可以从不同的分块里预测和生成新的视频分块，再变成一整条视频。<br /><br />即：用语言模型 👉🏻 把用户指令扩写和改写 👉🏻 输入视频模型 👉🏻 生成新视频<br /><br />这相当于人类给了一个作文题，语言模型写一篇描写场景的小作文，Sora再根据这篇作文生成视频，所以细节会比其他 AI 视频产品强太多。<br /><br />🟣 新世界降临前夕，我们普通人可以做什么？<br /><br />快刀青衣老师的观点：不管是文生视频、文生图，技术底层关注的是「生」，而我们普通人需要关注的是「文」。<br /><br />表达有短板、想象力不够，出来的图和视频是没有意境的。<br />🌅 有文化的你输入“大漠孤烟直，长河落日圆”，没文化的我输入“沙漠上空挂着一个圆太阳”，出来的效果就是卖家秀和买家秀的区别。<br /><br />保持阅读、在阅读的时候记录下具有画面感的段落、收集经典电影的精彩镜头…… 在技术逐渐平权的时代当下，期待我们每个人都能有“超能力”。<br /><img src="https://cdnv2.ruguoapp.com/lhXSDNQlNXVZjpLUyTGaD4dEFKLTv3.png" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/65d2b63e36246663240a34cb</id>
            <title>关于 Sora 的十点思考 1）Openai 出于狙击 Google 的目的，在相近的时间节点推出了 Sora，它是一个文本转视频的模型，可以做到输入 Prompt，输出视频内容；相较...</title>
            <link>https://m.okjike.com/originalPosts/65d2b63e36246663240a34cb</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/65d2b63e36246663240a34cb</guid>
            <pubDate></pubDate>
            <updated>Mon, 19 Feb 2024 02:00:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    关于 Sora 的十点思考<br /><br />1）Openai 出于狙击 Google 的目的，在相近的时间节点推出了 Sora，它是一个文本转视频的模型，可以做到输入 Prompt，输出视频内容；相较于竞争对手 Pika、Runway 等，Sora 拥有60s的视频长度、连贯性的画面、基础物理逻辑的遵循等特点；<br /><br />2）Sora 的诞生，让视频关联行业会产生较大的成本结构变化，会导致摄影、短视频、电影等行业产生较大的变化，同时会让一些原本受限于软件使用无法进入行业的人，可以依靠创意进入相关行业。鲁智深大战林黛玉的场景，以后只需要一句 Prompt 即可实现；<br /><br />3）对于创业公司来说，一定要仔细梳理Sam说过的话，考虑他的话不完全是为了营销目的进行宣传的话，而是有一定可能性已经实现或者即将实现；<br /><br />4）Sora 可能还需要至少3个月的时间才会推出，这段时间需要测试公众的反应，同时寻求规避潜在风险；<br /><br />5）有很大可能，Sora 的生成界面是嵌入在聊天对话内，但是第三方一定会考虑接入 Sora 的能力，宣传自己可以基于 Sora 的生成进行二次编辑，在这个过程当中，剪映、快影等应该都会跟进，Adobe 也会受到影响；<br /><br />6）从对公司的影响来看，做视频生成的相关公司受到直接的冲击，做视频编辑的公司受到一定的冲击，做视频分发的公司可能会需要想办法识别AI创作类视频。对于依靠视频作为主要素材来源的行业，比如广告、短视频博主等都可能会受到冲击，加剧竞争的烈度，淘汰一大批人，最后竞争升维；<br /><br />7）对普通人来说，要考虑的就是学会讲好故事，目前来说，文稿、视频、语音都可以通过不同AI工具的串联进行合并处理，一定会有公司化的方式运作视频内容的生成，这个过程会更简单以及轻量化，甚至可能5人以下的小团队就可以搞定；<br /><br />8）文生视频的进展可能比大部分人的预期最快的情况还要快很多，原本只想着能不能先到15秒，没想到可以直接推进到60秒，甚至1小时都不是难以想象的事情；<br /><br />9）目前整体的生成成本单次生成预估可能要超过1美元，对于 Openai 来说，如果不把成本降下来，工具可能还比较困难推进到公众面前。按照之前的迭代速度，通常半年左右会有一个新的版本出来，预估 Sora 到 3.0 或者 4.0 的时候，应该会产生飞跃；<br /><br />10）对于整体视频的生成，应该是一次性生成，甚至会支持一次性生成多机位多角度的视频，支持对单视频进行二次编辑，比如插入新素材或者处理已有素材等，但是如果想要做到更智能的生成，可能还需要一点时间。
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>