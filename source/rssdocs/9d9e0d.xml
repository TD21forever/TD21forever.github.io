<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>AI探索站 - 即刻圈子</title>
        <link>https://m.okjike.com/topics/63579abb6724cc583b9bba9a</link>
        
        <item>
            <id>https://m.okjike.com/originalPosts/6614c13dde5f287348867d28</id>
            <title>苹果发布了专门用于理解应用 UI 界面的 MLLLM Ferret-UI 。 专门针对移动UI屏幕进行了优化，具备了指向、定位和推理等多种能力。 看来 iOS 18 有可能会有类似通...</title>
            <link>https://m.okjike.com/originalPosts/6614c13dde5f287348867d28</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/6614c13dde5f287348867d28</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Apr 2024 04:17:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    苹果发布了专门用于理解应用 UI 界面的 MLLLM Ferret-UI 。<br /><br />专门针对移动UI屏幕进行了优化，具备了指向、定位和推理等多种能力。<br /><br />看来 iOS 18 有可能会有类似通过Siri自动操作应用界面的能力？<br /><br />---------详细介绍---------<br /><br />Ferret-UI模型的介绍与特点： <br /><br />Ferret-UI是一个新的MLLM，专门为提高对移动UI屏幕的理解而设计。它具备引用、定位和推理能力，<br /><br />能够处理UI屏幕上的各种任务。Ferret-UI的一个关键特点是其“任何分辨率”（any resolution）技术，该技术通过放大细节来解决UI屏幕中小型对象的识别问题，从而提高模型对UI元素的理解精度。<br /><br />移动用户界面（UI）屏幕的理解： <br /><br />UI屏幕的理解是一个复杂的问题，因为它不仅要求模型能够理解屏幕上的内容，还要能够识别和操作具体的UI元素。<br /><br />与传统的自然图像相比，UI屏幕通常具有更多的长宽比和更小的元素，这些元素对于模型来说是一个挑战。此外，UI屏幕的理解还涉及到对屏幕元素间关系的识别，以及对用户可能采取的行动的预测。<br /><br />“任何分辨率”（any resolution）技术的应用： <br /><br />为了克服UI屏幕中的小对象识别问题，Ferret-UI采用了“任何分辨率”技术。该技术通过将屏幕分割成基于原始宽高比的子图像，并对每个子图像进行单独编码，从而在不丢失重要视觉信号的情况下放大细节。<br /><br />这种方法使得模型能够更准确地识别和理解UI屏幕上的小型对象。<br /><br />训练样本的收集与任务制定：<br /><br /> Ferret-UI的训练涉及从基础UI任务到高级任务的广泛数据样本收集。这些样本被格式化为带有区域注释的指令遵循格式，以便于模型进行精确的引用和定位。<br /><br />此外，为了提高模型的推理能力，还特别编制了一个包含详细描述、感知/交互对话和功能推断等高级任务的数据集。<br /><br />模型架构与数据集的建立： <br /><br />Ferret-UI的架构基于Ferret模型，后者在自然图像的引用和定位任务中表现出色。为了适应UI屏幕的特点，Ferret-UI进行了架构调整，包括集成“任何分辨率”技术和使用预定义的网格配置来划分全图图像。<br /><br />此外，为了全面评估模型的能力，还建立了一个包含所有研究任务的综合测试基准。<br /><br />高级任务中的对话能力： <br /><br />Ferret-UI在高级任务中的对话能力表现突出，尤其是在详细描述和交互对话任务中。<br /><br />模型能够生成与视觉组件相关的详细讨论，并提出以特定目标为导向的行动计划。<br /><br />此外，Ferret-UI还能够通过功能推断来解释屏幕的整体目的，显示出在理解和生成自然语言指令方面的高级能力。<br /><br />论文地址：https://arxiv.org/abs/2404.05719<br /><img src="https://cdnv2.ruguoapp.com/FtiS1YdxCuvwIiXbxxwe-CHX5hEzv3.png" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/661481353b9c66cae404e17b</id>
            <title>大牛新作 完全开源的类 pplx 搜索引擎项目。 图片、来源、回答结构都跟 pplx 非常相似。 支持一键部署到 vercel。 项目地址 https://github.com/miurla/morphic?...</title>
            <link>https://m.okjike.com/originalPosts/661481353b9c66cae404e17b</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/661481353b9c66cae404e17b</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Apr 2024 23:43:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    大牛新作<br />完全开源的类 pplx 搜索引擎项目。<br />图片、来源、回答结构都跟 pplx 非常相似。<br />支持一键部署到 vercel。<br />项目地址 https://github.com/miurla/morphic?tab=readme-ov-file<br /><video controls="" src="https://videocdn.jellow.site/FshPbsrDo7YatY2Nj4DBL309b8cc.mp4?sign=a2516bc378c00d87ad3147721cb57f67&amp;t=66158630"></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/66139b75164d89e60154b96a</id>
            <title>这个可能比较重要，北大发布一个新的图像生成框架VAR。 VAR首次使GPT风格的AR模型在图像生成上超越了Diffusion transformer。 同时展现出了与大语言模型观察到的...</title>
            <link>https://m.okjike.com/originalPosts/66139b75164d89e60154b96a</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/66139b75164d89e60154b96a</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Apr 2024 07:23:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    这个可能比较重要，北大发布一个新的图像生成框架VAR。<br /><br />VAR首次使GPT风格的AR模型在图像生成上超越了Diffusion transformer。<br /><br />同时展现出了与大语言模型观察到的类似Scaling laws的规律。<br /><br />在ImageNet 256x256基准上,VAR将FID从18.65大幅提升到1.80,IS从80.4提升到356.4,推理速度提高了20倍。<br /><br />详细介绍：<br /><br />视觉自回归模型(VAR)是一种新的图像生成范式,它将自回归学习重新定义为从粗到细的"下一尺度预测"或"下一分辨率预测",有别于标准的光栅扫描"下一token预测"。<br /><br />这种简单直观的方法让自回归transformer能够快速学习视觉分布并具有良好的泛化能力:<br /><br />VAR首次使GPT风格的AR模型在图像生成上超越了扩散transformer。<br /><br />在ImageNet 256x256基准上,VAR将FID从18.65大幅提升到1.80,IS从80.4提升到356.4,推理速度提高了20倍。<br /><br />实证验证了VAR在多个维度包括图像质量、推理速度、数据效率和可扩展性上都优于Diffusion Transformer。<br /><br />随着VAR模型的扩大,它展现出了与大语言模型观察到的类似幂律缩放规律,线性相关系数接近-0.998,有力证明了这一点。<br /><br />VAR进一步展示了在下游任务如图像修复、外推和编辑上的零样本泛化能力。<br /><br />这些结果表明,VAR初步模拟了大语言模型的两个重要特性:缩放规律和零样本泛化。<br /><br />研究人员已经公开了所有模型和代码,以促进AR/VAR模型在视觉生成和统一学习中的探索。<br /><br />VAR算法为计算机视觉中的自回归算法设计提供了新的见解,有望推动这一领域的进一步发展。<br /><br />项目地址：https://github.com/FoundationVision/VAR<br />Demo 地址，生成速度真的非常快：https://var.vision/demo<br />模型下载：https://huggingface.co/FoundationVision/var/tree/main<br /><img src="https://cdnv2.ruguoapp.com/FoPTrLaClnuJl_dtiysPMeNtGPDmv3.png" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/66138b2a22562b4fb999056a</id>
            <title>很有意思的一个研究，让 LLM 帮助培训社交沟通技能，确实有很多人需要这样的服务，LLM 又擅长这个。 通过一个通用框架，利用大语言模型（LLM）进行社交技能训练...</title>
            <link>https://m.okjike.com/originalPosts/66138b2a22562b4fb999056a</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/66138b2a22562b4fb999056a</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Apr 2024 06:14:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    很有意思的一个研究，让 LLM 帮助培训社交沟通技能，确实有很多人需要这样的服务，LLM 又擅长这个。<br /><br />通过一个通用框架，利用大语言模型（LLM）进行社交技能训练。“AI伙伴，AI导师”框架将实际体验学习与真实场景练习和个性化反馈相结合。<br /><br />详细介绍：<br /><br />使用大语言模型进行社交技能训练的提议：<br /><br />研究者提出,可以利用大语言模型强大的对话生成能力,为社交技能练习提供一个随时可用、安全可控的环境。相关研究已经证实,当前的大语言模型已经能够较好地模拟各类人物,进行逼真的对话互动。这为将其应用于社交技能训练奠定了基础。<br /><br />AI Partner和AI Mentor框架的提出：<br /><br />论文提出了一个通用的社交技能训练框架,包括两个关键组件:AI Partner负责提供对话实践的环境,AI Mentor负责在关键节点给予个性化指导。二者协同,可以把体验式的实践学习与理论指导有机结合,有望大幅提升社交技能训练的可及性和有效性。<br /><br />使用该框架进行社交技能训练的应用场景<br /><br />该框架可以灵活应用于多个领域的社交技能训练,如心理咨询、谈判、教学等。通过调整AI Partner塑造的人物角色,以及AI Mentor搭载的领域知识库,就可以对应不同领域的训练需求。论文通过一系列案例展示了这种适用性和灵活性。<br /><br />论文地址：https://arxiv.org/abs/2404.04204<br /><img src="https://cdnv2.ruguoapp.com/FsEkF2ut7YWVnzGpnkTEPBCWJSXIv3.png" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/66136bb938849f879f3eab07</id>
            <title>Prompting 的核心技能可能只有一个…… 启动效应，是大脑最有趣的认知活动之一。每当一段旋律、一个拼图或一段故事出现，大脑就开始疯狂运算，猜测整个景观；不...</title>
            <link>https://m.okjike.com/originalPosts/66136bb938849f879f3eab07</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/66136bb938849f879f3eab07</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Apr 2024 03:59:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    Prompting 的核心技能可能只有一个……<br /><br />启动效应，是大脑最有趣的认知活动之一。每当一段旋律、一个拼图或一段故事出现，大脑就开始疯狂运算，猜测整个景观；不直觉的开始分析因果、构建起一个个可能的解释。<br /><br />不信的话，试着放松下来，聆听我这唱一首小曲：一闪一闪亮晶晶……（请接龙）<br /><br />启动效应的本质之一是基于先验的预测，它是多模态和多感官的。简单类比的话，Prompting 就是你如何激活大模型知识结构的「启动」。<br /><br />一旦能深刻意识到这一点，如何提升你与 AI 对话的技能、有效 激活 LLMs 效能的方法就会涌现出来了。<br /><br />通过成百上千小时的反复练习，你将意识到：真正提升 Prompt 核心技能在于，持续深化于你的认知体系。<br /><br />你无法提出你不知道的问题。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/66120aed164d89e601360692</id>
            <title>斯坦福这节课讲清楚了LLM做RAG所有最重要的问题。 这节课就是传说中的Stanford CS25中的一节讲座<Retrieval Augmented Language Models>。授课人就是RAG论文的作...</title>
            <link>https://m.okjike.com/originalPosts/66120aed164d89e601360692</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/66120aed164d89e601360692</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Apr 2024 02:54:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    斯坦福这节课讲清楚了LLM做RAG所有最重要的问题。<br /><br />这节课就是传说中的Stanford CS25中的一节讲座。授课人就是RAG论文的作者之一Douwe Kiela，课程中他分享了一个检索增强语言模型的架构图。<br /><br />这张图重要到让我觉得，做RAG只要记住这一张图就够了。所有相关概念和工程实践中的权衡，全都涵盖在这张图的架构和问题中了。<br /><br />这个架构主要包括input、prompt、通过retriever检索增强生成的context，然后把这三部分一起输入给generator即模型，最终输出output作为结果。<br /><br />❇️❇️关于这几个核心概念，值得注意的是：<br />1️⃣input和prompt的区别和联系在于，input可理解为既包含system prompt，又包含用户输入的检索范围的指向，而prompt则强调用户输入的指令。<br />🌟以公司知识库RAG举例，比如用户输入chatbot的内容为"检索公司2023年的财务数据并生成总结报告"，其中"公司2023年的财务数据"是对公司知识库检索范围的指向，应理解为input的一部分，而"检索并生成总结报告"则是指令，应理解为prompt。<br />2️⃣retriever的作用机制，我理解类似于在图书馆借书的过程，提供书名(query)-系统查找图书编号(query编码)-对应书架书籍编号(docs编码)-找到并借出图书(context)。<br />🌟接着上文公司知识库的例子，从input获取query(如"2023年资产负债表, 2023年利润表, 2023年现金流量表")，对应的query编码("2023年资产负债表, 2023年利润表, 2023年现金流量表"的向量化表达)在docs编码(公司知识库所有文本的向量化表达)中检索匹配，提取匹配的部分作为context(涉及公司2023年财务数据的文本)。<br />🌟其中query和input的关系，我想到两种可能性，一种是直接把input作为query，另一种是模型基于input生成的query，架构图简化表达了。<br />3️⃣retriever和context之间可加一步reranker架构，对检索结果按特定规则进行重新排序。reranking的机制既可通过模型判断，也可在模型基础上预设特定规则。<br />🌟比如根据员工职级限制其可获取的企业知识库信息范围。<br /><br />❇️❇️目前工程实践上，大家把优化的重点基本都放在了retrieve环节里，这里面涉及三个重要的问题：<br />1️⃣how and what do I retrieve：从传统的相似性检索、文本检索，到目前最常用的依托于embedding的语义检索，大家在实践中仍在不断迭代。Kiela后面也提到有研究希望把整个retriever过程做成一个模型，他也在课程中构想未来应该把retriever的训练也纳入到LLM的训练架构中。<br />🌟文本的embedding可简化理解为文本的向量化表达，并且可根据不同文本的向量化表达，判断出文本之间语义的远近亲疏关系。<br />🌟目前的文本emebedding也都是通过模型来实现的，这类模型也在不断迭代。OpenAI在今年1月份推出了text-embedding-3(small和large两版)，相比其2022年12月推出的ada-002模型，在性能上获得了显著提升。<br />🌟用于多语言检索的常用基准(MIRACL)平均分数已从 31.4%(ada-002)增加到 44.0%(3-small)和54.9%(3-large)。<br />🌟附图之一是OpenAI对其text emebedding模型作用机制的示意。<br />2️⃣When to retrieve: 一般就两种思路。一种是在获得检索范围后即retrieve，另一种是让模型判断何时retrieve。<br />3️⃣How to encode: 如何编码也直接影响了如何检索的过程。<br /><br />❇️❇️其他问题：<br />1️⃣how to pre-process: 实际上强调就是input要包含system prompt，可设定角色、技能、任务、工作流、限制条件等。<br />2️⃣how to prompt: 涉及提示词工程的方法论。<br />3️⃣how to pass context: 可以把context作为prompt的一部分以文本形式输入，也可通过代码的方式代入。<br />4️⃣how to post-process: 比如格式化输出的处理，如固定输出json格式，或固定在末尾输出reference列表等。<br />5️⃣how to verify: 指的是如何验证output的效果或质量，比如验证output与知识库的相关性、准确性等。<br /><br />❇️❇️最后，还有关于RAG整体架构的审视框架：<br />1️⃣How to optimize: 各环节哪些地方可以优化。架构中已经列出的问题都是思考的重点。<br />2️⃣How to learn: 这里的learn应该指的是机器学习的learn，探讨各环节从software 1.0的静态架构向机器学习和software 2.0的演进。<br />3️⃣how to scale: 如何应对规模化的问题。<br />🌟比如关于知识库如何chunk、何时编码，在知识库过大时就不适合提前预处理好chunk和编码。或者大量用户同时prompt该如何应对。<br /><br />❇️❇️前段时间判断过2024年会是RAG应用爆发的一年https://m.okjike.com/originalPosts/6602dca712ed2fda687ec0a3?s=ewoidSI6ICI2M2VlMjQ0NjhhMGY3NzVjODQyMmY1NzEiCn0=，自己在2B业务中也涉及RAG工程的落地，所以花了些精力来学习这节课。以上内容夹杂了不少自己的个人理解，欢迎批评指正，一起交流学习~<br /><br />❇️❇️links:<br />🌟Stanford CS25 V4 2024春季课程(面向公众开放，有人想一起学习搭子么？) https://web.stanford.edu/class/cs25/<br />🌟Stanford CS25 V3: Retrieval Augmented Language Models https://www.youtube.com/watch?v=mE7IDf2SmJg<br />🌟RAG论文原文 https://arxiv.org/abs/2005.11401<br />🌟OpenAI text-embedding-3 models https://openai.com/blog/new-embedding-models-and-api-updates?t<br />🌟OpenAI text-embedding-ada-002 model https://openai.com/blog/new-and-improved-embedding-model?t<br />🌟Software 2.0 by Andrej Karpathy https://karpathy.medium.com/software-2-0-a64152b37c35<br />🌟 Kiela在讲这节课几个月后在其创立的Contextual AI正式推出RAG 2.0 https://contextual.ai/introducing-rag2/<br /><img src="https://cdnv2.ruguoapp.com/Fk6Mop_pXFANq5a2xXwXJ-CgT996v3.jpg" /><br /><img src="https://cdnv2.ruguoapp.com/Fs6cm98LSOCQ9c5Y8RRNWy30Lo5tv3.jpg" /><br /><img src="https://cdnv2.ruguoapp.com/FqEqfyz3wHcRxWy7DJIHfNSAJWgsv3.png" /><br /><img src="https://cdnv2.ruguoapp.com/FrbI3mDJCUqH-jV2PFsm9ps1uCoHv3.jpg" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/66111a9ea922aa28d001a03e</id>
            <title>万字解析AI视频《Devices》工作流全网浏览2k+啦！ 工作流在OpenArt上有600views+140+下载啦！ 感谢AJ收录通往AGI之路！ 全文：https://mp.weixin.qq.com/s/tkcvm...</title>
            <link>https://m.okjike.com/originalPosts/66111a9ea922aa28d001a03e</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/66111a9ea922aa28d001a03e</guid>
            <pubDate></pubDate>
            <updated>Sat, 06 Apr 2024 09:49:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    万字解析AI视频《Devices》工作流全网浏览2k+啦！<br />工作流在OpenArt上有600views+140+下载啦！<br />感谢AJ收录通往AGI之路！<br /><br />全文：https://mp.weixin.qq.com/s/tkcvmtybmj2xHkIaeIQypw<br />工作流下载：https://openart.ai/workflows/pXkm6GZX8G19I22Odeo4<br /><br />Blender建模动画 + ComfyUI转绘 + 剪映剪辑<br />是一条很强大的工作流<br />如果你是想原创实验艺术、还是测试动画转绘，都可以去按照我分享的思路去试试。<br /><br />而且我分享的经验里不仅讲了我如何构思和实现的，还讲述了怎么debug修正遇到的很多错误。非常的干货。<br /><img src="https://cdnv2.ruguoapp.com/lkU-n97qKb3eQ_Ccey7M5H9F5HXmv3.jpg" /><br /><img src="https://cdnv2.ruguoapp.com/Fhpp7E4ASUdY3Tfec6lvWUyyDitWv3.png" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/660ed8046d9f1906314f9ce7</id>
            <title>斯坦福大学CS25课程将会向所有人开放，你可以通过Zoom接入直播参与课程。 英语好的人推荐看看，课程内容和讲师都很强。 这个课程主要会每周邀请Transformer研究...</title>
            <link>https://m.okjike.com/originalPosts/660ed8046d9f1906314f9ce7</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/660ed8046d9f1906314f9ce7</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Apr 2024 16:40:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    斯坦福大学CS25课程将会向所有人开放，你可以通过Zoom接入直播参与课程。<br /><br />英语好的人推荐看看，课程内容和讲师都很强。<br /><br />这个课程主要会每周邀请Transformer研究的前沿专家，来分享他们在最新突破上的成就。比如英伟达、OpenAI以及Mistral AI的人。<br /><br />内容涵盖从大型语言模型（如GPT和Gemini）到在艺术创作（比如DALL-E和Sora）、生物学和神经科学应用、机器人学等领域的创新应用。<br /><br />课程主页：https://web.stanford.edu/class/cs25/<br /><img src="https://cdnv2.ruguoapp.com/Fm4YCZ-ow5XcWbtK7vquh0nZrdGpv3.png" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/660e687738849f879fe66885</id>
            <title>最近在搞毕设，分享一个我的阅读论文文献SOP 论文，它的格式非常八股，基本都是按照这样的结构来呈现的：标题 → 概要 → 介绍 → 方法 → 实验 → 结论。 结合S...</title>
            <link>https://m.okjike.com/originalPosts/660e687738849f879fe66885</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/660e687738849f879fe66885</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Apr 2024 08:44:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    最近在搞毕设，分享一个我的阅读论文文献SOP<br /><br />论文，它的格式非常八股，基本都是按照这样的结构来呈现的：标题 → 概要 → 介绍 → 方法 → 实验 → 结论。<br /><br />结合Stanford教授 Srinivasan Keshav 分享的三遍法，我凝练出来一个结合AI的五步法<br /><br />1. 选读：通过检索工具选出合适的论文，以及在Twitter等社交媒体上看这篇论文的含金量<br /><br />2. 略读：读论文前，我们得搞清楚这篇论文值不值得读，我们不需要全部都读完，这样会浪费我们很多时间。使用AI Summary工具，回答关键问题：<br />  1. 主要解决了什么问题？<br />  2. 提出了什么解决方案？<br />  3. 解决方案中核心的方法/步骤/策略是什么？<br />  4. 结论是什么？<br />  5. 有什么限制条件？<br />  请有条理地组织以上信息，确保涵盖每一个点。<br />  这一类可以AI Summary的工具非常多，主要看总结能力和花费金额，推荐使用<br />  1. txyz：介绍在下面的工具安利里<br />  2. PopAI：介绍在下面的工具安利里<br />  3. ChatGPT：ChatGPT直接上传PDF，进行对话也可以<br />  4. Kimi：国内可以使用<br />  但是还有一个点需要注意，就是你自己需要去判断正确性，包括这篇论文的正确性——他有没有在和你胡扯<br /><br />3. 精读：接下来就开始精读了<br />  1. 精读的顺序：摘要 → 结论 → 方法 → 实验 → 数据和图表<br />  2. 对于需要翻译的同学来说，必备的就是“沉浸式翻译”这个插件，具体使用看下面<br />  3. 看图表和参考文献，判断是否正确<br />  4. 在这一步，遇到问题，也可以借助AI工具，<br />    1. 有不懂的细节继续提问，从 What、Why、How 三个方面抽取问题。<br /><br />What：即哪些概念是不熟悉的。<br /><br />Why：为什么要使用这种策略/方法，好在哪里？<br /><br />How：具体论文中是怎么实现某种策略/方法的。<br />    2. 对于不懂图表的，也可以借助AI工具，分析图表<br />      1. PopAI：接入了GPT-4V的API，可以对论文图表进行解读<br />      2. ChatGPT：直接读图<br />      3. 亿图图示：也可以解读图表<br /><br />4. 整理笔记：精读完就可以整理笔记了，这里我的思路是基于PDF本身去整理<br />  推荐一个工具UPDF，他有强大的编辑功能，能直接改动这篇PDF的文章内容，排版布局，能让你编辑PDF有编辑Word的那种体验，修改文字，字体、对齐、大小、颜色，复制文字。图片编辑也很丝滑，和Word一样。图片编辑功能可以修改页面原本图片的尺寸、旋转角度、裁剪、或者替换图像。也可以在页面任意位置添加新的文本或图片。辅助我们去理解文献。<br />  这样我们就可以直接在PDF里去记笔记，无论你是想添加高亮、还是 下划线 、文本注释、文本框、文本标注等都可以。还可以进行高亮和笔记。我们可以自定义我们高亮代表的意思，把文献拆分开来，辅助我们理解加深记忆，也方便我们日后查找相关内容，比如<br />  - 重点观点用黄色<br />  - 论证的过程和论据用橙色<br />  - 数据用红色<br />  - 等等<br />  他还有一些其他功能，有各种可爱的贴纸可以选择，无聊的时候可以鼓励一下自己<br /><br />5. 复盘：假设自己是作者，重新复盘整篇文章，如果是我，我会采取什么样的解决方案？我会怎么做实验？<br /><br />原文整理在：https://k5ms77k0o1.feishu.cn/wiki/wikcnRnDZ1yPM0PMpkAIoQipQnb?from=from_copylink<br /><img src="https://cdnv2.ruguoapp.com/FsrLaLAqPfLK0bGmoXqibOFDm55-v3.png" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/660e42b937f7165b21c733f8</id>
            <title>RAGFlow开源了一个RAG框架，有下面这些特点： RAGFlow的核心功能是文档的智能解析和管理，支持多种格式，并允许用户使用任何大型语言模型查询他们上传的文档。 R...</title>
            <link>https://m.okjike.com/originalPosts/660e42b937f7165b21c733f8</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/660e42b937f7165b21c733f8</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Apr 2024 06:03:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    RAGFlow开源了一个RAG框架，有下面这些特点：<br /><br />RAGFlow的核心功能是文档的智能解析和管理，支持多种格式，并允许用户使用任何大型语言模型查询他们上传的文档。<br /><br />RAGFlow提供了多种智能文档处理模板，以满足不同行业和角色的需求，如会计、人力资源专业人员和研究人员。<br /><br />它还强调了智能文档处理的可视化和可解释性，允许用户查看文档处理结果，进行比较、修改和查询。<br /><br />RAGFlow的一个关键优势是它允许LLM以受控方式回答问题，提供了一种理性和基于证据的方法来消除幻觉。<br /><br />项目地址：https://github.com/infiniflow/ragflow<br /><img src="https://cdnv2.ruguoapp.com/FmaDA5LqOYeGX8H_recEheKG-Uu5v3.png" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>