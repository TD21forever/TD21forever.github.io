<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>AI探索站 - 即刻圈子</title>
        <link>https://m.okjike.com/topics/63579abb6724cc583b9bba9a</link>
        
        <item>
            <id>https://m.okjike.com/originalPosts/661a3394de5f287348f20c02</id>
            <title>Vik Paruchuri 写了自己是如何从一个学历史的普通工程师，用了一年的时间学习AI并且训练出相当优秀的OCR PDF模型的历程。 里面给了一下他自己的学习路径和学习渠...</title>
            <link>https://m.okjike.com/originalPosts/661a3394de5f287348f20c02</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/661a3394de5f287348f20c02</guid>
            <pubDate></pubDate>
            <updated>Sat, 13 Apr 2024 07:26:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    Vik Paruchuri 写了自己是如何从一个学历史的普通工程师，用了一年的时间学习AI并且训练出相当优秀的OCR PDF模型的历程。<br /><br />里面给了一下他自己的学习路径和学习渠道，感觉想要入门的都可以看看。<br /><br />下面是总结的文章要点和全文翻译的链接。<br /><br />1️⃣实用技能<br /><br />如果你想进入AI领域，精通编程是首要任务。<br /><br />大多数情况下，掌握数据处理技能是必不可少的。<br /><br />能够辨别何时深入研究，何时采取快速简单的方案，是非常重要的技能。<br /><br />2️⃣学习资源<br /><br />书籍《深度学习》《机器学习的数学》<br /><br />视频教程：fast ai 和 Karpathy 的视频课程<br /><br />论文：RNN 注意力机制、Transformer、切换 Transformer、LoRA、视觉 Transformer、AdamW、GPT-2<br /><br />Discord：Nous Research和EleutherAI<br /><br />3️⃣学习要点<br /><br />理解基础知识对于训练高效模型至关重要。<br /><br />寻找并解决有趣的问题是提升你所构建系统影响力的最佳途径。<br /><br />实际上，并不需要很多GPU资源。<br /><br />详细的全文翻译：https://quail.ink/op7418/p/e5a682e4bd95e5bc80e5a78be6b7b1e5baa6e5ada6e4b9a0e79a84e69785e7a88b<br /><img src="https://cdnv2.ruguoapp.com/FnRJLeH6M9JPCD2CGcGi_F0ku6P6v3.png" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/6619543e37f7165b2196ad01</id>
            <title>3Blue1Brown 刚出了 Transformer 的系列科普视频，做得很好。之前看过不少讲 Transformer 的课程和文章，包括李宏毅老师的课程在内，最后都陷在矩阵运算的过程里...</title>
            <link>https://m.okjike.com/originalPosts/6619543e37f7165b2196ad01</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/6619543e37f7165b2196ad01</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Apr 2024 15:33:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <img src="https://i1.hdslb.com/bfs/archive/05b9cb88d0fdd73260b83ae7081e9139c8836a7e.jpg@100w_100h_1c.png" /><br />            <a href="https://www.bilibili.com/video/BV1wt421J7XB?spm_id_from=333.880.my_history.page.click">【3Blue1Brown熟肉】注意力机制可视化_哔哩哔哩_bilibili</a><br />        <br />3Blue1Brown 刚出了 Transformer 的系列科普视频，做得很好。之前看过不少讲 Transformer 的课程和文章，包括李宏毅老师的课程在内，最后都陷在矩阵运算的过程里，几乎没有能把 K、Q、V三个矩阵的象征意义讲清楚的。3Blue1Brown通过自己最擅长的动画和类比，把这套 Attention 的原理讲得比较浅显和直白。<br /><br />具体来说，“Attention 像是问每个 vector 一连串问题，然后根据这串问题的答案来更新自己。” Query 矩阵就像是在问："Are you in English?", “Are you a noun?”, "Do you refer to a person?", "Are you a number?", "Is your tone positive?" 等等，Key 矩阵就像是 vector 对这个问题的答案，而 Value 矩阵则代表向量自己根据这个答案和相关性权重进行的自我调整。整个过程有点像是物理中的受力分析，每个 Attention Head 代表一种力，通过 Q 和 K 找到所有施力的对象，再通过 V 来计算受力的大小， 最后，把多个 Attention Head 代表的多个力进行加总，计算出合力的方向和大小，作用在最后一个Vector上，从而指向 next embedding。之所以叫 transformer，就是指各个不同的力汇总在一起，将原本的 vector 扭曲到了一个新的方向上。<br /><br />相比之前的 RNN、LSTM 之类的模型，Transformer 的强大在于其支持并发计算。细想之下，这种并行的自注意机制颠覆了语言中的时间观，顺序不再重要。这让我想起《你一生的故事》/ 《降临》里七肢桶的语言 - 把完整的生命在眼前一下子同时铺开，没有先后，没有早晚，没有时间。类似的，Sora 中的所谓 spacetime patches，索性把空间也和时间打包在一起，颇像是爱因斯坦相对论里对“时空”的理解。或许，所谓的时间、空间，其实都是伪概念，只不过是 tokens/patches 的一种分布方式而已。还挺有趣的。<br /><br />P.S. 到目前为止看过的对 Diffusion 扩散思想的最好类比来自李宏毅老师的课程，他把扩散模型的去噪过程比作工匠雕刻石头的过程，“雕像本来就在石头里，米开朗基罗只是把不要的部分去掉”。某种程度上，这个减熵过程也颇像是逆转时间。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/6618d839362466632435d7eb</id>
            <title>红杉发布了 2024 他们评选出的前 50 家 AI 公司。 下面是一些观察到的一些要点，基于这些判断未来的公司应该是什么样的，后半部分尤其值得关注： 1️⃣ 当前的趋...</title>
            <link>https://m.okjike.com/originalPosts/6618d839362466632435d7eb</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/6618d839362466632435d7eb</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Apr 2024 06:44:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    红杉发布了 2024 他们评选出的前 50 家 AI 公司。<br /><br />下面是一些观察到的一些要点，基于这些判断未来的公司应该是什么样的，后半部分尤其值得关注：<br /><br />1️⃣ 当前的趋势<br /><br />突显了生成式人工智能如何提高企业和行业的生产力。企业通用生产力类别今年翻了一番，从四家公司增加到八家。<br /><br />图像编辑器 Photoroom，视频生成应用程序 Pika 和游戏构建器 Rosebud 表明，消费者和半专业用户之间的界限在创意软件领域变得模糊。<br /><br />行业垂直类别较少，但出现了一个新的工业领域。机器人领域的 Figure，工业维护领域的 Tractian 和自动驾驶领域的 Waabi 开始展示了人工智能软件与硬件的整合将如何改变物理世界的工作。<br /><br />2023年对基础设施来说是一个强劲的年份，其中包括一些强大的新参与者，如Mistral，它是基础模型领域的主要竞争者。<br /><br />2️⃣ 未来的公司应该是什么样的？<br /><br />技术创新的前几波浪潮——网络、互联网和移动技术——主要是通信革命。人工智能承诺带来不同的东西——一场生产力革命，更类似于个人电脑，它塑造了商业和工业的未来。<br /><br />随着越来越多的人工智能的发展，它们将开始作为人工智能网络一起工作。在过去的一年中，我们已经看到生成式人工智能不仅仅局限于简单的文本或代码生成，而是涉及到主动交互。<br /><br />在这个生成式的未来中，公司的建设本身可能成为人工智能Agents的工作；而有一天，整个公司可能像神经网络一样运作。<br /><br />公司的形成将变得更快、更灵活，拥有新的所有权和管理结构。也许有一天，会有由单个AI工程师运营的大公司。<br /><br />未来的大多数公司不会是个体经营的公司，但它们将有不同的需求和不同的痛点，与今天的公司不同。它们将需要能够解决知识管理和内容生成、信任、安全和认证方面挑战的企业产品。<br /><br />赢得未来企业的心与思想，创始人需要回答一些关键问题。这些公司将生产什么样的产品？他们需要什么样的基础设施和应用程序？劳动力将如何变化？分销和价值捕获的模式将如何改变？他们的总可寻址市场中，人类占多大比例，自主AI Agents占多大比例？<br /><br />3️⃣ 接下来会发生什么？<br /><br />生产力革命，如人工智能革命，推动成本下降。本世纪的技术进步已经彻底降低了硬件成本，但人类提供的服务成本，从医疗保健到教育，却飙升了。<br /><br />人工智能有潜力在这些关键领域降低成本，使其更加可获得和负担得起。这些变化需要负责任地进行，以减少失业并推动就业创造。人工智能将使我们能够用更少的资源做更多的事情，但我们需要政府和私人努力来重新培训和赋能每个人。<br /><br />人工智能被定位为改变我们社会中一些最关键领域的成本结构和提高生产力。它有潜力通过抽象化琐碎的工作，使我们能够将注意力集中在更重要的问题和更好的未来工具上，从而带来更好的教育、更健康的人口和更高效的人们。<br /><br />2024年AI 50捕捉到了人工智能的广泛范围。该列表的应用比以往任何时候都更加通用，我们预计它在未来几年将在深度和广度上进一步扩展。2024年只是个开始。<br /><br />全文：https://www.sequoiacap.com/article/ai-50-2024/<br /><img src="https://cdnv2.ruguoapp.com/FrwBTWkGhPYplSrTaoZG9s3bWnGKv3.png" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/6617a9c1a922aa28d07c4d45</id>
            <title>好东西，Stable Diffusion 生态最重要的 70 篇论文精选。 而且还根据不同的作用做了分类，除了论文地址还有对应的代码仓库和模型下载地址。 非常适合深入学习 SD...</title>
            <link>https://m.okjike.com/originalPosts/6617a9c1a922aa28d07c4d45</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/6617a9c1a922aa28d07c4d45</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Apr 2024 09:13:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    好东西，Stable Diffusion 生态最重要的 70 篇论文精选。<br /><br />而且还根据不同的作用做了分类，除了论文地址还有对应的代码仓库和模型下载地址。<br /><br />非常适合深入学习 SD 的朋友研究。<br /><br />https://latentbox.com/zh/sd-ecology<br /><img src="https://cdnv2.ruguoapp.com/FrkTrjTLWhgSNUehqysCou_nP3qxv3.png" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/6616979c12ed2fda68e88d9b</id>
            <title>AI生成PPT工具</title>
            <link>https://m.okjike.com/originalPosts/6616979c12ed2fda68e88d9b</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/6616979c12ed2fda68e88d9b</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Apr 2024 13:43:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    AI生成PPT工具<br /><img src="https://cdnv2.ruguoapp.com/FieTJgmFnrmEMsZf2rVmMESqCfkcv3.jpg" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/661674246d9f190631d9bc63</id>
            <title>有木有即友知道这个用什么AI软件生成的😂😂</title>
            <link>https://m.okjike.com/originalPosts/661674246d9f190631d9bc63</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/661674246d9f190631d9bc63</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Apr 2024 11:12:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    有木有即友知道这个用什么AI软件生成的😂😂<br /><img src="https://cdnv2.ruguoapp.com/FlR9VoAWEi_o7-y26qhvxRBXZE7gv3.png" /><br /><img src="https://cdnv2.ruguoapp.com/FgyFSF9oRbspjctsmbI_RdF0zT2iv3.png" /><br /><img src="https://cdnv2.ruguoapp.com/FggBddI1XnQx0jt8UtnuzTHKobpPv3.png" /><br /><img src="https://cdnv2.ruguoapp.com/Fj_YTUtOKEZEhXgcdLQqwueGQr54v3.png" /><br /><img src="https://cdnv2.ruguoapp.com/Fun081DFDzHwUBctaFYkC_GD8jKJv3.png" /><br /><img src="https://cdnv2.ruguoapp.com/lq7ldMhf9FRyScbi-DyG4PPfRiUtv3.png" /><br /><img src="https://cdnv2.ruguoapp.com/FvczvPvUbis9yBWM0iLAXZ3tQ5Tjv3.png" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/66139b75164d89e60154b96a</id>
            <title>这个可能比较重要，北大发布一个新的图像生成框架VAR。 VAR首次使GPT风格的AR模型在图像生成上超越了Diffusion transformer。 同时展现出了与大语言模型观察到的...</title>
            <link>https://m.okjike.com/originalPosts/66139b75164d89e60154b96a</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/66139b75164d89e60154b96a</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Apr 2024 07:23:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    这个可能比较重要，北大发布一个新的图像生成框架VAR。<br /><br />VAR首次使GPT风格的AR模型在图像生成上超越了Diffusion transformer。<br /><br />同时展现出了与大语言模型观察到的类似Scaling laws的规律。<br /><br />在ImageNet 256x256基准上,VAR将FID从18.65大幅提升到1.80,IS从80.4提升到356.4,推理速度提高了20倍。<br /><br />详细介绍：<br /><br />视觉自回归模型(VAR)是一种新的图像生成范式,它将自回归学习重新定义为从粗到细的"下一尺度预测"或"下一分辨率预测",有别于标准的光栅扫描"下一token预测"。<br /><br />这种简单直观的方法让自回归transformer能够快速学习视觉分布并具有良好的泛化能力:<br /><br />VAR首次使GPT风格的AR模型在图像生成上超越了扩散transformer。<br /><br />在ImageNet 256x256基准上,VAR将FID从18.65大幅提升到1.80,IS从80.4提升到356.4,推理速度提高了20倍。<br /><br />实证验证了VAR在多个维度包括图像质量、推理速度、数据效率和可扩展性上都优于Diffusion Transformer。<br /><br />随着VAR模型的扩大,它展现出了与大语言模型观察到的类似幂律缩放规律,线性相关系数接近-0.998,有力证明了这一点。<br /><br />VAR进一步展示了在下游任务如图像修复、外推和编辑上的零样本泛化能力。<br /><br />这些结果表明,VAR初步模拟了大语言模型的两个重要特性:缩放规律和零样本泛化。<br /><br />研究人员已经公开了所有模型和代码,以促进AR/VAR模型在视觉生成和统一学习中的探索。<br /><br />VAR算法为计算机视觉中的自回归算法设计提供了新的见解,有望推动这一领域的进一步发展。<br /><br />项目地址：https://github.com/FoundationVision/VAR<br />Demo 地址，生成速度真的非常快：https://var.vision/demo<br />模型下载：https://huggingface.co/FoundationVision/var/tree/main<br /><img src="https://cdnv2.ruguoapp.com/FoPTrLaClnuJl_dtiysPMeNtGPDmv3.png" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/66138b2a22562b4fb999056a</id>
            <title>很有意思的一个研究，让 LLM 帮助培训社交沟通技能，确实有很多人需要这样的服务，LLM 又擅长这个。 通过一个通用框架，利用大语言模型（LLM）进行社交技能训练...</title>
            <link>https://m.okjike.com/originalPosts/66138b2a22562b4fb999056a</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/66138b2a22562b4fb999056a</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Apr 2024 06:14:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    很有意思的一个研究，让 LLM 帮助培训社交沟通技能，确实有很多人需要这样的服务，LLM 又擅长这个。<br /><br />通过一个通用框架，利用大语言模型（LLM）进行社交技能训练。“AI伙伴，AI导师”框架将实际体验学习与真实场景练习和个性化反馈相结合。<br /><br />详细介绍：<br /><br />使用大语言模型进行社交技能训练的提议：<br /><br />研究者提出,可以利用大语言模型强大的对话生成能力,为社交技能练习提供一个随时可用、安全可控的环境。相关研究已经证实,当前的大语言模型已经能够较好地模拟各类人物,进行逼真的对话互动。这为将其应用于社交技能训练奠定了基础。<br /><br />AI Partner和AI Mentor框架的提出：<br /><br />论文提出了一个通用的社交技能训练框架,包括两个关键组件:AI Partner负责提供对话实践的环境,AI Mentor负责在关键节点给予个性化指导。二者协同,可以把体验式的实践学习与理论指导有机结合,有望大幅提升社交技能训练的可及性和有效性。<br /><br />使用该框架进行社交技能训练的应用场景<br /><br />该框架可以灵活应用于多个领域的社交技能训练,如心理咨询、谈判、教学等。通过调整AI Partner塑造的人物角色,以及AI Mentor搭载的领域知识库,就可以对应不同领域的训练需求。论文通过一系列案例展示了这种适用性和灵活性。<br /><br />论文地址：https://arxiv.org/abs/2404.04204<br /><img src="https://cdnv2.ruguoapp.com/FsEkF2ut7YWVnzGpnkTEPBCWJSXIv3.png" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/66136bb938849f879f3eab07</id>
            <title>Prompting 的核心技能可能只有一个…… 启动效应，是大脑最有趣的认知活动之一。每当一段旋律、一个拼图或一段故事出现，大脑就开始疯狂运算，猜测整个景观；不...</title>
            <link>https://m.okjike.com/originalPosts/66136bb938849f879f3eab07</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/66136bb938849f879f3eab07</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Apr 2024 03:59:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    Prompting 的核心技能可能只有一个……<br /><br />启动效应，是大脑最有趣的认知活动之一。每当一段旋律、一个拼图或一段故事出现，大脑就开始疯狂运算，猜测整个景观；不直觉的开始分析因果、构建起一个个可能的解释。<br /><br />不信的话，试着放松下来，聆听我这唱一首小曲：一闪一闪亮晶晶……（请接龙）<br /><br />启动效应的本质之一是基于先验的预测，它是多模态和多感官的。简单类比的话，Prompting 就是你如何激活大模型知识结构的「启动」。<br /><br />一旦能深刻意识到这一点，如何提升你与 AI 对话的技能、有效 激活 LLMs 效能的方法就会涌现出来了。<br /><br />通过成百上千小时的反复练习，你将意识到：真正提升 Prompt 核心技能在于，持续深化于你的认知体系。<br /><br />你无法提出你不知道的问题。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/66120aed164d89e601360692</id>
            <title>斯坦福这节课讲清楚了LLM做RAG所有最重要的问题。 这节课就是传说中的Stanford CS25中的一节讲座<Retrieval Augmented Language Models>。授课人就是RAG论文的作...</title>
            <link>https://m.okjike.com/originalPosts/66120aed164d89e601360692</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/66120aed164d89e601360692</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Apr 2024 02:54:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    斯坦福这节课讲清楚了LLM做RAG所有最重要的问题。<br /><br />这节课就是传说中的Stanford CS25中的一节讲座。授课人就是RAG论文的作者之一Douwe Kiela，课程中他分享了一个检索增强语言模型的架构图。<br /><br />这张图重要到让我觉得，做RAG只要记住这一张图就够了。所有相关概念和工程实践中的权衡，全都涵盖在这张图的架构和问题中了。<br /><br />这个架构主要包括input、prompt、通过retriever检索增强生成的context，然后把这三部分一起输入给generator即模型，最终输出output作为结果。<br /><br />❇️❇️关于这几个核心概念，值得注意的是：<br />1️⃣input和prompt的区别和联系在于，input可理解为既包含system prompt，又包含用户输入的检索范围的指向，而prompt则强调用户输入的指令。<br />🌟以公司知识库RAG举例，比如用户输入chatbot的内容为"检索公司2023年的财务数据并生成总结报告"，其中"公司2023年的财务数据"是对公司知识库检索范围的指向，应理解为input的一部分，而"检索并生成总结报告"则是指令，应理解为prompt。<br />2️⃣retriever的作用机制，我理解类似于在图书馆借书的过程，提供书名(query)-系统查找图书编号(query编码)-对应书架书籍编号(docs编码)-找到并借出图书(context)。<br />🌟接着上文公司知识库的例子，从input获取query(如"2023年资产负债表, 2023年利润表, 2023年现金流量表")，对应的query编码("2023年资产负债表, 2023年利润表, 2023年现金流量表"的向量化表达)在docs编码(公司知识库所有文本的向量化表达)中检索匹配，提取匹配的部分作为context(涉及公司2023年财务数据的文本)。<br />🌟其中query和input的关系，我想到两种可能性，一种是直接把input作为query，另一种是模型基于input生成的query，架构图简化表达了。<br />3️⃣retriever和context之间可加一步reranker架构，对检索结果按特定规则进行重新排序。reranking的机制既可通过模型判断，也可在模型基础上预设特定规则。<br />🌟比如根据员工职级限制其可获取的企业知识库信息范围。<br /><br />❇️❇️目前工程实践上，大家把优化的重点基本都放在了retrieve环节里，这里面涉及三个重要的问题：<br />1️⃣how and what do I retrieve：从传统的相似性检索、文本检索，到目前最常用的依托于embedding的语义检索，大家在实践中仍在不断迭代。Kiela后面也提到有研究希望把整个retriever过程做成一个模型，他也在课程中构想未来应该把retriever的训练也纳入到LLM的训练架构中。<br />🌟文本的embedding可简化理解为文本的向量化表达，并且可根据不同文本的向量化表达，判断出文本之间语义的远近亲疏关系。<br />🌟目前的文本emebedding也都是通过模型来实现的，这类模型也在不断迭代。OpenAI在今年1月份推出了text-embedding-3(small和large两版)，相比其2022年12月推出的ada-002模型，在性能上获得了显著提升。<br />🌟用于多语言检索的常用基准(MIRACL)平均分数已从 31.4%(ada-002)增加到 44.0%(3-small)和54.9%(3-large)。<br />🌟附图之一是OpenAI对其text emebedding模型作用机制的示意。<br />2️⃣When to retrieve: 一般就两种思路。一种是在获得检索范围后即retrieve，另一种是让模型判断何时retrieve。<br />3️⃣How to encode: 如何编码也直接影响了如何检索的过程。<br /><br />❇️❇️其他问题：<br />1️⃣how to pre-process: 实际上强调就是input要包含system prompt，可设定角色、技能、任务、工作流、限制条件等。<br />2️⃣how to prompt: 涉及提示词工程的方法论。<br />3️⃣how to pass context: 可以把context作为prompt的一部分以文本形式输入，也可通过代码的方式代入。<br />4️⃣how to post-process: 比如格式化输出的处理，如固定输出json格式，或固定在末尾输出reference列表等。<br />5️⃣how to verify: 指的是如何验证output的效果或质量，比如验证output与知识库的相关性、准确性等。<br /><br />❇️❇️最后，还有关于RAG整体架构的审视框架：<br />1️⃣How to optimize: 各环节哪些地方可以优化。架构中已经列出的问题都是思考的重点。<br />2️⃣How to learn: 这里的learn应该指的是机器学习的learn，探讨各环节从software 1.0的静态架构向机器学习和software 2.0的演进。<br />3️⃣how to scale: 如何应对规模化的问题。<br />🌟比如关于知识库如何chunk、何时编码，在知识库过大时就不适合提前预处理好chunk和编码。或者大量用户同时prompt该如何应对。<br /><br />❇️❇️前段时间判断过2024年会是RAG应用爆发的一年https://m.okjike.com/originalPosts/6602dca712ed2fda687ec0a3?s=ewoidSI6ICI2M2VlMjQ0NjhhMGY3NzVjODQyMmY1NzEiCn0=，自己在2B业务中也涉及RAG工程的落地，所以花了些精力来学习这节课。以上内容夹杂了不少自己的个人理解，欢迎批评指正，一起交流学习~<br /><br />❇️❇️links:<br />🌟Stanford CS25 V4 2024春季课程(面向公众开放，有人想一起学习搭子么？) https://web.stanford.edu/class/cs25/<br />🌟Stanford CS25 V3: Retrieval Augmented Language Models https://www.youtube.com/watch?v=mE7IDf2SmJg<br />🌟RAG论文原文 https://arxiv.org/abs/2005.11401<br />🌟OpenAI text-embedding-3 models https://openai.com/blog/new-embedding-models-and-api-updates?t<br />🌟OpenAI text-embedding-ada-002 model https://openai.com/blog/new-and-improved-embedding-model?t<br />🌟Software 2.0 by Andrej Karpathy https://karpathy.medium.com/software-2-0-a64152b37c35<br />🌟 Kiela在讲这节课几个月后在其创立的Contextual AI正式推出RAG 2.0 https://contextual.ai/introducing-rag2/<br /><img src="https://cdnv2.ruguoapp.com/Fk6Mop_pXFANq5a2xXwXJ-CgT996v3.jpg" /><br /><img src="https://cdnv2.ruguoapp.com/Fs6cm98LSOCQ9c5Y8RRNWy30Lo5tv3.jpg" /><br /><img src="https://cdnv2.ruguoapp.com/FqEqfyz3wHcRxWy7DJIHfNSAJWgsv3.png" /><br /><img src="https://cdnv2.ruguoapp.com/FrbI3mDJCUqH-jV2PFsm9ps1uCoHv3.jpg" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>