<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>AI探索站 - 即刻圈子</title>
        <link>https://m.okjike.com/topics/63579abb6724cc583b9bba9a</link>
        
        <item>
            <id>https://m.okjike.com/originalPosts/6819dcf6070109da49ad7802</id>
            <title>AI探索站 05月06日</title>
            <link>https://m.okjike.com/originalPosts/6819dcf6070109da49ad7802</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/6819dcf6070109da49ad7802</guid>
            <pubDate></pubDate>
            <updated>Tue, 06 May 2025 09:57:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    对于想要大概了解 Flow Matching 的童鞋，推荐 MIT 的这门小课 https://diffusion.csail.mit.edu/，对于核心概念讲解清晰且符合物理直觉。flow matching 的原理我感觉比 diffusion 更易理解（而且还SOTA :D）。讲一下我从小白视角的理解：<br />文生图（无 condition）的过程，我们可以理解成是从一个纯随机的正态分布采样一个点，逐渐把它变成很“真”的一张图片。diffusion 是逐渐把采样到的这个“白噪音”点不断“去噪”，变成一张图片。而 flow matching，是让一个“磁场”去推着这个点不断移动，最终“移动”变成一张图片（想象图片 vector 各个维度的数值有加有减不断变化）。<br />稍微展开来说，flow matching 本质上是在学习一个 vector field/向量场（一个vector field定义一个ODE），可以把它想象成一个磁场。一开始我们只有一个符合正态分布的“沙堆”，我们的目标是逐渐”推移“这个沙堆，让它最终的分布符合我们要的分布（真实世界的图片）。对于每粒沙子在每个时间点、每个位置，磁场力的方向（往哪个方向推）就是我们要 neural network 学习的东西。一粒沙子从初始位置到目标位置”被磁场推着“经过的路，就是一个 flow（ODE 的一个解），不同沙子走出了多条 flow 形成多个训练数据不断调教 NN 去学习磁场里的方向，大量平均下来就是我们想要的磁场/模型。<br />细心的童鞋可能会问，那 NN 咋知道往哪推啊？给定了一张图片，对于”想要成为它的沙子“，在一个时间点和一个位置，我们磁场力的方向是提前设计好的（conditional vector field），这样 NN 对于一个样本往哪推是知道的。我们不知道的是大量这样的数据，最终让 NN 平均下来会学成个啥样，即 marginal vector field（就是我们想要的）。<br />最后附几张作业截图证明我不是瞎吹牛（btw 作业很简单）
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/6819ca97070109da49ac17dc</id>
            <title>AI探索站 05月06日</title>
            <link>https://m.okjike.com/originalPosts/6819ca97070109da49ac17dc</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/6819ca97070109da49ac17dc</guid>
            <pubDate></pubDate>
            <updated>Tue, 06 May 2025 08:38:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    写了一篇详细的教程<br /><br />教大家如何生成这种一图流模型或者产品介绍宣传图<br /><br />顺便教一下怎么用 Figma 修改从网页生成的图片<br /><br />不会用 Figma 的也可以跟着学一下<br /><br />完整教程和提示词：https://mp.weixin.qq.com/s/uQQ7R8rBUXZ6EoxX4WxMRg
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/6819c64c45497b31a91f86c0</id>
            <title>AI探索站 05月06日</title>
            <link>https://m.okjike.com/originalPosts/6819c64c45497b31a91f86c0</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/6819c64c45497b31a91f86c0</guid>
            <pubDate></pubDate>
            <updated>Tue, 06 May 2025 08:20:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    谷歌又大发慈悲送 Gemini 高级会员了！！<br /><br />所有美国学生都可以免费用 Gemini 高级版到 2026 年<br /><br />你有一个美国 IP 就可以领取<br /><br />8 月会验证学生身份，所以最少可以白嫖三个月朋友们<br /><br />网页上点击“get offer”按钮就行<br /><br />这里领取：https://gemini.google/students/
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68199d047cd0e3a451361a48</id>
            <title>AI探索站 05月06日</title>
            <link>https://m.okjike.com/originalPosts/68199d047cd0e3a451361a48</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68199d047cd0e3a451361a48</guid>
            <pubDate></pubDate>
            <updated>Tue, 06 May 2025 05:24:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    Anthropic真是赛博菩萨，编写了大量关于prompt评测、提示词工程的交互式课程，完全都是免费的，质量超级高！<br /><br />都在这个git仓库里：https://github.com/anthropics/courses/blob/master/prompt_evaluations/README.md
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68199cc7070109da49a8e0d1</id>
            <title>AI探索站 05月06日</title>
            <link>https://m.okjike.com/originalPosts/68199cc7070109da49a8e0d1</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68199cc7070109da49a8e0d1</guid>
            <pubDate></pubDate>
            <updated>Tue, 06 May 2025 05:23:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    Google 今天这个大礼包。。。<br />已经分不清到底是营销，还是出 BUG 了。<br />只要登录就可以免费领取 Google One 会员到 2026 年底<br />包括 Google 旗下所有的 AI 服务会员。<br />仅限美国 IP，实测成功。<br />领取地址： https://gemini.google/students/
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68177c567cb8c547e28dcea8</id>
            <title>AI探索站 05月04日</title>
            <link>https://m.okjike.com/originalPosts/68177c567cb8c547e28dcea8</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68177c567cb8c547e28dcea8</guid>
            <pubDate></pubDate>
            <updated>Sun, 04 May 2025 14:40:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    现在养成的一个新习惯：<br /><br />每当我遇到困惑想请教别人时，先和 AI 聊一会。<br /><br />如果和 AI 讨论完，还没解决，再找「人」请教。<br /><br />这时候出手的那个「提问」，通常会超越我最早的那个「疑惑」。<br /><br />一个好问题，就这么诞生了。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/6817736997f435a697dac9d4</id>
            <title>AI探索站 05月04日</title>
            <link>https://m.okjike.com/originalPosts/6817736997f435a697dac9d4</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/6817736997f435a697dac9d4</guid>
            <pubDate></pubDate>
            <updated>Sun, 04 May 2025 14:02:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    很多大理的数字游民都去了良渚，我从杭州到了大理，今天租了一栋楼，要为大理扳回一局，把AI氛围搞起来。 只要为社区做点啥，就能来免费入住，欢迎带项目入驻，一起vibe coding
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/680afa8fc0d292d6fd3d4d79</id>
            <title>AI探索站 04月25日</title>
            <link>https://m.okjike.com/originalPosts/680afa8fc0d292d6fd3d4d79</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/680afa8fc0d292d6fd3d4d79</guid>
            <pubDate></pubDate>
            <updated>Fri, 25 Apr 2025 02:59:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    受Dan Shipper启发，也分享下我当前的模型堆栈。<br /><br /> AI带来的新世界是「非零和」的🌳
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/680a06ada4cf301fc47966f9</id>
            <title>AI探索站 04月24日</title>
            <link>https://m.okjike.com/originalPosts/680a06ada4cf301fc47966f9</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/680a06ada4cf301fc47966f9</guid>
            <pubDate></pubDate>
            <updated>Thu, 24 Apr 2025 09:38:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    妈的 Gemini 2.5 Pro 代码潜力无限啊<br /><br />针对性优化了一下我的提示词，看看这动画和这个排版<br /><br />是不是有苹果的味道了，还有这个数据可视化
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/6800a3c87cb8c547e207ff18</id>
            <title>AI探索站 04月17日</title>
            <link>https://m.okjike.com/originalPosts/6800a3c87cb8c547e207ff18</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/6800a3c87cb8c547e207ff18</guid>
            <pubDate></pubDate>
            <updated>Thu, 17 Apr 2025 06:46:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    o3测试<br />视觉推理能力大幅增强<br />😅已经能从照片推理地点和拍摄时间了<br /><br />科幻片！<br /><br />询问照片几几年拍摄的整个推理思路：<br />1.先是分析推理思路，推理出出租车涂装是关键的信息点<br />2.引用Python的Pillow库对照片进行裁剪放大<br />3.有自我纠正，分析分辨率过低导致坐标有误，重新裁剪<br />4.对比出租车的涂装和推测的年代是否一致<br />5.提取出租车的色值验证猜想<br />6.通过电线杆分析建筑是否已经完工，结合出租车涂装的信息，最终推测出最后的年份。<br /><br />😅福尔摩斯复活了也要拜师<br /><br />这一系列的推理和Tools调用行云流水且速度极快，看来OpenAI是要将模型即应用践行到底了，Agent刚大火，生存位又进一步被挤压，可能还是走隐私、长期记忆这个位置更保险。
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>