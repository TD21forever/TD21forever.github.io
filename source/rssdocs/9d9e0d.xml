<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>AI探索站 - 即刻圈子</title>
        <link>https://m.okjike.com/topics/63579abb6724cc583b9bba9a</link>
        
        <item>
            <id>https://m.okjike.com/originalPosts/6617a9c1a922aa28d07c4d45</id>
            <title>好东西，Stable Diffusion 生态最重要的 70 篇论文精选。 而且还根据不同的作用做了分类，除了论文地址还有对应的代码仓库和模型下载地址。 非常适合深入学习 SD...</title>
            <link>https://m.okjike.com/originalPosts/6617a9c1a922aa28d07c4d45</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/6617a9c1a922aa28d07c4d45</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Apr 2024 09:13:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    好东西，Stable Diffusion 生态最重要的 70 篇论文精选。<br /><br />而且还根据不同的作用做了分类，除了论文地址还有对应的代码仓库和模型下载地址。<br /><br />非常适合深入学习 SD 的朋友研究。<br /><br />https://latentbox.com/zh/sd-ecology<br /><img src="https://cdnv2.ruguoapp.com/FrkTrjTLWhgSNUehqysCou_nP3qxv3.png" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/66179b6b38849f879f8d96b6</id>
            <title>把我一款目前有5k+海外用户数的GPT（模拟Claude 3 Opus）调整成了必须输入密码才能用的付费模式，让我们瞧瞧海外英语世界的用户是不是真的有更高的付费意愿😉 ...</title>
            <link>https://m.okjike.com/originalPosts/66179b6b38849f879f8d96b6</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/66179b6b38849f879f8d96b6</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Apr 2024 08:12:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    把我一款目前有5k+海外用户数的GPT（模拟Claude 3 Opus）调整成了必须输入密码才能用的付费模式，让我们瞧瞧海外英语世界的用户是不是真的有更高的付费意愿😉<br /><br />你也可以试试能不能通过Prompt Engineering的技巧突破限制🚫使用：https://chat.openai.com/g/g-zXO6j2rED-claude-3-opus<br /><img src="https://cdnv2.ruguoapp.com/FnFSxnCgCq7ovK47qP_IvEDmB2U5v3.png" /><br /><img src="https://cdnv2.ruguoapp.com/FltAoBcefPfM1ihEc_QxMXWuHv2Nv3.jpg" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/661754fd6d9f190631e8386d</id>
            <title>肝了十天，我的片子终于要在CCTV6电影频道上线了，明天还要在晚会盛典亮相哈哈哈。这次不会感谢错了，谢谢CCTV6哈哈哈哈</title>
            <link>https://m.okjike.com/originalPosts/661754fd6d9f190631e8386d</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/661754fd6d9f190631e8386d</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Apr 2024 03:11:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    肝了十天，我的片子终于要在CCTV6电影频道上线了，明天还要在晚会盛典亮相哈哈哈。这次不会感谢错了，谢谢CCTV6哈哈哈哈<br /><img src="https://cdnv2.ruguoapp.com/Fhgq0txJNukq3QbuODWX_Oqu6tHSv3.jpg" /><br /><img src="https://cdnv2.ruguoapp.com/FhjmqwqkjapT3zos7HUhjlG9Jz2tv3.jpg" /><br /><img src="https://cdnv2.ruguoapp.com/Fut-dsjpoZWYk3Gziz7hEyuo7p6Wv3.jpg" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/6616979c12ed2fda68e88d9b</id>
            <title>AI生成PPT工具</title>
            <link>https://m.okjike.com/originalPosts/6616979c12ed2fda68e88d9b</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/6616979c12ed2fda68e88d9b</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Apr 2024 13:43:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    AI生成PPT工具<br /><img src="https://cdnv2.ruguoapp.com/FieTJgmFnrmEMsZf2rVmMESqCfkcv3.jpg" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/661674246d9f190631d9bc63</id>
            <title>有木有即友知道这个用什么AI软件生成的😂😂</title>
            <link>https://m.okjike.com/originalPosts/661674246d9f190631d9bc63</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/661674246d9f190631d9bc63</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Apr 2024 11:12:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    有木有即友知道这个用什么AI软件生成的😂😂<br /><img src="https://cdnv2.ruguoapp.com/FlR9VoAWEi_o7-y26qhvxRBXZE7gv3.png" /><br /><img src="https://cdnv2.ruguoapp.com/FgyFSF9oRbspjctsmbI_RdF0zT2iv3.png" /><br /><img src="https://cdnv2.ruguoapp.com/FggBddI1XnQx0jt8UtnuzTHKobpPv3.png" /><br /><img src="https://cdnv2.ruguoapp.com/Fj_YTUtOKEZEhXgcdLQqwueGQr54v3.png" /><br /><img src="https://cdnv2.ruguoapp.com/Fun081DFDzHwUBctaFYkC_GD8jKJv3.png" /><br /><img src="https://cdnv2.ruguoapp.com/lq7ldMhf9FRyScbi-DyG4PPfRiUtv3.png" /><br /><img src="https://cdnv2.ruguoapp.com/FvczvPvUbis9yBWM0iLAXZ3tQ5Tjv3.png" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/6615c33512ed2fda68d5f67f</id>
            <title>最近有点纠结，究竟是开始上手AI视频，还是深入研究些 agent。 本着“不看广告看疗效”的态度，调研了一圈两者在商业公司的落地情况，发现还是后者更加靠谱，更...</title>
            <link>https://m.okjike.com/originalPosts/6615c33512ed2fda68d5f67f</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/6615c33512ed2fda68d5f67f</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Apr 2024 22:37:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    最近有点纠结，究竟是开始上手AI视频，还是深入研究些 agent。<br /><br />本着“不看广告看疗效”的态度，调研了一圈两者在商业公司的落地情况，发现还是后者更加靠谱，更加适合我。<br /><br />吴恩达老师的“GPT-4+agent＞GPT-5”的判断，个人是比较认同和深有感触的。<br /><br />吴恩达老师在演讲中还提到agent的 4 大设计模式，并推荐了10篇相关的论文，我打算今天就开始啃起来：<br /><br />1、Reflection<br />https://arxiv.org/pdf/2303.17651.pdf<br />https://arxiv.org/pdf/2303.11366.pdf<br />https://arxiv.org/pdf/2305.11738.pdf<br /><br />2、Tool Use<br />https://arxiv.org/pdf/2305.15334.pdf<br />https://arxiv.org/pdf/2303.11381.pdf<br />https://arxiv.org/pdf/2401.17464.pdf<br /><br />3、Planning<br />https://arxiv.org/pdf/2201.11903.pdf<br />https://arxiv.org/pdf/2303.17580.pdf<br /><br />4、Multi-agent collaboration<br />https://arxiv.org/pdf/2307.07924.pdf<br />https://arxiv.org/pdf/2308.08155.pdf<br /><img src="https://cdnv2.ruguoapp.com/FsbaN3R0H1xTKnE6_xf7OXXf3r4Pv3.jpg" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/66139b75164d89e60154b96a</id>
            <title>这个可能比较重要，北大发布一个新的图像生成框架VAR。 VAR首次使GPT风格的AR模型在图像生成上超越了Diffusion transformer。 同时展现出了与大语言模型观察到的...</title>
            <link>https://m.okjike.com/originalPosts/66139b75164d89e60154b96a</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/66139b75164d89e60154b96a</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Apr 2024 07:23:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    这个可能比较重要，北大发布一个新的图像生成框架VAR。<br /><br />VAR首次使GPT风格的AR模型在图像生成上超越了Diffusion transformer。<br /><br />同时展现出了与大语言模型观察到的类似Scaling laws的规律。<br /><br />在ImageNet 256x256基准上,VAR将FID从18.65大幅提升到1.80,IS从80.4提升到356.4,推理速度提高了20倍。<br /><br />详细介绍：<br /><br />视觉自回归模型(VAR)是一种新的图像生成范式,它将自回归学习重新定义为从粗到细的"下一尺度预测"或"下一分辨率预测",有别于标准的光栅扫描"下一token预测"。<br /><br />这种简单直观的方法让自回归transformer能够快速学习视觉分布并具有良好的泛化能力:<br /><br />VAR首次使GPT风格的AR模型在图像生成上超越了扩散transformer。<br /><br />在ImageNet 256x256基准上,VAR将FID从18.65大幅提升到1.80,IS从80.4提升到356.4,推理速度提高了20倍。<br /><br />实证验证了VAR在多个维度包括图像质量、推理速度、数据效率和可扩展性上都优于Diffusion Transformer。<br /><br />随着VAR模型的扩大,它展现出了与大语言模型观察到的类似幂律缩放规律,线性相关系数接近-0.998,有力证明了这一点。<br /><br />VAR进一步展示了在下游任务如图像修复、外推和编辑上的零样本泛化能力。<br /><br />这些结果表明,VAR初步模拟了大语言模型的两个重要特性:缩放规律和零样本泛化。<br /><br />研究人员已经公开了所有模型和代码,以促进AR/VAR模型在视觉生成和统一学习中的探索。<br /><br />VAR算法为计算机视觉中的自回归算法设计提供了新的见解,有望推动这一领域的进一步发展。<br /><br />项目地址：https://github.com/FoundationVision/VAR<br />Demo 地址，生成速度真的非常快：https://var.vision/demo<br />模型下载：https://huggingface.co/FoundationVision/var/tree/main<br /><img src="https://cdnv2.ruguoapp.com/FoPTrLaClnuJl_dtiysPMeNtGPDmv3.png" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/66138b2a22562b4fb999056a</id>
            <title>很有意思的一个研究，让 LLM 帮助培训社交沟通技能，确实有很多人需要这样的服务，LLM 又擅长这个。 通过一个通用框架，利用大语言模型（LLM）进行社交技能训练...</title>
            <link>https://m.okjike.com/originalPosts/66138b2a22562b4fb999056a</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/66138b2a22562b4fb999056a</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Apr 2024 06:14:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    很有意思的一个研究，让 LLM 帮助培训社交沟通技能，确实有很多人需要这样的服务，LLM 又擅长这个。<br /><br />通过一个通用框架，利用大语言模型（LLM）进行社交技能训练。“AI伙伴，AI导师”框架将实际体验学习与真实场景练习和个性化反馈相结合。<br /><br />详细介绍：<br /><br />使用大语言模型进行社交技能训练的提议：<br /><br />研究者提出,可以利用大语言模型强大的对话生成能力,为社交技能练习提供一个随时可用、安全可控的环境。相关研究已经证实,当前的大语言模型已经能够较好地模拟各类人物,进行逼真的对话互动。这为将其应用于社交技能训练奠定了基础。<br /><br />AI Partner和AI Mentor框架的提出：<br /><br />论文提出了一个通用的社交技能训练框架,包括两个关键组件:AI Partner负责提供对话实践的环境,AI Mentor负责在关键节点给予个性化指导。二者协同,可以把体验式的实践学习与理论指导有机结合,有望大幅提升社交技能训练的可及性和有效性。<br /><br />使用该框架进行社交技能训练的应用场景<br /><br />该框架可以灵活应用于多个领域的社交技能训练,如心理咨询、谈判、教学等。通过调整AI Partner塑造的人物角色,以及AI Mentor搭载的领域知识库,就可以对应不同领域的训练需求。论文通过一系列案例展示了这种适用性和灵活性。<br /><br />论文地址：https://arxiv.org/abs/2404.04204<br /><img src="https://cdnv2.ruguoapp.com/FsEkF2ut7YWVnzGpnkTEPBCWJSXIv3.png" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/66136bb938849f879f3eab07</id>
            <title>Prompting 的核心技能可能只有一个…… 启动效应，是大脑最有趣的认知活动之一。每当一段旋律、一个拼图或一段故事出现，大脑就开始疯狂运算，猜测整个景观；不...</title>
            <link>https://m.okjike.com/originalPosts/66136bb938849f879f3eab07</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/66136bb938849f879f3eab07</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Apr 2024 03:59:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    Prompting 的核心技能可能只有一个……<br /><br />启动效应，是大脑最有趣的认知活动之一。每当一段旋律、一个拼图或一段故事出现，大脑就开始疯狂运算，猜测整个景观；不直觉的开始分析因果、构建起一个个可能的解释。<br /><br />不信的话，试着放松下来，聆听我这唱一首小曲：一闪一闪亮晶晶……（请接龙）<br /><br />启动效应的本质之一是基于先验的预测，它是多模态和多感官的。简单类比的话，Prompting 就是你如何激活大模型知识结构的「启动」。<br /><br />一旦能深刻意识到这一点，如何提升你与 AI 对话的技能、有效 激活 LLMs 效能的方法就会涌现出来了。<br /><br />通过成百上千小时的反复练习，你将意识到：真正提升 Prompt 核心技能在于，持续深化于你的认知体系。<br /><br />你无法提出你不知道的问题。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/66120aed164d89e601360692</id>
            <title>斯坦福这节课讲清楚了LLM做RAG所有最重要的问题。 这节课就是传说中的Stanford CS25中的一节讲座<Retrieval Augmented Language Models>。授课人就是RAG论文的作...</title>
            <link>https://m.okjike.com/originalPosts/66120aed164d89e601360692</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/66120aed164d89e601360692</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Apr 2024 02:54:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    斯坦福这节课讲清楚了LLM做RAG所有最重要的问题。<br /><br />这节课就是传说中的Stanford CS25中的一节讲座。授课人就是RAG论文的作者之一Douwe Kiela，课程中他分享了一个检索增强语言模型的架构图。<br /><br />这张图重要到让我觉得，做RAG只要记住这一张图就够了。所有相关概念和工程实践中的权衡，全都涵盖在这张图的架构和问题中了。<br /><br />这个架构主要包括input、prompt、通过retriever检索增强生成的context，然后把这三部分一起输入给generator即模型，最终输出output作为结果。<br /><br />❇️❇️关于这几个核心概念，值得注意的是：<br />1️⃣input和prompt的区别和联系在于，input可理解为既包含system prompt，又包含用户输入的检索范围的指向，而prompt则强调用户输入的指令。<br />🌟以公司知识库RAG举例，比如用户输入chatbot的内容为"检索公司2023年的财务数据并生成总结报告"，其中"公司2023年的财务数据"是对公司知识库检索范围的指向，应理解为input的一部分，而"检索并生成总结报告"则是指令，应理解为prompt。<br />2️⃣retriever的作用机制，我理解类似于在图书馆借书的过程，提供书名(query)-系统查找图书编号(query编码)-对应书架书籍编号(docs编码)-找到并借出图书(context)。<br />🌟接着上文公司知识库的例子，从input获取query(如"2023年资产负债表, 2023年利润表, 2023年现金流量表")，对应的query编码("2023年资产负债表, 2023年利润表, 2023年现金流量表"的向量化表达)在docs编码(公司知识库所有文本的向量化表达)中检索匹配，提取匹配的部分作为context(涉及公司2023年财务数据的文本)。<br />🌟其中query和input的关系，我想到两种可能性，一种是直接把input作为query，另一种是模型基于input生成的query，架构图简化表达了。<br />3️⃣retriever和context之间可加一步reranker架构，对检索结果按特定规则进行重新排序。reranking的机制既可通过模型判断，也可在模型基础上预设特定规则。<br />🌟比如根据员工职级限制其可获取的企业知识库信息范围。<br /><br />❇️❇️目前工程实践上，大家把优化的重点基本都放在了retrieve环节里，这里面涉及三个重要的问题：<br />1️⃣how and what do I retrieve：从传统的相似性检索、文本检索，到目前最常用的依托于embedding的语义检索，大家在实践中仍在不断迭代。Kiela后面也提到有研究希望把整个retriever过程做成一个模型，他也在课程中构想未来应该把retriever的训练也纳入到LLM的训练架构中。<br />🌟文本的embedding可简化理解为文本的向量化表达，并且可根据不同文本的向量化表达，判断出文本之间语义的远近亲疏关系。<br />🌟目前的文本emebedding也都是通过模型来实现的，这类模型也在不断迭代。OpenAI在今年1月份推出了text-embedding-3(small和large两版)，相比其2022年12月推出的ada-002模型，在性能上获得了显著提升。<br />🌟用于多语言检索的常用基准(MIRACL)平均分数已从 31.4%(ada-002)增加到 44.0%(3-small)和54.9%(3-large)。<br />🌟附图之一是OpenAI对其text emebedding模型作用机制的示意。<br />2️⃣When to retrieve: 一般就两种思路。一种是在获得检索范围后即retrieve，另一种是让模型判断何时retrieve。<br />3️⃣How to encode: 如何编码也直接影响了如何检索的过程。<br /><br />❇️❇️其他问题：<br />1️⃣how to pre-process: 实际上强调就是input要包含system prompt，可设定角色、技能、任务、工作流、限制条件等。<br />2️⃣how to prompt: 涉及提示词工程的方法论。<br />3️⃣how to pass context: 可以把context作为prompt的一部分以文本形式输入，也可通过代码的方式代入。<br />4️⃣how to post-process: 比如格式化输出的处理，如固定输出json格式，或固定在末尾输出reference列表等。<br />5️⃣how to verify: 指的是如何验证output的效果或质量，比如验证output与知识库的相关性、准确性等。<br /><br />❇️❇️最后，还有关于RAG整体架构的审视框架：<br />1️⃣How to optimize: 各环节哪些地方可以优化。架构中已经列出的问题都是思考的重点。<br />2️⃣How to learn: 这里的learn应该指的是机器学习的learn，探讨各环节从software 1.0的静态架构向机器学习和software 2.0的演进。<br />3️⃣how to scale: 如何应对规模化的问题。<br />🌟比如关于知识库如何chunk、何时编码，在知识库过大时就不适合提前预处理好chunk和编码。或者大量用户同时prompt该如何应对。<br /><br />❇️❇️前段时间判断过2024年会是RAG应用爆发的一年https://m.okjike.com/originalPosts/6602dca712ed2fda687ec0a3?s=ewoidSI6ICI2M2VlMjQ0NjhhMGY3NzVjODQyMmY1NzEiCn0=，自己在2B业务中也涉及RAG工程的落地，所以花了些精力来学习这节课。以上内容夹杂了不少自己的个人理解，欢迎批评指正，一起交流学习~<br /><br />❇️❇️links:<br />🌟Stanford CS25 V4 2024春季课程(面向公众开放，有人想一起学习搭子么？) https://web.stanford.edu/class/cs25/<br />🌟Stanford CS25 V3: Retrieval Augmented Language Models https://www.youtube.com/watch?v=mE7IDf2SmJg<br />🌟RAG论文原文 https://arxiv.org/abs/2005.11401<br />🌟OpenAI text-embedding-3 models https://openai.com/blog/new-embedding-models-and-api-updates?t<br />🌟OpenAI text-embedding-ada-002 model https://openai.com/blog/new-and-improved-embedding-model?t<br />🌟Software 2.0 by Andrej Karpathy https://karpathy.medium.com/software-2-0-a64152b37c35<br />🌟 Kiela在讲这节课几个月后在其创立的Contextual AI正式推出RAG 2.0 https://contextual.ai/introducing-rag2/<br /><img src="https://cdnv2.ruguoapp.com/Fk6Mop_pXFANq5a2xXwXJ-CgT996v3.jpg" /><br /><img src="https://cdnv2.ruguoapp.com/Fs6cm98LSOCQ9c5Y8RRNWy30Lo5tv3.jpg" /><br /><img src="https://cdnv2.ruguoapp.com/FqEqfyz3wHcRxWy7DJIHfNSAJWgsv3.png" /><br /><img src="https://cdnv2.ruguoapp.com/FrbI3mDJCUqH-jV2PFsm9ps1uCoHv3.jpg" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>