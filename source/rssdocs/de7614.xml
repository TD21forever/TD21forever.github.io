<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>微信公众号 - 机器之心</title>
        <link>https://posts.careerengine.us/author/5b0cc833f3a2e44bb474843d/posts</link>
        
        <item>
            <id>https://posts.careerengine.us/p/66974424a0f1585a4713fd8b</id>
            <title>无损加速最高5x，EAGLE-2让RTX 3060的生成速度超过A100</title>
            <link>https://posts.careerengine.us/p/66974424a0f1585a4713fd8b</link>
            <guid isPermaLink="false">https://posts.careerengine.us/p/66974424a0f1585a4713fd8b</guid>
            <pubDate></pubDate>
            <updated>Wed, 17 Jul 2024 04:10:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AIxiv, 大语言模型, EAGLE-2, 动态草稿树  
<br><br>  
总结: AIxiv专栏是机器之心发布学术和技术内容的平台，促进了学术交流。EAGLE-2是一种新的推理加速方法，通过动态草稿树投机采样提高大语言模型的推理速度，最高可达5倍，同时保持输出分布不变。该方法在多轮对话、代码生成等任务中表现优异，显示出较高的加速比和接受长度。EAGLE-2的设计旨在减少对原始大语言模型的访问，提升生成效率，并已在工业界得到应用。 </div>
                        <hr>
                    
                    <div class=" pTag sectionReplaced" style="text-align: start;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9OnnzCX2HjxlUqj24Vnns9NNNzu0PPwaOst5iciaSdlMlBvia0nHGUtk9XQhXRqPP6P8KXz8wUyXicmg/640?wx_fmt=other&amp;from=appmsg&amp;wxfrom=13&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp" /></div></div><br /></div><blockquote class="js_blockquote_wrap" style="text-align: start;"><div class="js_blockquote_digest"><p><span><span style="display: inline-block;">AIxiv专栏是机器之心发布学术、技术内容的栏目。过去数年，机器之心AIxiv专栏接收报道了2000多篇内容，覆盖全球各大高校与企业的顶级实验室，有效促进了学术交流与传播。如果您有优秀的工作想要分享，欢迎投稿或者联系报道。</span><span style="display: inline-block;">投稿邮箱：<a class="__cf_email__" href="https://posts.careerengine.us/cdn-cgi/l/email-protection">[email&nbsp;protected]</a>；<a class="__cf_email__" href="https://posts.careerengine.us/cdn-cgi/l/email-protection">[email&nbsp;protected]</a></span></span></p></div></blockquote><div class=" pTag sectionReplaced"><strong style="font-weight: 600;"><span style="font-size: 17px;">李堉晖：北京大学智能学院硕士，受张弘扬老师和张超老师指导，研究方向为大模型加速和对齐，正在寻找25届工作机会</span></strong></div><div class=" pTag sectionReplaced"><strong style="font-weight: 600;"><span style="font-size: 17px;">魏芳芸：微软亚研院研究员，研究方向为具身智能、图像生成和AI agents</span></strong></div><div class=" pTag sectionReplaced"><div class=" pTag"><strong style="font-weight: 600;"><span style="font-size: 17px;">张超：北京大学智能学院研究员，研究方向为计算机视觉和机器学习</span></strong></div><div class=" pTag"><strong style="font-weight: 600;"><span style="font-size: 17px;">张弘扬：滑铁卢大学计算机学院、向量研究院助理教授，研究方向为LLM加速和AI安全</span></strong></div></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">自回归解码已经成为了大语言模型（LLMs）的事实标准，大语言模型每次前向计算需要访问它全部的参数，但只能得到一个token，导致其生成昂贵且缓慢。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">今日，一篇题为《EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees》的论文提出了动态草稿树投机采样，依据草稿模型的置信度动态调整草稿树的结构，最高可以将大语言模型的推理速度提高5倍，同时不改变大语言模型的输出分布，确保无损。</span></div><div class=" pTag sectionReplaced"><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibhVv7TGHZypZRpiavBHtqltpKMSKnfDDEcSL9lGW5gedIOciczuWqfIXMwxScjD1nGVBuDSCLuZNXw/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><br /></div><ul class="list-paddingleft-1"><li><p style="text-align: left;"><span style="font-size: 17px;">论文链接：<span style="font-size: 17px;">https://arxiv.org/pdf/2406.16858</span></span></p></li><li><p style="text-align: left;"><span style="font-size: 17px;">项目链接：<span style="font-size: 17px;">https://github.com/SafeAILab/EAGLE</span></span></p></li><li><p style="text-align: left;"><span style="font-size: 17px;">Demo链接：<span style="font-size: 17px;">https://huggingface.co/spaces/yuhuili/EAGLE-2</span></span></p></li></ul><div class=" pTag sectionReplaced"><span style="font-size: 17px;">EAGLE-2在多轮对话数据集MT-bench上的加速效果（上图为贪婪生成，下图为采样生成）：</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;"><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibhVv7TGHZypZRpiavBHtqltYaRzdfFe2Y5gP1l6jcC7zeZWTkDC1nA5cHme1Km7zHxydeAV72hOaw/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div></span></div><div class=" pTag sectionReplaced"><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibhVv7TGHZypZRpiavBHtqlt7PpxpyTKRaHMLDYCfFkibcYL1phMibIpTicSh1Lezz8P4pPOoEiaOuHdRQ/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">使用EAGLE-2,2张RTX 3060（$300）的推理速度可以超过A100（$10000）。</span></div><div class=" pTag sectionReplaced"><div class=" ce-iframe-holder offset offset-old-291"></div></div><div class=" pTag sectionReplaced" style="text-align: center;"><span style="font-size: 17px;"><strong style="font-weight: 600;">背景</strong></span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">投机采样使用一个小的模型快速生成草稿，原始的大语言模型可以通过一次前向计算验证草稿的正确性，将正确的草稿作为输出，从而一次生成多个token，并确保无损。EAGLE是投机采样的一种改进。它在更有规律的特征层面而不是token层面进行自回归，同时输入采样结果（超前一个时间步的token）消除了不确定性，明显提升了草稿模型的准确率。</span></div><div class=" pTag sectionReplaced" style="text-align: left;"><span style="font-size: 17px;">到目前为止，EAGLE在第三方测试Spec-Bench（<span style="font-size: 17px;">https://github.com/hemingkx/Spec-Bench/blob/main/Leaderboard.md</span>）中排名第一。</span></div><div class=" pTag sectionReplaced" style="text-align: center;"><span style="font-size: 17px;"><strong style="font-weight: 600;">思路</strong></span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">EAGLE和Medusa等方法使用静态的草稿树，隐式地假设草稿token的接受率和上下文无关，下面是一个简单的例子</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibhVv7TGHZypZRpiavBHtqltDeajyIjqUlP7Pbnib2EqUjW82BTiaKcormv8VZPmicjfV8w9TTDvNrcDQ/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">上文是“10+2”时，下一个token难以预测，EAGLE在这个位置添加两个候选token以增加草稿命中率，“10+2=”和“10+2+”有一个正确即可。当上文是“10+2=”时，下一个token明显是“1”，但是EAGLE使用静态的草稿结构，仍然添加两个候选“1”和“3”，“10+2=3”不可能通过大语言模型的检查，存在浪费。EAGLE-2旨在解决这一问题，如下图所示，当上文是“10+2=”时，EAGLE-2只增加一个候选token“1”，将节约出的token用于让草稿树更深，这样“10+2=12”通过大语言模型的检查，EAGLE-2可以一次生成更多的token。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibhVv7TGHZypZRpiavBHtqltunsu3hTq4ugXRQoxzkEiapCXSFEmt0dJXib7uFq2p81xzadkdmXx4pUw/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">EAGLE-2的作者们在Alpaca数据集上进行了简单的测试，下图显示了不同位置的草稿token的接受率，左图中的P1-P6代表位置，与右图的横轴坐标对应。实验结果显示，在相同的位置上的草稿token的接受率也有较大的差异，这说明了使用动态草稿树可能取得比静态草稿树更好的效果。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibhVv7TGHZypZRpiavBHtqlttDltQjLzH36CP05xXDF4UBjfWcc7ibKSrGqcGynhibiacs0yp6gYjuRQg/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">上述例子中，EAGLE-2根据预测草稿token的难易程度决定草稿树的结构，精确计算难易程度（接受率）需要原始大语言模型的计算结果，这违背了投机采样减少对原始大语言模型访问的初衷。幸运的是，EAGLE的草稿模型的置信度与接受率（难易程度）高度正相关。下图显示了草稿模型不同置信度区间的草稿token的平均接受率，红色虚线连接（0,0）和（1,1）。由此可见，草稿模型的置信度可以作为接受率的有效近似。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibhVv7TGHZypZRpiavBHtqltSWwK2t0QOXv6ezAEWlPt0S3efsSAzBP1zrG2Ep3d9NSibiboUGseZ95Q/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced" style="text-align: center;"><span style="font-size: 17px;"><strong style="font-weight: 600;">方法</strong></span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">EAGLE-2包括两个阶段，扩展和重排，扩展阶段加深加大草稿树，重排阶段修剪草稿树，丢弃部分节点（token）。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">为了保证无损，一个草稿token被接受的前提是它的祖先节点都被接受，所以EAGLE-2将一个节点的价值定义为它和它祖先的接受率的乘积，用置信度的乘积来近似。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">在扩展阶段，EAGLE-2选择草稿树最后一层价值最高的m个节点（token）进行扩展。这些token被送入草稿模型，然后将草稿模型的输出作为子节点连接到输入节点，加深加大草稿树。在重排阶段，EAGLE-2按照价值对整棵草稿树进行重排序，保留前n个节点（token）。草稿token的置信度在0-1之间，两个节点价值相同时优先保留浅层节点，因此重排后保留的草稿树一定是连通的，保证了语义上的连贯性。重排后草稿树变小，降低了原始大语言模型验证的计算量。为了保证计算结果的正确性，还需要调整attention mask，确保每一个token只能看到它的祖先节点，不受其他分支的影响。下面是一个简单的例子。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibhVv7TGHZypZRpiavBHtqlt7bX7F1HT3ZYic786r95e6UI0NRm5D0dIgPnxiaHxJiaKhT3QA6iaETomicA/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">扩展（Expand）阶段的黄色框表示被选中进行扩展的节点，绿色框为以这些节点为输入时草稿模型的预测。重排（Rerank）阶段的蓝色框表示被保留的节点，之后它们被展平成一维作为原始大语言模型的输入。EAGLE-2根据树的结构调整attention mask，比如，”a”只能看到它的祖先“It”和“is”，看不到另一个分支的“has”。EAGLE-2也同时调整位置编码，确保和标准自回归解码的一致性。</span></div><div class=" pTag sectionReplaced" style="text-align: center;"><span style="font-size: 17px;"><strong style="font-weight: 600;">实验</strong></span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">EAGLE-2在多轮对话、代码、数学推理、指令遵循、问答、总结六项任务上分别使用MT-bench、Humaneval、GSM8K、Alpaca、CNN/DM、Natural Questions数据集进行了实验，与6种先进的投机采样方法（SpS、PLD、Medusa、Lookahead、Hydra、EAGLE）进行了比较。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibhVv7TGHZypZRpiavBHtqlty4numicIib2ZJYEx0lvpicqZASKnYlXl6lnA0z9BcibT8V9dFDX5cPA2eg/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;"><span style="display: none;">‍</span><span style="display: none;">‍</span><span style="display: none;">‍</span><span style="display: none;">‍</span><span style="display: none;">‍</span><span style="display: none;">‍</span><span style="display: none;">‍</span><span style="display: none;">‍</span></span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibhVv7TGHZypZRpiavBHtqltdUEBXF88seWzps5n9oOd6zFRnLdEhhjDAT7BBGOOibJsichKIY17gUfQ/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">表格中的Speedup为加速比，τ 为平均接受长度，也就是原始大语言模型每次前向计算能生成的token数。EAGLE-2每次前向计算能生成大约4-5个token，而自回归解码每次生成1个token，因此EAGLE-2明显加速了大语言模型的生成，加速比为2.5x-5x。加速比和接受长度在代码生成任务（Humaneval数据集）上最高，这是因为代码中存在大量确定性的模板，草稿更容易命中。在所有任务和大语言模型上，EAGLE-2的加速比和平均接受长度都是最高的，明显优于其他方法。</span></div><div class=" pTag sectionReplaced" style="text-align: center;"><span style="font-size: 17px;"><strong style="font-weight: 600;">应用</strong></span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">EAGLE-2也在工业界得到应用，集成至Intel/intel-extension-for-transformers等。</span></div><div class=" pTag sectionReplaced" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibtmpFW5KoyQWpAmJ6NnyDIVhOibdOONTggtx5XfvI6xWwufezyXiaFlqvefy9RAuvK1qibj2VgdP7kQ/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">©&nbsp;THE END&nbsp;</span></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">转载请联系本公众号获得授权</span></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">投稿或寻求报道：<a class="__cf_email__" href="https://posts.careerengine.us/cdn-cgi/l/email-protection">[email&nbsp;protected]</a></span></div>  <div class="read-more-button"><div class="cce-btn cce-btn-light-grey" id="readMore">继续阅读</div></div>  <a class="post-original-link" href="https://careerengine.us/redirect/to?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FTbBYdJWVE-FCajytzjyDzw">阅读原文 </a>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://posts.careerengine.us/p/6697440ee5cfd55a0aa2ca43</id>
            <title>早半年发arXiv，却被质疑抄袭：活在微软AutoGen阴影里的CAMEL</title>
            <link>https://posts.careerengine.us/p/6697440ee5cfd55a0aa2ca43</link>
            <guid isPermaLink="false">https://posts.careerengine.us/p/6697440ee5cfd55a0aa2ca43</guid>
            <pubDate></pubDate>
            <updated>Wed, 17 Jul 2024 04:09:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AutoGen, CAMEL, 论文争议, 多智能体  
<br>
<br>
总结: 文章讨论了微软的开源编程框架AutoGen与李国豪的CAMEL之间的相似性及其引发的争议。李国豪指出，AutoGen在方法论上与CAMEL高度相似，且AutoGen的论文在提及CAMEL时仅在附录中讨论，导致许多研究者未能注意到CAMEL的存在。尽管AutoGen的作者辩称在提交时CAMEL尚未被同行评审，但这一做法仍引发了学术界的关注和质疑。最终，AutoGen的论文因其他原因被拒绝，而李国豪希望通过发帖引起学术界的重视。 </div>
                        <hr>
                    
                    <div class=" pTag sectionReplaced"><div><p style="text-align: center;"><span style="font-size: 17px;">机器之心报道</span></p><div class=" pTag" style="text-align: center;"><strong style="font-size: 17px; font-weight: 600;">机器之心编辑部</strong></div></div></div><blockquote class="js_blockquote_wrap"><div class="js_blockquote_digest"><div><div class=" pTag"><span>arXiv 不是同行评审期刊，所以发在 arXiv 上的论文不必被引用，这合理吗？&nbsp;&nbsp;</span></div></div></div></blockquote><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;">如果你对 AI 智能体感兴趣，那你一定知道微软的 <a href="https://posts.careerengine.us/redirect/referral/id/669744460dbf980259d854e3">AutoGen</a>。</span><span style="font-size: 17px;">它是一个用于构建 AI 智能体的开源编程框架，允许多个智能体通过聊天来解决任务。</span><span style="font-size: 17px;">其间，LLM 智能体可以扮演多种角色，如程序员、设计师，或者各种角色的组合。</span></div><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;">在 GitHub 上，这个项目已经收获了 28k 的 star 量，论文还在 ICLR 2024 &nbsp;LLM Agent Workshop 上获得了最佳论文奖。</span></div><div class=" pTag" style="text-align: justify;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvMoY2TeVBiaDWvvAvIPc7sR9xwL1Om47qVJ1Jfz4FZHPRibibpznmImJhQ/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;">不过，这篇论文的背后其实是存在争议的。</span></div><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;">2023 年 11 月，一位 AI 研究者（阿卜杜拉国王科技大学博士，开源项目 Camel-AI.org、DeepGCNs.org 的发起人李国豪）发帖称，由于 AutoGen 与他们的论文 CAMEL 高度相似，他们每次出席活动的时候都会被问，二者有什么区别？</span></div><div class=" pTag" style="text-align: justify;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvXWlIEOUqlicA8icXGQFzibibZZ5K9flGph52HGxwAiczDSSQjnibYhiaMIz8Q/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: justify;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvVYIGUSDzucibDiaUjCSCIsDyyDvh60mBLBHhIafcIIg8HbWZbKqLewGg/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;">对此，李国豪表示非常无奈，因为他们的论文发布在 arXiv 上的时间要明显早于 AutoGen，如今却被当成了 AutoGen 的模仿者（CAMEL 发布于 2023 年 3 月；AutoGen 发布于 2023 年 8 月）。</span></div><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvzBaYPQEhqDTwYEgssW4T9on5IF8phKAM7rkssssetiaRt1iaxV4Ex90Q/640?wx_fmt=png&amp;from=appmsg" /></div></div></span></div><div class=" pTag" style="text-align: left;"><span><em><span style="font-size: 17px; text-align: left;">论文链接：https://arxiv.org/abs/2303.17760</span></em></span><br /></div><div class=" pTag" style="text-align: justify;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvaXVo1hFDopAgMa1BLHm7Fvg7mXWrqNzcvxTibL6hG1aLjRhGQGh6QIA/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: justify;"><em style="text-align: left; font-size: 17px;"><span style="font-size: 17px;">论文链接：https://arxiv.org/pdf/2308.08155</span></em><br /></div><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;">根据李国豪的说法，二者在方法论上存在以下相似之处：</span></div><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvomDW51ZMepSUpSIzSAce3nV6hnO4EYtHyH4P5jYtnWW5xIToaIp31g/640?wx_fmt=png&amp;from=appmsg" /></div></div><br /></span></div><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;">甚至用到的例子也有点相似：&nbsp; &nbsp;</span></div><div class=" pTag" style="text-align: justify;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJviblOuMribyZRaPWQaIVfSqFQLsvRfos7amdQ84Fz508qBunGkW5zK5JQ/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: justify;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvjC5oibYl5rnnmS2l9uUEPfGFnL6R4dicTJxiaPoDbaov7ia3JNKWdXTepg/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;">作为后来者，AutoGen 确实在论文中提到过 CAMEL，并指出了 CAMEL 与 AutoGen 之间的一些差异。但这些内容出现的位置令人费解 —— 它们统统出现在附录中。这可能也是导致其他研究者只知 AutoGen，不知 CAMEL 的一大原因。毕竟，有几个人会去仔细看附录呢？</span></div><div class=" pTag" style="text-align: justify;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvdibdAqziaJGube38ZOX5TI4uWWT0v8tROjfVg2242rRnDjnuXsCVdQTA/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><blockquote class="js_blockquote_wrap"><div class="js_blockquote_digest"><p>AutoGen 论文中提及 CAMEL 的段落：「CAMEL（Li et al., 2023b）是一个通信智能体框架，它展示了如何使用角色扮演来让聊天智能体相互交流以完成任务。CAMEL 还能记录智能体对话以进行行为分析和能力理解。CAMEL 使用了一种「inception-prompting」技术实现智能体之间的自主合作。与 AutoGen 不同的是，CAMEL 本身不支持工具使用（如代码执行）。尽管 CAMEL 被提议作为多智能体对话的基础设施，但它只支持静态对话模式，而 AutoGen 还支持动态对话模式。」</p></div></blockquote><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvEqn9licvs1ABApgwqfl5BHsjKaXWAPcicib6lbFzPBibKWSQnVrZYvjlmw/640?wx_fmt=png&amp;from=appmsg" /></div></div></span></div><div class=" pTag" style="text-align: left;"><span><em><span style="font-size: 17px;">表 1 为 AutoGen 与其他相关多智能体之间的差异总结，从四个指标着手判断：一是基础结构，即系统是否设计为构建 LLM 应用程序的通用基础结构；二是对话模式，即系统支持的模式类型。在「静态」模式下，无论输入如何，智能体拓扑结构都保持不变。AutoGen 允许灵活的对话模式，包括可以根据不同应用程序需求定制的静态和动态模式。三是可执行，即系统是否可以执行 LLM 生成的代码；四是人工参与，系统是否（以及如何）允许人工参与执行过程。AutoGen 允许人工灵活地参与多智能体对话，并允许人工选择跳过提供输入。</span></em></span></div><div class=" pTag" style="text-align: justify;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvsh1W1bnh3qk0HmflBibu6LkDY98Rx1R9dz6bO1mdQaeF5zLQ31OpaPA/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><blockquote class="js_blockquote_wrap"><div class="js_blockquote_digest"><p>AutoGen 论文中提及 CAMEL 的段落：「AutoGen 可以帮助开发能力超强的智能体，充分利用 LLM、工具和人类的优势。创建这样的智能体对于确保多智能体工作流能够有效地排除故障并在任务中取得进展至关重要。例如，我们观察到，另一个多智能体 LLM 系统 CAMEL 在大多数情况下无法有效解决问题，主要是因为它缺乏执行工具或代码的能力。这一失败表明，仅有简单角色扮演的 LLM 和多智能体对话是不够的，还必须有具备各种技能的高级能力智能体。我们认为，开展更系统的工作，制定针对特定应用的智能体指南，创建大型 OSS 知识库，并创建能够发现和提升自身技能的智能体是必要的。</p></div></blockquote><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;">在 AutoGen 提交给 ICLR 主会议审稿期间，CAMEL 一作李国豪在公共评论区指出了这一问题，并强调这是「值得注意的遗漏」。</span></div><div class=" pTag" style="text-align: justify;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJv4z2WAjY1ib4rRaRuFHyDScp85239xTgiabDUaUQeRV2GaUkRkfXexHrQ/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;">在针对 AutoGen 的审稿意见中，ICLR 的审稿人和领域主席也指出了这种做法的不妥之处。</span></div><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvcaY0ibe95HsREiaEDaJMUsbibwlCv08zVvAUAicgKcWYL3lia5jicT6ict9kg/640?wx_fmt=png&amp;from=appmsg" /></div></div></span></div><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;">其中，领域主席写道，「作者确实在附录中讨论了这一工作，但这种做法是不可取的，因为补充材料的审核级别与论文的审核级别不同。简而言之，这似乎允许作者说他们引用和讨论了论文，但实际上并没有在 99% 的人可能阅读的论文部分进行引用和讨论。我认为这种做法令人担忧。」</span></div><div class=" pTag" style="text-align: justify;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvKU880eITlZicwR1RAz84wIqickaL45ibHmvJg5xGdDyetJThBSVbk7iaFg/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;">那 AutoGen 的作者为什么这么做呢？他们回复说：在他们将论文提交给 ICLR 2024 时，CAMEL 等论文尚未在同行评审会议 / 期刊上发表。根据 ICLR 2024 审稿人指南，他们没有义务引用这篇论文或与之比较（CAMEL 于 2023 年 9 月被 NeurIPS 2023 录用；ICLR 2024 审稿人指南规定，在 2023 年 5 月 28 日之后发表的论文不需要引用）。</span></div><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvul4EQu5uibYBZBibtIiaUNzt9vCYjSTicglrM3vvvhvVt0qjZZGkSkweTw/640?wx_fmt=png&amp;from=appmsg" /></div></div></span></div><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;">同时，他们列出了论文中涉及 CAMEL 的部分：</span></div><div class=" pTag" style="text-align: justify;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvBA31Yl0cIKa1tdbTtGlRiaC4GB9JtY6wR19LLvphj98SfsMaAY2SIrA/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;">鉴于 ICLR 规定在先，领域主席也不好多说什么。他写道，「虽然我理解这项政策背后的基本原理，但在当前的出版氛围下，它可能会导致奇怪的结果。由于 ICLR 的政策，我不会将其纳入我的决定中，但这会降低我的信心。」</span></div><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;">关于李国豪提及的相似性，AutoGen 作者也给出了反驳意见：</span></div><div class=" pTag" style="text-align: justify;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvxcZ7r9CiaaObfJEYKdCy28uWzp9kk4uzvO3aZIqUiazMiaUSgh4ygY6ibA/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;">针对审稿人提出的问题，他们回复如下：&nbsp; &nbsp;</span></div><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvHREf0M7JmWF6SIntbt2ZdWrbRwVTyzv8XG75SibwULMKphTrVgTibmPw/640?wx_fmt=png&amp;from=appmsg" /></div></div></span></div><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;">最终，和 CAMEL 之间的相似性以及引用问题并未作为论文的主要问题被领域主席考虑在内。不过，AutoGen 论文最终还是因为其他原因被拒（所以后来作者转投了 ICLR 2024 &nbsp;LLM Agent Workshop）。</span></div><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;">根据李国豪的说法，两篇论文的作者其实在线下见过面，但发生了一些不愉快：</span></div><div class=" pTag" style="text-align: justify;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvDkE6oXro12UE6KpXOL4LFLGHwhQ4F1xHIGwUj4Gzia8oUG7nQviabhoA/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;">李国豪希望能够通过发帖引起学术界的重视。</span></div><div class=" pTag" style="text-align: justify;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvRAYoHNyficnPLkxbdqpExcpoTXRMXxEq0s41ZTicAgyGicORRglLicE7uQ/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: justify;"><span style="font-size: 17px;">对此，你怎么看呢？</span></div><div class=" pTag sectionReplaced" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibOrSA4rRUsPAQe533w3ib6XC4tj7asK9yplzGBGvraKTmRUAFGMiaOqCsZia2C395mvOGf5aQcky2yw/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">©&nbsp;THE END&nbsp;</span></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">转载请联系本公众号获得授权</span></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">投稿或寻求报道：<a class="__cf_email__" href="https://posts.careerengine.us/cdn-cgi/l/email-protection">[email&nbsp;protected]</a></span></div>  <div class="read-more-button"><div class="cce-btn cce-btn-light-grey" id="readMore">继续阅读</div></div>  <a class="post-original-link" href="https://careerengine.us/redirect/to?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FX3imM3pvGBKQ7mNC_jmf_A">阅读原文 </a>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://posts.careerengine.us/p/6697440ee5cfd55a0aa2ca3b</id>
            <title>Mistral AI两连发：7B数学推理专用、Mamba2架构代码大模型</title>
            <link>https://posts.careerengine.us/p/6697440ee5cfd55a0aa2ca3b</link>
            <guid isPermaLink="false">https://posts.careerengine.us/p/6697440ee5cfd55a0aa2ca3b</guid>
            <pubDate></pubDate>
            <updated>Wed, 17 Jul 2024 04:09:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数学推理, 大语言模型, Mathstral, Codestral Mamba  
<br><br>  
总结: 文章讨论了大语言模型在处理简单数学问题时的局限性，特别是提到「9.11和9.9谁大」的问题。为了解决复杂的数学推理问题，Mistral AI 发布了专注于数学推理的7B大模型「Mathstral」，其在多个基准测试中表现出色，尤其是在MATH数据集上。Mathstral的设计理念强调性能与速度的平衡，并支持微调功能。此外，Mistral还推出了用于代码生成的模型Codestral Mamba，具有线性时间推理优势，能够处理更长的输入序列。 </div>
                        <hr>
                    
                    <div class=" pTag sectionReplaced"><div><p style="text-align: center;"><span style="font-size: 17px;">机器之心报道</span></p><div class=" pTag" style="text-align: center;"><strong style="font-size: 17px; font-weight: 600;">机器之心编辑部</strong></div></div></div><blockquote class="js_blockquote_wrap"><div class="js_blockquote_digest"><p>网友很好奇，Mathstral能不能搞定「9.11和9.9谁大」这一问题。</p></div></blockquote><div class=" pTag sectionReplaced"><span style="font-size: 17px;">昨天，AI圈竟然被「9.11和9.9谁大」这样简单的问题攻陷了，包括OpenAI GPT-4o、Google Gemini等在内的大语言模型都翻了车。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9Ygj2Nu9XqsVaGnnDiaU8XDhr7lKwMtDHIjCNeb8PiciagqRDrNx9cq4lxhU2oDTIe7eb1icicGJU7xQA/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9Ygj2Nu9XqsVaGnnDiaU8XDalsnYribpsjohjgBgIia5iax3gK9nC0h4XrbHXv2oCiaIP1GLhQ6oBZ5HQ/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">这让我们看到，大语言模型在处理一些数字问题时并不能像人类那样理解并给出正确的答案。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">对于数字以及复杂的数学问题，专用模型更术业有专攻。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">今天，法国大模型独角兽 Mistral AI 发布了一个<span style="font-size: 17px;"><strong style="font-weight: 600;">专注于数学推理和科学发现的7B大模型「Mathstral」</strong></span>，来解决需要复杂、多步骤逻辑推理的高级数学问题。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">该模型基于 Mistral 7B 构建，支持的上下文窗口长度为32k，遵循的开源协议为Apache 2.0 license。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">Mathstral在构建时追求出色的性能与速度权衡，这是 Mistral AI积极推广的一种开发理念，尤其是微调功能。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9Ygj2Nu9XqsVaGnnDiaU8XDJTGuiaXib7dsJSSIxBDsyZ2EXkibL2hd7icKqP5CRNJK81ySib2YXGcsO0g/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">同时，Mathstral是一个指令型模型，可以使用它或者对它进行微调。模型权重已经放在了HuggingFace上。</span></div><ul class="list-paddingleft-1"><li><p style="text-align: left;"><span style="font-size: 17px;">模型权重：https://huggingface.co/mistralai/mathstral-7B-v0.1</span></p></li></ul><div class=" pTag sectionReplaced"><span style="font-size: 17px;">下图为 Mathstral 7B和Mistral 7B之间的MMLU性能差异（按学科划分）。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">Mathstral在各种行业标准基准上都达到其规模范围内的 SOTA 推理性能。尤其是在MATH数据集上，它取得了 56.6%的通过率，在MMLU上取得了63.47%的通过率。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9Ygj2Nu9XqsVaGnnDiaU8XDM9Xx1FgOlhuTyevBVAIxODPxrkp1Qia5iccdhrwyNWicibgmuanP5BZFTg/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">同时，Mathstral在MATH上的通过率（56.6%）比 Minerva 540B 高出 20% 以上。此外，Mathstral 在MATH 上以多数投票@64的成绩得分为68.4%，使用奖励模型的成绩为 74.6%。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;"><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9Ygj2Nu9XqsVaGnnDiaU8XDHHr7EzntCx1LtjwUIIXFWp06PsnI5rEg7EribuBs3sw9a9zajTr35DQ/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><br /></span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">这一成绩也让网友好奇，Mathstral能不能搞定「9.11和9.9谁大」这一问题。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9Ygj2Nu9XqsVaGnnDiaU8XDVLevyFv8bT7ciamxOOdvlOxz3zZTvIHnGYvY1DrL7lekBo6L4utmibWQ/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced" style="text-align: center;"><span style="font-size: 17px;"><strong style="font-weight: 600;">代码大模型：Codestral Mamba</strong></span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9Ygj2Nu9XqsVaGnnDiaU8XDwGE4gib4Z8cJrG9edaUxn416jkenUcbqyh5BRyDQct90JibIcaDJj6AQ/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><ul class="list-paddingleft-1"><li><p style="text-align: left;"><span style="font-size: 17px;">模型权重：https://huggingface.co/mistralai/mamba-codestral-7B-v0.1</span></p></li></ul><div class=" pTag sectionReplaced"><span style="font-size: 17px;">与Mathstral 7B一同发布的，还有一款专门用于代码生成的Codestral Mamba模型，使用的是Mamba2架构，同样遵循Apache 2.0 license开源协议。这是一个指导模型，有70多亿参数，研究者可以免费使用、修改和分发。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">值得一提的是，Codestral Mamba是在Mamba作者Albert Gu、Tri Dao帮助下设计完成的。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">一直以来，Transformer 架构撑起了AI领域的半壁江山，然而，与 Transformer 不同的是，Mamba 模型具有线性时间推理优势，并且理论上能够对无限长度的序列进行建模。该架构允许用户广泛地与模型互动，并且响应迅速，而不受输入长度的限制。这种效率对于代码生成尤其重要 。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">在基准测试中，Codestral Mamba 在 HumanEval 测试中的表现优于竞争对手开源模型 CodeLlama 7B、CodeGemma-1.17B 和 DeepSeek。&nbsp;</span></div><div class=" pTag sectionReplaced"><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9Ygj2Nu9XqsVaGnnDiaU8XDfIhsMKluj8KaKx2eOV1pgTicrJ6F4OQ3FYy3TiaHy3Kgymgr8NY8YQ2Q/640?wx_fmt=png&amp;from=appmsg" /></div></div></div></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">Mistral 测试了该模型，该模型可以在 Mistral 的 la Plateforme API 上免费使用，可处理多达 256,000 个token的输入——是 OpenAI 的 GPT-4o 的两倍。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">随着Codestral Mamba发布，就有网友在 VSCode中用起来了，很是丝滑。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9Ygj2Nu9XqsVaGnnDiaU8XDUjU6wVgXbiaTQs4KqpUPJlChcicEZnCqhZh9ETOcUwS7libNESgy5icAIA/640?wx_fmt=gif&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced"><em style="font-size: 17px;"><span style="font-size: 17px;">参考链接：</span></em></div><div class=" pTag sectionReplaced"><span><em><span style="font-size: 17px;">https://mistral.ai/news/mathstral/</span></em></span></div><div class=" pTag sectionReplaced"><span><em><span style="font-size: 17px;">https://mistral.ai/news/codestral-mamba/</span></em></span></div><div class=" pTag sectionReplaced" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9BcwDFz901ZX0iaIEk07W0Q9RffeGLiaoP5e31x1ADdDFULic1cUib7CJoIiarDSK1HUiaTNvKQgjDqang/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag"><span style="font-size: 17px;">©&nbsp;THE END&nbsp;</span></div><div class=" pTag"><span style="font-size: 17px;">转载请联系本公众号获得授权</span></div><div class=" pTag"><span style="font-size: 17px;">投稿或寻求报道：<a class="__cf_email__" href="https://posts.careerengine.us/cdn-cgi/l/email-protection">[email&nbsp;protected]</a></span></div>  <div class="read-more-button"><div class="cce-btn cce-btn-light-grey" id="readMore">继续阅读</div></div>  <a class="post-original-link" href="https://careerengine.us/redirect/to?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FCj4CSciSsNYPjiWPIIlrDA">阅读原文 </a>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://posts.careerengine.us/p/6697440ee5cfd55a0aa2ca4c</id>
            <title>快手开源LivePortrait，GitHub 6.6K Star，实现表情姿态极速迁移</title>
            <link>https://posts.careerengine.us/p/6697440ee5cfd55a0aa2ca4c</link>
            <guid isPermaLink="false">https://posts.careerengine.us/p/6697440ee5cfd55a0aa2ca4c</guid>
            <pubDate></pubDate>
            <updated>Wed, 17 Jul 2024 04:09:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: <快手, LivePortrait, 人像视频, 开源>
<br>
<br>
总结: 快手可灵大模型团队开源了名为LivePortrait的可控人像视频生成框架，能够实时将驱动视频的表情和姿态迁移到静态或动态人像视频上。该框架采用69M高质量训练帧，结合视频-图片混合训练策略，提升了生成能力和可控性。LivePortrait在GitHub上获得了广泛关注，短时间内收获了6.4K Stars和550 Forks。其技术创新包括隐式关键点框架、贴合和重定向模块，能够实现高效的面部动画生成。该技术已在快手的多个业务中落地，并将继续探索多模态驱动的人像视频生成。 </div>
                        <hr>
                    
                    <div class=" pTag sectionReplaced"><div><p style="text-align: center;"><span style="font-size: 17px;">机器之心发布<span style="display: none;">‍‍</span></span></p><div class=" pTag" style="text-align: center;"><strong style="font-size: 17px; font-weight: 600;">机器之心编辑部</strong></div></div></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">近日，快手可灵大模型团队开源了名为<span style="font-size: 17px;"><strong style="font-weight: 600;">LivePortrait</strong></span>的可控人像视频生成框架，该框架能够准确、实时地将驱动视频的表情、姿态迁移到静态或动态人像视频上，生成极具表现力的视频结果。如下动图所示：</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvEfKhLJPNEVNxLJiaFqTVfpPIr4nHTP9ER0htGtuUiah84ibpyoL3jR8mQ/640?wx_fmt=gif&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced" style="text-align: center;"><span><em><span style="font-size: 17px;"><span style="font-size: 17px;">来自网友测试LivePortrait</span></span></em></span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvrL4UzgPGY2icJSOr4WUJqqENtkPBTI3YUSABtS9hrPicia2O8icVek048g/640?wx_fmt=gif&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced" style="text-align: center;"><span><em><span style="font-size: 17px;">来自网友测试LivePortrait</span></em></span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">快手开源的LivePortrait对应的论文题目为：</span></div><div class=" pTag sectionReplaced" style="text-align: left;"><span style="font-size: 17px;">《 LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control 》<span style="display: none;">‍</span></span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvZHYWzhrHw7DFQr1skWryOHY2dvf2iboDXc7lXuy7SEGPQGKPnKcT7hw/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced" style="text-align: center;"><span><em><span style="font-size: 17px;">LivePortrait论文首页</span></em></span><br /></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">并且，LivePortrait发布即可用，秉承快手风格，论文、主页、代码一键三连。LivePortrait一经开源，就得到了<span style="font-size: 17px;"><span style="font-size: 17px;"><strong style="font-weight: 600;">HuggingFace首席执行官Clément Delangue</strong></span>的关注转发，</span></span><span style="font-size: 17px;"><strong style="font-weight: 600;">首席战略官 Thomas Wolf</strong></span><span style="font-size: 17px;">还亲自体验了功能，厉害了！</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvBBoXk7n9pAOjSWCTSp4RnNnp3U9uibgFmwGNpZVaibnUOiaTdn5BfP4EQ/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">并引起了全世界网友的<span style="font-size: 17px;"><strong style="font-weight: 600;">大规模评测</strong></span>：</span></div><div class=" pTag sectionReplaced"><div class=" ce-iframe-holder offset offset-old-123"></div></div><div class=" pTag sectionReplaced"><span><em><span style="font-size: 17px;">视频剪辑素材均来自X</span></em></span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">同时，LivePotrait获得了开源社区的广泛关注，短短一周多时间左右，在GitHub上总计收获了</span><span style="font-size: 17px;"><strong style="font-weight: 600;">6.4K Stars，550 Forks，140 Issues&amp;PRs</strong></span><span style="font-size: 17px;">，获得广泛好评，关注仍在持续增长中：</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvicsoEialBWIicPE7icibpKqnQZCI5TMebJzg0ExictjpDuof357Mawx2CyjA/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">此外，HuggingFace Space、Papers with code趋势榜<span style="font-size: 17px;"><strong style="font-weight: 600;">连续一周榜一</strong></span>，近日登顶HuggingFace所有主题排行榜<span style="font-size: 17px;"><strong style="font-weight: 600;">榜一</strong></span>：</span></div><div class=" pTag sectionReplaced" dir="ltr"><table align="center"><tbody><tr><td><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/ZxDY5TMPs0EbI86ppojBRrRhTmgJNianwtcgmxx4b59RrHeavWaTV5Rc0cHdCwtW3u8eHEPOFrakvHNdXts88vQ/640?wx_fmt=other&amp;from=appmsg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" /></div></div></div><p dir="ltr" style="text-align: center;"><span style="font-size: 17px;">HuggingFace Space榜一</span></p><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/ZxDY5TMPs0EbI86ppojBRrRhTmgJNianwWXUgXic2T93aesBT3FqsmicUQOPiaGw0icAMLXUQicoJOicibibUfBsQcWpVjg/640?wx_fmt=other&amp;from=appmsg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" /></div></div></div><p dir="ltr" style="text-align: center;"><span style="font-size: 17px;">Papers with code榜一</span></p></td><td><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/ZxDY5TMPs0EbI86ppojBRrRhTmgJNianwxSBCSsh8UopMeZruoooDn8YHEN0489OPKVUqJ6cXH1ExPsdR0Y5ISg/640?wx_fmt=other&amp;from=appmsg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" /></div></div></div><p dir="ltr" style="text-align: center;"><span style="font-size: 17px;">HuggingFace所有主题排行榜一</span></p></td></tr></tbody></table></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">更多资源信息，可以查看：</span></div><ul class="list-paddingleft-1"><li><p style="text-align: left;"><span style="font-size: 17px;">代码地址：https://github.com/KwaiVGI/LivePortrait</span></p></li><li><p style="text-align: left;"><span style="font-size: 17px;">论文链接：https://arxiv.org/abs/2407.03168</span></p></li><li><p style="text-align: left;"><span style="font-size: 17px;">项目主页：https://liveportrait.github.io/</span></p></li><li><p style="text-align: left;"><span style="font-size: 17px;">HuggingFace Space一键在线体验：https://huggingface.co/spaces/KwaiVGI/LivePortrait</span></p></li></ul><div class=" pTag sectionReplaced"><span style="font-size: 17px;">LivePortrait到底用了什么样的技术，能够在全网快速"走红"呢？</span></div><div class=" pTag sectionReplaced" style="text-align: center;"><span style="font-size: 17px;"><strong style="font-weight: 600;">方法介绍</strong></span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">和当前主流基于扩散模型的方法不同，LivePortrait探索并拓展了基于隐式关键点框架的潜力，从而平衡了模型计算效率和可控性。LivePortrait聚焦于更好的泛化性，可控性和实用的效率。为了提升生成能力和可控性，LivePortrait采用69M高质量训练帧，视频-图片混合训练策略，升级网络结构，并设计了更好的动作建模和优化方式。此外，LivePortrait将隐式关键点看成一种面部混合变形 (Blendshape) 的有效隐式表示，并基于此精心提出了贴合 (stitching) 和重定向 (retargeting) 模块。这两个模块为轻量MLP网络，因此在提升可控性的同时，计算成本可以忽略。即使是和一些已有的基于扩散模型的方法比较，LivePortrait依旧很能打。同时，在RTX4090 GPU上，LivePortrait的单帧生成速度能够达到12.8ms，若经过进一步优化，如TensorRT，预计能达10ms以内！</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">LivePortrait的模型训练分为两阶段。第一阶段为基础模型训练，第二阶段为贴合和重定向模块训练。</span></div><div class=" pTag sectionReplaced"><strong style="font-weight: 600;"><span style="font-size: 17px;">第一阶段基础模型训练</span></strong></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvtPJDON5wAvsXjlicAzkmrVRbiagFMYIKf7ibRFuyyAibDrw27jCYwPNrmQ/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced" style="text-align: center;"><span><em><span style="font-size: 17px;">第一阶段基础模型训练</span></em></span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">在第一阶段模型训练中，LivePortrait对基于隐式点的框架，如Face Vid2vid[1]，做了一系列改进，包括：</span></div><div class=" pTag sectionReplaced"><span><strong style="font-weight: 600;"><span style="font-size: 17px;">高质量训练数据收集</span></strong></span><span style="font-size: 17px;">：LivePortrait采用了公开视频数据集Voxceleb[2]，MEAD[3]，RAVDESS [4]和风格化图片数据集AAHQ[5]。此外，还使用了大规模4K分辨率的人像视频，包含不同的表情和姿态，200余小时的说话人像视频，一个私有的数据集LightStage[6]，以及一些风格化的视频和图片。LivePortrait将长视频分割成少于30秒的片段，并确保每个片段只包含一个人。为了保证训练数据的质量，LivePortrait使用快手自研的KVQ[7]（快手自研的视频质量评估方法，能够综合感知视频的质量、内容、场景、美学、编码、音频等特征，执行多维度评价）来过滤低质量的视频片段。总训练数据有69M视频，包含18.9K身份和60K静态风格化人像。</span></div><div class=" pTag sectionReplaced"><strong style="font-weight: 600;"><span style="font-size: 17px;">视频-图像混合训练</span></strong><span style="font-size: 17px;">：仅使用真人人像视频训练的模型对于真人人像表现良好，但对风格化人像（例如动漫）的泛化能力不足。风格化的人像视频是较为稀有的，LivePortrait从不到100个身份中收集了仅约1.3K视频片段。相比之下，高质量的风格化人像图片更为丰富，LivePortrait收集了大约60K身份互异的图片，提供多样身份信息。为了利用这两种数据类型，LivePortrait将每张图片视为一帧视频片段，并同时在视频和图片上训练模型。这种混合训练提升了模型的泛化能力。</span></div><div class=" pTag sectionReplaced"><span><strong style="font-weight: 600;"><span style="font-size: 17px;">升级的网络结构</span></strong></span><span style="font-size: 17px;">：LivePortrait将规范隐式关键点估计网络 (L)，头部姿态估计网络 (H) 和表情变形估计网络 (Δ) 统一为了一个单一模型 (M)，并采用ConvNeXt-V2-Tiny[8]为其结构，从而直接估计输入图片的规范隐式关键点，头部姿态和表情变形。此外，受到face vid2vid相关工作启发，LivePortrait采用效果更优的SPADE[9]的解码器作为生成器 (G)。隐式特征 (fs) 在变形后被细致地输入SPADE解码器，其中隐式特征的每个通道作为语义图来生成驱动后的图片。为了提升效率，LivePortrait还插入PixelShuffle[10]层作为 (G) 的最后一层，从而将分辨率由256提升为512。</span></div><div class=" pTag sectionReplaced"><span><strong style="font-weight: 600;"><span style="font-size: 17px;">更灵活的动作变换建模</span></strong></span><span style="font-size: 17px;">：原始隐式关键点的计算建模方式忽视了缩放系数，导致该缩放容易被学到表情系数里，使得训练难度变大。为了解决这个问题，LivePortrait在建模中引入了缩放因子。LivePortrait发现缩放正则投影会导致过于灵活的可学习表情系数，造成跨身份驱动时的纹理粘连。因此LivePortrait采用的变换是一种灵活性和驱动性之间的折衷。</span></div><div class=" pTag sectionReplaced"><span><strong style="font-weight: 600;"><span style="font-size: 17px;">关键点引导的隐式关键点优化</span></strong></span><span style="font-size: 17px;">：原始的隐式点框架似乎缺少生动驱动面部表情的能力，例如眨眼和眼球运动。具体来说，驱动结果中人像的眼球方向和头部朝向往往保持平行。LivePortrait将这些限制归因于无监督学习细微面部表情的困难。为了解决这个问题，LivePortrait引入了2D关键点来捕捉微表情，用关键点引导的损失 (Lguide)作为隐式关键点优化的引导。</span></div><div class=" pTag sectionReplaced"><span><strong style="font-weight: 600;"><span style="font-size: 17px;">级联损失函数</span></strong></span><span style="font-size: 17px;">：LivePortrait采用了face vid2vid的隐式关键点不变损失 (LE)，关键点先验损失 (LL)，头部姿态损失 (LH) 和变形先验损失 (LΔ)。为了进一步提升纹理质量，LivePortrait采用了感知和GAN损失，不仅对输入图的全局领域，面部和嘴部的局部领域也施加了这些损失，记为级联感知损失 (LP,cascade) 和级联GAN损失 (LG,cascade) 。面部和嘴部区域由2D语义关键点定义。LivePortrait也采用了人脸身份损失 (Lfaceid) 来保留参考图片的身份。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">第一阶段的所有模块为从头训练，总的训练优化函数 (Lbase) 为以上损失项的加权和。</span></div><div class=" pTag sectionReplaced"><strong style="font-weight: 600;"><span style="font-size: 17px;">第二阶段贴合和重定向模块训练</span></strong></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">LivePortrait将隐式关键点可以看成一种隐式混合变形，并发现这种组合只需借助一个轻量的MLP便可被较好地学习，计算消耗可忽略。考虑到实际需求，LivePortrait设计了一个贴合模块、眼部重定向模块和嘴部重定向模块。当参考人像被裁切时，驱动后的人像会从裁图空间被反贴回原始图像空间，贴合模块的加入是为了避免反贴过程中出现像素错位，比如肩膀区域。由此，LivePortrait能对更大的图片尺寸或多人合照进行动作驱动。眼部重定向模块旨在解决跨身份驱动时眼睛闭合不完全的问题，尤其是当眼睛小的人像驱动眼睛大的人像时。嘴部重定向模块的设计思想类似于眼部重定向模块，它通过将参考图片的嘴部驱动为闭合状态来规范输入，从而更好地进行驱动。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvaGw4oJeDvXNXPgsdSgkbpfG8vd8XHsvcs1nTVzPWow2LEmQeoXagAw/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced" style="text-align: center;"><span><em><span style="font-size: 17px;">第二阶段模型训练：贴合和重定向模块训练</span></em></span></div><div class=" pTag sectionReplaced"><span><strong style="font-weight: 600;"><span style="font-size: 17px;">贴合模块</span></strong></span><span style="font-size: 17px;">：在训练过程中，贴合模块 (S) 的输入为参考图的隐式关键点 (xs) 和另一身份驱动帧的隐式关键点 (xd)，并估计驱动隐式关键点 (xd) 的表情变化量 &nbsp;(Δst)。可以看到，和第一阶段不同，LivePortrait采用跨身份的动作替代同身份的动作来增加训练难度，旨在使贴合模块具有更好的泛化性。接着，驱动隐式关键点 (xd) 被更新，对应的驱动输出为 (Ip,st) 。LivePortrait在这一阶段也同时输出自重建图片 (Ip,recon)。最后，贴合模块的损失函数 (Lst) 计算两者肩膀区域的像素一致损失以及贴合变化量的正则损失。</span></div><div class=" pTag sectionReplaced"><span><strong style="font-weight: 600;"><span style="font-size: 17px;">眼部和嘴部重定向模块</span></strong></span><span style="font-size: 17px;">：眼部重定向模块 (Reyes) 的输入为参考图隐式关键点 (xs)，参考图眼部张开条件元组和一个随机的驱动眼部张开系数，由此估计驱动关键点的变形变化量 (Δeyes)。眼部张开条件元组表示眼部张开比例，越大表示眼部张开程度越大。类似的，嘴部重定向模块 (Rlip) 的输入为参考图隐式关键点 (xs)，参考图嘴部张开条件系数和一个随机的驱动嘴部张开系数，并由此估计驱动关键点的变化量 &nbsp;(Δlip)。接着，驱动关键点 (xd) 分别被眼部和嘴部对应的变形变化量更新，对应的驱动输出为 (Ip,eyes) 和 (Ip,lip) 。最后，眼部和嘴部重定向模块的目标函数分别为 (Leyes) 和 &nbsp;(Llip)，分别计算眼部和嘴部区域的像素一致性损失，眼部和嘴部变化量的正则损失，以及随机驱动系数与驱动输出的张开条件系数之间的损失。眼部和嘴部的变化量 (Δeyes) 和 (Δlip) 是相互独立的，因此在推理阶段，它们可以被线性相加并更新驱动隐式关键点。</span></div><div class=" pTag sectionReplaced" style="text-align: center;"><span style="font-size: 17px;"><strong style="font-weight: 600;">实验对比</strong></span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJv4DboBMBUAiarAbPZ4nXsaV2WvLZ8ADdITBII0yIIJ6RkNzB47kiakEog/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvb0Lic4ZIOq479Lr9DovRibiaujp1ZicKOOfpeOpqiaHKeyvspfervOWoUHA/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced"><span><strong style="font-weight: 600;"><span style="font-size: 17px;">同身份驱动</span></strong></span><span style="font-size: 17px;">：由如上同身份驱动对比结果可见，与已有的非扩散模型方法和基于扩散模型的方法相比，LivePortrait具有较好的生成质量和驱动精确度，可以捕捉驱动帧的眼部和嘴部细微表情，同时保有参考图片的纹理和身份。即使在较大的头部姿态下，LivePortrait也有较稳定的表现。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvfuet57Xn98LAg5cnFSAzwfqfbR0MicmdnNJ3IbRKibPsytC0lGjLLGfQ/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvtF6ibfxH1ricSZvp3ktumP2MJydQTk4PQdQE1FoYiaMsvGH8uPdfvt7lA/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced"><span><strong style="font-weight: 600;"><span style="font-size: 17px;">跨身份驱动</span></strong></span><span style="font-size: 17px;">：由如上跨身份驱动对比结果可见，与已有的方法相比，LivePortrait可以准确地继承驱动视频中细微的眼部和嘴部动作，同时在姿态较大时也比较稳定。LivePortrait在生成质量上略弱于基于扩散模型的方法AniPortrait[11]，但与后者相比，LivePortrait具有极快的推理效率且需要较少的FLOPs。</span></div><div class=" pTag sectionReplaced" style="text-align: center;"><span style="font-size: 17px;"><strong style="font-weight: 600;">拓展</strong></span></div><div class=" pTag sectionReplaced"><span><strong style="font-weight: 600;"><span style="font-size: 17px;">多人驱动</span></strong></span><span style="font-size: 17px;">：得益于LivePortrait的贴合模块，对于多人合照，LivePortrait可以用指定驱动视频对指定人脸进行驱动，从而实现多人合照驱动，拓宽了LivePortrait的实际应用。</span></div><div class=" pTag sectionReplaced"><div class=" ce-iframe-holder offset offset-old-133"></div></div><div class=" pTag sectionReplaced"><span><strong style="font-weight: 600;"><span style="font-size: 17px;">动物驱动</span></strong></span><span style="font-size: 17px;">：LivePortrait不仅对人像具有良好的泛化性，当在动物数据集上微调后，对动物肖像也可进行精准驱动。&nbsp;</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvLleFwJbHrdmqXQPbRnOckpaXf3jeKZ8IiaiaEng34CmTO74ZXbOcv3Fw/640?wx_fmt=gif&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced"><span><strong style="font-weight: 600;"><span style="font-size: 17px;">人像视频编辑</span></strong></span><span style="font-size: 17px;">：除了人像照片，给定一段人像视频，比如舞蹈视频，LivePortrait可以用驱动视频对头部区域进行动作编辑。得益于贴合模块，LivePortrait可以精准地编辑头部区域的动作，如表情、姿态等，而不影响非头部区域的画面。</span></div><div class=" pTag sectionReplaced"><div class=" ce-iframe-holder offset offset-old-134"></div></div><div class=" pTag sectionReplaced" style="text-align: center;"><span style="font-size: 17px;"><strong style="font-weight: 600;">落地与展望</strong></span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">LivePortrait的相关技术点，已在快手的诸多业务完成落地，包括<span style="font-size: 17px;"><strong style="font-weight: 600;">快手魔表、快手私信、快影的AI表情玩法、快手直播、以及快手孵化的面向年轻人的噗叽APP</strong></span>等，并将探索新的落地方式，持续为用户创造价值。此外，LivePortrait会基于可灵基础模型，进一步探索多模态驱动的人像视频生成，追求更高品质的效果。</span></div><div class=" pTag sectionReplaced"><span><em><span style="font-size: 17px;">参考文献</span></em></span></div><div class=" pTag sectionReplaced"><em><span><span style="font-size: 17px;">[1] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot free-view neural talking-head synthesis for video conferencing. In CVPR, 2021.</span></span></em></div><div class=" pTag sectionReplaced"><em><span><span style="font-size: 17px;">[2] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. Voxceleb: a large-scale speaker identification dataset. In Interspeech, 2017.</span></span></em></div><div class=" pTag sectionReplaced"><em><span><span style="font-size: 17px;">[3] Kaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian Yang, Wayne Wu, Chen Qian, Ran He, Yu Qiao, and Chen Change Loy. Mead: A large-scale audio-visual dataset for emotional talking-face generation. In ECCV, 2020.</span></span></em></div><div class=" pTag sectionReplaced"><em><span><span style="font-size: 17px;">[4] Steven R Livingstone and Frank A Russo. The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english. In PloS one, 2018</span></span></em></div><div class=" pTag sectionReplaced"><em><span><span style="font-size: 17px;">[5] Mingcong Liu, Qiang Li, Zekui Qin, Guoxin Zhang, Pengfei Wan, and Wen Zheng. Blendgan: Implicitly gan blending for arbitrary stylized face generation. In NeurIPS, 2021.</span></span></em></div><div class=" pTag sectionReplaced"><em><span><span style="font-size: 17px;">[6] Haotian Yang, Mingwu Zheng, Wanquan Feng, Haibin Huang, Yu-Kun Lai, Pengfei Wan, Zhongyuan Wang, and Chongyang Ma. Towards practical capture of high-fidelity relightable avatars. In SIGGRAPH Asia, 2023.</span></span></em></div><div class=" pTag sectionReplaced"><em><span><span style="font-size: 17px;">[7] Kai Zhao, Kun Yuan, Ming Sun, Mading Li, and Xing Wen. Quality-aware pre-trained models for blind image quality</span></span></em></div><div class=" pTag sectionReplaced"><em><span><span style="font-size: 17px;">assessment. In CVPR, 2023.</span></span></em></div><div class=" pTag sectionReplaced"><em><span><span style="font-size: 17px;">[8] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie. Con-</span></span></em></div><div class=" pTag sectionReplaced"><em><span><span style="font-size: 17px;">vnext v2: Co-designing and scaling convnets with masked autoencoders. In CVPR, 2023.</span></span></em></div><div class=" pTag sectionReplaced"><em><span><span style="font-size: 17px;">[9] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. In CVPR, 2019.</span></span></em></div><div class=" pTag sectionReplaced"><em><span><span style="font-size: 17px;">[10] Wenzhe Shi, Jose Caballero, Ferenc Husz ´ar, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In CVPR, 2016.</span></span></em></div><div class=" pTag sectionReplaced"><em><span><span style="font-size: 17px;">[11] Huawei Wei, Zejun Yang, and Zhisheng Wang. Aniportrait: Audio-driven synthesis of photorealistic portrait animation. arXiv preprint:2403.17694, 2024.</span></span></em></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">©&nbsp;THE END&nbsp;</span></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">转载请联系本公众号获得授权</span></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">投稿或寻求报道：<a class="__cf_email__" href="https://posts.careerengine.us/cdn-cgi/l/email-protection">[email&nbsp;protected]</a></span></div>  <div class="read-more-button"><div class="cce-btn cce-btn-light-grey" id="readMore">继续阅读</div></div>  <a class="post-original-link" href="https://careerengine.us/redirect/to?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FJrKF_7To8PEggEfw7W09ew">阅读原文 </a>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://posts.careerengine.us/p/669743fb2d71e359dfbf29fb</id>
            <title>AKOOL助力戛纳广告大奖，发布革命性实时数字人平台</title>
            <link>https://posts.careerengine.us/p/669743fb2d71e359dfbf29fb</link>
            <guid isPermaLink="false">https://posts.careerengine.us/p/669743fb2d71e359dfbf29fb</guid>
            <pubDate></pubDate>
            <updated>Wed, 17 Jul 2024 04:09:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能, 数字人, 直播技术, 创意营销  
<br><br>  
总结: 2024年欧洲杯期间，法国电信公司Orange发布了一则由人工智能生成的足球比赛视频，获得戛纳国际创意节体育类大奖。AKOOL为该作品提供了核心技术支持，开发了AI面部捕捉系统，能够精确捕捉人脸表情和动作。AKOOL还推出了数字人系统，具备实时展现流畅动作的能力，广泛应用于直播、远程教育、客服等领域。该技术通过云端计算和自然语言处理等先进技术，提升了用户体验和交互能力。AKOOL致力于在数字人技术领域进行创新，同时关注技术的责任与应用。 </div>
                        <hr>
                    
                    <div class=" pTag sectionReplaced"><div><p style="text-align: center;"><span style="font-size: 17px;">机器之心发布<span style="display: none;">‍</span><span style="display: none;">‍</span></span></p><div class=" pTag" style="text-align: center;"><strong style="font-size: 17px; font-weight: 600;">机器之心编辑部</strong></div></div></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">2024年欧洲杯如火如荼进行之际，一则由法国电信公司 Orange 创作的足球比赛视频也迅速走红。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">视频中，我们看到了姆巴佩、吉鲁、格里兹曼......，实际上，所有在球场上奔跑的运动员都不是真人，而是人工智能生成的虚拟角色。</span></div><div class=" pTag sectionReplaced"><span><strong style="font-weight: 600;"><span style="font-size: 17px;">凭借出色创意和独特性，该作品斩获了广告创意营销界“奥斯卡”——今年的戛纳国际创意节的体育类大奖。而 AKOOL 为这一大奖作品提供了核心技术支持。</span></strong></span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">他们开发的 AI 面部捕捉系统能精确捕捉人脸细微表情和动作，在精心设计的渲染技术加持下，作品中的虚拟人物几乎以假乱真。这种影像级别的视觉效果，足以满足高质量视频制作需求。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvjECrzA4WEesYAx1icGsOIqb5JrTaIRTQWN3Xp9xCCZAUbx6JAMrRHvw/640?wx_fmt=gif&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced"><span><em><span style="font-size: 17px;">AKOOL客户Orange的戛纳获奖视频截图</span></em></span><br /></div><div class=" pTag sectionReplaced" style="text-align: center;"><span style="font-size: 17px;"><strong style="font-weight: 600;">AKOOL 发力实时数字人，多领域应用显潜力</strong></span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">现在，AKOOL 进一步将目光投向<span style="font-size: 17px;"><strong style="font-weight: 600;">数字人业务和直播推流领域</strong></span>。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">在现有技术基础上，他们推出了一套数字人系统，不仅可以捕捉人脸的细微表情，还能实时展现流畅动作，生成数字人的娱乐性与互动性可与真人媲美。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">而直播技术致力于提升直播呈现方式，透过主播实时表情和动作的虚拟化，丰富观众收看体验。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;"><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvygYkHicvoIxxULUcqOOjaia9uW3DKBeTicThAicArqYJQlBHQWtPh0icuNA/640?wx_fmt=png&amp;from=appmsg" /></div></div></div></span></div><div class=" pTag sectionReplaced"><span><em><span style="font-size: 17px;">AKOOL直播技术展示</span></em></span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">目前，AKOOL有近300万用户，并与多家世界500强公司合作。去年他们还为可口可乐公司和英雄联盟游戏的联合营销活动提供了AI 面部增强技术支持。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">近年来，数字人技术在多个领域展现出巨大潜力，为用户体验和商业模式带来创新。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">前不久某平台主播直播时，突然晕厥倒地，而在直播和互动体验上，实时数字人技术为直播平台提供了新的可能性。数字人主播不仅语言流畅，能与观众实时互动，增强用户参与感，还可以24小时在线，永不疲惫。</span></div><div class=" pTag sectionReplaced"><div class=" ce-iframe-holder offset offset-old-5"></div></div><div class=" pTag sectionReplaced"><span><strong style="font-weight: 600;"><span style="font-size: 17px;">数字人教师</span></strong></span><span style="font-size: 17px;">也在改变远程教育的授课模式。借助实时互动、情景模拟等手段，数字人教师能让课堂体验更加生动和吸引人，提高学生的学习兴趣和课堂参与度。</span></div><div class=" pTag sectionReplaced"><div class=" ce-iframe-holder offset offset-old-5"></div></div><div class=" pTag sectionReplaced"><span><strong style="font-weight: 600;"><span style="font-size: 17px;">在客服领域</span></strong></span><span style="font-size: 17px;">，不管是常见问题解答、产品推荐、还是简单故障排除，数字人都可以轻松应对，并能提供全天候、个性化客户支持，为人工客服减负，提高商家服务效率。</span></div><div class=" pTag sectionReplaced"><div class=" ce-iframe-holder offset offset-old-5"></div></div><div class=" pTag sectionReplaced"><span><strong style="font-weight: 600;"><span style="font-size: 17px;">在社交媒体平台上，实时数字人技术为内容创作者提供了新工具</span></strong></span><span style="font-size: 17px;">。让雷军、埃隆·马斯克两位大佬隔空来一段虚拟脱口秀，是不是很有趣？无论是逗乐、寓教于乐、从中了解品牌故事，实时数字人都拿手。</span></div><div class=" pTag sectionReplaced"><div class=" ce-iframe-holder offset offset-old-5"></div></div><div class=" pTag sectionReplaced"><span><strong style="font-weight: 600;"><span style="font-size: 17px;">实时数字人技术还能为个性化营销带来创新和效果提升</span></strong></span><span style="font-size: 17px;">，使企业能够更好地吸引和留住客户。</span></div><div class=" pTag sectionReplaced"><div class=" ce-iframe-holder offset offset-old-5"></div></div><div class=" pTag sectionReplaced" style="text-align: center;"><span style="font-size: 17px;"><strong style="font-weight: 600;">AKOOL 实时数字人平台：亮点与关键技术</strong></span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">AKOOL 的实时数字人平台提供了<span style="font-size: 17px;"><strong style="font-weight: 600;">多样化的数字人模板</strong></span>，满足不同用户的风格和需求。用户可以根据偏好从中选择合适的形象，满足不同应用场景的需求。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">在声音处理方面，AKOOL 提供了两种主要的声音定制方式。用户既可以从预设声音库中选择，也可以上传自己或他人<span style="font-size: 17px;"><strong style="font-weight: 600;">声音样本进行克隆</strong></span>。系统会利用语音合成技术，尝试生成与用户样本高度相似的语音、语调和语速，为数字人赋予更加个性化的声音特征。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">AKOOL还提供<span style="font-size: 17px;"><strong style="font-weight: 600;">自定义数字人功能</strong></span>，用户能够详细定义数字人各项属性，包括职业背景、对话情境、交流风格、专业领域、语气倾向以及知识库内容等，尝试创建适应特定交流需求的数字人。如模拟一位资深雅思考试专家，能够根据雅思考试的标准，对考生回答进行专业评估和提问，提供更为专业化的交互体验。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">实时数字人技术在直播、远程教育、客服、社交媒体内容创作和营销等多个领域都有应用潜力，其技术的发展也离不开多项关键技术的积累。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">如<span style="font-size: 17px;"><strong style="font-weight: 600;">云端计算的突破性优势</strong></span>。AKOOL 实时虚拟人对话系统采用云端计算技术，减少了对本地硬件的依赖。无论使用何种设备，用户都能获得一致的高质量对话体验。云端强大处理能力在确保对话实时性的同时，也大幅提升了系统的灵活性和可扩展性。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">为了进一步提升用户体验，AKOOL相比<span style="font-size: 17px;"><strong style="font-weight: 600;">竞品还做了突破性升级</strong></span>。AKOOL集成了先进的人工智能算法。这些算法显著增强了系统的响应速度和对话理解力，使交互更贴近用户的自然语言习惯，带来更加个性化和富有洞察力的互动方式。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">在视觉呈现方面，AKOOL也做出了重要突破。为了大幅提高数字人的真实感，他们开发了<span style="font-size: 17px;"><strong style="font-weight: 600;">面部表情与口型同步技术</strong></span>，确保数字人的面部动作和口型与语音或文字输入精确对应，使得数字人表现更加生动自然。</span></div><div class=" pTag sectionReplaced"><div class=" ce-iframe-holder offset offset-old-5"></div></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">除了追求影视级视觉效果，AKOOL 还注重提升<span style="font-size: 17px;"><strong style="font-weight: 600;">数字人的智能交互能力</strong></span>。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">如系统集成了自然语言处理（NLP）技术和机器学习算法，使数字人能够理解和响应复杂的语言结构和语义，实现更加智能的人机交互。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">面对实时虚拟人所需的大量数据处理和复杂算法运算，AKOOL对计算资源的分配和使用进行了精心优化，旨在提供稳定而流畅的数字人交互体验。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">为了促进技术广泛应用，<span style="font-size: 17px;"><strong style="font-weight: 600;">AKOOL提供了API接口</strong></span>，方便企业快速接入并利用这一先进的人工智能技术。同时，系统支持超过40种主流语言，满足了全球化环境下的多语言需求。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">在整个系统的设计过程中，<span style="font-size: 17px;"><strong style="font-weight: 600;">AKOOL始终注重响应速度和可靠性</strong></span>，目标是在各种网络环境下提供稳定的用户体验。此外，系统采用4K超高清分辨率，旨在呈现高质量的画面和精细的动作细节，进一步提升数字人的真实感和沉浸式体验。</span></div><div class=" pTag sectionReplaced" style="text-align: center;"><span style="font-size: 17px;"><strong style="font-weight: 600;">AKOOL：“另一种对空间的征服”</strong></span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">据了解，AKOOL员工近50名，分布在多个国家和城市，体现了其国际化的运营模式。顾问团队包括来自全球知名企业的高管和一些顶尖高校的研究人员。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">公司创始人吕家俊在伊利诺伊大学香槟分校获得人工智能博士学位，曾在斯坦福大学进行访问研究，并参与过哈佛商学院总裁班（PLDA）的学习。在创立AKOOL之前，他参与过其他科技公司的早期运营，并在生成式人工智能领域有多年研究经验。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">吕家俊曾表示，公司正在开发的数字克隆技术旨在帮助人们以数字化形式保存自己的声音和形象。他认为这项技术未来可能成为保存个人智慧的一种方式。吕家俊将AKOOL在AI领域的工作比作"另一种对空间的征服"。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">为了应对Deepfake视频可能带来的滥用风险，AKOOL 也正在研发Fake视频鉴别功能。公司表示，将继续在数字人技术领域进行探索和创新，并在创新和责任之间寻找平衡，推动行业发展。</span></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">©&nbsp;THE END&nbsp;</span></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">转载请联系本公众号获得授权</span></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">投稿或寻求报道：<a class="__cf_email__" href="https://posts.careerengine.us/cdn-cgi/l/email-protection">[email&nbsp;protected]</a></span></div>  <div class="read-more-button"><div class="cce-btn cce-btn-light-grey" id="readMore">继续阅读</div></div>  <a class="post-original-link" href="https://careerengine.us/redirect/to?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FyueQuyeSAAUyxjqRv72K9A">阅读原文 </a>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://posts.careerengine.us/p/6695f7879b5bd52cb1e9042e</id>
            <title>谷歌机器人专家：机器人在现实中碰过的壁，AI也会碰</title>
            <link>https://posts.careerengine.us/p/6695f7879b5bd52cb1e9042e</link>
            <guid isPermaLink="false">https://posts.careerengine.us/p/6695f7879b5bd52cb1e9042e</guid>
            <pubDate></pubDate>
            <updated>Tue, 16 Jul 2024 04:31:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 机器人技术, 现实复杂性, 机器学习, LLM挑战  
<br>
<br>
总结: 文章探讨了机器人技术进展缓慢的原因，认为其与现实世界的复杂性密切相关。机器人学面临的挑战不仅是技术本身，还包括如何处理不完美的感知和执行。作者指出，机器学习领域也将面临类似的现实壁垒，尤其是在大语言模型（LLM）应用中。尽管LLM的炒作不断升温，但其在现实世界中的表现也将受到复杂性的影响。文章强调，评估模型的有效性和可靠性是未来的重要挑战，机器人学家在这方面可能会走在前面。 </div>
                        <hr>
                    
                    <div class=" pTag sectionReplaced"><div><p style="text-align: center;"><span style="font-size: 17px;">机器之心报道</span></p><div class=" pTag" style="text-align: center;"><strong style="font-size: 17px; font-weight: 600;">编辑：张倩</strong></div></div></div><blockquote class="js_blockquote_wrap"><div class="js_blockquote_digest"><div><div class=" pTag">「机器学习一直生活在一个令机器人专家、化学家、生物学家和神经科学家羡慕不已的泡沫中，随着它真正开始发挥作用，我们所有人都将遇到其他人多年来一直在应对的同样的现实壁垒。」</div></div></div></blockquote><div class=" pTag sectionReplaced"><span style="font-size: 17px;">有人说，机器人领域进展缓慢，甚至和机器学习的其他子领域相比显得毫无进展。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">谷歌 DeepMind 机器人科学家，SayCan、RT-1、RT-2 等具身智能项目参与者 Alex Irpan 同意这一说法。但他认为，这是因为机器人学是一个和现实紧密连接的领域，现实的复杂性决定了他们不免碰壁。他还指出，这些问题不是机器人技术所独有的。同样的问题也适用于大语言模型（LLM）等技术。这些模型在面对现实世界时，会遇到与机器人学类似的复杂性问题。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">最近，他写了一篇题为「The Tragedies of Reality Are Coming for You（现实的悲剧正在向你袭来）」的博客来阐述这一观点。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvBSRvJEbnYhWfeCIcxb4b1MVR8rkfqULYLeiaIEHIeKZQm74IwTa1EDw/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced" style="text-align: center;"><strong style="font-weight: 600;"><span style="font-size: 17px;">现实的悲剧正在向你袭来</span></strong></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">2023 年，我参加了一次 ML 会议。夜未央，酒酣耳热，话题转到了一个问题上：「如果你能把任何一个机器学习子领域的资源都给另一个子领域，你会砍掉哪个，把资源给谁？」</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">我不记得别人是怎么说的，但有一个人说他们会砍掉机器人。当我进一步追问时，他们说机器人技术进展太慢，相对于其他领域来说，什么都没有发生。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">他们说机器人技术比纯软件的机器学习子领域进展缓慢，我认为他们说得没错，但我还想补充两点：</span></div><ul class="list-paddingleft-1"><li><p><span style="font-size: 17px;">机器人学习进展较慢的原因是：如果不解决难题，就很难有所作为。</span></p></li><li><p><span style="font-size: 17px;">机器人技术的难题并非机器人独有。</span></p></li></ul><div class=" pTag sectionReplaced"><span style="font-size: 17px;">在机器人技术领域，人们常说的一句话是「现实是混乱的」。相对于代码而言，我会将其延伸为「现实是复杂的」。在机器人技术中，你往往要将混乱的现实推向一个足够好的抽象层，以便代码能够在其上发挥作用。作为一个领域，计算机科学花了数十年时间在硬件和软件之间创建了良好的抽象层。代码描述了如何将电力输送到硬盘、处理器和显示器，它足够可靠，我甚至不需要考虑它。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvHCicqNCAOG1mgvSmia4BcFYZCjZpfooicV8h6W1oJic7tCQJ7WT1ptgRmg/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">这样做有很多好处。一旦你完成了这项艰巨的工作，并将你的工作进展转移到抽象的逻辑空间中，一切都会变得更容易。代码和数据的可复制性令人难以置信。我在 3 台设备上同步了代表这篇博文草稿的文件副本，甚至不用花费任何精力思考。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">不过，就像 Joel Spolsky 所说，所有抽象在某种程度上都有漏洞，而我发现机器人技术中的漏洞往往更大。有很多出错的方式与代码的正确性无关。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">这和这个学科的一些基本原理有关吗？有一点。很多机器人硬件比笔记本电脑或 Linux 服务器更具实验性。消费类机器人还不是一大产业。「实验性」往往意味着「奇怪的、更容易出现故障的状态」。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">但是，我不认为硬件是造成问题的主要原因。现实才是问题的根源。Benjamin Holson 在他的「Mythical Non-Roboticist（神话般的非机器人学家）」一文中说得非常好：</span></div><blockquote class="js_blockquote_wrap"><div class="js_blockquote_digest"><div class=" pTag">第一个难点在于，机器人要处理的是现实世界中不完美的感知和不完美的执行。全局可变状态是一种糟糕的编程风格，因为它真的很难处理，但对于机器人软件来说，整个物理世界都是全局可变状态，你只能不可靠地观察它，并希望你的行动能接近你想要实现的目标。</div></div></blockquote><div class=" pTag sectionReplaced"><span style="font-size: 17px;">机器人研究依赖于在现实与软件之间搭建新的桥梁，但这也发生在机器人研究之外。任何与现实对接的软件，对现实的了解都是不完美的。任何试图影响现实世界变化的软件，都必须应对现实的全局可变状态。任何软件，如果其行为依赖于现实中发生的事情，就会招致对抗性的噪声和复杂性。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">游戏 AI 就是一个很好的例子。国际象棋 AI 是可靠的超人。然而，如果你以特定方式下棋，一些超人围棋 AI 是可以击败的，正如 Tony T. Wang 等人在 ICML 2023 论文「Adversarial Policies Beat Superhuman Go AIs」中发现的那样。对抗性技术找到了足够清晰以至于人类可以复制的策略。</span></div><div class=" pTag sectionReplaced"><blockquote class="js_blockquote_wrap"><div class="js_blockquote_digest"><div class=" pTag">在附录 G.2 中，我们的一位作者，一位围棋专家，能够在没有任何算法帮助的情况下，通过学习对手的对局记录来实现这种 [cyclic] 攻击。他们在 KGS 在线围棋服务器上以标准人类条件对局，在与作者无关的顶级 KataGo 机器人对局中取得了超过 90% 的胜率。</div></div></blockquote></div><blockquote class="js_blockquote_wrap"><div class="js_blockquote_digest"><div><div><div class=" pTag">作者甚至在给机器人 9 个让子的情况下获胜，这是一个巨大的优势：拥有这些让子的人类职业棋手在面对任何对手（无论是人类还是人工智能）时，胜率几乎都是 100%。他们还击败了 KataGo 和 Leela Zero，二者每局棋的搜索次数都达到了 10 万次，这通常远远超出了人类的能力范围。此后，其他人类也利用 cyclic 攻击击败了其他各种顶级围棋 AI。</div></div></div></div></blockquote><div class=" pTag sectionReplaced"><span style="font-size: 17px;">与此同时，几年前，OpenAI 创建了一个系统，该系统击败了 Dota 2 的卫冕世界冠军。在向公众开放该系统以测试其稳健性后，一个团队设计了一套策略，取得了 10 场连胜。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvtLb310HAuceZXnsAibeksJhiamAYLFgcqlERa7KeuzWRiclIoXttgxicOA/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">基于这一点，你可能会持一种悲观的观点，认为即使是连接 19 x 19 围棋棋盘或 Dota 2 这样一个简单的「现实」，其额外复杂性就足以使稳健行为具有挑战性。我认为这种观点有失公允，因为这两个系统都没有将稳健性作为最高目标，但我确实认为它们是一个有趣的案例研究。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">最近，围绕 LLM 的炒作浪潮一浪高过一浪 —— 他们能做什么，他们能在哪里应用。这其中隐含的一个信念是，LLM 可以极大地改变人们在工作和休闲中与技术交互的方式。换句话说，LLM 将改变我们与现实交互的方式。事实上，我也加入了这股炒作浪潮，具体来说，我怀疑基础模型短期炒作过度，长期炒作不足。然而，这也意味着，对于一个历来不善于考虑现实的领域来说，现实的一切混乱都将到来。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">就在这个人说机器人技术是浪费资源的同一个 ML 会议上，我提到我们正在用真实机器人进行基础模型实验。有人说这似乎有点吓人，我向他们保证这只是一个研究原型。但我也觉得 LLM 生成和执行软件有点吓人，我觉得他们隐隐担心一个却不担心另一个很有意思。硅谷的人有点自相矛盾。他们既相信软件能推动初创企业实现惊人的变革，又相信他们的软件不值得深思或反省。我认为，比特世界与原子世界一样，都是现实的一部分。它们在不同的层面上运行，但都是现实的一部分。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">我注意到（有些幸灾乐祸），LLM 从业者也开始遭遇之前机器人技术碰到过的痛点。比如「我们无法复制这些训练，因为这太耗费资金了」。是啊，这个问题在机器人领域已经讨论了至少十年。再比如，「我没法让必应告诉我《阿凡达 2》的上映日期，因为它一直在调出关于自己的新闻报道，并在生成前进行自我修正。」</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">我们现在所处的世界，任何公开的互联网文本都会不可逆转地影响检索增强生成。欢迎来到全局可变状态。每当我看到有人声称 ChatGPT 的行为出现了倒退，我就会想起我和其他人为了解释机器人性能突然莫名下降而想出的各种「阴谋论」，以及问题究竟是出在模型、环境，还是我们的过度推断。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">俗话说「所有的机器人 demo 都在撒谎」，人们发现所有的 LLM demo 也都在撒谎。我认为，从根本上说，这是无法避免的，因为人类的注意力是有限的。重要的是评估谎言的类型、大小和重要性。他们是否展示了模型 / 机器人如何泛化？他们有没有提到这些例子是如何精挑细选的？一旦将现实联系起来，这些问题就会变得更加复杂。梅西目前看起来是个不错的球员，但「他能在斯托克城寒冷的雨夜做到这一点吗」？</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">让问题变得复杂的是，这些问题的答案并不总是「否」。梅西可以在斯托克城的寒冷雨夜做到这一点。他足够优秀。这让问题变得困难，因为正确地回答一个「是」比正确地回答一个「否」要重要得多。随着 LLM 越来越优秀，随着 AI 在日常生活中越来越常见，作为一个社会，我们需要越来越善于判断模型是否已经证明了自己。我对未来的主要担忧之一，就是我们不善于评估模型是否已经证明了自己。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">但是，我期望机器人学家会走在曲线的前面。在 LLM 操纵通用基准测试的说法出现之前，我们就在抱怨评估问题。早在「我们需要更好的数据覆盖率」成为基础模型预训练团队的口号之前，我们就在努力获取足够的数据，以捕捉自动驾驶的长尾效应。机器学习一直生活在一个令机器人专家、化学家、生物学家和神经科学家羡慕不已的泡沫中，随着它真正开始发挥作用，我们所有人都将遇到其他人多年来一直在应对的同样的现实壁垒。这些挑战是可以克服的，但会很艰难。欢迎来到现实世界。欢迎来到痛苦的世界。</span></div><div class=" pTag sectionReplaced" style="text-align: left;"><span><em><span style="font-size: 17px;">原文链接：https://www.alexirpan.com/2024/07/08/tragedies-of-reality.html</span></em></span></div><div class=" pTag sectionReplaced" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibOrSA4rRUsPAQe533w3ib6XC4tj7asK9yplzGBGvraKTmRUAFGMiaOqCsZia2C395mvOGf5aQcky2yw/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">©&nbsp;THE END&nbsp;</span></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">转载请联系本公众号获得授权</span></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">投稿或寻求报道：<a class="__cf_email__" href="https://posts.careerengine.us/cdn-cgi/l/email-protection">[email&nbsp;protected]</a></span></div>  <div class="read-more-button"><div class="cce-btn cce-btn-light-grey" id="readMore">继续阅读</div></div>  <a class="post-original-link" href="https://careerengine.us/redirect/to?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FrW0qzhCBTRypNG8hPShAhA">阅读原文 </a>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://posts.careerengine.us/p/6695f7879b5bd52cb1e90436</id>
            <title>抛弃视觉编码器，这个「原生版」多模态大模型也能媲美主流方法</title>
            <link>https://posts.careerengine.us/p/6695f7879b5bd52cb1e90436</link>
            <guid isPermaLink="false">https://posts.careerengine.us/p/6695f7879b5bd52cb1e90436</guid>
            <pubDate></pubDate>
            <updated>Tue, 16 Jul 2024 04:31:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: <多模态大模型, 视觉语言模型, EVE, 训练策略>
<br>
<br>
总结: AIxiv专栏报道了多模态大模型的研究进展，特别是无编码器的视觉语言模型EVE的提出。EVE通过去除视觉编码器，采用纯解码器架构，整合视觉与语言的表征与推理，展现出优越的性能。该模型在多个基准测试中表现出色，且训练成本较低。研究团队还探讨了无编码器架构的未来发展方向，包括与带编码器模型的性能对比及多模态整合的可能性。EVE的成功为原生多模态模型的构建提供了新的思路和路径。 </div>
                        <hr>
                    
                    <div class=" pTag sectionReplaced" style="text-align: start;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9OnnzCX2HjxlUqj24Vnns9NNNzu0PPwaOst5iciaSdlMlBvia0nHGUtk9XQhXRqPP6P8KXz8wUyXicmg/640?wx_fmt=other&amp;from=appmsg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp" /></div></div></div><blockquote class="js_blockquote_wrap" style="text-align: start;"><div class="js_blockquote_digest"><div class=" pTag"><span><span style="display: inline-block;">AIxiv专栏是机器之心发布学术、技术内容的栏目。过去数年，机器之心AIxiv专栏接收报道了2000多篇内容，覆盖全球各大高校与企业的顶级实验室，有效促进了学术交流与传播。如果您有优秀的工作想要分享，欢迎投稿或者联系报道。</span><span style="display: inline-block;">投稿邮箱：<a class="__cf_email__" href="https://posts.careerengine.us/cdn-cgi/l/email-protection">[email&nbsp;protected]</a>；<a class="__cf_email__" href="https://posts.careerengine.us/cdn-cgi/l/email-protection">[email&nbsp;protected]</a></span></span></div></div></blockquote><div class=" pTag"><strong style="font-weight: 600;"><span style="font-size: 17px;">一作刁海文，是大连理工大学博士生，导师是卢湖川教授。目前在北京智源人工智能研究院实习，指导老师是王鑫龙博士。他的研究兴趣是视觉与语言，大模型高效迁移，多模态大模型等。共同一作崔玉峰，毕业于北京航空航天大学，是北京智源人工智能研究院视觉中心算法研究员。他的研究兴趣是多模态模型、生成模型和计算机视觉，主要工作有 Emu 系列。</span></strong></div><div class=" pTag"><span style="font-size: 17px;">近期，关于多模态大模型的研究如火如荼，工业界对此的投入也越来越多。国外相继推出了炙手可热的模型，例如 GPT-4o （OpenAI）、Gemini（Google）、Phi-3V （Microsoft）、Claude-3V（Anthropic），以及 Grok-1.5V（xAI）等。与此同时，国内的 GLM-4V（智谱 AI）、Step-1.5V（阶跃星辰）、Emu2（北京智源）、Intern-VL（上海 AI 实验室）、Qwen-VL（阿里巴巴）等模型百花齐放。</span></div><div class=" pTag"><span style="font-size: 17px;">当前的视觉语言模型（VLM）通常依赖视觉编码器（Vision Encoder, VE）来提取视觉特征，再结合用户指令传入大语言模型（LLM）进行处理和回答，主要挑战在于视觉编码器和大语言模型的训练分离。这种分离导致视觉编码器在与大语言模型对接时引入了视觉归纳偏置问题，例如受限的图像分辨率和纵横比，以及强烈的视觉语义先验。随着视觉编码器容量的不断扩大，多模态大模型在处理视觉信号时的部署效率也受到极大限制。此外，如何找到视觉编码器和大语言模型的最佳容量配置，也变得越来越具有复杂性和挑战性。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW84LUpvoJQJYzIJich9Ms0bibCAwGrr6KeYmGfL98pmW4DltKPmHBUdibwZL6URqOSibbIaCgJ2a6KknQ/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag"><span style="font-size: 17px;">在此背景下，一些更加前沿的构想迅速浮现：</span></div><ul class="list-paddingleft-1"><li><div class=" pTag" style="text-align: left;"><strong style="font-weight: 600;"><span style="font-size: 17px;">能否去除视觉编码器，即直接构建无视觉编码器的原生多模态大模型？</span></strong></div></li><li><div class=" pTag" style="text-align: left;"><strong style="font-weight: 600;"><span style="font-size: 17px;">如何高效且丝滑地将大语言模型演变为无视觉编码器的原生多模态大模型？</span></strong></div></li><li><div class=" pTag" style="text-align: left;"><strong style="font-weight: 600;"><span style="font-size: 17px;">如何弥合无编码器的原生多模态框架和基于编码器的主流多模态范式的性能差距？</span></strong></div></li></ul><div class=" pTag"><span style="font-size: 17px;">Adept AI 在 2023 年末发布了 Fuyu 系列模型并做出了一些相关尝试，但在训练策略、数据资源和设备信息方面没有任何披露。同时，Fuyu 模型在公开的视觉文本评测指标上与主流算法存在显著的性能差距。同期，我们进行的一些先导试验显示，即使大规模拉升预训练数据规模，无编码器的原生多模态大模型仍面临收敛速度慢和性能表现差等棘手问题。</span></div><div class=" pTag"><span style="font-size: 17px;">针对这些挑战，智源研究院视觉团队联合大连理工大学、北京大学等国内高校，推出了新一代无编码器的视觉语言模型 EVE。通过精细化的训练策略和额外的视觉监督，EVE 将视觉 - 语言表征、对齐和推理整合到统一的纯解码器架构中。使用公开数据，EVE 在多个视觉 - 语言基准测试中表现出色，与类似容量的基于编码器的主流多模态方法相媲美，并显著优于同类型 Fuyu-8B。EVE 的提出旨在为纯解码器的原生多模态架构发展提供一条透明且高效的路径。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW84LUpvoJQJYzIJich9Ms0bibXfRGcTjqiaxePNr062lq3MoWjY0x6VTmVxP95XXwpp5W6Ey6D3uNicmg/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW84LUpvoJQJYzIJich9Ms0bibReTFicaPbdaOg0FLectm0hqTf54rBkBMeRkEXibeXfzothg2vMsicSNAQ/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><ul class="list-paddingleft-1"><li><div class=" pTag" style="text-align: left;"><span style="font-size: 17px;">论文地址:&nbsp; https://arxiv.org/abs/2406.11832</span></div></li><li><div class=" pTag" style="text-align: left;"><span style="font-size: 17px;">项目代码:&nbsp; https://github.com/baaivision/EVE</span></div></li><li><div class=" pTag" style="text-align: left;"><span style="font-size: 17px;">模型地址:&nbsp; https://huggingface.co/BAAI/EVE-7B-HD-v1.0</span></div></li></ul><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;"><strong style="font-weight: 600;">1. 技术亮点&nbsp;</strong></span></div><ul class="list-paddingleft-1"><li><div class=" pTag" style="text-align: left;"><span style="font-size: 17px;">原生视觉语言模型：打破了主流的多模态模型的固定范式，去除视觉编码器，可处理任意图像长宽比。在多个视觉语言基准测试中显著优于同类型的 Fuyu-8B 模型，并接近主流的基于视觉编码器的视觉语言架构。</span></div></li><li><div class=" pTag" style="text-align: left;"><span style="font-size: 17px;">数据和训练代价少:&nbsp; EVE 模型的预训练仅筛选了来自 OpenImages、SAM 和 LAION 的公开数据，并利用了 66.5 万条 LLaVA 指令数据和额外的 120 万条视觉对话数据，分别构建了常规版本和高分辨版本的 EVE-7B。训练在两个 8-A100 (40G) 节点上约需 9 天完成，或者在四个 8-A100 节点上约需 5 天完成。</span></div></li><li><div class=" pTag" style="text-align: left;"><span style="font-size: 17px;">透明和高效的探索: EVE 尝试探索一条高效、透明且实用的路径通往原生视觉语言模型，为开发新一代纯解码器的视觉语言模型架构提供全新的思路和宝贵的经验，为未来多模态模型的发展开辟新的探索方向。</span></div></li></ul><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;"><strong style="font-weight: 600;">2. 模型结构</strong></span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW84LUpvoJQJYzIJich9Ms0bibzcicY93jUplygC7bqHjQOK8ucZXp4fricTRynciaZoVNcCgsFUozhUooQ/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag"><span style="font-size: 17px;">首先，通过 Vicuna-7B 语言模型进行初始化，使其具备丰富的语言知识和强大的指令跟随能力。在此基础上，去除深度视觉编码器，构建轻量级视觉编码层，高效无损地编码图像输入，并将其与用户语言命令输入到统一的解码器中。此外，通过视觉对齐层与通用的视觉编码器进行特征对齐，强化细粒度的视觉信息编码和表征。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW84LUpvoJQJYzIJich9Ms0bibI2EDwv2D6oLXm316K8LXuw3Qhc5ts6I4NutLvYqpmvpB8iaZCta5Klg/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag"><span style="font-size: 17px;">2.1 Patch Embedding Layer</span></div><ul class="list-paddingleft-1"><li><div class=" pTag" style="text-align: left;"><span style="font-size: 17px;">首先使用单层卷积层来获取图像的 2D 特征图，然后通过平均池化层进行下采样；</span></div></li><li><div class=" pTag" style="text-align: left;"><span style="font-size: 17px;">使用交叉注意力模块（CA1）在限定感受野中交互，增强每个 patch 的局部特征；</span></div></li><li><div class=" pTag" style="text-align: left;"><span style="font-size: 17px;">使用 &lt; CLS&gt; token 并结合交叉注意力模块（CA2），为后续每个 patch 特征提供全局信息；</span></div></li><li><div class=" pTag" style="text-align: left;"><span style="font-size: 17px;">在每个 patch 特征行的末尾插入了一个可学习的 &lt; SPL&gt; token，帮助网络理解图像的二维空间结构。</span></div></li></ul><div class=" pTag"><span style="font-size: 17px;">2.2 Patch Aligning Layer</span></div><ul class="list-paddingleft-1"><li><div class=" pTag" style="text-align: left;"><span style="font-size: 17px;">记录有效 patch 的二维形状；丢弃 &lt; CLS&gt;/&lt;PAD&gt; tokens，并利用自适应池化层还原到原始的二维形状；</span></div></li><li><div class=" pTag" style="text-align: left;"><span style="font-size: 17px;">通过层级交叉注意力模块（CA3），整合多层网络视觉特征，从而实现与视觉编码器输出的细粒度对齐。</span></div></li></ul><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;"><strong style="font-weight: 600;">3. 训练策略</strong></span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW84LUpvoJQJYzIJich9Ms0bibTqO71yHlk6nibtBkYW0w2mDe7ENmW7Ev2sCjJVhHUib9WTPULARLI7WQ/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><ul class="list-paddingleft-1"><li><div class=" pTag" style="text-align: left;"><span style="font-size: 17px;">大语言模型引导的预训练阶段：建立视觉和语言之间的初步联系，为后续稳定高效的大规模预训练打下基础；</span></div></li><li><div class=" pTag" style="text-align: left;"><span style="font-size: 17px;">生成式预训练阶段：进一步提高模型对视觉 - 语言内容的理解能力，实现纯语言模型到多模态模型的丝滑转变；</span></div></li><li><div class=" pTag" style="text-align: left;"><span style="font-size: 17px;">监督式的微调阶段：进一步规范模型遵循语言指令和学习对话模式的能力，满足各种视觉语言基准测试的要求。</span></div></li></ul><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW84LUpvoJQJYzIJich9Ms0bibsmibNf3TiaVVlq3LoM0MuTC0q47RBqicT9biaqs9YWYbSR7lVsTMFQC3lA/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><ul class="list-paddingleft-1"><li><div class=" pTag"><span style="font-size: 17px;">在预训练阶段，筛选了来自 SA-1B、OpenImages 和 LAION 等 3300 万公开数据，仅保留分辨率高于 448×448 的图像样本。特别地，针对 LAION 图像冗余度高的问题，通过在 EVA-CLIP 提取的图像特征上应用 K-means 聚类，生成 50,000 个聚类，并从中挑选出最接近每个聚类中心的 300 张图像，最终选出 1500 万张 LAION 图像样本。随后，利用 Emu2 （17B）和 LLaVA-1.5 （13B）重新生成高质量图像描述。</span></div></li><li><div class=" pTag"><span style="font-size: 17px;">在监督微调阶段，使用 LLaVA-mix-665K 微调数据集来训练得到标准版的 EVE-7B，并整合 AI2D、Synthdog、DVQA、ChartQA、DocVQA、Vision-Flan 和 Bunny-695K 等混合数据集来训练得到高分辨率版本的 EVE-7B。</span></div></li></ul><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;"><strong style="font-weight: 600;">4. 定量分析</strong></span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW84LUpvoJQJYzIJich9Ms0bibCD3EibWIef0KR1yvk6LiakRiaMROtLRndG73kz7sSbe0GC0W8aCxJ6ajA/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag"><span style="font-size: 17px;">EVE 模型在多个视觉语言基准测试中明显优于同类型的 Fuyu-8B 模型，并且与多种主流的基于编码器的视觉语言模型表现相当。然而，由于使用大量视觉语言数据训练，其在准确响应特定指令方面存在挑战，在部分基准测试中表现有待提高。令人兴奋的是，通过高效的训练策略，可以实现无编码器的 EVE 与带编码器基础的视觉语言模型取得相当的性能，从根本上解决主流模型在输入尺寸灵活性、部署效率和模态容量匹配方面的问题。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW84LUpvoJQJYzIJich9Ms0bibKzWOZlRW7l69mZf4gIPLKzsMy3LzuLN0sPHGNyOJCju6I14caJqQIQ/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag"><span style="font-size: 17px;">相较于带编码器的</span><span style="font-size: 17px;">模型易受到语言结构简化和丰富知识丢失等问题困扰，EVE 表现出随着数据规模的增加而逐步稳定地提升性能，逐渐逼近基于编码器模型的性能水平。</span><span style="font-size: 17px;">这可能是因为在统一网络中编码和对齐视觉和语言模态更具挑战性，使得无编码器模型相对于带编码器的模型更不容易过拟合。</span></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;"><strong style="font-weight: 600;">5. 同行怎么看？</strong></span></div><div class=" pTag"><span style="font-size: 17px;">英伟达高级研究员 Ali Hatamizadeh 表示，EVE 令人耳目一新，尝试提出全新的叙事，区别于构建繁杂的评测标准和渐进式的视觉语言模型改进。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW84LUpvoJQJYzIJich9Ms0bib2Ms8bzBvePvia6uAhXmch00QZ6YQEYhkib3qdts44MC34z1SbmianahpA/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag"><span style="font-size: 17px;">谷歌 Deepmind 首席研究员 Armand Joulin 表示，构建纯解码器的视觉语言模型令人兴奋。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW84LUpvoJQJYzIJich9Ms0bibpBfTeH84ZDQtT6Wr0xf7aiblrJfZsVHDOrNQuo5icmXTYJxbnialKcia2A/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag"><span style="font-size: 17px;">苹果机器学习工程师 Prince Canuma 表示，EVE 架构非常有趣，对 MLX VLM 项目集是一个很好的补充。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW84LUpvoJQJYzIJich9Ms0bibx2F1mkMdz5M4C4kz9LhIMDXtyHNIw49laCrz3Zh7WuiaibZiaaLGA3qZg/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;"><strong style="font-weight: 600;">6.未来展望</strong></span></div><div class=" pTag"><span style="font-size: 17px;">作为无编码器的原生视觉语言模型，目前 EVE 取得了令人鼓舞的结果。沿着这条路径，未来还有一些有趣的方向值得探索尝试：</span></div><ul class="list-paddingleft-1"><li><div class=" pTag" style="text-align: left;"><span style="font-size: 17px;">进一步的性能提升：实验发现，仅使用视觉 - 语言数据进行预训练显著地降低了模型的语言能力（SQA 得分从 65.3% 降至 63.0%），但逐步提升了模型的多模态性能。这表明在大语言模型更新时，内部存在语言知识的灾难性遗忘。建议适当融合纯语言的预训练数据，或采用专家混合（MoE）策略来减少视觉与语言模态间干扰。</span></div></li><li><div class=" pTag" style="text-align: left;"><span style="font-size: 17px;">无编码器架构的畅想：通过恰当策略和高质量数据的训练，无编码器视觉语言模型可以与带编码器的模型相匹敌。那么在相同的模型容量和海量的训练数据下，二者性能如何？我们推定通过扩大模型容量和训练数据量，无编码器架构是能够达到甚至超越基于编码器架构，因为前者几乎无损地输入图像，避开了视觉编码器的先验偏置。</span></div></li><li><div class=" pTag" style="text-align: left;"><span style="font-size: 17px;">原生多模态的构建: EVE 完整地展现了如何高效稳定地构建原生多模态模型，这为之后整合更多模态（如音频、视频、热成像、深度等）开辟了透明和切实可行的道路。核心思想是在引入大规模统一训练之前，先通过冻结的大语言模型对这些模态进行预对齐，并利用相应的单模态编码器和语言概念对齐进行监督。</span></div></li></ul><div class=" pTag sectionReplaced" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibOrSA4rRUsPAQe533w3ib6XC4tj7asK9yplzGBGvraKTmRUAFGMiaOqCsZia2C395mvOGf5aQcky2yw/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">©&nbsp;THE END&nbsp;</span></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">转载请联系本公众号获得授权</span></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">投稿或寻求报道：<a class="__cf_email__" href="https://posts.careerengine.us/cdn-cgi/l/email-protection">[email&nbsp;protected]</a></span></div>  <div class="read-more-button"><div class="cce-btn cce-btn-light-grey" id="readMore">继续阅读</div></div>  <a class="post-original-link" href="https://careerengine.us/redirect/to?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FAt2Wz9Kk2QzGEZ--4FnbZw">阅读原文 </a>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://posts.careerengine.us/p/6695f7879b5bd52cb1e9043f</id>
            <title>公理训练让LLM学会因果推理：6700万参数模型比肩万亿参数级GPT-4</title>
            <link>https://posts.careerengine.us/p/6695f7879b5bd52cb1e9043f</link>
            <guid isPermaLink="false">https://posts.careerengine.us/p/6695f7879b5bd52cb1e9043f</guid>
            <pubDate></pubDate>
            <updated>Tue, 16 Jul 2024 04:31:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: <因果推理, Transformer模型, 公理训练, 泛化能力>
<br>
<br>
总结: 本文介绍了一项研究，探讨了如何通过公理训练使Transformer模型学习因果推理。研究发现，在小图谱的因果传递性公理演示上训练的模型能够泛化到大图谱的传递性公理。该团队提出的公理训练框架基于被动数据，允许模型通过符号演示学习因果推理。实验表明，使用这种方法训练的模型在多种评估场景中表现出良好的泛化能力，甚至在复杂任务中也能超越一些大型语言模型。 </div>
                        <hr>
                    
                    <div class=" pTag sectionReplaced"><div><p style="text-align: center;"><span style="font-size: 17px;">机器之心报道</span></p><div class=" pTag" style="text-align: center;"><strong style="font-size: 17px; font-weight: 600;">编辑：Panda</strong></div></div></div><blockquote class="js_blockquote_wrap"><div class="js_blockquote_digest"><div><p><span>把因果链展示给 LLM，它就能学会公理。</span></p></div></div></blockquote><div class=" pTag sectionReplaced"><span style="font-size: 17px;">AI 已经在帮助数学家和科学家做研究了，比如著名数学家陶哲轩就曾多次分享自己借助 GPT 等 AI 工具研究探索的经历。AI 要在这些领域大战拳脚，强大可靠的因果推理能力是必不可少的。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">本文要介绍的这项研究发现：在小图谱的因果传递性公理演示上训练的 Transformer 模型可以泛化用于大图谱的传递性公理。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">也就是说，如果让 Transformer 学会执行简单的因果推理，就可能将其用于更为复杂的因果推理。该团队提出的公理训练框架是一种基于被动数据来学习因果推理的新范式，只有演示足够就能用于学习任意公理。</span></div><div class=" pTag sectionReplaced" style="text-align: center;"><strong style="font-weight: 600;"><span style="font-size: 17px;">引言</span></strong></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">因果推理（causal reasoning）可以定义成一组推理流程并且这组推理流程要符合专门针对因果性的预定义公理或规则。举个例子，d-separation（有向分离）和 do-calculus 规则可被视为公理，而 collider set 或 backdoor set 的规范则可被看作是由公理推导出的规则。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">通常来说，因果推理使用的数据对应于一个系统中的变量。通过正则化、模型架构或特定的变量选择，可以归纳偏置的形式将公理或规则集成到机器学习模型中。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">根据可用数据种类的差异（观察数据、干预数据、反事实数据），Judea Pearl 提出的「因果阶梯」定义了因果推理的可能类型。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">由于公理是因果性的基石，因此我们不禁会想是否可以直接使用机器学习模型来学习公理。也就是说，如果学习公理的方式不是学习通过某个数据生成流程得到的数据，而是直接学习公理的符号演示（并由此学习因果推理），哪又会如何呢？</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">相较于使用特定的数据分布构建的针对特定任务的因果模型，这样的模型有一个优势：其可在多种不同的下游场景中实现因果推理。随着语言模型具备了学习以自然语言表达的符号数据的能力，这个问题也就变得非常重要了。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">事实上，近期已有一些研究通过创建以自然语言编码因果推理问题的基准，评估了大型语言模型（LLM）是否能够执行因果推理。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">微软、MIT 和印度理工学院海得拉巴分校（IIT Hyderabad）的研究团队也朝这个方向迈出了重要一步：提出了一种<span><strong style="font-weight: 600;">通过公理训练（axiomatic training）学习因果推理的方法</strong></span>。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvLU25WRB3zhZvk1lGicMIMydB6mia3nVkuqbZUFFSVs2wicdYcKjvrsrTw/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><ul class="list-paddingleft-1"><li><p style="text-align: left;"><span style="font-size: 17px;">论文标题：Teaching Transformers Causal Reasoning through Axiomatic Training</span></p></li><li><p style="text-align: left;"><span style="font-size: 17px;">论文地址：https://arxiv.org/pdf/2407.07612</span></p></li></ul><div class=" pTag sectionReplaced" style="text-align: center;"><strong style="font-weight: 600;"><span style="font-size: 17px;">公理训练</span></strong></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">他们假设，可将因果公理表示成以下符号元组 ⟨premise, hypothesis, result⟩。其中 hypothesis 是指假设，即因果陈述；premise 是前提，是指用于确定该陈述是否为「真」的任意相关信息；result 自然就是结果了。结果可以是简单的「是」或「否」。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">举个例子，来自论文《Can large language models infer causation from correlation?》的 collider 公理可以表示成：</span><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvqSjPIibwT0kofICqj6ibLWfyp3ghRibFSibC3qmTwDzSbwjWqTxia6HpEqw/640?wx_fmt=png&amp;from=appmsg" /></div></div>，<span style="font-size: 17px;">结论就为「是」。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">基于这个模板，可通过修改变量名称、变量数量和变量顺序等来生成大量合成元组。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">为了用 Transformer 学习因果公理，实现公理训练，该团队采用了以下方法构建数据集、损失函数和位置嵌入。</span></div><div class=" pTag sectionReplaced"><strong style="font-weight: 600;"><span style="font-size: 17px;">公理训练：数据集、损失函数和位置编制</span></strong></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">训练数据</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">基于一个特定公理，可根据「前提」将「假设」映射成合适的标签（Yes 或 No）。要创建训练数据集，该团队的做法是在特定的变量设置 X、Y、Z、A 下枚举所有可能的元组 {(P, H, L)}_N，其中 P 是前提，H 是假设，L 是标签（Yes 或 No）。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">给定一个基于某个因果图谱的前提 P，如果可通过使用特定的公理（一次或多次）推导出假设 P，那么标签 L 就为 Yes；否则为 No。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">举个例子，假设一个系统的底层真实因果图谱具有链式的拓扑结构：X_1 → X_2 → X_3 →・・・→ X_n。那么，可能的前提是 X_1 → X_2 ∧ X_2 → X_3，那么假设 X_1 → X_3 有标签 Yes，而另一个假设 X_3 → X_1 有标签 No。上述公理可被归纳式地多次用于生成更复杂的训练元组。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">对于训练设置，使用传递性公理生成的 N 个公理实例构建一个合成数据集 D。D 中的每个实例都构建成了 (P_i, H_ij, L_ij) 的形式，</span><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJv2DUosJLtOqWCicuGiaKFBVOQ4S3c4fJMiceLcHYpVoficT4D8of6sLPibFQ/640?wx_fmt=png&amp;from=appmsg" /></div></div><span style="font-size: 17px;">，其中 n 是每第 i 个前提中的节点数量。P 是前提，即某种因果结构的自然语言表达（如 X 导致 Y，Y 导致 Z）；之后是问题 H（如 X 导致 Y 吗？）；L 为标签（Yes 或 No）。该形式能有效覆盖给定因果图谱中每条独特链的所有成对节点。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">损失函数</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">给定一个数据集，损失函数的定义基于每个元组的基本真值标签，表示为：</span><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvQacxC9gQkJlqNBFXj1FXB5icrXgZGHEubiak1V6sUic88JpbyCkdoNSxA/640?wx_fmt=png&amp;from=appmsg" /></div></div><span style="font-size: 17px;">分析表明，相比于下一 token 预测，使用该损失能得到很有希望的结果。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">位置编码</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">除了训练和损失函数，位置编码的选择也是另一个重要因素。位置编码能提供 token 在序列中绝对和相对位置的关键信息。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">著名论文《Attention is all you need》中提出了一种使用周期函数（正弦或余弦函数）来初始化这些编码的绝对位置编码策略。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">绝对位置编码能为任何序列长度的所有位置提供确定的值。但是，有研究表明绝对位置编码难以应对 Transformer 的长度泛化任务。在可学习的 APE 变体中，每个位置嵌入都是随机初始化的，并使用该模型完成了训练。该方法难以应对比训练时的序列更长的序列，因为新的位置嵌入依然未被训练和初始化。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">有趣的是，近期有发现表明移除自回归模型中的位置嵌入可以提升模型的长度泛化能力，而自回归解码期间的注意力机制足以编码位置信息。该团队使用了不同的位置编码来理解其对因果任务中的泛化的影响，包括可学习位置编码（LPE）、正弦位置编码（SPE）、无位置编码（NoPE）。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">为了提升模型的泛化能力，该团队也采用了数据扰动，其中包括长度、节点名称、链顺序和分支情况的扰动。</span></div><div class=" pTag sectionReplaced" style="text-align: center;"><strong style="font-weight: 600;"><span style="font-size: 17px;">实验</span></strong></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">下面问题又来了：如果使用这些数据训练一个模型，那么该模型能否学会将该公理应用于新场景？</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">为了解答这个问题，该团队使用这个因果无关型公理的符号演示从头开始训练了一个 Transformer 模型。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">为了评估其泛化性能，他们在简单的大小为 3-6 个节点的因果无关公理链上进行了训练，然后测试了泛化性能的多个不同方面，包括长度泛化性能（大小 7-15 的链）、名称泛化性能（更长的变量名）、顺序泛化性能（带有反向的边或混洗节点的链）、结构泛化性能（带有分支的图谱）。图 1 给出了评估 Transformer 的结构泛化的方式。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvTIvKevhF4DLTZQic8nofRvvw0PiaVzwgwSmpdmttFjb3VtmbF0PJIy2g/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">具体来说，他们基于 GPT-2 架构训练了一个基于解码器的有 6700 万参数的模型。该模型有 12 个注意力层、8 个注意力头和 512 嵌入维度。他们在每个训练数据集上从头开始训练了该模型。为了理解位置嵌入的影响，他们还研究了三种位置嵌入设置：正弦位置编码（SPE）、可学习位置编码（LPE）和无位置编码（NoPE）。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">结果如表 1、图 3 和图 4 所示。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvliaC9JXQyfOuxWCLru0arEgppGcrAAHQcfW5ZVZARfgGPxPXRv1f7pw/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">表 1 给出了在训练时未曾见过的更大因果链上评估时不同模型的准确度。可以看到，新模型 TS2 (NoPE) 的表现能与万亿参数规模的 GPT-4 相媲美。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">图 3 是在有更长节点名称（长于训练集的）的因果序列上的泛化能力评估结果以及不同位置嵌入的影响。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvvib0KJuC4BZ1wfCmbaggQxibf2XoTOfMerR0pB3cNYDCDeQVU5ZNFMeA/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">图 4 评估的是在更长的未见过的因果序列上的泛化能力。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvEOYpJ7VxNpoTIrNxiaX2Al4IibdVVXUm8tVeHg1ScsBmr3icrJvWrNibNg/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">他们发现，在简单链上训练的模型可以泛化到在更大的链上多次应用公理，但却无法泛化到顺序或结构泛化等更复杂的场景。但是，如果在简单链以及带有随机逆向边的链组成的混合数据集上训练模型，则模型可以很好地泛化到各种评估场景。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">通过扩展在 NLP 任务上的长度泛化研究结果，他们发现了位置嵌入在确保在长度和其它方面实现因果泛化的重要性。他们表现最佳的模型没有位置编码，但他们也发现正弦编码在某些情况下也很好用。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">这种公理训练方法还能泛化用于一个更困难的问题，如图 5 所示。即以包含统计独立性陈述的前提为基础，任务目标是根据因果关系分辨相关性。解决该任务需要多个公理的知识，包括 d-separation 和马尔可夫性质。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvTSrO98SJZDZiaR3mzOpeKtaacMuMwQJXug01UUvypVWxATib3eZkKGrg/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">该团队使用与上面一样的方法生成了合成训练数据，然后训练了一个模型，结果发现在包含 3-4 个变量的任务演示上训练得到的 Transformer 能学会解决包含 5 个变量的图谱任务。并且在该任务上，该模型的准确度高于 GPT-4 和 Gemini Pro 等更大型的 LLM。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvGsXSY5mkKwicBtKiaiaI77pxpibB185ibOT0bd3l9UI12xKDwa49PqBswnQ/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">该团队表示：「我们的研究提供了一种通过公理的符号演示教模型学习因果推理的新范式，我们称之为公理训练（axiomatic training）。」该方法的数据生成和训练流程是普适的：只要一个公理能被表示成符号元组的格式，就可使用此方法学习它。</span></div><div class=" pTag sectionReplaced" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibtmpFW5KoyQWpAmJ6NnyDIVhOibdOONTggtx5XfvI6xWwufezyXiaFlqvefy9RAuvK1qibj2VgdP7kQ/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">©&nbsp;THE END&nbsp;</span></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">转载请联系本公众号获得授权</span></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">投稿或寻求报道：<a class="__cf_email__" href="https://posts.careerengine.us/cdn-cgi/l/email-protection">[email&nbsp;protected]</a></span></div>  <div class="read-more-button"><div class="cce-btn cce-btn-light-grey" id="readMore">继续阅读</div></div>  <a class="post-original-link" href="https://careerengine.us/redirect/to?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FNnH8QmKo2JqVV47eY3QHkw">阅读原文 </a>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://posts.careerengine.us/p/6695f775506e722c8ba2a04a</id>
            <title>太酷了！iPhone、iPad、MacBook老旧设备组成异构集群，能跑Llama 3</title>
            <link>https://posts.careerengine.us/p/6695f775506e722c8ba2a04a</link>
            <guid isPermaLink="false">https://posts.careerengine.us/p/6695f775506e722c8ba2a04a</guid>
            <pubDate></pubDate>
            <updated>Tue, 16 Jul 2024 04:30:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: <异构集群, Llama3, Cake框架, 硬件组合>
<br>
<br>
总结: 本文介绍了一种利用闲置硬件设备（如iPhone、iPad和Macbook）组建异构集群以运行Llama3模型的方法。该集群支持多种操作系统，包括Windows、Linux和iOS，未来还将支持Android。项目使用名为Cake的Rust框架，旨在将消费级硬件组合成异构集群，以实现大模型的分布式推理。用户可以通过GitHub获取相关代码，并根据需求进行编译和配置。尽管该方案具有创新性，但也引发了对能耗和数据传输效率的担忧。 </div>
                        <hr>
                    
                    <div class=" pTag sectionReplaced"><div><p style="text-align: center;"><span style="font-size: 17px;">机器之心报道</span></p><div class=" pTag" style="text-align: center;"><strong style="font-size: 17px; font-weight: 600;">机器之心编辑部</strong></div></div></div><blockquote class="js_blockquote_wrap"><div class="js_blockquote_digest"><div><p><span>假如你有闲置的设备，或许可以试一试。</span></p></div></div></blockquote><div class=" pTag"><span style="font-size: 17px;">这次，你手里的硬件设备也能在 AI 领域大展拳脚了。</span></div><div class=" pTag"><span style="font-size: 17px;">将 iPhone、iPad、Macbook 进行组合，就能组装成「异构集群推理方案」， 然后顺畅的运行 Llama3 模型。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvxicfGv3u7ibSG6E7mLpn6C7YaexhtD6LKEwB4WlpOjolCj9NOYLXw4VQ/640?wx_fmt=gif&amp;from=appmsg" /></div></div></div><div class=" pTag"><span style="font-size: 17px;">值得一提的是，这个异构集群可以是 Windows 系统，也可以是Linux、iOS 系统，并且对 Android 的支持很快到来。</span></div><div class=" pTag" style="text-align: justify;"><span><em><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvmNud7gYOuCAuDvYClEfW8GvJV8B9bNMSfKpw77Fb5dV30jIp9M05hA/640?wx_fmt=gif&amp;from=appmsg" /></div></div><span style="text-align: justify; font-size: 17px;">异<span style="display: none;">‍</span>构<span style="display: none;">‍</span>集群正在运行中。<span style="display: none;">‍</span></span></em></span></div><div class=" pTag"><span style="font-size: 17px;">根据项目作者 @evilsocket 的介绍，这个异构集群包括 iPhone 15 Pro Max、iPad Pro、MacBook Pro (M1 Max)、NVIDIA GeForce 3080、2x NVIDIA Titan X Pascal。所有代码都已经上传到 GitHub。</span></div><div class=" pTag"><span style="font-size: 17px;">看到这，网友纷纷表示，这位老哥确实不简单。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvZ3mdwrxcGv5TvHJI7CrGKRBHbFZ2dzhwicp0n3IHwHhp8rLPlZmD0Og/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag"><span style="font-size: 17px;">不过也有网友开始担心能耗问题，暂且不管速度，电费都耗不起。来回搬数据，损耗太大了。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvKaB3UQHScc2diaj35NY9yPLAdwbNKp0MUGufCiaAqbzlia5rV9vCCfgtg/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvcYKwAeshjCM9xiamP7lfMUuJcicSn2X0LRYxDH0wuAPjsZmibKxzicFmtg/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: center;"><strong style="font-weight: 600;"><span style="font-size: 17px;">项目介绍</span></strong></div><div class=" pTag"><span style="font-size: 17px;">上述功能的实现，离不开一个名为 Cake 的 Rust 框架。Cake 可以完成大模型（例如 Llama3）的分布式推理，旨在将消费级硬件组合成异构集群，其中消费级硬件采用多种操作系统，包括：iOS、Android、macOS、Linux 和 Windows，从而使 AI 更易于访问。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJvb4Ih6ticLudUQsRGHXicomvd8rbxOpic6AgahmDQEvia6vESycyaYFPt0Q/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: left;"><span style="font-size: 17px;">项目地址：https://github.com/evilsocket/cake</span></div><div class=" pTag"><span style="font-size: 17px;">Cake 的主要思路是将 transformer 块分片到多个设备，以便能够让通常不适合单个设备 GPU 内存的模型运行推理。对同一工作线程上的连续 transformer 块的推理是分批进行的，以便最大限度地减少数据传输造成的延迟。</span></div><div class=" pTag"><span style="font-size: 17px;">Cake 目前支持的系统和设备如下：</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8zCZlWibuDtIljGicNSB2kJveiajDhmKpmLly3UUAbQ8J6aXrnIMMBbUiatk106r4aqGtON99JW95gOg/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag"><span style="font-size: 17px;"><strong style="font-weight: 600;">编译</strong></span></div><div class=" pTag"><span style="font-size: 17px;">安装 Rust 后，运行下列代码：</span></div><div class="code-snippet__fix code-snippet__js pTag sectionReplaced"><div class="code-snippet__js preReplaced"><code><span class="code-snippet_outer"><span class="code-snippet__attribute">cargo</span> build --release</span></code><br /></div></div><div class=" pTag"><span style="font-size: 17px;">假如用户想要在应用程序中生成 iOS 绑定，可以进行下述操作：</span></div><div class="code-snippet__fix code-snippet__js pTag sectionReplaced"><div class="code-snippet__js preReplaced"><code><span class="code-snippet_outer">make ios</span></code><br /></div></div><div class=" pTag"><strong style="font-weight: 600;"><span style="font-size: 17px;">使用</span></strong></div><div class=" pTag"><span style="font-size: 17px;">运行 worker 节点：</span></div><div class="code-snippet__fix code-snippet__js pTag sectionReplaced"><div class="code-snippet__js preReplaced"><code><span class="code-snippet_outer">cake-cli --model /path/to/Meta-Llama-3-8B \ # model path, read below on how to optimize model size for workers</span></code><br /><code><span class="code-snippet_outer">         --mode worker \                    # run as worker</span></code><br /><code><span class="code-snippet_outer">         --name worker0 \                   # worker name in topology file</span></code><br /><code><span class="code-snippet_outer">         --topology topology.yml \          # topology</span></code><br /><code><span class="code-snippet_outer">         --address 0.0.0.0:10128            # bind address</span></code><br /></div></div><div class=" pTag"><span style="font-size: 17px;">运行 master 节点：</span></div><div class="code-snippet__fix code-snippet__js pTag sectionReplaced"><div class="code-snippet__js preReplaced"><code><span class="code-snippet_outer">cake-cli --model /path/to/Meta-Llama-3-8B \</span></code><br /><code><span class="code-snippet_outer">         --topology topology.yml</span></code><br /></div></div><div class=" pTag"><span style="font-size: 17px;">其中 topology.yml 确定哪些层由哪个 worker 提供服务：</span></div><div class="code-snippet__fix code-snippet__js pTag sectionReplaced"><div class="code-snippet__js preReplaced"><code><span class="code-snippet_outer">linux_server_1:</span></code><br /><code><span class="code-snippet_outer">  host: 'linux_server.host:10128'</span></code><br /><code><span class="code-snippet_outer">description: 'NVIDIA Titan X Pascal (12GB)'</span></code><br /><code><span class="code-snippet_outer">layers:</span></code><br /><code><span class="code-snippet_outer">    - 'model.layers.0-5'</span></code><br /><code><span class="code-snippet_outer">linux_server_2:</span></code><br /><code><span class="code-snippet_outer">  host: 'linux_server2.host:10128'</span></code><br /><code><span class="code-snippet_outer">description: 'NVIDIA GeForce 3080 (10GB)'</span></code><br /><code><span class="code-snippet_outer">layers:</span></code><br /><code><span class="code-snippet_outer">    - 'model.layers.6-16'</span></code><br /><code><span class="code-snippet_outer">iphone:</span></code><br /><code><span class="code-snippet_outer">  host: 'iphone.host:10128'</span></code><br /><code><span class="code-snippet_outer">description: 'iPhone 15 Pro Max'</span></code><br /><code><span class="code-snippet_outer">layers:</span></code><br /><code><span class="code-snippet_outer">    - 'model.layers.17'</span></code><br /><code><span class="code-snippet_outer">ipad:</span></code><br /><code><span class="code-snippet_outer">  host: 'ipad.host:10128'</span></code><br /><code><span class="code-snippet_outer">description: 'iPad'</span></code><br /><code><span class="code-snippet_outer">layers:</span></code><br /><code><span class="code-snippet_outer">    - 'model.layers.18-19'</span></code><br /><code><span class="code-snippet_outer">macbook:</span></code><br /><code><span class="code-snippet_outer">  host: 'macbook.host:10128'</span></code><br /><code><span class="code-snippet_outer">description: 'M1 Max'</span></code><br /><code><span class="code-snippet_outer">layers:</span></code><br /><code><span class="code-snippet_outer">    - 'model.layers.20-31'</span></code><br /></div></div><div class=" pTag"><span style="font-size: 17px;">关于内存和磁盘空间优化问题，用户可能希望只向 worker 提供模型中实际需要的数据，而不是整个文件夹，在这种情况下，可以使用 cake-split-model 。例如，要生成较小版本的 llama3 safetensors，可以采用如下代码：</span></div><div class="code-snippet__fix code-snippet__js pTag sectionReplaced"><div class="code-snippet__js preReplaced"><code><span class="code-snippet_outer">cake-split-model --model-path path/to/Meta-Llama-3-8B \ # source model to split</span></code><br /><code><span class="code-snippet_outer">                 --topology path/to/topology.yml \      # topology file</span></code><br /><code><span class="code-snippet_outer">                 --output output-folder-name</span></code><br /></div></div><div class=" pTag" style="text-align: left;"><span style="font-size: 17px;"><em><span style="font-size: 17px;">参考链接：https://x.com/tuturetom/status/1812654489972973643</span></em></span></div><div class=" pTag sectionReplaced"><div style="text-align: left;"><div class=" pTag"><strong style="font-weight: 600;"><span style="font-size: 17px;"><strong style="font-weight: 600;"><span>创意为王安全为先，AIGC的双线作战</span></strong></span></strong></div><p><span style="font-size: 17px;">7月17日，《AIGC体验派》第五期，邀请到火山引擎内容安全与风控负责人张建洋和NVIDIA企业级开发者社区高级经理何琨<span style="font-size: 17px;">，一起聊聊AIGC在营销领域的创新与安全问题：</span></span></p><ul class="list-paddingleft-1"><li style="font-size: 17px;"><div class=" pTag"><span style="font-size: 17px;"><div class=" pTag">如何避免大语言模型不再胡言乱语？</div><br /></span></div></li><li style="font-size: 17px;"><div class=" pTag"><span style="font-size: 17px;">如何确保AIGC创作内容的质量与安全？</span></div></li><li style="font-size: 17px;"><p>如何避免营销活动成为黑产的提款机？</p></li></ul><div class=" pTag"><strong style="font-weight: 600;"><span style="font-size: 17px;">识别海报二维码</span></strong><span style="font-size: 17px;">或<strong style="font-weight: 600;">点击阅读原文</strong>，立即报名直播。</span></div><div class=" pTag"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibtmpFW5KoyQWpAmJ6NnyDIIYibhgm0CMppKCdpRkLyTPIg4qxo9n0knria67HYCicCYr0RrBibWJUmDg/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div><br /></div></div></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">©&nbsp;THE END&nbsp;</span></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">转载请联系本公众号获得授权</span></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">投稿或寻求报道：<a class="__cf_email__" href="https://posts.careerengine.us/cdn-cgi/l/email-protection">[email&nbsp;protected]</a></span></div>  <div class="read-more-button"><div class="cce-btn cce-btn-light-grey" id="readMore">继续阅读</div></div>  <a class="post-original-link" href="https://careerengine.us/redirect/to?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FcoMIiu8a2oAJXRx48tKgDQ">阅读原文 </a>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://posts.careerengine.us/p/6695f7663f5fa12a02049f9d</id>
            <title>全程免费！「真格星球AI+创业营」与十数位大咖导师一道，碰撞AI灵感、寻找落地商机</title>
            <link>https://posts.careerengine.us/p/6695f7663f5fa12a02049f9d</link>
            <guid isPermaLink="false">https://posts.careerengine.us/p/6695f7663f5fa12a02049f9d</guid>
            <pubDate></pubDate>
            <updated>Tue, 16 Jul 2024 04:30:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 真格星球, AI创业, 科技人才, 孵化项目  
<br><br>  
总结: 「真格星球 ZhenPlanet」是由真格基金主办的前沿科技人才孵化项目，自2019年起已成功举办四期，旨在支持科技领域的创业者。项目提供资本支持、资源对接和孵化指导，帮助参与者将科技成果转化为商业产品。活动期间，参与者将与行业专家和导师交流，分享创业经验，激发创新思维。尽管AI技术取得了一些成就，但仍面临许多挑战，真格星球希望吸引对AI充满热情的先锋者，共同探索其无限可能。 </div>
                        <hr>
                    
                    <blockquote class="js_blockquote_wrap"><div class="js_blockquote_digest"><div><p><span style="font-size: 17px;">「真格星球 ZhenPlanet」是真格主办的前沿科技人才孵化项目，自 2019 年起已成功举办四期。</span></p><div><div class=" pTag"><span style="font-size: 17px;">我们已陪伴多家优秀的前沿科技创业公司走过从 0 到 1 的旅程，因此更期待着帮助更多科技领域创业者们启航。真格星球是一个梦想实践地：在这里的三天两夜，你将结识创业路上的优秀伙伴，一起与导师们畅谈关于 AI 创业的一切。从创业想法到组建团队、从产品设计到商业落地，让创业灵感在交流中迸发。</span></div><div class=" pTag"><span style="font-size: 17px;">无论你已然启程，还是怀抱创业想法尚未行动，我们都在真格等你到来。我们希望你结识同行者，收获改变世界的可能性。</span></div></div></div></div></blockquote><div class=" pTag sectionReplaced"><span style="font-size: 17px;">目之所及的当下，各行各业正经历着一场由 AI 驱动的转型。AI 生成视频工具逐步替代传统广告和影视制作中的部分工作，为创意产业带来前所未有的效率与创新；LLM 技术成为研究人员不可或缺的助手，加速知识传播和学术探索；AI 辅助诊断技术正帮助医生做出更准确的判断，大大提升医疗服务的质量。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">虽然 AI 已经取得了一些令人瞩目的成就，但我们仍然面临着许多挑战和问题。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">例如：3D 资产生成技术将何时正式商用，为设计、游戏和电影制作等行业带来变革？Scaling Law 在 Manipulation 方面的应用何时能够得到验证，以更有效地扩展 AI 的能力？AI 对于开放世界游戏的赋能究竟有多大，将如何改变我们的游戏方式和体验？</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">正因如此，在这场技术与商业交织的革命浪潮中，<span><strong style="font-weight: 600;">真格星球 2024 即将启程 —— 诚邀每一位对 AI 充满热情的先锋者加入，与行业领袖、技术专家和创新者一起，共同探索 AI 的无限可能</strong></span>。</span></div><div class=" pTag sectionReplaced" style="text-align: center;"><strong style="font-weight: 600;"><span style="font-size: 17px;">什么是「真格星球」</span></strong></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">「真格星球 ZhenPlanet」是由早期投资机构真格基金主办的<span><strong style="font-weight: 600;">前沿科技人才孵化项目</strong></span>。旨在寻找<strong style="font-weight: 600;"><span>兼具创新意识和商业能力、有创业激情的科技人才</span></strong>，通过资本支持、资源对接、孵化指导，帮助他们<strong style="font-weight: 600;"><span>将科技成果转化为商业产品</span></strong>，完成「从 0 到 1」的跨越，实现科技创新梦想。</span></div><div class=" pTag sectionReplaced" style="text-align: center;"><strong style="font-weight: 600;"><span style="font-size: 17px;">「星球居民」都有谁</span></strong></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">2019 年至今，「真格星球 ZhenPlanet」已成功举行四期。这是一个由前沿科技人、优秀创业者、行业专家、意见领袖等构成的<strong style="font-weight: 600;"><span>生态网络</span></strong>。星球居民或来自学术积淀深厚、科研水平一流的国内外高校，或出自技术领先的企业，在各自的领域经历了不同程度的历练。</span></div><div class=" pTag sectionReplaced"><strong style="font-weight: 600;"><span style="font-size: 17px;">「真格星球 ZhenPlanet・AI+ 创业营</span></strong><strong style="font-weight: 600;"><span style="font-size: 17px;">」第五期，即将于 8 月 2 日启航</span></strong><span style="font-size: 17px;">，敬请把握此次飞行机遇，以免错失精彩旅程！</span></div><div class=" pTag sectionReplaced" style="text-align: center;"><strong style="font-weight: 600;"><span style="font-size: 17px;">顶级创业导师阵容</span></strong></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">我们相信科技创造未来，已然启航的前辈可以帮助最具潜力的人才快速实现从零到一的突破。本次真格邀请到多位行业专家、科技创业先锋、明星投资人组成真格星球导师团，他们将对 <span><strong style="font-weight: 600;">AI 领域的最新进展、转化应用、行业趋势、产品研发、商业落地及团队管理</strong></span>等内容进行经验授课与心得分享。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">*以下排名不分先后</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibhVv7TGHZypZRpiavBHtqltYRufkAMG1hial3IxJrxK8Ll7mPiceGpf3RRvPtIicOxcfF0xG53174V1w/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibhVv7TGHZypZRpiavBHtqlt8dHMic6OW5W1r5LEJVsJeHMibdasm2MFUc7CUichbQcBIh3uSH5QIrkVA/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibhVv7TGHZypZRpiavBHtqltEyuNH0pD4hFTqT9BZAR3sdjIdq0FQicnkMKbYMJubYShkV5wRbZ0f6g/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibhVv7TGHZypZRpiavBHtqltemcQLUUGvY5MqsBsVNicNLC71QFFxicbsdXBlWn5mGUJdjsicck3kblqw/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibhVv7TGHZypZRpiavBHtqlt9BAEbvutzCz7tnpnHoJpLa0Aicribqg8v45ibGkjf20NAqzlLszpibjqvA/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibhVv7TGHZypZRpiavBHtqltnibmC9sRRJ9ibueDAJ0icU5bCrN8l1gAy3hibwB1EjqF65XPC7ukqtDiaVg/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibhVv7TGHZypZRpiavBHtqltf8k0eiapBgtHR7atu8SU0ctkIHDNN6ZlQAOILBOqW44Ofv6JWySXHqg/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibhVv7TGHZypZRpiavBHtqltLImHelGGL7Miaiatz0H4kEGrbjOKceKO2jvicPf7XFBg8GEtL9TLqZic5Q/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibhVv7TGHZypZRpiavBHtqltA1UXWkWnyBBxkcn9icib0SKcwYVNHh2TwhAmCjyIkRicpRxNTiaEy7wOJQ/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibhVv7TGHZypZRpiavBHtqlt7LvQmUEXMqf1RbLnTf2zZwY3UTTADuh1ZPJwtxTaqO1AGVcPIBkicicg/640?wx_fmt=png&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced" style="text-align: center;"><span><strong style="font-weight: 600;"><span style="font-size: 17px;">现场将有更多神秘嘉宾莅临</span></strong></span></div><div class=" pTag sectionReplaced" style="text-align: center;"><span><strong style="font-weight: 600;"><span style="font-size: 17px;">助力星球梦想之旅</span></strong></span></div><div class=" pTag sectionReplaced" style="text-align: center;"><strong style="font-weight: 600;"><span style="font-size: 17px;">航程信息</span></strong></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">真格星球是一个充满活力和机遇的学习平台，为星球居民提供了一个全方位的成长环境，旨在<span><strong style="font-weight: 600;">激发创新思维、增强实战能力，并构建有价值的联系</strong></span>。在真格星球，你将这样度过 ——</span></div><ul class="list-paddingleft-1"><li><p><span><strong style="font-weight: 600;"><span style="font-size: 17px;">Icebreaker</span></strong></span><span style="font-size: 17px;">：星球航程启动点，在轻松愉快的氛围中打开心扉，与你的新朋友迅速拉近距离，建立起联系和信任</span></p></li><li><p><strong style="font-weight: 600;"><span style="font-size: 17px;">ZhenSeminar</span></strong><span style="font-size: 17px;">：资深行业导师分享创业实战经验和商业运作心得，你将<strong style="font-weight: 600;"><span>有机会直接向行业领袖学习</span></strong>，获取第一手的商业智慧</span></p></li><li><p><strong style="font-weight: 600;"><span style="font-size: 17px;">ZhenWorkshop</span></strong><span style="font-size: 17px;">：与伙伴团结协作，通过<strong style="font-weight: 600;"><span>小组讨论、案例分析</span></strong>等方式，掀起星球最强<span><strong style="font-weight: 600;">头脑风暴</strong></span>，共同解决问题和挑战</span></p></li><li><p><span><strong style="font-weight: 600;"><span style="font-size: 17px;">ZhenFun</span></strong></span><span style="font-size: 17px;">：穿插其中的丰富活动，放松身心再出发，保持创造力和动力</span></p></li><li><p><span><strong style="font-weight: 600;"><span style="font-size: 17px;">ZhenReflection</span></strong></span><span style="font-size: 17px;">：回顾和思考所学、所感、所获，<span><strong style="font-weight: 600;">分享心得体会，聆听他人见解</strong></span>，与伙伴畅所欲言，获得更全面的成长</span></p></li><li><p><span><strong style="font-weight: 600;"><span style="font-size: 17px;">DemoDay</span></strong></span><span style="font-size: 17px;">：展示你的创业想法和项目，<span><strong style="font-weight: 600;">有机会获得天使投资</strong></span></span></p></li></ul><div class=" pTag sectionReplaced"><span style="font-size: 17px;">这里不仅是一系列课程和活动的集合，更是一个全面支持创业梦想的生态系统。我们致力于打造一个</span><span style="font-size: 17px;"><strong style="font-weight: 600;">沉浸式的学习环境</strong></span><span style="font-size: 17px;">，让学员们在深度互动与实践中，相互启发、共同进步。希望每一位星球居民都能在创业的星辰大海中，发现属于自己的一片宇宙。</span></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibhVv7TGHZypZRpiavBHtqltC8eicSr1vwgjVnSp4R4PvCh3u2YqjoibJ254QHoE5mTiaN2UqrYdYAARA/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced" style="text-align: center;"><strong style="font-weight: 600;"><span style="font-size: 17px;">报名指南</span></strong></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">➢ 活动时间：2024 年 8 月 2 日 - 8 月 4 日</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">➢ 地点：北京</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">➢ 规模：25 人</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">➢ 费用：免费（营期食宿由主办方统一安排）</span></div><div class=" pTag sectionReplaced" style="text-align: center;"><strong style="font-weight: 600;"><span style="font-size: 17px;">报名通道</span></strong></div><div class=" pTag" style="text-align: center;"><div class=" ce-ao-image-holder pTag"><div class=" ce-ao-image-holder-inner"><img class=" ce-ao-image" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibhVv7TGHZypZRpiavBHtqlt2WibXhtcD9iar89y8ewEicUkiaBBZML84ic9P1oM4xosDfP5Y6BhqjBZx2w/640?wx_fmt=jpeg&amp;from=appmsg" /></div></div></div><div class=" pTag sectionReplaced" style="text-align: center;"><strong style="font-weight: 600;"><span style="font-size: 17px;">扫描上方二维码</span></strong></div><div class=" pTag sectionReplaced" style="text-align: center;"><strong style="font-weight: 600;"><span style="font-size: 17px;">或点击「阅读原文」立即报</span></strong><strong style="font-weight: 600;"><span style="font-size: 17px;">名！</span></strong></div><div class=" pTag sectionReplaced" style="text-align: center;"><strong style="font-weight: 600;"><span style="font-size: 17px;">FAQ</span></strong></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">Q：报名多久后获得通知？</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">A: 「真格星球」将滚动录取，越早报名越将有机会通过筛选，我们将在报名后尽快与您联络。若被录取，您<strong style="font-weight: 600;"><span style="font-size: 17px;">最晚将于 7 月 27 日 24：00 前收到我们的邮件通知</span></strong>。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">Q: 导师阵容如此豪华，内容设置这般用心，参加活动需要多少费用？</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">A: 完全免费！「真格星球」只为支持科技人才的创业梦想，不收取任何费用。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">Q：暂时没有明确的创业方向，可以报名吗？</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">A：完全可以！只要你有科技创业梦想，真格星球助你走出创业第一步。</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">Q: 报名后是否有筛选流程？</span></div><div class=" pTag sectionReplaced"><span style="font-size: 17px;">A: 报名后我们将进行筛选和面试，所以报名信息越详细将越有机会通过筛选获得面试资格。</span></div><div class=" pTag sectionReplaced"><span><strong style="font-weight: 600;"><span style="font-size: 17px;">加入我们，携手开启 AI 赋能的新时代，共同塑造一个更加智能、高效和创新的未来</span></strong></span><span style="font-size: 17px;">。</span></div><div class=" pTag sectionReplaced"><strong style="font-weight: 600;"><span style="font-size: 17px;">如有其他疑问，可添加真格小助手微信咨询 （ID：Zhencraft）</span></strong></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">©&nbsp;THE END&nbsp;</span></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">转载请联系本公众号获得授权</span></div><div class=" pTag" style="text-align: center;"><span style="font-size: 17px;">投稿或寻求报道：<a class="__cf_email__" href="https://posts.careerengine.us/cdn-cgi/l/email-protection">[email&nbsp;protected]</a></span></div>  <div class="read-more-button"><div class="cce-btn cce-btn-light-grey" id="readMore">继续阅读</div></div>  <a class="post-original-link" href="https://careerengine.us/redirect/to?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F5AE1d9lRovtyOVg4duh2cA">阅读原文 </a>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>