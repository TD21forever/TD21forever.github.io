<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>Quinn Leng / @quinn_leng</title>
        <link>https://nitter.cz/quinn_leng</link>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1735881767482241348#m</id>
            <title>虽然学界已经证明用 GPT 的输出内容来训练模型可以达到不错的效果，但是作为一家商业公司这么做就有点作弊了，而且长期来看也会对研究方向产生误导</title>
            <link>https://nitter.cz/quinn_leng/status/1735881767482241348#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1735881767482241348#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 16 Dec 2023 04:37:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>虽然学界已经证明用 GPT 的输出内容来训练模型可以达到不错的效果，但是作为一家商业公司这么做就有点作弊了，而且长期来看也会对研究方向产生误导</p>
<p><a href="https://nitter.cz/dotey/status/1735878580943335679#m">nitter.cz/dotey/status/1735878580943335679#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1734301666344862087#m</id>
            <title>Mistral 在第二个开源模型 8x7B 上选择了混合专家（MoE）架构可以看出来这家公司的野心是很大的，MoE 的优势在于用户量多的时候可以充分利用到不同专家模块，显著降低成本。一同发布的还有AI 平台，包含了 embedding 和 LLM，其中 small 对标 gpt-3.5，medium 超过 3.5 ，下一步就是 gpt-4 了</title>
            <link>https://nitter.cz/quinn_leng/status/1734301666344862087#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1734301666344862087#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 19:58:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mistral 在第二个开源模型 8x7B 上选择了混合专家（MoE）架构可以看出来这家公司的野心是很大的，MoE 的优势在于用户量多的时候可以充分利用到不同专家模块，显著降低成本。一同发布的还有AI 平台，包含了 embedding 和 LLM，其中 small 对标 gpt-3.5，medium 超过 3.5 ，下一步就是 gpt-4 了</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JGNS1hLWFvQUVwNkFoLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JGNkZvT2E0QUFRd2Z2LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1734297913663332828#m</id>
            <title>有网友说 Mistral 8x7B 有 56B 参数（实际 46.7B），才勉强超过 Llama-2-70b 的性能，这个对比是有问题的。混合专家模型训练和推理中不需要同时激活所有模型参数，就像一个中转站可以在八个下游中选择，占用了空间，但是实际处理效率更高。Mistral 的实际推理速度接近 12.9B 模型，可以说非常强了。</title>
            <link>https://nitter.cz/quinn_leng/status/1734297913663332828#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1734297913663332828#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 19:43:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>有网友说 Mistral 8x7B 有 56B 参数（实际 46.7B），才勉强超过 Llama-2-70b 的性能，这个对比是有问题的。混合专家模型训练和推理中不需要同时激活所有模型参数，就像一个中转站可以在八个下游中选择，占用了空间，但是实际处理效率更高。Mistral 的实际推理速度接近 12.9B 模型，可以说非常强了。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JGMkZhUWFZQUVpZVBkLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JGMlAyOWFvQUFhNG5ULmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1734243175408263394#m</id>
            <title>Mistral 8x7B 在多数测试数据集上的效果超过 Llama 2 70B 和 GPT-3.5 ，并且因为是混合专家模型，实际推理速度等同一个 12B 的模型，可以预见所有模型平台都会迅速支持这款最新最强的开源模型</title>
            <link>https://nitter.cz/quinn_leng/status/1734243175408263394#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1734243175408263394#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 16:06:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mistral 8x7B 在多数测试数据集上的效果超过 Llama 2 70B 和 GPT-3.5 ，并且因为是混合专家模型，实际推理速度等同一个 12B 的模型，可以预见所有模型平台都会迅速支持这款最新最强的开源模型</p>
<p><a href="https://nitter.cz/GuillaumeLample/status/1734216541099507929#m">nitter.cz/GuillaumeLample/status/1734216541099507929#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1733196609976963115#m</id>
            <title>当很多公司想尽一切办法做好看的官网，demo 视频，博客的时候，Mistral AI，曾经发布最强 7B 模型的公司，刚刚随性地发了一个 bittorrent 链接，里面是他们最新的 8x7b MOE 混合专家模型。社区还在评估模型的效果，应该会是个很强的模型。越来越觉得 Mistral 这家公司很有意思。</title>
            <link>https://nitter.cz/quinn_leng/status/1733196609976963115#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1733196609976963115#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 18:47:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>当很多公司想尽一切办法做好看的官网，demo 视频，博客的时候，Mistral AI，曾经发布最强 7B 模型的公司，刚刚随性地发了一个 bittorrent 链接，里面是他们最新的 8x7b MOE 混合专家模型。社区还在评估模型的效果，应该会是个很强的模型。越来越觉得 Mistral 这家公司很有意思。</p>
<p><a href="https://nitter.cz/MistralAI/status/1733150512395038967#m">nitter.cz/MistralAI/status/1733150512395038967#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/DrJimFan/status/1732473280148500786#m</id>
            <title>RT by @quinn_leng: AlphaCode-2 is also announced today, but seems to be buried in news. It's a competitive coding model finetuned from Gemini. In the technical report, DeepMind shares a surprising amount of details on an inference-time search, filtering, and re-ranking system. This may be Google's Q*? 🤔

They also discussed the finetuning procedure, which is 2 rounds of GOLD (an offline RL algorithm for LLM from 2020), and the training dataset. AlphaCode-2 scores at 87% percentile among the human competitors. 

Don't miss it: https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf</title>
            <link>https://nitter.cz/DrJimFan/status/1732473280148500786#m</link>
            <guid isPermaLink="false">https://nitter.cz/DrJimFan/status/1732473280148500786#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 18:53:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AlphaCode-2 is also announced today, but seems to be buried in news. It's a competitive coding model finetuned from Gemini. In the technical report, DeepMind shares a surprising amount of details on an inference-time search, filtering, and re-ranking system. This may be Google's Q*? 🤔<br />
<br />
They also discussed the finetuning procedure, which is 2 rounds of GOLD (an offline RL algorithm for LLM from 2020), and the training dataset. AlphaCode-2 scores at 87% percentile among the human competitors. <br />
<br />
Don't miss it: <a href="https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf">storage.googleapis.com/deepm…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FyNXZUSWFRQUFFTU01LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FyNlVCcmJjQUlmdXppLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FyNmFDNWFRQUFuRmVaLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1732463866326773793#m</id>
            <title>演示视频里 Gemini 原生多模态的优势，可以同时理解视频声音图片和文字，与人的互动更加准确自然。想象五到十年后大部分手机都能本地运行这样一个这么听真看真感觉的 AI 助手，值得期待！</title>
            <link>https://nitter.cz/quinn_leng/status/1732463866326773793#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1732463866326773793#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 18:15:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>演示视频里 Gemini 原生多模态的优势，可以同时理解视频声音图片和文字，与人的互动更加准确自然。想象五到十年后大部分手机都能本地运行这样一个这么听真看真感觉的 AI 助手，值得期待！</p>
<p><a href="https://nitter.cz/FinanceYF5/status/1732423841799078116#m">nitter.cz/FinanceYF5/status/1732423841799078116#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1732450371384078522#m</id>
            <title>Google Deepmind 发布 Gemini 1.0 ，在 32 项指标中有 30 项超过 GPT-4。Gemini 从架构到训练过程都是原生的多模态，可以支持图像、视频、语音输入，同时也可以直接输出图像和文字，而 GPT-4 目前还不能直接输出图像。其中 1.8B 和 3.25B 参数的模型性能都超过了 llama-2-7b ，可以运行在移动设备上。</title>
            <link>https://nitter.cz/quinn_leng/status/1732450371384078522#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1732450371384078522#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 17:22:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google Deepmind 发布 Gemini 1.0 ，在 32 项指标中有 30 项超过 GPT-4。Gemini 从架构到训练过程都是原生的多模态，可以支持图像、视频、语音输入，同时也可以直接输出图像和文字，而 GPT-4 目前还不能直接输出图像。其中 1.8B 和 3.25B 参数的模型性能都超过了 llama-2-7b ，可以运行在移动设备上。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FyajlfcVdvQUFFVHFHLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Fya1VaVFc0QUEzdkZ2LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Fya3k4clhFQUFZREV0LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FybVphdFdVQUEwX0h5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1731820918811341151#m</id>
            <title>刚发布的 Difussion model ，输入一张小姐姐参考图和姿势，就能生成逼真的小姐姐跳舞视频。比同类型模型的拟真程度提升了 37%，效果肉眼可见。</title>
            <link>https://nitter.cz/quinn_leng/status/1731820918811341151#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1731820918811341151#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 04 Dec 2023 23:40:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>刚发布的 Difussion model ，输入一张小姐姐参考图和姿势，就能生成逼真的小姐姐跳舞视频。比同类型模型的拟真程度提升了 37%，效果肉眼可见。</p>
<p><a href="https://nitter.cz/_akhaliq/status/1731754853238501522#m">nitter.cz/_akhaliq/status/1731754853238501522#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1730749076554305843#m</id>
            <title>R to @quinn_leng: 对于很多开源 LLM 来说这个漏洞并没有什么影响，但是对于 OpenAI GPT 这种闭源模型，训练数据可能涉及商业机密甚至版权官司，泄漏训练数据就很敏感了。</title>
            <link>https://nitter.cz/quinn_leng/status/1730749076554305843#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1730749076554305843#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 02 Dec 2023 00:41:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>对于很多开源 LLM 来说这个漏洞并没有什么影响，但是对于 OpenAI GPT 这种闭源模型，训练数据可能涉及商业机密甚至版权官司，泄漏训练数据就很敏感了。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1730748799025549397#m</id>
            <title>研究显示通过让 LLM 不断重复一个关键词可以从它嘴中套出训练数据，比如某位研究人员上传到互联网的邮箱和电话，甚至是几千字的文章原文（截图4）。目前这个漏洞已经被 OpenAI 屏蔽，但是当时研究人员破解过程的聊天记录还能被找到：https://chat.openai.com/share/456d092b-fb4e-4979-bea1-76d8d904031f 。论文原文：https://not-just-memorization.github.io/extracting-training-data-from-chatgpt.html?utm_source=aitidbits.substack.com&amp;utm_medium=newsletter</title>
            <link>https://nitter.cz/quinn_leng/status/1730748799025549397#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1730748799025549397#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 02 Dec 2023 00:40:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>研究显示通过让 LLM 不断重复一个关键词可以从它嘴中套出训练数据，比如某位研究人员上传到互联网的邮箱和电话，甚至是几千字的文章原文（截图4）。目前这个漏洞已经被 OpenAI 屏蔽，但是当时研究人员破解过程的聊天记录还能被找到：<a href="https://chat.openai.com/share/456d092b-fb4e-4979-bea1-76d8d904031f">chat.openai.com/share/456d09…</a> 。论文原文：<a href="https://not-just-memorization.github.io/extracting-training-data-from-chatgpt.html?utm_source=aitidbits.substack.com&amp;utm_medium=newsletter">not-just-memorization.github…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FUWS1fcmJVQUFVVUdnLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FUWkI0UWE4QUFKSVluLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FUWnZVbWIwQUFGM0pfLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FUYXBPMWJFQUFXVUUzLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/GregKamradt/status/1727018183608193393#m</id>
            <title>RT by @quinn_leng: Claude 2.1 (200K Tokens) - Pressure Testing Long Context Recall

We all love increasing context lengths - but what's performance like?

Anthropic reached out with early access to Claude 2.1 so I repeated the “needle in a haystack” analysis I did on GPT-4

Here's what I found:

Findings:
* At 200K tokens (nearly 470 pages), Claude 2.1 was able to recall facts at some document depths
* Facts at the very top and very bottom of the document were recalled with nearly 100% accuracy
* Facts positioned at the top of the document were recalled with less performance than the bottom (similar to GPT-4)
* Starting at ~90K tokens, performance of recall at the bottom of the document started to get increasingly worse
* Performance at low context lengths was not guaranteed

So what:
* Prompting Engineering Matters - It’s worth tinkering with your prompt and running A/B tests to measure retrieval accuracy
* No Guarantees - Your facts are not guaranteed to be retrieved. Don’t bake the assumption they will into your applications
* Less context = more accuracy - This is well know, but when possible reduce the amount of context you send to the models to increase its ability to recall
* Position Matters - Also well know, but facts placed at the very beginning and 2nd half of the document seem to be recalled better

Why run this test?:
* I’m a big fan of Anthropic! They are helping to push the bounds on LLM performance and creating powerful tools for the world
* As a practitioner of LLMs, it’s important to build an intuition for how they work, where they excel and their limits
* Tests like these, while not bulletproof, help showcase real world examples and get a feeling for how they work. The goal is to transfer this knowledge to productive use cases

Overview of the process:
* Use Paul Graham essays as ‘background’ tokens. With 218 essays it’s easy to get up to 200K tokens (repeated essays when necessary)
* Place a random statement within the document at various depths. Fact used: “The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.”
* Ask Claude 2.1 to answer this question only using the context provided
* Evaluate Claude 2.1s answer with GPT-4 using @LangChainAI evals
* Rinse and repeat for 35x document depths between 0% (top of document) and 100% (bottom of document) (sigmoid distribution) and 35x context lengths (1K Tokens > 200K Tokens)

Next Steps To Take This Further:
* For rigor, one should do a key:value retrieval step. However for relatability I did a San Francisco line within PGs essays for clarity and practical relevance
* Repeat test multiple times for increased statistical significance

Notes:
* Amount Of Recall Matters - The model's performance is hypothesized to diminish when tasked with multiple fact retrievals or when engaging in synthetic reasoning steps
* Changing your prompt, question, fact to be retrieved and background context will impact performance
* The Anthropic team reached out and offered credits to repeat this test. They also offered prompt advice to maximize performance. It's important to clarify that their involvement was strictly logistical. The integrity and independence of the results were maintained, ensuring that the findings reflect my unbiased evaluation and are not influenced by their support.
* This test cost ~$1,016 for API calls ($8 per million tokens)</title>
            <link>https://nitter.cz/GregKamradt/status/1727018183608193393#m</link>
            <guid isPermaLink="false">https://nitter.cz/GregKamradt/status/1727018183608193393#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 17:36:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Claude 2.1 (200K Tokens) - Pressure Testing Long Context Recall<br />
<br />
We all love increasing context lengths - but what's performance like?<br />
<br />
Anthropic reached out with early access to Claude 2.1 so I repeated the “needle in a haystack” analysis I did on GPT-4<br />
<br />
Here's what I found:<br />
<br />
Findings:<br />
* At 200K tokens (nearly 470 pages), Claude 2.1 was able to recall facts at some document depths<br />
* Facts at the very top and very bottom of the document were recalled with nearly 100% accuracy<br />
* Facts positioned at the top of the document were recalled with less performance than the bottom (similar to GPT-4)<br />
* Starting at ~90K tokens, performance of recall at the bottom of the document started to get increasingly worse<br />
* Performance at low context lengths was not guaranteed<br />
<br />
So what:<br />
* Prompting Engineering Matters - It’s worth tinkering with your prompt and running A/B tests to measure retrieval accuracy<br />
* No Guarantees - Your facts are not guaranteed to be retrieved. Don’t bake the assumption they will into your applications<br />
* Less context = more accuracy - This is well know, but when possible reduce the amount of context you send to the models to increase its ability to recall<br />
* Position Matters - Also well know, but facts placed at the very beginning and 2nd half of the document seem to be recalled better<br />
<br />
Why run this test?:<br />
* I’m a big fan of Anthropic! They are helping to push the bounds on LLM performance and creating powerful tools for the world<br />
* As a practitioner of LLMs, it’s important to build an intuition for how they work, where they excel and their limits<br />
* Tests like these, while not bulletproof, help showcase real world examples and get a feeling for how they work. The goal is to transfer this knowledge to productive use cases<br />
<br />
Overview of the process:<br />
* Use Paul Graham essays as ‘background’ tokens. With 218 essays it’s easy to get up to 200K tokens (repeated essays when necessary)<br />
* Place a random statement within the document at various depths. Fact used: “The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.”<br />
* Ask Claude 2.1 to answer this question only using the context provided<br />
* Evaluate Claude 2.1s answer with GPT-4 using <a href="https://nitter.cz/LangChainAI" title="LangChain">@LangChainAI</a> evals<br />
* Rinse and repeat for 35x document depths between 0% (top of document) and 100% (bottom of document) (sigmoid distribution) and 35x context lengths (1K Tokens > 200K Tokens)<br />
<br />
Next Steps To Take This Further:<br />
* For rigor, one should do a key:value retrieval step. However for relatability I did a San Francisco line within PGs essays for clarity and practical relevance<br />
* Repeat test multiple times for increased statistical significance<br />
<br />
Notes:<br />
* Amount Of Recall Matters - The model's performance is hypothesized to diminish when tasked with multiple fact retrievals or when engaging in synthetic reasoning steps<br />
* Changing your prompt, question, fact to be retrieved and background context will impact performance<br />
* The Anthropic team reached out and offered credits to repeat this test. They also offered prompt advice to maximize performance. It's important to clarify that their involvement was strictly logistical. The integrity and independence of the results were maintained, ensuring that the findings reflect my unbiased evaluation and are not influenced by their support.<br />
* This test cost ~$1,016 for API calls ($8 per million tokens)</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9lWXJESWFBQUFzV1ZwLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/DrJimFan/status/1727023421899620642#m</id>
            <title>RT by @quinn_leng: Instead of taking OAI's merger offer, Anthropic launched major updates for Claude 2.1🎉. I think the below chart is the most interesting: this is how all LLM papers that claim "long context" should report: error rates on "Beginning", "Middle", and "End".

There're a bunch of papers making wild claims, all the way up to "1B context tokens". Here's a friendly reminder that the 30-year-old LSTM literally supports infinite context. It's a meaningless number unless you show detailed evaluations at different locations in the context. LLMs tend to be "Lost in the Middle", i.e. struggle to remember and reason on information at the middle section of the context window: https://arxiv.org/abs/2307.03172

Claude 2.1 also claims "2x hallucination" - please take this with a BIG grain of salt. A while back, I expressed my concerns about Vectara's benchmarking protocol. Same concerns apply here too. 

The trivial solution to achieve 0% hallucination is simply refusing to answer every query. One cannot claim victory here without a careful Safety vs Usefulness analysis. How many questions that Claude used to answer correctly are now rejected?

In any case, kudos to Dario &amp; Anthropic team on assuring us a solid alternative during turmoil! 🩷https://www.anthropic.com/index/claude-2-1</title>
            <link>https://nitter.cz/DrJimFan/status/1727023421899620642#m</link>
            <guid isPermaLink="false">https://nitter.cz/DrJimFan/status/1727023421899620642#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 17:57:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Instead of taking OAI's merger offer, Anthropic launched major updates for Claude 2.1🎉. I think the below chart is the most interesting: this is how all LLM papers that claim "long context" should report: error rates on "Beginning", "Middle", and "End".<br />
<br />
There're a bunch of papers making wild claims, all the way up to "1B context tokens". Here's a friendly reminder that the 30-year-old LSTM literally supports infinite context. It's a meaningless number unless you show detailed evaluations at different locations in the context. LLMs tend to be "Lost in the Middle", i.e. struggle to remember and reason on information at the middle section of the context window: <a href="https://arxiv.org/abs/2307.03172">arxiv.org/abs/2307.03172</a><br />
<br />
Claude 2.1 also claims "2x hallucination" - please take this with a BIG grain of salt. A while back, I expressed my concerns about Vectara's benchmarking protocol. Same concerns apply here too. <br />
<br />
The trivial solution to achieve 0% hallucination is simply refusing to answer every query. One cannot claim victory here without a careful Safety vs Usefulness analysis. How many questions that Claude used to answer correctly are now rejected?<br />
<br />
In any case, kudos to Dario & Anthropic team on assuring us a solid alternative during turmoil! 🩷<a href="https://www.anthropic.com/index/claude-2-1">anthropic.com/index/claude-2…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9lWmZQSmJnQUE2WmVOLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1727013248602640734#m</id>
            <title>Anthropic 果然还是 OpenAI 的最强对手，Claude 2.1 版本增加到了 200k 上下文，同时增加了非常实用的使用工具的能力，以及 system prompt ，同时回答准确度也有了显著提升。模型整体水平跟 OpenAI 接近了一大步</title>
            <link>https://nitter.cz/quinn_leng/status/1727013248602640734#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1727013248602640734#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 17:16:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Anthropic 果然还是 OpenAI 的最强对手，Claude 2.1 版本增加到了 200k 上下文，同时增加了非常实用的使用工具的能力，以及 system prompt ，同时回答准确度也有了显著提升。模型整体水平跟 OpenAI 接近了一大步</p>
<p><a href="https://nitter.cz/AnthropicAI/status/1727001773888659753#m">nitter.cz/AnthropicAI/status/1727001773888659753#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1726764513905848597#m</id>
            <title>另一个我这两天一直在思考的问题是，人类在 AI 面前显得多脆弱，世界上最有前景的公司几个小时之内被三个人搞得七零八落。如果人类真的灭绝了，大概率应该是自己搞砸了，而不是因为 AGI</title>
            <link>https://nitter.cz/quinn_leng/status/1726764513905848597#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1726764513905848597#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 00:48:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>另一个我这两天一直在思考的问题是，人类在 AI 面前显得多脆弱，世界上最有前景的公司几个小时之内被三个人搞得七零八落。如果人类真的灭绝了，大概率应该是自己搞砸了，而不是因为 AGI</p>
<p><a href="https://nitter.cz/oran_ge/status/1726748365109825689#m">nitter.cz/oran_ge/status/1726748365109825689#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1726712806702203168#m</id>
            <title>这次核心成员出走，以及大部分员工联名要求董事会改组，影响有些过于巨大，鉴于微软在其中拥有的决定性资源，后面会向谁倾斜都不好说，我确实挺担心 OpenAI 的受影响程度</title>
            <link>https://nitter.cz/quinn_leng/status/1726712806702203168#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1726712806702203168#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 Nov 2023 21:23:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这次核心成员出走，以及大部分员工联名要求董事会改组，影响有些过于巨大，鉴于微软在其中拥有的决定性资源，后面会向谁倾斜都不好说，我确实挺担心 OpenAI 的受影响程度</p>
<p><a href="https://nitter.cz/onenewbite/status/1726573171678331345#m">nitter.cz/onenewbite/status/1726573171678331345#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1726712045884826108#m</id>
            <title>OpenAI 发家于非盈利学术机构，诞生以来就一直有这个巨大的内在冲突：学术上对安全行，稳定性，平等和造福全人类的追求，以及作为一家成长最快的在巨头之间斡旋的创业公司需要最快速度发布产品，获得最大利润，锁定下一阶段的史诗级巨量资源。这两者非常容易产生剧烈矛盾。这次的结局确实有些让人唏嘘</title>
            <link>https://nitter.cz/quinn_leng/status/1726712045884826108#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1726712045884826108#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 Nov 2023 21:20:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI 发家于非盈利学术机构，诞生以来就一直有这个巨大的内在冲突：学术上对安全行，稳定性，平等和造福全人类的追求，以及作为一家成长最快的在巨头之间斡旋的创业公司需要最快速度发布产品，获得最大利润，锁定下一阶段的史诗级巨量资源。这两者非常容易产生剧烈矛盾。这次的结局确实有些让人唏嘘</p>
<p><a href="https://nitter.cz/onenewbite/status/1726570213263503556#m">nitter.cz/onenewbite/status/1726570213263503556#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/burkaygur/status/1725626154231492684#m</id>
            <title>RT by @quinn_leng: Live demo of Real-Time Image Generation powered by >@fal_ai_data  on @huggingface Spaces

Link below</title>
            <link>https://nitter.cz/burkaygur/status/1725626154231492684#m</link>
            <guid isPermaLink="false">https://nitter.cz/burkaygur/status/1725626154231492684#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Nov 2023 21:25:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Live demo of Real-Time Image Generation powered by <a href="https://nitter.cz/fal_ai_data" title="fal (Features &amp; Labels)">@fal_ai_data</a>  on <a href="https://nitter.cz/huggingface" title="Hugging Face">@huggingface</a> Spaces<br />
<br />
Link below</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjU2MjU2MjAzMDQ4ODc4MDgvcHUvaW1nLzZGTTV6VWxiaTF6OXhBaXcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1725621405327454627#m</id>
            <title>Sam Altman 离开 OpenAI ，出这种新闻，看样子公司 board 出大事了啊</title>
            <link>https://nitter.cz/quinn_leng/status/1725621405327454627#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1725621405327454627#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Nov 2023 21:06:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Sam Altman 离开 OpenAI ，出这种新闻，看样子公司 board 出大事了啊</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9LamhOMmJzQUFmaU9fLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1724260450463093120#m</id>
            <title>GPTs 一出来就有一堆想做导航站的，多如牛毛，作为一个 GPT 开发者一时不知道去哪个导航上发布 GPT。是时候做一个 GPT 导航站的导航站了</title>
            <link>https://nitter.cz/quinn_leng/status/1724260450463093120#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1724260450463093120#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 Nov 2023 02:58:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>GPTs 一出来就有一堆想做导航站的，多如牛毛，作为一个 GPT 开发者一时不知道去哪个导航上发布 GPT。是时候做一个 GPT 导航站的导航站了</p>
<p><a href="https://nitter.cz/FinanceYF5/status/1724013556285456826#m">nitter.cz/FinanceYF5/status/1724013556285456826#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>