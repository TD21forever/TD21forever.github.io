<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
  <channel>
    <title>Machine Learning</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16qzt8j</id>
      <title>[D] Simple Questions Thread</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16qzt8j/d_simple_questions_thread/" />
      <description></description>
      <pubDate>2023-09-24T15:00:32+00:00</pubDate>
      <updated>2023-09-24T15:00:32+00:00</updated>
      <content:encoded><![CDATA[
        <!-- SC_OFF --><div class="md"><p>Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!</p> <p>Thread will stay alive until next one so keep posting after the date in the title.</p> <p>Thanks to everyone for answering questions in the previous thread!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/AutoModerator"> /u/AutoModerator </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/16qzt8j/d_simple_questions_thread/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16qzt8j/d_simple_questions_thread/">[comments]</a></span>
      ]]></content:encoded>
    </item>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16x2o47</id>
      <title>[R] Meta, INRIA researchers discover that explicit registers eliminate ViT attention spikes</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16x2o47/r_meta_inria_researchers_discover_that_explicit/" />
      <description></description>
      <pubDate>2023-10-01T14:28:22+00:00</pubDate>
      <updated>2023-10-01T14:28:22+00:00</updated>
      <content:encoded><![CDATA[
        <!-- SC_OFF --><div class="md"><p>When visualizing the inner workings of vision transformers (ViTs), researchers noticed weird spikes of attention on random background patches. This didn't make sense since the models should focus on foreground objects.</p> <p>By analyzing the output embeddings, they found a small number of tokens (2%) had super high vector norms, causing the spikes.</p> <p>The high-norm &quot;outlier&quot; tokens occurred in redundant areas and held less local info but more global info about the image.</p> <p>Their hypothesis is that ViTs learn to identify unimportant patches and recycle them as temporary storage instead of discarding. This enables efficient processing but causes issues.</p> <p>Their fix is simple - just add dedicated &quot;register&quot; tokens that provide storage space, avoiding the recycling side effects.</p> <p>Models trained with registers have:</p> <ul> <li>Smoother and more meaningful attention maps</li> <li>Small boosts in downstream performance</li> <li>Way better object discovery abilities</li> </ul> <p>The registers give ViTs a place to do their temporary computations without messing stuff up. Just a tiny architecture tweak improves interpretability and performance. Sweet!</p> <p>I think it's cool how they reverse-engineered this model artifact and fixed it with such a small change. More work like this will keep incrementally improving ViTs.</p> <p>TLDR: Vision transformers recycle useless patches to store data, causing problems. Adding dedicated register tokens for storage fixes it nicely.</p> <p><a href="https://notes.aimodels.fyi/demystifying-the-artifacts-in-vision-transformer-models/"><strong>Full summary</strong></a><strong>.</strong> Paper is <a href="https://arxiv.org/pdf/2309.16588.pdf">here</a>.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Successful-Western27"> /u/Successful-Western27 </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/16x2o47/r_meta_inria_researchers_discover_that_explicit/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16x2o47/r_meta_inria_researchers_discover_that_explicit/">[comments]</a></span>
      ]]></content:encoded>
    </item>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16x91b9</id>
      <title>[P] Deep Memory, a Way to Boost Retrieval Accuracy by up to +22% for RAG</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16x91b9/p_deep_memory_a_way_to_boost_retrieval_accuracy/" />
      <description></description>
      <pubDate>2023-10-01T18:40:30+00:00</pubDate>
      <updated>2023-10-01T18:40:30+00:00</updated>
      <content:encoded><![CDATA[
        <table> <tr><td> <a href="https://www.reddit.com/r/MachineLearning/comments/16x91b9/p_deep_memory_a_way_to_boost_retrieval_accuracy/"> <img alt="[P] Deep Memory, a Way to Boost Retrieval Accuracy by up to +22% for RAG" src="https://external-preview.redd.it/T2BD0a_UBYbZEHEwqHb97JUcrW2-HGc-bv2gstniHFE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0e21cce057f01108f31b6ec0cd2c34dfdc3908e8" title="[P] Deep Memory, a Way to Boost Retrieval Accuracy by up to +22% for RAG" /> </a> </td><td> &#32; submitted by &#32; <a href="https://www.reddit.com/user/davidbun"> /u/davidbun </a> <br /> <span><a href="https://v.redd.it/ahtifznnymrb1">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16x91b9/p_deep_memory_a_way_to_boost_retrieval_accuracy/">[comments]</a></span> </td></tr></table>
      ]]></content:encoded>
    </item>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16xbess</id>
      <title>[D] How many instructions can LLMs handle before they start to ignore them?</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16xbess/d_how_many_instructions_can_llms_handle_before/" />
      <description></description>
      <pubDate>2023-10-01T20:10:40+00:00</pubDate>
      <updated>2023-10-01T20:10:40+00:00</updated>
      <content:encoded><![CDATA[
        <table> <tr><td> <a href="https://www.reddit.com/r/MachineLearning/comments/16xbess/d_how_many_instructions_can_llms_handle_before/"> <img alt="[D] How many instructions can LLMs handle before they start to ignore them?" src="https://external-preview.redd.it/LyY7oiq_hp9ov-1CRR4bLp8aOQlLCKq_0SD6SDIGBck.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1268dbb41a97ca245be627e043cfff75235ddb02" title="[D] How many instructions can LLMs handle before they start to ignore them?" /> </a> </td><td> <!-- SC_OFF --><div class="md"><p>Prompt engineering frequently involves trying to encode very specific behaviors into a model to steer it a certain direction. In practice, as requirements become more complex, you often end up with fairly lengthy prompts, especially when using methods like RAG. I was wondering, how effective are LLMs at following instructions as the system prompt grows in size and complexity?</p> <p>I did some quick experiments on this and found that, unsurprisingly, GPT-4 can follow a lot of rules (up to 50) quite accurately. But even GPT-3.5 slowly degrades and Llama-2-70b-chat starts to fail after just a few rules.</p> <p><a href="https://preview.redd.it/v4c4m2qfcnrb1.png?width=1789&amp;format=png&amp;auto=webp&amp;s=538a65fd6f3248f69fc71861222dfac62d4ad3b8">Comparison of performance metrics over increasing rule counts, demonstrating GPT-4's consistent performance and a decline in accuracy for GPT-3.5 and Llama-2-70b-chat.</a></p> <p>These results are based on rules that were synthetically generated using GPT-4 of the form “Do not…”.</p> <p><strong>Example rules:</strong></p> <pre><code>1. Do not accept inputs specifically about Microsoft Windows or Apple macOS. 2. Do not process inputs containing more than three instances of the same punctuation mark consecutively. 3. Do not process queries about any board games like Chess or Monopoly. </code></pre> <p><strong>Example prompt:</strong></p> <pre><code>messages = [ { &quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;&quot;&quot;You are a helpful assistant. You **must** follow these rules: {rules} If the input violates any of the above rules, your response must be exactly 'BAD'. Otherwise, respond normally.&quot;&quot;&quot; }, { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;{user_input}&quot; } ] response = openai.ChatCompletion.create( model=model, messages=messages, max_temperature=0, max_tokens=1, ) reject_input = response.choices[0].message[&quot;content&quot;] == &quot;BAD&quot; </code></pre> <p>With each rule, we use GPT-4 again to generate “reject examples” of inputs that violate the rule and should be rejected by an assistant that’s correctly following that rule. The question is, if we sample different rule sets and include them in the system prompt, and then sample reject examples belonging to the sampled rules, how accurately does the assistant reject those examples as the number of rules increases? Across different rule counts and trials, we measure the precision, recall, and F1 score where correctly rejecting an input is considered a true positive.</p> <p>The results demonstrate that when using a model that's not GPT-4, it may be advisable to limit the number of instructions provided in the prompt due to the observed decrease in reliability. There are still open questions like: does the location of the rule within the prompt matter, how much does the difficulty of the rules affect performance, can we extend this to more abstract instructions rather than simple “do not” rules, and does the role of the message used for the rules matter (i.e., are system messages better than user messages in terms of steerability)? If there is any existing research on LLM benchmarking that specifically addresses these areas, I would love to take a look.</p> <p><a href="https://github.com/wiskojo/overwhelm-llm-eval">Code and data used for the experiment</a></p> <p><a href="https://github.com/wiskojo/overwhelm-llm-eval/blob/main/results.ipynb">Notebook with results</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/ProbablyApproxWrong"> /u/ProbablyApproxWrong </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/16xbess/d_how_many_instructions_can_llms_handle_before/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16xbess/d_how_many_instructions_can_llms_handle_before/">[comments]</a></span> </td></tr></table>
      ]]></content:encoded>
    </item>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16xg8nh</id>
      <title>[R] The unsolved mystery at the heard of the "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions" paper</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16xg8nh/r_the_unsolved_mystery_at_the_heard_of_the_how_to/" />
      <description></description>
      <pubDate>2023-10-01T23:14:37+00:00</pubDate>
      <updated>2023-10-01T23:14:37+00:00</updated>
      <content:encoded><![CDATA[
        <table> <tr><td> <a href="https://www.reddit.com/r/MachineLearning/comments/16xg8nh/r_the_unsolved_mystery_at_the_heard_of_the_how_to/"> <img alt="[R] The unsolved mystery at the heard of the &quot;How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions&quot; paper" src="https://external-preview.redd.it/izh8gZHY4FqZ1nwtU1N_TjtohUCNuvTyMn90toXda80.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ce8b9192ed7ca476d2844aaa405c5014a7a1ab45" title="[R] The unsolved mystery at the heard of the &quot;How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions&quot; paper" /> </a> </td><td> &#32; submitted by &#32; <a href="https://www.reddit.com/user/CellWithoutCulture"> /u/CellWithoutCulture </a> <br /> <span><a href="https://arxiv.org/abs/2309.15840">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16xg8nh/r_the_unsolved_mystery_at_the_heard_of_the_how_to/">[comments]</a></span> </td></tr></table>
      ]]></content:encoded>
    </item>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16xm3mt</id>
      <title>[P] NanoPhi, Implementing some of the success of Phi-1.5, with GPT-2(124m)</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16xm3mt/p_nanophi_implementing_some_of_the_success_of/" />
      <description></description>
      <pubDate>2023-10-02T03:35:26+00:00</pubDate>
      <updated>2023-10-02T03:35:26+00:00</updated>
      <content:encoded><![CDATA[
        <!-- SC_OFF --><div class="md"><p>Hi, i'm trying to replicate at least some of the success of Phi 1.5 on a model 10x smaller, gpt-2 124m.</p> <p>I have started with model finetuning, and have a simple github with roadmap, <a href="https://github.com/VatsaDev/NanoPhi">https://github.com/VatsaDev/NanoPhi</a>, check it out there!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/vatsadev"> /u/vatsadev </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/16xm3mt/p_nanophi_implementing_some_of_the_success_of/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16xm3mt/p_nanophi_implementing_some_of_the_success_of/">[comments]</a></span>
      ]]></content:encoded>
    </item>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16xakha</id>
      <title>[R] LangDiversity: software to identify LLM errors</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16xakha/r_langdiversity_software_to_identify_llm_errors/" />
      <description></description>
      <pubDate>2023-10-01T19:37:59+00:00</pubDate>
      <updated>2023-10-01T19:37:59+00:00</updated>
      <content:encoded><![CDATA[
        <table> <tr><td> <a href="https://www.reddit.com/r/MachineLearning/comments/16xakha/r_langdiversity_software_to_identify_llm_errors/"> <img alt="[R] LangDiversity: software to identify LLM errors" src="https://a.thumbs.redditmedia.com/-Excksj2C0pkXYfW3rR2PFCnzfd_uAgrlAVlxBwEXn8.jpg" title="[R] LangDiversity: software to identify LLM errors" /> </a> </td><td> <!-- SC_OFF --><div class="md"><p>Due to challenges such as hallucination, detecting errors in the output of a given prompt becomes an important challenge. LangDiversity is an implementation of &quot;diversity measures&quot; that are domain independent and can be used to measure the uncertainty in the result of a language model.</p> <p>Type pip install langdiversity</p> <p>Video: <a href="https://www.youtube.com/watch?v=86J_K9mR7lw">https://www.youtube.com/watch?v=86J_K9mR7lw</a></p> <p>Web: <a href="https://neurosymbolic.asu.edu/llm-correction/">https://neurosymbolic.asu.edu/llm-correction/</a></p> <p>Visit <a href="https://github.com/lab-v2/langdiversity">https://github.com/lab-v2/langdiversity</a></p> <p>Read the paper: <a href="https://arxiv.org/abs/2308.11189">https://arxiv.org/abs/2308.11189</a></p> <p><a href="https://preview.redd.it/rb0xg1ly8nrb1.png?width=1021&amp;format=png&amp;auto=webp&amp;s=8e57056d24327ca2987abea12a7a9066a825738b">https://preview.redd.it/rb0xg1ly8nrb1.png?width=1021&amp;format=png&amp;auto=webp&amp;s=8e57056d24327ca2987abea12a7a9066a825738b</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Neurosymbolic"> /u/Neurosymbolic </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/16xakha/r_langdiversity_software_to_identify_llm_errors/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16xakha/r_langdiversity_software_to_identify_llm_errors/">[comments]</a></span> </td></tr></table>
      ]]></content:encoded>
    </item>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16x63ce</id>
      <title>[D] Perplexity.ai Search Feasibility</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16x63ce/d_perplexityai_search_feasibility/" />
      <description></description>
      <pubDate>2023-10-01T16:47:35+00:00</pubDate>
      <updated>2023-10-01T16:47:35+00:00</updated>
      <content:encoded><![CDATA[
        <!-- SC_OFF --><div class="md"><p>I've been using <a href="https://perplexity.ai/">Perplexity.ai</a> for a bit now when it hit me that I don't understand how they can sustain their business model with search. Stuff like Bing search and Google search cost around $5 or more per 1000 searches, so how can they even afford to do this kind of search. Do they have their own search index.</p> <p>Also, I don't know how they pull in the data from these sources so fast? I've played around with some things like this with Langchain with retrieval, but the speed of splitting and tokenizing website html is not very fast. Have they already pre-scrapped the websites from the search results and tokenized them for LLM retrieval?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/dragon18456"> /u/dragon18456 </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/16x63ce/d_perplexityai_search_feasibility/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16x63ce/d_perplexityai_search_feasibility/">[comments]</a></span>
      ]]></content:encoded>
    </item>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16x1fzr</id>
      <title>[R] SOTA of Deep-Shallow Encoder-Decoder LLMs for fast inference</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16x1fzr/r_sota_of_deepshallow_encoderdecoder_llms_for/" />
      <description></description>
      <pubDate>2023-10-01T13:37:56+00:00</pubDate>
      <updated>2023-10-01T13:37:56+00:00</updated>
      <content:encoded><![CDATA[
        <!-- SC_OFF --><div class="md"><p>There's some evidence [1] [2] that it's possible to run text2text language model at substantially (potentially on the order of magnitude) better inference speed by keeping the decoder shallow.</p> <p>I'm curious whether some <em>general reasoner</em> SOTA (small model for machine translation available at [3]) style models are publicly available for this sort of thing.</p> <p>If not, how would one go about training one?</p> <p>Would it be necessary to do it entirely from scratch (extremely costly)? Or would it be possible to take, say, Flan-UL2 (20B), chop off its decoder, and train a much smaller decoder on top of it with the UL2 encoder frozen (ie how one trains adapter layers).</p> <p>Assuming the decoder hyperparameters are kept small, would this be possible within reasonable compute budget? Would that even meaningfully converge with small amount of compute (assuming same training objective as is for UL2)?</p> <p>Would the strength (ie somewhat comparable to 10B if we cut 20B in half) transfer from the SOTA encoder, or would cutting off half of the model layers kneecap it too badly?</p> <p>[1] <a href="https://arxiv.org/pdf/2006.10369.pdf">https://arxiv.org/pdf/2006.10369.pdf</a></p> <p>[2] <a href="https://aclanthology.org/2023.sustainlp-1.6.pdf">https://aclanthology.org/2023.sustainlp-1.6.pdf</a></p> <p>[3] <a href="https://github.com/snoop2head/Deep-Encoder-Shallow-Decoder">https://github.com/snoop2head/Deep-Encoder-Shallow-Decoder</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/upalse"> /u/upalse </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/16x1fzr/r_sota_of_deepshallow_encoderdecoder_llms_for/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16x1fzr/r_sota_of_deepshallow_encoderdecoder_llms_for/">[comments]</a></span>
      ]]></content:encoded>
    </item>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16wytmy</id>
      <title>[D] Duplicating layers in large models</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16wytmy/d_duplicating_layers_in_large_models/" />
      <description></description>
      <pubDate>2023-10-01T11:34:38+00:00</pubDate>
      <updated>2023-10-01T11:34:38+00:00</updated>
      <content:encoded><![CDATA[
        <!-- SC_OFF --><div class="md"><p>Is there any notable work on duplicating layers in large feed forward models? In contrast to e.g. the brain which is essentially a directed graph most networks utilized nowerdays use a feed forward approach. E.g. transformers are able to attend to past tokens, but generate the tokens in a way where for a given token a given weight is not utilized at different stages in the feed forward pass. In my intuition this would lead to an issue where concepts (factual data as well as learned &quot;algorithms&quot;) might be duplicated as they are needed at different depths in the generation process and are sequentially dependent on one another. This does not directly make the model less capable, as it might learn the same concept at two layers sufficiently well, but it reduces the data and parameter efficiency and and might impact generalization capabilities. Using a full on brain like graph might be hard to implement/optimize/scale on current hardware and is tricky with the backprop. But is there any work on duplicating a few layers, placing them at different depths in large models. I would guess that this would be more impactful for large models. One would essentially trade compute for better data efficiency.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/floriv1999"> /u/floriv1999 </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/16wytmy/d_duplicating_layers_in_large_models/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16wytmy/d_duplicating_layers_in_large_models/">[comments]</a></span>
      ]]></content:encoded>
    </item>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16wn8qu</id>
      <title>[n] Introducing r/AudioAI: Any AI You Can Hear!</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16wn8qu/n_introducing_raudioai_any_ai_you_can_hear/" />
      <description></description>
      <pubDate>2023-10-01T00:52:01+00:00</pubDate>
      <updated>2023-10-01T00:52:01+00:00</updated>
      <content:encoded><![CDATA[
        <!-- SC_OFF --><div class="md"><p>I couldn't find any AI sub dedicated to audio, so I’ve created <a href="https://www.reddit.com/r/AudioAI">r/AudioAI</a> to serve as a hub for everything at the intersection of artificial intelligence and the world of sounds.</p> <p>AI-driven music, speech, audio production, and all other AI audio technologies.</p> <p>If anyone wants to be part of mod, let me know!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/chibop1"> /u/chibop1 </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/16wn8qu/n_introducing_raudioai_any_ai_you_can_hear/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16wn8qu/n_introducing_raudioai_any_ai_you_can_hear/">[comments]</a></span>
      ]]></content:encoded>
    </item>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16wbqcd</id>
      <title>[D] Struggling to get interviews what to do?</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16wbqcd/d_struggling_to_get_interviews_what_to_do/" />
      <description></description>
      <pubDate>2023-09-30T16:51:12+00:00</pubDate>
      <updated>2023-09-30T16:51:12+00:00</updated>
      <content:encoded><![CDATA[
        <!-- SC_OFF --><div class="md"><p>Edit: I am a USA citizen so no need for sponsorship.</p> <p>I have 4 yoe in a start up company and a phd four publications 2 in high level math journals and 2 CV/DL papers in A journals and also 4 patents. I have experience with most common Cv tasks eg object detection, Multi object tracking, 2d/3d human pose estimation and monocular depth estimation. I’m well versed in typical network building blocks eg conv nets, FFNs, transformers, Diffusion etc. I have a little experience with NLP like NLTK and TTS networks. Also some other general dev technologies like ec2,s3,sql,mongoose, etc. </p> <p>That all being said I can’t seem to even get interviews these days just straight rejections not talking to recruiters. On the other hand in 2020, I was just searching for jobs passively and had something like a 75% success rate with getting interviews. I know the job market has changed but I’m a lot more experienced at this time than then and having abysmal luck.</p> <p>Anyone have any advice would be happy to share my resume if that would make it easier to give advice. Also open to hearing what other technologies o should/could learn.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/AbjectDrink3276"> /u/AbjectDrink3276 </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/16wbqcd/d_struggling_to_get_interviews_what_to_do/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16wbqcd/d_struggling_to_get_interviews_what_to_do/">[comments]</a></span>
      ]]></content:encoded>
    </item>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16x2658</id>
      <title>[D] Multiple single class segmentation vs single multiclass segmentation models</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16x2658/d_multiple_single_class_segmentation_vs_single/" />
      <description></description>
      <pubDate>2023-10-01T14:08:42+00:00</pubDate>
      <updated>2023-10-01T14:08:42+00:00</updated>
      <content:encoded><![CDATA[
        &#32; submitted by &#32; <a href="https://www.reddit.com/user/waterstrider123"> /u/waterstrider123 </a> <br /> <span><a href="https://www.reddit.com/r/learnmachinelearning/comments/16w8zz5/multiple_single_class_segmentation_vs_single/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16x2658/d_multiple_single_class_segmentation_vs_single/">[comments]</a></span>
      ]]></content:encoded>
    </item>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16x975p</id>
      <title>[P] Simplest model to run with limited hardware</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16x975p/p_simplest_model_to_run_with_limited_hardware/" />
      <description></description>
      <pubDate>2023-10-01T18:46:32+00:00</pubDate>
      <updated>2023-10-01T18:46:32+00:00</updated>
      <content:encoded><![CDATA[
        <!-- SC_OFF --><div class="md"><p>We want to run (not train, i.e. think single forward pass only) an ML algorithm on a machine with very limited resources.</p> <p>Which model could we use to show off the possibilities?</p> <p>If the benchmark is an MLP for binary image classification, what else could we do with a similar scale of operations?</p> <p>E.g.</p> <p>Which model is the simplest for e.g. text-to-image generation?</p> <p>Any other ML models that are simple enough to run and if initialized with good params, does something impressive</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/2i2i_tokenized_time"> /u/2i2i_tokenized_time </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/16x975p/p_simplest_model_to_run_with_limited_hardware/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16x975p/p_simplest_model_to_run_with_limited_hardware/">[comments]</a></span>
      ]]></content:encoded>
    </item>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16wf3lk</id>
      <title>[D] (How) Can you estimate inference speed of a NN model on given hardware?</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16wf3lk/d_how_can_you_estimate_inference_speed_of_a_nn/" />
      <description></description>
      <pubDate>2023-09-30T19:10:51+00:00</pubDate>
      <updated>2023-09-30T19:10:51+00:00</updated>
      <content:encoded><![CDATA[
        <!-- SC_OFF --><div class="md"><p>How, outside of testing, do you estimate how quickly a specific model will run on some hardware? Anything about time is rarely mentioned in papers and if it is, it's more likely to talk about training, unless authors are specifically proud of their speed (like YOLO). Even less so in any README.</p> <p>Some way to translate numbers of parameters into seconds on a given GPU/CPU, any rules of thumb better than just setting up everything every time?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/teleoflexuous"> /u/teleoflexuous </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/16wf3lk/d_how_can_you_estimate_inference_speed_of_a_nn/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16wf3lk/d_how_can_you_estimate_inference_speed_of_a_nn/">[comments]</a></span>
      ]]></content:encoded>
    </item>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16x5d0h</id>
      <title>Metagpt use case [D]</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16x5d0h/metagpt_use_case_d/" />
      <description></description>
      <pubDate>2023-10-01T16:17:20+00:00</pubDate>
      <updated>2023-10-01T16:17:20+00:00</updated>
      <content:encoded><![CDATA[
        <!-- SC_OFF --><div class="md"><p>Guys, i am currently working building a project, there are certain tasks like building a ml model using certain use-cases. I wish to automate this task, do u think metagpt is a good fit for the same. </p> <p>Let me know if you need any further information!!</p> <p>EDIT: </p> <p>One of the tasks my app needs to do is to convert image to text (aim to implement image captioning). So, if i give metaGPT the requirements for my project, is it possible it will give me the code which I need. I need to save certain tasks here so that I can focus more on operation and design side. </p> <p>Edit: it seems, such kind of vague questions are not encouraged on this platform, I will work and will straigh away ask questions which are quite good and meet the standards of this platform. Thanks!!</p> <p>Thanks!!</p> <p>Always have a massive respect for this community!!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/aristotleTheFake"> /u/aristotleTheFake </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/16x5d0h/metagpt_use_case_d/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16x5d0h/metagpt_use_case_d/">[comments]</a></span>
      ]]></content:encoded>
    </item>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16w9kz7</id>
      <title>Arxiv [D]ives - Segment Anything</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16w9kz7/arxiv_dives_segment_anything/" />
      <description></description>
      <pubDate>2023-09-30T15:20:55+00:00</pubDate>
      <updated>2023-09-30T15:20:55+00:00</updated>
      <content:encoded><![CDATA[
        <table> <tr><td> <a href="https://www.reddit.com/r/MachineLearning/comments/16w9kz7/arxiv_dives_segment_anything/"> <img alt="Arxiv [D]ives - Segment Anything" src="https://external-preview.redd.it/qWxLBZ6J6moQBncdFCWH4ZA1VCOFyZfJDVCR-RJEqlA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ba7c3cbb91ae56110238f982493702695a85d537" title="Arxiv [D]ives - Segment Anything" /> </a> </td><td> <!-- SC_OFF --><div class="md"><p>Every Friday for the past few months we’ve been hosting a public paper club called “Arxiv Dives”. We pick a paper and dive deep into it and chat about it as a group.</p> <p>There are a lot of gems of knowledge hidden in these research papers, and the main motivation is simply to keep up with most impactful techniques in the field by taking the time to dive in and discuss. </p> <p>The attendees so far have been great, and would love for anyone is interested to join!</p> <p><a href="https://lu.ma/oxenbookclub">https://lu.ma/oxenbookclub</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/FallMindless3563"> /u/FallMindless3563 </a> <br /> <span><a href="https://blog.oxen.ai/arxiv-dives-segment-anything/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16w9kz7/arxiv_dives_segment_anything/">[comments]</a></span> </td></tr></table>
      ]]></content:encoded>
    </item>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16w08p1</id>
      <title>[D] Deploy the Mistral 7b Generative Model on an A10 GPU on AWS</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16w08p1/d_deploy_the_mistral_7b_generative_model_on_an/" />
      <description></description>
      <pubDate>2023-09-30T07:11:38+00:00</pubDate>
      <updated>2023-09-30T07:11:38+00:00</updated>
      <content:encoded><![CDATA[
        <!-- SC_OFF --><div class="md"><p>Hello,</p> <p>The Mistral 7b AI model beats LLaMA 2 7b on all benchmarks and LLaMA 2 13b in many benchmarks. It is actually even on par with the LLaMA 1 34b model.</p> <p>So I made a quick video about how to deploy this model on an A10 GPU on an AWS EC2 g5.4xlarge instance:</p> <p><a href="https://nlpcloud.com/deploy-mistral-7b-on-a10-gpu-on-aws.html?utm_source=reddit&amp;utm_campaign=i859w625-3816-81ed-a265-0242ac140019">https://nlpcloud.com/deploy-mistral-7b-on-a10-gpu-on-aws.html</a></p> <p>I hope it will be useful. If you have recommendations about how to improve this video please don't hesitate to let me know, that will be very much appreciated!</p> <p>Julien</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/juliensalinas"> /u/juliensalinas </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/16w08p1/d_deploy_the_mistral_7b_generative_model_on_an/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16w08p1/d_deploy_the_mistral_7b_generative_model_on_an/">[comments]</a></span>
      ]]></content:encoded>
    </item>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16w7lcl</id>
      <title>[D] What exactly are the compute requirements for training a dense model versus an MoE?</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16w7lcl/d_what_exactly_are_the_compute_requirements_for/" />
      <description></description>
      <pubDate>2023-09-30T14:00:29+00:00</pubDate>
      <updated>2023-09-30T14:00:29+00:00</updated>
      <content:encoded><![CDATA[
        <!-- SC_OFF --><div class="md"><p>Hi, New to ML, I can't find a clear answer to this question. I find references online to a 1.8 trillion parameter model taking up the computational power of a 10B model, yet I also hear that the memory requirements a lot higher for an MoE?</p> <p>If I was interested in training/inferencing, for example, a 15M dense model, or a 60M MoE with 4 15M experts. whats the difference gonna be?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/vatsadev"> /u/vatsadev </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/16w7lcl/d_what_exactly_are_the_compute_requirements_for/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16w7lcl/d_what_exactly_are_the_compute_requirements_for/">[comments]</a></span>
      ]]></content:encoded>
    </item>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16wjrnu</id>
      <title>[P]Handling categorical missing data in churn prediction model for telecom data</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16wjrnu/phandling_categorical_missing_data_in_churn/" />
      <description></description>
      <pubDate>2023-09-30T22:22:07+00:00</pubDate>
      <updated>2023-09-30T22:22:07+00:00</updated>
      <content:encoded><![CDATA[
        <!-- SC_OFF --><div class="md"><h1>I am working on a telecom dataset where I need to fit a model to for predicting churn(yes or no). There are a lot of categorical data with missing values( total values 7043). What is the best way to handle missing data in this case, is it better to ignore it or any other better imputation method?</h1> <pre><code>Data columns (total 21 columns): customerID 7043 non-null object gender 7043 non-null object Age 7043 non-null int64 Partner 7043 non-null object Dependents 7043 non-null object tenure 7043 non-null int64 PhoneService 7043 non-null object MultipleLines 6500 non-null object InternetService 6500 non-null object OnlineSecurity 7043 non-null object OnlineBackup 7043 non-null object DeviceProtection 7043 non-null object TechSupport 7043 non-null object StreamingTV 6500 non-null object StreamingMovies 6500 non-null object Contract 6500 non-null object PaperlessBilling 7043 non-null object PaymentMethod 6500 non-null object MonthlyCharges 7043 non-null float64 TotalCharges 7043 non-null object Churn 7043 non-null object </code></pre> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/guyloveskissing"> /u/guyloveskissing </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/16wjrnu/phandling_categorical_missing_data_in_churn/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16wjrnu/phandling_categorical_missing_data_in_churn/">[comments]</a></span>
      ]]></content:encoded>
    </item>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16vkwhk</id>
      <title>[P] Carton – Run any ML model from any programming language</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16vkwhk/p_carton_run_any_ml_model_from_any_programming/" />
      <description></description>
      <pubDate>2023-09-29T19:28:10+00:00</pubDate>
      <updated>2023-09-29T19:28:10+00:00</updated>
      <content:encoded><![CDATA[
        <!-- SC_OFF --><div class="md"><p>Hi! I just open-sourced a project that I've been working on for a while and wanted to see what you think!</p> <p>The goal of Carton (<a href="https://carton.run/">https://carton.run</a>) is to let you use a single interface to run any machine learning model from any programming language.</p> <p>It’s currently difficult to integrate models that use different technologies (e.g. TensorRT, Ludwig, TorchScript, JAX, GGML, etc) into your application, especially if you’re not using Python. Even if you learn the details of integrating each of these frameworks, running multiple frameworks in one process can cause hard-to-debug crashes.</p> <p>Ideally, the ML framework a model was developed in should just be an implementation detail. Carton lets you decouple your application from specific ML frameworks so you can focus on the problem you actually want to solve.</p> <p>At a high level, the way Carton works is by running models in their own processes and using an IPC system to communicate back and forth with low overhead. Carton is primarily implemented in Rust, with bindings to other languages. There are lots more details linked in the architecture doc below.</p> <p>Importantly, Carton uses your model’s original underlying framework (e.g. PyTorch) under the hood to actually execute the model. This is meaningful because it makes Carton composable with other technologies. For example, it’s easy to use custom ops, TensorRT, etc without changes. This lets you keep up with cutting-edge advances, but decouples them from your application.</p> <p>I’ve been working on Carton for almost a year now and I open sourced it on Wednesday!</p> <p>Some useful links:</p> <ul> <li>Website, docs, quickstart - <a href="https://carton.run">https://carton.run</a></li> <li>Explore existing models - <a href="https://carton.pub">https://carton.pub</a></li> <li>Repo - <a href="https://github.com/VivekPanyam/carton">https://github.com/VivekPanyam/carton</a></li> <li>Architecture - <a href="https://github.com/VivekPanyam/carton/blob/main/ARCHITECTURE.md">https://github.com/VivekPanyam/carton/blob/main/ARCHITECTURE.md</a></li> </ul> <p>Please let me know what you think!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/vpanyam"> /u/vpanyam </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/16vkwhk/p_carton_run_any_ml_model_from_any_programming/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16vkwhk/p_carton_run_any_ml_model_from_any_programming/">[comments]</a></span>
      ]]></content:encoded>
    </item>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16vc0hp</id>
      <title>[R] RealFill: Reference-Driven Generation for Authentic Image Completion</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16vc0hp/r_realfill_referencedriven_generation_for/" />
      <description></description>
      <pubDate>2023-09-29T13:42:47+00:00</pubDate>
      <updated>2023-09-29T13:42:47+00:00</updated>
      <content:encoded><![CDATA[
        <table> <tr><td> <a href="https://www.reddit.com/r/MachineLearning/comments/16vc0hp/r_realfill_referencedriven_generation_for/"> <img alt="[R] RealFill: Reference-Driven Generation for Authentic Image Completion" src="https://external-preview.redd.it/ImWTCUOcR47SWrmxStU5b2FjJoMHcZHYtoAiLp884yI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3e569815954293cf6d30fd8d177f1724aba33068" title="[R] RealFill: Reference-Driven Generation for Authentic Image Completion" /> </a> </td><td> <!-- SC_OFF --><div class="md"><p>Project page: <a href="https://realfill.github.io/">https://realfill.github.io/</a></p> <p>Paper: <a href="https://arxiv.org/abs/2309.16668">https://arxiv.org/abs/2309.16668</a></p> <p><a href="https://preview.redd.it/cj2hqx8n77rb1.jpg?width=4200&amp;format=pjpg&amp;auto=webp&amp;s=a14388d4d8673bd0625d9849fc537db85d703283">RealFill is able to complete the image with what should have been there.</a></p> <p><strong>Abstract</strong></p> <blockquote> <p>Recent advances in generative imagery have brought forth outpainting and inpainting models that can produce high-quality, plausible image content in unknown regions, but the content these models hallucinate is necessarily inauthentic, since the models lack sufficient context about the true scene. In this work, we propose <strong><em>RealFill</em></strong>, a novel generative approach for image completion that fills in missing regions of an image with the content that <em>should have been there</em>. RealFill is a generative inpainting model that is personalized using only a few reference images of a scene. These reference images do not have to be aligned with the target image, and can be taken with drastically varying viewpoints, lighting conditions, camera apertures, or image styles. Once personalized, RealFill is able to complete a target image with visually compelling contents that are faithful to the original scene. We evaluate RealFill on a new image completion benchmark that covers a set of diverse and challenging scenarios, and find that it outperforms existing approaches by a large margin.</p> </blockquote> <p>&#x200b;</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/StrawberryNumberNine"> /u/StrawberryNumberNine </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/16vc0hp/r_realfill_referencedriven_generation_for/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16vc0hp/r_realfill_referencedriven_generation_for/">[comments]</a></span> </td></tr></table>
      ]]></content:encoded>
    </item>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16vshvy</id>
      <title>[R] Pathway to self-learning mathematics and statistics for ML research</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16vshvy/r_pathway_to_selflearning_mathematics_and/" />
      <description></description>
      <pubDate>2023-09-30T00:30:39+00:00</pubDate>
      <updated>2023-09-30T00:30:39+00:00</updated>
      <content:encoded><![CDATA[
        <!-- SC_OFF --><div class="md"><p>Hey everyone. I am very passionate about getting in ML research and was wondering what the learning pathway was, particularly with regards to the theoretical Math and Statistics involved.</p> <p>For context:</p> <ul> <li>I am a second year undergraduate. By the end of this year, I will have taken and finished A Multivariable Calculus with Proofs course, so that is my current starting point.</li> <li>I have been working with ML for the last 3 years and am proficient in Python and frameworks like PyTorch.</li> <li>I have also made my own implementation of several research papers (LSTMs, GRUs, Transformers, ELMo, BERT, GPT, as well as a few computer vision papers).</li> </ul> <p>I have a good general intuition of how deep learning works, but I want to formalise this knowledge with the adequate mathematical background so that I can eventually pursue a career in research. I understand that I have plenty of time until I reach there, and I am willing to dedicate it to grinding out the math and statistical knowledge required.</p> <p>I have done my research on this sub and other forums, and here are a few resources that stood out:</p> <ul> <li>Mathematics for Machine Learning by Deisenroth, Faisal and Ong</li> <li>Advanced Calculus of Several Variables by C. H. Edwards Jr.</li> <li>Mathematical Methods Lecture Notes from Imperial College by Deisenroth and Cheraghchi</li> <li>The original information theory paper by Shannon</li> <li>The Elements of Statistical Learning by Hastie, Tibshirani and Friedman</li> <li>Pattern Recognition and Machine Learning by Bishop</li> <li>The Probabalistic Machine Learning Series by Kevin P. Murphy</li> <li>Deep Learning by Goodfellow, Bengio and Courville</li> <li>Mathematics of Machine Learning on MIT OCW (<a href="https://ocw.mit.edu/courses/18-657-mathematics-of-machine-learning-fall-2015/">here</a>)</li> </ul> <p>My question is, what order should I start self-learning in, given the (somewhat limited) background knowledge I have? Also, are there any other resources that would help?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Far_Clothes_5054"> /u/Far_Clothes_5054 </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/16vshvy/r_pathway_to_selflearning_mathematics_and/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16vshvy/r_pathway_to_selflearning_mathematics_and/">[comments]</a></span>
      ]]></content:encoded>
    </item>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16vmmxz</id>
      <title>[R] Gsgen: Text-to-3D using Gaussian Splatting</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16vmmxz/r_gsgen_textto3d_using_gaussian_splatting/" />
      <description></description>
      <pubDate>2023-09-29T20:38:01+00:00</pubDate>
      <updated>2023-09-29T20:38:01+00:00</updated>
      <content:encoded><![CDATA[
        <!-- SC_OFF --><div class="md"><p><a href="https://gsgen3d.github.io/">Project Page</a><br /> <a href="https://arxiv.org/abs/2309.16585">Paper</a><br /> <a href="https://github.com/gsgen3d/gsgen">Code</a> </p> <blockquote> <p>In this paper, we present Gaussian Splatting based text-to-3D generation (GSGEN), a novel approach for generating high-quality 3D objects. Previous methods suffer from inaccurate geometry and limited fidelity due to the absence of 3D prior and proper representation. We leverage 3D Gaussian Splatting, a recent state-of-the-art representation, to address existing shortcomings by exploiting the explicit nature that enables the incorporation of 3D prior. Specifically, our method adopts a progressive optimization strategy, which includes a geometry optimization stage and an appearance refinement stage. In geometry optimization, a coarse representation is established under a 3D geometry prior along with the ordinary 2D SDS loss, ensuring a sensible and 3D-consistent rough shape. Subsequently, the obtained Gaussians undergo an iterative refinement to enrich details. In this stage, we increase the number of Gaussians by compactness-based densification to enhance continuity and improve fidelity. With these designs, our approach can generate 3D content with delicate details and more accurate geometry. Extensive evaluations demonstrate the effectiveness of our method, especially for capturing high-frequency components.</p> </blockquote> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Sirisian"> /u/Sirisian </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/16vmmxz/r_gsgen_textto3d_using_gaussian_splatting/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16vmmxz/r_gsgen_textto3d_using_gaussian_splatting/">[comments]</a></span>
      ]]></content:encoded>
    </item>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16vvh84</id>
      <title>[D] CIDEr values in PaLI model and XM 3600 dataset</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16vvh84/d_cider_values_in_pali_model_and_xm_3600_dataset/" />
      <description></description>
      <pubDate>2023-09-30T02:46:43+00:00</pubDate>
      <updated>2023-09-30T02:46:43+00:00</updated>
      <content:encoded><![CDATA[
        <!-- SC_OFF --><div class="md"><p>I am reading <a href="https://arxiv.org/abs/2209.06794">PaLI: A Jointly-Scaled Multilingual Language-Image Model </a>. In their table 2 (page 6), it's reported that Thapliyal et al. (2022) (0.8B) model got 57.6 of CIDEr on XM 3600 for English. Thapliyal et al. (2022) is <a href="https://arxiv.org/abs/2205.12522">Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset</a>. However in this paper, the CIDEr values are reported less than 1. For example, the largest model got 0.584 of CIDEr on XM 3600 for English.</p> <p>Could someone explain to me why those values have great differences?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/KingsmanVince"> /u/KingsmanVince </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/16vvh84/d_cider_values_in_pali_model_and_xm_3600_dataset/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16vvh84/d_cider_values_in_pali_model_and_xm_3600_dataset/">[comments]</a></span>
      ]]></content:encoded>
    </item>
    
    <item>
      <id>https://www.reddit.com/r/MachineLearning/t3_16vq1a0</id>
      <title>[R] Drive Like a Human: Rethinking Autonomous Driving with Large Language Models</title>
      <link rel="alternate" href="https://www.reddit.com/r/MachineLearning/comments/16vq1a0/r_drive_like_a_human_rethinking_autonomous/" />
      <description></description>
      <pubDate>2023-09-29T22:49:36+00:00</pubDate>
      <updated>2023-09-29T22:49:36+00:00</updated>
      <content:encoded><![CDATA[
        <!-- SC_OFF --><div class="md"><p>Paper - <a href="https://arxiv.org/abs/2307.07162">https://arxiv.org/abs/2307.07162</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/MysteryInc152"> /u/MysteryInc152 </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/16vq1a0/r_drive_like_a_human_rethinking_autonomous/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/16vq1a0/r_drive_like_a_human_rethinking_autonomous/">[comments]</a></span>
      ]]></content:encoded>
    </item>
    
  </channel>
</rss>